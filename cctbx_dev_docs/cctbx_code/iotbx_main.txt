

 *******************************************************************************
iotbx/__init__.py
from __future__ import absolute_import, division, print_function
import libtbx.forward_compatibility
from libtbx.version import get_version

__version__ = get_version()


 *******************************************************************************


 *******************************************************************************
iotbx/builders.py
"""Tools for creating crystal_symmetry, x-ray structure, twinning objects
"""
from __future__ import absolute_import, division, print_function

import libtbx
import scitbx.math
from cctbx.array_family import flex
from cctbx import crystal
from cctbx import xray
from cctbx import sgtbx
from cctbx import adp_restraints, geometry_restraints

import libtbx.load_env
import six
if (libtbx.env.has_module(name="smtbx")):
  from smtbx.refinement.restraints import adp_restraints as smtbx_adp_restraints
else:
  smtbx_adp_restraints = None


def mixin_builder_class(mixin_name, *mixed_builders):
  """ This function will make on-the-fly a builder class with the given
  name, that inherits from the given builders """
  return type.__new__(type, mixin_name, mixed_builders, {})


class crystal_symmetry_builder(object):

  def make_crystal_symmetry(self, unit_cell, space_group):
    self.crystal_symmetry = crystal.symmetry(unit_cell=unit_cell,
                                             space_group=space_group)
    self.crystal_symmetry.unit_cell().parameter_sigmas = (0,)*6

  def set_unit_cell_parameter_sigmas(self, s):
    self.crystal_symmetry.unit_cell().parameter_sigmas = s


class electron_density_peak(object):

  __slots__ = ('site', 'height')

  def __init__(self, site, height):
    self.site = site
    self.height = height


class crystal_structure_builder(crystal_symmetry_builder):

  def __init__(self,
               set_grad_flags=True,
               min_distance_sym_equiv=0.5):
    super(crystal_structure_builder, self).__init__()
    self.set_grad_flags = set_grad_flags
    self.min_distance_sym_equiv = min_distance_sym_equiv
    self.conformer_indices = None
    self.sym_excl_indices = None
    self.electron_density_peaks = []
    self.index_of_scatterer_named = {}
    self.residue_class_of_residue_number = {}
    self.residue_numbers_having_class = {}

  def make_structure(self):
    self.structure = xray.structure(
      special_position_settings=crystal.special_position_settings(
        crystal_symmetry=self.crystal_symmetry,
        min_distance_sym_equiv=self.min_distance_sym_equiv))

  def add_residue(self, residue_number, residue_class):
    self.residue_class_of_residue_number[residue_number] = residue_class
    self.residue_numbers_having_class.setdefault(
      residue_class, []).append(residue_number)


  def add_scatterer(self, scatterer, is_refined,
                    occupancy_includes_symmetry_factor,
                    conformer_index=None,
                    sym_excl_index=None,
                    residue_number=None,
                    residue_class=None):
    """ If the parameter set_grad_flags passed to the constructor was True,
        the scatterer.flags.grad_xxx() will be set to True
        if the corresponding variables have been found to be refined
        by the parser using this builder.
    """
    if self.set_grad_flags:
      f = scatterer.flags
      if all(is_refined[0:3]):
        f.set_grad_site(True)
      if is_refined[3]:
        f.set_grad_occupancy(True)
      if f.use_u_iso():
        if is_refined[4]:
          f.set_grad_u_iso(True)
      else:
        if all(is_refined[-6:]):
          f.set_grad_u_aniso(True)
    self.structure.add_scatterer(scatterer)
    scatterer_index = len(self.structure.scatterers()) - 1

    if occupancy_includes_symmetry_factor:
      sc = self.structure.scatterers()[-1]
      sc.occupancy /= sc.weight_without_occupancy()
      occ = scitbx.math.continued_fraction.from_real(sc.occupancy, eps=1e-5)
      r_occ = occ.as_rational()
      sc.occupancy = round(r_occ.numerator() / r_occ.denominator(), 5)

    if conformer_index is not None:
      if self.conformer_indices is None: self.conformer_indices = flex.size_t()
      self.conformer_indices.append(conformer_index)
    if sym_excl_index is not None:
      if self.sym_excl_indices is None: self.sym_excl_indices = flex.size_t()
      self.sym_excl_indices.append(sym_excl_index)
    self.index_of_scatterer_named[
      (residue_number, scatterer.label.lower())] = scatterer_index


  def add_electron_density_peak(self, site, height):
    self.electron_density_peaks.append(electron_density_peak(site, height))


class restrained_crystal_structure_builder(crystal_structure_builder):

  geometry_restraint_types = {
    'bond': geometry_restraints.bond_simple_proxy,
    'angle': geometry_restraints.angle_proxy,
    'dihedral': geometry_restraints.dihedral_proxy,
    'planarity': geometry_restraints.planarity_proxy,
    'chirality': geometry_restraints.chirality_proxy,
    'bond_similarity': geometry_restraints.bond_similarity_proxy,
  }
  if smtbx_adp_restraints is not None:
    adp_restraint_types = {
      'adp_similarity': smtbx_adp_restraints.adp_similarity_restraints,
      'rigid_bond': smtbx_adp_restraints.rigid_bond_restraints,
      'rigu': smtbx_adp_restraints.rigu_restraints,
      'isotropic_adp': smtbx_adp_restraints.isotropic_adp_restraints,
      'fixed_u_eq_adp': smtbx_adp_restraints.fixed_u_eq_adp_restraints,
      'adp_u_eq_similarity': smtbx_adp_restraints.adp_u_eq_similarity_restraints,
      'adp_volume_similarity': smtbx_adp_restraints.adp_volume_similarity_restraints,
    }
    adp_restraint_types_requiring_connectivity_only = {
      'adp_similarity': smtbx_adp_restraints.adp_similarity_restraints,
      'rigid_bond': smtbx_adp_restraints.rigid_bond_restraints,
      'rigu': smtbx_adp_restraints.rigu_restraints,
    }
  else:
    adp_restraint_types = {}
    adp_restraint_types_requiring_connectivity_only = {}

  def __init__(self, *args, **kwds):
    super(restrained_crystal_structure_builder, self).__init__(*args, **kwds)
    geom = geometry_restraints
    adp = adp_restraints
    self._proxies = {}

    self._proxies = {
      'bond': geometry_restraints.shared_bond_simple_proxy(),
      'angle': geometry_restraints.shared_angle_proxy(),
      'dihedral': geometry_restraints.shared_dihedral_proxy(),
      'planarity': geometry_restraints.shared_planarity_proxy(),
      'chirality': geometry_restraints.shared_chirality_proxy(),
      'bond_similarity': geometry_restraints.shared_bond_similarity_proxy(),
      'adp_similarity': adp_restraints.shared_adp_similarity_proxy(),
      'rigid_bond': adp_restraints.shared_rigid_bond_proxy(),
      'rigu': adp_restraints.shared_rigu_proxy(),
      'isotropic_adp': adp_restraints.shared_isotropic_adp_proxy(),
      'fixed_u_eq_adp': adp_restraints.shared_fixed_u_eq_adp_proxy(),
      'adp_u_eq_similarity': adp_restraints.shared_adp_u_eq_similarity_proxy(),
      'adp_volume_similarity': adp_restraints.shared_adp_volume_similarity_proxy(),
    }

  def add_proxy(self, restraint_type, *args, **kwds):
    if restraint_type in self.adp_restraint_types:
      kwds['proxies'] = self._proxies[restraint_type]
      if restraint_type not in self.adp_restraint_types_requiring_connectivity_only or\
         'pair_sym_table' not in kwds:
        kwds['xray_structure'] = self.structure
      self.adp_restraint_types[restraint_type](**kwds)
    else:
      proxy_type = self.geometry_restraint_types[restraint_type]
      proxy=proxy_type(**kwds)
      self._proxies[restraint_type].append(proxy)

  def process_restraint(self, restraint_type, *args, **kwds):
    def replace_None_with_unit_matrix(sym_ops):
      for i, sym_op in enumerate(sym_ops):
        if sym_op is None:
          sym_ops[i] = sgtbx.rt_mx()
      return sym_ops
    if 'sym_ops' in kwds:
      sym_ops = kwds['sym_ops']
      for i, sym_op in enumerate(sym_ops):
        if sym_op is not None and not isinstance(sym_op, sgtbx.rt_mx):
          if len(sym_op) == 2:
            assert restraint_type == 'bond_similarity'
            sym_op_pair = sym_op
            for j, sym_op in enumerate(sym_op_pair):
              if sym_op is not None and not isinstance(sym_op, sgtbx.rt_mx):
                sym_op_pair[j] = sgtbx.rt_mx(sym_op)
            sym_ops[i] = sym_op_pair
          else:
            sym_ops[i] = sgtbx.rt_mx(sym_op)
      if sym_ops.count(None) == len(sym_ops):
        del kwds['sym_ops']
      elif restraint_type == 'bond':
        # map to asu if necessary
        if sym_ops.count(None) == 2:
          rt_mx_ji = None
        elif sym_ops.count(None) == 1:
          rt_mx_ji = sym_ops[1]
          if rt_mx_ji is None:
            rt_mx_ji = sym_ops[0]
            kwds['i_seqs'].reverse()
        else:
          rt_mx_ji_1 = sym_ops[0]
          rt_mx_ji_2 = sym_ops[1]
          rt_mx_ji_inv = rt_mx_ji_1.inverse()
          rt_mx_ji = rt_mx_ji_inv.multiply(rt_mx_ji_2)
        kwds['rt_mx_ji'] = rt_mx_ji
        del kwds['sym_ops']
      elif restraint_type == 'bond_similarity':
        kwds['sym_ops'] = []
        for i, sym_ops_ in enumerate(sym_ops):
          if sym_ops_ is None or isinstance(sym_ops_, sgtbx.rt_mx): continue
          if sym_ops_.count(None) == 2:
            rt_mx_ji = None
          elif sym_ops_.count(None) == 1:
            rt_mx_ji = sym_ops_[1]
            if rt_mx_ji is None:
              rt_mx_ji = sym_ops_[0]
              kwds['i_seqs'][i].reverse()
          else:
            rt_mx_ji_1 = sym_ops_[0]
            rt_mx_ji_2 = sym_ops_[1]
            rt_mx_ji_inv = rt_mx_ji_1.inverse()
            rt_mx_ji = rt_mx_ji_inv.multiply(rt_mx_ji_2)
          kwds['sym_ops'].append(rt_mx_ji)
        if kwds['sym_ops'].count(None) == len(sym_ops):
          del kwds['sym_ops']
        else:
          kwds['sym_ops'] = replace_None_with_unit_matrix(kwds['sym_ops'])
      else:
        kwds['sym_ops'] = replace_None_with_unit_matrix(sym_ops)
    elif 'rt_mx_ji' in kwds and kwds['rt_mx_ji'] is None:
      del kwds['rt_mx_ji']
    self.add_proxy(restraint_type, **kwds)

  @property
  def restraints_manager(self):
    from smtbx.refinement.restraints import manager
    kwds = dict([ ("%s_proxies" % name, value)
                  for name, value in six.iteritems(self.proxies()) ])
    return manager(**kwds)

  def proxies(self):
    return dict([
      (proxy_type, proxies) for proxy_type, proxies in six.iteritems(self._proxies)
      if len(proxies) != 0])

class reflection_data_source_builder(object):
  """ A builder which is passed the information about the reflections
  corresponding to the parsed

  Attributes:

    - reflection_file_format: a string to identify the format, compatible with
              iotbx.reflection_file_reader.any_reflection_file
    - data_change_of_basis_op: the instance of sgtbx.change_of_basis_op that
                          shall be applied to the Miller indices to match
                          the parsed model
    - data_scale: the scale of the data and their standard deviations
  """

  def create_shelx_reflection_data_source(self,
                                          format,
                                          indices_transform=None,
                                          change_of_basis_op=None,
                                          data_scale=1):
    """ format is one of 3, 4, 5, etc.
    data_scale scales the data and their standard deviations
    """
    assert [indices_transform, change_of_basis_op].count(None) == 1
    if change_of_basis_op is None:
      if indices_transform.is_unit_mx():
        change_of_basis_op = sgtbx.change_of_basis_op()
      else:
        r = sgtbx.rt_mx(indices_transform.new_denominator(24).transpose())
        change_of_basis_op = sgtbx.change_of_basis_op(r).inverse()
    self.reflection_file_format = "hklf%i" % format
    self.data_change_of_basis_op = change_of_basis_op
    self.data_scale = data_scale

class twinning_builder(object):
  """ Construct twin components
      They come as a tuple of instances of xray.twin_component.
      If R is the twin law, the i-th component corresponds to the domain
      where Miller indices are to be transformed by R^(i+1). The domain
      with untransformed Miller indices is omitted as its fraction is just
      1 minus the sum of the listed fractions.
      As a result, the 0th component holds the twin law.
  """

  twin_components = ()
  non_merohedral_twin_components_with_transformed_hkl = ()

  def make_merohedral_twinning(self, twin_law, fractions):
    self.twin_components = []
    if isinstance(twin_law, sgtbx.rt_mx): twin_law = twin_law.r()
    t = twin_law
    for f in fractions:
      self.twin_components.append(xray.twin_component(twin_law=t,
                                                      value=f,
                                                      grad=True))
      t = t.multiply(twin_law)

  def make_non_merohedral_twinning_with_transformed_hkl(self, fractions):
    self.non_merohedral_twin_components_with_transformed_hkl = fractions


# **********************************************
# Conditional import of smtbx-dependent builders
# **********************************************
if (libtbx.env.has_module(name="smtbx")):
  from iotbx.builders_depending_on_smtbx import *


 *******************************************************************************


 *******************************************************************************
iotbx/builders_depending_on_smtbx.py
"""Tools for creating builders using smtbx
"""
from __future__ import absolute_import, division, print_function

from smtbx.refinement import constraints, least_squares
import smtbx.refinement.constraints.adp
import smtbx.refinement.constraints.geometrical.all
import smtbx.refinement.constraints.occupancy

from iotbx.builders import \
     crystal_structure_builder, \
     restrained_crystal_structure_builder
from six.moves import range


class constrained_crystal_structure_builder(crystal_structure_builder):

  def __init__(self, *args, **kwds):
    super(constrained_crystal_structure_builder, self).__init__(*args, **kwds)
    self.constraints = []
    self.temperature_in_celsius = None

  def add_occupancy_pair_affine_constraint(self, scatterer_indices, linear_form):
    """ Add a constraint on the occupancies of a pair of scatterers that is
        affine, i.e. linear_form shall be ((a0, a1), b) and then
           a0*occ0 + a1*occ1 = b
        where (occ0, occ1) are the occupancies of the scatterers whose indices
        are given in `scatterer_indices`.
    """
    self.constraints.append(
      constraints.occupancy.occupancy_pair_affine_constraint(scatterer_indices,
                                                             linear_form))

  def add_u_iso_proportional_to_pivot_u_eq(self,
                                           u_iso_scatterer_index,
                                           u_eq_scatterer_index,
                                           multiplier):
    sc = self.structure.scatterers()
    sc_eq = sc[u_eq_scatterer_index]
    sc_iso = sc[u_iso_scatterer_index]
    if sc_iso.flags.use_u_iso():
      self.constraints.append(
        constraints.adp.u_iso_proportional_to_pivot_u_eq(
          u_eq_scatterer_idx=u_eq_scatterer_index,
          u_iso_scatterer_idx=u_iso_scatterer_index,
          multiplier=multiplier))

  def make_geometrical_constraint_type(self, constraint_name):
    return getattr(constraints.geometrical.all, constraint_name)

  def start_geometrical_constraint(self, type_,
                                   bond_length, rotating, stretching,
                                   pivot_relative_pos):
    self.first = len(self.structure.scatterers())

    self.current = type_(rotating=rotating,
                         stretching=stretching,
                         bond_length=bond_length,
                         pivot=self.first + pivot_relative_pos)

  def end_geometrical_constraint(self):
    last = len(self.structure.scatterers())
    self.current.constrained_site_indices = tuple(range(self.first, last))
    self.constraints.append(self.current)


class weighting_scheme_builder(object):

  def make_shelx_weighting_scheme(self, a, b, c=0, d=0, e=0, f=1/3):
    assert f == 1/3, "Non-Wilsonian ShelX weighting not supported"
    if c == 0 and d == 0 and e == 0:
      self.weighting_scheme = \
          least_squares.mainstream_shelx_weighting(a, b)
    else:
      raise NotImplementedError(
        "ShelX weighting scheme with non-zero parameter c, d or e")

class weighted_constrained_restrained_crystal_structure_builder(
  constrained_crystal_structure_builder,
  restrained_crystal_structure_builder,
  weighting_scheme_builder):
  pass


 *******************************************************************************


 *******************************************************************************
iotbx/cif_mtz_data_labels.py
'''
Data labels used in CIF and MTZ formats and a mapping
between the two formats

References:
http://mmcif.wwpdb.org/dictionaries/mmcif_pdbx_v50.dic/Categories/refln.html
http://www.ccp4.ac.uk/html/cif2mtz.html
'''
from __future__ import absolute_import, division, print_function

# =============================================================================
# intensities
phenix_to_cif_intensities = {
  # 'IOBS': '_refln.F_squared_meas',
  'IOBS': '_refln.intensity_meas',           # This is used in PDB
  # 'SIGIOBS': '_refln.F_squared_sigma',
  'SIGIOBS': '_refln.intensity_sigma',       # This is used in PDB
  'IOBS(+)': '_refln.pdbx_I_plus',
  'SIGIOBS(+)': '_refln.pdbx_I_plus_sigma',
  'IOBS(-)': '_refln.pdbx_I_minus',
  'SIGIOBS(-)': '_refln.pdbx_I_minus_sigma',
  # 'I-obs': '_refln.F_squared_meas',        # obsoleted
  # 'SIGI-obs': '_refln.F_squared_sigma',    # obsoleted
  'I-obs': '_refln.intensity_meas',
  'SIGI-obs': '_refln.intensity_sigma',
  'I-obs(+)': '_refln.pdbx_I_plus',
  'SIGI-obs(+)': '_refln.pdbx_I_plus_sigma',
  'I-obs(-)': '_refln.pdbx_I_minus',
  'SIGI-obs(-)': '_refln.pdbx_I_minus_sigma',
}

ccp4_to_cif_intensities = {
  # 'I': '_refln.F_squared_meas',            # which I to prefer?
  'I': '_refln.intensity_meas',              # This is used in PDB
  'SIGI': '_refln.intensity_sigma',          # This is used in PDB
  # 'SIGI': '_refln.F_squared_sigma',        # which SIGI to prefer?
  'I(+)': '_refln.pdbx_I_plus',
  'SIGI(+)': '_refln.pdbx_I_plus_sigma',
  'I(-)': '_refln.pdbx_I_minus',
  'SIGI(-)': '_refln.pdbx_I_minus_sigma',
  'DP': '_refln.pdbx_anom_difference',
  'SIGDP': '_refln.pdbx_anom_difference_sigma',
}

# -----------------------------------------------------------------------------
# amplitudes
phenix_to_cif_amplitudes = {
  'FOBS': '_refln.F_meas_au',
  'SIGFOBS': '_refln.F_meas_sigma_au',
  'FOBS(+)': '_refln.pdbx_F_plus',
  'SIGFOBS(+)': '_refln.pdbx_F_plus_sigma',
  'FOBS(-)': '_refln.pdbx_F_minus',
  'SIGFOBS(-)': '_refln.pdbx_F_minus_sigma',
  'F-obs': '_refln.F_meas_au',
  'SIGF-obs': '_refln.F_meas_sigma_au',
  'F-obs(+)': '_refln.pdbx_F_plus',
  'SIGF-obs(+)': '_refln.pdbx_F_plus_sigma',
  'F-obs(-)': '_refln.pdbx_F_minus',
  'SIGF-obs(-)': '_refln.pdbx_F_minus_sigma',
}

ccp4_to_cif_amplitudes = {
  'F': '_refln.F_meas_au',
  'SIGF': '_refln.F_meas_sigma_au',
  'FP': '_refln.F_meas_au',
  'SIGFP': '_refln.F_meas_sigma_au',
  'FC': '_refln.F_calc_au',
  'PHIC': '_refln.phase_calc',
  'PHIB': '_refln.phase_meas',
  'FOM': '_refln.fom',
  'FPART': '_refln.F_part_au',
  'PHIP': '_refln.phase_part',
  'F(+)': '_refln.pdbx_F_plus',
  'SIGF(+)': '_refln.pdbx_F_plus_sigma',
  'F(-)': '_refln.pdbx_F_minus',
  'SIGF(-)': '_refln.pdbx_F_minus_sigma',
  'DP': '_refln.pdbx_anom_difference',
  'SIGDP': '_refln.pdbx_anom_difference_sigma',
}

# -----------------------------------------------------------------------------
# map coefficients
phenix_to_cif_map_coefficients = {
  '2FOFCWT': '_refln.pdbx_FWT',
  'PH2FOFCWT': '_refln.pdbx_PHWT',
  'FOFCWT': '_refln.pdbx_DELFWT',
  'PHFOFCWT': '_refln.pdbx_DELPHWT',
}

ccp4_to_cif_map_coefficients = {
  'FWT': '_refln.pdbx_FWT',
  'PHWT': '_refln.pdbx_PHWT',
  'DELFWT': '_refln.pdbx_DELFWT',
  'DELPHWT': '_refln.pdbx_DELPHWT',
}

# -----------------------------------------------------------------------------
# Hendrickson-Lattman coefficients
phenix_to_cif_HL = {
  'HLA': '_refln.pdbx_HL_A_iso',
  'HLB': '_refln.pdbx_HL_B_iso',
  'HLC': '_refln.pdbx_HL_C_iso',
  'HLD': '_refln.pdbx_HL_D_iso',
}

ccp4_to_cif_HL = {
  'HLA': '_refln.pdbx_HL_A_iso',
  'HLB': '_refln.pdbx_HL_B_iso',
  'HLC': '_refln.pdbx_HL_C_iso',
  'HLD': '_refln.pdbx_HL_D_iso',
}

# -----------------------------------------------------------------------------
# R-free flag
phenix_to_cif_rfree = {
  'R-free-flags': '_refln.pdbx_r_free_flag'
}

ccp4_to_cif_rfree = {
  'FREE': '_refln.pdbx_r_free_flag',
}

# -----------------------------------------------------------------------------
# all mapping dictionaries
phenix_to_cif_labels_dict = dict()
phenix_to_cif_labels_dict.update(phenix_to_cif_intensities)
phenix_to_cif_labels_dict.update(phenix_to_cif_amplitudes)
phenix_to_cif_labels_dict.update(phenix_to_cif_map_coefficients)
phenix_to_cif_labels_dict.update(phenix_to_cif_HL)
phenix_to_cif_labels_dict.update(phenix_to_cif_rfree)

ccp4_to_cif_labels_dict = dict()
ccp4_to_cif_labels_dict.update(ccp4_to_cif_intensities)
ccp4_to_cif_labels_dict.update(ccp4_to_cif_amplitudes)
ccp4_to_cif_labels_dict.update(ccp4_to_cif_map_coefficients)
ccp4_to_cif_labels_dict.update(ccp4_to_cif_HL)
ccp4_to_cif_labels_dict.update(ccp4_to_cif_rfree)

# =============================================================================
# separate MTZ and CIF labels
mtz_intensity_labels = set(phenix_to_cif_intensities.keys())
mtz_intensity_labels.update(ccp4_to_cif_intensities.keys())

cif_intensity_labels = set(phenix_to_cif_intensities.values())
cif_intensity_labels.update(ccp4_to_cif_intensities.values())

mtz_amplitude_labels = set(phenix_to_cif_amplitudes.keys())
mtz_amplitude_labels.update(ccp4_to_cif_amplitudes.keys())

cif_amplitude_labels = set(phenix_to_cif_amplitudes.values())
cif_amplitude_labels.update(ccp4_to_cif_amplitudes.values())

mtz_map_coefficient_labels = set(phenix_to_cif_map_coefficients.keys())
mtz_map_coefficient_labels.update(ccp4_to_cif_map_coefficients.keys())

cif_map_coefficient_labels = set(phenix_to_cif_map_coefficients.values())
cif_map_coefficient_labels.update(ccp4_to_cif_map_coefficients.values())

mtz_HL_labels = set(phenix_to_cif_HL.keys())
mtz_HL_labels.update(ccp4_to_cif_HL.keys())

cif_HL_labels = set(phenix_to_cif_HL.values())
cif_HL_labels.update(ccp4_to_cif_HL.values())

mtz_rfree_labels = set(phenix_to_cif_rfree.keys())
mtz_rfree_labels.update(ccp4_to_cif_rfree.keys())

cif_rfree_labels = set(phenix_to_cif_rfree.values())
cif_rfree_labels.update(ccp4_to_cif_rfree.values())

# =============================================================================
# check new dictionary matches old dictionary (needs old version of
# mtz_as_cif.py)
# if (__name__ == '__main__'):
#   from iotbx.command_line import mtz_as_cif

#   assert (phenix_to_cif_labels_dict == mtz_as_cif.phenix_to_cif_labels_dict)
#   assert (ccp4_to_cif_labels_dict == mtz_as_cif.ccp4_to_cif_labels_dict)
def ccp4_label_from_cif(ciflabel):
  for mtzlabl,ciflabl in ccp4_to_cif_labels_dict.items():
    if ciflabel==ciflabl:
      return mtzlabl
  return None


 *******************************************************************************


 *******************************************************************************
iotbx/cli_parser.py
'''
Standard command-line parser for CCTBX programs

The CCTBXParser class will read files and process PHIL parameters from the
command-line as well as have standard command-line flags for showing the
PHIL scope and citations for a program.

'''
from __future__ import absolute_import, division, print_function

import argparse, getpass, logging, os, sys, time

from six.moves import cStringIO as StringIO

import iotbx.phil
import libtbx.phil

from iotbx.data_manager import DataManager, data_manager_type
from iotbx.file_reader import any_file
from libtbx import citations
from libtbx.program_template import ProgramTemplate
from libtbx.str_utils import wordwrap
from libtbx.utils import multi_out, null_out, show_times, Sorry

# =============================================================================
class ParserBase(argparse.ArgumentParser):

  def __init__(self, parse_files=True, parse_phil=True, parse_dir=False,
               *args, **kwargs):
    super(ParserBase, self).__init__(*args, **kwargs)

    # store options
    self.parse_files = parse_files
    self.parse_phil = parse_phil
    self.parse_dir = parse_dir

    # add default behavior for positional arguments
    if self.parse_files:
      self.add_argument('files', nargs='*', help='Input file(s) (e.g. model.cif)',
                        action=ParsePositionalArgumentsAction)
    if self.parse_phil:
      self.add_argument('phil', nargs='*', help='Parameter(s) (e.g. d_min=2.0)',
                        action=ParsePositionalArgumentsAction)
    if self.parse_dir:
      self.add_argument('dir', nargs='*', help='Input directory',
                        action=ParsePositionalArgumentsAction)

# =============================================================================
class ParsePositionalArgumentsAction(argparse.Action):
  '''
  This action is a first pass for command-line arguments. It does basic checks
  to see if an argument is a file, a directory, or a phil parameter (contains
  an equals sign). Command-line switches (options beginning with "-") are
  handled by default actions in the parser
  '''

  def __call__(self, parser, namespace, values, option_string=None):
    # figure out what options are set
    parse_files = hasattr(namespace, 'files')
    parse_phil = hasattr(namespace, 'phil')
    parse_dir = hasattr(namespace, 'dir')

    # get previous values or define default
    if parse_files and getattr(namespace, 'files') is not None:
      files = namespace.files
    else:
      files = []
    if parse_phil and getattr(namespace, 'phil') is not None:
      phil = namespace.phil
    else:
      phil = []
    if parse_dir and getattr(namespace, 'dir') is not None:
      directory = namespace.dir
    else:
      directory = []
    if hasattr(namespace, 'unknown') \
      and getattr(namespace, 'unknown') is not None:
      unknown = namespace.unknown
    else:
      unknown = []

    # separate values
    for value in values:
      if os.path.isfile(value):
        files.append(value)
      elif isinstance(value, str) and '=' in value:
        phil.append(value)
      elif os.path.isdir(value):
        directory.append(value)
      else:
        unknown.append(value)

    # update options
    if parse_files:
      setattr(namespace, 'files', files)
    else:
      unknown.extend(files)
    if parse_phil:
      setattr(namespace, 'phil', phil)
    else:
      unknown.extend(phil)
    if parse_dir:
      setattr(namespace, 'dir', directory)
    else:
      unknown.extend(directory)

    # store unknown values for custom processing (if available)
    setattr(namespace, 'unknown', unknown)

# =============================================================================
class CCTBXParser(ParserBase):

  def __init__(self, program_class, custom_process_arguments=None,
               unused_phil_raises_sorry=True, logger=None, *args, **kwargs):
    '''
    '''
    # program name
    # Order of precedence:
    # 1) ProgramTemplate.program_name
    # 2) LIBTBX_DISPATCHER_NAME
    # 3) Calling command
    if hasattr(sys, 'argv') and sys.argv:
      self.prog = os.getenv('LIBTBX_DISPATCHER_NAME', sys.argv[0])
    else:
      self.prog = 'unknown.unknown'
    if program_class.program_name is not None:
      self.prog = program_class.program_name
    if program_class.program_name is None:
      program_class.program_name = self.prog
    self.prefix = self.prog.split('.')[-1]
    # Windows dispatchers may have the .bat extension
    if sys.platform == 'win32' and self.prefix.lower() == 'bat':
      self.prefix = self.prog.split('.')[-2]

    # PHIL filenames
    self.data_filename = self.prefix + '_data.eff'
    self.defaults_filename = self.prefix + '_defaults.eff'
    self.modified_filename = self.prefix + '_modified.eff'
    self.all_filename = self.prefix + '_all.eff'

    # JSON filename
    self.json_filename = self.prefix + '_result.json'

    # terminal width
    self.text_width = 79

    # DataManager diff
    self.data_manager_diff = None

    # print header
    border = '-' * self.text_width
    description = border + program_class.description + border
    epilog = border + program_class.epilog
    super(CCTBXParser, self).__init__(
      prog=self.prog, description=description, epilog=epilog,
      formatter_class=argparse.RawDescriptionHelpFormatter,
      *args, **kwargs)

    # default values
    self.program_class = program_class
    self.custom_process_arguments = custom_process_arguments
    self.unused_phil_raises_sorry = unused_phil_raises_sorry
    self.unused_phil = []
    self.logger = logger
    if self.logger is None:
      self.logger = logging.getLogger('main')
    self.data_manager = DataManager(
      datatypes=program_class.datatypes,
      custom_options=program_class.data_manager_options,
      custom_master_phil_str=program_class.data_manager_custom_master_phil_str,
      logger=self.logger)
    self.namespace = None

    # add PHIL converters if available
    if len(program_class.phil_converters) > 0:
      iotbx.phil.default_converter_registry = \
        libtbx.phil.extended_converter_registry(
          additional_converters=program_class.phil_converters,
          base_registry=iotbx.phil.default_converter_registry)

    # set up master and working PHIL scopes
    self.master_phil = iotbx.phil.parse(
      program_class.master_phil_str, process_includes=True)
    required_output_phil = iotbx.phil.parse(ProgramTemplate.output_phil_str)
    self.master_phil.adopt_scope(required_output_phil)
    self.working_phil = None

    # add master PHIL to description
    extra_description = '\n\nPHIL arguments:\n'
    phil_str = ''
    if self.program_class.show_data_manager_scope_by_default:
      phil_str += self.data_manager.master_phil.as_str(
        prefix='  ', expert_level=3, attributes_level=0)
    phil_str += self.master_phil.as_str(
      prefix='  ', expert_level=3, attributes_level=0)
    if len(phil_str) > 0:
      extra_description += phil_str
      self.description += extra_description

    self.add_default_options()

  # ---------------------------------------------------------------------------
  def add_default_options(self):
    '''
    '''
    # --show-defaults by itself is set to 0
    # --show-defaults=n sets it to n and it can only be {0, 1, 2, 3}
    self.add_argument(
      '--show-defaults', '--show_defaults',
      nargs='?', const=3, type=int, choices=range(0,4),
      help='show default parameters with expert level (default=3)'
    )

    # --attributes-level by itself is set to 0
    # --attributes-level=n sets it to n and it can only be {0, 1, 2, 3}
    self.add_argument(
      '--attributes-level', '--attributes_level',
      nargs='?', const=0, type=int, choices=list(range(0,4)),
      help='show parameters with extra attributes (default=0)'
    )

    # --write-data
    # switch for writing only DataManager PHIL parameters
    self.add_argument(
      '--write-data', '--write_data', action='store_true',
      help='write DataManager PHIL parameters to file (%s)' %
      self.data_filename
    )

    # --write-defaults
    # switch for writing all the default PHIL parameters
    self.add_argument(
      '--write-defaults', '--write_defaults', action='store_true',
      help='write default PHIL parameters to file (%s)' %
      self.defaults_filename
    )

    # --write-modified
    # switch for writing only modified PHIL parameters
    self.add_argument(
      '--write-modified', '--write_modified', action='store_true',
      help='write modifed PHIL parameters to file (%s)' %
      self.modified_filename
    )

    # --write-all
    # switch for writing all PHIL parameters
    self.add_argument(
      '--write-all', '--write_all', action='store_true',
      help='write all (modified + default + data) PHIL parameters to file (%s)' %
      self.all_filename
    )

    # --diff-params
    # switch for writing the differences between the input PHIL files and
    # the current defaults
    self.add_argument(
      '--diff-params', '--diff_params', action='store_true',
      help='similar to --write-modified, but stops program execution after writing and always overwrites'
    )

    # --json
    # return JSON output from program
    self.add_argument(
      '--json', action='store_true',
      help='''\
writes or overwrites the JSON output for the program to file (%s).
Use --json-filename to specify a different filename for the output.''' %
      self.json_filename,
    )

    # --json-filename
    # set a non-default filename for JSON output
    self.add_argument(
      '--json-filename', '--json_filename', action='store',
      type=str, default=None,
      help='''\
optionally specify a filename for JSON output. If a filename is provided,
the .json extension will be added automatically if it does not already exist.
Also, specifying this flag implies that --json is also specified.'''
    )

    # --check-current-dir
    # if a file does exist in the path specified in the PHIL, check the current directory
    self.add_argument(
      '--check-current-dir', '--check_current_dir', action='store_true',
      help='check current directory for a file if the file path specified in a PHIL file does not work'
    )

    # --overwrite
    # switch for overwriting files, takes precedence over PHIL definition
    self.add_argument(
      '--overwrite', action='store_true',
      help='overwrite files, this overrides the output.overwrite PHIL parameter'
    )

    # --profile
    # enable profiling output
    # the output file is hardcoded to "profile.out" to avoid parsing confusion
    # e.g. --profile model.pdb should pass model.pdb to the program, not dump
    # the profiling stats to model.pdb.
    self.add_argument(
      '--profile', action='store_true',
      help='enable profiling and outputs statistics to a file (profile.out).'
    )

    # --dry-run
    # proceeds until the validate step
    self.add_argument(
      '--dry-run', '--dry_run', action='store_true',
      help='performs basic validation the input arguments, but does not run the program'
    )

    # --citations will use the default format
    # --citations=<format> will use the specified format
    self.add_argument(
      '--citations',
      nargs='?', const='default', type=str, choices=['default', 'cell', 'iucr'],
      help='show citation(s) for program in different formats'
    )

    # --quiet
    # suppress output
    self.add_argument(
      '--quiet', action='store_true',
      help='suppress output to terminal'
    )

    # --version
    # returns the program version
    self.add_argument(
      '--version', '-v', action='store_true',
      help='show version information'
    )

  # ---------------------------------------------------------------------------
  def parse_args(self, args, skip_help = False):
    '''
    '''
    # default behavior with no arguments
    if (len(args) == 0) and (not skip_help):
      self.print_help()
      self.exit()

    # parse arguments
    if sys.version_info >= (3, 7):
      # https://docs.python.org/3/library/argparse.html#argparse.ArgumentParser.parse_intermixed_args
      # https://bugs.python.org/issue9338
      # https://bugs.python.org/issue15112
      self.namespace = super(CCTBXParser, self).parse_intermixed_args(args)
    else:
      self.namespace = super(CCTBXParser, self).parse_args(args)

    # process command-line options
    if self.namespace.attributes_level is not None:
      if self.namespace.show_defaults is None:
        self.error('--attributes-level requires --show-defaults to be set')
    if self.namespace.show_defaults is not None:
      if self.namespace.attributes_level is None:
        self.namespace.attributes_level = 0
      if self.program_class.show_data_manager_scope_by_default:
        self.data_manager.master_phil.show(
          expert_level=self.namespace.show_defaults,
          attributes_level=self.namespace.attributes_level,
          out=self.logger)
      self.master_phil.show(expert_level=self.namespace.show_defaults,
                            attributes_level=self.namespace.attributes_level,
                            out=self.logger)
      self.exit()
    if self.namespace.citations is not None:
      self.show_citations()
      self.exit()
    if self.namespace.version:
      print(self.program_class.get_version(), file=self.logger)
      self.exit()

    # start program header
    print ('Starting %s' % self.prog, file=self.logger)
    print('on %s by %s' % (time.asctime(), getpass.getuser()), file=self.logger)
    print('='*self.text_width, file=self.logger)
    print('', file=self.logger)
    self.logger.flush()

    # process files
    if self.parse_files:
      self.process_files(self.namespace.files)

    # process phil and phil files
    if self.parse_phil:
      self.process_phil(self.namespace.phil)

    # process directories
    if self.parse_dir:
      self.process_dir(self.namespace.dir)

    # custom processing of arguments (if available)
    # the function for custom processing of arguments should take a CCTBXParser
    # object as its argument. The function should modify the
    # CCTBXParser.namespace.unknown, CCTBXParser.data_manager,
    # CCTBXParser.working_phil, or other CCTBXParser members.
    # At the end of the function, CCTBXParser.working_phil should have the final
    # libtbx.phil.scope object (not libtbx.phil.scope_extract) for the program
    # A libtbx.phil.scope_extract object can be converted into a
    # libtbx.phil.scope object by
    #    CCTBXParser.master_phil.format(python_object=<scope_extract>)
    if self.custom_process_arguments is not None:
      self.custom_process_arguments(self)
      assert(isinstance(self.working_phil, libtbx.phil.scope))

    # post processing after all arguments are parsed
    self.post_process()

    # show final PHIL parameters
    self.show_phil_summary()

    return self.namespace

  # ---------------------------------------------------------------------------
  def process_files(self, file_list, message = 'Processing files:'):
    '''
    Second pass to process files. The first pass already checked that these
    files exist. There may be conditions where the file is deleted in the time
    between the first pass and calling this function.

    Use iotbx.file_reader.any_file to process files.
    Will need updating to work with mmtbx.model.manager class more efficiently
    '''
    print(message, file=self.logger)
    print('-'*self.text_width, file=self.logger)
    print('', file=self.logger)
    printed_something = False

    unused_files = []

    for filename in file_list:
      a = any_file(filename)
      process_function = 'process_%s_file' % data_manager_type.get(a.file_type)
      if hasattr(self.data_manager, process_function):
        getattr(self.data_manager, process_function)(filename)
        print('  Found %s, %s' % (data_manager_type[a.file_type], filename),
              file=self.logger)
        printed_something = True
      else:
        unused_files.append(filename)

    # show unrecognized files
    if len(unused_files) > 0:
      if printed_something:
        print('', file=self.logger)
      print('  Files not used by program:', file=self.logger)
      print('  --------------------------', file=self.logger)
      for filename in unused_files:
        print('  %s' % filename, file=self.logger)
      printed_something = True

    # process PHIL files for DataManager scope in order from command-line
    # files are appended and the default is not overridden
    # files from the command-line take precedence
    phil_names = self.data_manager.get_phil_names()
    for name in phil_names:
      phil = self.data_manager.get_phil(name)
      if hasattr(phil.extract(), 'data_manager'):
        phil = self._update_phil_paths_to_cwd(phil)
        self.data_manager.load_phil_scope(phil, process_files=not self.namespace.diff_params)

    if not printed_something:
      print('  No files found', file=self.logger)

    print('', file=self.logger)

  # ---------------------------------------------------------------------------
  def raise_Sorry_for_unused_phil(self):
    '''
    Convenience function for aborting when there are unused PHIL
    parameters. This function is useful when custom PHIL handling is
    necessary.
    '''
    if len(self.unused_phil) > 0 and self.unused_phil_raises_sorry:
      advice = ''
      print('  Unrecognized PHIL parameters:', file=self.logger)
      print('  -----------------------------', file=self.logger)
      for phil in self.unused_phil:
        print('    %s' % phil, file=self.logger)
        if str(phil).find('.qi.')>-1:
          advice = 'Consider setting a QM package using PHENIX_MOPAC, PHENIX_ORCA or similar.'
      print('', file=self.logger)
      error_message = 'Some PHIL parameters are not recognized by %s.\n' % \
                      self.prog
      error_message += wordwrap('Please run this program with the --show-defaults option to see what parameters are available.', max_chars=self.text_width) + '\n'
      error_message += wordwrap('PHIL parameters in files should be fully specified (e.g. "output.overwrite" instead of just "overwrite")', max_chars=self.text_width) + '\n'
      if advice:
        error_message += wordwrap(advice, max_chars=self.text_width) + '\n'
      if self.unused_phil_raises_sorry and not self.namespace.diff_params:
        raise Sorry(error_message)

  # ---------------------------------------------------------------------------
  def process_phil(self, phil_list):
    ''''
    Process PHIL arguments
    Also checks PHIL arguments (command line and files) for parameters that
    specify files (.type = path)
    '''
    print('Processing PHIL parameters:', file=self.logger)
    print('-'*self.text_width, file=self.logger)
    print('', file=self.logger)

    printed_something = False

    # DataManager PHIL
    data_manager_data_sources = []  # from files
    data_manager_sources = []       # from command line

    # program PHIL
    data_sources = []
    sources = []

    # PHIL files are processed in order from command-line
    if self.data_manager.has_phils():
      phil_names = self.data_manager.get_phil_names()
      print('  Adding PHIL files:', file=self.logger)
      print('  ------------------', file=self.logger)
      for name in phil_names:
        # remove DataManager scope since input files are already loaded
        phil_scope = self.data_manager.get_phil(name)
        for phil_object in phil_scope.objects:
          if phil_object.name == 'data_manager':
            phil_scope.objects.remove(phil_object)
            data_manager_data_sources.append(phil_scope.customized_copy(objects=[phil_object]))
        data_sources.append(phil_scope)
        print('    %s' % name, file=self.logger)
      print('', file=self.logger)
      printed_something = True

    # command-line PHIL arguments override any previous settings and are
    # processed in given order
    if len(phil_list) > 0:
      interpreter = self.master_phil.command_line_argument_interpreter(
        assume_when_ambiguous=False)
      data_manager_interpreter = self.data_manager.master_phil.command_line_argument_interpreter(
        assume_when_ambiguous=False)
      print('  Adding command-line PHIL:', file=self.logger)
      print('  -------------------------', file=self.logger)
      for phil in phil_list:
        print('    %s' % phil, file=self.logger)
      print('', file=self.logger)
      printed_something = True
      # check each parameter
      for phil in phil_list:
        processed_arg = None
        data_manager_processed_arg = None
        try:
          processed_arg = interpreter.process_arg(arg=phil)
        except Sorry as e:
          if e.__str__().startswith('Unknown'):
            # check if it is a DataManager parameter
            try:
              data_manager_processed_arg = data_manager_interpreter.process_arg(arg=phil)
            except Sorry as e2:
              if e2.__str__().startswith('Unknown'):
                self.unused_phil.append(phil)
              else:
                raise
          else:
            raise
        if processed_arg is not None:
          sources.append(processed_arg)
        if data_manager_processed_arg is not None:
          data_manager_sources.append(data_manager_processed_arg)

      # collect multiple DataManager label specifications from command-line
      #   data_manager.miller_array.labels.name
      #   data_manager.miller_array.user_selected_labels
      def _get_last_object(phil_scope):
        if phil_scope.is_definition:
          return phil_scope
        return _get_last_object(phil_scope.objects[0])

      label_objects = []
      type_objects = []
      for source in data_manager_sources:
        phil_object = _get_last_object(source)
        if phil_object.full_path() in [
          'data_manager.miller_array.labels.name',
          'data_manager.miller_array.user_selected_labels',
          'data_manager.map_coefficients.labels.name',
          'data_manager.map_coefficients.user_selected_labels']:
          label_objects.append(source)
        if phil_object.full_path() == 'data_manager.model.type':
          type_objects.append(source)

      if len(label_objects) > 0:
        print('  Found labels in command-line', file=self.logger)
        print('  ----------------------------', file=self.logger)

        # combine labels of the same datatype
        datatypes = ['miller_array', 'map_coefficients']
        for datatype in datatypes:
          for parent_label_object in label_objects:
            parent_object = _get_last_object(parent_label_object)  # name / user_selected_labels
            pps = parent_object.primary_parent_scope  # miller_array
            if parent_object.primary_parent_scope.name == 'datatype':
              break
          if parent_object.full_path() == 'data_manager.%s.labels.name' % datatype:
            pps = parent_object.primary_parent_scope.primary_parent_scope
          for child_label_object in label_objects:
            if child_label_object is not parent_label_object \
              and child_label_object in data_manager_data_sources \
              and child_label_object in label_objects:
              data_manager_sources.remove(child_label_object)
              phil_object = _get_last_object(child_label_object)
              if phil_object.full_path() == 'data_manager.%s.labels.name' % datatype:
                phil_object = phil_object.primary_parent_scope
              pps.objects.append(phil_object)
              phil_object.primary_parent_scope = pps

        # finally combine labels into one scope
        label_phil = label_objects[0]
        if len(label_objects) == len(datatypes):
          for label_object in label_objects[1:]:
            label_phil.adopt_scope(label_object)
            if label_object in data_manager_sources:
              data_manager_sources.remove(label_object)

        print('', file=self.logger)

        print('  Combined labels PHIL', file=self.logger)
        print('  --------------------', file=self.logger)
        tmp_working_phil = self.data_manager.master_phil.fetch_diff(label_phil)
        print(tmp_working_phil.as_str(prefix='    '), file=self.logger)
        print('', file=self.logger)

      # set model types for each model
      # if model type is specified, a model type needs to be specified
      # for each model even if it is the default.
      if len(type_objects) > 0:
        model_names = self.data_manager.get_model_names()
        if len(type_objects) != len(model_names):
          raise Sorry('Please specify exactly one "model.type" for each model.')
        print('  Matching model type PHIL:', file=self.logger)
        print('  -------------------------', file=self.logger)
        for model_name, type_object in zip(model_names, type_objects):
          type_phil = self.data_manager.master_phil.fetch(source=type_object)
          type_extract = type_phil.extract()
          model_extract = type_extract.data_manager.model[0]
          model_extract.file = model_name
          print('    %s %s' % (model_extract.file, model_extract.type), file=self.logger)
          new_type_object = self.data_manager.master_phil.format(python_object=type_extract)
          for object in new_type_object.objects[0].objects:
            if object.name == 'model':
              e = object.extract()
              if e.file == model_name:
                type_object.objects[0].objects = [object]
        print('', file=self.logger)

    if self.namespace.overwrite:  # override overwrite if True
      sources.append(iotbx.phil.parse('output.overwrite=True'))

    # process program parameters
    if len(data_sources) + len(sources) > 0:
      self.working_phil, more_unused_phil = self.master_phil.fetch(
        sources=data_sources + sources, track_unused_definitions=True)
      self.unused_phil.extend(more_unused_phil)
    elif self.working_phil is None:
      self.working_phil = self.master_phil.fetch()

    # process DataManager parameters
    if len(data_manager_data_sources) + len(data_manager_sources) > 0:
      diff_phil, more_unused_phil = self.data_manager.master_phil.fetch_diff(
        sources=data_manager_data_sources + data_manager_sources, track_unused_definitions=True)
      self.unused_phil.extend(more_unused_phil)
      # load remaining files and final fmodel parameters
      diff_phil = self._update_phil_paths_to_cwd(diff_phil)
      self.data_manager.load_phil_scope(diff_phil, process_files=not self.namespace.diff_params)
      self.data_manager_diff = diff_phil

    # show unrecognized parameters and abort
    self.raise_Sorry_for_unused_phil()

    # process input phil for file/directory defintions and add to DataManager
    # Note: if a PHIL file is input as a PHIL parameter, the contents of the
    # file will NOT be parsed and validated. The PHIL file should be provided
    # as a command-line argument. This is mostly for finding data files
    # defined by PHIL parameters that should be added to the DataManager
    diff_phil = self.master_phil.fetch_diff(self.working_phil)
    diff_phil = self._update_phil_paths_to_cwd(diff_phil)
    paths = self.check_phil_for_paths(diff_phil)
    if len(paths) > 0:
      files = set()
      dirs = set()
      for path in paths:
        if path is not None:
          if os.path.isfile(path):
            files.add(path)
          elif os.path.isdir(path):
            dirs.add(path)
      if self.parse_files:
        self.process_files(files, message='Processing files from PHIL:')
      if self.parse_dir:
        self.process_dir(dirs, message='Processing directories from PHIL:')

    if not printed_something:
      print('  No PHIL parameters found', file=self.logger)
      print('', file=self.logger)

  # ---------------------------------------------------------------------------
  def process_dir(self, dir_list, message = 'Processing directories:'):
    '''
    '''
    print(message, file=self.logger)
    print('-'*self.text_width, file=self.logger)
    print('', file=self.logger)

  # ---------------------------------------------------------------------------
  def check_phil_for_paths(self, phil_scope):
    '''
    Recursively check PHIL scope if there is a 'path' type.
    Returns the paths (empty list means no paths were found)
    '''
    paths = []
    if phil_scope.is_definition:
      if phil_scope.type.phil_type == 'path':
        if phil_scope.style is not None and 'new_file' in phil_scope.style:
          pass
        else:
          paths.append(phil_scope.extract())
    elif phil_scope.is_scope:
      for phil_object in phil_scope.objects:
        paths.extend(self.check_phil_for_paths(phil_object))
    return paths

  # ---------------------------------------------------------------------------
  def _update_phil_paths_to_cwd(self, phil=None):
    '''
    Convenience function for modifying a phil scope so that file paths
    are reset to use the current working directory if the filename exists.
    '''
    if self.namespace.check_current_dir:
      diff_phil = self.data_manager.master_phil.fetch_diff(phil)
      paths = self.check_phil_for_paths(diff_phil)
      phil_str = diff_phil.as_str()
      for path in paths:
        new_path = os.path.join(os.getcwd(), os.path.basename(path))
        if not os.path.isfile(path) and os.path.isfile(new_path):
          phil_str = phil_str.replace(path, new_path)
      phil = iotbx.phil.parse(phil_str, process_includes=True)
    return phil

  # ---------------------------------------------------------------------------
  def post_process(self):
    '''
    Post processing of inputs after all arguments are parsed
    '''

    working_phil_extract = self.working_phil.extract()

  # ---------------------------------------------------------------------------
  def show_phil_summary(self):
    '''
    Show final, modified PHIL parameters after all processing is complete
    Also, write phil scopes based on command-line flags
    '''

    overwrite = (self.namespace.overwrite or \
                 self.working_phil.extract().output.overwrite)

    # check for any remaining unknown arguments
    if len(self.namespace.unknown) > 0:
      error_message = 'The following arguments are not recognized:\n'
      for value in self.namespace.unknown:
        error_message += '  %s\n' % value
      raise Sorry(error_message)

    # get differences
    try:
      data_diff = self.data_manager.master_phil.fetch_diff(
        self.data_manager.export_phil_scope())
      # keep original DataManager scope when using --diff-params
      if self.namespace.diff_params and self.data_manager_diff is not None:
        data_diff = self.data_manager_diff
    except RuntimeError as err:
      raise Sorry(err)
    try:
      phil_diff = self.master_phil.fetch_diff(self.working_phil)
    except RuntimeError as err:
      raise Sorry(err)
    data_is_different = (len(data_diff.as_str()) > 0)
    phil_is_different = (len(phil_diff.as_str()) > 0)
    is_different = data_is_different or phil_is_different

    # show final processed phil scope
    print('Final processed PHIL parameters:', file=self.logger)
    print('-'*self.text_width, file=self.logger)
    if is_different:
      data_diff.show(prefix='  ', out=self.logger)
      phil_diff.show(prefix='  ', out=self.logger)
    else:
      print('  All parameters are set to their defaults', file=self.logger)
    print('', file=self.logger)

    # write scopes if requested
    if self.namespace.write_data or self.namespace.write_defaults \
      or self.namespace.write_modified or self.namespace.write_all:
      print('Writing program PHIL file(s):', file=self.logger)

    if self.data_manager.get_default_output_filename() is None:
      self.data_manager.set_default_output_filename('cctbx_program')

    # write DataManager scope
    if self.namespace.write_data:
      if data_is_different:
        self.data_manager.write_phil_file(
          self.data_manager.export_phil_scope().as_str(),
          filename=self.data_filename,
          overwrite=overwrite)
        print('  Input file PHIL written to %s.' % self.data_filename,
              file=self.logger)
      else:
        print('  No input file PHIL to write', file=self.logger)

    # write all default parameters
    if self.namespace.write_defaults:
      self.data_manager.write_phil_file(self.master_phil.as_str(expert_level=3),
        filename=self.defaults_filename, overwrite=overwrite)
      print('  Default PHIL parameters written to %s.' % self.defaults_filename,
        file=self.logger)

    # write differences
    if self.namespace.write_modified or self.namespace.diff_params:
      if is_different:
        ow = overwrite or self.namespace.diff_params
        self.data_manager.write_phil_file(
          data_diff.as_str() + phil_diff.as_str(), filename=self.modified_filename,
          overwrite=ow)
        print('  Modified PHIL parameters written to %s.' %
              self.modified_filename, file=self.logger)
      else:
        print('  No PHIL modifications to write', file=self.logger)
      if self.namespace.diff_params:
        sys.exit(0)

    # write all parameters (DataManager + Program)
    if self.namespace.write_all:
      all_phil = self.data_manager.export_phil_scope().as_str()
      all_phil += self.working_phil.as_str(expert_level=3)
      self.data_manager.write_phil_file(
        all_phil, filename=self.all_filename, overwrite=overwrite)
      print('  All PHIL parameters written to %s.' % self.all_filename,
            file=self.logger)

    print('', file=self.logger)

  # ---------------------------------------------------------------------------
  def show_citations(self):
    # build list of program-specific citations
    program_citations = []
    if self.program_class.citations is not None:
      class_citations = citations.master_citation_phil.fetch(
        source=self.program_class.citations).extract()
      for citation in class_citations.citation:
        program_citations.append(citation)
    for article_id in self.program_class.known_article_ids:
      citation = citations.citations_db.get(article_id)
      if citation is not None:
        program_citations.append(citation)
      else:
        raise Sorry('"%s" not found citations database' % article_id)

    # show program-specific citations and general citation for CCTBX
    if len(program_citations) > 0:
      print('Citation(s) for %s:' % self.prog, file=self.logger)
      print('-'*self.text_width, file=self.logger)
      print('', file=self.logger)
      for citation in program_citations:
        citations.show_citation(citation, out=self.logger,
                                format=self.namespace.citations)
    self.program_class.show_template_citation(
      text_width=self.text_width, logger=self.logger,
      citation_format=self.namespace.citations)

# =============================================================================
def run_pyside_check():
  '''
  Function for checking if PySide2 is available
  '''
  try:
    import PySide2
  except ImportError:
    msg = '''
------------------------------------------------------------------------
To run this GUI, PySide2 is required. To install with conda run

  conda install pyside2

or with pip run

  pip install pyside2

------------------------------------------------------------------------
'''
    raise Sorry(msg)

# =============================================================================
def run_program(program_class=None, parser_class=CCTBXParser, custom_process_arguments=None,
                unused_phil_raises_sorry=True, args=None, json=False, logger=None,
                hide_parsing_output=False, allow_default_args=False):
  '''
  Function for running programs using CCTBXParser and the program template

  Parameters
  ----------
  program_class: ProgramTemplate
    The class defining the program. It must be a subclass of ProgramTemplate
  parser_class: CCTBXParser
    The parser class to use for parsing. It must be the CCTBXParser or a subclass
  custom_process_arguments: function(parser)
    Custom function to parse unknown arguments (optional)
  unused_phil_raises_sorry: bool
    If False, any unused PHIL parameters are kept for parsing later
  args: list
    List of command-line arguments (optional)
  json: bool
    If True, get_results_as_JSON is called for the return value instead of get_results
  logger: multi_out
    For logging output (optional)
  hide_parsing_output: bool
    If True, hides the output from parsing the command-line arguments

  Returns
  -------
    Whatever is returned from program_class.get_results() or program_class.get_results_as_JSON()
  '''

  assert program_class is not None

  if args is None:
    args = sys.argv[1:]
  elif allow_default_args and type(args) in [list, tuple]:
    args = sys.argv[1:] + args

  # start profiling
  pr = None
  if '--profile' in args:
    import cProfile
    pr = cProfile.Profile()
    pr.enable()

  # keep output in quiet mode
  if '--quiet' in args:
    logger = multi_out()
    logger.register('parser_log', StringIO())

  # create logger
  if logger is None:
    logger = multi_out()
    logger.register('stdout', sys.stdout)
    logger.register('parser_log', StringIO())

  program_logger = logger
  if hide_parsing_output:
    logger = multi_out()
    logger.register('stderr', sys.stderr)
    logger.register('parser_log', StringIO())

  # start timer
  t = show_times(out=logger)

  # create parser
  parser = parser_class(program_class=program_class,
                        custom_process_arguments=custom_process_arguments,
                        unused_phil_raises_sorry=unused_phil_raises_sorry,
                        logger=logger)
  namespace = parser.parse_args(args)

  # start program
  if namespace.dry_run:
    print('Starting dry run', file=logger)
  else:
    print('Starting job', file=logger)
  print('='*79, file=logger)
  task = program_class(parser.data_manager, parser.working_phil.extract(),
                       master_phil=parser.master_phil,
                       logger=program_logger)

  # validate inputs
  task.validate()

  # stop if dry_run is set
  if namespace.dry_run:
    print('\nArguments have been validated by the program.\n', file=logger)
    print('='*79, file=logger)
    return

  # run program
  task.run()

  # clean up (optional)
  task.clean_up()

  # dump profiling stats
  if pr is not None:
    pr.disable()
    pr.dump_stats('profile.out')

  # output JSON
  if namespace.json or namespace.json_filename:
    result = task.get_results_as_JSON()
    if result is not None:
      json_filename = parser.json_filename
      if namespace.json_filename is not None:
        json_filename = namespace.json_filename
        if not json_filename.endswith('.json'):
          json_filename += '.json'
      with open(json_filename, 'w') as f:
        f.write(result)
    else:
      print('', file=logger)
      print('!'*79, file=logger)
      print('WARNING: The get_results_as_JSON function has not been defined for this program', file=logger)
      print('!'*79, file=logger)

  # stop timer
  print('', file=logger)
  print('='*79, file=logger)
  print('Job complete', file=logger)
  t()

  # clean up file for quiet mode
  if namespace.quiet:
    logger.close()

  if json:
    result = task.get_results_as_JSON()
  else:
    result = task.get_results()

  return result

# =============================================================================
def get_program_params(run):
  """Tool to get parameters object for a program that runs with
     the program template.
  params: run:  the program template object
  returns: parameters for this program as set up by the program template
  Get the run something like this way:
    from phenix.programs import map_to_model as run
  """

  parser = CCTBXParser(program_class=run.Program,
                       logger=null_out())
  _ = parser.parse_args([], skip_help = True)
  return parser.working_phil.extract()

# =============================================================================
# end


 *******************************************************************************


 *******************************************************************************
iotbx/crystal_symmetry_from_any.py
"""
Tools to obtain crystal_symmetry from any suitable object or string
"""
from __future__ import absolute_import, division, print_function
import libtbx.load_env

from iotbx.scalepack import crystal_symmetry_from_hkl as from_scalepack_hkl
from iotbx.xds import crystal_symmetry_from_hkl as from_xds_hkl
from iotbx.dtrek import crystal_symmetry_from_ref as from_dtrek_ref
if (libtbx.env.has_module("ccp4io")):
  from iotbx.mtz import crystal_symmetry_from_mtz as from_mtz
else:
  from_mtz = None
from iotbx.shelx import crystal_symmetry_from_ins as from_shelx_ins
from iotbx.cns import crystal_symmetry_from_inp as from_cns_inp
from iotbx.cns import crystal_symmetry_from_sdb as from_cns_sdb
from iotbx.pdb import crystal_symmetry_from_pdb as from_pdb
from iotbx.solve import crystal_symmetry_from_inp as from_solve_inp
from iotbx.xplor import crystal_symmetry_from_map as from_xplor_map
try:
  from iotbx.mrcfile import crystal_symmetry_from_ccp4_map as from_ccp4_map
except ImportError:
  from_ccp4_map = None
from iotbx.cif import crystal_symmetry_from_cif as from_cif
from cctbx import crystal
from libtbx.path import canonical_path
import os

def from_string(string):
  '''
     Interprets a symmetry-string object in the following format:
     a, b, c, alpha, beta, gamma, space-group
     Please be aware that this is comma delimited, so you may not
     have commas within the space-group substring.
  '''
  parts = string.split(",")
  unit_cell = None
  space_group = None
  if len(parts) == 7:
    unit_cell = parts[:-1]
    space_group = parts[-1]
  elif len(parts) == 6:
    unit_cell = parts
  elif len(parts) == 1:
    space_group = parts[0]
  else:
    return None
  if unit_cell is not None:
    try:
      unit_cell = [float(number) for number in unit_cell]
    except KeyboardInterrupt: raise
    except Exception:
      return None
  try:
    return crystal.symmetry(unit_cell=unit_cell,space_group=space_group)
  except KeyboardInterrupt: raise
  except Exception:
    return None

def extract_from(file_name):
  '''
     extract_from takes a file name or a string description.
     If given a file name, it attempts to open the file with all the known
     symmetry-containing file-types and extract the symmetry information.
     If all of these fail, it attempts to interpret the input string
     according to the function cyrstal_symmetry_from_any.from_string.
     If given a crystal.symmetry it returns the crystal.symmetry.
  '''
  if type(file_name) == type(crystal.symmetry()):
    return file_name
  for fmt in (from_scalepack_hkl,
              from_xds_hkl,
              from_dtrek_ref,
              from_ccp4_map,
              from_xplor_map,
              from_mtz,
              from_shelx_ins,
              from_cns_inp,
              from_cns_sdb,
              from_pdb,
              from_solve_inp,
              from_cif):
    if (fmt is None): continue
    try: return fmt.extract_from(file_name)
    except KeyboardInterrupt: raise
    except Exception: pass
  return from_string(file_name)

def extract_and_append(file_names, target_list, extract_function=extract_from):
  file_names_done = set()
  for file_name in file_names:
    if (file_name is None): continue
    if (not os.path.isfile(file_name)): continue
    file_name = canonical_path(file_name)
    if (file_name in file_names_done): continue
    file_names_done.add(file_name)
    try:
      crystal_symmetry = extract_function(file_name=file_name)
    except KeyboardInterrupt: raise
    except Exception: pass
    else:
      if (crystal_symmetry is not None):
        target_list.append(crystal_symmetry)


 *******************************************************************************


 *******************************************************************************
iotbx/csv_utils.py
"""
Tools to read and write csv formatted files
"""

from __future__ import absolute_import, division, print_function
import csv
from six.moves import range

if 'unix' not in csv.list_dialects():
  csv.register_dialect('unix', lineterminator='\n', quoting=csv.QUOTE_NONE)

class writer(object):
  def __init__(self,
               file_object,
               fields,
               field_names=None,
               delimiter=','):
    for field in fields:
      assert len(field) == len(fields[0])
    iter_object = self._iter_rows(fields)
    writer = csv.writer(file_object, delimiter=delimiter,
                        dialect='unix', quoting=csv.QUOTE_NONE)
    if field_names:
      writer.writerow(field_names)
    writer.writerows(iter_object)

  def _iter_rows(self, args):
    for i in range(len(args[0])):
      yield tuple([arg[i] for arg in args])

class reader(object):
  def __init__(self,
               file_object,
               data_type=None,
               data_type_list=None,
               field_names=False,
               delimiter=','):
    """Supply a single data_type which is to be applied to all fields,
    or supply a data_type_list containing the type of each field.
    String is the default type if none is supplied.
    """
    assert data_type is None or data_type_list is None
    if data_type is None and data_type_list is None:
      data_type = str

    reader = csv.reader(file_object, delimiter=delimiter)
    data = []

    for i_row,row in enumerate(reader):
      if i_row == 0:
        n_data = len(row)
        if data_type_list is not None:
          assert len(data_type_list) == n_data
        else:
          data_type_list = [data_type] * n_data
        for i in range(n_data):
          if field_names:
            data.append([])
          else:
            data.append([data_type_list[i](row[i])])
        n_data = len(data)
      else:
        for i in range(n_data):
          data[i].append(data_type_list[i](row[i]))
    self.data = data


 *******************************************************************************


 *******************************************************************************
iotbx/data_plots.py

"""
Tools for handling plottable data, usually similar to CCP4's loggraph format
(which may be parsed and output by this module).
"""

from __future__ import absolute_import, division, print_function
from libtbx import adopt_init_args
from libtbx.utils import Sorry
import os.path
import math
import re
from six.moves import range
from six.moves import zip

class plot_data(object):
  def __init__(self,
               plot_title,
               x_label,
               y_label,
               x_data,
               y_data,
               y_legend,
               comments):
    self.plot_title = plot_title

    self.x_label = x_label
    self.y_label = y_label
    self.x_data = x_data
    self.comments = comments
    self.domain_flag = 'A'

    ## The x_data is an (flex) array of numbers
    ## The Y_data should be an list of (flex) arrays
    ## The legends should be an array of strings
    self.y_legend = []
    self.y_data = []
    if y_data is not None:
      assert ( len(y_data)==len(self.x_data) )
      self.y_data.append(y_data)
      self.y_legend.append(y_legend)

  def add_data(self,
               y_data,
               y_legend):
    assert ( len(y_data)==len(self.x_data) )
    self.y_data.append(y_data)
    self.y_legend.append(y_legend)


def plot_data_loggraph(plot_data,output):
  ## First we need to print the header information
  print(file=output)
  print(file=output)
  print('$TABLE: %s:'%(plot_data.plot_title), file=output)
  print('$GRAPHS', file=output)
  print(':%s' %(plot_data.comments), end=' ', file=output)
  index_string = ''
  for ii in range(len(plot_data.y_data)+1):
    index_string += '%d,'%(ii+1)
  print(':%s:%s:'%(plot_data.domain_flag,index_string[:-1]), file=output)
  print('$$', file=output)
  ## replace spaces for loggraph with underscores
  tmp_legend = plot_data.x_label
  spaces = 0
  spaces = tmp_legend.find(' ')
  if spaces>0:
    tmp_legend = tmp_legend.replace(' ','_')
  label_string = '%s'%(tmp_legend)
  for ii in range(len(plot_data.y_data)):
    tmp_legend = plot_data.y_legend[ii]
    ## loggraph does not like spaces in the legend names
    ## lets replace them with underscores
    spaces = 0
    spaces = tmp_legend.find(' ')
    if spaces>0:
      tmp_legend = tmp_legend.replace(' ','_')
    label_string += '   %s'%( tmp_legend )
  print('%s   $$ '%(label_string), file=output)
  print('$$', file=output)
  for ii in range(len(plot_data.x_data)):
    data_string = '%f'%(plot_data.x_data[ii])
    for jj in range(len(plot_data.y_data)):
      data_string +='   %f'%(plot_data.y_data[jj][ii])
    print('%s'%(data_string), file=output)
  print('$$', file=output)

#-----------------------------------------------------------------------
# Nat's utilities for plottable data
def flip_table(table):
  if table is None or len(table) == 0 :
    return []
  new_table = []
  for elem in table[0] :
    new_table.append([elem])
  if len(table) > 1 :
    for row in table[1:] :
      assert len(row) == len(new_table)
      for i, elem in enumerate(row):
        new_table[i].append(elem)
  return new_table

class table_data(object):
  """
  Container for tabular data.  Originally this was solely used as a container
  for CCP4 'loggraph' plots, but it can also be used to display tables that
  are not intended to be plotted.  A variety of output formats are supported,
  including export to JSON for eventual HTML display.  If graphs are defined,
  these objects can be passed to the loggraph frontend in wxtbx.plots.
  """
  def __init__(self,
      title,
      column_names=None,
      column_types=None,
      column_labels=None,
      column_formats=None,
      graph_names=None,
      graph_types=None,
      graph_labels=None,
      graph_columns=None,
      data=None,
      comments=None,
      x_is_inverse_d_min=False,
      first_two_columns_are_resolution=False,
      force_exact_x_labels=False,
      reference_marks=None):
    adopt_init_args(self, locals())
    if (reference_marks is not None):
      assert (len(reference_marks) == len(graph_columns) == 1)
    if (data is not None) : # check column size consistency
      lengths = { len(column) for column in data }
      assert len(lengths) == 1
    self._is_complete = False
    self._graphs = {}
    self._column_width = 10
    self.plot_type = "GRAPH"

  # Backwards compatibility
  def __setstate__(self, state):
    self.__dict__.update(state)
    if (not hasattr(self, "first_two_columns_are_resolution")):
      self.first_two_columns_are_resolution = None
    if (not hasattr(self, "force_exact_x_labels")):
      self.force_exact_x_labels = None
    if (not hasattr(self, "reference_marks")):
      self.reference_marks = None

  @property
  def n_rows(self):
    return len(self.data[0])

  @property
  def n_cols(self):
    return len(self.data)

  @property
  def n_graphs(self):
    return len(self.graph_names)

  def add_graph(self, name, type, columns):
    if self.graph_names is None :
      self.graph_names = [name.strip()]
    else :
      self.graph_names.append(name.strip())
    if self.graph_types is None :
      self.graph_types = [type]
    else :
      self.graph_types.append(type)
    if self.graph_columns is None :
      self.graph_columns = [columns]
    else :
      self.graph_columns.append(columns)

  def import_loggraph(self, lines):
    """
    Parse CCP4 loggraph format and populate internal data structures.
    Input may be provided as list of individual lines or as a string.
    """
    if isinstance(lines, list):
      lines = "\n".join(lines)

    blocks = [ b.strip() for b in lines.split('$$') ]

    # Loggraph format is defined by mandatory 4 blocks, separated by '$$', followed by nothing.
    # http://www.ccp4.ac.uk/html/loggraphformat.html
    assert len(blocks) == 5, 'input not in loggraph format (%d blocks found)' % len(blocks)
    header, columns, comment, data, remainder = blocks
    assert remainder == '', 'loggraph table has %d bytes following the table end' % len(remainder)

    if '$TABLE' in header:
      title = re.search('\\$TABLE\\s*:(.*?)(:|\n|$)', header, re.MULTILINE)
      if title:
        self.title = title.group(1).strip()

    graphs = re.search('\\$(GRAPHS|SCATTER)[\\s\n]*((:[^:\n]*:[^:\n]*:[^:\n]*(:|$)[\\s\n]*)*)($|\\$)', header, re.MULTILINE)
    if graphs:
      if graphs.group(1) == 'GRAPHS':
        self.plot_type = "GRAPH"
      elif graphs.group(1) == 'SCATTER':
        self.plot_type = "SCATTER"
      else:
        raise TypeError('Unknown graph type %s' % graphs.group(1))
      graphs = graphs.group(2)
      for graph in re.finditer(':([^:\n]*):([^:\n]*):([^:\n]*)(:|$)', graphs, re.MULTILINE):
        self.add_graph(name=graph.group(1),
                       type=graph.group(2),
                       columns=[ int(col)-1 for col in graph.group(3).split(',') ])

    # Newlines, spaces, tabs etc. are not allowed in column names.
    # Treat them as separators.
    columns = re.sub(r'\s+', ' ', columns).split(' ')
    data_width = len(columns)
    self.column_labels = [ lbl.replace('_', ' ') for lbl in columns ]
    if self.column_labels[0] in ("1/d^2", "1/d**2", "1/resol^2"):
      self.x_is_inverse_d_min = True

    self.data = [[] for x in range(data_width)]

    # Now load the data
    data = re.sub(r'\s+', ' ', data).split(' ')
    for entry, datum in enumerate(data):
      self.data[entry % data_width].append(_tolerant_float(datum))

    # Int-ify integer-like columns
    # It is unclear to me if this actually serves any purpose.
    for i, column in enumerate(self.data):
      column_is_ints = True
      for x in column:
        if (x is not None) and \
           (str(x).lower() not in ('nan', 'inf', '-inf')) and \
           (x != int(x)):
          column_is_ints = False
          break
      if column_is_ints:
        self.data[i] = [ int(x) if x and x == x else None
                         for x in column ]

  def add_row(self, row):
    '''Unclear if this is used from outside the class'''
    if self.data is None or len(self.data) == 0 :
      self.data = [ [x] for x in row ]
    else :
      assert len(self.data) == len(row), row
      for i, value in enumerate(row):
        self.data[i].append(value)

  def add_column(self, column, column_name=None, column_label=None):
    if self.data is None or len(self.data) == 0 :
      self.data = [ list(column) ]
    else :
      assert len(self.data[0]) == len(column)
      self.data.append(column)
    if column_name is not None :
      if self.column_names is None : self.column_names = [column_name]
      else :                         self.column_names.append(column_name)
    if column_label is not None :
      if self.column_labels is None : self.column_labels = [column_label]
      else :                          self.column_labels.append(column_label)

  def _max_column_width(self, precision):
    assert isinstance(precision, int)
    if self.column_labels is None : return precision
    label_widths = [ len(lab) for lab in self.column_labels ]
    cwidth = max(label_widths)
    if cwidth < precision :
      cwidth = precision
    return cwidth

  def get_value_as_resolution(self, x):
    if (self.x_is_inverse_d_min):
      return x ** -0.5
    return x

  def _format_num_row(self, row, precision=None, column_width=None):
    if row is None : return []
    if (self.column_formats is not None):
      if (self.first_two_columns_are_resolution):
        d_max = self.column_formats[0] % self.get_value_as_resolution(row[0])
        d_min = self.column_formats[1] % self.get_value_as_resolution(row[1])
        frow = [ "%s - %s" % (d_max, d_min) ]
        return frow + \
               [ (f % x) for f, x in zip(self.column_formats[2:], row[2:]) ]
      else :
        if (self.x_is_inverse_d_min):
          row = [ math.sqrt(1/row[0]) ] + row[1:]
        return [
          (f % x) if x is not None else "*" for f, x in zip(self.column_formats, row)
        ]
    else :
      f1 = "%-g"
      if (precision is not None):
        f1 = "%s-.%dg" % (r'%', precision)
      f2 = "%s"
      if (column_width is not None):
        f2 = "%s-%ds" % (r'%', column_width)
      return [ f2 % ftoa(x, f1) for x in row ]

  def _format_labels(self, labels, column_width):
    if labels is None : return []
    f1 = "%s-%ds" % (r'%', column_width)
    return [ f1 % lab for lab in labels ]

  def format_simple(self, precision=6, indent=0):
    data = self.data
    assert data is not None and len(data) > 0
    column_width = self._max_column_width(precision)
    column_headers = self._format_labels(self.column_labels, column_width)
    out = _formatting_buffer(indent)
    if self.title is not None :
      out += self.title
    trailing_spaces = re.compile(r"\ *$")
    labels = " ".join(self._format_labels(self.column_labels, column_width))
    out += trailing_spaces.sub("", labels)
    nrows = len(data[0])
    f1 = "%s-%ds" % (r'%', column_width)
    for j in range(nrows):
      row = [ col[j] for col in data ]
      frow = self._format_num_row(row, column_width, precision)
      frows = " ".join([ f1 % cv for cv in frow ])
      out += trailing_spaces.sub("", frows)
    return str(out)

  def format(self, precision=6, indent=0, equal_widths=True):
    """Formats the table for printout in a log file, with equal-width
    columns and cell boundaries, e.g.:
      -------------------------------
      | Title                       |
      |-----------------------------|
      | col1    | col1    | col3    |
      |-----------------------------|
      | 0.1     | 5       | *       |
      -------------------------------
    """
    data = self.data
    assert data is not None and len(data) > 0
    column_labels = self.column_labels
    # most of the code here deals with generic numerical values, but the
    # use of resolution ranges comes up often enough to merit special handling
    if (self.first_two_columns_are_resolution):
      column_labels = ["Res. range"] + column_labels[2:]
    formatted_rows = [ column_labels ]
    for j in range(self.n_rows):
      row = [ col[j] for col in data ]
      formatted_rows.append(self._format_num_row(row, precision=precision))
    column_widths = []
    for j in range(len(column_labels)):
      column_widths.append(max([ len(r[j]) for r in formatted_rows ]))
    if (equal_widths):
      # if the first column is the resolution range, we allow that to be
      # different in width than the remaining columns
      if (self.first_two_columns_are_resolution):
        max_w = max(column_widths[1:])
        column_widths = [column_widths[0]] + [ max_w ] * (len(column_widths)-1)
      else :
        max_w = max(column_widths)
        column_widths = [ max_w ] * len(column_widths)
    column_headers = []
    for cw, cl in zip(column_widths, column_labels):
      f1 = "%s-%ds" % (r'%', cw)
      column_headers.append(f1 % cl)
    column_headers = " | ".join(column_headers)
    f2 = "%s-%ds" % (r'%', len(column_headers))
    table_width = len(column_headers) + 4
    sep_line = "-" * table_width
    out = _formatting_buffer(indent)
    out += sep_line
    out += "| " + f2 % self.title + " |"
    out += "|" + sep_line[1:-1] + "|"
    out += "| " + column_headers + " |"
    out += "|" + sep_line[1:-1] + "|"
    for j in range(self.n_rows):
      frow = []
      for cw, cv in zip(column_widths, formatted_rows[j+1]):
        f1 = "%s-%ds" % (r'%', cw)
        frow.append(f1 % cv)
      frow = " | ".join(frow)
      out += "| " + frow + " |"
    out += sep_line
    return str(out)

  def format_loggraph(self, precision=6, column_width=None):
    """
    Create CCP4 loggraph format text for embedding in logfiles.
    """
    data = self.data
    graph_columns = self.graph_columns
    graph_names = self.graph_names
    graph_types = self.graph_types
    column_labels = self.column_labels
    assert data is not None and len(data) > 0
    assert column_width is None or isinstance(column_width, int)
    assert graph_types is None or len(graph_types) == len(graph_names)
    out = "$TABLE: %s\n" % self.title
    out += "$GRAPHS\n"
    if graph_types is None :
      graph_types = ["A" for graph in graph_names ]
    for i, graph_name in enumerate(graph_names):
      out += ":%s" % graph_name
      out += ":%s:%s:\n" % (graph_types[i],
                           ",".join([ "%d"%(x+1) for x in graph_columns[i] ]))
    out += "$$\n"
    re_spaces = re.compile(r"[\ ]{1,}")
    labels = [ re_spaces.sub("_", lab) for lab in column_labels ]
    if column_width is None :
      column_width = max(self._max_column_width(precision),
                         max([ len(lab) for lab in labels ]))
    f1 = "%s.%dg" % (r'%', precision)
    f2 = "%s-%ds" % (r'%', column_width)
    labels = [ re_spaces.sub("_", lab) for lab in column_labels ]
    out += "%s $$\n" % "  ".join([ f2 % lab for lab in labels ])
    out += "$$\n"
    trailing_spaces = re.compile(r"\ *$")
    for j in range(len(data[0])):
      row = [ col[j] for col in data ]
      frow = self._format_num_row(row, column_width, precision)
      frow = [ f2 % cv for cv in frow ]
      out += trailing_spaces.sub("", "  ".join(frow))
      out += "\n"
    out += "$$\n"
    return out

  def export_json(self, convert=True):
    import json
    graph_types = self.graph_types
    if graph_types is None :
      graph_types = ["A" for graph in self.graph_names ]
    export_dict = {
      "title" : self.title,
      "graph_columns" : self.graph_columns,
      "graph_names" : self.graph_names,
      "graph_types" : graph_types,
      "column_labels" : self.column_labels,
      "x_is_inverse_d_min" : self.x_is_inverse_d_min,
      "data" : self.data,
    }
    if (convert):
      return json.dumps(export_dict)
    return export_dict

  def export_rows(self, convert=True, precision=6):
    column_labels = self.column_labels
    # most of the code here deals with generic numerical values, but the
    # use of resolution ranges comes up often enough to merit special handling
    if (self.first_two_columns_are_resolution):
      column_labels = ["Res. range"] + column_labels[2:]
    formatted_rows = [ column_labels ]
    for j in range(self.n_rows):
      row = [ col[j] for col in self.data ]
      formatted_rows.append(self._format_num_row(row, precision=precision))
    return formatted_rows

  def export_json_table(self, convert=True):
    import json
    export_dict = {
      "title" : self.title,
      "rows" : self.export_rows(),
    }
    if (convert):
      return json.dumps(export_dict)
    return export_dict

  def as_rows(self):
    return flip_table(self.data)

  def only_plot(self):
    assert (len(self.graph_names) == 1)
    return self.graph_names[0]

  def get_reference_marks(self):
    if (self.reference_marks is not None):
      return self.reference_marks[0]
    return None

  def get_graph(self, graph_name=None, column_list=[]):
    graph_names = self.graph_names
    column_labels = self.column_labels
    graph_columns = self.graph_columns
    data = self.data
    _graphs = self._graphs
    if graph_name is not None :
      if not graph_name in _graphs :
        if not graph_name in graph_names :
          return None
        n = graph_names.index(graph_name)
        gdata = self._extract_data_column(graph_columns[n])
        if len(column_labels) == len(data):
          labels = [column_labels[i] for i in graph_columns[n]]
        else :
          labels = []
        x_axis = None
        y_axis = None
        if self.graph_labels is not None :
          (x_axis, y_axis) = self.graph_labels[n]
        _graphs[graph_name] = graph_data(graph_name, gdata, "plot", labels,
          x_axis, y_axis)
      return _graphs[graph_name]
    elif len(column_list) > 1 :
      if not column_list in self._graphs :
        data = None
        if isinstance(column_list[0], int):
          data = self._extract_data_column(column_list)
          labels = [ column_labels[i] for i in column_list ]
        elif isinstance(column_list[0], str):
          n_list = []
          for col in column_list :
            n_list.append(column_labels.index(col))
          gdata = self._extract_data_column(n_list)
          labels = [ column_labels[i] for i in n_list ]
        if gdata is None :
          return None
        _graphs[column_list] = graph_data(None, gdata, "plot", labels)
      return _graphs[column_list]

  def get_column_by_label(self, column_label):
    if not column_label in self.column_labels :
      raise RuntimeError(
        "Couldn't find column %s in this table.  (Valid columns: %s)" %
        (column_label, ",".join(self.column_labels)))
    i = self.column_labels.index(column_label)
    return self.data[i]

  def _extract_data_column(self, column_list):
    assert len(column_list) <= len(self.data)
    data = self.data
    new_data = [ [ x for x in data[i] ] for i in column_list ]
    return new_data

  def __str__(self):
    return self.format_simple()

  def get_x_values(self):
    return self.data[0]

  def get_x_as_resolution(self):
    assert self.x_is_inverse_d_min
    oldx = self.data[0]
    newx = []
    for x in oldx :
      newx.append(math.sqrt(1.0/x))
    return newx

class graph_data(object):
  def __init__(self, name, data, type="plot", data_labels=None, x_axis=None,
      y_axis=None):
    self.name = name
    self.data = data
    self.type = type
    if data_labels is None or len(data_labels) == 0 :
      self.x_label = "X"
      self.y_labels = [ "Y" for i in range(1, len(data)) ]
    else :
      self.x_label = data_labels[0]
      self.y_labels = [ data_labels[i] for i in range(1, len(data)) ]
    self.x_axis_label = x_axis
    self.y_axis_label = y_axis

  def get_plots(self, fill_in_missing_y=None):
    plots = []
    data = self.data
    for i in range(1, len(data)):
      plot_x = []
      plot_y = []
      for j in range(0, len(data[i])):
        if data[0][j] is not None :
          if data[i][j] is not None :
            plot_x.append(data[0][j])
            plot_y.append(data[i][j])
          elif fill_in_missing_y is not None :
            plot_x.append(data[0][j])
            plot_y.append(fill_in_missing_y)
      plots.append((plot_x, plot_y))
    return plots

class histogram_data(object):
  pass

def _tolerant_float(string):
  try:
    return float(string)
  except ValueError:
    return None

# backwards-atof
def ftoa(val, format_string='%.6g'):
  if val is None :
    return '*'
  else :
    return format_string % val

class _formatting_buffer(object):
  def __init__(self, indent=0):
    self._initial_space = " " * indent
    self._buffer = []

  def write(self, strdata):
    self._buffer.append(self._initial_space + strdata)

  def append(self, strdata):
    self.write(strdata)

  def __add__(self, strdata):
    self.write(strdata)
    return self

  def __str__(self):
    out = "\n".join(self._buffer) + "\n"
    return out

def import_ccp4i_logfile(file_name=None, log_lines=None):
  assert file_name is not None or log_lines is not None
  if not log_lines :
    with open(file_name) as f:
      log_lines = f.readlines()
  current_lines = None
  tables_raw = []
  sections_read = 0
  for line in log_lines :
    line = line.strip()
    if re.match(r"\$TABLE\s*:", line):
      current_lines = [ line ]
      sections_read = 0
    elif line[-2:] == "$$" and current_lines is not None :
      current_lines.append(line)
      sections_read += line.count("$$")
      if sections_read == 4 :
        tables_raw.append(current_lines)
        current_lines = None
    elif sections_read < 4 and current_lines is not None :
      current_lines.append(line)
  tables = []
  for loggraph in tables_raw :
    t = table_data(None)
    t.import_loggraph(loggraph)
    tables.append(t)
  return tables

class simple_matplotlib_plot(object):
  """
  Class for writing a Matplotlib plot to a static image file without a GUI.
  This should be subclassed and combined with whatever mixin is used to
  actually responsible for the plotting.
  """
  def __init__(self,
                figure_size=(8,8),
                font_size=12,
                title_font_size=12,
                facecolor='white',
                transparent=False):
    adopt_init_args(self, locals())
    try :
      import matplotlib
      import matplotlib.figure
      from matplotlib.backends.backend_agg import FigureCanvasAgg
    except ImportError as e :
      print(e)
      raise Sorry("Plotting requires that matplotlib be installed.")
    self.figure = matplotlib.figure.Figure(figure_size, 72, linewidth=0,
      facecolor=facecolor)
    if transparent :
      self.figure.figurePatch.set_alpha(0.0)
    self.canvas = FigureCanvasAgg(self.figure)
    #self.canvas.toolbar = oop.null()
    #self.figmgr = FigureManager(self.canvas, 1, self)

  def save_image(self, file_name, dpi):
    assert (file_name is not None) and (file_name != "")
    base, ext = os.path.splitext(file_name)
    if (ext == ".pdf"):
      self.figure.savefig(file_name, orientation="landscape", format="pdf")
    elif (ext == ".ps"):
      self.figure.savefig(file_name, orientation="landscape", format="ps")
    elif (ext == ".png"):
      self.figure.savefig(file_name, format="png", dpi=dpi)
    else :
      raise RuntimeError("Extension %s not supported" % s)

#---end


 *******************************************************************************


 *******************************************************************************
iotbx/extract_xtal_data.py
"""
Extract crystallographic data from an array.
  Encapsulates logic for extracting experimental amplitudes and R-free flags
  from the given input file(s) via using reflection_file_server.
"""
from __future__ import absolute_import, division, print_function
from cctbx import miller
from cctbx.array_family import flex
from libtbx.utils import Sorry
import iotbx.phil
from iotbx import reflection_file_utils
from libtbx import adopt_init_args
import os
from cctbx import french_wilson
import libtbx.callbacks # import dependency
from six.moves import zip
from six.moves import range
from libtbx import group_args
from six.moves import cStringIO as StringIO

def miller_array_symmetry_safety_check(miller_array, data_description,
                                       working_point_group):
  """
  Returns None or string.
  """
  return miller_array.crystal_symmetry_is_compatible_with_symmetry_from_file(
    working_point_group = working_point_group).format_error_message(
      data_description = data_description)

def explain_how_to_generate_array_of_r_free_flags(scope = '<parent scope>.r_free_flags.generate'):
  part1 = """\
If previously used R-free flags are available run this command again
with the name of the file containing the original flags as an
additional input. If the structure was never refined before, or if the
original R-free flags are unrecoverable, run this command again with
the additional definition:

"""
  part3 = """

If the structure was refined previously using different R-free flags,
the values for R-free will become meaningful only after many cycles of
refinement.
"""
  return part1 + """%s=True""" %(scope) + part3

data_and_flags_str_part1a = """\
  file_name = None
    .type=path
    .short_caption=Reflections file
    .style = bold input_file file_type:hkl noauto process_hkl \
      child:fobs:labels child:d_min:high_resolution \
      child:d_max:low_resolution child:rfree_file:r_free_flags.file_name
    .expert_level = 0
  labels = None
    .type=strings
    .input_size = 160
    .short_caption = Data labels
    .style = bold renderer:draw_fobs_label_widget noauto \
      OnChange:auto_update_label_choice child:d_min:high_resolution \
      child:d_max:low_resolution parent:file_name:file_name
    .expert_level = 0
"""

data_and_flags_str_part1b = """\
  high_resolution = None
    .type=float
    .input_size = 80
    .style = bold renderer:draw_resolution_widget noauto
    .expert_level = 0
  low_resolution = None
    .type=float
    .input_size = 80
    .style = bold renderer:draw_resolution_widget noauto
    .expert_level = 0
  twin_law = None
    .type=str
    .input_size = 80
    .style = bold noauto
  outliers_rejection = True
    .type=bool
    .short_caption = Reject outliers
    .help = Remove "basic wilson outliers", "extreme wilson outliers", and \
              "beamstop shadow outliers"
    .expert_level = 0
  french_wilson_scale = True
    .type=bool
    .short_caption = use French-Wilson method to handle negative intensities
  french_wilson
  {
     include scope cctbx.french_wilson.master_phil
  }
  sigma_fobs_rejection_criterion = None
    .type=float
    .short_caption = Sigma(Fobs) rejection criterion
    .expert_level = 0
  sigma_iobs_rejection_criterion = None
    .type=float
    .short_caption = Sigma(Iobs) rejection criterion
    .expert_level = 0
"""

data_and_flags_str_part1 = """\
  %s
  %s
"""%(data_and_flags_str_part1a, data_and_flags_str_part1b)

data_and_flags_str_part2a = """\
  file_name = None
    .type=path
    .short_caption=File with R(free) flags
    .help = This is normally the same as the file containing Fobs and is \
      usually selected automatically.
    .input_size = 200
    .style = noauto input_file file_type:hkl process_hkl child:rfree:label
    .expert_level = 0
  label = None
    .type=str
    .short_caption = R-free label
    .input_size = 160
    .style = bold renderer:draw_rfree_label_widget noauto \
             OnChange:update_rfree_flag_value
    .expert_level = 0
"""

data_and_flags_str_part2b = """\
  required = True
    .type = bool
    .help = Specify if free-r flags are must be present (or else generated)
  test_flag_value = None
    .type=int
    .help = This value is usually selected automatically - do not change \
      unless you really know what you're doing!
    .style = noauto
    .expert_level = 0
  ignore_r_free_flags = False
    .type=bool
    .short_caption = Ignore R-free flags
    .help = Use all reflections in refinement (work and test)
    .expert_level=0
"""

data_and_flags_str_part2 = """\
  %s
  %s
"""%(data_and_flags_str_part2a, data_and_flags_str_part2b)

misc1 = """\
  ignore_all_zeros = True
    .type=bool
    .short_caption = Ignore all-zero arrays
    .expert_level = 1
  force_anomalous_flag_to_be_equal_to = None
    .type=bool
    .short_caption = Use anomalous data
    .style = tribool
    .expert_level = 1
  convert_to_non_anomalous_if_ratio_pairs_lone_less_than_threshold=0.5
    .type = float
    .expert_level = 2
  r_free_flags
    .expert_level=0
    .style = box auto_align
    .caption = This information will be extracted automatically if possible. \
      If no test set is present in the reflections file, one can be generated \
      automatically, or you can use the reflection file editor to combine an \
      existing set with your X-ray or neutron data.
"""
misc2 = """\
    disable_suitability_test = False
      .type=bool
      .expert_level = 2
    ignore_pdb_hexdigest = False
      .type=bool
      .short_caption = Ignore PDB hexdigest sanity check
      .help=If True, disables safety check based on MD5 hexdigests stored in \
            PDB files produced by previous runs.
      .expert_level=2
    generate = False
      .type=bool
      .short_caption = Generate new R-free flags
      .help = Generate R-free flags (if not available in input files)
"""

data_and_flags_str = """\
  %s
  %s
  {
    %s
    %s
    %s
  }
""" % (data_and_flags_str_part1,
       misc1,
       data_and_flags_str_part2,
       misc2,
       miller.generate_r_free_params_str)

data_and_flags_str_no_filenames = """\
  %s
  %s
  {
    %s
    %s
    %s
  }
""" % (data_and_flags_str_part1b,
       misc1,
       data_and_flags_str_part2b,
       misc2,
       miller.generate_r_free_params_str)

xray_data_str = """\
xray_data
  .help=Scope of X-ray data and free-R flags
  .style = scrolled auto_align
{
  %s
}
"""%data_and_flags_str

xray_data_str_no_filenames = """\
xray_data
  .help=Scope of X-ray data and free-R flags
  .style = scrolled auto_align
{
  %s
}
"""%data_and_flags_str_no_filenames

neutron_data_str = """\
neutron_data
  .help=Scope of neutron data and neutron free-R flags
  .style = scrolled auto_align
{
  ignore_xn_free_r_mismatch = False
    .type = bool
    .expert_level=2
    .short_caption = Ignore Xray/neutron R-free flags set mismatch
  %s
}

"""%data_and_flags_str

neutron_data_str_no_filenames = """\
neutron_data
  .help=Scope of neutron data and neutron free-R flags
  .style = scrolled auto_align
{
  ignore_xn_free_r_mismatch = False
    .type = bool
    .expert_level=2
    .short_caption = Ignore Xray/neutron R-free flags set mismatch
  %s
}

"""%data_and_flags_str_no_filenames

def data_and_flags_master_params(master_scope_name=None):
  if(master_scope_name is not None):
    p = """\
%s
{
%s
}
"""
    return iotbx.phil.parse(p%(master_scope_name, data_and_flags_str), process_includes=True)
  else:
    return iotbx.phil.parse(data_and_flags_str, process_includes=True)

class run(object):
  """
  Encapsulates logic for extracting experimental amplitudes and R-free flags
  from the given input file(s) via using reflection_file_server. This expects
  that the standard parameter block is being used.  Determination of appropriate
  data labels will be as automatic as possible, or will give clear feedback when
  ambiguity exists. If not found in the inputs, the R-free flags can be created
  if desired.

  It is supposed to run silently.
  Error messages are accumulated in self.err, and can be rised as desired after
  the execution. Log info messages are stored in self.log and can be flushed
  after the execution as well.
  """
  def __init__(self,
               reflection_file_server,
               parameters = None,
               experimental_phases_params = None,
               working_point_group = None,
               remark_r_free_flags_md5_hexdigest = None,
               extract_r_free_flags = True,
               extract_experimental_phases = True,
               keep_going = False,
               prefer_anomalous = None,
               force_non_anomalous = False,
               allow_mismatch_flags = False,
               free_r_flags_scope = 'miller_array.labels.name',
               ):
    adopt_init_args(self, locals())
    # Buffers for error and log messages.
    self.err = []
    self.log = StringIO() # container for all log messages
    #
    if(self.parameters is None):
      self.parameters = data_and_flags_master_params().extract()
    self.r_free_flags               = None
    self.test_flag_value            = None
    self.r_free_flags_md5_hexdigest = None
    self.experimental_phases        = None
    self.raw_experimental_phases    = None
    # Get data first
    self.raw_data            = self._extract_data()
    # Apply resolution and sigma cutoffs, if requested
    self.raw_data_truncated  = self._apply_cutoffs()
    self.raw_flags_truncated = None
    # Convert to usable Fobs
    self.f_obs               = self._data_as_f_obs()
    # Then extract or generate flags
    self.raw_flags = None
    if(extract_r_free_flags):
      self.raw_flags = self._extract_flags()
      if(self.raw_flags is not None):
        flags_info = self.raw_flags.info()
        if(self.raw_flags.anomalous_flag() and
           not self.raw_data_truncated.anomalous_flag()):
          self.raw_flags = self.raw_flags.as_non_anomalous_array()
        self.raw_flags_truncated = self.raw_flags.common_set(
          self.raw_data_truncated)
    if(extract_r_free_flags and self.raw_flags is not None):
      self._get_r_free_flags()
      if(self.r_free_flags is not None):
        self.r_free_flags.set_info(flags_info)
    # Make sure they match
    if(self.r_free_flags is not None):
      f_obs_info = self.f_obs.info()
      flags_info = self.r_free_flags.info()
      self.f_obs, self.r_free_flags = self.f_obs.common_sets(self.r_free_flags)
      self.f_obs        = self.f_obs.set_info(f_obs_info)
      self.r_free_flags = self.r_free_flags.set_info(flags_info)
    # extract phases
    if extract_experimental_phases:
      self.experimental_phases = self._determine_experimental_phases(
        parameters      = experimental_phases_params,
        parameter_scope = 'miller_array.labels.name')
    # Fill in log
    self._show_summary()

  def _compose_mtz_object(self):
    f_obs_label = "F-obs"
    i_obs_label = "I-obs"
    flag_label  = "R-free-flags"
    phase_label = "HL"
    if(self.raw_data.is_xray_intensity_array()):
      column_root_label = i_obs_label
    else:
      column_root_label = f_obs_label
    mtz_dataset = self.raw_data.as_mtz_dataset(
      column_root_label = column_root_label)
    if (self.raw_flags is not None):
      mtz_dataset.add_miller_array(
        miller_array      = self.raw_flags,
        column_root_label = flag_label)
    if(self.experimental_phases is not None):
      mtz_dataset.add_miller_array(
        miller_array      = self.raw_experimental_phases,
        column_root_label = phase_label)
    labels = group_args(
      data   = column_root_label,
      flags  = flag_label,
      phases = phase_label)
    return group_args(
      mtz_dataset = mtz_dataset,
      labels      = labels)

  def result(self):
    """
    Container for:
      - raw arrays as extracted from inputs.
      - arrays that are ready to use (after aply cutoffs, map-to-asu, etc).
      - error messages (to rise if desired).
      - log messaged (to flush if desired).
      - misc.

    """
    i_obs = None
    if(self.raw_data.is_xray_intensity_array()):
      i_obs = self.raw_data
    o = group_args(
      raw_data                   = self.raw_data,
      raw_flags                  = self.raw_flags,
      raw_experimental_phases    = self.raw_experimental_phases,
      f_obs                      = self.f_obs,
      i_obs                      = i_obs,
      r_free_flags               = self.r_free_flags,
      test_flag_value            = self.test_flag_value,
      experimental_phases        = self.experimental_phases,
      r_free_flags_md5_hexdigest = self.r_free_flags_md5_hexdigest,
      err                        = self.err,
      log                        = self.log,
      mtz_object                 = self._compose_mtz_object())
    o.stop_dynamic_attributes()
    return o

  def show_summary(self, log, prefix=""):
    print(self.log.getvalue(), file=log)

  def _print(self, m): print(m, file=self.log)

  def _show_summary(self, prefix=""):
    log = self.log
    self._print("%sInput data summary:"%prefix)
    self.raw_data.show_comprehensive_summary(f=log, prefix="  "+prefix)
    if(not self.raw_data.indices().all_eq(self.f_obs.indices()) or
       type(self.raw_data.observation_type()) !=
       type(self.f_obs.observation_type())):
      print(prefix, file=log)
      print("%sData used summary:"%prefix, file=log)
      self.f_obs.show_comprehensive_summary(f=log, prefix="  "+prefix)
    if(self.r_free_flags is not None):
      print(prefix, file=log)
      self._print("%sFree-R flags summary:"%prefix)
      self.r_free_flags.show_comprehensive_summary(f=log, prefix="  "+prefix)
      print("%s  Test (R-free) flag value: %d"%(prefix, self.test_flag_value),
        file=self.log)
      self.r_free_flags.show_r_free_flags_info(out=log, prefix="  "+prefix)
    if(self.experimental_phases is not None):
      self._print("%sExperimental phases:"%prefix)
      self.experimental_phases.show_comprehensive_summary(f=log, prefix="  "+prefix)
      print("%s  Average figures of merit by resolution:"%prefix, file=log)
      figures_of_merit = abs(self.experimental_phases.phase_integrals())
      figures_of_merit.setup_binner(n_bins=10)
      legend_len = figures_of_merit.mean(use_binning=True).show(
        data_fmt="%6.3f", show_unused=False, f = log, prefix="    "+prefix)
      print("   ", ("%%%ds"%legend_len)%"overall", \
        "%6.3f"%figures_of_merit.mean(), file=log)
      print("%s  Note: Figures of merit are determined by integration of"%prefix, file=log)
      print("%s        Hendrickson-Lattman coefficients."%prefix, file=log)
      print(file=log)
      print("%s  Number and fraction of available experimental phases by resolution:"%prefix, file=log)
      self.experimental_phases.setup_binner(n_bins=10)
      self.experimental_phases.count_and_fraction_in_bins(
        data_value_to_count = (0,0,0,0),
        count_not_equal= True).show(show_unused = False, f = log, prefix="    "+prefix)
      print(file=log)

  def _determine_experimental_phases(
        self,
        parameters=None,
        parameter_scope=None,
        ignore_all_zeros = True):
    try:
      file_name = None
      labels = None
      if(parameters is not None):
        file_name = parameters.file_name
        labels    = parameters.labels
      experimental_phases = \
        self.reflection_file_server.get_experimental_phases(
          file_name        = file_name,
          labels           = labels,
          raise_no_array   = False,
          parameter_name   = "",
          ignore_all_zeros = ignore_all_zeros,
          parameter_scope  = parameter_scope)

      if(experimental_phases is None): return None
    except reflection_file_utils.Sorry_No_array_of_the_required_type:
      experimental_phases = None
    else:
      info = experimental_phases.info()
      if(parameters is not None):
        parameters.file_name = experimental_phases.info().source
        parameters.labels = [experimental_phases.info().label_string()]
      msg = miller_array_symmetry_safety_check(
        miller_array        = experimental_phases,
        data_description    = "Experimental phases",
        working_point_group = self.working_point_group)
      if(msg is not None and not self.keep_going):
        self.err.append(msg)
      experimental_phases = experimental_phases.regularize()
      self.raw_experimental_phases = experimental_phases
      if(not self.f_obs.anomalous_flag()):
        if(experimental_phases.anomalous_flag()):
          experimental_phases = experimental_phases.average_bijvoet_mates()
      elif(not experimental_phases.anomalous_flag()):
         experimental_phases = experimental_phases.generate_bijvoet_mates()
      self.experimental_phases = experimental_phases.matching_set(
        other = self.f_obs, data_substitute=(0,0,0,0))
      experimental_phases.set_info(info)
    return experimental_phases

  def _get_r_free_flags(self):
    self.r_free_flags,self.test_flag_value,self.r_free_flags_md5_hexdigest =\
      self._flags_as_r_free_flags(f_obs = self.f_obs, r_free_flags =
      self.raw_flags)
    if(self.r_free_flags is not None):
      self.r_free_flags.set_info(self.raw_flags.info())

  def _extract_data(self):
    data = self.reflection_file_server.get_xray_data(
      file_name        = self.parameters.file_name,
      labels           = self.parameters.labels,
      ignore_all_zeros = self.parameters.ignore_all_zeros,
      parameter_name   = "",
      parameter_scope  = "miller_array.labels.name",
      prefer_anomalous = self.prefer_anomalous)
    self.parameters.file_name = data.info().source
    self.parameters.labels    = [data.info().label_string()]
    if(self.working_point_group is not None):
      msg = miller_array_symmetry_safety_check(
        miller_array        = data,
        data_description    = "Reflection data",
        working_point_group = self.working_point_group)
      if(msg is not None and not self.keep_going):
        self.err.append(msg)
    return data.regularize()

  def _extract_flags(self, data_description = "R-free flags"):
    r_free_flags, test_flag_value = None, None
    params = self.parameters.r_free_flags
    # Extract
    if(not self.parameters.r_free_flags.generate):
      try:
        r_free_flags, test_flag_value = \
          self.reflection_file_server.get_r_free_flags(
            file_name                = params.file_name,
            label                    = params.label,
            test_flag_value          = params.test_flag_value,
            disable_suitability_test = params.disable_suitability_test,
            parameter_scope          = self.free_r_flags_scope)
      except reflection_file_utils.Sorry_No_array_of_the_required_type as e:
        if(self.parameters.r_free_flags.generate is not None):
          if(not self.keep_going):
            self.err.append(explain_how_to_generate_array_of_r_free_flags(
              scope = "xray_data.r_free_flags.generate"))
            self.err.append("Please try again.")
          return None
        r_free_flags, test_flag_value = None, None
      else:
        params.file_name       = r_free_flags.info().source
        params.label           = r_free_flags.info().label_string()
        params.test_flag_value = test_flag_value
        msg = miller_array_symmetry_safety_check(
          miller_array        = r_free_flags,
          data_description    = data_description,
          working_point_group = self.working_point_group)
        if(msg is not None and not self.keep_going):
          self.err.append(msg)
        info = r_free_flags.info()
        try:
          processed = r_free_flags.regularize()
        except RuntimeError as e:
          self.err.append("Bad free-r flags:\n %s"%str(e))
          return None
        if (self.force_non_anomalous):
          processed = processed.average_bijvoet_mates()
        r_free_flags = processed.set_info(info)
    # Generate or stop
    if(r_free_flags is None):
      if ((params.fraction is None) or
          (params.lattice_symmetry_max_delta is None) or
          (params.use_lattice_symmetry is None)):
        msg = """
No R-free flags are available, but one or more parameters required to generate
new flags is undefined.
"""
        self.err.append(msg)
        return None
      print("Generating a new array of R-free flags.", file=self.log)
      print(file=self.log)
      r_free_flags = self.f_obs.generate_r_free_flags(
        fraction                   = params.fraction,
        max_free                   = params.max_free,
        lattice_symmetry_max_delta = params.lattice_symmetry_max_delta,
        use_lattice_symmetry       = params.use_lattice_symmetry,
        use_dataman_shells         = params.use_dataman_shells,
        n_shells                   = params.n_shells
        ).set_info(miller.array_info(labels = ["R-free-flags"]))
      params.label           = r_free_flags.info().label_string()
      params.test_flag_value = 1
    # check if anomalous pairs are sound
    if(r_free_flags is not None):
      r_free_flags.deep_copy().as_non_anomalous_array()
    # make sure flags match anomalous flag of data
    if(self.raw_data.anomalous_flag() and not r_free_flags.anomalous_flag()):
      info = r_free_flags.info()
      observation_type = r_free_flags.observation_type()
      r_free_flags = r_free_flags.generate_bijvoet_mates()
      r_free_flags.set_observation_type(observation_type)
      r_free_flags.set_info(info)
    return r_free_flags

  def _apply_cutoffs(self):
    if(self.raw_data is None): return None
    selection = self.raw_data.all_selection()
    dd = self.raw_data.d_spacings().data()
    if(self.parameters.low_resolution is not None):
      selection &= dd <= self.parameters.low_resolution
    if(self.parameters.high_resolution is not None):
      selection &= dd >= self.parameters.high_resolution
    if(self.raw_data.sigmas() is not None):
      sigma_cutoff = self.parameters.sigma_fobs_rejection_criterion
      if(sigma_cutoff is not None and sigma_cutoff > 0):
        s = self.raw_data.data() > self.raw_data.sigmas()*sigma_cutoff
        selection &= s
    if(selection.count(True) == 0):
      self.err.append("No data left after applying resolution and sigma cutoffs.")
      return None
    return self.raw_data.select(selection)

  def _data_as_f_obs(self):
    """
    Convert input data array to amplitudes, adjusting the data type and
    applying additional filters if necessary.

    :param f_obs: selected input data
    :returns: :py:class:`cctbx.miller.array` of real numbers with observation
      type set to amplitudes
    """
    if(self.raw_data_truncated is None): return None
    f_obs = self.raw_data_truncated.deep_copy()
    # Convert to non-anomalous if requested
    if(self.force_non_anomalous):
      f_obs = f_obs.average_bijvoet_mates()
    #
    d_min = f_obs.d_min()
    if(d_min < 0.25):
      self.err.append("Resolution of data is too high: %-6.4f A"%d_min)
      return None
    if(f_obs.is_complex_array()): f_obs = abs(f_obs)
    if(f_obs.is_xray_intensity_array()):
      if(self.parameters.french_wilson_scale):
        try:
          sigI_cutoff = self.parameters.sigma_iobs_rejection_criterion
          f_obs = french_wilson.french_wilson_scale(
            miller_array                   = f_obs,
            params                         = self.parameters.french_wilson,
            sigma_iobs_rejection_criterion = sigI_cutoff,
            log                            = self.log)
        except Exception as e:
          print(str(e), file=self.log)
          print("Using alternative Iobs->Fobs conversion.", file=self.log)
          f_obs = f_obs.f_sq_as_f()
    #
    f_obs.set_observation_type_xray_amplitude()
    if(self.parameters.force_anomalous_flag_to_be_equal_to is not None):
      if(not self.parameters.force_anomalous_flag_to_be_equal_to):
        if(f_obs.anomalous_flag()):
          merged = f_obs.as_non_anomalous_array().merge_equivalents()
          f_obs = merged.array().set_observation_type( f_obs )
      elif(not f_obs.anomalous_flag()):
        observation_type = f_obs.observation_type()
        f_obs = f_obs.generate_bijvoet_mates()
        f_obs.set_observation_type(observation_type)
    else:
      f_obs = f_obs.convert_to_non_anomalous_if_ratio_pairs_lone_less_than(
        threshold=self.parameters.
          convert_to_non_anomalous_if_ratio_pairs_lone_less_than_threshold)
    f_obs.set_info(self.raw_data.info())
    return f_obs

  def _flags_as_r_free_flags(self,
        f_obs,
        r_free_flags,
        missing_show_max_lines=10):
    test_flag_value = self.parameters.r_free_flags.test_flag_value
    if(test_flag_value is None):
      msg = """
Could not determine an appropriate test flag for the data with label(s) '%s'.
This may happen if they are all a single value; please check the file to make
sure the flags are suitable for use.
"""
      self.err.append(msg % self.parameters.r_free_flags.label)
    if (isinstance(r_free_flags.data(), flex.bool)):
      r_free_flags = r_free_flags.array(
        data = r_free_flags.data() == bool(test_flag_value))
    else:
      r_free_flags = r_free_flags.array(
        data = r_free_flags.data() == test_flag_value)
    r_free_flags_md5_hexdigest = \
      r_free_flags.map_to_asu().sort(by_value="packed_indices").data() \
        .md5().hexdigest()
    if(self.remark_r_free_flags_md5_hexdigest is not None):
      self._verify_r_free_flags_md5_hexdigest(
        ignore_pdb_hexdigest = self.parameters.r_free_flags.ignore_pdb_hexdigest,
        current              = r_free_flags_md5_hexdigest,
        records              = self.remark_r_free_flags_md5_hexdigest)
    if(not f_obs.anomalous_flag()):
      if(r_free_flags.anomalous_flag()):
        print("Reducing R-free flags to non-anomalous array.", file=self.log)
        r_free_flags = r_free_flags.average_bijvoet_mates()
        print(file=self.log)
    elif(not r_free_flags.anomalous_flag()):
       print("Generating Bijvoet mates of R-free flags.", file=self.log)
       r_free_flags = r_free_flags.generate_bijvoet_mates()
       print(file=self.log)
    r_free_flags = r_free_flags.map_to_asu().common_set(f_obs)
    n_missing_r_free_flags = f_obs.indices().size() \
      - r_free_flags.indices().size()
    if(not self.allow_mismatch_flags and n_missing_r_free_flags != 0):
      msg = [
        "R-free flags not compatible with F-obs array:"
        " missing flag for %d F-obs selected for refinement"
          % n_missing_r_free_flags]
      if (missing_show_max_lines is not None and missing_show_max_lines <= 0):
        msg[0] += "."
      else:
        msg[0] += ":"
        lone = f_obs.lone_set(other=r_free_flags)
        if (missing_show_max_lines is None):
          n_not_shown = 0
        else:
          n_not_shown = lone.indices().size() - missing_show_max_lines
          if (n_not_shown > missing_show_max_lines * 0.5):
            lone = lone[:missing_show_max_lines]
          else:
            n_not_shown = 0
        if (lone.sigmas() is None):
          msg.append("    h   k   l   data")
          for hkl,f in zip(lone.indices(), lone.data()):
            msg.append("  %3d %3d %3d" % hkl + "   %.6g" % f)
        else:
          msg.append("    h   k   l   data  sigmas")
          for hkl,f,s in zip(lone.indices(), lone.data(), lone.sigmas()):
            msg.append("  %3d %3d %3d" % hkl + "   %.6g  %.6g" % (f,s))
        if (n_not_shown != 0):
          msg.append("    ... (remaining %d not shown)" % n_not_shown)
      self.err.append("\n".join(msg))
      return None,None,None
    return r_free_flags, test_flag_value, r_free_flags_md5_hexdigest

  def _verify_r_free_flags_md5_hexdigest(self,
        ignore_pdb_hexdigest,
        current,
        records):
    from_file = set()
    for record in records:
      flds = record.split()
      if (len(flds) == 3):
        from_file.add(flds[2])
    if(len(from_file) > 1):
      msg="""
Multiple conflicting REMARK r_free_flags.md5.hexdigest records found in the
input PDB file.
"""
      self.err.append(msg)
    if (len(from_file) == 1 and current not in from_file):
      log = self.log
      for i in range(2): print("*"*79, file=log)
      if(ignore_pdb_hexdigest):
        print(file=log)
        print(" ".join(["WARNING"]*9), file=log)
      print("""
The MD5 checksum for the R-free flags array summarized above is:
  %s

The corresponding MD5 checksum in the PDB file summarized above is:
  %s

These checksums should be identical but are in fact different. This is
because the R-free flags used at previous stages of refinement are
different from the R-free flags summarized above. As a consequence,
the values for R-free could be biased and misleading.

However, there is no problem if the R-free flags were just extended to
a higher resolution, or if some reflections with no data or that are
not part of the R-free set have been added or removed.""" % (
  current, sorted(from_file)[0]), end=' ', file=log)
      if (not ignore_pdb_hexdigest):
        print("""\
In this case,
simply remove the

  REMARK r_free_flags.md5.hexdigest %s

record from the input PDB file to proceed with the refinement.""" % (
  sorted(from_file)[0]), end=' ', file=log)
      print("""

Otherwise it is best to recover the previously used R-free flags
and use them consistently throughout the refinement of the model.
Run this command again with the name of the file containing the
original flags as an additional input.
""", file=log)
      if (not ignore_pdb_hexdigest):
        print("""\
If the original R-free flags are unrecoverable, remove the REMARK
record as indicated above. In this case the values for R-free will
become meaningful only after many cycles of refinement.
""", file=log)
      else:
        print("""\
If the original R-free flags are unrecoverable, the values for R-free
will become meaningful only after many cycles of refinement.
""", file=log)
      for i in range(2): print("*"*79, file=log)
      print(file=log)
      if (not ignore_pdb_hexdigest):
        if ("PHENIX_GUI_ENVIRONMENT" in os.environ):
          log.flush()
          raise Sorry("This model appears to have previously been refined "+
            "against a different set of R-free flags.  Please resolve the "+
            "mismatch; additional information and instructions are available "+
            "at the end of the log output.")
        else :
          raise Sorry("Please resolve the R-free flags mismatch.")

map_coefficients_params_str = """\
  file_name=None
    .type=path
    .short_caption=Map coefficients file
  labels=None
    .type=strings
"""

experimental_phases_params_str = """\
  file_name=None
    .type=path
    .short_caption=Experimental phase file
    .style = bold input_file file_type:hkl process_hkl child:hl_coeffs:labels
  labels=None
    .type=strings
    .input_size = 160
    .short_caption = Phase labels
    .style = renderer:draw_hl_label_widget bold
"""

experimental_phases_params = iotbx.phil.parse(
  input_string=experimental_phases_params_str)

def experimental_phases_master_params():
  return experimental_phases_params


 *******************************************************************************


 *******************************************************************************
iotbx/file_reader.py
"""
Generic file input module, used in Phenix GUI and elsewhere.  This trades some
loss of efficiency for a simplified API for loading any file type more or less
automatically.  It will first try to guess the format based on the extension;
if this fails, it will then try other formats.  This is used on the command
line and the Phenix GUI to process bulk file input.  In most other cases a
specific file type is desired, and the force_type argument will ensure that
only this format is attempted.

Examples
--------
>>> from iotbx.file_reader import any_file
>>> input_file = any_file(sys.argv[1:])
>>> file_data = input_file.file_object

>>> pdb_in = any_file("model.pdb", force_type="pdb")
>>> pdb_in.assert_file_type("pdb")
>>> hierarchy = pdb_in.file_object.hierarchy

>>> mtz_in = any_file("data.mtz", force_type="hkl")
>>> miller_arrays = mtz_in.file_server.miller_arrays
"""
from __future__ import absolute_import, division, print_function

# MTZ file handling is kludgy, but unfortunately there are circumstances
# where only an MTZ file will do, so it requires some extra code to work
# around the automatic behavior

from libtbx.utils import Sorry, to_str
from six.moves import cPickle as pickle
import os
import sys
import six

standard_file_types = ["hkl", "ccp4_map", "xplor_map", "pdb", "cif", "phil",
  "hhr", "ncs", "aln", "seq", "xml", "pkl", "txt",]

standard_file_extensions = {
  'pdb'  : ["pdb", "ent"],
  'hkl'  : ["mtz", "hkl", "sca", "cns", "xplor", "cv", "ref", "fobs"],
  'cif'  : ["cif", "mmcif"],
  'seq'  : ["fa", "faa", "seq", "pir", "dat", "fasta"],
  'xplor_map' : ["xplor", "map"],
  'ccp4_map'  : ["ccp4", "map", "mrc"],
  'map'  : ["xplor", "map", "ccp4"],
  'phil' : ["params", "eff", "def", "phil", "param"],
  'xml'  : ["xml"],
  'pkl'  : ["pickle", "pkl"],
  'txt'  : ["txt", "log", "html", "geo"],
  'mtz'  : ["mtz"],
  'aln'  : ["aln", "ali", "clustal"],
  'a3m'  : ["a3m"],
  'hhr'  : ["hhr"],
  'ncs'  : ["ncs","ncs_spec"],
  'img'  : ["img", "osc", "mccd", "cbf", "nxs", "h5", "hdf5"],
  # XXX these are not supported by this module, but are used in Phenix, so we
  # need to be able to include them in GUI tools in wxtbx.
  'smi' : ['smi'],
  'sdf' : ['sdf'],
  'rosetta' : ['gz'],
}
compression_extensions = ["gz", "Z", "bz2", "zip"] # gz and bz2 work with maps... gz, Z and bz2 work with models

standard_file_descriptions = {
  'pdb'  : "Model",
  'hkl'  : "Reflections",
  'cif'  : "Restraints",
  'seq'  : "Sequence",
  'xplor_map'  : "XPLOR map",
  'ccp4_map' : "CCP4 map",
  'phil' : "Parameters",
  'xml'  : "XML",
  'pkl'  : "Python pickle",
  'txt'  : "Text",
  'mtz'  : "Reflections (MTZ)",
  'aln'  : "Sequence alignment",
  'a3m'  : "MSA (a3m)",
  'hhr'  : "HHpred alignment",
  'ncs'  : "NCS information file",
  'img'  : "Detector image",
  # XXX see comment above
  'smi' : "SMILES",
  'sdf' : "Structure data file",
  'rosetta' : "Rosetta fragments",
}

supported_file_types = ["pdb","hkl","cif","pkl","ncs","seq","phil",
  "aln", "a3m", "txt", "xplor_map", "ccp4_map"]

binary_types = ["hkl","ccp4_map","img","pkl"]
ascii_types = ["hkl","xplor_map","pdb","cif","phil","hhr", "ncs", "aln", "a3m",
   "seq", "xml", "txt"]

# Try files with these extensions only with their associated file types
extensions_absolutely_defining_type = ['ccp4','mrc']

def get_wildcard_string(format):
  assert (format in standard_file_extensions), format
  wildcards = [ "*.%s" % ext for ext in standard_file_extensions[format] ]
  # Add wildcard without breaking other features by having cif in two places
  if format == 'pdb':
    cif_format = "*.%s" % "cif"
    if not cif_format in wildcards:
      wildcards.append(cif_format)

  wildcard_str = "%s file (%s)|%s" % (standard_file_descriptions[format],
    ", ".join(wildcards), ";".join(wildcards))
  return wildcard_str

def get_wildcard_strings(formats, include_any=True):
  wildcards = [ get_wildcard_string(format) for format in formats ]
  if (include_any):
    if (sys.platform != "darwin"):
      wildcards.insert(0, "All files (*.*)|*.*")
    else :
      wildcards.append("All files (*.*)|*.*")
  wildcards_str = "|".join(wildcards)
  return wildcards_str

class FormatError(Sorry):
  pass

def strip_shelx_format_extension(file_name):
  if (file_name.endswith("=hklf3") or file_name.endswith("=hklf4") or
      file_name.endswith("=amplitudes") or file_name.endswith("=intensities")):
    file_name = "".join(file_name.split("=")[:-1])
  return file_name

def splitext(file_name):
  """
  Args:
    file_name: A plain text string of the file path

  Returns: A tuple of the base filename, the file format extension, and possibly a compression extension
  """

  folder_name, file_only = os.path.split(file_name)
  period_split = file_only.split(".")
  compressed = period_split[-1] in compression_extensions
  if compressed:
    file_ext = '.'+period_split[-2]
    compress_ext = '.'+period_split[-1]
    file_base = file_only.replace('.' + file_ext + '.' + compress_ext, '')
  else:
    file_ext = '.'+period_split[-1]
    compress_ext = None
    file_base = file_only.replace('.' + file_ext, '')
  file_base = os.path.join(folder_name, file_base)

  return (file_base, file_ext, compress_ext)


def guess_file_type(file_name, extensions=standard_file_extensions):
  base, ext, compress_ext = splitext(file_name)
  if (ext == ""):
    return None
  if (ext == ".mtz") : # XXX gross
    return "hkl"
  for known_type, known_extensions in six.iteritems(extensions):
    if ext[1:] in known_extensions :
      return known_type
  return None

def sort_by_file_type(file_names, sort_order=None):
  if (sort_order is None):
    sort_order = standard_file_types
  def _score_extension(ext):
    for n, format in enumerate(sort_order):
      extensions = standard_file_extensions.get(format, [])
      if (ext[1:] in extensions):
        return len(sort_order) - n
    return 0
  def _sort(f1, f2):
    base1, ext1, compress_ext1  = splitext(f1)
    base2, ext2, compress_ext2 = splitext(f2)
    s1 = _score_extension(ext1)
    s2 = _score_extension(ext2)
    if (s1 > s2):
      return -1
    elif (s2 > s1):
      return 1
    else :
      return 0
  from functools import cmp_to_key
  file_names.sort(key = cmp_to_key(_sort))
  return file_names

def any_file(file_name,
              get_processed_file=False,
              valid_types=supported_file_types,
              extensions_absolutely_defining_type=
                  extensions_absolutely_defining_type,
              allow_directories=False,
              force_type=None,
              input_class=None,
              raise_sorry_if_errors=False,
              raise_sorry_if_not_expected_format=False):
  """
  Main input method, wrapper for any_file_input class.

  :param file_name: path to file (relative or absolute)
  :param get_processed_file: TODO
  :param valid_types: file types to consider
  :param allow_directories: process directory if given as file_name
  :param force_type: read as this format, don't try any others
  :param input_class: optional substitute for any_file_input, with additional
    parsers
  :param raise_sorry_if_errors: raise a Sorry exception if parsing fails (used
    with force_type)
  :param raise_sorry_if_not_expected_format: raise a Sorry exception if the
    file extension does not match the parsed file type
  :param extensions_absolutely_defining_type: if the file has one of these
    extensions, only try the associated file type
  :returns: any_file_input object, or an instance of the input_class param
  """
  file_name_raw = file_name
  file_name = strip_shelx_format_extension(file_name)
  if (file_name != file_name_raw) and (force_type is None):
    force_type = "hkl"
  # See if extension is in extensions_absolutely_defining_type
  _, ext = os.path.splitext(file_name_raw)
  ext = ext[1:]  # remove . in .ccp4
  if (force_type is None) and extensions_absolutely_defining_type and (
      ext in extensions_absolutely_defining_type):
    for ft in supported_file_types:
      if ext in standard_file_extensions[ft]:
        force_type = ft
        break
    assert force_type is not None

  if not os.path.exists(file_name):
    raise Sorry("Couldn't find the file %s" % file_name)
  elif os.path.isdir(file_name):
    if not allow_directories :
      raise Sorry("This application does not support folders as input.")
    else :
      return directory_input(file_name)
  elif not os.path.isfile(file_name):
    raise Sorry("%s is not a valid file.")
  else :
    if input_class is None :
      input_class = any_file_input
    return input_class(file_name=file_name_raw,
      get_processed_file=get_processed_file,
      valid_types=valid_types,
      force_type=force_type,
      raise_sorry_if_errors=raise_sorry_if_errors,
      raise_sorry_if_not_expected_format=raise_sorry_if_not_expected_format)



class any_file_input(object):
  """
  Container for file data of any supported type.  Usually obtained via the
  any_file() function rather than being instantiated directly.
  """

  __extensions__ = standard_file_extensions
  __descriptions__ = standard_file_descriptions

  def __init__(self,
      file_name,
      get_processed_file,
      valid_types,
      force_type,
      raise_sorry_if_errors=False,
      raise_sorry_if_not_expected_format=False) : # XXX should probably be True
    self.valid_types = valid_types
    self._file_name = file_name
    self._file_object = None
    self._file_type = None
    self._file_server = None
    self.file_description = None
    self._cached_file = None # XXX: used in phenix.file_reader
    self._tried_types = []
    self._errors = {}
    file_name_clean = strip_shelx_format_extension(file_name)
    self.file_size = os.path.getsize(file_name_clean)
    self.get_processed_file = get_processed_file
    file_base, file_ext, compress_ext = splitext(file_name)
    file_ext = file_ext.lower()
    if (force_type not in [None, "None"]):
      read_method = getattr(self, "_try_as_%s" % force_type, None)
      if (read_method is None):
        raise Sorry("Couldn't force file type to '%s' - unrecognized format." %
                    force_type)
      else :
        if (raise_sorry_if_errors):
          try :
            read_method()
          except Exception as e :
            raise Sorry("Couldn't read '%s' as file type '%s': %s" %
              (file_name, force_type, str(e)))
        else :
          read_method()
    else :
      # XXX this is probably not the best way to do this - if the file format
      # is obviously something we don't want, this should be determined first
      # isntead of trying the limited set of parsers which won't work.
      for file_type in valid_types :
        if ((file_ext[1:] in self.__extensions__[file_type]) and
            (not file_ext in [".txt"])):
          read_method = getattr(self, "_try_as_%s" % file_type)
          self._tried_types.append(file_type)
          try :
            read_method()
          except KeyboardInterrupt :
            raise
          except FormatError as e :
            raise e
          except Exception as e :
            # XXX we need to be a little careful about what extensions we
            # do this for - they are not necessarily all unambiguous!
            if ((raise_sorry_if_not_expected_format) and
                (file_ext in [".pdb",".mtz",".cif",".sca",".xml",".phil"])):
              raise Sorry("File format error:\n" + str(e))
            self._errors[file_type] = str(e)
            self._file_type = None
            self._file_object = None
          else :
            break
      if self._file_type is None :
        self.try_all_types()
    if self._file_type is not None :
      self.file_description = self.__descriptions__[self.file_type]

  @property
  def file_name(self):
    return self._file_name

  @property
  def file_type(self):
    """
    Return a string representing the generic data type, for example 'pdb' or
    'hkl'.  Note that this is not necessarily the same as the underlying
    format, for example 'pdb' can mean either PDB or mmCIF format, and 'hkl'
    could mean MTZ, CIF, XDS, Scalepack, or SHELX format.
    """
    return self._file_type

  def set_file_type(self, file_type):
    self._file_type = file_type

  @property
  def file_object(self):
    """Synonym for file_content()"""
    return self._file_object

  @property
  def file_content(self):
    """Return the underlying format-specific object containing file data."""
    return self._file_object

  @property
  def file_server(self):
    """
    For reflection files only, returns an
    :py:class:`iotbx.reflection_file_utils.reflection_file_server` object
    containing the extracted Miller arrays.  Note that this will implicitly
    merge any non-unique observations.
    """
    from iotbx.reflection_file_utils import reflection_file_server
    if (self._file_server is None):
      if (self._file_type == "hkl"):
        self._file_server = reflection_file_server(
          crystal_symmetry=None,
          force_symmetry=True,
          reflection_files=[self._file_object],
          err=sys.stderr)
    return self._file_server

  def _try_as_pdb(self):
    """
    PDB parser, actually tries both 'classic' PDB and mmCIF formats.
    """
    import iotbx.pdb.hierarchy
    try :
      pdb_inp = iotbx.pdb.hierarchy.input(self.file_name)
    except ValueError as e :
      raise Sorry(str(e))
    if (pdb_inp.hierarchy.models_size() == 0):
      raise ValueError("No ATOM or HETATM records found in %s."%self.file_name)
    self._file_type = "pdb"
    self._file_object = pdb_inp

  def _try_as_hkl(self):
    from iotbx.reflection_file_reader import any_reflection_file
    # XXX this is unfortunate, but unicode breaks Boost.Python extensions
    hkl_file = any_reflection_file(str(self.file_name))
    assert (hkl_file.file_type() is not None), "Not a valid reflections file."
    self._file_type = "hkl"
    self._file_object = hkl_file

  def _try_as_cif(self):
    # XXX hack to avoid choking on CCP4 maps and images
    file_ext = os.path.splitext(self.file_name)[1]
    assert (not file_ext in [".ccp4", ".img", ".osc", ".mccd"])
    import iotbx.cif
    from iotbx.reflection_file_reader import any_reflection_file
    cif_file = any_reflection_file(str(self.file_name))
    if cif_file.file_type() is not None:
      self._file_object = cif_file
      self._file_type = "hkl"
    else:
      # Try to read as simple cif model file.  If it fails, use the
      #  input_hierarchy_pair reader as previously (totally unknown
      #  function or reason for this reader. See:
      #    modules/cctbx_project/iotbx/pdb/hierarchy.py
      try :
        pdb_inp = iotbx.pdb.hierarchy.input(self.file_name)
        if len(pdb_inp.input.atoms()) > 0:
          self._file_object = pdb_inp
          self._file_type = "pdb"
      except Exception as e :
        pass
      if not self._file_object:
        from iotbx.pdb.mmcif import cif_input
        from iotbx.pdb.hierarchy import input_hierarchy_pair
        try:
          cif_in = cif_input(file_name=self.file_name)
          pdb_inp = input_hierarchy_pair(cif_in, cif_in.hierarchy)
          self._file_object  = pdb_inp
          self._file_type = "pdb"
        except Exception as e:
          if (str(e).startswith("Space group is incompatible") or
              str(e).startswith("The space group") ):
            raise
          else:
            pdb_inp = iotbx.cif.reader(file_path=self.file_name,
              strict=False)
            self._file_type = "cif"
            self._file_object = pdb_inp

  def _try_as_phil(self):
    from iotbx.phil import parse as parse_phil
    phil_object = parse_phil(file_name=self.file_name, process_includes=True)
    assert (len(phil_object.objects) > 0), "Empty parameter file."
    self._file_type = "phil"
    self._file_object = phil_object

  def _try_as_seq(self):
    # XXX hack to avoid choking on CCP4 maps
    assert (not self.file_name.endswith(".ccp4"))
    # XXX hack to avoid choking on NCS files:
    assert (not self.file_name.endswith(".ncs"))
    assert (not self.file_name.endswith(".ncs_spec"))

    from iotbx.bioinformatics import any_sequence_format
    objects, non_compliant = any_sequence_format(self.file_name)
    assert (objects is not None), "No sequence data found in file."
    assert (len(non_compliant) == 0), "Misformatted data in file."
    for seq_obj in objects :
      assert (not "-" in seq_obj.sequence)
    self._file_object = objects
#    self._try_as_txt()
#    assert len(self._file_object) != 0
#    for _line in self._file_object.splitlines():
#      assert not _line.startswith(" ")
#      line = re.sub(" ", "", _line)
#      assert ((len(line) == 0) or
#              (line[0] == ">") or
#              (line == "*") or
#              ((line[-1] == '*') and line[:-1].isalpha()) or
#              line.isalpha())
    # Filter out empty sequences
    tmp = []
    for o in self._file_object:
      if(len(o)!=0): tmp.append(o)
    self._file_object = tmp
    self._file_type = "seq"

  def _try_as_hhr(self):
    from iotbx.bioinformatics import any_hh_file
    hh_object = any_hh_file(self.file_name)
    assert (not hh_object.query in ["", None])
    self._file_object = hh_object
    self._file_type = "hhr"

  def _try_as_a3m(self):
    from iotbx.bioinformatics import any_a3m_file
    a3m_object = any_a3m_file(self.file_name)
    self._file_object = a3m_object
    self._file_type = "a3m"

  def _try_as_aln(self):
    from iotbx.bioinformatics import any_alignment_file
    aln_object = any_alignment_file(self.file_name)
    self._file_object = aln_object
    self._file_type = "aln"

  def _try_as_xplor_map(self):
    import iotbx.xplor.map
    map_object = iotbx.xplor.map.reader(file_name=str(self.file_name))
    self._file_type = "xplor_map"
    self._file_object = map_object

  def _try_as_ccp4_map(self):
    from iotbx.map_manager import map_manager
    from libtbx.utils import null_out
    map_object=map_manager(file_name=str(self.file_name),log=null_out())
    self._file_type = "ccp4_map"
    self._file_object = map_object

  def _try_as_pkl(self):
    with open(self.file_name, "rb") as fh:
      pkl_object = pickle.load(fh)
    self._file_type = "pkl"
    self._file_object = pkl_object

  def _try_as_txt(self):
    with open(self.file_name) as fh:
      file_as_string = fh.read()
    file_as_ascii = to_str(file_as_string)
    self._file_type = "txt"
    self._file_object = file_as_string

  def _try_as_xml(self):
    import xml.dom.minidom
    xml_in = xml.dom.minidom.parse(self.file_name)
    self._file_type = "xml"
    self._file_object = xml_in

  def _try_as_img(self):
    from iotbx.detectors import ImageFactory
    img = ImageFactory(self.file_name)
    img.read()
    self._file_type = "img"
    self._file_object = img

  def _try_as_ncs(self):
    from mmtbx.ncs.ncs import ncs
    from libtbx.utils import null_out
    ncs_object=ncs()
    try: # see if we can read biomtr records
      pdb_inp=iotbx.pdb.input(file_name=self.file_name)
      ncs_object.ncs_from_pdb_input_BIOMT(pdb_inp=pdb_inp,log=null_out(),
        quiet=True)
    except Exception as e: # try as regular ncs object
      ncs_object.read_ncs(file_name=self.file_name,log=sys.stdout,quiet=True)
    assert ncs_object.max_operators() > 0
    self._file_object = ncs_object
    self._file_type = "ncs"

  def try_all_types(self):
    for filetype in self.valid_types :
      if (filetype in self._tried_types) : continue
      read_method = getattr(self, "_try_as_%s" % filetype)
      try :
        read_method()
      except KeyboardInterrupt :
        raise
      except Exception as e :
        if (str(e).startswith("Space group is incompatible") or
            str(e).startswith("The space group") ):
          raise
        self._errors[filetype] = str(e)
        self._file_type = None
        self._file_object = None
        continue
      else :
        if self._file_type is not None :
          break

  def crystal_symmetry(self):
    """
    Extract the crystal symmetry (if any).  Only valid for model (PDB/mmCIF)
    and reflection files.
    """
    from cctbx import crystal
    if(self._file_type == "pdb"):
      return self._file_object.input.crystal_symmetry()
    elif(self._file_type == "hkl"):
      try:
        return self._file_object.file_content().crystal_symmetry()
      except AttributeError:
        return self._file_object.as_miller_arrays()[0].crystal_symmetry()
    elif(self._file_type == "ccp4_map"):
      return self._file_object.crystal_symmetry()
    else:
      raise NotImplementedError()

  def file_info(self, show_file_size=True):
    """
    Format a string containing the file type and size.
    """
    file_size_str = ""
    if show_file_size :
      file_size = self.file_size
      if file_size > 10000000 :
        file_size_str = " (%.1f MB)" % (self.file_size / 1000000.0)
      elif file_size > 1000000 :
        file_size_str = " (%.2f MB)" % (self.file_size / 1000000.0)
      elif file_size > 100000 :
        file_size_str = " (%d KB)" % (self.file_size / 1000.0)
      elif file_size > 1000 :
        file_size_str = " (%.1f KB)" % (self.file_size / 1000.0)
      else :
        file_size_str = " (%d B)" % self.file_size
    if self._file_type == None :
      return "Unknown file%s" % file_size_str
    else :
      return "%s%s" % (self.__descriptions__[self._file_type],
        file_size_str)

  def assert_file_type(self, expected_type):
    """
    Verify that the automatically determined file type is the expected format.
    """
    if (expected_type is None):
      return self
    elif (self._file_type == expected_type):
      return self
    else :
      raise Sorry(("Expected file type '%s' for %s, got '%s'.  This is " +
        "almost certainly a bug; please contact the developers.") %
        (expected_type, str(self.file_name), str(self._file_type)))

  def check_file_type(self, expected_type=None, multiple_formats=()):
    """
    Verify that the automatically determined file type is the expected format,
    with the option to consider multiple formats.
    """
    if (expected_type is not None):
      if (self._file_type != expected_type):
        raise Sorry(("This file format ('%s') is not supported as input for "+
          "this field; only files of type '%s' are allowed.") % (
          standard_file_descriptions.get(self._file_type, "Unknown"),
          standard_file_descriptions.get(expected_type, "Unknown")))
    else :
      assert (len(multiple_formats) > 0)
      if (not self._file_type in multiple_formats):
        raise Sorry(("This file format ('%s') is not supported as input for "+
          "this field; only the following types are supported:\n  %s") % (
          standard_file_descriptions.get(self._file_type, "Unknown"),
          "\n  ".join([ standard_file_descriptions.get(f, "Unknown")
                        for f in multiple_formats ])))
    return self

  def show_summary(self, out=sys.stdout):
    """
    Print out some basic information about the file.
    """
    if (self._file_type is None):
      print("File type could not be determined.", file=out)
    else :
      print("File name: %s" % self.file_name, file=out)
      print("Format: %s (%s)" % (self._file_type,
        standard_file_descriptions.get(self._file_type, "unknown")), file=out)
    if (self._file_type == "pdb"):
      print("Atoms in file: %d" % (len(self._file_object.input.atoms())), file=out)
      title = "\n".join(self._file_object.input.title_section())
      if (title != ""):
        print("Title section:", file=out)
        print(title, file=out)
    elif (self._file_type == "hkl"):
      for array in self.file_server.miller_arrays :
        print("", file=out)
        array.show_comprehensive_summary(f=out)
    elif (self._file_type == "ccp4_map"):
      self._file_object.show_summary(out)

def any_file_fast(file_name,
              get_processed_file=False,
              valid_types=supported_file_types,
              allow_directories=False,
              force_type=None,
              input_class=None):
  """
  mimics any_file, but without parsing - will instead guess the file type from
  the extension.  for most output files produced by cctbx/phenix this is
  relatively safe; for files of unknown provenance it is less effective.
  """
  assert (not get_processed_file) and (force_type is None)
  if not os.path.exists(file_name):
    raise Sorry("Couldn't find the file %s" % file_name)
  elif os.path.isdir(file_name):
    if not allow_directories :
      raise Sorry("This application does not support folders as input.")
    else :
      return directory_input(file_name)
  elif not os.path.isfile(file_name):
    raise Sorry("%s is not a valid file.")
  else :
    if input_class is None :
      input_class = any_file_fast_input
    return input_class(file_name=file_name,
      valid_types=valid_types)

class any_file_fast_input(object):
  __extensions__ = standard_file_extensions
  __descriptions__ = standard_file_descriptions
  def __init__(self, file_name, valid_types):
    self.valid_types = valid_types
    self.file_name = file_name
    self.file_object = None
    self.file_type = None
    self.file_server = None
    self.file_description = None
    file_base, file_ext, compress_ext = splitext(file_name)
    for file_type in valid_types :
      if file_ext[1:] in self.__extensions__[file_type] :
        self.file_type = file_type
    if self.file_type is not None :
      self.file_description = self.__descriptions__[self.file_type]
    # XXX this is a huge hole in this method - for reflection files, the
    # PHENIX GUI displays the specific format, which requires actually reading
    # in the file.  the stub class below will work for obvious formats.
    if (self.file_type == "hkl"):
      class fake_data_object(object):
        def __init__(self, ext):
          self.ext = ext
        def file_type(self):
          if (self.ext == ".mtz") : return "CCP4 MTZ"
          elif (self.ext == ".sca") : return "Scalepack"
          else : return "Data (other)"
      self.file_object = fake_data_object(file_ext)

class directory_input(object):
  def __init__(self, dir_name):
    self.file_name = dir_name
    self.file_object = dircache.listdir(dir_name)
    self.file_server = None
    self.file_type = "dir"
    self.file_size = os.path.getsize(dir_name)

  def file_info(self, show_file_size=False):
    return "Folder"

class group_files(object):
  def __init__(self,
                file_names,
                template_format="pdb",
                group_by_directory=True):
    import iotbx.pdb
    self.file_names = file_names
    self.grouped_files = []
    self.ungrouped_files = []
    self.ambiguous_files = []
    templates = []
    other_files = []
    template_dirs = []
    for file_name in file_names :
      file_type = guess_file_type(file_name)
      if (file_type == template_format):
        base, ext, compress_ext = splitext(file_name)
        templates.append(base)
        template_dirs.append(os.path.dirname(file_name))
        self.grouped_files.append([file_name])
      else :
        other_files.append(file_name)
    if (len(templates) == 0):
      raise Sorry("Can't find any files of the expected format ('%s')." %
        template_format)
    if (len(set(templates)) != len(templates)):
      raise Sorry("Multiple files with identical root names.")
    for file_name in other_files :
      group_name = find_closest_base_name(
        file_name=file_name,
        base_name=splitext(file_name)[0],
        templates=templates)
      if (group_name == ""):
        self.ambiguous_files.append(file_name)
      elif (group_name is not None):
        i = templates.index(group_name)
        self.grouped_files[i].append(file_name)
      else :
        if group_by_directory :
          dir_name = os.path.dirname(file_name)
          group_name = find_closest_base_name(
            file_name=dir_name,
            base_name=dir_name,
            templates=template_dirs)
          if (group_name == ""):
            self.ambiguous_files.append(file_name)
          elif (group_name is not None):
            i = template_dirs.index(group_name)
            self.grouped_files[i].append(file_name)
          else :
            self.ungrouped_files.append(file_name)
        else :
          self.ungrouped_files.append(file_name)

def find_closest_base_name(file_name, base_name, templates):
  groups = []
  for base in templates :
    if file_name.startswith(base) or base.startswith(base_name):
      groups.append(base)
  if (len(groups) == 1):
#    print file_name, groups[0]
    return groups[0]
  elif (len(groups) > 1):
#    print file_name, groups
    prefix_len = [ os.path.commonprefix([g, file_name]) for g in groups ]
    max_common_prefix = max(prefix_len)
    if (prefix_len.count(max_common_prefix) > 1):
      return ""
    else :
      return groups[ prefix_len.index(max_common_prefix) ]
  return None

#---end


 *******************************************************************************


 *******************************************************************************
iotbx/format.py
"""Format crystal symmetry"""
from __future__ import absolute_import, division, print_function

def crystal_symmetry(cs):
  if (cs.unit_cell() is None):
    u = "None"
  else:
    u = "(%.6g, %.6g, %.6g, %.6g, %.6g, %.6g)" % cs.unit_cell().parameters()
  if (cs.space_group_info() is None):
    s = "None"
  else:
    s = "'%s'" % str(cs.space_group_info()).replace("'", "\\'")
  print("""\
crystal.symmetry(
  unit_cell=%s,
  space_group_symbol=%s)""" % (u, s))


 *******************************************************************************


 *******************************************************************************
iotbx/libtbx_refresh.py
from __future__ import absolute_import, division, print_function
import os
op = os.path

if (self.env.is_ready_for_build()):
  f = self.env.under_dist("iotbx", path="pdb/hybrid_36_f.f")
  from libtbx.path import tail_levels
  print("  Using fable to convert", tail_levels(f, 3))
  import fable.cout
  cpp_lines = fable.cout.process(file_names=[f], fem_do_safe=False)
  d = self.env.under_build("iotbx/pdb")
  if (not op.isdir(d)):
    os.makedirs(d)
  t = op.join(d, "hybrid_36_fem.cpp")
  print("    Writing:", tail_levels(t, 3))
  with open(t, "w") as fh:
    fh.write("\n".join(cpp_lines))


 *******************************************************************************


 *******************************************************************************
iotbx/logfiles.py
"""Routines to read denzo/scalepack log files
"""

from __future__ import absolute_import, division, print_function
import sys, os, re
from six.moves import cStringIO as StringIO
from six.moves import zip

def check_bin_format(bin):
  try :
    d_max, d_min = float(bin[0]), float(bin[1])
  except ValueError as e :
    raise RuntimeError("%s\nOffending values: %s, %s"%(str(e),bin[0],bin[1]))

def float_or_none(n):
  if n is None : return None
  else :         return float(n)

def percent_to_float(value):
  assert value.endswith("%")
  return float(re.sub(r"\%$", "", value))

class experiment_info(object):
  def extract_all_stats(self):
    return self

class integration_info(object):
  def __init__(self, program_name="NULL"):
    self.program_name = program_name
    self.wavelength = None
    self.distance = None
    self.twotheta = None

  def set_wavelength(self, wavelength):
    self.wavelength = float(wavelength)

  def set_distance(self, distance):
    self.distance = float(distance)

  def set_2theta(self, twotheta):
    self.twotheta = twotheta

  def extract_all_stats(self):
    return self

class scaling_info(object):
  def __init__(self, program_name="NULL"):
    self.program_name = program_name
    self.stats_overall = {}
    self.binned_stats = {}
    self.bins = None
    self.d_max = None
    self.d_min = None
    self.n_refl = None
    self.n_refl_all = None

  def set_bins(self, bins):
    for bin in bins :
      check_bin_format(bin)
    self.bins = bins
    if self.d_max is None :
      d_max = float(self.bins[0][0])
      d_min = float(self.bins[-1][1])
      self.set_d_max_min(d_max, d_min)

  def set_n_refl(self, n_refl, n_refl_all):
    self.n_refl = n_refl
    self.n_refl_all = n_refl_all

  def add_bin_stat(self, bin, stat_name, value):
    check_bin_format(bin)
    if stat_name in self.binned_stats :
      self.binned_stats[stat_name].append(value)
    else :
      self.binned_stats[stat_name] = [value]

  def add_overall_stat(self, stat_name, value):
    self.stats_overall[stat_name] = value

  def set_d_max_min(self, d_max, d_min):
    self.d_max = d_max
    self.d_min = d_min

  def extract_all_stats(self):
    from libtbx import group_args
    d_min = float(self.bins[-1][1])
    d_max = float(self.bins[0][0])
    comp_overall = self.stats_overall.get("completeness", None)
    mult_overall = self.stats_overall.get("multiplicity", None)
    rmerg_overall = self.stats_overall.get("r_merge", None)
    s2n_overall = self.stats_overall.get("i/sigma", None)
    return group_args(d_max_min=(d_max, d_min),
                      n_refl=self.n_refl,
                      n_refl_all=self.n_refl_all,
                      completeness=float_or_none(comp_overall),
                      multiplicity=float_or_none(mult_overall),
                      r_sym=float_or_none(rmerg_overall),
                      r_meas=None, # TODO?
                      i_over_sigma=float_or_none(s2n_overall))

  def extract_outer_shell_stats(self):
    from libtbx import group_args
    d_min = float(self.bins[-1][1])
    d_max = float(self.bins[-1][0])
    comp_bin = self.binned_stats.get("completeness", [None])[-1]
    mult_bin = self.binned_stats.get("multiplicity", [None])[-1]
    rmerg_bin = self.binned_stats.get("r_merge", [None])[-1]
    s2n_bin = self.binned_stats.get("i/sigma", [None])[-1]
    return group_args(d_max_min=(d_max, d_min),
                      n_refl=None, # FIXME
                      n_refl_all=None,
                      completeness=float_or_none(comp_bin),
                      multiplicity=float_or_none(mult_bin),
                      r_sym=float_or_none(rmerg_bin),
                      r_meas=None, # TODO?
                      i_over_sigma=float_or_none(s2n_bin))

class all_none(object):
  def __getattr__(self, name):
    return None

class empty_info(object):
  def extract_all_stats(self):
    return all_none()
  def extract_outer_shell_stats(self):
    return all_none()

class processing_info(object):
  def __init__(self, experiment, integration, scaling):
    self.experiment = experiment
    self.integration = integration
    self.scaling = scaling

  def get_experiment_info(self):
    if (self.experiment is not None):
      return self.experiment
    return all_none() #empty_info()

  def get_integration_info(self):
    if (self.integration is not None):
      return self.integration
    return all_none() #empty_info()

  def get_scaling_info(self):
    if (self.scaling is not None):
      return self.scaling
    return empty_info()

  def format_remark_200(self):
    from libtbx.str_utils import format_value
    from libtbx.test_utils import approx_equal
    def format(obj, attr, fs="%.4f"):
      value = getattr(obj, attr, None)
      return format_value(fs, value, replace_none_with="NULL").strip()
    e = None
    if self.experiment is not None :
      e = self.experiment.extract_all_stats()
    i = None
    if self.integration is not None :
      i = self.integration.extract_all_stats()
    s = None
    if self.scaling is not None :
      s = self.scaling.extract_all_stats()
    lines = []
    lines.append("")
    lines.append("EXPERIMENTAL DETAILS")
    lines.append(" EXPERIMENT TYPE                : X-RAY DIFFRACTION")
    lines.append(" DATE OF DATA COLLECTION        : NULL")
    lines.append(" TEMPERATURE           (KELVIN) : NULL")
    lines.append(" PH                             : NULL")
    lines.append(" NUMBER OF CRYSTALS USED        : NULL")
    lines.append("")
    # TODO
    wavelength = getattr(e, "wavelength", None)
    if (wavelength is None):
      wavelength = getattr(i, "wavelength", None)
    synchrotron = "NULL"
    if (wavelength is not None):
      out = StringIO()
      if (not approx_equal(wavelength, 1.5418, eps=0.01, out=out) and
          not approx_equal(wavelength, 0.7107, eps=0.01, out=out)):
        synchrotron = "Y"
      else :
        synchrotron = "N"
      wl = "%.4f" % wavelength
    else :
      wl = "NULL"
    lines.append(" SYNCHROTRON              (Y/N) : %s" % synchrotron)
    lines.append(" RADIATION SOURCE               : NULL")
    lines.append(" BEAMLINE                       : NULL")
    lines.append(" X-RAY GENERATOR MODEL          : NULL")
    lines.append(" MONOCHROMATIC OR LAUE    (M/L) : M")
    lines.append(" WAVELENGTH OR RANGE        (A) : %s" % wl)
    lines.append(" MONOCHROMATOR                  : NULL")
    lines.append(" OPTICS                         : NULL")
    lines.append("")
    int_software = getattr(self.integration, "program_name", "NULL")
    lines.append(" DETECTOR TYPE                  : NULL")
    lines.append(" DETECTOR MANUFACTURER          : NULL")
    lines.append(" INTENSITY-INTEGRATION SOFTWARE : %s" % int_software)
    scale_software = getattr(self.scaling, "program_name", "NULL")
    lines.append(" DATA SCALING SOFTWARE          : %s" % scale_software)
    lines.append("")
    lines.append("OVERALL.")
    comp_overall = format(s, "completeness", "%.1f")
    mult_overall = format(s, "multiplicity", "%.1f")
    rmerg_overall = format(s, "r_sym", "%.5f")
    s2n_overall = format(s, "i_over_sigma", "%.4f")
    lines.append(" COMPLETENESS FOR RANGE     (%%) : %s" % comp_overall)
    lines.append(" DATA REDUNDANCY                : %s" % mult_overall)
    lines.append(" R MERGE                    (I) : %s" % rmerg_overall)
    lines.append(" R SYM                      (I) : NULL")
    lines.append(" <I/SIGMA(I)> FOR THE DATA SET  : %s" % s2n_overall)
    lines.append("")
    lines.append("IN THE HIGHEST RESOLUTION SHELL.")
    shell = None
    if self.scaling is not None :
      shell = self.scaling.extract_outer_shell_stats()
    (_d_max, _d_min) = getattr(shell, "d_max_min", (None, None))
    d_max = format_value("%.2f", _d_max, replace_none_with="NULL").strip()
    d_min = format_value("%.2f", _d_min, replace_none_with="NULL").strip()
    comp_lastbin = format(shell, "completeness", "%.1f")
    mult_lastbin = format(shell, "multiplicity", "%.1f")
    rmerg_lastbin = format(shell, "r_sym", "%.5f")
    s2n_lastbin = format(shell, "i_over_sigma", "%.4f")
    lines.append(" HIGHEST RESOLUTION SHELL, RANGE HIGH (A) : %s" % d_min)
    lines.append(" HIGHEST RESOLUTION SHELL, RANGE LOW  (A) : %s" % d_max)
    lines.append(" COMPLETENESS FOR SHELL     (%%) : %s" % comp_lastbin)
    lines.append(" DATA REDUNDANCY IN SHELL       : %s" % mult_lastbin)
    lines.append(" R MERGE FOR SHELL          (I) : %s" % rmerg_lastbin)
    lines.append(" R SYM FOR SHELL            (I) : NULL")
    lines.append(" <I/SIGMA(I)> FOR SHELL         : %s" % s2n_lastbin)
    lines.append("")
    remark_lines = [ "REMARK 200 %s" % line for line in lines ]
    return "\n".join(remark_lines)

#-----------------------------------------------------------------------
# PARSERS
#
def parse_denzo(lines):
  info = integration_info("HKL-2000")
  for i, line in enumerate(lines):
    if line.strip().startswith("Wavelength "):
      fields = line.strip().split()
      for field in fields :
        try :
          wavelength = float(field)
        except ValueError :
          pass
        else :
          info.set_wavelength(wavelength)
          break
    elif line.strip().startswith("Detector to crystal distance"):
      fields = line.strip().split()
      info.set_distance(float(fields[4]))
  return info

def parse_mosflm(lines):
  info = integration_info("MOSFLM")
  for i, line in enumerate(lines):
    line = line.strip()
    if line.startswith("Beam Parameters"):
      j = i
      while (j < len(lines)):
        line = lines[j].strip()
        if line.startswith("Wavelength"):
          wavelength = float(line.split()[1])
          info.set_wavelength(wavelength)
          break
        j += 1
    elif line.startswith("Detector Parameters"):
      j = i
      while (j < (i + 100)):
        line = lines[j].strip()
        if line.startswith("Crystal to detector distance"):
          fields = line.split()
          distance = float(fields[-2])
          info.set_distance(distance)
        elif line.startswith("Detector swing angle"):
          fields = line.split()
          twotheta = float(fields[-2])
          info.set_twotheta(twotheta)
        j += 1
      break
  return info

def parse_scalepack(lines):
  n_lines = len(lines)
  mode = 0
  info = scaling_info("SCALA")
  def is_table_end(fields):
    return (fields[0] == "All" and fields[1] == "hkl")
  n_refl_all = None
  n_refl = None
  for i, line in enumerate(lines):
    if ("intensities and R-factors by batch number" in line):
      j = i + 3
      while j < n_lines :
        line2 = lines[j].strip()
        if line2.startswith("All films"):
          n_refl_all = int(line2.split()[2])
          break
        j+= 1
    elif "Summary of observation redundancies by shells" in line :
      bins = []
      j = i + 3
      while (j < (i+100)):
        line2 = lines[j]
        fields = line2.strip().split()
        if is_table_end(fields):
          n_refl = int(fields[-1])
          info.set_n_refl(n_refl, n_refl_all)
          break
        else :
          bin_d_max_min = (fields[0], fields[1])
          bins.append(bin_d_max_min)
        j += 1
      assert (len(bins) > 0)
      info.set_bins(bins)
    elif "Average Redundancy Per Shell" in line :
      j = i + 3
      while (j < (i+100)):
        line2 = lines[j]
        fields = line2.strip().split()
        if is_table_end(fields):
          info.add_overall_stat("multiplicity", fields[-1])
          break
        else :
          bin = (fields[0], fields[1])
          info.add_bin_stat(bin, "multiplicity", fields[-1])
        j += 1
    elif "I/Sigma in resolution shells:" in line :
      j = i + 3
      while (j < (i+100)):
        line2 = lines[j]
        fields = line2.strip().split()
        if is_table_end(fields):
          info.add_overall_stat("completeness", fields[-1])
          break
        else :
          bin = (fields[0], fields[1])
          info.add_bin_stat(bin, "completeness", fields[-1])
        j += 1
    elif "Summary of reflections intensities and R-factors by shells" in line :
      j = i
      while (j < (i+100)):
        line2 = lines[j]
        fields = line2.strip().split()
        j += 1
        if (len(fields) > 0 and fields[0] == "limit"):
          break
      while (j < (i+200)):
        line2 = lines[j]
        fields = line2.strip().split()
        i_mean = float(fields[2])
        sig_i_mean = float(fields[3])
        r_merge = fields[-2] # XXX -1 (linear) or -2 (square) ???
        if (fields[0] == "All" and fields[1] == "reflections"):
          info.add_overall_stat("i/sigma", "%.2f" % (i_mean / sig_i_mean))
          info.add_overall_stat("r_merge", r_merge)
          break
        else :
          bin = (fields[0], fields[1])
          info.add_bin_stat(bin, "i/sigma", "%.2f" % (i_mean / sig_i_mean))
          info.add_bin_stat(bin, "r_merge", r_merge)
        j += 1
  return info

def parse_scala(lines):
  from iotbx import data_plots
  info = scaling_info("SCALA")
  tables = data_plots.import_ccp4i_logfile(log_lines=lines)
  d_max = None
  for i, line in enumerate(lines):
    if ("Summary data for " in line):
      if (lines[i+1].startswith("</p>")) or ("<br" in line):
        continue
      j = i
      n_refl = None
      n_refl_all = None
      while (j < len(lines)):
        line = lines[j].strip()
        if line.startswith("Low resolution limit"):
          d_max = float(line.split()[3])
        elif line.startswith("Rmerge") and (not "bin" in line):
          info.add_overall_stat("r_merge", float(line.split()[1]))
        elif line.startswith("Total number of observations"):
          n_refl_all = float(line.split()[4])
        elif line.startswith("Total number unique"):
          n_refl = float(line.split()[3])
          info.set_n_refl(n_refl, n_refl_all)
        elif (line.startswith("Mean((I)/sd(I))") or
              line.startswith("Mean(I)/sd(I)")):
          info.add_overall_stat("i/sigma", float(line.split()[1]))
        elif line.startswith("Completeness"):
          info.add_overall_stat("completeness", float(line.split()[1]))
        elif line.startswith("Multiplicity"):
          info.add_overall_stat("multiplicity", float(line.split()[1]))
        elif ("Outlier rejection" in line) or ("$$" in line):
          break
        j += 1
  assert (d_max is not None)
  for table in tables :
    if table.title.startswith("Analysis against resolution"):
      d_min_by_bin = table.get_column_by_label("Dmin(A)")
      bin_d_max = d_max
      bins = []
      for bin_d_min in d_min_by_bin :
        bins.append((bin_d_max, bin_d_min))
        bin_d_max = bin_d_min
      info.set_bins(bins)
      rmerge = table.get_column_by_label("Rmrg")
      for (rmerge_bin, bin) in zip(rmerge, bins):
        info.add_bin_stat(bin, "r_merge", rmerge_bin)
      try :
        s2n = table.get_column_by_label("Mn(I/sd)")
      except Exception :
        s2n = table.get_column_by_label("Mn(I)/sd")
      for (s2n_bin, bin) in zip(s2n, bins):
        info.add_bin_stat(bin, "i/sigma", s2n_bin)
    elif table.title.startswith("Completeness, multiplicity, Rmeas"):
      completeness = table.get_column_by_label("%poss")
      for (comp_bin, bin) in zip(completeness, bins):
        info.add_bin_stat(bin, "completeness", comp_bin)
      multiplicity = table.get_column_by_label("Mlplct")
      for (mult_bin, bin) in zip(multiplicity, bins):
        info.add_bin_stat(bin, "multiplicity", mult_bin)
      break
  return info

def parse_xds(lines):
  info = integration_info("XDS")
  for i, line in enumerate(lines):
    line = line.strip()
    if line.startswith("X-RAY_WAVELENGTH"):
      fields = line.split("=")[1].strip().split()
      info.set_wavelength(float(fields[0]))
      break
  return info

def parse_xscale(lines):
  info = scaling_info("XSCALE")
  d_max = 0.0
  d_min = 999.99
  for i, line in enumerate(lines):
    line = line.strip()
    if (line.startswith("INPUT_FILE=")):
      fields = line.split()
      if (len(fields) < 4):
        continue
      try :
        (d_max_file, d_min_file) = (float(fields[-2]), float(fields[-1]))
      except ValueError :
        pass
      except Exception as e :
        print(line)
        raise
      else :
        if (d_max_file > d_max):
          d_max = d_max_file
        if (d_min_file < d_min):
          d_min = d_min_file
        info.set_d_max_min(d_max, d_min)
    elif line.startswith("SUBSET OF INTENSITY DATA WITH SIGNAL/NOISE >= -3.0"):
      j = i+3
      bins = []
      overall = []
      while (j < len(lines)):
        line = lines[j].strip()
        fields = line.split()
        if (len(fields) < 12):
          pass
        elif (fields[0] == "total"):
          overall = fields
          break
        elif re.match(r"^\d", line):
          bins.append(fields)
        j += 1
      assert (len(bins) > 0) and (len(overall) == len(bins[0]))
      n_refl_all = int(overall[1])
      n_refl = int(overall[2])
      info.set_n_refl(n_refl, n_refl_all)
      info.add_overall_stat("completeness", percent_to_float(overall[4]))
      info.add_overall_stat("multiplicity", n_refl_all / n_refl)
      info.add_overall_stat("r_merge", percent_to_float(overall[5]) * 0.01)
      info.add_overall_stat("i/sigma", float(overall[8]))
      bins_d_max_min = []
      for bin in bins :
        d_min = float(bin[0])
        bins_d_max_min.append((d_max, d_min))
        d_max = d_min
      info.set_bins(bins_d_max_min)
      for fields, bin  in zip(bins, bins_d_max_min):
        bin_n_refl_all = int(fields[1])
        bin_n_refl = int(fields[2])
        info.add_bin_stat(bin, "completeness", percent_to_float(fields[4]))
        info.add_bin_stat(bin, "multiplicity", bin_n_refl_all / bin_n_refl)
        info.add_bin_stat(bin, "r_merge", percent_to_float(fields[5]) * 0.01)
        info.add_bin_stat(bin, "i/sigma", float(fields[8]))
      break
  return info

def parse_all_files(args):
  experiment = None
  integration = None
  scaling = None
  for arg in args :
    if os.path.isfile(arg):
      lines = open(arg, "r").readlines()
      for line in lines :
        if "reading from a file" in line :
          scaling = parse_scalepack(lines)
          break
        elif "Oscillation starts at" in line :
          integration = parse_denzo(lines)
          break
        elif "SCALA - continuous scaling program" in line :
          scaling = parse_scala(lines)
          break
        elif "A.G.W. Leslie" in line :
          integration = parse_mosflm(lines)
          break
        elif "XSCALE (VERSION" in line :
          scaling = parse_xscale(lines)
          break
        elif "***** INTEGRATE *****" in line :
          integration = parse_xds(lines)
          break
  info = processing_info(experiment=experiment,
    integration=integration,
    scaling=scaling)
  return info

def exercise():
  import libtbx.load_env
  denzo_log = libtbx.env.find_in_repositories(
    relative_path="phenix_regression/tracking/denzo.log",
    test=os.path.isfile)
  scalepack_log = libtbx.env.find_in_repositories(
    relative_path="phenix_regression/tracking/scalepack.log",
    test=os.path.isfile)
  if (denzo_log is None):
    print("DENZO log not found, skipping test.")
    return False
  info = parse_all_files([denzo_log, scalepack_log])
  output = info.format_remark_200().splitlines()
  assert ("\n".join([ line.strip() for line in output[-16:]]) == """\
REMARK 200 OVERALL.
REMARK 200  COMPLETENESS FOR RANGE     (%) : 88.4
REMARK 200  DATA REDUNDANCY                : 3.5
REMARK 200  R MERGE                    (I) : 0.09600
REMARK 200  R SYM                      (I) : NULL
REMARK 200  <I/SIGMA(I)> FOR THE DATA SET  : 11.5700
REMARK 200
REMARK 200 IN THE HIGHEST RESOLUTION SHELL.
REMARK 200  HIGHEST RESOLUTION SHELL, RANGE HIGH (A) : 1.98
REMARK 200  HIGHEST RESOLUTION SHELL, RANGE LOW  (A) : 2.05
REMARK 200  COMPLETENESS FOR SHELL     (%) : 34.1
REMARK 200  DATA REDUNDANCY IN SHELL       : 1.5
REMARK 200  R MERGE FOR SHELL          (I) : 0.93400
REMARK 200  R SYM FOR SHELL            (I) : NULL
REMARK 200  <I/SIGMA(I)> FOR SHELL         : 0.6000
REMARK 200""")

if __name__ == "__main__" :
  #info = parse_all_files(sys.argv[1:])
  #print info.format_remark_200()
  exercise()
  print("OK")


 *******************************************************************************


 *******************************************************************************
iotbx/map_manager.py
"""
High-level manager for reading and writing 3D maps and functions to operate on maps.  This is the class to use for most map operations.
"""
from __future__ import absolute_import, division, print_function
from libtbx.utils import to_str, null_out, Sorry
from libtbx import group_args, Auto
from libtbx.test_utils import approx_equal
import sys
import io
from cctbx import miller
from iotbx.mrcfile import map_reader, write_ccp4_map
from scitbx.array_family import flex
from cctbx import maptbx
from cctbx import miller
import mmtbx.ncs.ncs
from copy import deepcopy
from scitbx.matrix import col


class map_manager(map_reader, write_ccp4_map):
  '''
   map_manager, includes map_reader and write_ccp4_map and functions to
   operate on a 3D map object.

   This class is intended to be the principal mechanism for reading
   and writing map information.  It is intended to be used by the
   iotbx.data_manager for both of these purposes.

   Use map_manager to read, write, and carry information about
   one map.  Map_manager keeps track of the origin shifts and also the
   original full unit cell and cell dimensions.  It writes out the map
   in the same place as it was read in.

   Note on wrapping:  Wrapping means that the map value outside the map
   boundaries can be obtained as the value inside the boundaries, (translated
   by some multiple of the unit cell translations.)  Normally crystallographic
   maps can be wrapped and cryo EM maps cannot.

   Wrapping should be specified on initialization if not read from a file. If
   read from a file, the value from the file labels is used if available,
   otherwise it is assumed to be wrapping = False unless specified (normal
   for a cryo-EM map. If not specified at all, it will need to be specified
   before a map_model_manager is created or the map_manager is written out.

   Map_manager also keeps track of any changes in magnification. These
   are reflected in changes in unit_cell and crystal_symmetry cell dimensions
   and angles.

   You normally create a new map_manager by initializing map_manager with a
   file name.  Then you apply the shift_origin() method and the map is
   shifted to place the origin at (0, 0, 0) and the original origin is
   recorded as self.origin_shift_grid_units.

   NOTE: do not set self.origin_shift_grid_units directly; instead use
   the method set_original_origin_and_gridding() or supply the value when
   initializing the map_manager.

   You can also create a map_manager with a map_data object (3D flex.double()
   array) along with the meta-data below.

   NOTE: MRC Maps may not represent the entire unit cell.  Normally maps that
    have an origin (corner with minimum i, j, k) that is not zero will be
    shifted at a later stage to have the origin at (0, 0, 0), along with
    any models and ncs objects (typically done with iotbx.map_and_model).
    To be able to write out a map in the same place as it was read in
    after shifting the origin and/or boxing the map, you need to keep track
    of 3 things.  These are:
    1. unit_cell_grid: grid representing one full unit cell as read in.
        Saved in map_manager as self.unit_cell_grid
    2. unit_cell_crystal_symmetry: dimensions and space group of full unit cell
        Saved in map_manager as self._unit_cell_crystal_symmetry
    3. origin_shift_grid_units: the shift in grid units to apply to the
       working map to superimpose it on the original map. When you read the
       map in this is (0, 0, 0). If you shift the map origin from (i, j, k) to
       (0, 0, 0) then the origin_shift_grid_units is (i, j, k).
         Saved in map_manager as self.origin_shift_grid_units

   Magnification (pixel size scaling) of a map: there is no general parameter
   describing magnification of an MRC map.  Changes in scaling are
   recorded in map_manager as changes in the scaling matrix/translation that
   relates grid points in a map to real-space position.

   Normal usage (NOTE: read/write should normally be done through data_manager):

     Read in a map:
       mm = map_manager('input_map.mrc')
     Summarize:
       mm.show_summary()

     Normally shift origin of map to (0, 0, 0) (you can do this here
         or you can use iotbx.map_and_model to shift models and maps together):
       mm.shift_origin()

     Get the map_data (shifted if origin was shifted above):
       map_data = mm.map_data()

     Get the crystal_symmetry of the box of data that is present:
       cs = mm.crystal_symmetry()

     Get the crystal_symmetry of the whole unit cell (even if not present):
       unit_cell_cs = mm.unit_cell_crystal_symmetry()

     Write out the map in map_data() in original location:
       mm.write_map(file_name = 'output_map.ccp4')

   CONVENTIONS
   See http://www.ccpem.ac.uk/mrc_format/mrc2014.php for MRC format
   See https://pypi.org/project/mrcfile/ for mrcfile library documentation

   Same conventions as iotbx.ccp4_map

   Default is to write maps with INTERNAL_STANDARD_ORDER of axes of [3, 2, 1]
     corresponding to columns in Z, rows in Y, sections in X to match
     flex array layout.  This can be modified by changing the values in
     output_axis_order.

   Hard-wired to convert input maps of any order to
     INTERNAL_STANDARD_ORDER = [3, 2, 1] before conversion to flex arrays
     This is not modifiable.

    INTERNAL_STANDARD_ORDER = [3, 2, 1]

  Standard limitations and associated message.
  These can be checked with: limitations = mrc.get_limitations()
    which returns a group_args object with a list of limitations and a list
    of corresponding error messages, or None if there are none
    see phenix.show_map_info for example
  These limitations can also be accessed with specific calls placed below:
   For example:
   mrc.can_be_sharpened()  returns False if "extract_unique" is present

  Map labels that are not limitations can be accessed with:
      additional_labels = mrc.get_additional_labels()

  STANDARD_LIMITATIONS_DICT = {
    "extract_unique":
     "This map is masked around unique region and not suitable for auto-sharpening.",
    "map_is_sharpened":
     "This map is auto-sharpened and not suitable for further auto-sharpening.",
    "map_is_density_modified": "This map has been density modified.",
     }


   NOTES ON ORDER OF AXES

    Phenix standard order is 3, 2, 1 (columns Z, rows Y, sections in X).
        Convert everything to this order.

    This is the order that allows direct conversion of a numpy 3D array
     with axis order (mapc, mapr, maps) to a flex array.

    For reverse = True, supply order that converts flex array to numpy 3D array
     with order (mapc, mapr, maps)

    Note that this does not mean input or output maps have to be in this order.
     It just means that before conversion of numpy to flex or vice-versa
     the array has to be in this order.

     Note that MRC standard order for input/ouput is 1, 2, 3.

     NOTE: numpy arrays indexed from 0 so this is equivalent to
      order of 2, 1, 0 in the numpy array

    NOTE:  MRC format allows data axes to be swapped using the header
      mapc mapr and maps fields. However the mrcfile library does not
      attempt to swap the axes and assigns the columns to X, rows to Y and
      sections to Z. The data array is indexed C-style, so data values can
      be accessed using mrc.data[z][y][x].

    NOTE: normal expectation is that phenix will read/write with the
      order 3, 2, 1. This means X-sections (index = 3), Y rows (index = 2),
      Z columns (index = 1). This correxponds to
       mapc (columns) =   3 or Z
       mapr (rows)    =   2 or Y
       maps (sections) =  1 or X

    In the numpy array (2, 1, 0 instead of 3, 2, 1):

    To transpose, specify i0, i1, i2 where:
        i0 = 2 means input axis 0 becomes output axis 2
        NOTE:  axes are 0, 1, 2 etc, not 1, 2, 3
      Examples:
        np.transpose(a, (0, 1, 2))  does nothing
        np.transpose(a, (1, 2, 0)) output axis 0 is input axis 1



    We want output axes to always be 2, 1, 0 and input axes for numpy array are
      (mapc-1, mapr-1, maps-1):

    For example, in typical phenix usage, the transposition is:
      i_mapc = 3    i_mapc_np = 2
      i_mapr = 2    i_mapr_np = 1
      i_maps = 1    i_maps_np = 0

   END CONVENTIONS

  '''


  def __init__(self,
     file_name = None,  # USUAL: Initialize from file and specify wrapping
     map_data = None,   # OR map_data, unit_cell_grid, unit_cell_crystal_symmetry
     unit_cell_grid = None,
     unit_cell_crystal_symmetry = None,
     origin_shift_grid_units = None, # OPTIONAL first point in map in full cell
     ncs_object = None, # OPTIONAL ncs_object with map symmetry
     wrapping = Auto,   # OPTIONAL but recommended if not read from file
     experiment_type = Auto,   # OPTIONAL can set later also
     scattering_table = Auto,   # OPTIONAL can set later also
     resolution = Auto,   # OPTIONAL can set later also
     log = None,
     ):

    '''
      Allows reading a map file or initialization with map_data

      Normally call with file_name to read map file in CCP4/MRC format.

      Alternative is initialize with map_data and metadata
       Required: specify map_data, unit_cell_grid, unit_cell_crystal_symmetry
       Optional: specify origin_shift_grid_units

      Optional in either case: supply
        ncs_object with map symmetry of full map
        wrapping (True if map repeats infinitely with repeat unit of unit cell)
        experiment_type (xray cryo_em neutron)
        scattering_table (electron n_gaussian wk1995 it1992 neutron)
        resolution (nominal resolution of map)

     Note on external origin: If input map had ORIGIN specified,
       so that the value of self.external_origin is not (0,0,0) and not None,
       and apply_external_origin_if_present is set, and shift_origin() is
       called, then:
       determine if self.external_origin is on a grid point and if so, convert
        and use negative of it as origin .
       NOTE: apply_external_origin_if_present will be ignored if
       origin_shift_grid_units is also set and is not (0,0,0).

      NOTE on "crystal_symmetry" objects
      There are two objects that are "crystal_symmetry" objects:
      A.  unit_cell_crystal_symmetry():  This is the symmetry of the
        entire unit cell. It can be any space group. The dimensions
        correspond to the dimensions of unit_cell_grid.

      B.  crystal_symmetry():  This is the symmetry of the part of the map
        that is present.  If the entire map is present, this can be any
        space group. Otherwise it is set to P 1 (no symmetry other than unity).
        The dimensions correspond to the dimensions of the map_data.all().

      NOTE: As of 2020-05-22 both map_reader and map_manager ALWAYS convert
      map_data to flex.double.

      Map_manager does not save any extra information about
      the map except the details specified in this __init__.

      After reading you can access map data with self.map_data()
        and other attributes (see class utils in ccp4_map/__init__py)
    '''

    assert (file_name is not None) or [map_data,unit_cell_grid,
        unit_cell_crystal_symmetry].count(None)==0

    assert (ncs_object is None) or isinstance(ncs_object, mmtbx.ncs.ncs.ncs)
    assert (wrapping is Auto) or isinstance(wrapping, bool)

    if origin_shift_grid_units is not None:
      origin_shift_grid_units = tuple(origin_shift_grid_units)
      assert len(origin_shift_grid_units) ==3
    else:
      origin_shift_grid_units = (0, 0, 0)

    # Initialize log filestream
    self.set_log(log)


    # NOTE: If you add anything here to be initialized, add it to the
    #  customized_copy method

    # Initialize that we don't have crystal_symmetry:
    self._crystal_symmetry = None

    # Initialize mask to be not present
    self._created_mask = None

    # Initialize that this is not a mask
    self._is_mask = False

    # Initialize that this is not a dummy map_manager
    self._is_dummy_map_manager = False

    # Initialize that there is no output_external_origin
    self.output_external_origin = None

    # Initialize program_name, limitations, labels
    self.file_name = file_name # input file (source of this manager)
    self.program_name = None  # Name of program using this manager
    self.limitations = None  # Limitations from STANDARD_LIMITATIONS_DICT
    self.labels = None  # List of labels (usually from input file) to be written

    # Initialize wrapping
    self._wrapping = None
    self._cannot_figure_out_wrapping = None

    # Initialize ncs_object
    self._ncs_object = ncs_object


    # Initialize origin shift representing position of original origin in
    #  grid units.  If map is shifted, this is updated to reflect where
    #  to place current origin to superimpose map on original map.

    # Usual initialization with a file

    if self.file_name is not None:
      self._read_map()
      # Sets self.unit_cell_grid, self._unit_cell_crystal_symmetry, self.data,
      #  self._crystal_symmetry.  Sets also self.external_origin

      # read_map does not set self.origin_shift_grid_units. Set them here:

      # Set starting values:
      self.origin_shift_grid_units = (0, 0, 0)

      # Assume this map is not wrapped unless wrapping is set or is obvious
      if isinstance(wrapping, bool):  # Take it...
        self._wrapping = wrapping
      elif self.wrapping_from_input_file() is not None:
        self._wrapping = self.wrapping_from_input_file()
      elif self.crystal_symmetry().space_group_number() > 1 and \
         self.is_full_size():  # crystal structure and full size
        self._wrapping = True
      else:
        self._wrapping = False

    else:
      '''
         Initialization with map_data object and metadata
      '''

      assert map_data and unit_cell_grid and unit_cell_crystal_symmetry
      # wrapping must be specified

      assert wrapping in [True, False]

      # Required initialization information:
      self.data = map_data
      self.unit_cell_grid = unit_cell_grid
      self._unit_cell_crystal_symmetry = unit_cell_crystal_symmetry
      self._wrapping = wrapping
      self.external_origin = (0, 0, 0)

      # Calculate values for self._crystal_symmetry
      # Must always run this method after changing
      #    self._unit_cell_crystal_symmetry  or self.unit_cell_grid
      self.set_crystal_symmetry_of_partial_map()

      # Optional initialization information
      self.origin_shift_grid_units = origin_shift_grid_units

    # Initialization steps always done:

    # Make sure map is full size if wrapping is set
    if self._wrapping:
      assert self.is_full_size()

    # make sure labels are strings
    if self.labels is not None:
      self.labels = [to_str(label, codec = 'utf8') for label in self.labels]

    # Initialize experiment type and scattering_table and set defaults
    self._experiment_type = experiment_type
    self._scattering_table = scattering_table
    self._resolution = resolution
    self._minimum_resolution = None
    self._set_up_experiment_type_and_scattering_table_and_resolution()


  # prevent pickling error in Python 3 with self.log = sys.stdout
  # unpickling is limited to restoring sys.stdout
  def __getstate__(self):
    pickle_dict = self.__dict__.copy()
    if isinstance(self.log, io.TextIOWrapper):
      pickle_dict['log'] = None
    return pickle_dict

  def __setstate__(self, pickle_dict):
    self.__dict__ = pickle_dict
    if not hasattr(self, 'log') or self.log is None:
      self.log = sys.stdout

  def __repr__(self):
    if self.is_dummy_map_manager():
      return "Dummy map_manager"
    text = "Map manager (from %s)" %(self.file_name)+\
        "\n%s, \nUnit-cell grid: %s, (present: %s), origin shift %s " %(
      str(self.unit_cell_crystal_symmetry()).replace("\n"," "),
      str(self.unit_cell_grid),
      str(self.map_data().all()),
      str(self.origin_shift_grid_units)) + "\n"+\
      "Working coordinate shift %s" %( str(self.shift_cart()))
    if self._ncs_object:
      text += "\n%s" %str(self._ncs_object)
    return text


  def set_log(self, log = sys.stdout):
    '''
       Set output log file
    '''
    if log is None:
      self.log = null_out()
    else:
      self.log = log

  def _read_map(self):
      '''
       Read map using mrcfile/__init__.py
       Sets self.unit_cell_grid, self._unit_cell_crystal_symmetry, self.data
           self._crystal_symmetry
       Does not set self.origin_shift_grid_units
       Does set self.file_name
      '''
      self._print("Reading map from %s " %(self.file_name))

      self.read_map_file(file_name = self.file_name)  # mrcfile/__init__.py

  def _print(self, m):
    if (self.log is not None) and hasattr(self.log, 'closed') and (
        not self.log.closed):
      print(m, file = self.log)

  def set_unit_cell_crystal_symmetry(self, unit_cell_crystal_symmetry):
    '''
      Specify the dimensions and space group of the full unit cell for this
      map.  This also changes the crystal_symmetry of the part that is
      present and the grid spacing. Also resets crystal_symmetry of ncs object

      Purpose is to redefine the dimensions of the map without changing values
      of the map.

      Can be used to correct the dimensions of a map where something was
      defined incorrectly.

      Can also be used to redefine the dimensions of a map after re-estimating
      the magnification of the map.

      **********************************************************************
      NOTE: Be careful using this function if the map is boxed.  If a map is
      boxed then it has a unit_cell_crystal_symmetry which describes the
      symmetry and cell dimensions of the original (full) map, and it also
      has a (different) crystal_symmetry that describes the symmetry and
      cell dimensions of the current boxed version of the map.

      This function operates on the current map to change its
      unit_cell_crystal_symmetry to the value that is supplied. This
      automatically also changes the crystal_symmetry.

      If the map is boxed, you need to supply the new value of the
      unit_cell_crystal_symmetry, NOT the new value of crystal_symmetry.
      **********************************************************************


      Does not change self.unit_cell_grid or self.origin_shift_grid_units

      Does change the result of self.shift_cart(), which is based on
        self.origin_shift_grid_units and self.unit_cell_grid
        and self.crystal_symmetry

       Fundamental parameters set:
        self._unit_cell_crystal_symmetry: dimensions of full unit cell
        self._crystal_symmetry: dimensions of part of unit cell that is present
    '''

    from cctbx import crystal
    assert isinstance(unit_cell_crystal_symmetry, crystal.symmetry)
    self._unit_cell_crystal_symmetry = unit_cell_crystal_symmetry

    # Always follow a set of _unit_cell_crystal_symmetry with this:
    self.set_crystal_symmetry_of_partial_map()

    # Now apply crystal symmetry and new shift cart to ncs object if any
    if self._ncs_object:
      self._ncs_object = self.shift_ncs_object_to_match_map_and_return_new_ncs_object(self._ncs_object)

  def set_output_external_origin(self, value):
    '''Set the value of the output external origin'''
    assert isinstance(value, tuple) or isinstance(value,list)
    self.output_external_origin = tuple(value)


  def set_original_origin_and_gridding(self,
      original_origin = None,
      gridding = None):
    '''
       Specify the location in the full unit cell grid where the origin of
       the map that is present is to be placed to match its original position.
       This is referred to here as the "original" origin, as opposed to the
       current origin of this map.

       Note that this method does not actually shift the origin of the working
       map.  It just defines where that origin is going to be placed when
       restoring the map to its original position.

       Also optionally redefine the definition of the unit cell, keeping the
       grid spacing the same.

       This allows redefining the location of the map that is present
       within the full unit cell.  It also allows redefining the
       unit cell itself.  Only use this to create a new partial map
       in a defined location.

       Previous definition of the location of the map that is present
       is discarded.

       Fundamental parameters set:
        self.origin_shift_grid_units: shift to place origin in original location
        self._unit_cell_crystal_symmetry: dimensions of full unit cell
        self.unit_cell_grid: grid units of full unit cell

       At end, recheck wrapping
    '''
    if original_origin:
      if (self.origin_shift_grid_units !=  (0, 0, 0)) or (
          not self.origin_is_zero()):
        self.shift_origin()
        self._print("Previous origin shift of %s will be discarded" %(
          str(self.origin_shift_grid_units)))

      # Set the origin
      self.origin_shift_grid_units = original_origin
      self._print("New origin shift will be %s " %(
        str(self.origin_shift_grid_units)))

    if gridding: # reset definition of full unit cell.  Keep grid spacing

       # If gridding does not match original, set space group always to P1

       current_unit_cell_parameters = self.unit_cell_crystal_symmetry(
            ).unit_cell().parameters()
       current_unit_cell_grid = self.unit_cell_grid
       new_unit_cell_parameters = []
       for a, g, new_g in zip(
          current_unit_cell_parameters[:3], current_unit_cell_grid, gridding):
         new_a = a*new_g/g
         new_unit_cell_parameters.append(new_a)

       unit_cell_parameters = \
          new_unit_cell_parameters+list(current_unit_cell_parameters[3:])

       if current_unit_cell_grid !=  gridding:
         space_group_number_use = 1
       else:
         space_group_number_use = \
            self._unit_cell_crystal_symmetry.space_group_number()
       from cctbx import crystal
       self._unit_cell_crystal_symmetry = crystal.symmetry(
          unit_cell_parameters, space_group_number_use)

       self.unit_cell_grid = gridding
       if current_unit_cell_grid !=  gridding:
         self._print ("Resetting gridding of full unit cell from %s to %s" %(
           str(current_unit_cell_grid), str(gridding)))
         self._print ("Resetting dimensions of full unit cell from %s to %s" %(
           str(current_unit_cell_parameters),
            str(new_unit_cell_parameters)))

       # Always run after setting unit_cell_grid or _unit_cell_crystal_symmetry
       # This time it should not change anything
       original_crystal_symmetry = self.crystal_symmetry()
       self.set_crystal_symmetry_of_partial_map()
       new_crystal_symmetry = self.crystal_symmetry()
       assert original_crystal_symmetry.is_similar_symmetry(
         new_crystal_symmetry)

       if not self.is_full_size():
         self.set_wrapping(False)

  def is_dummy_map_manager(self):
    ''' Is this a dummy map manager'''
    return self._is_dummy_map_manager

  def is_mask(self):
    ''' Is this a mask '''
    return self._is_mask

  def set_is_mask(self, value=True):
    ''' define if this is a mask'''
    assert isinstance(value, bool)
    self._is_mask = value

  def origin_is_zero(self):
    '''Return whether this map currently has an origin of (0,0,0)'''
    if self.map_data().origin() == (0, 0, 0):
      return True
    else:
      return False

  def shift_origin(self, desired_origin = (0, 0, 0),
     apply_external_origin_if_present = True,):

    '''
    Shift the origin of the map to desired_origin
      (normally desired_origin is (0, 0, 0), so just update
      origin_shift_grid_units

    Origin is the value of self.map_data().origin()
    origin_shift_grid_units is the shift to apply to self.map_data() to
      superimpose it on the original map.

    If you shift the origin by (i, j, k) then add -(i, j, k) to
      the current origin_shift_grid_units.

    If current origin is at (a, b, c) and
       desired origin = (d, e, f) and
       existing origin_shift_grid_units is (g, h, i):

    the shift to make is  (d, e, f) - (a, b, c)

    the new value of origin_shift_grid_units will be:
       (g, h, i)+(a, b, c)-(d, e, f)
       or new origin_shift_grid_units is: (g, h, i)- shift

    the new origin of map_data will be (d, e, f)

     Note on external origin: If input map had ORIGIN specified,
       so that the value of self.external_origin is not (0,0,0) and not None,
       and apply_external_origin_if_present is set, then:
       determine if self.external_origin is on a grid point and if so, convert
        and use negative of it as origin. Then set self.external_origin to zero.
       Does not apply if origin is already not (0,0,0).

    '''
    if(self.map_data() is None): return

    # Figure out what the shifts are (in grid units)
    shift_info = self._get_shift_info(desired_origin = desired_origin,
      apply_external_origin_if_present = apply_external_origin_if_present)

    # Update the value of origin_shift_grid_units
    #  This is position of the origin of the new map in the full unit cell grid
    self.origin_shift_grid_units = shift_info.new_origin_shift_grid_units

    # Set external_origin to zero as it has now been used
    self.external_origin = (0, 0, 0)

    # Shift map_data if necessary
    if shift_info.shift_to_apply !=  (0, 0, 0):
      # map will start at desired_origin and have current size:
      acc = flex.grid(shift_info.desired_origin, shift_info.new_end)
      self.map_data().reshape(acc)

    # Checks
    new_current_origin = self.map_data().origin()
    assert new_current_origin == shift_info.desired_origin

    assert add_tuples_int(shift_info.current_origin, shift_info.shift_to_apply
        ) == shift_info.desired_origin

    # Original location of first element of map should agree with previous

    assert shift_info.map_corner_original_location  ==  add_tuples_int(
       new_current_origin, self.origin_shift_grid_units)

    # If there is an associated ncs_object, shift it too
    if self._ncs_object:
      self._ncs_object=self._ncs_object.coordinate_offset(
        shift_info.shift_to_apply_cart)

  def _get_shift_info(self, desired_origin = None,
    apply_external_origin_if_present = True):
    '''
      Utility to calculate the shift necessary (grid units)
      map to place the origin of the current map
      at the position defined by desired_origin.
      See definitions in shift_origin method.

    '''
    if(desired_origin is None):
      desired_origin = (0, 0, 0)
    desired_origin = tuple(desired_origin)

    if(self.origin_shift_grid_units is None):
      self.origin_shift_grid_units = (0, 0, 0)

    # Current origin and shift to apply
    current_origin = self.map_data().origin()

    self._warning_message = ""
    if apply_external_origin_if_present and \
         tuple(self.external_origin) != (0,0,0): # check for external origin
      if self.external_origin_is_compatible_with_gridding():
         external_origin_as_grid_units = self.external_origin_as_grid_units()
      else:
        external_origin_as_grid_units = (0,0,0)
        self._warning_message="External origin is not on a grid point" +\
         "...ignoring external origin" +\
         "\n***Please contact the Phenix "+\
          "developers if you need Phenix to use this external_origin***\n"
    else:
      external_origin_as_grid_units = (0,0,0)

    if self.external_origin_as_grid_units and \
        (external_origin_as_grid_units != (0,0,0)):
      if current_origin and \
          (current_origin != (0,0,0)):
        self._warning_message="Map has external origin as well as existing " +\
         "origin shift...ignoring external origin" +\
         "\n***Please contact the Phenix "+\
          "developers if you need Phenix to use this external_origin***\n"
      else:  # take it
        self._warning_message="Map has external origin " +\
         "...using external origin as origin shift after "+\
         "conversion to grid units"
        current_origin = external_origin_as_grid_units

    # Original location of first element of map
    map_corner_original_location = add_tuples_int(current_origin,
         self.origin_shift_grid_units)

    shift_to_apply = subtract_tuples_int(desired_origin, current_origin)

    assert add_tuples_int(current_origin, shift_to_apply) == desired_origin

    new_origin_shift_grid_units = subtract_tuples_int(
        self.origin_shift_grid_units, shift_to_apply)

    current_end = add_tuples_int(current_origin, self.map_data().all())
    new_end = add_tuples_int(desired_origin, self.map_data().all())
    shift_to_apply_cart = self.grid_units_to_cart(shift_to_apply)

    shift_info = group_args(
      map_corner_original_location = map_corner_original_location,
      current_origin = current_origin,
      current_end = current_end,
      current_origin_shift_grid_units = self.origin_shift_grid_units,
      shift_to_apply = shift_to_apply,
      desired_origin = desired_origin,
      new_end = new_end,
      new_origin_shift_grid_units = new_origin_shift_grid_units,
      shift_to_apply_cart = shift_to_apply_cart,
       )
    return shift_info

  def external_origin_is_compatible_with_gridding(self):
    '''Determine if external origin falls on a grid point.'''
    value = self.external_origin_as_grid_units()
    if value is not None:
      return True
    else:
      return False

  def external_origin_as_grid_units(self, as_inverse = False):
    ''' Convert external_origin to value in grid units.
        See notes on external origin. '''
    unit_cell = self.unit_cell_crystal_symmetry().unit_cell()
    unit_cell_grid = self.unit_cell_grid
    spacings = [(a/n) for a,n in zip(unit_cell.parameters()[:3],
        unit_cell_grid)]
    if self.external_origin:
      external_origin = flex.double(self.external_origin)
    else:
      external_origin = flex.double((0.,0.,0.))
    if flex.sum(flex.abs(external_origin)) > 0:
      import math
      # external_origin is the location of the external origin in xyz coords
      # origin_shift is the location in gridding space of the external_origin
      # we are hoping that it will fall on an integer grid point in this space
      # The gridding coordinate system is the unit_cell, with one unit
      #  in each direction corresponding to the spacings in that
      #   direction (e.g., a/grid_points_along_a along the x axis)

      # use unit_cell.fractionalize(origin_shift) to get fractional coords
      fractional_external_origin = unit_cell.fractionalize(col(external_origin))

      # origin_shift in grid units is fractional_external_origin multiplied
      #   by the number of grid units along the 3 axes, rounded
      origin_shift = tuple(
         [round(f * s) for f,s in zip(
            fractional_external_origin, unit_cell_grid)])

      # origin_check is position of external_origin calculated from
      #   origin_shift and the unit_cell_grid
      origin_check = unit_cell.orthogonalize(col( tuple(
        [os / a for os, a in zip(origin_shift, unit_cell_grid)])))

      origin_distance_to_grid = math.sqrt(flex.sum(
            flex.pow2(flex.double(external_origin)-flex.double(origin_check))))
      if origin_distance_to_grid > 0.001:
        return None # not compatible with grid
      else:
        if as_inverse:
          return tuple([-a for a in origin_shift])
        else: # as-is:
          return origin_shift
    else: # no shift
      return (0,0,0)

  def shift_origin_to_match_original(self):
    '''
     Shift origin by self.origin_shift_grid_units to place origin in its
     original location
    '''
    original_origin = add_tuples_int(self.map_data().origin(),
                               self.origin_shift_grid_units)

    self.shift_origin(desired_origin = original_origin)

  def set_ncs_object(self, ncs_object):
    '''
      set the ncs object for this map_manager.  Incoming ncs_object must
     be compatible (shift_cart values must match or be defined).
      Incoming ncs_object is deep_copied.
    '''
    if not ncs_object:
      return # Nothing to do
    assert isinstance(ncs_object, mmtbx.ncs.ncs.ncs)
    if (not self.is_compatible_ncs_object(ncs_object)):
      ncs_object = self.shift_ncs_object_to_match_map_and_return_new_ncs_object(ncs_object)
    self._ncs_object = deepcopy(ncs_object)

  def set_program_name(self, program_name = None):
    '''
      Set name of program doing work on this map_manager for output
      (string)
    '''
    self.program_name = program_name
    self._print("Program name of %s added" %(program_name))

  def add_limitation(self, limitation = None):
    '''
      Add a limitation from STANDARD_LIMITATIONS_DICT
      Supply the key (such as "map_is_sharpened")
    '''
    from iotbx.mrcfile import STANDARD_LIMITATIONS_DICT
    assert limitation in list(STANDARD_LIMITATIONS_DICT.keys())

    if not self.limitations:
      self.limitations = []
    self.limitations.append(limitation)
    self._print("Limitation of %s ('%s') added to map_manager" %(
      limitation, STANDARD_LIMITATIONS_DICT[limitation]))

  def remove_labels(self):
    '''
     Remove all labels
    '''
    self.labels = []

  def add_label(self, label = None, verbose = False):
    '''
     Add a label (up to 80-character string) to write to output map.
     Default is to specify the program name and date
    '''
    if not self.labels:
      self.labels = []
    if len(label)>80:  label = label[:80]
    self.labels.reverse()  # put at beginning
    self.labels.append(to_str(label, codec = 'utf8')) # make sure it is a string
    self.labels.reverse()
    if verbose:
      self._print("Label added: %s " %(label))

  def write_map(self, file_name):

    '''
      Simple version of write

      file_name is output file name
      map_data is map_data object with 3D values for map. If not supplied,
        use self.map_data()

      Normally call with file_name (file to be written)
      Output labels are generated from existing self.labels,
      self.program_name, and self.limitations

      If self.output_external_origin is specified, write that value to file


    '''

    # Make sure we have map_data
    assert self.map_data()

    map_data = self.map_data()

    assert isinstance(self.wrapping(), bool)  # need wrapping set to write file
    # remove any labels about wrapping
    for key in ["wrapping_outside_cell","no_wrapping_outside_cell"]:
      self.remove_limitation(key)
    # Add limitation on wrapping
    new_labels=[]
    if self.wrapping():
      self.add_limitation("wrapping_outside_cell")
    else:
      self.add_limitation("no_wrapping_outside_cell")


    from iotbx.mrcfile import create_output_labels
    labels = create_output_labels(
      program_name = self.program_name,
      input_file_name = self.file_name,
      input_labels = self.labels,
      limitations = self.limitations)

    crystal_symmetry = self.unit_cell_crystal_symmetry()
    unit_cell_grid = self.unit_cell_grid
    origin_shift_grid_units = self.origin_shift_grid_units

    if map_data.origin()  ==  (0, 0, 0):  # Usual
      self._print("Writing map with origin at %s and size of %s to %s" %(
        str(origin_shift_grid_units), str(map_data.all()), file_name))
      from six.moves import StringIO
      f=StringIO()
      write_ccp4_map(
        file_name   = file_name,
        crystal_symmetry = crystal_symmetry, # unit cell and space group
        map_data    = map_data,
        unit_cell_grid = unit_cell_grid,  # optional gridding of full unit cell
        origin_shift_grid_units = origin_shift_grid_units, # origin shift
        labels      = labels,
        external_origin = self.output_external_origin,


        out = f)
      self._print(f.getvalue())
    else: # map_data has not been shifted to (0, 0, 0).  Shift it and then write
          # and then shift back
      self._print("Writing map after shifting origin")
      if self.origin_shift_grid_units and origin_shift_grid_units!= (0, 0, 0):
        self._print (
          "WARNING: map_data has origin at %s " %(str(map_data.origin()))+
         " and this map_manager will apply additional origin shift of %s " %(
          str(self.origin_shift_grid_units)))

      # Save where we are
      current_origin = map_data.origin()

      # Set origin at (0, 0, 0)
      self.shift_origin(desired_origin = (0, 0, 0))
      self.write_map(file_name = file_name)
      self.shift_origin(desired_origin = current_origin)

  def create_mask_with_map_data(self, map_data):
    '''
      Set mask to be map_data

      Does not apply the mask (use apply_mask_to_map etc for that)

      Uses cctbx.maptbx.mask.create_mask_with_mask_data to do it

      Requires origin to be zero of both self and new mask
    '''

    assert isinstance(map_data, flex.double)
    assert self.map_data().all() == map_data.all()
    assert map_data.origin() == (0,0,0)
    assert self.origin_is_zero()

    from cctbx.maptbx.mask import create_mask_with_map_data as cm
    self._created_mask = cm(map_data = map_data,
      map_manager = self)


  def create_mask_around_density(self,
      resolution = None,
      molecular_mass = None,
      sequence = None,
      solvent_content = None):
    '''
      Use cctbx.maptbx.mask.create_mask_around_density to create a
       mask automatically

      Does not apply the mask (use apply_mask_to_map etc for that)

      Parameters are:
       resolution : resolution of map, taken from self.resolution() if not
          specified
       molecular_mass: optional mass (Da) of object in density
       sequence: optional sequence of object in density
       solvent_content : optional solvent_content of map


    '''

    if not resolution:
      resolution = self.resolution()

    from cctbx.maptbx.mask import create_mask_around_density as cm
    self._created_mask = cm(map_manager = self,
        resolution = resolution,
        molecular_mass = molecular_mass,
        sequence = sequence,
        solvent_content = solvent_content, )

  def create_mask_around_edges(self, boundary_radius = None):
    '''
      Use cctbx.maptbx.mask.create_mask_around_edges to create a mask around
      edges of map.  Does not make a soft mask.  For a soft mask,
      follow with soft_mask(boundary_radius =boundary_radius)
      The radius is to define the boundary around the map.

      Does not apply the mask (use apply_mask_to_map etc for that)
    '''

    if boundary_radius is None:
      boundary_radius = self.resolution()

    from cctbx.maptbx.mask import create_mask_around_edges as cm
    self._created_mask = cm(map_manager = self,
      boundary_radius = boundary_radius)

  def create_mask_around_atoms(self, model, mask_atoms_atom_radius = None,
      invert_mask = None):
    '''
      Use cctbx.maptbx.mask.create_mask_around_atoms to create a mask around
      atoms in model

      Does not apply the mask (use apply_mask_to_map etc for that)

      mask_atoms_atom_radius default is max(3, resolution)

      invert_mask makes outside 1 and inside 0
    '''

    assert model is not None
    if mask_atoms_atom_radius is None:
      mask_atoms_atom_radius = max(3, self.resolution())

    from cctbx.maptbx.mask import create_mask_around_atoms as cm
    self._created_mask = cm(map_manager = self,
      model = model,
      mask_atoms_atom_radius = mask_atoms_atom_radius,
      invert_mask = invert_mask)

  def soft_mask(self, soft_mask_radius = None):
    '''
      Make mask a soft mask. Just uses method in cctbx.maptbx.mask
      Use resolution for soft_mask radius if not specified
    '''
    assert self._created_mask is not None
    if soft_mask_radius is None:
      soft_mask_radius = self.resolution()
    self._created_mask.soft_mask(soft_mask_radius = soft_mask_radius)

  def apply_mask(self, set_outside_to_mean_inside = False):
    '''
      Replace map_data with masked version based on current mask
      Just uses method in create_mask_around_atoms
    '''

    assert self._created_mask is not None
    new_mm = self._created_mask.apply_mask_to_other_map_manager(
      other_map_manager = self,
      set_outside_to_mean_inside = set_outside_to_mean_inside)
    self.set_map_data(map_data = new_mm.map_data())  # replace map data

  def delete_mask(self):
    '''Remove working mask'''
    self._created_mask = None

  def get_mask_as_map_manager(self):
    '''Return a map_manager containing the working mask'''
    assert self._created_mask is not None
    return self._created_mask.map_manager()

  def initialize_map_data(self, map_value = 0):
    '''
      Set all values of map_data to map_value
    '''
    s = (self.map_data() != map_value )
    self.map_data().set_selected(s, map_value)

  def set_map_data(self, map_data = None):
    '''
      Replace self.data with map_data. The two maps must have same gridding

      NOTE: This uses selections to copy all the data in map_data into
      self.data.  The map_data object is not associated with self.data, the
      data is simply copied.  Also as self.data is modified in place, any
      objects that currently are just pointers to self.data are affected.
    '''
    assert self.map_data().origin() == map_data.origin()
    assert self.map_data().all() == map_data.all()
    sel = flex.bool(map_data.size(), True)
    self.data.as_1d().set_selected(sel, map_data.as_1d())

  def as_map_model_manager(self):
    '''  Return a map_model_manager'''
    from iotbx.map_model_manager import map_model_manager
    return map_model_manager(map_manager = self)

  def invert_hand(self):
    ''' Invert the hand of this map by swapping the order of all sections
        in Z'''

    map_data = self.map_data()
    # Swap all sections in Z, one pair at a time, in place
    nx,ny,nz = [ne - ns  for ne,ns in zip(map_data.all(), map_data.origin())]
    assert nx*ny*nz == map_data.size()
    nxstart,nystart,nzstart= map_data.origin()
    for k in range (int(nz//2)):
      i = k + nzstart
      ii = nzstart + nz -i -1  # sections to swap are i, ii
      data_i = maptbx.copy(map_data,
         (nxstart,nystart,i),
         (nxstart+nx-1, nystart+ny-1,i)).deep_copy()
      assert data_i.size() == nx * ny
      data_ii = maptbx.copy(map_data,
         (nxstart,nystart,ii),
         (nxstart+nx-1, nystart+ny-1,ii)).deep_copy()
      assert data_i.size() == nx * ny
      assert data_ii.size() == data_i.size()
      acc = flex.grid(data_i.all())
      data_i.reshape(acc)
      data_ii.reshape(acc)

      maptbx.set_box(
        map_data_from = data_ii,
        map_data_to   = map_data,
        start         = (nxstart,nystart,i),
        end           = (nxstart+nx, nystart+ny, i+1))

      maptbx.set_box(
        map_data_from = data_i,
        map_data_to   = map_data,
        start         = (nxstart,nystart,ii),
        end           = (nxstart+nx, nystart+ny, ii+1))

  def as_full_size_map(self):
    '''
      Create a full-size map with the current map inside it, padded by zero

      A little tricky because the starting map is going to have its origin at
      (0, 0, 0) but the map we are creating will have that point at
      self.origin_shift_grid_units.

      First use box.with_bounds to create map from -self.origin_shift_grid_units
       to -self.origin_shift_grid_units+self.unit_cell_grid-(1, 1, 1).  Then
      shift that map to place origin at (0, 0, 0)

      If the map is full size already, return the map as is
      If the map is bigger than full size stop as this is not suitable

    '''

    # Check to see if this is full size or bigger
    full_size_minus_working=subtract_tuples_int(self.unit_cell_grid,
      self.map_data().all())


    if full_size_minus_working in [(-1,-1,-1),(0, 0, 0)]: # Exactly full size already. Done
      assert self.origin_shift_grid_units == (0, 0, 0)
      assert self.map_data().origin() == (0, 0, 0)
      return self
    # Must not be bigger than full size already
    assert flex.double(full_size_minus_working).min_max_mean().min >= -1
    working_lower_bounds = self.origin_shift_grid_units
    working_upper_bounds = tuple([i+j-1 for i, j in zip(working_lower_bounds,
      self.map_data().all())])
    lower_bounds = tuple([-i for i in self.origin_shift_grid_units])
    upper_bounds = tuple([i+j-1 for i, j in zip(lower_bounds, self.unit_cell_grid)])
    new_lower_bounds = tuple([i+j for i, j in zip(
      lower_bounds, self.origin_shift_grid_units)])
    new_upper_bounds = tuple([i+j for i, j in zip(
      upper_bounds, self.origin_shift_grid_units)])
    print("Creating full-size map padding outside of current map with zero",
      file = self.log)
    print("Bounds of current map: %s to %s" %(
     str(working_lower_bounds), str(working_upper_bounds)), file = self.log)
    print("Bounds of new map: %s to %s" %(
     str(new_lower_bounds), str(new_upper_bounds)), file = self.log)

    from cctbx.maptbx.box import with_bounds
    box = with_bounds(self,
       lower_bounds = lower_bounds,
       upper_bounds = upper_bounds,
       log = self.log)
    box.map_manager().set_original_origin_and_gridding(
       original_origin = (0, 0, 0))

    box.map_manager().add_label(
       "Restored full size from box %s - %s, pad with zero" %(
     str(working_lower_bounds), str(working_upper_bounds)))
    assert box.map_manager().origin_shift_grid_units == (0, 0, 0)
    assert box.map_manager().map_data().origin() == (0, 0, 0)
    assert box.map_manager().map_data().all() == box.map_manager().unit_cell_grid
    if box.map_manager().unit_cell_crystal_symmetry().space_group_number() == 1:
      assert box.map_manager().unit_cell_crystal_symmetry().is_similar_symmetry(
        box.map_manager().crystal_symmetry())
    else:
      assert box.map_manager().crystal_symmetry().space_group_number() == 1
      from cctbx import crystal
      assert box.map_manager().crystal_symmetry().is_similar_symmetry(
        crystal.symmetry(
           box.map_manager().unit_cell_crystal_symmetry().unit_cell(),
           1))
    return box.map_manager()


  def cc_to_other_map_manager(self, other_map_manager):
    '''Get map correlation to other map manager'''
    assert self.is_similar(other_map_manager)

    return flex.linear_correlation(self.map_data().as_1d(),
     other_map_manager.map_data().as_1d()).coefficient()

  def density_at_sites_cart(self, sites_cart):
    '''
    Return flex.double list of density values corresponding to sites (Cartesian
    coordinates in A).
    Note that coordinates are relative to the current
    origin of the map (normally set to (0,0,0) before working with the map,
    see sites_cart_to_sites_cart_absolute and
    sites_cart_absolute_to_sites_cart.)
    '''
    assert isinstance(sites_cart, flex.vec3_double)

    from cctbx.maptbx import real_space_target_simple_per_site
    density_values = real_space_target_simple_per_site(
      unit_cell = self.crystal_symmetry().unit_cell(),
      density_map = self.map_data(),
      sites_cart = sites_cart)

    # Cross off anything outside the box if wrapping is false
    if not self.wrapping():
      sites_frac = self.crystal_symmetry().unit_cell().fractionalize(sites_cart)
      x,y,z = sites_frac.parts()
      s = (
         (x < 0) |
         (y < 0) |
         (z < 0) |
         (x > 1) |
         (y > 1) |
         (z > 1)
         )
      density_values.set_selected(s,0)
    return density_values



  def get_density_along_line(self,
      start_site = None,
      end_site = None,
      n_along_line = 10,
      include_ends = True):

    '''
      Return group_args object with density values and coordinates
      along a line segment from start_site to end_site
      (Cartesian coordinates in A) with n_along_line sampling points.
      Optionally include/exclude ends.
      Note that coordinates are relative to the current
      origin of the map (normally set to (0,0,0) before working with the map,
      see sites_cart_to_sites_cart_absolute and
      sites_cart_absolute_to_sites_cart.)
    '''
    along_sites = flex.vec3_double()
    if include_ends:
      start = 0
      end = n_along_line+1
    else:
      start = 1
      end = n_along_line

    for i in range(start, end):
      weight = (i/n_along_line)
      along_line_site = col(start_site)*weight+col(end_site)*(1-weight)
      along_sites.append(along_line_site)
    along_density_values = self.density_at_sites_cart(sites_cart = along_sites)
    return group_args(
     along_density_values = along_density_values,
       along_sites = along_sites)

  def apply_spectral_scaling(self, d_min = None, d_max = None,
    n_bins = 100):
    '''Apply spectral scaling to a map to approximate intensity vs
       resolution expected for a protein structure'''

    print("Applying spectral scaling", file = self.log)
    map_coeffs = self.map_as_fourier_coefficients(d_min = d_min,
      d_max = d_max)
    from iotbx.map_model_manager import get_map_coeffs_as_fp_phi
    f_array_info = get_map_coeffs_as_fp_phi(
        map_coeffs, d_min= map_coeffs.d_min(), n_bins = n_bins)

    from cctbx.development.approx_amplitude_vs_resolution import \
       approx_amplitude_vs_resolution
    aavr = approx_amplitude_vs_resolution(generate_mock_rms_fc_list=False)
    target_scale_factors = aavr.get_target_scale_factors(f_array_info.f_array)

    # Now interpolate these scale factors:

    scale_array=f_array_info.f_array.binner().interpolate(
        target_scale_factors, 1) # d_star_power=1
    scaled_f_array=f_array_info.f_array.customized_copy(
          data=f_array_info.f_array.data()*scale_array)

    new_map_coeffs = scaled_f_array.phase_transfer(
       phase_source=f_array_info.phases, deg=True)
    new_mm = self.fourier_coefficients_as_map_manager(new_map_coeffs)

    self.set_map_data(map_data = new_mm.map_data())  # replace map data


  def scale_map(self, scale = None):
    '''
      Multiply values in map by scale.
    '''
    self.set_map_data(map_data = scale * self.map_data())  # replace map data

  def resolution_filter(self, d_min = None, d_max = None):
    '''
      High- or low-pass filter the map in map_manager.
      Changes and overwrites contents of this map_manager.
      Remove all components with resolution < d_min or > d_max
      Either d_min or d_max or both can be None.
      To make a low_pass filter with cutoff at 3 A, set d_min=3
      To make a high_pass filter with cutoff at 2 A, set d_max=2

    '''
    map_coeffs = self.map_as_fourier_coefficients(d_min = d_min,
      d_max = d_max)
    mm = self.fourier_coefficients_as_map_manager( map_coeffs=map_coeffs)
    self.set_map_data(map_data = mm.map_data())  # replace map data


  def gaussian_filter(self, smoothing_radius):
    '''
      Gaussian blur the map in map_manager with given smoothing radius.
      Changes and overwrites contents of this map_manager.
    '''
    assert smoothing_radius is not None

    map_data = self.map_data()
    from cctbx.maptbx import smooth_map
    smoothed_map_data = smooth_map(
        map              = map_data,
        crystal_symmetry = self.crystal_symmetry(),
        rad_smooth       = smoothing_radius)
    self.set_map_data(map_data = smoothed_map_data)  # replace map data

  def binary_filter(self, threshold = 0.5):
    '''
      Apply a binary filter to the map (value at pixel i,j,k=1 if average
      of all 27 pixels within 1 of this one is > threshold, otherwise 0)
      Changes and overwrites contents of this map_manager.
    '''

    assert self.origin_is_zero()

    map_data=self.map_data()

    from cctbx.maptbx import binary_filter
    bf=binary_filter(map_data,threshold).result()
    self.set_map_data(map_data = bf)  # replace map data

  def randomize(self,
      d_min = None,
      low_resolution_fourier_noise_fraction=0.01,
      high_resolution_fourier_noise_fraction=2,
      low_resolution_real_space_noise_fraction=0,
      high_resolution_real_space_noise_fraction=0,
      low_resolution_noise_cutoff=None,
      random_seed = None,
         ):
    '''
      Randomize a map.

      Unique aspect of this noise generation is that it can be specified
      whether the noise is local in real space (every point in a map
      gets a random value before Fourier filtering), or local in Fourier
      space (every Fourier coefficient gets a complex random offset).
      Also the relative contribution of each type of noise vs resolution
      can be controlled.

      Parameters:

      d_min:  high-resolution limit in Fourier transformations

      low_resolution_fourier_noise_fraction (float, 0): Low-res Fourier noise
      high_resolution_fourier_noise_fraction (float, 0): High-res Fourier noise
      low_resolution_real_space_noise_fraction(float, 0): Low-res
          real-space noise
      high_resolution_real_space_noise_fraction (float, 0): High-res
          real-space noise
      low_resolution_noise_cutoff (float, None):  Low resolution where noise
          starts to be added

    '''

    assert self.origin_is_zero()

    if d_min is None:
      d_min = self.resolution()

    map_data=self.map_data()
    if random_seed is None:
      import random
      random_seed = random.randint(1,100000)
    from cctbx.development.create_models_or_maps import generate_map
    new_map_manager =generate_map(
      map_manager = self,   # gridding etc
      map_coeffs = self.map_as_fourier_coefficients(),
      d_min = d_min,
      low_resolution_fourier_noise_fraction=
         low_resolution_fourier_noise_fraction,
      high_resolution_fourier_noise_fraction=
         high_resolution_fourier_noise_fraction,
      low_resolution_real_space_noise_fraction=
         low_resolution_real_space_noise_fraction,
      high_resolution_real_space_noise_fraction=
         high_resolution_real_space_noise_fraction,
      low_resolution_noise_cutoff=
         low_resolution_noise_cutoff,
      random_seed = random_seed)

    self.set_map_data(map_data = new_map_manager.map_data())  # replace map data

  def deep_copy(self):
    '''
      Return a deep copy of this map_manager object
      Uses customized_copy to deepcopy everything including map_data

      Origin does not have to be at (0, 0, 0) to apply
    '''
    return self.customized_copy(map_data = self.map_data())

  def customized_copy(self, map_data = None, origin_shift_grid_units = None,
      use_deep_copy_for_map_data = True,
      crystal_symmetry_space_group_number = None,
      wrapping = None,):
    '''
      Return a customized deep_copy of this map_manager, replacing map_data with
      supplied map_data.

      The map_data and any _created_mask will be deep_copied before using
      them unless use_deep_copy_for_map_data = False

      Normally this customized_copy is applied with a map_manager
      that has already shifted the origin to (0, 0, 0) with shift_origin.

      Normally the new map_data will have the same dimensions of the current
      map_data. Normally new map_data will also have origin at (0, 0, 0).

      NOTE: It is permissible for map_data to have different bounds or origin
      than the current self.map_data.  In this case you must specify a new
      value of origin_shift_grid_units corresponding to this new map_data.
      This new origin_shift_grid_units specifies the original position in the
      full unit cell grid of the most-negative corner grid point of the
      new map_data. The new map_manager will still have the same unit
      cell dimensions and grid as the original.

      NOTE: It is permissible to get a customized copy before shifting the
      origin.  Applying with non-zero origin requires that:
         self.origin_shift_grid_units == (0, 0, 0)
         origin_shift_grid_units = (0, 0, 0)
         map_data.all() (size in each direction)  of current and new maps
            are the same.
         origins of current and new maps are the same

       NOTE: wrapping is normally copied from original map, but if new map is
       not full size then wrapping is always set to False.

      If crystal_symmetry_space_group_number is specified, use it
    '''

    # Make a deep_copy of map_data and _created_mask unless
    #    use_deep_copy_for_map_data = False

    if use_deep_copy_for_map_data:
      map_data = map_data.deep_copy()
      created_mask = deepcopy(self._created_mask)
    else:
      created_mask = self._created_mask

    assert map_data is not None # Require map data for the copy

    if map_data.origin() !=  (0, 0, 0):

      # Make sure all the assumptions are satisfied so we can just copy
      assert self.origin_shift_grid_units == (0, 0, 0)
      assert origin_shift_grid_units in [None, (0, 0, 0)]
      assert self.map_data().all() == map_data.all()
      assert self.map_data().origin() == map_data.origin()

      # Now just go ahead and copy using origin_shift_grid_units = (0, 0, 0)
      origin_shift_grid_units = (0, 0, 0)

    elif origin_shift_grid_units is None:  # use existing origin shift
      assert map_data.all()  ==  self.map_data().all() # bounds must be same
      origin_shift_grid_units = deepcopy(self.origin_shift_grid_units)

    # Keep track of change in shift_cart
    original_shift_cart=self.shift_cart()

    # Deepcopy this object and then set map_data and origin_shift_grid_units


    mm = map_manager(
     file_name = None,
     map_data = map_data,
     unit_cell_grid = self.unit_cell_grid,
     unit_cell_crystal_symmetry = self._unit_cell_crystal_symmetry,
     origin_shift_grid_units = origin_shift_grid_units,
     ncs_object = self._ncs_object.deep_copy() if self._ncs_object else None,
     wrapping = False,
     experiment_type = self._experiment_type,
     scattering_table = self._scattering_table,
     resolution = self._resolution,
     log = self.log,)

    mm._is_mask = self._is_mask
    mm._is_dummy_map_manager = self._is_dummy_map_manager
    # Set things that are not necessarily the same as in self:
    mm._created_mask = created_mask  # using self._created_mask or a
    mm.file_name = self.file_name
    mm.program_name = self.program_name
    mm.limitations = self.limitations
    mm.labels = self.labels


    if wrapping is not None:
      desired_wrapping = wrapping
    else:
      desired_wrapping = self._wrapping

    if mm.is_full_size():
      mm.set_wrapping(desired_wrapping)
    else: #
      mm.set_wrapping(False)

    # Set up _crystal_symmetry for the new object
    mm.set_crystal_symmetry_of_partial_map(
      space_group_number = crystal_symmetry_space_group_number)
      # Required and must be last


    # Keep track of change in shift_cart
    delta_origin_shift_grid_units = tuple([new - orig for new, orig in zip (
        origin_shift_grid_units, self.origin_shift_grid_units)])
    delta_shift_cart = tuple([-x for x in self.grid_units_to_cart(
       delta_origin_shift_grid_units)])
    new_shift_cart= tuple([
        o+d for o,d in zip(original_shift_cart,delta_shift_cart)])

    if self._ncs_object:
      mm._ncs_object = self._ncs_object.deep_copy(
        coordinate_offset=delta_shift_cart)
      assert approx_equal(mm.shift_cart(),mm._ncs_object.shift_cart())
    else:
      mm._ncs_object = None

    return mm

  def set_experiment_type(self, experiment_type):
    ''' Set the experiment type
       xray,neutron, or cryo_em
       If scattering_table is not defined, it is guessed from experiment_type
    '''
    self._experiment_type = experiment_type
    self._set_up_experiment_type_and_scattering_table_and_resolution()

  def set_scattering_table(self, scattering_table):
    ''' Set the scattering table (type of scattering)
       electron:  cryo_em
       n_gaussian x-ray (standard)
       wk1995:    x-ray (alternative)
       it1992:    x-ray (alternative)
       neutron:   neutron scattering
    '''
    self._scattering_table = scattering_table
    self._set_up_experiment_type_and_scattering_table_and_resolution()

  def set_resolution(self, resolution):
    ''' Set the nominal resolution of map
    '''
    self._resolution = resolution

  def experiment_type(self):
    '''Return the experiment type (xray or cryo_em)'''
    return self._experiment_type

  def minimum_resolution(self, set_minimum_resolution = True):
    '''
      Get minimum resolution.  If set previously, use that value
    '''
    if self._minimum_resolution:
      return self._minimum_resolution

    from cctbx.maptbx import d_min_from_map
    minimum_resolution = d_min_from_map(
           map_data=self.map_data(),
           unit_cell=self.crystal_symmetry().unit_cell())

    if set_minimum_resolution:
      self._minimum_resolution = minimum_resolution

    return minimum_resolution

  def resolution(self, force = False, method = 'd99', set_resolution = True):
    ''' Get nominal resolution
        Return existing if present unless force is True
        choices:
                  d9: resolution correlated at 0.9 with original
                  d99: resolution correlated at 0.99 with original
                  d999: resolution correlated at 0.999 with original
                  d_min: d_min_from_map
    '''
    if self._resolution is not None and (not force):
      return self._resolution


    assert method in ['d99','d9','d999','d_min']


    working_resolution = -1 # now get it

    if method in ['d99','d9','d999'] and \
        self.map_data().count(0) != self.map_data().size():
      from cctbx.maptbx import d99
      if self.origin_is_zero():
        map_data = self.map_data()
      else:
        map_data = self.map_data().deep_copy()
      d99_object = d99(
         map = map_data, crystal_symmetry = self.crystal_symmetry())

      working_resolution = getattr(d99_object.result,method,-1)

    from cctbx.maptbx import d_min_from_map  # get this to check
    d_min_estimated_from_map = self.minimum_resolution()

    if working_resolution < d_min_estimated_from_map:  # we didn't get it or want to use d_min
      working_resolution = d_min_estimated_from_map

    if set_resolution:
      self._resolution = working_resolution
    return working_resolution

  def scattering_table(self):
    '''Return the scattering table to use:
       electron:  cryo_em
       n_gaussian x-ray (standard)
       wk1995:    x-ray (alternative)
       it1992:    x-ray (alternative)
       neutron:   neutron scattering
    '''
    return self._scattering_table

  def ncs_object(self):
    ''' Return the NCS object '''
    return self._ncs_object

  def _set_up_experiment_type_and_scattering_table_and_resolution(self):
    '''Set up the experiment type, scattering table, and resolution
    '''
    default_scattering_table_dict = {
     'xray':'n_gaussian',
     'neutron':'neutron',
     'cryo_em':'electron',
     }

    if self._experiment_type not in [None, Auto]:
      assert self._experiment_type in ['xray','neutron','cryo_em']
      if self.wrapping() and self._experiment_type=='cryo_em':
        raise Sorry("Cannot use wrapping if experiment_type is 'cryo_em'")

    else:  # Try to guess experiment_type
      if self.crystal_symmetry().space_group_number() > 1:
        # Has space-group symmmetry, not cryo_em
        self._experiment_type = 'xray'  # could be neutron of course
      elif self.is_full_size() and self.wrapping() is False:
        # No space-group symmetry, full size map, no wrapping: cryo_em
        self._experiment_type = 'cryo_em'  # full size map and no wrapping
      elif self.is_full_size() and self.wrapping() is True:
        # P1 symmetry, full size map, wrapping True: xray
        self._experiment_type = 'xray'  # full size map and wrapping
      else:
        # P1 symmetry, not a full-size map...cannot tell
        self._experiment_type = None

    if self._experiment_type is not None:
      if self._scattering_table is None:
        self._scattering_table = default_scattering_table_dict[
          self._experiment_type]

    if self._scattering_table not in [None, Auto]:
      assert self._scattering_table in ['electron','n_gaussian',
       'wk1995','it1992','neutron']

    if self._scattering_table is Auto:
      self._scattering_table = None

    if self._resolution is Auto:
      self._resolution = None

    if self._experiment_type is Auto:
      self._experiment_type = None

    assert not (self._wrapping is Auto)


  def set_wrapping(self, wrapping_value):
    '''
       Set wrapping to be wrapping_value
    '''
    assert isinstance(wrapping_value, bool)
    self._wrapping = wrapping_value
    if self._wrapping:
      if not self.is_full_size():
        raise Sorry("You cannot set wrapping=True for a map that is not full size")

  def wrapping(self):
    '''
      Report if map can be wrapped

    '''
    return self._wrapping

  def is_full_size(self):
    '''
      Report if map is full unit cell
    '''
    if self.map_data().all()  ==  self.unit_cell_grid:
      return True
    else:
      return False

  def is_consistent_with_wrapping(self, relative_sd_tol = 0.01):
    '''
      Report if this map looks like it is a crystallographic map and can be
      wrapped

      If it is not full size...no wrapping
      If origin is not at zero...no wrapping
      If it is not periodic, no wrapping
      If very small or resolution_factor for map is close to 0.5...cannot tell
      If has all zeroes (or some other constant on edges) ... no wrapping

      relative_sd_tol defines how close to constant values at edges must be
      to qualify as "constant"

      Returns True, False, or None (unsure)

    '''
    if not self.is_full_size():
      return False

    if self.map_data().origin() != (0, 0, 0):
      return False
    from cctbx.maptbx import is_periodic, is_bounded_by_constant
    if is_bounded_by_constant(self.map_data(),
       relative_sd_tol = relative_sd_tol):  # Looks like a cryo-EM map
      return False

    # Go with whether it looks periodic (cell translations give similar values
    #  or transform of high-res data is mostly at edges of cell)
    return is_periodic(self.map_data())  # Can be None if unsure


  def is_similar(self, other = None,
     absolute_angle_tolerance = 0.01,
     absolute_length_tolerance = 0.01,
     ):
    '''Determine whether this map_manager is similar (symmetry, gridding,
        size) to another map_manager.
    '''
    # Check to make sure origin, gridding and symmetry are similar
    self._warning_message=""

    if tuple(self.origin_shift_grid_units) !=  tuple(
        other.origin_shift_grid_units):
      self._warning_message="Origin shift grid units "+  \
        "(%s) does not match other (%s)" %(
        str(self.origin_shift_grid_units),str(other.origin_shift_grid_units))
      return False
    if not self.unit_cell_crystal_symmetry().is_similar_symmetry(
      other.unit_cell_crystal_symmetry(),
      absolute_angle_tolerance = absolute_angle_tolerance,
      absolute_length_tolerance = absolute_length_tolerance,):
      self._warning_message="Unit cell crystal symmetry:"+ \
        "\n%s\n does not match other:\n%s\n" %(
        str(self.unit_cell_crystal_symmetry()),
         str(other.unit_cell_crystal_symmetry()))
      return False
    if not self.crystal_symmetry().is_similar_symmetry(
      other.crystal_symmetry(),
      absolute_angle_tolerance = absolute_angle_tolerance,
      absolute_length_tolerance = absolute_length_tolerance):
      self._warning_message="Crystal symmetry:"+ \
        "\n%s\ndoes not match other: \n%s\n" %(
        str(self.crystal_symmetry()),
         str(other.crystal_symmetry()))
      return False
    if self.map_data().all()!=  other.map_data().all():
      self._warning_message="Existing map gridding "+ \
        "(%s) does not match other (%s)" %(
         str(self.map_data().all()),str(other.map_data().all()))
      return False
    if self.unit_cell_grid !=  other.unit_cell_grid:
      self._warning_message="Full map gridding "+ \
        "(%s) does not match other (%s)" %(
         str(self.map_data().all()),str(other.map_data().all()))
      return False

    # Make sure wrapping is same for all
    if ( self.wrapping() !=  other.wrapping()):
      self._warning_message="Wrapping "+ "(%s) does not match other (%s)" %(
         str(self.wrapping()),
         str(other.wrapping()))
      return False

    # Make sure ncs objects are similar if both have one
    if (self.ncs_object() is not None) and (
        other.ncs_object() is not None):
      if not other.ncs_object().is_similar_ncs_object(self.ncs_object()):
        text1=self.ncs_object().as_ncs_spec_string()
        text2=other.ncs_object().as_ncs_spec_string()
        self._warning_message="NCS objects do not match"+ \
           ":\n%s\n does not match other:\n%s" %( text1,text2)
        return False

    return True

  def cart_to_grid_units(self, xyz):
    '''Convert xyz (Cartesian) to grid units'''

    return tuple([int(0.5 + x * n) for x,n in
       zip(self.crystal_symmetry().unit_cell().fractionalize(xyz),
        self.map_data().all())])

  def grid_units_to_cart(self, grid_units):
    ''' Convert grid units to Cartesian coordinates '''
    x = grid_units[0]/self.unit_cell_grid[0]
    y = grid_units[1]/self.unit_cell_grid[1]
    z = grid_units[2]/self.unit_cell_grid[2]
    return self.unit_cell().orthogonalize(tuple((x, y, z)))


  def shift_cart(self):
    '''
     Return the shift_cart of this map from its original location.

     (the negative of the origin shift ) in Cartesian coordinates
     '''
    return tuple(
       [-x for x in self.grid_units_to_cart(self.origin_shift_grid_units)])

  def shift_ncs_object_to_match_map_and_return_new_ncs_object(self,ncs_object):
    '''
      Move the ncs_object to match this map. Also sets ncs_object shift_cart
      Returns new copy of ncs_obect and does not affect the original

      Note difference from set_ncs_object_shift_cart_to_match_map which
        sets the shift_cart but does not move the object
    '''
    if ncs_object.shift_cart():
      offset = tuple(
        [s - n for s, n in zip(self.shift_cart(), ncs_object.shift_cart())])
      ncs_object = ncs_object.coordinate_offset(offset)

    else:
      ncs_object = ncs_object.coordinate_offset(self.shift_cart())
    return ncs_object

  def shift_model_to_match_map(self, model):
    '''
      Move the model to match this map.
      Note difference from set_model_symmetries_and_shift_cart_to_match_map
       which sets model symmetry and shift_cart but does not move the model
    '''
    if model.shift_cart():
      offset = tuple(
        [s - n for s, n in zip(self.shift_cart(), model.shift_cart())])
      model.shift_model_and_set_crystal_symmetry(shift_cart=offset)
    else:
      model.shift_model_and_set_crystal_symmetry(shift_cart=self.shift_cart())

  def set_ncs_object_shift_cart_to_match_map(self, ncs_object):
    '''
      Set the ncs_object shift_cart to match map

      Overwrites any information in ncs_object on shift_cart
      Modifies ncs_object in place. Does not return anything

      Do not use this to try to shift the ncs object. That is done in
      the ncs object itself with ncs_object.coordinate_offset(shift_cart),
      which returns a new ncs object

      You can use ncs_object =
       self.shift_ncs_object_to_match_map_and_return_new_ncs_object(ncs_object)
         to shift the ncs object and set its shift_cart and get a new copy.
    '''

    # Set shift_cart (shift since readin) to match shift_cart for
    #   map (shift of origin is opposite of shift applied)
    ncs_object.set_shift_cart(self.shift_cart())

  def set_crystal_symmetry_to_p1(self,
     space_group_number = 1):
    '''
      Change the working crystal symmetry to P1
      This changes map in place
      Do a deep_copy first if you do not want it changed
      (Actually can set space group number to anything so you can set it back)
    '''
    print("\nSetting working crystal symmetry to P1 so "+
       "that edges can be masked", file = self.log)

    self.set_crystal_symmetry_of_partial_map(
      space_group_number = space_group_number)


  def set_model_symmetries_and_shift_cart_to_match_map(self,model):
    '''
      Set the model original and working crystal_symmetry to match map.

      Overwrites any information in model on symmetry and shift_cart
      Modifies model in place

      NOTE: This does not shift the coordinates in model.  It is used
      to fix crystal symmetry and set shift_cart, not to actually shift
      a model.
      For shifting a model, use:
         model.shift_model_and_set_crystal_symmetry(shift_cart=shift_cart)
    '''
    # Check if we really need to do anything
    if not model:
      return # nothing to do

    if self.is_compatible_model(model,
       require_match_unit_cell_crystal_symmetry = True):
      return # already fine

    if model.shift_cart() is not None and tuple(model.shift_cart()) != (0,0,0):
      # remove shift_cart
      model.set_shift_cart((0,0,0))

    # Set crystal_symmetry to match map. This changes the xray_structure.
    model.set_crystal_symmetry(self.crystal_symmetry())

    # Set original crystal symmetry to match map unit_cell_crystal_symmetry
    # This just changes a specification in the map, nothing else changes
    model.set_unit_cell_crystal_symmetry(self.unit_cell_crystal_symmetry())

    # Set shift_cart (shift since readin) to match shift_cart for
    #   map (shift of origin is opposite of shift applied)
    model.set_shift_cart(self.shift_cart())

  def check_consistency(self, stop_on_errors = True, print_errors = True,
        absolute_angle_tolerance = None,
        absolute_length_tolerance = None,
        shift_tol = None):
    """
    Carry out overall consistency checks. Used in map_model_manager
    Note: the stop_on_errors, print_errors, and 3 tolerance kw are used in
      map_model_manager when checking consistency there
    """

    if absolute_angle_tolerance is None:
      absolute_angle_tolerance = 0.01
    if absolute_length_tolerance is None:
      absolute_length_tolerance = 0.01
    if shift_tol is None:
      shift_tol = 0.001

    # Check crystal_symmetry, unit_cell_crystal_symmetry, shift_cart
    # For now, only shift_cart and only ncs_object are relevant

    ok = True
    if self.ncs_object():
      if (not self.is_compatible_ncs_object(self.ncs_object(),
         tol = shift_tol)):
        ok = False
        text = "NCS object does not have same shift_cart as map_manager" +\
          " %s vs %s" %(self.ncs_object().shift_cart(),
           self.shift_cart())

    if (not ok):
      if print_errors:
         print("** Mismatch in model object\n%s" %(text))
      if stop_on_errors:
        raise AssertionError(text)

    return ok

  def is_compatible_ncs_object(self, ncs_object, tol = 0.001):
    '''
      ncs_object is compatible with this map_manager if shift_cart is
      the same as map
    '''

    ok=True
    text=""

    map_shift=flex.double(self.shift_cart())
    ncs_object_shift=flex.double(ncs_object.shift_cart())
    delta=map_shift-ncs_object_shift
    mmm=delta.min_max_mean()
    if mmm.min < -tol or mmm.max > tol: # shifts do not match
      text="Shift of ncs object (%s) does not match shift of map (%s)" %(
         str(ncs_object_shift),str(map_shift))
      ok=False

    self._warning_message=text
    return ok

  def is_compatible_model(self, model,
       require_match_unit_cell_crystal_symmetry = False,
        absolute_angle_tolerance = 0.01,
        absolute_length_tolerance = 0.01,
        shift_tol = 0.001):
    '''
      Model is compatible with this map_manager if it is not specified as being
      different.

      They are different if:
        1. original and current symmetries are present and different from each
          other and do not match
        2. model current symmetry does not match map original or current
        3. model has a shift_cart (shift applied) different than map shift_cart

      If require_match_unit_cell_crystal_symmetry is True, then they are
      incompatible if anything is different.

      If require_match_unit_cell_crystal_symmetry is False, original
       symmetries do not have to match.  Model crystal_symmetry can match
       the unit_cell_crystal_symmetry or crystal_symmetry of the map.
    '''

    ok=None
    text=""

    if not model:
      return None

    model_uc=model.unit_cell_crystal_symmetry()
    model_sym=model.crystal_symmetry()
    map_uc=self.unit_cell_crystal_symmetry()
    map_sym=self.crystal_symmetry()

    model_uc = model_uc if model_uc and model_uc.unit_cell() is not None else None
    model_sym = model_sym if model_sym and model_sym.unit_cell() is not None else None
    map_uc = map_uc if map_uc and map_uc.unit_cell() is not None else None
    map_sym = map_sym if map_sym and map_sym.unit_cell() is not None else None


    if (not require_match_unit_cell_crystal_symmetry) and \
        (model_uc and model_sym and model_uc.is_similar_symmetry(model_sym)):
      # Ignore the model_uc because it may or may not have come from
      # model_sym
      model_uc = None

    if (not require_match_unit_cell_crystal_symmetry) and \
        (map_uc and map_sym and map_uc.is_similar_symmetry(map_sym)):
      # Ignore the map_uc because it may or may not have come from
      # map_sym
      map_uc = None

    text_model_uc=str(model_uc).replace("\n"," ")
    text_model=str(model_sym).replace("\n"," ")
    text_map_uc=str(map_uc).replace("\n"," ")
    text_map=str(map_sym).replace("\n"," ")

    assert map_sym # map_sym should always should be there. model_sym could be missing

    if require_match_unit_cell_crystal_symmetry and map_uc and (
      not model_uc) and (
       not map_sym.is_similar_symmetry(map_uc,
        absolute_angle_tolerance = absolute_angle_tolerance,
        absolute_length_tolerance = absolute_length_tolerance,
         )):
      ok=False
      text="Model and map are different because "+\
          "require_match_unit_cell_crystal_symmetry is set and "+\
          "model does not have original_crystal_symmetry, and " +\
        "model symmetry: \n%s\n does not match map original symmetry:" %(
          model_sym) +\
        "\n%s\n. Current map symmetry is: \n%s\n " %(
         text_map_uc,text_map)

    elif model_sym and model_uc and map_uc and (
        (not map_uc.is_similar_symmetry(map_sym,
        absolute_angle_tolerance = absolute_angle_tolerance,
        absolute_length_tolerance = absolute_length_tolerance,))
         or (not model_uc.is_similar_symmetry(model_sym,
        absolute_angle_tolerance = absolute_angle_tolerance,
        absolute_length_tolerance = absolute_length_tolerance,))
         ) and (
         (not model_uc.is_similar_symmetry(map_uc,
        absolute_angle_tolerance = absolute_angle_tolerance,
        absolute_length_tolerance = absolute_length_tolerance,
        )) or
         (not model_sym.is_similar_symmetry(map_sym,
        absolute_angle_tolerance = absolute_angle_tolerance,
        absolute_length_tolerance = absolute_length_tolerance,
         ) ) ):
       ok=False # model and map_manager uc present and some symmetries do not match
       text="Model original symmetry: \n%s\n and current symmetry :\n%s\n" %(
          text_model_uc,text_model)+\
          "do not match map unit_cell symmetry:"+\
         " \n%s\n and map current symmetry: \n%s\n symmetry" %(
           text_map_uc,text_map)
    elif map_uc and model_sym and (not model_sym.is_similar_symmetry(map_uc,
        absolute_angle_tolerance = absolute_angle_tolerance,
        absolute_length_tolerance = absolute_length_tolerance,
        )) and (not
              model_sym.is_similar_symmetry(map_sym,
        absolute_angle_tolerance = absolute_angle_tolerance,
        absolute_length_tolerance = absolute_length_tolerance,
        )):
       ok=False # model does not match either map symmetry
       text="Model current symmetry: \n%s\n" %(
          text_model)+\
          " does not match map unit_cell symmetry:"+\
           " \n%s\n or map current symmetry: \n%s\n" %(
           text_map_uc,text_map)
    elif model_sym and (not model_uc) and (not map_uc) and (
           not model_sym.is_similar_symmetry(map_sym,
        absolute_angle_tolerance = absolute_angle_tolerance,
        absolute_length_tolerance = absolute_length_tolerance,
        )):
       ok=False # model has no uc and model symmetry does not match map symmetry
       text="Model current symmetry: \n%s\n" %(
          text_model)+\
          " does not match map symmetry: \n%s\n" %( text_map)

    elif require_match_unit_cell_crystal_symmetry and (
        not model_sym) and (not model_uc):
       ok=False # model does not have any symmetry so it does not match
       text="Model has no symmetry and cannot match any map"

    elif (not model_sym) and (not model_uc):
       ok=True # model does not have any symmetry so anything is ok
       text="Model has no symmetry and can match any map symmetry"

    else:  # match

       ok=True
       text="Model original symmetry: \n%s\n and current symmetry: \n%s\n" %(
          text_model_uc,text_model)+\
          "are compatible with "+\
          "map unit_cell symmetry:\n%s\n and current map symmetry:\n%s\n" %(
           text_map_uc,text_map)

    assert isinstance(ok, bool)  # must have chosen

    map_shift_cart=self.shift_cart()
    if ok and (map_shift_cart != (0,0,0)):
      if model.shift_cart() is None: # map is shifted but not model
        ok=False
        text+=" However map is shifted (shift_cart=%s) but model is not" %(
           str(map_shift_cart))
      else:
        map_shift=flex.double(map_shift_cart)
        model_shift=flex.double(model.shift_cart())
        delta=map_shift-model_shift
        mmm=delta.min_max_mean()
        if mmm.min<-shift_tol or mmm.max > shift_tol: # shifts do not match
          ok=False
          text+=" However map shift "+\
              "(shift_cart=%s) does not match model shift (%s)" %(
           str(map_shift),str(model_shift))
    self._warning_message=text
    return ok

  def warning_message(self):
    '''Return the warning message, if any'''
    if hasattr(self,'_warning_message'):
       return self._warning_message

  def set_mean_zero_sd_one(self):
    '''
     Function to normalize the map
     If the map has a constant value, do nothing
    '''
    map_data = self.map_data()
    map_data = map_data - flex.mean(map_data)
    sd = map_data.sample_standard_deviation()
    if sd is not None and sd != 0:
      map_data = map_data/sd
      self.set_map_data(map_data)

  def ncs_cc(self):
    '''Return value of NCS correlation if available'''
    if hasattr(self,'_ncs_cc'):
       return self._ncs_cc

  def absolute_center_cart(self,
       use_assumed_end = False,
       place_on_grid_point = False,
       use_unit_cell_grid = False):
    '''
     Return center of map (absolute position) in Cartesian coordinates
     A little tricky because for example the map goes from 0 to nx-1, not nx
       If use_assumed_end, go to nx
     Also map could start at non-zero origin
     If place_on_grid_point then guess the end by whether the center ends
       on a grid point
     If use_unit_cell_grid just find center of full unit cell

    '''

    if use_unit_cell_grid:  # Find center of unit cell
      return self.unit_cell_crystal_symmetry().unit_cell().orthogonalize(
        (0.5, 0.5, 0.5))

    elif place_on_grid_point:
      return tuple(col(self.crystal_symmetry().unit_cell().orthogonalize(
        tuple(col([int (0.5*n)/n + o/n for n,o in zip(
          self.map_data().all(),
          self.map_data().origin())])))) - col(self.shift_cart()))

    else:
      if use_assumed_end:
        n_end = 0
      else:
        n_end = 1
      return tuple(col(self.crystal_symmetry().unit_cell().orthogonalize(
        tuple(col([0.5*(n-n_end)/n + o/n for n,o in zip(
          self.map_data().all(),
          self.map_data().origin())])))) - col(self.shift_cart()))


  def map_map_cc(self, other_map_manager):
   ''' Return simple map correlation to other map_manager'''
   import iotbx.map_manager
   assert isinstance(other_map_manager, iotbx.map_manager.map_manager)
   return flex.linear_correlation(
      self.map_data().as_1d(), other_map_manager.map_data().as_1d()
       ).coefficient()


  def find_map_symmetry(self,
      include_helical_symmetry = False,
      symmetry_center = None,
      min_ncs_cc = None,
      symmetry = None,
      ncs_object = None,
      check_crystal_symmetry = True,
      only_proceed_if_crystal_symmetry = False,):

    '''
       Use run_get_symmetry_from_map tool in segment_and_split_map to find
       map symmetry and save it as an mmtbx.ncs.ncs.ncs object

       Here map symmetry is the reconstruction symmetry used to generate the
       map. Normally it is essentially perfect symmetry and normally the
       principal axes are aligned with x,y,z and normally the center is at
       the original center of the map.

       Sets self._warning_message if failure, sets self._ncs_object and
           self._ncs_cc if success

       This procedure may fail if the above assumptions do not hold.
       Optional center of map can be supplied, and minimum NCS correlation
       can also be supplied

       Requires that map_manager is already shifted to place origin at (0, 0, 0)

       Assumes that center of symmetry is at (1/2, 1/2, 1/2) in the full map

       It is optional to include search for helical symmetry. Reason is that
       this is much slower than other symmetries.

       symmetry (symbol such as c1, O, D7) can be supplied and search will be
       limited to that symmetry

       ncs_object can be supplied in which case it is just checked

       If check_crystal_symmetry, try to narrow down possibilities by looking
       for space-group symmetry first

       If only_proceed_if_crystal_symmetry, skip looking if nothing comes up
        with check_crystal_symmetry


    '''

    assert self.origin_is_zero()

    self._warning_message = ""
    self._ncs_cc = None

    from cctbx.maptbx.segment_and_split_map import \
       run_get_ncs_from_map, get_params

    if symmetry is None:
      symmetry = 'ALL'


    if symmetry_center is None:
      # Most likely map center is (1/2,1/2,1/2) in full grid
      symmetry_center = self.absolute_center_cart(use_assumed_end=True)
      # Our map is already shifted, so subtract off shift_cart
      symmetry_center = tuple(
        flex.double(symmetry_center) + flex.double(self.shift_cart()))

    params = get_params(args=[],
      symmetry = symmetry,
      include_helical_symmetry = include_helical_symmetry,
      symmetry_center = symmetry_center,
      min_ncs_cc = min_ncs_cc,
      return_params_only = True,
      )

    space_group_number = None
    if check_crystal_symmetry and symmetry == 'ALL' and (not ncs_object):
      # See if we can narrow it down looking at intensities at low-res
      d_min = 0.05*self.crystal_symmetry().unit_cell().volume()**0.333
      map_coeffs = self.map_as_fourier_coefficients(d_min=d_min)
      from iotbx.map_model_manager import get_map_coeffs_as_fp_phi
      f_array_info = get_map_coeffs_as_fp_phi(map_coeffs, d_min = d_min,
        n_bins = 15)
      ampl = f_array_info.f_array
      data = ampl.customized_copy(
        data = ampl.data(),sigmas = flex.double(ampl.size(),1.))
      from mmtbx.scaling.twin_analyses import symmetry_issues
      si = symmetry_issues(data)
      cs_possibility = si.xs_with_pg_choice_in_standard_setting
      space_group_number = cs_possibility.space_group_number()
      if space_group_number < 2:
        space_group_number = None
      if space_group_number is None and only_proceed_if_crystal_symmetry:
        return # skip looking further

    params.reconstruction_symmetry.\
          must_be_consistent_with_space_group_number = space_group_number
    new_ncs_obj, ncs_cc, ncs_score = run_get_ncs_from_map(params = params,
        map_data = self.map_data(),
        crystal_symmetry = self.crystal_symmetry(),
        out = self.log,
        ncs_obj = ncs_object)
    if (space_group_number) and (not new_ncs_obj):
      # try again without limits
      params.reconstruction_symmetry.\
          must_be_consistent_with_space_group_number = None
      new_ncs_obj, ncs_cc, ncs_score = run_get_ncs_from_map(params = params,
        map_data = self.map_data(),
        crystal_symmetry = self.crystal_symmetry(),
        out = self.log,
        ncs_obj = ncs_object)

    if new_ncs_obj:
      self._ncs_object = new_ncs_obj
      self._ncs_cc = ncs_cc
      self._ncs_object.set_shift_cart(self.shift_cart())
    else:
      self._warning_message = "No map symmetry found; ncs_cc cutoff of %s" %(
        min_ncs_cc)

  def _resample_on_different_grid_and_rebox(self, n_real = None,
       target_grid_spacing = None):
    '''
      Resample the boxed map on a grid of n_real and return new map_manager
      If an ncs_object is present, set its shift_cart too

      Returns new boxed map of similar size
      and location
    '''

    # Get starting lower and upper bounds (current map)
    lower_bounds_cart = self.grid_units_to_cart(self.origin_shift_grid_units)
    upper_bounds_cart = self.grid_units_to_cart(
     [o + a for o,a in zip(self.origin_shift_grid_units,
       self.map_data().all())]
     )

    # Determine if box is cubic
    box_dims = self.map_data().all()
    is_cubic_box = (box_dims[0] == box_dims[1]) and (box_dims[0] == box_dims[2])

    # Save NCS object if any and current shift_cart
    if self.ncs_object():
      working_ncs_object = self.ncs_object().deep_copy()
    else:
      working_ncs_object = None
    working_shift_cart = self.shift_cart()

    # Create full size map so that we can work easily
    mm_boxed_fs = self.as_full_size_map()
    mm_boxed_fs.set_ncs_object(working_ncs_object)

    assert tuple(mm_boxed_fs.origin_shift_grid_units) == (0,0,0)

    # Resample the full size map on new grid
    mm_boxed_fs_resample = mm_boxed_fs.resample_on_different_grid(
       n_real = n_real,
       target_grid_spacing = target_grid_spacing)

    # Now rebox the newly-gridded map

    # New bounds
    lower_bounds = mm_boxed_fs_resample.cart_to_grid_units(lower_bounds_cart)
    upper_bounds = mm_boxed_fs_resample.cart_to_grid_units(upper_bounds_cart)
    if is_cubic_box:
      box_dims = [1 + a - b for a,b in zip (upper_bounds,lower_bounds)]
      min_dim = min(box_dims)
      max_dim = max(box_dims)
      if min_dim != max_dim: # make them the same
       new_upper_bounds = []
       for lb, ub in zip(lower_bounds, upper_bounds):
         new_upper_bounds.append(lb + min_dim - 1)
       upper_bounds = new_upper_bounds
      box_dims = [1 + a - b for a,b in zip (upper_bounds,lower_bounds)]
      assert min(box_dims) == max(box_dims)
    mmm_boxed_fs_resample = mm_boxed_fs_resample.as_map_model_manager()

    mmm_boxed_fs_resample_boxed = \
      mmm_boxed_fs_resample.extract_all_maps_with_bounds(
        lower_bounds=lower_bounds, upper_bounds=upper_bounds)
    return mmm_boxed_fs_resample_boxed.map_manager()

  def resample_on_different_grid(self, n_real = None,
       target_grid_spacing = None):
    '''
      Resample the map on a grid of n_real and return new map_manager
      If an ncs_object is present, set its shift_cart too

      Allows map to be boxed; if so returns new boxed map of similar size
      and location.  If boxed map has cubic gridding, keep that after
      resampling.
    '''

    assert n_real or target_grid_spacing

    if tuple(self.origin_shift_grid_units) != (0,0,0):
      return self._resample_on_different_grid_and_rebox(n_real = n_real,
       target_grid_spacing = target_grid_spacing)

    if n_real is None:
      n_real = []
      for a, nn in zip(self.crystal_symmetry().unit_cell().parameters()[:3],
         self.map_data().all()):
        new_n = int (0.5+ a/target_grid_spacing)
        n_real.append( new_n)

    original_n_real = self.map_data().all()
    original_shift_cart = self.shift_cart()
    original_origin_shift_grid_units = self.origin_shift_grid_units

    map_coeffs = self.map_as_fourier_coefficients()
    map_data=maptbx.map_coefficients_to_map(
        map_coeffs       = map_coeffs,
        crystal_symmetry = map_coeffs.crystal_symmetry(),
        n_real           = n_real)


    new_origin_shift_grid_units = (0,0,0)

    if self.ncs_object():
      new_ncs_object = self.ncs_object().deep_copy()
      new_ncs_object.set_shift_cart(self.shift_cart())
    else:
      new_ncs_object = None
    mm = map_manager(
      map_data = map_data,
      unit_cell_grid = n_real,
      unit_cell_crystal_symmetry = map_coeffs.crystal_symmetry(),
      origin_shift_grid_units = new_origin_shift_grid_units,
      ncs_object = new_ncs_object,
      wrapping = self.wrapping(),
      experiment_type = self.experiment_type(),
      scattering_table = self.scattering_table(),
      resolution = self.resolution(),
     )
    return mm

  def get_boxes_to_tile_map(self,
     target_for_boxes = 24,
     box_cushion = 3,
     get_unique_set_for_boxes = None,
     dist_min = None,
     do_not_go_over_target = None,
     target_xyz_center_list = None,
       ):
    '''
     Return a group_args object with a list of lower_bounds and upper_bounds
     corresponding to a set of boxes that tiles the part of the map that is
     present.  The boxes may not be the same size but will tile to exactly
     cover the existing part of the map.
     Approximately target_for_boxes will be returned (may be fewer or greater)
     Also return boxes with cushion of box_cushion
     If get_unique_set_for_boxes is set, try to use map symmetry to identify
       duplicates and set ncs_object
     If target_xyz_center_list is set, use these points as centers but try
      to use standard box size.
    '''
    assert self.origin_is_zero()
    cushion_nx_ny_nz = tuple([int(0.5 + x * n) for x,n in
       zip(self.crystal_symmetry().unit_cell().fractionalize(
        (box_cushion,box_cushion,box_cushion)),
        self.map_data().all())])
    from cctbx.maptbx.box import get_boxes_to_tile_map
    box_info = get_boxes_to_tile_map(
       target_for_boxes = target_for_boxes,
       n_real = self.map_data().all(),
       crystal_symmetry = self.crystal_symmetry(),
       cushion_nx_ny_nz = cushion_nx_ny_nz,
       wrapping = self.wrapping(),
       do_not_go_over_target = do_not_go_over_target,
       target_xyz_center_list = target_xyz_center_list,
     )
    box_info.ncs_object = None
    if get_unique_set_for_boxes:
      if dist_min:
         max_distance = dist_min
      else:
         max_distance = self.resolution()
      n_before = len(box_info.lower_bounds_list)
      box_info = self._get_unique_box_info(
         box_info = box_info,
         max_distance = max_distance)

    return box_info

  def get_n_real_for_grid_spacing(self, grid_spacing = None):
    '''Identify values of gridding to match target grid spacing'''
    n_real = []
    for n,a in zip(self.map_data().all(),
       self.crystal_symmetry().unit_cell().parameters()):
      spacing = a/n
      target_n = (spacing/grid_spacing) * n
      n_real.append(int(target_n + 0.999))
    return n_real

  def sites_cart_to_sites_cart_absolute(self, sites_cart):
    """ Shift sites_cart that are relative to the boxed map to
        make them relative to the point (0,0,0) in absolute coordinates
   NOTE: sites_cart is a flex.vec3_double array
   NOTE: This is the opposite of sites_cart_absolute_to_sites_cart

   NOTE on shift_cart:

    Position of origin of boxed map:
     origin_position = self.grid_units_to_cart(self.origin_shift_grid_units)
     shift_cart = self.shift_cart() == - origin_position

    If you have Cartesian coordinates xyz for an atom relative to the boxed map,
    the absolute coordinates are:
      coords_abs =  col(xyz) + col(origin_position)
      coords_abs =  col(xyz) - col(self.shift_cart())

    """
    return  sites_cart - col(self.shift_cart())

  def sites_cart_absolute_to_sites_cart(self, sites_cart_absolute):
    """ Shift sites_cart that are in absolute coordinates to make them
        relative to the boxed map.
       NOTE: This is the opposite of sites_cart_to_sites_cart_absolute
       NOTE: sites_cart is a flex.vec3_double array
    """
    return  sites_cart_absolute + col(self.shift_cart())

  def peak_search(self,
      peak_search_level = 3,
      max_peaks = None,
      peak_cutoff            = None,
      interpolate            = True,
      min_distance_sym_equiv = 0,
      general_positions_only = False,
      min_cross_distance     = None,
      min_cubicle_edge       = 5):

    """ Run peak search on this map.
     returns group_args with:
        sites (fractional)
        sites_cart (orthogonal)
        sites_cart_absolute (orthogonal, with shift_cart applied)
        heights
        full_result (original peak_search_result object)

     Note: normally supply at least max_peaks or peak_cutoff

    position of origin of boxed map:
     origin_position = self.grid_units_to_cart(self.origin_shift_grid_units)
     shift_cart = self.shift_cart() == - origin_position

    """
    if peak_cutoff is None and max_peaks is None: # give them 1000
      max_peaks = 1000
    if min_cross_distance is None: # use half resolution
      min_cross_distance = 0.5 * self.resolution()
    if peak_search_level is None:  # this is how finely to search 1 to 3
      peak_searchlevel = 3


    map_data = self.map_data()
    cg = maptbx.crystal_gridding(
      space_group_info = self.crystal_symmetry().space_group_info(),
      symmetry_flags   = maptbx.use_space_group_symmetry,
      unit_cell        = self.crystal_symmetry().unit_cell(),
      pre_determined_n_real = map_data.all())

    # Set parameters for peak peaking and find peaks
    cgt = maptbx.crystal_gridding_tags(gridding = cg)
    peak_search_parameters = maptbx.peak_search_parameters(
      peak_search_level = peak_search_level,
      max_peaks = max_peaks,
      peak_cutoff = peak_cutoff,
      interpolate            = interpolate,
      min_distance_sym_equiv = min_distance_sym_equiv,
      general_positions_only = general_positions_only,
      min_cross_distance     = min_cross_distance,
      min_cubicle_edge       = min_cubicle_edge)
    psr = cgt.peak_search(
      parameters = peak_search_parameters,
      map        = map_data).all(max_clusters = 99999999)
    sites_cart = self.crystal_symmetry().unit_cell().orthogonalize(psr.sites())
    sites_cart_absolute = self.sites_cart_to_sites_cart_absolute(sites_cart)
    result = group_args(group_args_type = 'peak search result',
      full_result = psr,
      heights = psr.heights(),
      sites = psr.sites(),
      sites_cart = sites_cart,
      sites_cart_absolute = sites_cart_absolute,
     )

    return result

  def find_n_highest_grid_points_as_sites_cart(self, n = 0,
    n_tolerance = 0, max_tries = 100):
    '''
      Return the n highest grid points in the map as sites_cart
    '''

    # Find threshold to get exactly n points
    low_bounds = 0.
    high_bounds = 20
    mm = self.deep_copy()
    mm.set_mean_zero_sd_one() # avoid altering the working map
    tries = 0

    # Check ends
    count_high = (mm.map_data() >= high_bounds).count(True)
    count_low = (mm.map_data() >=  low_bounds).count(True)
    if count_low < n or count_high > n:
      return flex.vec3_double()

    last_threshold = None
    while tries < max_tries:
      tries += 1
      threshold = 0.5 * (low_bounds + high_bounds)
      count = (mm.map_data() >= threshold ).count(True)
      if count == n or low_bounds == high_bounds or threshold == last_threshold:
        break
      elif count > n:
        low_bounds = max(low_bounds, threshold)
      else:
        high_bounds = min(high_bounds, threshold)
      last_threshold = threshold
    if abs (count - n ) > n_tolerance:
      return flex.vec3_double()
    # Now convert to xyz and we are done
    sel = (mm.map_data() >= threshold )
    from scitbx.array_family.flex import grid
    g = grid(mm.map_data().all())
    mask_data = flex.int(mm.map_data().size(),0)
    mask_data.reshape(g)
    mask_data.set_selected(sel,1)
    mask_data.set_selected(~sel,0)

    volume_list = flex.int((sel.count(False),sel.count(True)))
    sampling_rates = flex.int((1,1))
    from cctbx.maptbx import sample_all_mask_regions
    sample_regs_obj = maptbx.sample_all_mask_regions(
      mask = mask_data,
      volumes = volume_list,
      sampling_rates = sampling_rates,
      unit_cell = mm.crystal_symmetry().unit_cell())

    return sample_regs_obj.get_array(1)

  def trace_atoms_in_map(self,
       dist_min,
       n_atoms):
     '''
       Utility to find positions where n_atoms atoms separated by
       dist_min can be placed in density in this map
     '''
     assert self.origin_is_zero()
     assert dist_min > 0.01
     assert n_atoms > 0
     n_real = self.get_n_real_for_grid_spacing(grid_spacing = dist_min)
     # temporarily remove origin shift information so we can resample
     origin_shift_grid_units_sav = tuple(self.origin_shift_grid_units)
     if self.ncs_object():
       assert tuple(self.ncs_object().shift_cart()) == tuple(
         self.shift_cart())
       self.ncs_object().set_shift_cart((0,0,0))
     self.origin_shift_grid_units = (0,0,0)
     working_map_manager = self.resample_on_different_grid(n_real = n_real)
     self.origin_shift_grid_units = origin_shift_grid_units_sav
     if self.ncs_object():
       self.ncs_object().set_shift_cart(self.shift_cart())
     return working_map_manager.find_n_highest_grid_points_as_sites_cart(
          n = n_atoms)

  def map_as_fourier_coefficients(self, d_min = None, d_max = None, box=True,
     resolution_factor=1./3, include_000 = True):
    '''
       Convert a map to Fourier coefficients to a resolution of d_min,
       if d_min is provided, otherwise box full of map coefficients
       will be created.

       Filter results with low resolution of d_max if provided

       NOTE: Fourier coefficients are relative the working origin (not
       original origin).  A map calculated from the Fourier coefficients will
       superimpose on the working (current map) without origin shifts.

       This method and fourier_coefficients_as_map_manager interconvert
       map_data and
       map_coefficients without changing origin.  Both are intended for use
       with map_data that has an origin at (0, 0, 0).

       The map coefficients are always in space group P1.
    '''
    assert self.map_data()
    assert self.map_data().origin() == (0, 0, 0)
    # Choose d_min and make sure it is bigger than smallest allowed
    if(d_min is None and not box):
      d_min = maptbx.d_min_from_map(
        map_data  = self.map_data(),
        unit_cell = self.crystal_symmetry().unit_cell(),
        resolution_factor = resolution_factor)
      print("\nResolution of map coefficients using "+\
        "resolution_factor of %.2f: %.1f A\n" %(resolution_factor, d_min),
        file=self.log)
    elif (not box):  # make sure d_min is big enough
      d_min_allowed = maptbx.d_min_from_map(
        map_data  = self.map_data(),
        unit_cell = self.crystal_symmetry().unit_cell(),
        resolution_factor = 0.5)
      if d_min < d_min_allowed:
        print("\nResolution of map coefficients allowed by gridding is %.3f " %(
          d_min_allowed),file=self.log)
        d_min=d_min_allowed
    from cctbx import crystal
    crystal_symmetry = crystal.symmetry(
      self.crystal_symmetry().unit_cell().parameters(), 1)
    ma = miller.structure_factor_box_from_map(
      crystal_symmetry = crystal_symmetry,
      include_000      = include_000,
      map              = self.map_data(),
      d_min            = d_min)
    if(d_max is not None):
      ma = ma.resolution_filter(d_max = d_max)
    return ma

  def fourier_coefficients_as_map_manager(self, map_coeffs):
    '''
       Convert Fourier coefficients into to a real-space map with gridding
        matching this existing map_manager.  Returns a map_manager object.

       Requires that this map_manager has origin at (0, 0, 0) (i.e.,
       shift_origin() has been applied if necessary)

       NOTE: Assumes that the map_coeffs are in the same frame of reference
       as this map_manager (i.e., similar to those that would be written out
       using map_as_fourier_coefficients).
    '''

    assert isinstance(map_coeffs, miller.array)
    assert isinstance(map_coeffs.data(), flex.complex_double)
    assert self.map_data() and self.map_data().origin() == (0, 0, 0)

    return self.customized_copy(
      map_data=maptbx.map_coefficients_to_map(
        map_coeffs       = map_coeffs,
        crystal_symmetry = map_coeffs.crystal_symmetry(),
        n_real           = self.map_data().all())
      )

  def shift_aware_rt(self,
     from_obj = None,
     to_obj = None,
     working_rt_info = None,
     absolute_rt_info = None):
   '''
   Returns shift_aware_rt object

   Uses rt_info objects (group_args with members of r, t).

   Simplifies keeping track of rotation/translation between two
    objects that each may have an offset from absolute coordinates.

   absolute rt is rotation/translation when everything is in original,
      absolute Cartesian coordinates.

   working_rt is rotation/translation of anything in "from_obj" object
      to anything in "to_obj" object using working coordinates in each.

   Usage:
   shift_aware_rt = self.shift_aware_rt(absolute_rt_info = rt_info)
   shift_aware_rt = self.shift_aware_rt(working_rt_info = rt_info,
      from_obj=from_obj, to_obj = to_obj)

   apply RT using working coordinates in objects
   sites_cart_to_obj = shift_aware_rt.apply_rt(sites_cart_from_obj,
      from_obj=from_obj, to_obj=to_obj)

   apply RT absolute coordinates
   sites_cart_to = shift_aware_rt.apply_rt(sites_cart_from)

   '''
   return shift_aware_rt(
     from_obj = from_obj,
     to_obj = to_obj,
     working_rt_info = working_rt_info,
     absolute_rt_info = absolute_rt_info)

  def _get_unique_box_info(self, box_info, max_distance = 1):
    if self.ncs_object() is None:
      # try to get map symmetry but do not try too hard..
      try:
        self.find_map_symmetry()
      except Exception as e:
        pass
    if not self.ncs_object() or self.ncs_object().max_operators()<2:
      return box_info # nothing to do

    box_info.ncs_object = self.ncs_object() # save it

    # Get just the unique parts of this box (apply symmetry later)
    new_lower_bounds_list = []
    new_upper_bounds_list = []
    new_lower_bounds_with_cushion_list = []
    new_upper_bounds_with_cushion_list = []
    existing_xyz_list = flex.vec3_double()
    existing_unique_xyz_list = flex.vec3_double()
    from scitbx.matrix import col
    for lower_bounds, upper_bounds,lower_bounds_with_cushion, \
      upper_bounds_with_cushion in zip (
        box_info.lower_bounds_list,
        box_info.upper_bounds_list,
        box_info.lower_bounds_with_cushion_list,
        box_info.upper_bounds_with_cushion_list,
      ):
      # NOTE: lower_bounds, upper_bounds are relative to the working
      #    map_data with origin at (0,0,0).  Our ncs_object is also
      #    relative to this same origin

      xyz = tuple([ a * 0.5*(lb+ub-1) / n for a, lb, ub, n in zip(
         self.crystal_symmetry().unit_cell().parameters()[:3],
         lower_bounds,
         upper_bounds,
         self.map_data().all())])
      target_site = flex.vec3_double((xyz,))
      ncs_object = self.ncs_object()
      if existing_xyz_list.size() > 0 :
       dist_n, id1_n, id2_n = target_site.min_distance_between_any_pair_with_id(
              existing_xyz_list)
      else:
        dist_n = 1.e+30
      if dist_n <= max_distance:  # duplicate
        pass
      else:
        ncs_sites = ncs_object.apply_ncs_to_sites( sites_cart=target_site)
        existing_xyz_list.extend(ncs_sites)
        existing_unique_xyz_list.extend(
          flex.vec3_double((xyz,)*ncs_sites.size()))
        new_lower_bounds_list.append(lower_bounds)
        new_upper_bounds_list.append(upper_bounds)
        new_lower_bounds_with_cushion_list.append(lower_bounds_with_cushion)
        new_upper_bounds_with_cushion_list.append(upper_bounds_with_cushion)

    box_info.lower_bounds_list = new_lower_bounds_list
    box_info.upper_bounds_list = new_upper_bounds_list
    box_info.lower_bounds_with_cushion_list = new_lower_bounds_with_cushion_list
    box_info.upper_bounds_with_cushion_list = new_upper_bounds_with_cushion_list

    return box_info

#   Methods for map_manager

class shift_aware_rt:
  '''
  Class to simplify keeping track of rotation/translation between two
  objects that each may have an offset from absolute coordinates.

  Basic idea:  absolute rt is rotation/translation when everything is in
  original, absolute Cartesian coordinates.

  working_rt is rotation/translation of anything in "from_obj" object to anything
   in "to_obj" object using working coordinates in each.

  The from_obj and to objects must have a shift_cart method
  '''

  def __init__(self,
     from_obj = None,
     to_obj = None,
     working_rt_info = None,
     absolute_rt_info = None):

     assert (
      (absolute_rt_info and (not from_obj) and (not to_obj) and (not working_rt_info))
      or
      (from_obj and to_obj and working_rt_info))

     if from_obj:
       assert hasattr(from_obj, 'shift_cart')
     if to_obj:
       assert hasattr(to_obj, 'shift_cart')

     if not absolute_rt_info:
       absolute_rt_info = self.get_absolute_rt_info(
         working_rt_info = working_rt_info,
         from_obj = from_obj, to_obj = to_obj)

     self._absolute_rt_info = group_args(
        r =  absolute_rt_info.r,
        t =  absolute_rt_info.t,)


  def is_similar(self, other_shift_aware_rt_info, tol = 0.001):
    '''Check whether this shift_aware_rt is similar to another one'''
    r = self._absolute_rt_info.r
    t = self._absolute_rt_info.t
    other_r = other_shift_aware_rt_info._absolute_rt_info.r
    other_t = other_shift_aware_rt_info._absolute_rt_info.t
    for x,y in zip(r,other_r):
      if (abs(x-y)) > tol:
        print(x,y,abs(x-y))
        return False
    for x,y in zip(t,other_t):
      if (abs(x-y)) > tol:
        print(x,y,abs(x-y))
        return False
    return True

  def apply_rt(self, site_cart = None, sites_cart = None,
    from_obj = None, to_obj = None):
    '''
    Apply absolute rt if from and to not specified.
    Apply relative if specified
    '''
    # get absolute if from and to not specified, otherwise working
    rt_info = self.working_rt_info(from_obj=from_obj, to_obj=to_obj)
    if site_cart:
      return rt_info.r * col(site_cart) + rt_info.t

    else:
      return rt_info.r.elems * sites_cart + rt_info.t.elems

  def get_absolute_rt_info(self, working_rt_info = None,
      from_obj = None, to_obj = None):

    '''
    working_rt_info describes how to map from_xyz -> to_xyz in local coordinates
    from_xyz is shifted from absolute by from.shift_cart()
    to_xyz is shifted from absolute by to.shift_cart()

    We have:
      r from_xyz + t = to_xyz    in working frame of reference

    We want to describe how to map:
       (from_xyz - from.shift_cart()) -> (to_xyz - to.shift_cart())
    where r is going to be the same and T will be different than t
       r ((from_xyz - from.shift_cart()) + T = (to_xyz - to.shift_cart())
       T = (to_xyz - to.shift_cart() - r from_xyz + r from.shift_cart()
         but: to_xyz -  r from_xyz = t
       T =  t - to.shift_cart() + r from.shift_cart()

    Note reverse:
       t = T + to.shift_cart() - r from.shift_cart()
    '''

    r = working_rt_info.r
    t = working_rt_info.t
    new_t =  t -  col(to_obj.shift_cart())  + r * col(from_obj.shift_cart())

    return group_args(
      r = r,
      t = new_t
    )

  def working_rt_info(self, from_obj=None, to_obj=None):
    ''' Get rt in working frame of reference
    '''
    if (not from_obj) and (not to_obj):  # as is
      return self._absolute_rt_info

    assert hasattr(from_obj, 'shift_cart')
    assert hasattr(to_obj, 'shift_cart')
    r = self._absolute_rt_info.r
    t = self._absolute_rt_info.t
    working_t =  t + col(to_obj.shift_cart()) - r * col(from_obj.shift_cart())
    return group_args(
      r = r,
      t = working_t)


  def absolute_rt_info(self):
    '''Return the absolute RT info for this shift_aware_rt object'''
    return self._absolute_rt_info


  def inverse(self):
    '''Return the inverse for this shift_aware_rt object'''
    r = self._absolute_rt_info.r
    t = self._absolute_rt_info.t

    r_inv = r.inverse()
    t_inv = - r_inv * t
    inverse_absolute_rt_info = group_args(
      r = r_inv,
      t = t_inv,)

    return shift_aware_rt(absolute_rt_info = inverse_absolute_rt_info)


def dummy_map_manager(crystal_symmetry, n_grid = 12):
  '''
   Make a map manager with crystal symmetry and unit sized map
  '''

  map_data = flex.double(n_grid*n_grid*n_grid,1)
  acc = flex.grid((n_grid, n_grid, n_grid))
  map_data.reshape(acc)
  mm = map_manager(
    map_data = map_data,
    unit_cell_grid = (n_grid, n_grid, n_grid),
    unit_cell_crystal_symmetry = crystal_symmetry,
    wrapping = False)
  mm.set_resolution(min(crystal_symmetry.unit_cell().parameters()[:3])/n_grid)
  mm._is_dummy_map_manager = True
  return mm


def get_indices_from_index(index = None, all = None):
        '''Get indices (in a 3D map) for a grid point with given 1D index'''
        #index = k+j*all[2]+i*(all[1]*all[2])
        i = index//(all[1]*all[2])
        j =  (index-i*(all[1]*all[2]))//all[2]
        k =  index-i*(all[1]*all[2])-j*all[2]
        assert k+j*all[2]+i*(all[1]*all[2]) == index
        return (i, j, k)

def get_sites_cart_from_index(
      indices_list = None,
      points = None, map_data = None, crystal_symmetry = None, all = None):
    '''  Get sites_cart from linear (1d) map indices.
       Supply either map_data or all to provide n_real
       crystal_symmetry is required
       Supply either 3D indices (i,j,k) or 1-D indices (points)
    '''

    if all is None:
      all = map_data.all()
    sites_frac = flex.vec3_double()
    if not indices_list:
      if not points: return sites_frac # nothing there
      indices_list = []
      for point in points:
        if point is None: continue
        indices_list.append(get_indices_from_index(index = point, all = all))
    for indices in indices_list:
      i, j, k = indices
      site_frac = tuple((i/all[0], j/all[1], k/all[2]))
      sites_frac.append(site_frac)
    sites_cart = crystal_symmetry.unit_cell().orthogonalize(sites_frac)
    return sites_cart

def _round_tuple_int(t):
  new_t = []
  for x in t:
    new_t.append(int(round(x)))
  return new_t

def add_tuples_int(t1, t2):
  ''' Add two tuples (can be integers or floats)'''
  try:
    return tuple(flex.int(t1)+flex.int(t2))
  except Exception as e: # not integers
    return tuple(
       flex.int(_round_tuple_int(t1)) + flex.int(_round_tuple_int(t2)))

def subtract_tuples_int(t1, t2):
  try:
    return tuple(flex.int(t1)-flex.int(t2))
  except Exception as e: # not integers
    return tuple(
       flex.int(_round_tuple_int(t1)) - flex.int(_round_tuple_int(t2)))

def remove_site_with_most_neighbors(sites_cart):
  '''Remove the site with the most neighbors'''
  useful_norms_list = []
  closest_distance = 1.e+30
  for i in range(sites_cart.size()):
    compare_xyz = flex.vec3_double(sites_cart.size(), sites_cart[i])
    delta_xyz = sites_cart - compare_xyz
    norms = delta_xyz.norms()
    useful_norms = norms[:i]
    useful_norms.extend(norms[i+1:])
    assert useful_norms.size() == sites_cart.size() -1
    useful_norms_list.append(useful_norms)
    closest_distance=min(closest_distance,useful_norms.min_max_mean().min)

  distance_list=[]
  for i in range(sites_cart.size()):
    useful_norms = useful_norms_list[i]
    s = (useful_norms <= closest_distance * 1.25)
    count = s.count(True)
    distance_list.append([count,i])
  distance_list.sort()
  distance_list.reverse()
  i = distance_list[0][1]
  new_sites_cart = sites_cart[:i]
  new_sites_cart.extend(sites_cart[i+1:])
  return new_sites_cart

def select_n_in_biggest_cluster(sites_cart,
   dist_min = None,
   n = None,
   dist_min_ratio = 1.,
   dist_min_ratio_min = 0.5,
   minimize_density_of_points = None):
  '''
    Select n of sites_cart, taking those near biggest cluster if possible
    If minimize_density_of_points, remove those with the most neighbors
  '''

  if sites_cart.size() < 1:
    return sites_cart

  if minimize_density_of_points:
    while sites_cart.size() > n:
      sites_cart = remove_site_with_most_neighbors(sites_cart)
    return sites_cart

  # Guess size of cluster (n atoms, separated by about dist_min)
  target_radius = dist_min * float(n)**0.5
  dist_list = []
  for i in range (sites_cart.size()):
    diffs = sites_cart.deep_copy() - col(sites_cart[i])
    norms = diffs.norms()
    sel = (norms <= target_radius)
    dist_list.append([sel.count(True),i])
  dist_list.sort()
  dist_list.reverse()
  i = dist_list[0][1]

  # Now take the n points close to center_point but separated from
  #  each other and we are done
  diffs = sites_cart.deep_copy() - col(sites_cart[i])
  norms = diffs.norms()  # how close each one is to the center point
  used_sites=flex.bool(sites_cart.size(), False)

  new_sites_cart = flex.vec3_double()
  unused_sites_cart = flex.vec3_double()
  for j in range(n):  # pick closest to center_point that is at least
                      # dist_min from all in new_sites_cart
    found = False
    for k in range(n):
      if found: break # go on to next
      if used_sites[k]: continue
      ok = False
      test_sites = sites_cart[k:k+1]
      if new_sites_cart.size() == 0:
        ok = True
      else:
        dist, id1, id2= new_sites_cart.min_distance_between_any_pair_with_id(
            test_sites)
        if dist >= dist_min_ratio*dist_min: # keep it
           ok = True
      if ok:
        new_sites_cart.append(test_sites[0])
        used_sites[k] = True
        found = True # go on to next one
    if not found:  # didn't get anything ... reduce dist_min_ratio
      if dist_min_ratio >= dist_min_ratio_min:
        return select_n_in_biggest_cluster(sites_cart,
          dist_min = dist_min,
          n = n,
          dist_min_ratio = dist_min_ratio * 0.9)

  return new_sites_cart


 *******************************************************************************
