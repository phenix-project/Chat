

 *******************************************************************************
simtbx/__init__.py
from __future__ import division
from simtbx import nanoBragg # fix for issue #868

def get_exascale(interface, context):
  if context == "kokkos_gpu":
    from simtbx.kokkos import gpu_instance, gpu_energy_channels, gpu_detector, exascale_api
  elif context == "cuda":
    from simtbx.gpu import gpu_instance, gpu_energy_channels, gpu_detector, exascale_api
  else: raise NotImplementedError(context)

  return dict(gpu_instance = gpu_instance, gpu_energy_channels = gpu_energy_channels,
              gpu_detector = gpu_detector, exascale_api = exascale_api)[interface]



 *******************************************************************************


 *******************************************************************************
simtbx/command_line/complete_an_F.py
from __future__ import division, print_function
from argparse import ArgumentParser
parser = ArgumentParser()
parser.add_argument("mtzin", help="input mtz file", type=str)
parser.add_argument("mtzout", help="output mtz file", type=str)
args = parser.parse_args()

# LIBTBX_SET_DISPATCHER_NAME diffBragg.completeF

import numpy as np
from iotbx.reflection_file_reader import any_reflection_file
from dials.array_family import flex
from scipy.interpolate import interp1d
from cctbx import miller

F = any_reflection_file(args.mtzin).as_miller_arrays()[0]
if not F.is_xray_amplitude_array():
    F = F.as_amplitude_array()
assert F.is_xray_amplitude_array()

print("Bin-ID    Res-range    Completeness    #ASU-indices")
F.show_completeness()
d_max,d_min = F.resolution_range()
print("d_min, d_max (Angstrom): ", d_min, d_max)
mset_full = F.build_miller_set(True, d_min=d_min)
mset_full_d = {h: d for h,d in zip(mset_full.d_spacings().indices(), mset_full.d_spacings().data())}
Fmap = {h:val for h,val in zip(F.indices(), F.data())}
xvals = np.array(F.d_spacings().data())
yvals = np.array(F.data())
fill_vals = yvals[np.argmin(xvals)], yvals[np.argmax(xvals)]
I = interp1d(xvals, yvals, fill_value=fill_vals, bounds_error=False)
data = []
for h in mset_full.indices():
    if h not in Fmap:
        d_h = mset_full_d[h]
        amp = I(d_h)
    else:
        amp = Fmap[h]
    data.append(amp)

complete_amps = flex.double(data)
complete_inds = mset_full.indices()
ma = miller.array(mset_full, complete_amps)
if not ma.is_xray_amplitude_array():
    ma = ma.set_observation_type_xray_amplitude()
ma = ma.as_anomalous_array()
assert ma.anomalous_flag()
ma.as_mtz_dataset(column_root_label="F").mtz_object().write(args.mtzout)


 *******************************************************************************


 *******************************************************************************
simtbx/command_line/errors.py
from __future__ import division

# LIBTBX_SET_DISPATCHER_NAME diffBragg.errors

from argparse import ArgumentParser
parser = ArgumentParser()
parser.add_argument("phil", type=str, help="the phil file used for per spot refinement")
parser.add_argument("indirs", type=str, nargs="+", help="hopper output folders with expers inside")
parser.add_argument("outdir", type=str, help="output folder where integration tables will be saved")
parser.add_argument("--ndev", type=int, default=1, help="number of gpu devices")
args = parser.parse_args()

from libtbx.mpi4py import MPI
COMM = MPI.COMM_WORLD

import os
from simtbx.diffBragg.parameter_errors import get_errors
import glob

if COMM.rank==0:
    if not os.path.exists(args.outdir):
        os.makedirs(args.outdir)

fnames = []
for dirname in args.indirs:
    fnames += glob.glob(dirname + "/expers/rank*/*expt")
if COMM.rank==0:
    print("Found %d expts total" %len(fnames))

failed_assert = []
devid = COMM.rank % args.ndev

for i,exp_f in enumerate(fnames):

    if i% COMM.size != COMM.rank:
            continue

    ref_f = exp_f.replace("expers/", "refls/").replace(".expt", ".refl")
    assert os.path.exists(ref_f)
    pkl_f = exp_f.replace("expers/", "pandas/").replace(".expt", ".pkl")
    assert os.path.exists(pkl_f)
    out_f = os.path.join(args.outdir + "/shot%d"% i)
    print("expt %s, shot %d on GPU device %d" % (out_f,i, devid  ))

    try:
        get_errors(args.phil, exp_f, ref_f, pkl_f, out_f, verbose=False, devid=devid)
    except AssertionError:
        failed_assert.append(exp_f)

failed_assert = COMM.reduce(failed_assert)
if COMM.rank==0:
        print("the following expts failed(!):", failed_assert)


 *******************************************************************************


 *******************************************************************************
simtbx/command_line/estimate_Ncells_Eta.py
from __future__ import division

# LIBTBX_SET_DISPATCHER_NAME diffBragg.estimate_Ncells_Eta

from argparse import ArgumentParser
parser = ArgumentParser()
parser.add_argument("dirname", help="still process output folder", type=str, nargs="+")
parser.add_argument("--updatePhil", default=None, help="name of an exisiting stage 1 phil file  to update (just the init.Ncells portion)", type=str)
parser.add_argument("--expSuffix", help="extension of refined experiments (default: _refined.expt)", type=str, default="_refined.expt")
parser.add_argument("--thresh", type=float, default=7, help="MAD score for outliers (default=7 standard deviation above the median)")
parser.add_argument("--useMean", action="store_true", help="set Eta and Nabc using the mean (default is median)")
parser.add_argument("--NabcMax",  type=float, default=70, help="If estaimated Nabc is above this value, it will set to this value")
parser.add_argument("--NabcMin",  type=float, default=5, help="If estaimated Nabc is BELOW this value, it will be set to this value")
parser.add_argument("--EtaMax", type=float, default=0.5, help="If estimated Eta is above this range, it will be set to this value")
parser.add_argument("--EtaMin", type=float, default=1e-3, help="If estimated Eta is BELOW this range, it will be set to this value")

#parser.add_argument("--njobs", type=int, default=5, help="number of jobs (only runs on single node, no MPI)")
parser.add_argument("--plot", action="store_true", help="show a histogram at the end")
args = parser.parse_args()
from libtbx.mpi4py import MPI
COMM = MPI.COMM_WORLD
#from joblib import Parallel, delayed
import json
import numpy as np
from cctbx import uctbx
from scitbx.matrix import sqr
import os
import glob
from dxtbx.model import ExperimentList

fnames = []
for dirname in args.dirname:
    glob_s = os.path.join(dirname, "*%s" % args.expSuffix)
    fnames += glob.glob(glob_s)
if not fnames:
    if COMM.rank==0:
        print("no fnames")
    exit()

#def main(jid):
all_Ns = []
all_mos_spreads = []
for i, f in enumerate(fnames):
    if i % COMM.size != COMM.rank:
        continue
    print(f)
    #Cs = ExperimentList.from_file(f, False).crystals()
    Cs = json.load(open(f, 'r'))['crystal']
    dom_sizes = np.array([C['ML_domain_size_ang'] for C in Cs])
    mos_spreads = [2*C['ML_half_mosaicity_deg'] for C in Cs]
    uc_vols = []
    for C in Cs:
        a = C['real_space_a']
        b = C['real_space_b']
        c = C['real_space_c']
        uc = uctbx.unit_cell(orthogonalization_matrix=sqr(a + b + c).transpose())
        uc_vols.append(uc.volume())

    Ns = dom_sizes / np.power(uc_vols, 1 / 3.)
    all_Ns += list(Ns)
    all_mos_spreads += list(mos_spreads)
#    return all_Ns, all_mos_spreads

all_Ns = COMM.reduce(all_Ns)
all_mos_spreads = COMM.reduce(all_mos_spreads)
#results = Parallel(n_jobs=args.njobs)(delayed(main)(j) for j in range(args.njobs))
#all_Ns = []
#all_mos_spreads = []
#for N,mos in results:
#    all_Ns += N
#    all_mos_spreads += mos
# template of the additional phil:
phil = """\ninit {{
  Nabc = [{n},{n},{n}]
  eta_abc = [{m},{m},{m}]
}}\n
"""

if COMM.rank==0:
    print("Obtained %d estimates ..." % len(all_Ns))
    import pandas
    import pylab as plt
    from simtbx.diffBragg import utils
    all_Ns = np.array(all_Ns)
    all_mos_spreads = np.array(all_mos_spreads)
    bad_Ns = utils.is_outlier(all_Ns, args.thresh)
    bad_mos_spreads = utils.is_outlier(all_mos_spreads, args.thresh)
    is_bad = np.logical_or(bad_Ns, bad_mos_spreads)
    print("Removing %d outlier estiamtes" % is_bad.sum())
    all_Ns = all_Ns[~is_bad]
    all_mos_spreads = all_mos_spreads[~is_bad]

    df = pandas.DataFrame({"Ncells": all_Ns, "mos_spread_deg": all_mos_spreads})
    print(df.Ncells.describe())
    print(df.mos_spread_deg.describe())
    if args.useMean:
        mean_N = df.Ncells.mean()
        mean_mos = df.mos_spread_deg.mean()
    else:
        mean_N = df.Ncells.median()
        mean_mos = df.mos_spread_deg.median()
    print("mean Ncells=%f" % mean_N)
    print("mean mos_spread=%f (deg.)" % mean_mos)

    if mean_mos > args.EtaMax or mean_mos < args.EtaMin:
        temp = mean_mos
        mean_mos = args.EtaMax if mean_mos > args.EtaMax else args.EtaMin
        print("Estimated Eta=%f, setting it to %f" % (temp, mean_mos))
    if mean_N > args.NabcMax or mean_N < args.NabcMin:
        temp = mean_N
        mean_N = args.NabcMax if mean_N > args.NabcMax else args.NabcMin
        print("Estimated N=%f, setting it to %f" %(temp, mean_N))

    phil = phil.format(n=round(mean_N,4), m=mean_mos)

    if args.updatePhil is not None:
        with open(args.updatePhil, "r") as o:
            s = o.read()
        s += phil
        with open(args.updatePhil, "w") as o:
            o.write(s)
    if args.plot:
        df.hist(bins=100, log=True)
        plt.show()


 *******************************************************************************


 *******************************************************************************
simtbx/command_line/filt_refls.py
from __future__ import division

# LIBTBX_SET_DISPATCHER_NAME diffBragg.filt_refls

import argparse as ap

parser = ap.ArgumentParser(formatter_class=ap.ArgumentDefaultsHelpFormatter)
parser.add_argument("input", type=str, help="hopper output dir (simtbx.diffBragg.hopper) or a pandas dataframe (combined pickle files from hopper output folder)")
parser.add_argument("out", type=str, help="output pickle filename, which will then be a suitable input for diffBragg.geometry_refiner")
parser.add_argument("--thresh", type=float, default=10, help="MAD score threshold for outliers. Lower to remove more outliers")
parser.add_argument("--tag", type=str, default="filt", help="New refl tables will be written with this tag added to end of the filename")
parser.add_argument("--ers", type=str, default=None, help="if provided, write an exp-ref-spec file suitable input for simtbx.diffBragg.hopper")
parser.add_argument("--mind", type=float, default=None, help="if provided, filter shots whose median prediction offset is above this number (units=pixels)")
parser.add_argument("--minN", type=int, default=None, help="filter expts if number of refls is below this number")
parser.add_argument("--reflMaxD", type=float, default=None)

args = parser.parse_args()


import pandas
import glob
import h5py
from dials.array_family import flex
import numpy as np
from simtbx.diffBragg import utils

try:
    fnames = glob.glob(args.input + "/pandas/rank*/*.pkl")
    assert fnames
    comb_df = pandas.concat([pandas.read_pickle(f) for f in fnames])
except Exception:
    comb_df = pandas.read_pickle(args.input)
comb_df.reset_index(inplace=True, drop=True)

R2names = []

def get_dist_from_R(R):
    """ returns prediction offset, R is reflection table"""
    x,y,_ = R['xyzobs.px.value'].parts()
    x2,y2,_ = R['xyzcal.px'].parts()
    dist = np.sqrt((x-x2)**2 + (y-y2)**2)
    return dist

from libtbx.mpi4py import MPI
COMM = MPI.COMM_WORLD

# one ranks dataframe
df = np.array_split(comb_df, COMM.size)[COMM.rank]

keep = []
n = 0
n2 = 0
all_pred_off = []
for i in range(len(df)):
    #if i % COMM.size != COMM.rank:
    #    continue
    row = df.iloc[i]
    h = h5py.File(row.stage1_output_img, 'r')
    Rname = row.opt_exp_name.replace("/expers/", "/refls/").replace(".expt", ".refl")
    R = flex.reflection_table.from_file(Rname)
    all_dists = get_dist_from_R(R)
    bad_dists = np.zeros(len(R), bool)
    if args.reflMaxD is not None:
        bad_dists = np.array(all_dists) > args.reflMaxD

    d = -1
    if len(all_dists) > 0:
        d = np.median(all_dists)
    K = True
    if args.mind is not None:
        K =  d < args.mind
    keep.append(K)

    vals = h['sigmaZ_vals'][()]
    vals[np.isnan(vals)] = np.inf

    bad = list(np.where(utils.is_outlier(vals, args.thresh))[0])
    sel = [R[i_r]['h5_roi_idx'] not in bad for i_r in range(len(R))]

    sel = np.logical_and(sel, ~bad_dists)
    R2 = R.select(flex.bool(sel))
    d2 = -1
    if len(R2)> 0:
        d2 = np.median(get_dist_from_R(R2))
    if args.minN is not None and  len(R2) < args.minN:
        keep[-1] = False

    if d < -1 or d2 < -1:
        keep[-1] = False

    if args.tag is not None and K:
        R2name = Rname.replace(".refl", "_%s.refl" % args.tag)
        R2.as_file(R2name)
        if COMM.rank==0:
            print(i, len(df), "New refl table written=%s" % R2name)
    else:
        R2name = Rname.replace(".refl", "_KEEP=False.refl")
    R2names.append(R2name)
    all_pred_off.append(d2)
    if keep[-1]:
        n += len(R)
        n2 += len(R2)


df['filtered_refls'] = R2names
df['pred_offsets'] = all_pred_off
df = df.loc[keep]
rank_dfs = COMM.gather(df)
n = COMM.reduce(n)
n2 = COMM.reduce(n2)
if COMM.rank==0:
    df = pandas.concat(rank_dfs)
    df.reset_index(inplace=True, drop=True)
    df.to_pickle(args.out)
    print("\nSummary\n<><><><><>")
    print("%d expts tot" % len(df))
    print("Filtered refls have median pred offset=%.3f pix" % df.pred_offsets.median())
    print("Wrote %s which can be passed into diffBragg.geometry_refiner  input_pickle=%s" % (args.out, args.out))
    print("Kept %d / %d refls. Removed %.2f %% "
        % (n2, n, (n-n2)/float(n)*100. ))
    if args.ers is not None:
        with open(args.ers, "w") as ersFile:
            for e, r, s in df[["exp_name", "filtered_refls", "spectrum_filename"]].values:
                if s is None:
                    s = ""
                ersFile.write("%s %s %s\n" % (e,r,s))
        print("Wrote %s which can be passed into simtbx.diffBragg.hopper exp_ref_spec_file=%s" % (args.ers, args.ers))
    with open(args.out +".exectution.txt", "w") as o:
        import sys,os
        o.write("filt_refls was run from folder: %s\n" % os.getcwd())
        o.write("The command line input was:\n")
        o.write(" ".join(sys.argv))


 *******************************************************************************


 *******************************************************************************
simtbx/command_line/geometry_refiner.py
from __future__ import print_function, division

# LIBTBX_SET_DISPATCHER_NAME diffBragg.geometry_refiner
from argparse import ArgumentParser
parser = ArgumentParser()
parser.add_argument("--phil", type=str, required=True, help="path to a phil string")
parser.add_argument("--cmdlinePhil", nargs="+", default=None, type=str, help="command line phil params")
progargs = parser.parse_args()

import sys

from libtbx.mpi4py import MPI
from libtbx.phil import parse
from simtbx.diffBragg.device import DeviceWrapper
from simtbx.diffBragg.phil import philz, hopper_phil

COMM = MPI.COMM_WORLD

phil_scope = parse(philz + hopper_phil)
arg_interp = phil_scope.command_line_argument_interpreter(home_scope="")

phil_file = open(progargs.phil, "r").read()
user_phil = parse(phil_file)
phil_sources = [user_phil]

if progargs.cmdlinePhil is not None:
    command_line_phils = [arg_interp.process(phil) for phil in progargs.cmdlinePhil]
    phil_sources += command_line_phils

working_phil, unused = phil_scope.fetch(sources=phil_sources, track_unused_definitions=True)
for loc in unused:
    print("WARNING: unused phil:", loc)
params = working_phil.extract()
device_Id = COMM.rank % params.refiner.num_devices
with DeviceWrapper(device_Id) as _:
    from simtbx.diffBragg.refiners.geometry import geom_min
    from simtbx.diffBragg.utils import find_diffBragg_instances

    if COMM.rank > 0:
        sys.tracebacklimit = 0
    geom_min(params)
    for name in find_diffBragg_instances(globals()): del globals()[name]


 *******************************************************************************


 *******************************************************************************
simtbx/command_line/hopper.py
from __future__ import absolute_import, division, print_function
import socket
import glob
from simtbx.diffBragg import hopper_utils
from dxtbx.model.experiment_list import ExperimentListFactory
import time
import sys
try:
    import pandas
except ImportError:
    print("Please install pandas, libtbx.python -m pip install pandas")
    exit()

try:
    from line_profiler import LineProfiler
except ImportError:
    LineProfiler = None

from simtbx.diffBragg.device import DeviceWrapper

ROTX_ID = 0
ROTY_ID = 1
ROTZ_ID = 2
NCELLS_ID = 9
UCELL_ID_OFFSET = 3
DETZ_ID = 10

# LIBTBX_SET_DISPATCHER_NAME simtbx.diffBragg.hopper
# LIBTBX_SET_DISPATCHER_NAME hopper

import numpy as np
np.seterr(invalid='ignore')
import os
from libtbx.mpi4py import MPI

COMM = MPI.COMM_WORLD
# TODO, figure out why next 3 lines are sometimes necessary?!
if not hasattr(COMM, "rank"):
    COMM.rank = 0
    COMM.size = 1
from libtbx.phil import parse

from simtbx.diffBragg import utils
from simtbx.diffBragg.phil import philz

import logging
from simtbx.diffBragg.phil import hopper_phil


philz = hopper_phil + philz
phil_scope = parse(philz)



class Script:
    def __init__(self):
        from dials.util.options import ArgumentParser

        self.params = None
        if COMM.rank == 0:
            self.parser = ArgumentParser(
                usage="",  # stage 1 (per-shot) diffBragg refinement",
                sort_options=True,
                phil=phil_scope,
                read_experiments=True,
                read_reflections=True,
                check_format=False,
                epilog="PyCuties")
            self.params, _ = self.parser.parse_args(show_diff_phil=True)
            assert self.params.outdir is not None
            utils.safe_makedirs(self.params.outdir)
            ts = time.strftime("%Y%m%d-%H%M%S")
            diff_phil_outname = os.path.join(self.params.outdir, "diff_phil_run_at_%s.txt" % ts)
            with open(diff_phil_outname, "w") as o:
                o.write("command line:\n%s\n" % (" ".join(sys.argv)))
                o.write("workding directory: \n%s\n" %os.getcwd())
                o.write("diff phil:\n")
                o.write(self.parser.diff_phil.as_str())
            just_diff_phil_outname = os.path.join(self.params.outdir, "diff.phil")
            with open(just_diff_phil_outname, "w") as o:
                o.write(self.parser.diff_phil.as_str())
        self.params = COMM.bcast(self.params)

        self.dev = COMM.rank % self.params.refiner.num_devices
        logging.info("Rank %d will use device %d on host %s" % (COMM.rank, self.dev, socket.gethostname()))

        if self.params.logging.logname is None:
            self.params.logging.logname = "main_stage1.log"
        if self.params.profile_name is None:
            self.params.profile_name = "prof_stage1.log"
        from simtbx.diffBragg import mpi_logger
        mpi_logger.setup_logging_from_params(self.params)

    def run(self):
        MAIN_LOGGER = logging.getLogger("diffBragg.main")
        assert os.path.exists(self.params.exp_ref_spec_file)
        input_lines = None
        best_models = None
        pd_dir = os.path.join(self.params.outdir, "pandas")
        if COMM.rank == 0:
            input_lines = open(self.params.exp_ref_spec_file, "r").readlines()
            if self.params.skip is not None:
                input_lines = input_lines[self.params.skip:]
            if self.params.first_n is not None:
                input_lines = input_lines[:self.params.first_n]
            if self.params.sanity_test_input:
                hopper_utils.sanity_test_input_lines(input_lines)

            if self.params.best_pickle is not None:
                logging.info("reading pickle %s" % self.params.best_pickle)
                best_models = pandas.read_pickle(self.params.best_pickle)

            if self.params.dump_gathers:
                if self.params.gathers_dir is None:
                    raise ValueError("Need to provide a file dir path in order to dump_gathers")
                utils.safe_makedirs(self.params.gathers_dir)

            utils.safe_makedirs(pd_dir)

        COMM.barrier()
        input_lines = COMM.bcast(input_lines)
        best_models = COMM.bcast(best_models)

        if self.params.ignore_existing:
            exp_names_already =None
            refl_names_already = None
            if COMM.rank==0:
                exp_names_already = {os.path.basename(f) for f in glob.glob("%s/expers/rank*/*.expt" % self.params.outdir)}
                refl_names_already = {os.path.basename(f) for f in glob.glob("%s/refls/rank*/*.refl" % self.params.outdir)}
            exp_names_already = COMM.bcast(exp_names_already)
            refl_names_already = COMM.bcast(refl_names_already)

        exp_gatheredRef_spec = []  # optional list of expt, refls, spectra
        trefs = []
        this_rank_dfs = []  # dataframes storing the modeling results for each shot
        for i_shot, line in enumerate(input_lines):
            if i_shot == self.params.max_process:
                break
            if i_shot % COMM.size != COMM.rank:
                continue

            logging.info("COMM.rank %d on shot  %d / %d" % (COMM.rank, i_shot + 1, len(input_lines)))
            line_fields = line.strip().split()
            num_fields = len(line_fields)
            assert num_fields in [2, 3, 4]
            exp, ref = line_fields[:2]
            spec = None
            exp_idx = 0
            if num_fields==3:
                try:
                    exp_idx = int(line_fields[2])
                except ValueError:
                    spec = line_fields[2]
                    exp_idx = 0
            elif num_fields==4:
                assert os.path.isfile(line_fields[2])
                spec = line_fields[2]
                exp_idx = int(line_fields[3])

            if self.params.ignore_existing:
                basename = os.path.splitext(os.path.basename(exp))[0]
                exists = False
                for ii in [i_shot, 0]:
                    opt_exp = "%s_%s_%d_%d.expt" % (self.params.tag, basename, exp_idx, ii)
                    opt_refl = opt_exp.replace(".expt", ".refl")
                    if opt_exp in exp_names_already and opt_refl in refl_names_already:
                        exists = True
                        break
                if exists:
                    print("Found existing!! %d" % i_shot)
                    continue

            best = None
            if best_models is not None:
                best = best_models.query("exp_name=='%s'" % exp).query("exp_idx==%d" % exp_idx)
                if len(best) != 1:
                    raise ValueError("Should be 1 entry for exp %s in best pickle %s" % (exp, self.params.best_pickle))
            self.params.simulator.spectrum.filename = spec
            Modeler = hopper_utils.DataModeler(self.params)
            Modeler.exper_name = exp
            Modeler.exper_idx = exp_idx
            Modeler.refl_name = ref
            Modeler.rank = COMM.rank
            Modeler.i_shot = i_shot
            if self.params.load_data_from_refls:
                gathered = Modeler.GatherFromReflectionTable(exp, ref, sg_symbol=self.params.space_group)
            else:
                gathered = Modeler.GatherFromExperiment(exp, ref,
                                                        remove_duplicate_hkl=self.params.remove_duplicate_hkl,
                                                        sg_symbol=self.params.space_group,
                                                        exp_idx=exp_idx)
            if not gathered:
                logging.warning("No refls in %s; CONTINUE; COMM.rank=%d" % (ref, COMM.rank))
                continue
            MAIN_LOGGER.info("Modeling %s (%d refls)" % (exp, len(Modeler.refls)))
            if self.params.dump_gathers:
                output_name = os.path.splitext(os.path.basename(exp))[0]
                output_name += "_withData.refl"
                output_name = os.path.join(self.params.gathers_dir, output_name)
                Modeler.dump_gathered_to_refl(output_name, do_xyobs_sanity_check=True)  # NOTE do this is modelin strong spots only
                if self.params.test_gathered_file:
                    all_data = Modeler.all_data.copy()
                    all_roi_id = Modeler.roi_id.copy()
                    all_bg = Modeler.all_background.copy()
                    all_trusted = Modeler.all_trusted.copy()
                    all_pids = np.array(Modeler.pids)
                    all_rois = np.array(Modeler.rois)
                    new_Modeler = hopper_utils.DataModeler(self.params)
                    assert new_Modeler.GatherFromReflectionTable(exp, output_name)
                    assert np.allclose(new_Modeler.all_data, all_data)
                    assert np.allclose(new_Modeler.all_background, all_bg)
                    assert np.allclose(new_Modeler.rois, all_rois)
                    assert np.allclose(new_Modeler.pids, all_pids)
                    assert np.allclose(new_Modeler.all_trusted, all_trusted)
                    assert np.allclose(new_Modeler.roi_id, all_roi_id)

                exp_gatheredRef_spec.append((exp, os.path.abspath(output_name), spec))
                if self.params.only_dump_gathers:
                    continue

            if self.params.refiner.reference_geom is not None:
                detector = ExperimentListFactory.from_json_file(self.params.refiner.reference_geom, check_format=False)[0].detector
                Modeler.E.detector = detector

            # here we support inputting an experiment list with multiple crystals
            # the first crystal in the exp list is used to instantiate a diffBragg instance,
            # the remaining crystals are added to the sim_data instance for use during hopper_utils modeling
            # best pickle is not supported yet for multiple crystals
            # also, if number of crystals is >1 , then the params.number_of_xtals flag will be overridden
            exp_list = ExperimentListFactory.from_json_file(exp, False)
            xtals = exp_list.crystals()  # TODO: fix as this is broken now that we allow multi image experiments
            if self.params.consider_multicrystal_shots and len(xtals) > 1:
                assert best is None, "cannot pass best pickle if expt list has more than one crystal"
                assert self.params.number_of_xtals==1, "if expt list has more than one xtal, leave number_of_xtals as the default"
                self.params.number_of_xtals = len(xtals)
                MAIN_LOGGER.debug("Found %d xtals with unit cells:" %len(xtals))
                for xtal in xtals:
                    MAIN_LOGGER.debug("%.4f %.4f %.4f %.4f %.4f %.4f" % xtal.get_unit_cell().parameters())
            if self.params.record_device_timings and COMM.rank >0:
                self.params.record_device_timings = False  # only record for rank 0 otherwise there's too much output
            SIM = hopper_utils.get_simulator_for_data_modelers(Modeler)
            Modeler.set_parameters_for_experiment(best)
            Modeler.Umatrices = [Modeler.E.crystal.get_U()]

            # TODO: move this to SimulatorFromExperiment
            # TODO: fix multi crystal shot mode
            if best is not None and "other_spotscales" in list(best) and "other_Umats" in list(best):
                Modeler.Umatrices[0] = Modeler.E.get_U()
                assert len(xtals) == len(best.other_spotscales.values[0])+1
                for i_xtal in range(1, len(xtals),1):
                    scale_xt = best.other_spotscales.values[0][i_xtal]
                    Umat_xt = best.other_Umats.values[0][i_xtal]
                    Modeler.Umatrices[i_xtal] = Umat_xt
                    Modeler.P["G_xtal%d" %i_xtal] = scale_xt

            SIM.D.store_ave_wavelength_image = self.params.store_wavelength_images
            if self.params.refiner.verbose is not None and COMM.rank==0:
                SIM.D.verbose = self.params.refiner.verbose
            if self.params.profile:
                SIM.record_timings = True
            if self.params.use_float32:
                Modeler.all_data = Modeler.all_data.astype(np.float32)
                Modeler.all_background = Modeler.all_background.astype(np.float32)

            SIM.D.device_Id = self.dev

            nparam = len(Modeler.P)
            if SIM.refining_Fhkl:
                nparam += SIM.Num_ASU*SIM.num_Fhkl_channels
            x0 = [1] * nparam
            tref = time.time()
            MAIN_LOGGER.info("Beginning refinement of shot %d / %d" % (i_shot+1, len(input_lines)))
            try:
                x = Modeler.Minimize(x0, SIM, i_shot=i_shot)
                for i_rep in range(self.params.filter_after_refinement.max_attempts):
                    final_sigz = Modeler.target.all_sigZ[-1]
                    niter = len(Modeler.target.all_sigZ)
                    too_few_iter = niter < self.params.filter_after_refinement.min_prev_niter
                    too_high_sigz = final_sigz > self.params.filter_after_refinement.max_prev_sigz
                    if too_few_iter or too_high_sigz:
                        Modeler.filter_pixels(self.params.filter_after_refinement.threshold)
                        x = Modeler.Minimize(x0, SIM, i_shot=i_shot)

            except StopIteration:
                x = Modeler.target.x0
            tref = time.time()-tref
            sigz = niter = None
            try:
                niter = len(Modeler.target.all_hop_id)
                sigz = Modeler.target.all_sigZ[-1]
            except Exception:
                pass

            trefs.append(tref)
            print_s = "Finished refinement of shot %d / %d in %.4f sec. (rank mean t/im=%.4f sec.)" \
                        % (i_shot+1, len(input_lines), tref, np.mean(trefs))
            if sigz is not None and niter is not None:
                print_s += " Ran %d iterations. Final sigmaZ = %.1f," % (niter, sigz)
            if COMM.rank==0:
                MAIN_LOGGER.info(print_s)
            else:
                MAIN_LOGGER.debug(print_s)
            if self.params.profile:
                SIM.D.show_timings(COMM.rank)

            dbg = self.params.debug_mode
            shot_df = Modeler.save_up(x, SIM, rank=COMM.rank, i_shot=i_shot,
                            save_fhkl_data=dbg, save_refl=dbg, save_modeler_file=dbg,
                            save_sim_info=dbg, save_pandas=dbg, save_traces=dbg, save_expt=dbg)
            this_rank_dfs.append(shot_df)
            if Modeler.params.refiner.debug_pixel_panelfastslow is not None:
                # TODO separate diffBragg logger
                utils.show_diffBragg_state(SIM.D, Modeler.params.refiner.debug_pixel_panelfastslow)

            # TODO verify this works:
            if SIM.D.record_timings:
                SIM.D.show_timings(COMM.rank)
            Modeler.clean_up(SIM)
            del SIM.D  # TODO: is this necessary ?

        if self.params.dump_gathers and self.params.gathered_output_file is not None:
            exp_gatheredRef_spec = COMM.reduce(exp_gatheredRef_spec)
            if COMM.rank == 0:
                o = open(self.params.gathered_output_file, "w")
                for e, r, s in exp_gatheredRef_spec:
                    if s is not None:
                        o.write("%s %s %s\n" % (e,r,s))
                    else:
                        o.write("%s %s\n" % (e,r))
                o.close()

        if this_rank_dfs:
            this_rank_dfs = pandas.concat(this_rank_dfs).reset_index(drop=True)
            df_name = os.path.join(pd_dir, "hopper_results_rank%d.pkl" % COMM.rank)
            this_rank_dfs.to_pickle(df_name)

        #MAIN_LOGGER.info("MPI-Gathering data frames across ranks")
        #all_rank_dfs = COMM.gather(this_rank_dfs)
        #if COMM.rank==0:
        #    all_rank_dfs = pandas.concat(all_rank_dfs)
        #    all_rank_dfs.reset_index(inplace=True, drop=True)
        #    all_df_name = os.path.join(self.params.outdir, "hopper_results.pkl")
        #    all_rank_dfs.to_pickle(all_df_name)




if __name__ == '__main__':
    from dials.util import show_mail_on_error

    with show_mail_on_error():
        script = Script()
        RUN = script.run
        lp = None
        if LineProfiler is not None and script.params.profile:
            lp = LineProfiler()
            lp.add_function(hopper_utils.model)
            lp.add_function(hopper_utils.target_func)
            RUN = lp(script.run)
        elif script.params.profile:
            print("Install line_profiler in order to use logging: libtbx.python -m pip install line_profiler")

        with DeviceWrapper(script.dev) as _:
            #with np.errstate(all='raise'):
            RUN()

        if lp is not None:
            stats = lp.get_stats()
            hopper_utils.print_profile(stats, ["model", "target_func"])


 *******************************************************************************


 *******************************************************************************
simtbx/command_line/hopper_ensemble.py
from __future__ import absolute_import, division, print_function

# LIBTBX_SET_DISPATCHER_NAME ens.hopper

from argparse import ArgumentParser
parser = ArgumentParser()
parser.add_argument("input", type=str, help="combined pandas pickle")
parser.add_argument("phil", type=str, help="user phil file used to run hopper (see simtbx/diffBragg/phil.py)")
parser.add_argument("--outdir", type=str, default=None, help="output folder")
parser.add_argument("--exp", type=str, default="exp_name", help="column name for input expeirments (default is opt_exp_name)")
parser.add_argument("--refl", type=str, default="stage2_refls", help="column name for refls (default is stage2_refls)")
parser.add_argument("--cmdlinePhil", nargs="+", default=None, type=str, help="command line phil params")
parser.add_argument("--cell", nargs=6, type=float, default=None, help="unit cell to use when writing MTZ files. If not provided, average will be used")
parser.add_argument("--maxSigma", type=float, default=1e20, help="Fhkls are written to MTZ only if they have a sigma < than this value(default=1e20)")
parser.add_argument("--saveFreq", type=int, default=None, help="save an mtz each N iterations (default is None, i.e. only save after the last iteration)")
parser.add_argument("--preImport", action="store_true", help="convert the data to reflection table format, then exit. Subsequent runs will be quicker, and the data are not portable.")
parser.add_argument("--saveAll", action="store_true", help="save a pandas pickle for each modeled shot whenever an MTZ is written (--saveFreq and/or once at refinement termination)")
parser.add_argument("--saveTag", type=str, default="stage2", help="filename tag (only matters if --saveAll)")
args = parser.parse_args()

import os
import sys
import logging
import pandas

from simtbx.diffBragg.hopper_ensemble_utils import load_inputs
from libtbx.mpi4py import MPI
from simtbx.diffBragg.device import DeviceWrapper

COMM= MPI.COMM_WORLD
LOGGER = logging.getLogger("diffBragg.main")


def write_commandline(params):
    if COMM.rank==0:
        if not os.path.exists(params.outdir):
            os.makedirs(params.outdir)

        command_fname = os.path.join(params.outdir, "command_line_input.txt")
        with open(command_fname, "w") as o:
            o.write("Command line input:\n")
            o.write(" ".join(sys.argv) + "\n")


if __name__ == "__main__":
    from libtbx.phil import parse
    from simtbx.diffBragg.phil import philz, hopper_phil
    from simtbx.diffBragg import mpi_logger

    # phil stuff ==========
    phil_scope = parse(philz+hopper_phil)
    arg_interp = phil_scope.command_line_argument_interpreter(home_scope="")

    phil_file = open(args.phil, "r").read()
    user_phil = parse(phil_file)
    phil_sources = [user_phil]

    if args.cmdlinePhil is not None:
        command_line_phils = [arg_interp.process(phil) for phil in args.cmdlinePhil]
        phil_sources += command_line_phils

    working_phil, unused = phil_scope.fetch(sources=phil_sources, track_unused_definitions=True)
    for loc in unused:
        print("WARNING: unused phil:", loc)
    params = working_phil.extract()
    if args.outdir is not None:
        params.outdir = args.outdir
    params.tag = args.saveTag
    if params.record_device_timings and COMM.rank > 0:
        params.record_device_timings = False  # only record for rank 0 otherwise there's too much output
    # end of phil stuff ========

    write_commandline(params)

    if params.logging.disable:
        logging.disable(level=logging.CRITICAL)  # disables CRITICAL and below
    else:
        mpi_logger.setup_logging_from_params(params)

    df = pandas.read_pickle(args.input)

    if params.skip is not None:
        df = df.iloc[params.skip:]
    if params.max_process is not None:
        df = df.iloc[:params.max_process]
    df.reset_index(inplace=True, drop=True)

    gather_dir=None
    if args.preImport:
        assert args.outdir is not None
        gather_dir = os.path.join(args.outdir, "stage2_imported_data")
        if COMM.rank == 0:
            if not os.path.exists(gather_dir):
                os.makedirs(gather_dir)

    for col in [args.exp, args.refl]:
        if col not in list(df):
            raise KeyError("Col %s is missing from dataframe" % col)

    modelers = load_inputs(df, params, exper_key=args.exp, refls_key=args.refl, gather_dir=gather_dir)
    # note, we only go beyond this point if perImport flag was not passed
    modelers.cell_for_mtz = args.cell
    modelers.max_sigma = args.maxSigma
    modelers.outdir = args.outdir if args.outdir is not None else modelers.params.outdir
    modelers.save_freq = args.saveFreq

    modelers.prep_for_refinement()

    with DeviceWrapper(modelers.SIM.D.device_Id) as _:
        modelers.alloc_max_pix_per_shot()
        modelers.save_modeler_params = args.saveAll

        # do all sanity checks up front before minimization
        modelers.Minimize(save=True)

        LOGGER.debug("Done!")


 *******************************************************************************


 *******************************************************************************
simtbx/command_line/hopper_process.py
from __future__ import absolute_import, division, print_function
import time
import sys

# LIBTBX_SET_DISPATCHER_NAME diffBragg.stills_process
# LIBTBX_SET_DISPATCHER_NAME diffBragg.hopper_process

from dials.command_line.stills_process import Processor
from xfel.small_cell.small_cell import small_cell_index_detail
from simtbx.diffBragg import hopper_utils
from dials.array_family import flex
from simtbx.diffBragg.hopper_io import save_to_pandas
from dxtbx.model import ExperimentList
import numpy as np
import socket
from libtbx.mpi4py import MPI
COMM = MPI.COMM_WORLD

import logging
from libtbx.utils import Sorry
from simtbx.modeling import predictions

logger = logging.getLogger("dials.command_line.stills_process")


#include scope dials.command_line.stills_process.phil_scope
phil_str = """
include scope xfel.small_cell.command_line.small_cell_process.phil_scope
diffBragg {
  include scope simtbx.command_line.hopper.phil_scope
}
skip_hopper=False
  .type = bool
  .help = if True, then skip the hopper refinement, i.e. just run stills
  .help = process without refinement like usual
silence_dials_loggers = False
  .type = bool
  .help = if True, reduce the output clutter from the dials.refine and dials.index methods
save_pandas = True
  .type = bool
  .help = save the model parameters in a pandas frame
combine_pandas = True
  .type = bool
  .help = if True, combine all pandas frames after hopper_process completes
partial_correct = False
  .type = bool
  .help = if True, compute partialities from the prediction models  and use them to normalize measurements
  .help = for examining the modeling results, can be loaded using modeler = np.load(fname, allow_pickle=True)[()]
refspec = None
  .type = str
  .help = path to a reference .lam file to use as the spectra for each shot
reidx_obs = False
  .type = bool
  .help = optionally reindex the strong spot observations after running sills indexer refinement
db_loglevel = 0 1 *2
  .type = choice
  .help = Log level for diffBragg main logger
  .help = 0=critical (less verbose), 1=info (verbose), 2=debug (most verbose)
refine_predictions = False
  .type = bool
  .help = optionally refine the list of predicted reflections before integrating
use_small_cell_indexing = False
  .type = bool
  .help = use this to index sparse patterns from small unit cell xtals with at least 3 reflections
"""
import os
from libtbx.phil import parse
phil_scope = parse(phil_str, process_includes=True)


class Hopper_Processor(Processor):

    def __init__(self, *args, **kwargs):
        super(Hopper_Processor, self).__init__(*args, **kwargs)
        command_file = os.path.join(self.params.output.output_dir, "hopper_process_cmdline.txt")
        if COMM.rank==0:
            with open(command_file, "w") as o:
                o.write(" ".join(sys.argv))
                o.close()
            hop_proc_params_file = os.path.join(self.params.output.output_dir, "hopperProcessParams.npy")
            np.save(hop_proc_params_file, self.params)
        if self.params.output.composite_output:
            print("WARNING!! COMPOSITE OUTPUT NOT YET SUPPORTED, disabling for now")
        self.params.output.composite_output = False

        self.stage1_df = None  # the pandas dataframe containing model parameters after running stage 1 (see method refine)
        self.stage1_modeler = None  # the data modeler used during stage 1 refinement
        self.modeler_dir = None  # path for writing the data modelers
        self.known_crystal_models = None  # default flag, should be defined in stills_process init
        self.SIM = None # place holder for simtbx/nanoBragg/sim_data.SimData instance

        if self.params.silence_dials_loggers:
            #dials.algorithms.indexing.indexer: model
            logging.getLogger("dials.algorithms.indexing.nave_parameters").setLevel(logging.ERROR)
            logging.getLogger("dials.algorithms.indexing.stills_indexer").setLevel(logging.ERROR)
            logging.getLogger("dials.algorithms.refinement.refiner").setLevel(logging.ERROR)
            logging.getLogger("dials.algorithms.refinement.reflection_manager").setLevel(logging.ERROR)
            logging.getLogger("dials.algorithms.refinement.reflection_manager").setLevel(logging.ERROR)
        # configure the diffBragg logger
        dblog = logging.getLogger("diffBragg.main")
        H = logging.StreamHandler()
        if self.params.db_loglevel=='2':
            dblog.setLevel(logging.DEBUG)
            H.setLevel(logging.DEBUG)
        elif self.params.db_loglevel=='1':
            dblog.setLevel(logging.INFO)
            H.setLevel(logging.INFO)
        else:
            dblog.setLevel(logging.CRITICAL)
            H.setLevel(logging.CRITICAL)
        dblog.addHandler(H)

        self._create_modeler_dir()

    def index(self, experiments, reflections):
        """optionally do small cell indexing , else default to stills process"""
        if self.params.use_small_cell_indexing:
            max_clique_len, experiments, indexed = small_cell_index_detail(experiments, reflections, self.params, write_output=False)
        else:
            experiments, indexed = super(Hopper_Processor, self).index(experiments, reflections)

        return experiments, indexed

    def _create_modeler_dir(self):
        """makes the directory where data modelers, logs, and spectra files will be written"""
        if self.params.diffBragg.debug_mode:
            self.modeler_dir = os.path.join(self.params.output.output_dir, "modelers")
            if COMM.rank == 0:
                if not os.path.exists(self.modeler_dir):
                    os.makedirs(self.modeler_dir)
            COMM.barrier()

    @property
    def device_id(self):
        dev = COMM.rank % self.params.diffBragg.refiner.num_devices
        print("Rank %d will use fixed device %d on host %s" % (COMM.rank, dev, socket.gethostname()), flush=True)
        return dev

    def find_spots(self, experiments):
        st = time.time()

        logger.info("*" * 80)
        logger.info("Finding Strong Spots")
        logger.info("*" * 80)

        # Find the strong spots
        observed = flex.reflection_table.from_observations(
            experiments, self.params, is_stills=True
        )

        # Reset z coordinates for dials.image_viewer; see Issues #226 for details
        xyzobs = observed["xyzobs.px.value"]
        for i in range(len(xyzobs)):
            xyzobs[i] = (xyzobs[i][0], xyzobs[i][1], 0)
        bbox = observed["bbox"]
        for i in range(len(bbox)):
            bbox[i] = (bbox[i][0], bbox[i][1], bbox[i][2], bbox[i][3], 0, 1)

        if self.params.output.composite_output:
            n = len(self.all_strong_reflections.experiment_identifiers())
            for i, experiment in enumerate(experiments):
                refls = observed.select(observed["id"] == i)
                refls["id"] = flex.int(len(refls), n)
                del refls.experiment_identifiers()[i]
                refls.experiment_identifiers()[n] = experiment.identifier
                self.all_strong_reflections.extend(refls)
                n += 1
        else:
            # Save the reflections to file
            logger.info("\n" + "-" * 80)
            if self.params.output.strong_filename:
                self.save_reflections(observed, self.params.output.strong_filename)

        logger.info("")
        logger.info("Time Taken = %f seconds", time.time() - st)
        self.observed = observed  # note this is the only change needed to dials.stills_process.find_spots
        return observed

    def refine(self, exps, ref, refining_predictions=False, best=None):
        exps_out = exps
        if not self.params.skip_hopper:
            if self.params.dispatch.refine:
                exps, ref = super(Hopper_Processor, self).refine(exps, ref)
                print("WARNING: hopper_process will always run its own refinement, ignoring dials.refine phil scope")
            self.params.dispatch.refine = False
            assert len(exps) == 1
            if self.params.reidx_obs:
                exps, ref = self._reindex_obs(exps, self.observed)

            exp, ref, self.stage1_modeler, self.SIM, x = hopper_utils.refine(exps[0], ref,
                                               self.params.diffBragg,
                                               spec=self.params.refspec,
                                               gpu_device=self.device_id, return_modeler=True, best=best)
            orig_exp_name = os.path.abspath(self.params.output.refined_experiments_filename)
            refls_name = os.path.abspath(self.params.output.indexed_filename)
            self.params.diffBragg.outdir = self.params.output.output_dir
            # TODO: what about composite mode ?

            #def save_to_pandas(x, Mod, SIM, orig_exp_name, params, expt, rank_exp_idx, stg1_refls, stg1_img_path=None,
            #                   rank=0, write_expt=True, write_pandas=True, exp_idx=0):
            self.stage1_df = save_to_pandas(x, self.stage1_modeler, self.SIM, orig_exp_name, self.params.diffBragg,
                                            self.stage1_modeler.E, rank_exp_idx=0, stg1_refls=refls_name, stg1_img_path=None, rank=COMM.rank,
                                            write_expt=False, write_pandas=True, exp_idx=0)
            exps_out = ExperimentList()
            exps_out.append(exp)

            basename = os.path.splitext(os.path.basename(refls_name))[0]
            self._save_modeler_info(basename)

        out = super(Hopper_Processor, self).refine(exps_out, ref)
        return out

    def _reindex_obs(self, exps, ref):
        """use known_crystal_models indexing method to add more indexed spots which can
        then be refined using diffBragg
        This is useful when dials.stills_indexer happens to prefer a lower resolution cutoff
        to obtain a descent xtal model (controlled by refinement_protocol.d_min_start)
        but one still wants to try refining the higher resolution spots obtained with spot finder
        """
        # NOTE: this method assumes known_crystal_models is not being used in any other way ...
        self.known_crystal_models = exps.crystals()
        # cache these parameters in case they are set for stills_indexer
        tmp_tol = self.params.indexing.index_assignment.simple.hkl_tolerance
        tmp_prot = self.params.indexing.refinement_protocol.mode
        self.params.indexing.index_assignment.simple.hkl_tolerance = 0.5  # go for broke!
        self.params.indexing.refinement_protocol.mode = "repredict_only"  # no more refinement from dials
        exps, ref = self.index(exps, ref)

        self.params.indexing.index_assignment.simple.hkl_tolerance = tmp_tol
        self.params.indexing.refinement_protocol.mode = tmp_prot
        self.known_crystal_models = None
        return exps, ref

    def _save_modeler_info(self, basename):
        if self.params.diffBragg.debug_mode:
            modeler_fname = "%s_%s" % (basename, "modeler.npy")
            modeler_fname = os.path.abspath(os.path.join(self.modeler_dir, modeler_fname))
            np.save(modeler_fname, self.stage1_modeler)  # pickle the modeler, set __setstate__

            spectra_fname = "%s_%s" % (basename, "spectra.npy")
            spectra_fname = os.path.abspath(os.path.join(self.modeler_dir, spectra_fname))
            SIM_state_fname = "%s_%s" % (basename, "SIM_state.npy")
            SIM_state_fname = os.path.abspath(os.path.join(self.modeler_dir, SIM_state_fname))
            hopper_utils.write_SIM_logs(self.SIM, log=SIM_state_fname, lam=spectra_fname)

    def integrate(self, experiments, indexed):
        if self.params.skip_hopper:
            return super(Hopper_Processor, self).integrate(experiments, indexed)
        st = time.time()

        logger.info("*" * 80)
        logger.info("Integrating Reflections")
        logger.info("*" * 80)

        indexed, _ = self.process_reference(indexed)

        if self.params.integration.integration_only_overrides.trusted_range:
            for detector in experiments.detectors():
                for panel in detector:
                    panel.set_trusted_range(
                        self.params.integration.integration_only_overrides.trusted_range
                    )

        if self.params.dispatch.coset:
            from dials.algorithms.integration.sublattice_helper import integrate_coset

            integrate_coset(self, experiments, indexed)

        # Get the integrator from the input parameters
        logger.info("Configuring integrator from input parameters")
        from dials.algorithms.integration.integrator import create_integrator
        from dials.algorithms.profile_model.factory import ProfileModelFactory

        # Compute the profile model
        # Predict the reflections
        # Match the predictions with the reference
        # Create the integrator
        experiments = ProfileModelFactory.create(self.params, experiments, indexed)
        new_experiments = ExperimentList()
        new_reflections = flex.reflection_table()
        for expt_id, expt in enumerate(experiments):
            if (
                self.params.profile.gaussian_rs.parameters.sigma_b_cutoff is None
                or expt.profile.sigma_b()
                < self.params.profile.gaussian_rs.parameters.sigma_b_cutoff
            ):
                refls = indexed.select(indexed["id"] == expt_id)
                refls["id"] = flex.int(len(refls), len(new_experiments))
                # refls.reset_ids()
                del refls.experiment_identifiers()[expt_id]
                refls.experiment_identifiers()[len(new_experiments)] = expt.identifier
                new_reflections.extend(refls)
                new_experiments.append(expt)
            else:
                logger.info(
                    "Rejected expt %d with sigma_b %f"
                    % (expt_id, expt.profile.sigma_b())
                )
        experiments = new_experiments
        indexed = new_reflections
        if len(experiments) == 0:
            raise RuntimeError("No experiments after filtering by sigma_b")
        logger.info("")
        logger.info("=" * 80)
        logger.info("")
        logger.info("Predicting reflections")
        logger.info("")
        # NOTE: this is the only changed needed to dials.stills_process
        # TODO: multi xtal
        # TODO: add in normal dials predictions as an option
        predicted, model = predictions.get_predicted_from_pandas(
            self.stage1_df, self.params.diffBragg, self.observed,
            experiments[0].identifier, self.device_id,
            spectrum_override=self.SIM.beam.spectrum)
        if self.params.refine_predictions:
            experiments, rnd2_refls = self.refine(experiments, predicted, refining_predictions=True, best=self.stage1_df)
            # TODO: match rnd2_refls with indexed.refl and re-save indexed.refl
            predicted, model = predictions.get_predicted_from_pandas(
                self.stage1_df, self.params.diffBragg, self.observed,
                experiments[0].identifier, self.device_id,
                spectrum_override=self.SIM.beam.spectrum)

        predicted.match_with_reference(indexed)
        integrator = create_integrator(self.params, experiments, predicted)

        # Integrate the reflections
        integrated = integrator.integrate()

        if self.params.partial_correct:
            integrated = predictions.normalize_by_partiality(
                integrated, model, default_F=self.params.diffBragg.predictions.default_Famplitude,
                gain=self.params.diffBragg.refiner.adu_per_photon)

        # correct integrated intensities for absorption correction, if necessary
        for abs_params in self.params.integration.absorption_correction:
            if abs_params.apply:
                if abs_params.algorithm == "fuller_kapton":
                    from dials.algorithms.integration.kapton_correction import (
                        multi_kapton_correction,
                    )
                elif abs_params.algorithm == "kapton_2019":
                    from dials.algorithms.integration.kapton_2019_correction import (
                        multi_kapton_correction,
                    )

                experiments, integrated = multi_kapton_correction(
                    experiments, integrated, abs_params.fuller_kapton, logger=logger
                )()

        if self.params.significance_filter.enable:
            from dials.algorithms.integration.stills_significance_filter import (
                SignificanceFilter,
            )

            sig_filter = SignificanceFilter(self.params)
            filtered_refls = sig_filter(experiments, integrated)
            accepted_expts = ExperimentList()
            accepted_refls = flex.reflection_table()
            logger.info(
                "Removed %d reflections out of %d when applying significance filter",
                len(integrated) - len(filtered_refls),
                len(integrated),
                )
            for expt_id, expt in enumerate(experiments):
                refls = filtered_refls.select(filtered_refls["id"] == expt_id)
                if len(refls) > 0:
                    accepted_expts.append(expt)
                    refls["id"] = flex.int(len(refls), len(accepted_expts) - 1)
                    accepted_refls.extend(refls)
                else:
                    logger.info(
                        "Removed experiment %d which has no reflections left after applying significance filter",
                        expt_id,
                    )

            if len(accepted_refls) == 0:
                raise Sorry("No reflections left after applying significance filter")
            experiments = accepted_expts
            integrated = accepted_refls

        # Delete the shoeboxes used for intermediate calculations, if requested
        if self.params.integration.debug.delete_shoeboxes and "shoebox" in integrated:
            del integrated["shoebox"]

        if self.params.output.composite_output:
            if (
                self.params.output.integrated_experiments_filename
                or self.params.output.integrated_filename
            ):
                assert (
                    self.params.output.integrated_experiments_filename is not None
                    and self.params.output.integrated_filename is not None
                )

                n = len(self.all_integrated_experiments)
                self.all_integrated_experiments.extend(experiments)
                for i, experiment in enumerate(experiments):
                    refls = integrated.select(integrated["id"] == i)
                    refls["id"] = flex.int(len(refls), n)
                    del refls.experiment_identifiers()[i]
                    refls.experiment_identifiers()[n] = experiment.identifier
                    self.all_integrated_reflections.extend(refls)
                    n += 1
        else:
            # Dump experiments to disk
            if self.params.output.integrated_experiments_filename:

                experiments.as_json(self.params.output.integrated_experiments_filename)

            if self.params.output.integrated_filename:
                # Save the reflections
                self.save_reflections(
                    integrated, self.params.output.integrated_filename
                )

        self.write_integration_pickles(integrated, experiments)
        from dials.algorithms.indexing.stills_indexer import (
            calc_2D_rmsd_and_displacements,
        )

        rmsd_indexed, _ = calc_2D_rmsd_and_displacements(indexed)
        log_str = "RMSD indexed (px): %f\n" % rmsd_indexed
        for i in range(6):
            bright_integrated = integrated.select(
                (
                    integrated["intensity.sum.value"]
                    / flex.sqrt(integrated["intensity.sum.variance"])
                )
                >= i
            )
            if len(bright_integrated) > 0:
                rmsd_integrated, _ = calc_2D_rmsd_and_displacements(bright_integrated)
            else:
                rmsd_integrated = 0
            log_str += (
                "N reflections integrated at I/sigI >= %d: % 4d, RMSD (px): %f\n"
                % (i, len(bright_integrated), rmsd_integrated)
            )

        for crystal_model in experiments.crystals():
            if hasattr(crystal_model, "get_domain_size_ang"):
                log_str += ". Final ML model: domain size angstroms: {:f}, half mosaicity degrees: {:f}".format(
                    crystal_model.get_domain_size_ang(),
                    crystal_model.get_half_mosaicity_deg(),
                )

        logger.info(log_str)

        logger.info("")
        logger.info("Time Taken = %f seconds", time.time() - st)
        return integrated


def run(args=None):
    from dials.command_line import stills_process
    stills_process.Processor = Hopper_Processor
    stills_process.phil_scope = phil_scope
    script = stills_process.Script()
    from simtbx.diffBragg.device import DeviceWrapper
    params, options, all_paths = script.parser.parse_args(
        args, show_diff_phil=True, return_unhandled=True, quick_parse=True
    )
    if params.output.composite_output:
        raise NotImplementedError("diffBragg.stills_process currently does not support composite_output mode")
    dev = COMM.rank % params.diffBragg.refiner.num_devices
    with DeviceWrapper(dev) as _:
        script.run(args)
    return script


if __name__ == "__main__":
    script_that_was_run = run()
    if COMM.rank==0:
        try:
            params = script_that_was_run.params
        except AttributeError as err:
            print(err)
            print("Looks like the program never launched, check input paths, image files, phil files, current working dir etc.. ")
            sys.exit()
        if params.combine_pandas:
            if not params.save_pandas:
                print("No pandas tables saved, so will not combine")
                exit()
            import pandas
            import glob
            fnames = glob.glob("%s/pandas/rank*/*pkl" % params.output.output_dir)
            logging.info("There are %d pandas output files to combine" % len(fnames))
            if fnames:
                df = pandas.concat([pandas.read_pickle(f) for f in fnames])
                combined_table = os.path.join(params.output.output_dir, "hopper_process_summary.pkl")
                df.to_pickle(combined_table)
                logging.info("Saved summary pandas table: %s" % combined_table)


 *******************************************************************************


 *******************************************************************************
simtbx/command_line/integrate.py
from __future__ import division, print_function

# LIBTBX_SET_DISPATCHER_NAME diffBragg.integrate

import argparse as ap
parser = ap.ArgumentParser()
parser.add_argument("predPhil", type=str, help="path to a phil config file for diffbragg prediction")
parser.add_argument("procPhil", type=str, help="path to a phil config file for stills process (used for spot finding and integration)")
parser.add_argument("inputGlob", type=str, help="glob of input pandas tables (those that are output by simtbx.diffBragg.hopper or diffBragg.hopper_process")
parser.add_argument("outdir", type=str, help="path to output refls")

parser.add_argument("--cmdlinePhil", nargs="+", default=None, type=str, help="command line phil params")
parser.add_argument("--dialsInteg", action="store_true", help="Integrate new shoeboxes using dials and write *integrated.expt files")
parser.add_argument("--numdev", type=int, default=1, help="number of GPUs (default=1)")
parser.add_argument("--pklTag", type=str, help="optional suffix for globbing for pandas pickles (default .pkl)", default=".pkl")
parser.add_argument("--loud", action="store_true", help="show lots of screen output")
parser.add_argument("--hopInputName", default="preds_for_hopper", type=str, help="write exp_ref_spec file and best_pickle pointing to the preditction models, such that one can run predicted rois through simtbx.diffBragg.hopper (e.g. to fit per-roi scale factors)")
parser.add_argument("--filterDupes", action="store_true", help="filter refls with same HKL")
parser.add_argument("--keepShoeboxes", action="store_true", help="Optionally keep shoeboxes present in the prediction refl tables (can lead to OOM errors)")
parser.add_argument("--scanWeakFracs", action="store_true", help="optionally stores a variety of inputs for stage2 based filtering different fractions of weak reflections")

args = parser.parse_args()

from libtbx.mpi4py import MPI
COMM = MPI.COMM_WORLD

import logging
if not args.loud:
    logging.disable(logging.CRITICAL)
else:
    if COMM.rank==0:
        logger = logging.getLogger("diffBragg.main")
        logger.setLevel(logging.DEBUG)


def printR(*args, **kwargs):
    print("RANK %d" % COMM.rank, *args, **kwargs)
def print0(*args, **kwargs):
    if COMM.rank==0:
        print(*args, **kwargs)

import numpy as np
import json
from simtbx.diffBragg import hopper_utils, utils
from simtbx.modeling import predictions
from simtbx.diffBragg.hopper_utils import downsamp_spec_from_params
import glob
import pandas
import os
from dials.algorithms.integration.stills_significance_filter import SignificanceFilter
from dials.algorithms.indexing.stills_indexer import calc_2D_rmsd_and_displacements
import sys


def filter_weak_reflections(refls, weak_fraction):
    """
    :param pred:  reflection table created by this script
    :param weak_fraction: number from 0-1 (if 0, only strong spots are saved)
    :return: new reflection table with weak reflections filtered according to weak_fraction
    """
    new_refls = None
    for idx in set(refls['id']):
        pred = refls.select(refls['id']==idx)
        weaks = pred.select(pred['is_weak'])
        nweak = len(weaks)
        weaks_sorted = np.argsort(weaks["scatter"])[::-1]
        num_keep = int(nweak * weak_fraction)
        weak_refl_inds_keep = set(np.array(weaks["refl_idx"])[weaks_sorted[:num_keep]])
        weak_sel = flex.bool([i in weak_refl_inds_keep for i in pred['refl_idx']])
        keeps = np.logical_or(pred['is_strong'], weak_sel)
        pred = pred.select(flex.bool(keeps))
        if new_refls is None:
            new_refls = deepcopy(pred)
        else:
            new_refls.extend(pred)
    return new_refls


# Note: these imports and following 3 methods will eventually be in CCTBX/simtbx/diffBragg/utils
from dials.algorithms.spot_finding.factory import SpotFinderFactory
from dials.algorithms.spot_finding.factory import FilterRunner
from dials.model.data import PixelListLabeller, PixelList
from dials.algorithms.spot_finding.finder import pixel_list_to_reflection_table
from libtbx.phil import parse
from dials.command_line.stills_process import phil_scope
from dials.algorithms.integration.integrator import create_integrator
from dials.algorithms.profile_model.factory import ProfileModelFactory
from dxtbx.model import ExperimentList
from dials.array_family import flex

from copy import deepcopy
from collections import Counter

from simtbx.diffBragg.device import DeviceWrapper

def filter_refls(R):
    vec3_dbl_keys = 'xyzcal.px', 'xyzcal.mm', 'xyzobs.px.value', 'xyzobs.px.value', 'rlp', 's1'

    hkl_dupes = [h for h,count in Counter(R['miller_index']).items() if count > 1]
    print("%d miller indices are duplicates" % len(hkl_dupes))
    hkls = list(R['miller_index'])
    Rnew = None #flex.reflection_table()
    ndupe = 0
    for hkl in hkl_dupes:
        is_h = [h==hkl for h in hkls]
        ndupe += np.sum(is_h)
        Rdupes = R.select(flex.bool(is_h))
        R0 = deepcopy(Rdupes[0:1])
        for k in vec3_dbl_keys:
            xyz = np.mean(Rdupes[k].as_numpy_array(),axis=0)
            R0[k] = flex.vec3_double(1, tuple(xyz))
        if Rnew is None:
            Rnew = R0
        else:
            Rnew.extend(R0)
    print("%d refls belong to duplicates hkls" % ndupe)

    hkl_singles = set(hkls).difference(hkl_dupes)
    for hkl in hkl_singles:
        is_h = [h==hkl for h in hkls]
        i_R = np.where(is_h)[0][0]
        refl = R[i_R: i_R+1]
        if Rnew is None:
            Rnew = refl
        else:
            Rnew.extend(refl)
    print("filtered %d / %d refls" % (len(Rnew), len(R)))
    return Rnew


for i,arg in enumerate(sys.argv):
    if os.path.isfile(arg) or os.path.isdir(arg):
        sys.argv[i] = os.path.abspath(arg)
print0("COMMANDLINE: libtbx.python %s" % " ".join(sys.argv))

def stills_process_params_from_file(phil_file):
    """
    :param phil_file: path to phil file for stills_process
    :return: phil params object
    """
    phil_file = open(phil_file, "r").read()
    user_phil = parse(phil_file)
    phil_sources = [user_phil]
    working_phil, unused = phil_scope.fetch(
        sources=phil_sources, track_unused_definitions=True)
    params = working_phil.extract()
    return params



def process_reference(reference):
    """Load the reference spots."""
    assert "miller_index" in reference
    assert "id" in reference
    mask = reference.get_flags(reference.flags.indexed)
    rubbish = reference.select(~mask)
    if mask.count(False) > 0:
        reference.del_selected(~mask)
    if len(reference) == 0:
        raise RuntimeError(
            """
    Invalid input for reference reflections.
    Expected > %d indexed spots, got %d
  """
            % (0, len(reference))
        )
    mask = reference["miller_index"] == (0, 0, 0)
    if mask.count(True) > 0:
        rubbish.extend(reference.select(mask))
        reference.del_selected(mask)
    mask = reference["id"] < 0
    if mask.count(True) > 0:
        raise RuntimeError(
            """
    Invalid input for reference reflections.
    %d reference spots have an invalid experiment id
  """
            % mask.count(True)
        )
    return reference, rubbish



def integrate(phil_file, experiments, indexed, predicted):
    """
    integrate a single experiment at the locations specified by the predicted table
    The predicted table should have a column specifying strong reflections
    """
    assert len(experiments)==1

    for refls in [predicted, indexed]:
        refls['id'] = flex.int(len(refls), 0)
        refls['entering'] = flex.bool(len(refls), False)
        eid = refls.experiment_identifiers()
        for k in eid.keys():
            del eid[k]
        eid[0] = '0'
    experiments[0].identifier = '0'

    params = stills_process_params_from_file(phil_file)
    indexed,_ = process_reference(indexed)
    experiments = ProfileModelFactory.create(params, experiments, indexed)

    new_experiments = ExperimentList()
    new_reflections = flex.reflection_table()
    for expt_id, expt in enumerate(experiments):
        if (
                params.profile.gaussian_rs.parameters.sigma_b_cutoff is None
                or expt.profile.sigma_b()
                < params.profile.gaussian_rs.parameters.sigma_b_cutoff
        ):
            refls = indexed.select(indexed["id"] == expt_id)
            refls["id"] = flex.int(len(refls), len(new_experiments))
            del refls.experiment_identifiers()[expt_id]
            refls.experiment_identifiers()[len(new_experiments)] = expt.identifier
            new_reflections.extend(refls)
            new_experiments.append(expt)

    experiments = new_experiments
    indexed = new_reflections
    if len(experiments) == 0:
        raise RuntimeError("No experiments after filtering by sigma_b")

    predicted.match_with_reference(indexed)
    integrator = create_integrator(params, experiments, predicted)
    integrated = integrator.integrate()

    if params.significance_filter.enable:

        sig_filter = SignificanceFilter(params)
        filtered_refls = sig_filter(experiments, integrated)
        accepted_expts = ExperimentList()
        accepted_refls = flex.reflection_table()
        for expt_id, expt in enumerate(experiments):
            refls = filtered_refls.select(filtered_refls["id"] == expt_id)
            if len(refls) > 0:
                accepted_expts.append(expt)
                refls["id"] = flex.int(len(refls), len(accepted_expts) - 1)
                accepted_refls.extend(refls)

        if len(accepted_refls) == 0:
            raise RuntimeError("No reflections left after applying significance filter")
        experiments = accepted_expts
        integrated = accepted_refls

    # Delete the shoeboxes used for intermediate calculations, if requested
    if params.integration.debug.delete_shoeboxes and "shoebox" in integrated:
        del integrated["shoebox"]


    rmsd_indexed, _ = calc_2D_rmsd_and_displacements(indexed)
    log_str = "RMSD indexed (px): %f\n" % rmsd_indexed
    for i in range(6):
        bright_integrated = integrated.select(
            (
                    integrated["intensity.sum.value"]
                    / flex.sqrt(integrated["intensity.sum.variance"])
            )
            >= i
        )
        if len(bright_integrated) > 0:
            rmsd_integrated, _ = calc_2D_rmsd_and_displacements(bright_integrated)
        else:
            rmsd_integrated = 0
        log_str += (
                "N reflections integrated at I/sigI >= %d: % 4d, RMSD (px): %f\n"
                % (i, len(bright_integrated), rmsd_integrated)
        )

    for crystal_model in experiments.crystals():
        if hasattr(crystal_model, "get_domain_size_ang"):
            log_str += ". Final ML model: domain size angstroms: {:f}, half mosaicity degrees: {:f}".format(
                crystal_model.get_domain_size_ang(),
                crystal_model.get_half_mosaicity_deg(),
            )

    #print0(log_str)
    return experiments, integrated




def dials_find_spots(data_img, params, trusted_flags=None):
    """
    :param data_img: numpy array image
    :param params: instance of stills_process params.spotfinder
    :param trusted_flags:
    :return:
    """
    if trusted_flags is None:
        trusted_flags = np.ones(data_img.shape, bool)
    thresh = SpotFinderFactory.configure_threshold(params)
    flex_data = flex.double(np.ascontiguousarray(data_img))
    flex_trusted_flags = flex.bool(np.ascontiguousarray(trusted_flags))
    spotmask = thresh.compute_threshold(flex_data, flex_trusted_flags)
    return spotmask.as_numpy_array()


def refls_from_sims(panel_imgs, detector, beam, thresh=0, filter=None, panel_ids=None,
                    max_spot_size=1000, phil_file=None, **kwargs):
    """
    This is for converting the centroids in the noiseless simtbx images
    to a multi panel reflection table
    :param panel_imgs: list or 3D array of detector panel simulations
    :param detector: dxtbx  detector model of a caspad
    :param beam:  dxtxb beam model
    :param thresh: threshol intensity for labeling centroids
    :param filter: optional filter to apply to images before
        labeling threshold, typically one of scipy.ndimage's filters
    :param pids: panel IDS , else assumes panel_imgs is same length as detector
    :param kwargs: kwargs to pass along to the optional filter
    :return: a reflection table of spot centroids
    """
    if panel_ids is None:
        panel_ids = np.arange(len(detector))
    pxlst_labs = []
    badpix_all =None
    min_spot_size=1
    if phil_file is not None:
        params = stills_process_params_from_file(phil_file)
        min_spot_size = params.spotfinder.filter.min_spot_size
    for i, pid in enumerate(panel_ids):
        plab = PixelListLabeller()
        img = panel_imgs[i]
        if phil_file is not None:
            params = stills_process_params_from_file(phil_file)
            badpix = None
            if params.spotfinder.lookup.mask is not None:
                if badpix_all is None:
                    badpix_all = utils.load_mask(params.spotfinder.lookup.mask)
                badpix = badpix_all[pid]
            mask = dials_find_spots(img, params, badpix)
        elif filter is not None:
            mask = filter(img, **kwargs) > thresh
        else:
            mask = img > thresh
        img_sz = detector[int(pid)].get_image_size()  # for some reason the int cast is necessary in Py3
        flex_img = flex.double(img)
        flex_img.reshape(flex.grid(img_sz))

        flex_mask = flex.bool(mask)
        flex_mask.resize(flex.grid(img_sz))
        pl = PixelList(0, flex.double(img), flex.bool(mask))
        plab.add(pl)

        pxlst_labs.append(plab)

    El = utils.explist_from_numpyarrays(panel_imgs, detector, beam)
    iset = El.imagesets()[0]
    refls = pixel_list_to_reflection_table(
        iset, pxlst_labs,
        min_spot_size=min_spot_size,
        max_spot_size=max_spot_size,  # TODO: change this ?
        filter_spots=FilterRunner(),  # must use a dummie filter runner!
        write_hot_pixel_mask=False)[0]
    if phil_file is not None:
        x,y,z = refls['xyzobs.px.value'].parts()
        x -=0.5
        y -=0.5
        refls['xyzobs.px.value'] = flex.vec3_double(x,y,z)

    return refls


if __name__=="__main__":

    if COMM.rank==0:
        if not os.path.exists( args.outdir):
            os.makedirs(args.outdir)
    COMM.barrier()

    #rank_outdir = os.path.join( args.outdir, "rank%d" % COMM.rank)
    #if not os.path.exists(rank_outdir):
    #    os.makedirs(rank_outdir)

    params = utils.get_extracted_params_from_phil_sources(args.predPhil, args.cmdlinePhil)

    # inputGlob can be a glob in strings, a single pandas file, or a hopper output folder
    if os.path.isfile(args.inputGlob) or os.path.isdir(args.inputGlob):
        if os.path.isfile(args.inputGlob):
            fnames = [args.inputGlob]
        else:
            dirname = args.inputGlob
            fnames = glob.glob( os.path.join(dirname, "pandas/hopper_results_rank*.pkl"))
    else:
        fnames = glob.glob(args.inputGlob)

    if not fnames:
        raise OSError("Found no filenames to load!")
    Nf = 0
    shots_per_df = []
    print0("getting total number of shots")
    for i_f, f in enumerate(fnames):
        if i_f % COMM.size != COMM.rank:
            continue
        n = len(pandas.read_pickle(f))
        shots_per_df += [(f, str(x)) for x in range(n)]  # note we cast to string because of mpi reduce
        Nf += n

    shots_per_df = COMM.bcast(COMM.reduce( shots_per_df))
    Nf = COMM.bcast(COMM.reduce(Nf))
    print0("total num shots is %d" % Nf)
    df_rows_per_rank = np.array_split(shots_per_df, COMM.size)[COMM.rank]

    print0("getting dataframe handles")
    df_handles = {}
    dfs = []
    if df_rows_per_rank.size:
        dfs, _ = zip(*df_rows_per_rank)
    for f in set(dfs):
        df_handles[f] = pandas.read_pickle(f).reset_index(drop=True)

    def df_iter():
        for i_df, (df_name, row_idx) in enumerate(df_rows_per_rank):
            row_idx = int(row_idx)
            printR("Opening shot %d / %d" % (i_df+1, len(df_rows_per_rank)))
            df = df_handles[df_name].iloc[row_idx: row_idx+1].copy()
            yield i_df, df

    if params.predictions.verbose:
        params.predictions.verbose = COMM.rank==0

    dev = COMM.rank % args.numdev

    EXPT_DIRS = os.path.join(args.outdir, "expts_and_refls")
    if COMM.rank==0:
        utils.safe_makedirs(EXPT_DIRS)

    if args.scanWeakFracs and params.predictions.weak_fraction != 1:
        print("WARNING: overriding weak_fracion because of scanWeakFracs")
        params.predictions.weak_fraction=1

    print0("Found %d input files" % Nf)
    with DeviceWrapper(dev) as _:
        all_dfs = []
        all_pred_names = []
        exp_ref_spec_lines = []
        all_rank_pred = None
        all_rank_expt = None

        rank_shot_count = 0
        rank_pred_file = os.path.join(EXPT_DIRS, "rank%d_preds.refl" % COMM.rank)
        rank_pred_file = os.path.abspath(rank_pred_file)
        rank_expt_file = rank_pred_file.replace(".refl", ".expt")
        for i_f, df in df_iter():

            expt_name = df.exp_name.values[0]
            expt_idx = df.exp_idx.values[0]
            tag = os.path.splitext(os.path.basename(expt_name))[0]

            data_expt = hopper_utils.DataModeler.exper_json_single_file(expt_name, expt_idx)
            data_exptList = ExperimentList()
            data_exptList.append(data_expt)

            try:
                spectrum_override = None
                if params.spectrum_from_imageset:
                    spectrum_override = downsamp_spec_from_params(params, data_expt)
                pred = predictions.get_predicted_from_pandas(
                    df, params, strong=None, device_Id=dev, spectrum_override=spectrum_override)
                if args.filterDupes:
                    pred = filter_refls(pred)
            except ValueError:
                #os.remove(new_expt_name)
                continue

            data = utils.image_data_from_expt(data_expt)
            Rstrong = refls_from_sims(data, data_expt.detector, data_expt.beam, phil_file=args.procPhil )
            Rstrong['id'] = flex.int(len(Rstrong), 0)
            num_panels = len(data_expt.detector)
            if num_panels > 1:
                assert params.predictions.label_weak_col == "rlp"

            Rstrong.centroid_px_to_mm(data_exptList)
            Rstrong.map_centroids_to_reciprocal_space(data_exptList)
            predictions.label_weak_predictions(pred, Rstrong, q_cutoff=params.predictions.qcut, col=params.predictions.label_weak_col )

            pred['is_strong'] = flex.bool(np.logical_not(pred['is_weak']))
            strong_sel = np.logical_not(pred['is_weak'])

            pred["refl_idx"] = flex.int(np.arange(len(pred)))

            #weaks = pred.select(pred['is_weak'])
            #weaks_sorted = np.argsort(weaks["scatter"])[::-1]
            #nweak = len(weaks)
            #num_keep = int(nweak*params.predictions.weak_fraction)
            #weak_refl_inds_keep = set(np.array(weaks["refl_idx"])[weaks_sorted[:num_keep]])
            #weak_sel = flex.bool([i in weak_refl_inds_keep for i in pred['refl_idx']])
            #keeps = np.logical_or( pred['is_strong'], weak_sel)
            #printR("Sum keeps=%d; num_strong=%d, num_kept_weak=%d" % (sum(keeps), sum(strong_sel), sum(weak_sel)))
            #pred = pred.select(flex.bool(keeps))
            pred = filter_weak_reflections(pred, weak_fraction=params.predictions.weak_fraction)

            nstrong = np.sum(strong_sel)
            printR("Will save %d refls (%d strong, %d weak)" % (len(pred), np.sum(pred["is_strong"]), np.sum(pred["is_weak"])))
            pred['id'] = flex.int(len(pred), rank_shot_count)
            if 'shoebox' in list(pred) and not args.keepShoeboxes:
                del pred['shoebox']
            if all_rank_pred is None:
                all_rank_pred = deepcopy(pred)
            else:
                all_rank_pred.extend(pred)

            # Note, the simple append causes memory leak:
            #all_rank_expt.append(data_expt)
            if all_rank_expt is None:
                all_rank_expt = deepcopy(data_exptList.to_dict())
            else:
                Edict = data_exptList.to_dict()
                for exp_key in 'beam', 'detector', 'crystal', 'imageset':
                    Edict['experiment'][0][exp_key] = rank_shot_count
                for exp_key in 'experiment', 'beam', 'detector', 'crystal', 'imageset':
                    assert len( Edict[exp_key])==1
                    all_rank_expt[exp_key] .append(Edict[exp_key][0])

            Rindexed = Rstrong.select(Rstrong['indexed'])
            if len(Rindexed)==0:
                print("No strong indexed refls for shot %s" % expt_name)
                continue

            utils.refls_to_hkl(Rindexed, data_expt.detector, data_expt.beam, data_expt.crystal, update_table=True)
            if args.dialsInteg:
                # TODO: save these files as multi-shot experiment/refls
                try:
                    int_expt, int_refl = integrate(args.procPhil, data_exptList, Rindexed, pred)
                    int_expt_name = "%s/%s_%d_integrated.expt" % (rank_outdir, tag, i_f)
                    int_expt.as_file(int_expt_name)
                    int_refl['bbox'] = int_refl['shoebox'].bounding_boxes()
                    int_refl_name = int_expt_name.replace(".expt", ".refl")
                    int_refl.as_file(int_refl_name)
                except RuntimeError:
                    print("Integration failed" )

            df['old_exp_name'] = expt_name
            df['old_exp_idx'] = expt_idx
            df['exp_name'] = rank_expt_file
            df['exp_idx'] = rank_shot_count

            df['predictions'] = rank_pred_file
            df['predicted_refs'] = rank_pred_file
            df['num_pred'] = len(pred)

            all_dfs.append(df)
            rank_shot_count += 1

            spec_name = df.spectrum_filename.values[0]
            if spec_name is None:
                spec_name = ""
            exp_ref_spec_lines.append("%s %s %s %d\n" % (rank_expt_file, rank_pred_file, spec_name, rank_shot_count))

        all_rank_pred.as_file(rank_pred_file)
        # NOTE: all_rank_expt is a dictionary to avoid weird OOM, so we write a simple json
        #all_rank_expt.as_file(rank_expt_file)
        with open(rank_expt_file, "w") as file_O:
            json.dump(all_rank_expt, file_O)
        print0("Done with predictions, combining dataframes")
        if all_dfs:
            all_dfs = pandas.concat(all_dfs)
        else:
            all_dfs = None

        if args.scanWeakFracs and all_dfs is not None:
            assert len(all_dfs.predictions.unique()) == 1
            pred_file = all_dfs.predictions.values[0]
            n_total_weak = np.sum(all_rank_pred['is_weak'])
            n_total = len(all_rank_pred)
            weak_fracs = [.11,.22,.33,.44,.55,.66,.77,.88]
            labels = []
            for i_frac, weak_frac in enumerate(weak_fracs):
                filt_refls = filter_weak_reflections(all_rank_pred, weak_frac)
                label="%dperc"%(weak_frac*100,)
                labels.append(label)
                new_pred_file = os.path.splitext(pred_file)[0]+"_%s.refl" % label
                all_dfs['predictions_%s' % label] = new_pred_file
                all_dfs['predicted_refs_%s' %label] = new_pred_file
                num_preds = []
                for exp_id in all_dfs.exp_idx.values:
                    n = np.sum(filt_refls['id'] == int(exp_id))
                    num_preds.append(n)
                all_dfs['num_pred_%s' %label] = num_preds
                filt_refls.as_file(new_pred_file)
                printR("Saved %d/%d refls (%d strong, %d/%d weak) to %s"
                       % (len(filt_refls), n_total, np.sum(filt_refls["is_strong"]), np.sum(filt_refls["is_weak"]), n_total_weak, new_pred_file))
            # note this sanity check below requires that weaK_fracs be sorted
            if sorted(weak_fracs) == weak_fracs:
                # then as weak frac increases, there should be an increasing number of predictions
                num_preds_per_frac = [all_dfs["num_pred_%s" % lab].sum() for lab in labels]
                assert num_preds_per_frac == sorted(num_preds_per_frac)

        print0("MPI gather all_dfs")
        all_dfs = COMM.gather(all_dfs)
        print0("MPI reduce lines")
        exp_ref_spec_lines = COMM.reduce(exp_ref_spec_lines)
        if COMM.rank==0:
            hopper_input_name = os.path.abspath(os.path.join(args.outdir , "%s.txt" % args.hopInputName))
            o = open(hopper_input_name, "w")
            for l in exp_ref_spec_lines:
                o.write(l)
            o.close()
            all_dfs = [df for df in all_dfs if df is not None]
            if not all_dfs:
                raise ValueError("No dataframes to concat: prediction/integration failed for all shots..")

            print("Concat frames")
            all_dfs = pandas.concat([df for df in all_dfs if df is not None])
            all_dfs.reset_index(inplace=True, drop=True)
            best_pkl_name = os.path.abspath(os.path.join(args.outdir , "%s.pkl" % args.hopInputName))
            all_dfs.to_pickle(best_pkl_name)
            print("Wrote %s (best_pickle option for simtbx.diffBragg.hopper) and %s (exp_ref_spec option for simtbx.diffBragg.hopper). Use them to run the predictions through hopper (use phil centroid=cal) or simtbx.diffBragg.stage_two." % (best_pkl_name, hopper_input_name))

            cmd_log_file = os.path.join(args.outdir, "cmdline_execution.txt")
            with open(cmd_log_file, "w") as o:
                o.write("integrate was run from folder: %s\n" % os.getcwd())
                o.write("The command line input was:\n")
                o.write(" ".join(sys.argv) + "\n")
                #TODO: write the diff phils here:


 *******************************************************************************


 *******************************************************************************
simtbx/command_line/make_input_file.py
from __future__ import division

# LIBTBX_SET_DISPATCHER_NAME diffBragg.make_input_file

from argparse import ArgumentParser
from argparse import ArgumentDefaultsHelpFormatter as show_defaults_formatter
parser = ArgumentParser(formatter_class=show_defaults_formatter)
parser.add_argument("dirnames", nargs="+", type=str, help="processing dirs")
parser.add_argument("filename", type=str, help="the name of the diffBragg input file written at the end of this script")
parser.add_argument("--splitDir", default=None, type=str, help="optional folder for writing split expts and refls. If None, then "
                                                               "split files will be written in same folders as their sources")
parser.add_argument("--exptSuffix", type=str, default="refined.expt", help="find experiments with this suffix")
parser.add_argument("--reflSuffix", type=str, default="indexed.refl", help="find reflection files with this suffix")
parser.add_argument("--write", action="store_true")


args = parser.parse_args()

import glob
import os
apath = os.path.abspath
import json

from dxtbx.model import ExperimentList
from dials.array_family import flex
from simtbx.diffBragg import hopper_io
import hashlib


from libtbx.mpi4py import MPI
COMM = MPI.COMM_WORLD

if COMM.rank==0 and args.write:
    if args.splitDir is not None and not os.path.exists(args.splitDir):
        os.makedirs(args.splitDir)


def hash_name(name):
    hash_obj = hashlib.md5(name.encode('utf-8'))
    return hash_obj.hexdigest()


def get_idx_path(El):
    """return the idx and path in a single-experiment experimentList"""
    assert len(El)==1
    iset = El[0].imageset
    assert len(iset)==1
    idx = iset.indices()[0]
    path = iset.paths()[0]
    return idx, path


def split_stills_expts(expt_f, refl_f, split_dir, write=False):
    El = ExperimentList.from_file(expt_f, False)
    R = flex.reflection_table.from_file(refl_f)
    expt_names = []
    refl_names = []
    orig_expt_names, orig_refl_names = [],[]  # store the original names for recordkeeping
    seen_isets = {}
    for i_expt in range(len(El)):
        one_exp_El = El[i_expt: i_expt+1]
        idx, path = get_idx_path(one_exp_El)
        iset_id = idx,path
        if iset_id not in seen_isets:
            seen_isets[iset_id] = 0
        else:
            seen_isets[iset_id] += 1
        tag = "%s-%d" % (os.path.basename(os.path.splitext(path)[0]), idx)
        new_expt_name = os.path.splitext(expt_f)[0] + "_%s_xtal%d.expt" % (tag, seen_isets[iset_id])
        if write and split_dir is not None:
            unique_tag = "shot_%s" % hash_name(new_expt_name) + ".expt"
            new_expt_name = os.path.join(split_dir, unique_tag)
        new_refl_name = new_expt_name.replace(".expt", ".refl")
        refls = R.select(R['id'] == i_expt)
        refls.reset_ids()

        if write:
            one_exp_El.as_file(new_expt_name)
            refls.as_file(new_refl_name)
        expt_names.append(new_expt_name)
        refl_names.append(new_refl_name)
        orig_expt_names.append((apath(new_expt_name), (apath(expt_f), i_expt)))
        orig_refl_names.append((apath(new_refl_name), (apath(refl_f), i_expt)))
    return expt_names, refl_names, orig_expt_names, orig_refl_names


exp_names, ref_names = [], []
orig_exp_names, orig_ref_names = [], []
for i_dir, dirname in enumerate(args.dirnames):
    expt_glob = os.path.join( dirname, "*%s" % args.exptSuffix)
    expt_fnames = glob.glob(expt_glob)
    if COMM.rank==0:
        print("dir %s, glob=%s, num files=%d" % (dirname, expt_glob, len(expt_fnames)))

    for i_f, f in enumerate(expt_fnames):
        if i_f % COMM.size != COMM.rank:
            continue
        if COMM.rank == 0:
            print("processing job %d / %d in dir %d / %d"
                  %(i_f+1, len(expt_fnames), i_dir+1, len(args.dirnames) ))
        ref_f = f.replace(args.exptSuffix, args.reflSuffix)
        if not os.path.exists(ref_f):
            raise FileNotFoundError("No matching refl file for expt %s" % f)
        El = ExperimentList.from_file(f, False)
        if len(El.imagesets()) > 1 or len(El.crystals()) > 1:
            exp_fs, ref_fs, orig_exp_fs, orig_ref_fs = split_stills_expts(f, ref_f, args.splitDir)
            exp_names += exp_fs
            ref_names += ref_fs
            orig_exp_names += orig_exp_fs
            orig_ref_names += orig_ref_fs
        else:
            exp_names.append(f)
            ref_names.append(ref_f)
            orig_exp_names.append((apath(f),    (apath(f),0))  )
            orig_ref_names.append((apath(ref_f),(apath(ref_f),0))    )

exp_names = COMM.reduce(exp_names)
ref_names = COMM.reduce(ref_names)
orig_exp_names = COMM.reduce(orig_exp_names)
orig_ref_names = COMM.reduce(orig_ref_names)


if COMM.rank==0:

    if args.write:
        print("Saving the input file for diffBragg")
        hopper_io.save_expt_refl_file(args.filename, exp_names, ref_names, check_exists=True)
        print("Saved %s" % args.filename)
        jname = args.filename + ".json"
        jdat = {"expt": dict(orig_exp_names), "refl": dict(orig_ref_names)}
        with open(jname, "w") as fp:
            json.dump(jdat, fp, indent=1)
        print("Wrote json %s, which maps the hashnames to the original expt files" % jname)
    else:
        _, exp_and_idx = zip(*orig_exp_names)
        _, ref_and_idx = zip(*orig_ref_names)
        exp, exp_idx = zip(*exp_and_idx)
        ref, ref_idx = zip(*ref_and_idx)
        assert exp_idx == ref_idx
        hopper_io.save_expt_refl_file(args.filename, exp, ref, check_exists=True,
                                      indices=exp_idx)


 *******************************************************************************


 *******************************************************************************
simtbx/command_line/model_img.py
from __future__ import division, print_function

from argparse import ArgumentParser

parser = ArgumentParser()
parser.add_argument("mod", type=str, help="path to the data modeler file")
parser.add_argument("out", type=str, help="output file name")
parser.add_argument("--cudaDevice", type=int, default=None,
                    help="use this gpu device (if not provided, will not use CUDA and will instead use OpenMP)")
parser.add_argument("--plotSpec", action="store_true", help="plot the energy spectrum")
parser.add_argument("--compareData", action="store_true", help="include the data image in the plot")
parser.add_argument("--j", type=int, default=8,
                    help="number of processes to use for background extraction, only relevant if --compareData flag is present (default: 8)")
parser.add_argument("--filtsz", type=int, default=10,
                    help="the median filter used to extract backgroun will have this dimension, pixel units, higher values are slower (default: 10)")
parser.add_argument("--specFile", type=str, default=None,
                    help="Path to the spectrum .lam file. If None, then assume hopper_process output tree and try to autolocate ")
parser.add_argument("--pandaFile", type=str, default=None,
                    help="Path to the pandas model file. If None, then assume hopper_process output tree and try to autolocate ")

args = parser.parse_args()

import glob
import os
import pandas
import numpy as np
from joblib import Parallel, delayed

from simtbx.diffBragg import utils
from simtbx.nanoBragg.utils import H5AttributeGeomWriter
from simtbx.modeling import forward_models
from scipy.ndimage import median_filter
from dxtbx.model import ExperimentList

# LIBTBX_SET_DISPATCHER_NAME diffBragg.model_img


def extract_background_from_data(data, njobs, filt_shape=None):
    """
    :param data: numpy array (3d) of multi panel data
    :param njobs: number of processors to use
    :param filt_shape: shape of median filter used to extract background (default is 10,10, better is 20,20 but its slower)
    :return: background image, same shape as data
    """
    extracted_bg = np.zeros_like(data)
    if filt_shape is None:
        filt_shape = 10,10
    def get_bg(jid, njobs):
        bgs = {}
        for pid, p in enumerate(data):
            if pid % njobs != jid:
                continue
            bg = median_filter(p, filt_shape)
            if jid==0:
                print("processing panel %d / %d" % (pid, len(data)) )
            bgs[pid] = bg
        return bgs
    results = Parallel(n_jobs=njobs)(delayed(get_bg)(jid,njobs) for jid in range(njobs))
    for bgs in results:
        for pid in bgs:
            extracted_bg[pid]+= bgs[pid]
    return extracted_bg


def get_pandas_name_from_mod_name(mod_name):
    """searches the diffBragg.hopper_process output filder for the pandas pickle corresponding to the
    modeler file mod_name"""
    pd_glob = os.path.join(os.path.dirname(mod_name), "../pandas/rank*/*.pkl")
    fnames = glob.glob(pd_glob)
    basename = os.path.basename(mod_name).split("_indexed_modeler.npy")[0]
    pd_name = [f for f in fnames if basename in f]
    assert len(pd_name)==1
    pd_name = pd_name[0]
    return pd_name


if __name__=="__main__":


    # the spectrum file
    spec_file = args.specFile
    if spec_file is None:
        spec_file = args.mod.replace("_modeler.npy", "_spectrum.lam")

    M = np.load(args.mod, allow_pickle=True)[()]
    pd_name = args.pandaFile
    if pd_name is None:
        pd_name = get_pandas_name_from_mod_name(args.mod)
    spec = utils.load_spectra_file(spec_file, as_spectrum=True)
    if args.plotSpec:
        import pylab as plt
        from simtbx.nanoBragg.utils import ENERGY_CONV
        wave,wt = zip(*spec)
        en = ENERGY_CONV/ np.array(wave)
        plt.plot(en, wt)
        plt.xlabel("spectrum energy (ev)")
        plt.ylabel("spectrum fluence")
        plt.suptitle("Close to continue...")
        plt.show()

    print("Will load %s" % pd_name)
    df = pandas.read_pickle(pd_name)
    print(df.iloc[0].to_string())

    Fname = M.params.simulator.structure_factors.mtz_name
    Fcol = M.params.simulator.structure_factors.mtz_column
    no_det_thick = M.params.simulator.detector.force_zero_thickness
    print("Will use mtz file %s, column %s" % (Fname, Fcol))
    print("no detector thickness=%s" % str(no_det_thick))
    print("Loaded data frame:")
    print("Wil simulate %d energy channels loaded from %s (add --plotSpec to display the spectrum)" % (len(spec), spec_file))

    cuda = False
    dev = 0
    if args.cudaDevice is not None:
        cuda = True
        dev = args.cudaDevice
        print("Will use cuda GPU device %d" % dev)
    else:
        print("Will not use cuda, but will use openmp, control with env var OMP_NUM_THREADS")
    imgs, expt = forward_models.model_spots_from_pandas(
            df, mtz_file=Fname, mtz_col=Fcol,
            spectrum_override=spec, cuda=cuda, device_Id=dev,
            force_no_detector_thickness=no_det_thick,
            use_db=True)

    if args.compareData:
        print("Extracting data from experiment list %s..." % df.exp_name.values[0])
        El = ExperimentList.from_file(df.exp_name.values[0])
        iset = El[0].imageset
        # NOTE : assumes a multi-panel detector model, otherwise get_raw_data should have no arg, e.g. iset.get_raw_data()
        data = np.array([a.as_numpy_array() for a in iset.get_raw_data(0)])
        # divide the data by the adu factor
        data /= M.params.refiner.adu_per_photon
        # extract the background:
        print("Extracting background from data ...")
        bg = extract_background_from_data(data, args.j, (args.filtsz, args.filtsz))
        imgs_w_bg  = imgs + bg  # this image is the extracted background and the optimized forward Bragg model

        # this image inclues data except for those pixels that were modeled during stage 1
        imgs2 = data.copy()
        P = M.all_pid
        F = M.all_fast
        S = M.all_slow
        imgs2[P,S,F] = M.best_model + M.all_background

    comp_args = {"compression": "gzip", "compression_opts": 9}
    kwargs ={
        "filename": args.out,
        "image_shape": imgs.shape,
        "num_images": 5 if args.compareData else 1,
        "detector": expt.detector,
        "beam": expt.beam,
        "dtype": np.float32,
        "compression_args": comp_args,
    }

    print("Saving compressed output...")
    with H5AttributeGeomWriter(**kwargs) as writer:
        writer.add_image(imgs)
        if args.compareData:
            writer.add_image(bg)
            writer.add_image(imgs_w_bg)
            writer.add_image(data)
            writer.add_image(imgs2)
    print("Wrote %s" % args.out)


 *******************************************************************************


 *******************************************************************************
simtbx/command_line/pred_offsets.py
from __future__ import division

# LIBTBX_SET_DISPATCHER_NAME diffBragg.pred_offsets

from argparse import ArgumentParser
parser = ArgumentParser()
parser.add_argument("globs", nargs="+", help="input globs  (in quotes) for groups of reflection tables.",
                    type=str)
parser.add_argument("--noPlot", action="store_true", help = "dont display graphic")
parser.add_argument("--linear", action="store_true", help="use linear histogram")
parser.add_argument("--nbins", type=int, nargs=2, default=[200,40])
args = parser.parse_args()

from pylab import *
from dials.array_family import flex
import glob


style.use("ggplot")
for glob_s in args.globs:
    fnames = glob.glob(glob_s)
    all_d = []
    all_shotd = []
    nref_per_shot = []
    for f in fnames:
        R = flex.reflection_table.from_file(f)
        if len(R)==0:
            continue
        x,y,_=R['xyzobs.px.value'].parts()
        x2,y2,_=R['xyzcal.px'].parts()
        assert x2 > 0
        assert y2 > 0
        d = np.sqrt( (x-x2)**2 + (y-y2)**2)
        med_d = np.median(d)
        print('diffBragg: %.3f (%s) Nref=%d' % (np.median(d), f, len(d) ) )
        all_d.append(d)
        all_shotd.append( np.median(d))
        nref_per_shot .append( len(d))

    all_d = hstack(all_d)
    print("median over %d shots=%f pixels (%d refls)" % (len(nref_per_shot), median(all_d), len(all_d)))
    print("Min refls per shot=%d, max refls per shot = %d, ave refls per shot=%.1f" % (min(nref_per_shot), max(nref_per_shot), mean(nref_per_shot)))
    subplot(121)
    hist( all_d, bins=args.nbins[0], histtype='step', lw=2, label=glob_s)
    if not args.linear:
        gca().set_yscale("log")
    xlabel("|calc-obs| (pixels)", fontsize=17)
    ylabel("num refls", fontsize=15)
    #legend(prop={'size':12})
    gca().tick_params(direction='in', which='both', labelsize=15)
    subplot(122)
    hist( all_shotd, bins=args.nbins[1], histtype='step', lw=2, label=glob_s)
    if not args.linear:
        gca().set_yscale("log")
    xlabel("median |calc-obs| (pixels)", fontsize=17)
    ylabel("num shots", labelpad=0, fontsize=17)
    #legend(prop={'size':11})
    gca().tick_params(direction='in', which='both', labelsize=15)

gcf().set_size_inches((7,4))
suptitle(" %d shots ; %d refls" % (len(nref_per_shot), len(all_d)))
subplots_adjust(bottom=0.2, right=0.97, top=0.92, left=0.1)

if args.noPlot:
    exit()

show()


 *******************************************************************************


 *******************************************************************************
simtbx/command_line/shoebx.py
from __future__ import division

# LIBTBX_SET_DISPATCHER_NAME diffBragg.shoebx

from pylab import *

from argparse import ArgumentParser
parser = ArgumentParser()
parser.add_argument("modeler_file", type=str, help="path to a diffBragg modeler file (output from hopper, see the imgs folder in the outdir)")
parser.add_argument("--stacked", action="store_true")
parser.add_argument("--model_clim", nargs=2, default=[0,None], type=float)
parser.add_argument("--data_clim", nargs=2, default=[0,None], type=float)
args = parser.parse_args()


FIG,(ax0,ax1,ax2) = subplots(nrows=1,ncols=3)
FIG.set_size_inches((5,2))


def press(event):
    if event.key == 'right':
        FIG.loop_counter += 1
    elif event.key=="left":
        FIG.loop_counter = FIG.loop_counter -1
    FIG.loop_counter = max(FIG.loop_counter,0)
    FIG.loop_counter = min(FIG.loop_counter, FIG.nspots-1)

    if event.key=="escape":
        FIG.loop_counter = FIG.nspots

def centroid_poly(x,y):
    y1 = y[:-1]
    y2 = y[1:]

    x1 = x[:-1]
    x2 = x[1:]
    xy = (x1*y2 - x2*y1)
    Cx = np.sum((x1+x2)*xy)/6.
    Cy = np.sum((y1+y2)*xy)/6.
    A = 0.5*np.sum(xy)
    return Cx/A, Cy/A


M = np.load(args.modeler_file, allow_pickle=True)[()]
num_spots = len(M.pids)

FIG.loop_counter = 0
FIG.nspots = num_spots
FIG.canvas.mpl_connect('key_press_event', press)

assert len(set(M.roi_id)) == max(M.roi_id)+1
sigma_rdout = M.params.refiner.sigma_r / M.params.refiner.adu_per_photon
#M.hi = (0,0,0)]
#hkls = M.Hi
cmap = 'gray_r'

from cctbx import uctbx
# TODO put unit cell manager in the modeler file
#PSII
uc = uctbx.unit_cell((116.92, 221.635, 307.834, 90, 90, 90))
#lysozyme?
uc = uctbx.unit_cell((79.1,79.1,38.4,90,90,90))
diffs = []
d_max = 0
d_min = 9999

#F2 = plt.figure()
#AX2 = F2.gca()
#AX2.add_patch(Rectangle(xy=(0,0), width=4000, height=4000, fc='none', ec='b'))
#AX2.set_xlim(-10,4010)
#AX2.set_ylim(-10,4010)


if args.stacked:

    if isinstance(M.all_sigma_rdout, np.ndarray):
        data_subimg, model_subimg, trusted_subimg, bragg_subimg, sigma_rdout_subimg = M.get_data_model_pairs()
    else:
        data_subimg, model_subimg, trusted_subimg, bragg_subimg = M.get_data_model_pairs()
        sigma_rdout_subimg = None

    sub_sh = tuple(np.max([im.shape for im in model_subimg], axis=0))
    size_edg = int(np.sqrt(len(data_subimg))) + 1
    full_im = np.zeros((size_edg * sub_sh[0], size_edg * sub_sh[1]))
    full_dat_im = np.zeros((size_edg * sub_sh[0], size_edg * sub_sh[1]))
    for j in range(size_edg):
        for i in range(size_edg):
            mod_idx = j * size_edg + i
            if mod_idx >= len(model_subimg):
                continue
            im = model_subimg[mod_idx].copy()
            dat_im = data_subimg[mod_idx].copy()
            ydim, xdim = im.shape
            if im.shape != sub_sh:
                im = np.pad(im, ((0, sub_sh[0] - ydim), (0, sub_sh[1] - xdim)), mode='constant', constant_values=np.nan)
                dat_im = np.pad(dat_im, ((0, sub_sh[0] - ydim), (0, sub_sh[1] - xdim)), mode='constant',
                                constant_values=np.nan)
            Ysl = slice(j * sub_sh[0], (j + 1) * sub_sh[0], 1)
            Xsl = slice(i * sub_sh[1], (i + 1) * sub_sh[1], 1)
            full_im[Ysl, Xsl] = im
            full_dat_im[Ysl, Xsl] = dat_im
    subplot(121)
    gca().set_title("DATA", fontsize=18)
    imshow(full_dat_im, vmin=args.data_clim[0], vmax=args.data_clim[1], cmap='gray_r')
    gca().set_xticks(np.arange(sub_sh[1], size_edg*sub_sh[1], sub_sh[1]))
    gca().set_yticks(np.arange(sub_sh[0], size_edg*sub_sh[0], sub_sh[0]))
    gca().grid(1, color='k', ls='--')
    gca().set_xticklabels([])
    gca().set_yticklabels([])
    gca().tick_params(length=0)
    #gca().set_xlim(0, size_edg*sub_sh[1])
    #gca().set_ylim(0, size_edg*sub_sh[0])
    #gca().set_yticks([])
    subplot(122)
    gca().set_title("MODEL", fontsize=18)
    imshow(full_im, vmin=args.model_clim[0], vmax=args.model_clim[1], cmap='gnuplot')
    gca().set_xticks(np.arange(sub_sh[1], size_edg*sub_sh[1], sub_sh[1]))
    gca().set_yticks(np.arange(sub_sh[0], size_edg*sub_sh[0], sub_sh[0]))
    gca().grid(1, color='w', ls='--')
    gca().set_xticklabels([])
    gca().set_yticklabels([])
    gca().tick_params(length=0)
    #gca().set_xlim(0, size_edg*sub_sh[1])
    #gca().set_ylim(0, size_edg*sub_sh[0])
    #gca().set_xticks([])
    #gca().set_yticks([])
    subplots_adjust(wspace=0, hspace=0, bottom=0.02, top=0.95, left=0.01, right=0.99)
    show()
    exit()


a = b = None
while FIG.loop_counter < num_spots:
    i_h = FIG.loop_counter
    h = k = l = np.nan
    d = -1 #uc.d((h,k,l))
    #if d > d_max: d_max = d
    #if d < d_min: d_min = d

    # you could put a filter here to choose only those refls in a certain resolution
    sel = M.roi_id == i_h
    x1, x2, y1, y2 = M.rois[i_h]
    sh = y2 - y1, x2 - x1
    data = M.all_data[sel].reshape(sh)
    trusted = M.all_trusted[sel].reshape(sh)
    any_trusted = np.any(trusted)

    bg = M.all_background[sel].reshape(sh)
    bragg = M.best_model[sel].reshape(sh)
    model = bragg + bg
    diff = data.sum() - model.sum()

    Z = (model - data) / np.sqrt(model + sigma_rdout ** 2)
    diffs.extend(Z.ravel())

    vals = Z[trusted]
    data_trust = data[trusted]
    if not any_trusted:
        data_thresh = 0
    else:
        data_thresh = np.percentile(data_trust,97)
    y,x = np.indices(data.shape)

    ycent = (bragg.ravel()*y.ravel()).sum() /bragg.sum()
    xcent = (bragg.ravel()*x.ravel()).sum() / bragg.sum()

    if not any_trusted:
        sigZ_val = np.nan
    else:
        sigZ_val = Z[trusted].std()

    for ax in ax0,ax1,ax2:
        ax.clear()
    im = ax0.imshow(data, cmap=cmap)
    vmin, vmax = im.get_clim()
    ax1.imshow(model,cmap=cmap, vmin=vmin, vmax=vmax)

    ax1.contour(bragg, levels=[1e-4,1e-3,1e-2,1e-1,1,10], cmap='jet')
    C = ax0.contour(bragg, levels=[1e-4,1e-3,1e-2,1e-1,1,10], cmap='jet')
    ax1.plot( xcent, ycent, 'rx')
    #C = ax0.contour(bragg, levels=[0.1*data_thresh, 0.5*data_thresh,data_thresh], cmap='jet')
    #from IPython import embed;embed()

    #try:
    #    segs = C.allsegs[-1][0]
    #    xcent, ycent = centroid_poly(segs[:,0], segs[:,1])
    #    ax0.plot( [xcent], [ycent], 'o', mec='b', ms=10, alpha=0.5)
    #    ax0.plot( segs[:,0], segs[:,1], '.', ms=3, color='k')
    #except IndexError:
    #    pass

    v = np.max(np.abs(Z))
    v = 10
    ax2.imshow(Z, cmap='RdYlBu', vmin=-v, vmax=v)

    for AX in [ax0, ax1, ax2]:
        AX.grid(1, color='#777777', ls='--', lw=0.4)
        AX.set_xticklabels([])
        AX.set_yticklabels([])
    ax0.set_title("data %d,%d,%d,%d"% (x1,x2,y1,y2), pad=0, fontsize=8)
    ax1.set_title("model", pad=0, fontsize=8)
    ax2.set_title("Z (sigZ=%f)" % sigZ_val, pad=0, fontsize=8)

    FIG.suptitle("spot %d" % i_h)
    if a is not None:
        a.remove()
    if b is not None:
        b.remove()
    a = FIG.add_axes([.91, .25, 0.02, .5])
    FIG.colorbar(ax2.images[0], cax=a)
    a.tick_params(length=0, labelsize=8, pad=1)
    a.set_title("Z", pad=0, fontsize=8)

    b = FIG.add_axes([.2, .1, .33, .02])
    FIG.colorbar(ax0.images[0], cax=b, orientation='horizontal')
    b.yaxis.set_ticks_position('left')
    b.tick_params(length=0, labelsize=8, pad=5)
    b.set_title("counts", pad=0, fontsize=8)
    draw()
    waitforbuttonpress()

    i_h += 1

plt.close()


 *******************************************************************************


 *******************************************************************************
simtbx/command_line/simbtx_launch.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME simtbx
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1

'''
Author      : Lyubimov, A.Y.
Created     : 12/12/2017
Last Changed: 10/30/2018
Description : SIMTBX (nanoBragg) GUI startup module.
'''

import wx
from simtbx.nanoBragg.nanoBragg_gui_init import MainWindow

class MainApp(wx.App):
  ''' App for the main GUI window  '''
  def OnInit(self):
    self.frame = MainWindow(None, -1, title='SIMTBX (nanoBragg)')

    # Find mouse position and open window there
    mx, my = wx.GetMousePosition()
    self.frame.SetPosition((mx, my))

    # Get display index and geometry
    n_display = wx.Display.GetFromPoint((mx, my))
    display = wx.Display(index=n_display)
    geom = display.GetGeometry()

    # Set main frame size to fraction of display geometry and center window
    self.frame.SetSize((0.5*geom[2], 0.75*geom[3]))
    self.frame.Center()

    self.frame.Show(True)
    self.SetTopWindow(self.frame)
    return True

if __name__ == '__main__':
  app = MainApp(0)
  app.MainLoop()


 *******************************************************************************


 *******************************************************************************
simtbx/command_line/spectra.py
from __future__ import division

# LIBTBX_SET_DISPATCHER_NAME diffBragg.spectra

#TODO: add text entry boxes for other spectrum filter params, position entry boxes sensibly

from simtbx.diffBragg import hopper_utils
from pylab import *
import dxtbx
from simtbx.diffBragg.phil import philz, hopper_phil
from libtbx.phil import parse
from matplotlib.widgets import TextBox

from argparse import ArgumentParser
parser = ArgumentParser()
parser.add_argument("image_file", type=str, help="path to a diffBragg modeler file (output from hopper, see the imgs folder in the outdir)")
parser.add_argument("--filt_freq", default=0.07, type=float)
parser.add_argument("--filt_order", default=3, type=float)
parser.add_argument("--tail", default=50, type=int)
parser.add_argument("--delta_en", default=0.5, type=float)
parser.add_argument("--skip", action="store_true")
args = parser.parse_args()

FIG,ax0 = subplots(nrows=1,ncols=1)
FIG.set_size_inches((5,3))

class P:

    def __init__(self, params, imgset, ax0, FIG):
        self.params = params
        self.imgset = imgset
        self.ax0 = ax0
        self.FIG= FIG

    def entry(self, text):
        delta_en = float(text)
        self.params.downsamp_spec.delta_en = delta_en
        print(delta_en)
        self.update_plot(self.imgset)

    def update_plot(self, i_img):
        raw_spec = self.imgset.get_spectrum(i_img)
        raw_en = raw_spec.get_energies_eV()
        raw_wt = raw_spec.get_weights()

        spec = hopper_utils.downsamp_spec_from_params(self.params, imgset=self.imgset, i_img=i_img)
        en, wt = map(np.array, zip(*spec))
        en = hopper_utils.utils.ENERGY_CONV / en

        self.ax0.clear()
        self.ax0.plot( raw_en, raw_wt, lw=2, label="raw spec (%d chan)" % len(raw_en))
        self.ax0.plot( en, wt, '--', lw=1, label="filt spec (%d chan)" % len(en))
        self.FIG.suptitle("image %d" % (i_img, ), fontsize=14)
        self.ax0.set_xlabel("Energy (eV)", fontsize=12)
        self.ax0.tick_params(labelsize=11)
        self.ax0.legend(prop={"size":11}, loc="upper right")
        draw()


def press(event):
    if event.key == 'right':
        FIG.loop_counter += 1
    elif event.key=="left":
        FIG.loop_counter = FIG.loop_counter -1
    FIG.loop_counter = max(FIG.loop_counter,0)
    FIG.loop_counter = min(FIG.loop_counter, FIG.nspots-1)

    if event.key=="escape":
        FIG.loop_counter = FIG.nspots

FIG.loop_counter = 0
loader = dxtbx.load(args.image_file)
imgset = loader.get_imageset(loader.get_image_file())
FIG.nspots = len(imgset)
FIG.canvas.mpl_connect('key_press_event', press)

phil_scope = parse(philz + hopper_phil)
params = phil_scope.fetch(sources=[phil_scope]).extract()

params.downsamp_spec.filt_freq = args.filt_freq
params.downsamp_spec.filt_order = args.filt_order
params.downsamp_spec.tail = args.tail
params.downsamp_spec.delta_en = args.delta_en
params.downsamp_spec.skip = args.skip

Pinst = P(params, imgset, ax0, FIG)
axbox = plt.axes([0.25, 0.75, 0.1, 0.075])
text_box = TextBox(axbox, 'delta_en', initial="%.2f" % args.delta_en)
text_box.on_submit(Pinst.entry)

while FIG.loop_counter < len(imgset):
    i_img = FIG.loop_counter

    Pinst.update_plot(i_img)

    waitforbuttonpress()
    i_img += 1

plt.close()


 *******************************************************************************


 *******************************************************************************
simtbx/command_line/stage_two.py
from __future__ import absolute_import, division, print_function

# LIBTBX_SET_DISPATCHER_NAME simtbx.diffBragg.stage_two

from libtbx.mpi4py import MPI
from simtbx.command_line.hopper import hopper_phil
import time
import logging
import os
from simtbx.diffBragg import mpi_logger

from simtbx.diffBragg.device import DeviceWrapper

COMM = MPI.COMM_WORLD

if COMM.rank > 0:
    import warnings
    warnings.filterwarnings("ignore")


from simtbx.diffBragg.phil import philz
from simtbx.diffBragg import ensemble_refine_launcher
from libtbx.phil import parse
from dials.util import show_mail_on_error

help_message = "stage 2 (global) diffBragg refinement"

script_phil = """
pandas_table = None
  .type = str
  .help = path to an input pandas table (usually output by simtbx.diffBragg.predictions)
refls_key = predictions
  .type = str
  .help = name of the predicted refls column in the pandas table input
max_sigz = 10.
  .type = float
  .help = Maximum allowed value ot sigz in the input pandas table  (dataframe)
  .help = (high sigz above 10 usually indicates failed stage 1 refinement)
"""

philz = script_phil + philz + hopper_phil
phil_scope = parse(philz)


class Script:

    def __init__(self):
        from dials.util.options import ArgumentParser

        self.params = None
        if COMM.rank == 0:
            self.parser = ArgumentParser(
                usage="",  # stage 1 (per-shot) diffBragg refinement",
                sort_options=True,
                phil=phil_scope,
                read_experiments=False,
                read_reflections=False,
                check_format=False,
                epilog=help_message)
            self.params, _ = self.parser.parse_args(show_diff_phil=COMM.rank==0)
            outdir = self.params.refiner.io.output_dir
            if outdir is not None:
                if not os.path.exists(outdir):
                    os.makedirs(outdir)
                diff_phil_outname = os.path.join(outdir, "diff.phil")
                with open(diff_phil_outname, "w") as o:
                    o.write(self.parser.diff_phil.as_str())
        self.params = COMM.bcast(self.params)

    def run(self):
        #self.params = None
        #if COMM.rank == 0:
        #self.params = COMM.bcast(self.params)
        if self.params.pandas_table is None:
            raise ValueError("Pandas table input required")

        refine_starttime = time.time()
        refiner = ensemble_refine_launcher.global_refiner_from_parameters(self.params)
        print("Time to refine experiment: %f" % (time.time()- refine_starttime))

        #TODO save MTZ


if __name__ == '__main__':
    try:
        from line_profiler import LineProfiler
    except ImportError:
        LineProfiler = None
    from simtbx.diffBragg.refiners import stage_two_refiner
    from simtbx.diffBragg import hopper_utils
    with show_mail_on_error():
        script = Script()
        RUN = script.run
        lp = None
        if LineProfiler is not None and script.params.profile:
            lp = LineProfiler()
            lp.add_function(ensemble_refine_launcher.RefineLauncher.launch_refiner)
            lp.add_function(ensemble_refine_launcher.RefineLauncher.load_inputs)
            lp.add_function(stage_two_refiner.StageTwoRefiner._compute_functional_and_gradients)
            lp.add_function(stage_two_refiner.StageTwoRefiner._run_diffBragg_current)
            lp.add_function(stage_two_refiner.StageTwoRefiner._update_Fcell)
            lp.add_function(stage_two_refiner.StageTwoRefiner._scale_pixel_data)
            lp.add_function(stage_two_refiner.StageTwoRefiner._Fcell_derivatives)
            lp.add_function(stage_two_refiner.StageTwoRefiner._mpi_aggregation)
            lp.add_function(stage_two_refiner.StageTwoRefiner._setup)
            lp.add_function(hopper_utils.DataModeler.GatherFromExperiment)
            RUN = lp(script.run)

        if script.params.outdir is None:
            od = script.params.refiner.io.output_dir
            script.params.outdir = od if od is not None else '.'

        if script.params.logging.logname is None:
            script.params.logging.logname = "main_stage2.log"
        if script.params.profile_name is None:
            script.params.profile_name = "prof_stage2.log"
        if script.params.logging.disable:
            logging.disable(level=logging.CRITICAL)  # disables CRITICAL and below
        else:
            mpi_logger.setup_logging_from_params(script.params)

        script.params.simulator.device_id = COMM.rank % script.params.refiner.num_devices
        with DeviceWrapper(script.params.simulator.device_id) as _:
            RUN()

        if lp is not None:
            stats = lp.get_stats()
            from simtbx.diffBragg import hopper_utils
            hopper_utils.print_profile(stats,
                    ["launch_refiner", "_compute_functional_and_gradients", "_run_diffBragg_current",
                     "_update_Fcell", "_scale_pixel_data", "_Fcell_derivatives", "_mpi_aggregation",
                     "GatherFromExperiment", "_setup", "load_inputs"])


 *******************************************************************************


 *******************************************************************************
simtbx/command_line/update_stage1_phil.py

from __future__ import division
from argparse import ArgumentParser
parser = ArgumentParser()
parser.add_argument("dirname", help="stage1 output folder with pandas and diff.phil", type=str)
parser.add_argument("newPhil", default=None, help="new stage 1 phil file", type=str)
parser.add_argument("--setGlim", action="store_true", help="Use unrestrained stage1 to set bounds on G")
#parser.add_argument("--njobs", type=int, default=5, help="number of jobs (only runs on single node, no MPI)")
#parser.add_argument("--plot", action="store_true", help="show a histogram at the end")
args = parser.parse_args()

# LIBTBX_SET_DISPATCHER_NAME diffBragg.update_stage1_phil

import os
import pandas
import numpy as np
import glob

glob_s = os.path.join(args.dirname, "pandas/*pkl")
fnames = glob.glob(glob_s)

# read in the pandas pickles
df1 = pandas.concat( [pandas.read_pickle(f) for f in fnames]).reset_index(drop=True)

# unit cell phil
a,b,c,al,be,ga = df1[['a', 'b', 'c', 'al', 'be', 'ga']].median()

# Ncells abc and Nvol
na, nb, nc = np.vstack(df1.ncells).T
nvol = na*nb*nc
nvol = np.median(na*nb*nc)
na, nb, nc = map(np.median, (na, nb, nc))

# eta
ea, eb, ec = map( np.median, np.vstack(df1.eta_abc).T)

# spot scale (G)
Gmed = df1.spot_scales.median()
Gmin = df1.spot_scales.min()/100
Gmax = df1.spot_scales.max()*100

update_phil = """
init {{
  G = {G}
  Nabc = [{na},{nb},{nc}]
  eta_abc = [{ea},{eb},{ec}]
}}
centers {{
  Nvol = {nvol}
  ucell_a = {a}
  ucell_b = {b}
  ucell_c = {c}
  ucell_alpha = {al}
  ucell_beta = {be}
  ucell_gamma = {ga}
}}
betas {{
  Nvol = 1e-2
  ucell_a = 1e-7
  ucell_b = 1e-7
  ucell_c = 1e-7
  ucell_alpha = 1e-7
  ucell_beta = 1e-7
  ucell_gamma = 1e-7
}}
use_restraints = True
""".format(G=Gmed,na=na, nb=nb, nc=nb, ea=ea, eb=eb, ec=ec,a=a,b=b,c=c,al=al,be=be,ga=ga, nvol=nvol)

Gmin_Gmax="""
mins.G={Gmin}
maxs.G={Gmax}\n""".format(Gmin=Gmin, Gmax=Gmax)

if args.setGlim:
    update_phil += Gmin_Gmax

diff_phil_name = os.path.join(args.dirname, "diff.phil")
assert os.path.exists(diff_phil_name)
diff_phil = open(diff_phil_name, "r").read()
with open(args.newPhil, "w") as o:
    o.write(diff_phil + "\n"+ update_phil)

print("Done. added \n%s\n to the %s and saved to %s" % (update_phil, diff_phil_name, args.newPhil) )


 *******************************************************************************


 *******************************************************************************
simtbx/diffBragg/__init__.py
from __future__ import absolute_import, division, print_function
from boost_adaptbx import boost
import boost_adaptbx.boost.python as bp
import cctbx.uctbx # possibly implicit
from simtbx import nanoBragg # implicit import
ext = boost.python.import_ext("simtbx_diffBragg_ext")
import numpy as np
from simtbx_diffBragg_ext import *

@bp.inject_into(ext.diffBragg)
class _():
    def get_derivative_pixels(self, refine_id):
        return self.__get_derivative_pixels(refine_id)[:self.__number_of_pixels_modeled_using_diffBragg]

    def update_Fhkl_channels(self, channel_ids):
        """channel_ids is a  numpy int array the same length as the number of energy sources,
        that specifies the mapping of structure factor to energy channel,
        allowing one to refine multiple energy-dependent structure factors
        for example in a two-color experiment
        """
        assert isinstance(channel_ids, np.ndarray)
        channel_ids = self._check_contig(channel_ids)
        if channel_ids.dtype != np.int32:
            channel_ids = channel_ids.astype(np.int32)
        # assert len(channel_ids)== number_of_sources
        self.__update_Fhkl_channels(channel_ids)

    def update_Fhkl_scale_factors(self, scales, num_Fhkl_channels):
        assert isinstance(scales, np.ndarray)
        scales = self._check_contig(scales)
        if scales.dtype != np.float64:
            print("Warning, converting scale factors to double!")
            scales = scales.astype(np.float64)
        # TODO add num_asu property, then do the assertion
        #assert len(scales) == self.num_asu*num_Fhkl_channels
        self.__update_Fhkl_scale_factors(scales, num_Fhkl_channels)

    def _check_contig(self, arr):
        if not arr.flags.c_contiguous:
            print("WARNINGS:Making contiguous")
            arr = np.ascontiguousarray(arr)
        return arr

    def get_rotate_principal_axes(self):
        return self._ext_rotate_principal_axes

    def set_rotate_principal_axes(self, val):
        # do stuff to set val, e.g renormalize
        # val =
        self._ext_rotate_principal_axes = val  # as a 9-tuple or a scitbx matrix sqr

    def add_Fhkl_gradients(self, psf, residuals, variance, trusted, freq, num_Fhkl_channels, spot_scale,
                           track=False, errors=False):
        """

        :param psf:
        :param residuals:
        :param variance:
        :param trusted:
        :param freq:
        :param num_Fhkl_channels:
        :param spot_scale:
        :return:
        """

        assert isinstance(residuals, np.ndarray)
        assert isinstance(variance, np.ndarray)
        assert isinstance(trusted, np.ndarray)
        assert isinstance(freq, np.ndarray)
        residuals = self._check_contig(residuals)
        variance = self._check_contig(variance)
        trusted = self._check_contig(trusted)
        Npix = len(psf)/3
        assert Npix == len(residuals)
        assert Npix == len(variance)
        assert Npix == len(trusted)
        assert Npix == len(freq)
        # TODO check or contiguous arrays..

        assert trusted.dtype==bool
        assert freq.dtype==np.int32
        assert residuals.dtype==np.float64
        assert variance.dtype==np.float64
        return self.__add_Fhkl_gradients(psf, residuals, variance, trusted, freq, num_Fhkl_channels, spot_scale,
                                         track,errors)

    def ave_I_cell(self, use_Fhkl_scale=False, i_channel=0, use_geometric_mean=False):
        """

        :param use_Fhkl_scale:
        :param i_channel:
        :param use_geometric_mean: boolean flag, whether arithmetic mean or geometric mean is computed per res bin
        :return:
        """
        # TODO add sanity checks here,
        if not self.dspace_bins:
            raise ValueError("Set the dspace_bins property first . See sim_data.SimData method set_dspace_binning")
        if use_Fhkl_scale and not self.Fhkl_have_scale_factors:
            raise RuntimeError("Set the Fhkl scale factors first! See method self.update_Fhkl_scale_factors")
        return self._ave_I_cell(use_Fhkl_scale, i_channel, use_geometric_mean)

    def Fhkl_restraint_data(self, i_channel=0, restraint_beta=1e12, use_geometric_mean=False, how="ave"):
        """

        :param i_channel:  Fhkl channel, should be 0 unless refining wavelength-dependent Fhkl (e.g. two color)
        :param restraint_beta: restraing the average Fhkl to the initial average Fhkl(per res bin)
        :param use_geometric_mean: boolean flag, whether arithmetic mean or geometric mean is computed per res bin
        :return: (target, gradient vector) , 2-tuple contribution to target and gradient in hopper_utils
        """
        assert how in ["ave", "Friedel", "init"]
        if how=="ave" or how == "init":
            if not self.dspace_bins:
                raise ValueError("Set the dspace_bins property first . See sim_data.SimData method set_dspace_binning")
            assert self.dspace_bins

        if not self.Fhkl_have_scale_factors:
            raise RuntimeError("Set the Fhkl scale factors first! See method self.update_Fhkl_scale_factors")

        flags = {"ave":0, "Friedel": 1, "init": 2}   # controls which underlying restraint method is called in diffBragg

        restraint_data = self._Fhkl_restraint_data(i_channel, restraint_beta, use_geometric_mean, flags[how])
        # the restraint data is always the gradient terms (1 per ASU) followed by the contribution to the target function
        grad_portion = restraint_data[:-1]
        assert grad_portion.shape[0] == self.Num_ASU
        target = restraint_data[-1]
        return target, grad_portion

    def prep_Friedel_restraints(self):
        """
        set the indices of all the positive and negative Fridel mates, for use within diffBragg
        to compute Friedel mate retraints .
        During refinement of Fhkls, Friedel mates should not drift too far apart, especially in the absence
        of anomalous signals
        """
        asu_map = self.get_ASUid_map()
        asu_map_int = {tuple(map(int, k.split(','))): v for k, v in asu_map.items()}
        pos_h = set([h for h in asu_map_int if np.all(np.array(h) > 0)])
        pos_inds = []
        neg_inds = []
        for h in pos_h:
            h_minus = -h[0],-h[1],-h[2]
            if h_minus not in asu_map_int:
                continue
            pos_inds.append(int(asu_map_int[h]))
            neg_inds.append(int(asu_map_int[h_minus]))

        self._set_Friedel_mate_inds(pos_inds, neg_inds)


 *******************************************************************************


 *******************************************************************************
simtbx/diffBragg/attr_list.py
from __future__ import division

"""
critical properties of diffBragg objects which should be logged for reproducibility
"""
# TODO :  implement a savestate and getstate for these objects

# attrs of diffBragg() instances
DIFFBRAGG_ATTRS = [
 'Amatrix',
 'Bmatrix',
 'Ncells_abc',
 'Ncells_abc_aniso',
 'Ncells_def',
 'Npix_to_allocate',
 'Omatrix',
 'Umatrix',
 'beamsize_mm',
 'compute_curvatures',
 'default_F',
 'detector_thick_mm',
 'detector_thickstep_mm',
 'detector_thicksteps',
 'detector_twotheta_deg',
 'device_Id',
 'diffuse_gamma',
 'diffuse_sigma',
 'exposure_s',
 'fluence',
 'flux',
 'has_anisotropic_mosaic_spread',
 'interpolate',
 'isotropic_ncells',
 'lambda_coefficients',
 'mosaic_domains',
 'mosaic_spread_deg',
 'no_Nabc_scale',
 'nopolar',
 'only_diffuse',
 'only_save_omega_kahn',
 'oversample',
 'oversample_omega',
 'phi_deg',
 'phistep_deg',
 'phisteps',
 'point_pixel',
 'polar_vector',
 'polarization',
 'spindle_axis',
 'spot_scale',
 'twotheta_axis',
 'unit_cell_Adeg',
 'unit_cell_tuple',
 'use_diffuse',
 'use_lambda_coefficients']


# properties of nanoBragg_crystal.NBcryst instances
NB_CRYST_ATTRS = [
 'anisotropic_mos_spread_deg',
 'isotropic_ncells',
 'miller_is_complex',
 'mos_spread_deg',
 'n_mos_domains',
 'symbol',
 'xtal_shape']


# properties of nanoBragg_beam.NBbeam instances
NB_BEAM_ATTRS = [
 'divergence_mrad',
 'divsteps',
 'polarization_fraction',
 'size_mm',
 'number_of_sources',
 'unit_s0']


 *******************************************************************************


 *******************************************************************************
simtbx/diffBragg/device.py
from __future__ import division
import libtbx.load_env  # implicit import
DIFFBRAGG_HAS_KOKKOS = libtbx.env.build_options.enable_kokkos

if DIFFBRAGG_HAS_KOKKOS:
    # TODO fix simtbx.kokkos import ...
    # from simtbx.kokkos import gpu_instance
    from simtbx.diffBragg import initialize_kokkos, finalize_kokkos

import os
USE_KOKKOS_GPU = DIFFBRAGG_HAS_KOKKOS and ("DIFFBRAGG_USE_KOKKOS" in os.environ)


class DeviceWrapper:

    def __init__(self, gpu_id, force=False):
        """

        :param gpu_id: gpu device id  (int from 0 to Ngpu-1)
        :param force: force initialization of kokkos, otherwise, the os.environ will be searched for DIFFBRAGG_USE_KOKKOS
        """
        self.gpu_id = gpu_id
        self.force_kokkos_init = force

    def __enter__(self):
        if USE_KOKKOS_GPU or self.force_kokkos_init:
            initialize_kokkos(self.gpu_id)
        return self

    def __exit__(self, tp, val, tbk):
        if USE_KOKKOS_GPU or self.force_kokkos_init:
            finalize_kokkos()


 *******************************************************************************


 *******************************************************************************
simtbx/diffBragg/ensemble_refine_launcher.py
from __future__ import absolute_import, division, print_function
from simtbx.diffBragg.stage_two_utils import PAR_from_params
from collections import Counter
from itertools import chain
import os
import sys
from libtbx.mpi4py import MPI
COMM = MPI.COMM_WORLD
from dials.array_family import flex
import numpy as np
try:
    import pandas
except ImportError:
    print("Pandas is required. Install using 'libtbx.python -m pip install pandas'")
    exit()
from simtbx.diffBragg.refiners.stage_two_refiner import StageTwoRefiner
from simtbx.diffBragg import utils
from simtbx.diffBragg import hopper_utils
from dxtbx.model.experiment_list import ExperimentList, ExperimentListFactory
from simtbx.diffBragg.prep_stage2_input import prep_dataframe
from cctbx import miller, crystal
import logging

LOGGER = logging.getLogger("diffBragg.main")


def global_refiner_from_parameters(params):
    launcher = RefineLauncher(params)
    # TODO read on each rank, or read and broadcast ?
    LOGGER.info("EVENT: read input pickle")
    pandas_table = pandas.read_pickle(params.pandas_table)
    if params.max_sigz is not None and "sigz" in list(pandas_table):
        Nframe = len(pandas_table)
        pandas_table = pandas_table.query("sigz < %f" % params.max_sigz)
        pandas_table.reset_index(drop=True, inplace=True)
        LOGGER.info("Removed %d / %d dataframes due to max_sigz=%.2f filter"
                    % (Nframe - len(pandas_table), Nframe, params.max_sigz))
    if params.max_process > 0:
        pandas_table = pandas_table.iloc[:params.max_process]
    LOGGER.info("EVENT: BEGIN prep dataframe")
    if "exp_idx" not in list(pandas_table):
        pandas_table["exp_idx"] = 0
    work_distribution = prep_dataframe(pandas_table, res_ranges_string=params.refiner.res_ranges)
    LOGGER.info("EVENT: DONE prep dataframe")
    return launcher.launch_refiner(pandas_table, work_distribution=work_distribution, refls_key=params.refls_key)


class RefineLauncher:

    def __init__(self, params):
        self.params = self.check_parameter_integrity(params)
        self.n_shots_on_rank = None
        self.df = None
        self.Modelers = {}
        self.Hi = {}
        self.Hi_asu = {}
        self.symbol = None
        self.DEVICE_ID = 0


    @property
    def NPIX_TO_ALLOC(self):
        return self._NPIX_TO_ALLOC

    @NPIX_TO_ALLOC.setter
    def NPIX_TO_ALLOC(self, val):
        assert val> 0 or val == -1
        self._NPIX_TO_ALLOC = int(val)

    @staticmethod
    def check_parameter_integrity(params):
        if params.refiner.max_calls is None or len(params.refiner.max_calls) == 0:
            raise ValueError("Cannot refine because params.refiner.max_calls is empty")

        if os.environ.get("DIFFBRAGG_CUDA") is not None:
            params.refiner.use_gpu = True

        return params

    @property
    def num_shots_on_rank(self):
        return len(self.Modelers)

    def _init_panel_group_information(self, detector):
        # default is one group per panel:
        self.panel_group_from_id = {pid: 0 for pid in range(len(detector))}
        # if specified in a file, then overwrite:
        if self.params.refiner.panel_group_file is not None:
            self.panel_group_from_id = utils.load_panel_group_file(self.params.refiner.panel_group_file)
            if not self.panel_group_from_id:
                raise ValueError("Loading panel group file %s  produced an panel group dict!"
                                 % self.params.refiner.panel_group_file)

        # how many unique panel groups :
        panel_groups = set(self.panel_group_from_id.values())
        self.n_panel_groups = len(panel_groups)

        # make dict where the key is the panel group id, and the value is a list of the panel ids within the group
        panels_per_group = {group_id: [] for group_id in panel_groups}
        for pid in self.panel_group_from_id:
            group_id = self.panel_group_from_id[pid]
            panels_per_group[group_id].append(pid)

        # we should rotate each panel in a group about the same reference point
        # Make a dict where key is panel id, and the  value is a reference origin
        self.panel_reference_from_id = {}
        for pid in self.panel_group_from_id:
            group_id = self.panel_group_from_id[pid]

            # take as reference, the origin of the first panel in the group
            reference_panel = detector[panels_per_group[group_id][0]]
            self.panel_reference_from_id[pid] = reference_panel.get_origin()

    def _init_simulator(self, expt, miller_data):
        self.SIM = utils.simulator_for_refinement(expt, self.params)
        # note self.SIM.D is a now diffBragg instance
        # update the miller data ?
        if miller_data is not None:
            self.SIM.crystal.miller_array = miller_data.as_amplitude_array()
            self.SIM.update_Fhkl_tuple()

    @staticmethod
    def _check_experiment_integrity(expt):
        for model in ["crystal", "detector", "beam", "imageset"]:
            if not hasattr(expt, model):
                raise ValueError("No %s in experiment, exiting. " % model)

    def launch_refiner(self, pandas_table, miller_data=None, work_distribution=None, refls_key="predictions"):
        self.load_inputs(pandas_table, miller_data=miller_data, work_distribution=work_distribution, refls_key=refls_key)
        LOGGER.info("EVENT: launch refiner")
        self._launch()
        return self.RUC

    def load_inputs(self, pandas_table, miller_data=None, work_distribution=None, refls_key='predictions'):
        """

        :param pandas_table: contains path to the experiments (pandas column exp_name) to be loaded
            the pandas table is expected to have been written by diffBragg.hopper or
            diffBragg.hopper_process . See method save_to_pandas in simtbx/command_line/hopper.py
            For example, if the outputdir of diffBragg.hopper was set to `all_shots`, then
            there should be a folder all_shots/pandas created which contains all of the per-shot pandas
            dataframes. They should be concatenated as follows, forming a suitable argument for this method
            >> import glob,pandas
            >> fnames = glob.glob("all_shots/pandas/rank*/*pkl")
            >> df = pandas.concat([ pandas.read_pickle(f) for f in fnames])
            >> df.reset_index(inplace=True, drop=True)
            >> df.to_pickle("all_shots.pkl")
            >> # Then later, as part of an MPI application, the following will load all data:
            >> RefineLauncher_instance.load_inputs(df, refls_key="stage1_refls")

        :param miller_data: Optional miller array for the structure factor component of the model
        :param refls_key: key specifying the reflection tables in the pandas table
            Modeled pixels will lie in shoeboxes centered on each x,y,z in xyzobs.px.value
        :return:
        """
        COMM.Barrier()
        num_exp = len(pandas_table)
        if "exp_idx" not in list(pandas_table):
            pandas_table["exp_idx"] = 0
        first_exper_file = pandas_table.exp_name.values[0]
        detector = ExperimentListFactory.from_json_file(first_exper_file, check_format=False)[0].detector
        if detector is None and self.params.refiner.reference_geom is None:
            raise RuntimeError("No detector in experiment, must provide a reference geom.")
        # TODO verify all shots have the same detector ?
        if self.params.refiner.reference_geom is not None:
            detector = ExperimentListFactory.from_json_file(self.params.refiner.reference_geom, check_format=False)[0].detector
            print("Using reference geom from expt %s" % self.params.refiner.reference_geom)

        if COMM.size > num_exp:
            raise ValueError("Requested %d MPI ranks to process %d shots. Reduce number of ranks to %d"
                             % (COMM.size, num_exp, num_exp))
        self._init_panel_group_information(detector)

        self.verbose = False
        if COMM.rank == 0:
            self.verbose = self.params.refiner.verbose > 0
            if self.params.refiner.gather_dir is not None and not os.path.exists(self.params.refiner.gather_dir):
              os.makedirs(self.params.refiner.gather_dir)
              LOGGER.info("MADE GATHER DIR %s" % self.params.refiner.gather_dir)
        COMM.barrier()
        shot_idx = 0  # each rank keeps index of the shots local to it
        rank_panel_groups_refined = set()
        exper_names = pandas_table.exp_name
        exper_ids = pandas_table.exp_idx.values
        shot_ids = list(zip(exper_names, exper_ids))
        assert len(shot_ids) == len(set(shot_ids))
        # TODO assert all exper are single-file, probably way before this point
        if work_distribution is None:
            worklist = range(COMM.rank, len(exper_names), COMM.size)
        else:
            worklist = work_distribution[COMM.rank]
        LOGGER.info("EVENT: begin loading inputs")

        # load the Fhkl model once here to check which hkl are missing (and filter from the refls below)
        first_exper = ExperimentList.from_file(exper_names[0], check_format=False)[0]
        Fhkl_model = utils.load_Fhkl_model_from_params_and_expt(self.params, first_exper)
        self.symbol = Fhkl_model.space_group().info().type().lookup_symbol()
        if self.params.refiner.force_symbol is not None:
            self.symbol = self.params.refiner.force_symbol
        LOGGER.info("Will use space group symbol %s" % self.symbol)
        Fhkl_model_p1 = Fhkl_model.expand_to_p1().generate_bijvoet_mates()
        Fhkl_model_p1_indices = set(Fhkl_model_p1.indices())

        for i_work, i_df in enumerate(worklist):
            exper_name = exper_names[i_df]
            exper_id = int(exper_ids[i_df])
            LOGGER.info("EVENT: BEGIN loading experiment list")
            # TODO: test that the diffBragg_benchmarks is not broken
            expt = hopper_utils.DataModeler.exper_json_single_file(exper_name, exper_id)
            expt_list = ExperimentList()
            expt_list.append(expt)
            LOGGER.info("EVENT: DONE loading experiment list")
            expt.detector = detector  # in case of supplied ref geom
            self._check_experiment_integrity(expt)

            exper_dataframe = pandas_table.query("exp_name=='%s'" % exper_name).query("exp_idx==%d" % exper_id)

            refl_name = exper_dataframe[refls_key].values[0]
            refls = flex.reflection_table.from_file(refl_name)
            refls = refls.select(refls['id'] == exper_id)

            try:
                miller_inds = list(refls["miller_index"])
                is_not_000 = [h != (0, 0, 0) for h in miller_inds]
                is_in_Fhkl_model = [h in Fhkl_model_p1_indices for h in miller_inds]
                LOGGER.debug("Only refining %d/%d refls whose HKL are in structure factor model" % (
                    np.sum(is_in_Fhkl_model), len(refls)))
                refl_sel = flex.bool(np.logical_and(is_not_000, is_in_Fhkl_model))
                refls = refls.select(refl_sel)
            except KeyError:
                pass

            opt_uc_param = exper_dataframe[["a","b","c","al","be","ga"]].values[0]
            UcellMan = utils.manager_from_params(opt_uc_param)

            if shot_idx == 0:  # each rank initializes a simulator only once
                if self.params.simulator.init_scale != 1:
                    print("WARNING: For stage_two , it is assumed that total scale is stored in the pandas dataframe")
                    print("WARNING: resetting params.simulator.init_scale to 1!")
                    self.params.simulator.init_scale = 1
                self._init_simulator(expt, miller_data)
                if self.params.profile:
                    self.SIM.record_timings = True
                if self.params.refiner.stage_two.Fref_mtzname is not None:
                    self.Fref = utils.open_mtz(self.params.refiner.stage_two.Fref_mtzname,
                                               self.params.refiner.stage_two.Fref_mtzcol)

            LOGGER.info("EVENT: LOADING ROI DATA")
            shot_modeler = hopper_utils.DataModeler(self.params)
            shot_modeler.exper_name = exper_name
            shot_modeler.exper_idx = exper_id
            shot_modeler.refl_name = refl_name
            shot_modeler.rank = COMM.rank
            if self.params.refiner.load_data_from_refl:
                gathered = shot_modeler.GatherFromReflectionTable(expt, refls, sg_symbol=self.symbol)
            else:
                # Note: no need to pass exper_id here because expt and refls have already been sliced out
                gathered = shot_modeler.GatherFromExperiment(expt, refls, sg_symbol=self.symbol)
            if not gathered:
                raise IOError("Failed to gather data from experiment %s", exper_name)
                COMM.abort()

            if self.params.refiner.gather_dir is not None:
                gathered_name = os.path.splitext(os.path.basename(exper_name))[0]
                gathered_name += "_withData.refl"
                gathered_name = os.path.join(self.params.refiner.gather_dir, gathered_name )
                shot_modeler.dump_gathered_to_refl(gathered_name, do_xyobs_sanity_check=False) #True)
                LOGGER.info("SAVED ROI DATA TO %s" % gathered_name)
                if self.params.refiner.test_gathered_file:
                    all_data = shot_modeler.all_data.copy()
                    all_roi_id = shot_modeler.roi_id.copy()
                    all_bg = shot_modeler.all_background.copy()
                    all_trusted = shot_modeler.all_trusted.copy()
                    all_pids = np.array(shot_modeler.pids)
                    all_rois = np.array(shot_modeler.rois)
                    new_Modeler = hopper_utils.DataModeler(self.params)
                    assert new_Modeler.GatherFromReflectionTable(exper_name, gathered_name, sg_symbol=self.symbol)
                    assert np.allclose(new_Modeler.all_data, all_data)
                    assert np.allclose(new_Modeler.all_background, all_bg)
                    assert np.allclose(new_Modeler.rois, all_rois)
                    assert np.allclose(new_Modeler.pids, all_pids)
                    assert np.allclose(new_Modeler.all_trusted, all_trusted)
                    assert np.allclose(new_Modeler.roi_id, all_roi_id)
                    LOGGER.info("Gathered file approved!")

            self.Hi[shot_idx] = shot_modeler.Hi
            self.Hi_asu[shot_idx] = shot_modeler.Hi_asu

            LOGGER.info("EVENT: DONE LOADING ROI")
            shot_modeler.ucell_man = UcellMan
            self.SIM.num_ucell_param = len(shot_modeler.ucell_man.variables)  # for convenience

            loaded_spectra = False
            if self.params.spectrum_from_imageset:
                try:
                    shot_spectra = hopper_utils.downsamp_spec(self.SIM, self.params, expt, return_and_dont_set=True)
                    loaded_spectra = True
                except Exception as err:
                    LOGGER.warning("spectrum_from_imageset is set to True, however failed to load spectra: %s" % err)
                    loaded_spectra = False

            if not loaded_spectra:
                if "spectrum_filename" in list(exper_dataframe) and exper_dataframe.spectrum_filename.values[0] is not None:
                    shot_spectra = utils.load_spectra_from_dataframe(exper_dataframe)
                    LOGGER.debug("Loaded specta from %s" % exper_dataframe.spectrum_filename.values[0])
                    shot_modeler.spec_name = exper_dataframe.spectrum_filename.values[0]

                else:
                    total_flux = exper_dataframe.total_flux.values[0]
                    if total_flux is None:
                        total_flux = self.params.simulator.total_flux
                    shot_spectra = [(expt.beam.get_wavelength(), total_flux)]

            shot_modeler.spectra = shot_spectra
            if self.params.refiner.gather_dir is not None and not self.params.refiner.load_data_from_refl:
                spec_wave, spec_weights = map(np.array, zip(*shot_spectra))
                spec_filename = os.path.splitext(os.path.basename(exper_name))[0]
                spec_filename = os.path.join(self.params.refiner.gather_dir, spec_filename+".lam")
                utils.save_spectra_file(spec_filename, spec_wave, spec_weights)
                LOGGER.info("saved spectra filename %s" % spec_filename)

            LOGGER.info("Will simulate %d energy channels" % len(shot_spectra))

            if "detz_shift_mm" in list(exper_dataframe):
                shot_modeler.originZ_init = exper_dataframe.detz_shift_mm.values[0]*1e-3
            else:
                shot_modeler.originZ_init = 0
            # TODO: is there a reason these 3 attribs are set once more after being set above?
            shot_modeler.exper_name = exper_name
            shot_modeler.exper_idx = exper_id
            shot_modeler.refl_name = refl_name

            shot_panel_groups_refined = self.determine_refined_panel_groups(shot_modeler.pids)
            rank_panel_groups_refined = rank_panel_groups_refined.union(set(shot_panel_groups_refined))

            shot_idx += 1
            LOGGER.info(utils.memory_report('Process memory usage'))
            LOGGER.info("Finished loading image %d / %d (%d / %d)"
                  % (i_df+1, len(exper_names), i_work+1, len(worklist))) #, flush=True)

            shot_modeler.PAR = PAR_from_params(self.params, expt, best=exper_dataframe)
            self.Modelers[i_df] = shot_modeler  # TODO: verify that i_df as a key is ok everywhere

        LOGGER.info("DONE LOADING DATA; ENTER BARRIER")
        COMM.Barrier()
        LOGGER.info("DONE LOADING DATA; EXIT BARRIER")
        self.shot_roi_darkRMS = None

        # TODO warn that per_spot_scale refinement not intended for ensemble mode
        all_refined_groups = COMM.gather(rank_panel_groups_refined)
        panel_groups_refined = None
        if COMM.rank == 0:
            panel_groups_refined = set()
            for set_of_panels in all_refined_groups:
                panel_groups_refined = panel_groups_refined.union(set_of_panels)
        self.panel_groups_refined = list(COMM.bcast(panel_groups_refined))
        LOGGER.info(utils.memory_report('Mem after panel groups'))

        LOGGER.info("EVENT: Gathering global HKL information")
        try:
            self._gather_Hi_information()
        except TypeError:  # TODO: should we siltently fail here ?
            pass
        LOGGER.info("EVENT: FINISHED gather global HKL information")
        if self.params.roi.cache_dir_only:
            print("Done creating cache directory and cache_dir_only=True, so goodbye.")
            sys.exit()
        LOGGER.info(utils.memory_report('Mem after gather Hi info'))

        # in case of GPU
        LOGGER.info("BEGIN DETERMINE MAX PIX")
        self.NPIX_TO_ALLOC = self._determine_per_rank_max_num_pix()
        # TODO in case of randomize devices, shouldnt this be total max across all ranks?
        n = COMM.gather(self.NPIX_TO_ALLOC)
        if COMM.rank == 0:
            n = max(n)
        self.NPIX_TO_ALLOC = COMM.bcast(n)
        LOGGER.info("DONE DETERMINE MAX PIX")
        LOGGER.info(utils.memory_report('Mem after determine max num pix'))

        self.DEVICE_ID = COMM.rank % self.params.refiner.num_devices
        LOGGER.info(utils.memory_report('Mem after load_inputs'))

    def determine_refined_panel_groups(self, pids):
        refined_groups = []
        #assert len(pids) == len(selection_flags)
        for i, pid in enumerate(pids):
            if self.panel_group_from_id[pid] not in refined_groups:
                refined_groups.append(self.panel_group_from_id[pid])
        return refined_groups

    def _determine_per_rank_max_num_pix(self):
        max_npix = 0
        for i_shot in self.Modelers:
            modeler = self.Modelers[i_shot]
            x1, x2, y1, y2 = map(np.array, zip(*modeler.rois))
            npix = np.sum((x2-x1)*(y2-y1))
            max_npix = max(npix, max_npix)
            #print("Rank %d, shot %d has %d pixels" % (COMM.rank, i_shot+1, npix))
        LOGGER.info("Rank %d, max pix to be modeled: %d" % (COMM.rank, max_npix))
        return max_npix

    def _try_loading_spectrum_filelist(self):
        file_list = None
        fpath = self.params.simulator.spectrum.filename_list
        if fpath is not None:
            file_list = [l.strip() for l in open(fpath, "r").readlines()]
            assert all([len(l.split()) == 1 for l in file_list]), "weird spectrum file %s"% fpath
        return file_list

    def _gather_Hi_information(self):
        # aggregate all miller indices

        self.hiasu = HiAsu(self)

        # TODO Restore this diagnostics step within the scope of `HiAsu` class
        # Hi_asu_all_ranks used to be a list of all Hi_asu from all ranks,
        # (None of ranks > 0) but was removed when moving to new `HiAsu` class.
        # marr_unique_h = self._get_unique_Hi(Hi_asu_all_ranks)

        # TODO: I think this code does absolutely nothing, but might be useful (it was used for B-factor modeling, and maybe more..)
        # fres = marr_unique_h.d_spacings()
        # self.res_from_asu = {h: res for h, res in zip(fres.indices(), fres.data())}
        # TODO: End of code I think does absolutely nothing

    def get_first_modeller_symmetry(self):
        uc = next(iter(self.Modelers.values())).ucell_man
        params = uc.a, uc.b, uc.c, uc.al * 180 / np.pi, uc.be * 180 / np.pi, uc.ga * 180 / np.pi
        if self.params.refiner.force_unit_cell is not None:
            params = self.params.refiner.force_unit_cell
        return crystal.symmetry(unit_cell=params, space_group_symbol=self.symbol)

    def _get_unique_Hi(self, Hi_asu_all_ranks):
        if COMM.rank == 0:
            from cctbx.array_family import flex as cctbx_flex

            symm = self.get_first_modeller_symmetry()
            hi_asu_flex = cctbx_flex.miller_index(Hi_asu_all_ranks)
            mset = miller.set(symm, hi_asu_flex, anomalous_flag=True)
            marr = miller.array(mset)
            binner = marr.setup_binner(d_max=self.params.refiner.stage_two.d_max, d_min=self.params.refiner.stage_two.d_min,
                                       n_bins=self.params.refiner.stage_two.n_bin)
            print("Average multiplicities:")
            print("<><><><><><><><><><><><>")
            for i_bin in range(self.params.refiner.stage_two.n_bin - 1):
                dmax, dmin = binner.bin_d_range(i_bin + 1)
                F_in_bin = marr.resolution_filter(d_max=dmax, d_min=dmin)
                multi_in_bin = np.array(list(Counter(F_in_bin.indices()).values()))
                print("%2.5g-%2.5g : Multiplicity=%.4f" % (dmax, dmin, multi_in_bin.mean()))
                for ii in range(1, 100, 8):
                    print("\t %d refls with multi %d" % (sum(multi_in_bin == ii), ii))

            print("Overall completeness\n<><><><><><><><>")
            unique_Hi_asu = set(Hi_asu_all_ranks)
            hi_flex_unique = cctbx_flex.miller_index(list(unique_Hi_asu))
            mset = miller.set(symm, hi_flex_unique, anomalous_flag=True)
            self.binner = mset.setup_binner(d_min=self.params.refiner.stage_two.d_min,
                                            d_max=self.params.refiner.stage_two.d_max,
                                            n_bins=self.params.refiner.stage_two.n_bin)
            mset.completeness(use_binning=True).show()
            marr_unique_h = miller.array(mset)
            print("Rank %d: total miller vars=%d" % (COMM.rank, len(unique_Hi_asu)))
        else:
            marr_unique_h = None

        marr_unique_h = COMM.bcast(marr_unique_h)
        return marr_unique_h

    def _launch(self):
        """
        Usually this method should be modified when new features are added to refinement
        """
        # TODO return None or refiner instance
        LOGGER.info("begin _launch")
        x_init = None
        nmacro = self.params.refiner.num_macro_cycles
        n_trials = len(self.params.refiner.max_calls)

        for i_trial in range(n_trials*nmacro):

            self.RUC = StageTwoRefiner(self.Modelers, self.symbol, self.params)

            if self.will_refine(self.params.refiner.refine_spot_scale):
                self.RUC.refine_crystal_scale = (self.params.refiner.refine_spot_scale*nmacro)[i_trial]

            if self.will_refine(self.params.refiner.refine_Fcell):
                self.RUC.refine_Fcell = (self.params.refiner.refine_Fcell*nmacro)[i_trial]

            self.RUC.panel_group_from_id = self.panel_group_from_id
            self.RUC.panel_reference_from_id = self.panel_reference_from_id
            self.RUC.panel_groups_being_refined = self.panel_groups_refined

            # TODO verify not refining Fcell in case of local refiner
            self.RUC.max_calls = (self.params.refiner.max_calls*nmacro)[i_trial]
            self.RUC.x_init = x_init
            self.RUC.ignore_line_search_failed_step_at_lower_bound = True  # TODO: why was this necessary?

            # plot things
            self.RUC.trial_id = i_trial

            self.RUC.log_fcells = True
            self.RUC.request_diag_once = False
            self.RUC.trad_conv = True
            self.RUC.hiasu = self.hiasu

            self.RUC.S = self.SIM
            self.RUC.restart_file = self.params.refiner.io.restart_file
            self.RUC.S.update_nanoBragg_instance('update_oversample_during_refinement',
                                                 self.params.refiner.update_oversample_during_refinement)
            self.RUC.S.update_nanoBragg_instance("Npix_to_allocate", self.NPIX_TO_ALLOC)
            self.RUC.S.update_nanoBragg_instance('device_Id', self.DEVICE_ID)
            self.RUC.use_curvatures_threshold = self.params.refiner.use_curvatures_threshold
            if not self.params.refiner.curvatures:
                self.RUC.S.update_nanoBragg_instance('compute_curvatures', False)
            if COMM.rank==0:
                self.RUC.S.update_nanoBragg_instance('verbose', self.params.refiner.verbose)

            LOGGER.info("_launch run setup")
            LOGGER.info(utils.memory_report('Mem usage before _setup'))
            self.RUC.run(setup_only=True)
            LOGGER.info(utils.memory_report('Mem usage after _setup'))
            LOGGER.info("_launch done run setup")
            # for debug purposes:
            #if not self.params.refiner.quiet:
            #    print("\n<><><><><><><><>TRIAL %d refinement status:" % i_trial)
            #    self.RUC.S.D.print_if_refining()

            self.RUC.num_positive_curvatures = 0
            self.RUC.use_curvatures = self.params.refiner.start_with_curvatures
            self.RUC.hit_break_to_use_curvatures = False

            # selection flags set here:
            #self.RUC.selection_flags = self.shot_selection_flags
            #if self.params.refiner.res_ranges is not None:
            #    assert self.shot_reso is not None, "cant set reso flags is rlp is not in refl tables"
            #    nshots = len(self.shot_selection_flags)
            #    more_sel_flags = {}
            #    res_ranges = utils.parse_reso_string(self.params.refiner.res_ranges)
            #    for i_shot in range(nshots):
            #        rhigh, rlow = (res_ranges*nmacro)[i_trial]
            #        sel_flags = self.shot_selection_flags[i_shot]
            #        res_flags = [rhigh < r < rlow for r in self.shot_reso[i_shot]]
            #        more_sel_flags[i_shot] = [flag1 and flag2 for flag1,flag2 in zip(sel_flags, res_flags)]
            #    self.RUC.selection_flags = more_sel_flags

            LOGGER.info("_launcher running optimization")

            self.RUC.run(setup=False)
            LOGGER.info("_launcher done running optimization")
            if self.RUC.hit_break_to_use_curvatures:
                self.RUC.fix_params_with_negative_curvature = False
                self.RUC.num_positive_curvatures = 0
                self.RUC.use_curvatures = True
                self.RUC.run(setup=False)

            if self.RUC.hit_break_signal:
                if self.params.profile:
                    self.RUC.S.D.show_timings(self.RUC.rank)
                self.RUC._MPI_barrier()
                break

            if self.params.refiner.debug_pixel_panelfastslow is not None:
                utils.show_diffBragg_state(self.RUC.S.D, self.params.refiner.debug_pixel_panelfastslow)
                s = self.RUC._get_spot_scale(0)
                print("refiner spot scale=%f" % (s**2))

            x_init = self.RUC.x

            if self.params.profile:
                self.RUC.S.D.show_timings(self.RUC.rank)
            if os.environ.get("DIFFBRAGG_USE_CUDA") is not None or os.environ.get("DIFFBRAGG_USE_KOKKOS") is not None:
                self.RUC.S.D.gpu_free()

    def will_refine(self, param):
        return param is not None and any(param)


class HiAsu(object):
    """
    Object which stores possible & present Miller Indices, their counts,
    counters, maps between them and their integer indexes etc.

    Within `HiAsu`, the following notation is used for parameter names:
    * no trailing `_`: global parameter - total or the same for all ranks.
    * with trailing `_`: local parameter – value is unique for each rank.
    Example: `counts_` would be specific to rank, but `counts` would be global.
    """
    def __init__(self, refine_launcher):
        self.rl = refine_launcher
        self.possible = self.get_possible()
        self.possible_len = len(self.possible)
        self.possible_counts = self.get_counts()
        self.present_len = len(list(self.present_zip))
        self.from_idx, self.to_idx = self._get_dicts()

    def get_possible(self):
        if COMM.rank == 0:
            sym = self.rl.get_first_modeller_symmetry()
            res_ranges_str = self.rl.params.refiner.res_ranges
            if res_ranges_str:
                res_ranges = utils.parse_reso_string(res_ranges_str)
                d_min = min([d_min for d_min, _ in res_ranges])
            else:
                expt = next(iter(self.rl.Modelers.values())).E
                det, s0 = expt.detector, expt.beam.get_s0()
                d_min = min([p.get_max_resolution_at_corners(s0) for p in det])
            d_min *= 0.8  # accommodate variations in uc or det across expts
            mset_full = sym.build_miller_set(anomalous_flag=True, d_min=d_min)
            possible = list(mset_full.indices())
        else:
            possible = None
        return COMM.bcast(possible, root=0)

    def get_counts(self):
        hi_asu_ = chain.from_iterable(self.rl.Hi_asu.values())
        hi_asu_counter_ = Counter(hi_asu_)
        hi_asu_possible_counts_ = [hi_asu_counter_[k] for k in self.possible]
        hi_asu_possible_counts_ = np.array(hi_asu_possible_counts_, dtype=np.uint16)
        hi_asu_possible_counts = np.zeros_like(hi_asu_possible_counts_, dtype=np.uint16)
        COMM.Allreduce(hi_asu_possible_counts_, hi_asu_possible_counts, op=MPI.SUM)
        return hi_asu_possible_counts

    @property
    def present(self):
        return (p for p, c in self.present_zip)

    @property
    def present_counts(self):
        return (c for p, c in self.present_zip)

    @property
    def present_counter(self):
        return Counter({p: c for p, c in self.present_zip})

    @property
    def present_idx_counter(self):
        return Counter({self.to_idx[p]: c for p, c in self.present_zip})

    @property
    def present_zip(self):
        return ((p, c) for p, c in zip(self.possible, self.possible_counts) if c)

    def _get_dicts(self):
        """from_idx maps miller indices to index in LBFGS par. array self.x;
        to_ids is an inverse map during refinement to update diffBragg m.arr"""
        from_idx = {i: h for i, h in enumerate(self.present)}
        to_idx = {h: i for i, h in enumerate(self.present)}
        return from_idx, to_idx


 *******************************************************************************


 *******************************************************************************
simtbx/diffBragg/hopper_ensemble_utils.py
from __future__ import division, print_function

import time
import sys
import socket
import logging
import os
import numpy as np
from scipy.optimize import basinhopping


from libtbx.mpi4py import MPI
from simtbx.diffBragg import hopper_ensemble_utils, hopper_utils, utils
from simtbx.diffBragg.prep_stage2_input import prep_dataframe
from cctbx import miller, crystal, sgtbx
from dials.array_family import flex
from dxtbx.model import ExperimentList


COMM = MPI.COMM_WORLD

MAIN_LOGGER = logging.getLogger("diffBragg.main")

F32 = np.finfo(np.float32)


class TargetFuncEnsemble:

    def __init__(self, vary, xinit=None):
        self.vary = vary
        self.x0 = np.ones(len(self.vary), np.float64)  # initial full parameter list
        if xinit is not None:
            self.x0 = xinit
        self.niter = 0
        self.t_per_iter = np.array([])

    def jac(self, x, *args):
        if self.g is not None:
            return self.g[self.vary]

    @property
    def ave_t_per_iter(self):
        ave_t_per_iter = 0
        if self.t_per_iter.shape[0] > 1:
            ave_t_per_iter = np.mean(self.t_per_iter[1:] - self.t_per_iter[:-1])
        return ave_t_per_iter

    def __call__(self, x, *args, **kwargs):
        self.t_per_iter = np.append(self.t_per_iter, time.time())
        modelers = args[0]

        self.x0[self.vary] = x

        # sync the centric amplitudes
        if modelers.SIM.num_Fhkl_channels >1:
            num_fhkl_x = modelers.SIM.Num_ASU*modelers.SIM.num_Fhkl_channels
            fhkl_param_start = len(self.x0) - num_fhkl_x
            channel0_amps = self.x0[fhkl_param_start: fhkl_param_start+modelers.SIM.Num_ASU]
            centric_amps = channel0_amps[modelers.SIM.is_centric]
            for i_chan in range(1, modelers.SIM.num_Fhkl_channels):
                offset = fhkl_param_start + i_chan*modelers.SIM.Num_ASU
                np.put(self.x0, modelers.SIM.where_is_centric + offset, centric_amps)

        f,  self.g, ave_zscore_sig = target_func(self.x0, modelers)

        # resitribute all centric gradients into channel0 centrics
        if modelers.SIM.num_Fhkl_channels >1:
            num_fhkl_x = modelers.SIM.Num_ASU*modelers.SIM.num_Fhkl_channels
            fhkl_param_start = len(self.x0) - num_fhkl_x
            where_to_add_grad = modelers.SIM.where_is_centric + fhkl_param_start
            for i_chan in range(1, modelers.SIM.num_Fhkl_channels):
                chan_start = fhkl_param_start + i_chan*modelers.SIM.Num_ASU
                chan_grad = self.g[chan_start: chan_start+modelers.SIM.Num_ASU]
                chan_centric_grad = chan_grad[modelers.SIM.is_centric]
                np.add.at(self.g, where_to_add_grad, chan_centric_grad)

        self.niter += 1

        min_info = "it=%d | t/it=%.4fs | F=%10.7g | sigZ=%10.7g" \
                  % (self.niter,self.ave_t_per_iter, f, ave_zscore_sig)
        if COMM.rank==0:
            #print(min_info, flush=True)
            MAIN_LOGGER.info(min_info)
        if modelers.save_freq is not None and self.niter % modelers.save_freq == 0:
            modelers.save_up(self.x0, ref_iter=self.niter)
            if modelers.SIM.D.record_timings:
                modelers.SIM.D.show_timings()

        return f


def target_func(x, modelers):
    """

    :param x: refinement parameters
    :param modelers: instance of DataModelers class
    :return:
    """
    assert modelers.SIM is not None
    assert modelers.SIM.refining_Fhkl

    num_fhkl_params = modelers.SIM.Num_ASU * modelers.SIM.num_Fhkl_channels
    num_shot_params = len(modelers[0].P)  # all modelers will have same number of per-shot parameters to refine
    assert len(x) == num_fhkl_params + modelers.num_total_modelers * num_shot_params

    f = 0  # target functional
    g = np.zeros(modelers.num_total_modelers * num_shot_params)
    g_fhkl = np.zeros(num_fhkl_params)
    zscore_sigs = []
    fcell_params = x[-num_fhkl_params:]
    for ii, i_shot in enumerate(modelers):
        shot_modeler = modelers[i_shot]
        shot_x_slice = modelers.x_slices[i_shot]
        per_shot_params = x[shot_x_slice]
        x_for_shot = np.hstack((per_shot_params, fcell_params))
        model_bragg, Jac = hopper_utils.model(x_for_shot, shot_modeler, modelers.SIM, compute_grad=True, update_spectrum=True,
                                              update_Fhkl_scales=ii==0)

        model_pix = model_bragg + shot_modeler.all_background

        if modelers.SIM.use_psf:
            model_pix, J = hopper_utils.convolve_model_with_psf(model_pix, Jac, shot_modeler, modelers.SIM)

        resid = shot_modeler.all_data - model_pix

        # data contributions to target function
        V = model_pix + shot_modeler.all_sigma_rdout**2
        resid_square = resid**2
        shot_fLogLike = (.5*(np.log(2*np.pi*V) + resid_square / V))
        if shot_modeler.params.roi.allow_overlapping_spots:
            shot_fLogLike /= shot_modeler.all_freq
        shot_fLogLike = shot_fLogLike[shot_modeler.all_trusted].sum()   # negative log Likelihood target
        f += shot_fLogLike

        zscore_sig = np.std((resid / np.sqrt(V))[shot_modeler.all_trusted])
        zscore_sigs.append(zscore_sig)

        # get this shots contribution to the gradient
        common_grad_term_all = (0.5 / V * (1 - 2 * resid - resid_square / V))
        if shot_modeler.params.roi.allow_overlapping_spots:
            common_grad_term_all /= shot_modeler.all_freq
        common_grad_term = common_grad_term_all[shot_modeler.all_trusted]

        shot_g = np.zeros(num_shot_params)
        for name in shot_modeler.non_fhkl_params:
            p = shot_modeler.P[name]
            Jac_p = Jac[p.xpos]
            shot_g[p.xpos] += (Jac_p[shot_modeler.all_trusted] * common_grad_term).sum()
        np.add.at(g, shot_x_slice, shot_g)

        spot_scale_p = shot_modeler.P["G_xtal0"]
        G = spot_scale_p.get_val(x[spot_scale_p.xpos])
        g_fhkl += modelers.SIM.D.add_Fhkl_gradients(
            shot_modeler.pan_fast_slow, resid, V, shot_modeler.all_trusted,
            shot_modeler.all_freq, modelers.SIM.num_Fhkl_channels, G)

    # add up target and gradients across all ranks
    f = COMM.bcast(COMM.reduce(f))

    # average z-score sigma for reporting
    zscore_sigs = COMM.reduce(zscore_sigs)
    ave_zscore_sig = np.mean(COMM.bcast(zscore_sigs))

    # consider sanity checks on g, e.g. at this point it should be 0's outside of all x_slices on this rank
    g = COMM.bcast(COMM.reduce(g))
    g_fhkl = COMM.bcast(COMM.reduce(g_fhkl))

    if COMM.rank==0:
        t = time.time()
        for beta, how in [(modelers.params.betas.Fhkl, "ave"),
                          (modelers.params.betas.Friedel, "Friedel"),
                          (modelers.params.betas.Finit, "init")]:
            if beta is None:
                continue

            for i_chan in range(modelers.SIM.num_Fhkl_channels):
                fhkl_restraint_f, fhkl_restraint_grad = modelers.SIM.D.Fhkl_restraint_data(
                    i_chan,
                    beta,
                    modelers.params.use_geometric_mean_Fhkl,
                    how)
                f += fhkl_restraint_f
                fhkl_slice = slice(i_chan*modelers.SIM.Num_ASU, (i_chan+1)*modelers.SIM.Num_ASU, 1)
                np.add.at(g_fhkl, fhkl_slice, fhkl_restraint_grad)
        t = time.time()-t
        MAIN_LOGGER.debug("Fhkl restraint comp took %.4f sec" %t)
    f = COMM.bcast(f)
    g_fhkl = COMM.bcast(g_fhkl)

    g_fhkl *= modelers.SIM.Fhkl_scales*modelers.params.sigmas.Fhkl  # need to rescale the Fhkl gradient according to the reparameterization on Fhkl scale factord

    g = np.append(g, g_fhkl)

    return f, g, ave_zscore_sig


class DataModelers:

    def __init__(self):
        self.data_modelers = {}
        self.x_slices = {}
        self.num_modelers = 0  # this is the number of modelers on this MPIrank
        self.num_total_modelers = 0  # this is a total summed across MPI ranks
        self.num_param_per_shot = 0
        self._vary = None  # flags for refined variables
        self.SIM = None  # sim_data.SimData instance (one per rank) to be shared amongst the data modelers
        self.cell_for_mtz = None  # unit cell for writing the mtz
        self.max_sigma = 1e20  # max sigma allowed for an optimized amplitude to be included in mtz
        self.outdir = None  # output folder, if None, defaults to the folder used when running hopper
        self.save_freq = None  # optional integer, if provided, save mtz files each 'save_freq' iterations
        self.npix_to_alloc = 0
        self.save_modeler_params = False  # if True, save modelers to pandas files at each iteration

    def set_Fhkl_channels(self):
        if self.SIM is None:
            raise AttributeError("cant set Fhkl channels without a SIM attribute")
        for i_shot, mod in self.data_modelers.items():
            mod.set_Fhkl_channels(self.SIM, set_in_diffBragg=False)
            self.data_modelers[i_shot] = mod

    def _determine_per_rank_max_num_pix(self):
        max_npix = 0
        for i_shot, modeler in self.data_modelers.items():
            x1, x2, y1, y2 = map(np.array, zip(*modeler.rois))
            npix = np.sum((x2-x1)*(y2-y1))
            max_npix = max(npix, max_npix)
        return max_npix

    def _mpi_set_allocation_volume(self):
        assert self.SIM is not None
        assert hasattr(self.SIM, "D")

        MAIN_LOGGER.debug("BEGIN DETERMINE MAX PIX")
        self.npix_to_alloc = self._determine_per_rank_max_num_pix()
        # TODO in case of randomize devices, shouldnt this be total max across all ranks?
        n = COMM.gather(self.npix_to_alloc)
        if COMM.rank == 0:
            n = max(n)
        self.npix_to_alloc = COMM.bcast(n)
        MAIN_LOGGER.debug("DONE DETERMINE MAX PIX (each GPU will allocate space for %d pixels" % self.npix_to_alloc)
        self.SIM.D.Npix_to_allocate = int(self.npix_to_alloc)  # needs to be int32

    def _mpi_sanity_check_num_params(self):
        num_param_per_shot = []
        for i_shot in self.data_modelers:
            mod = self.data_modelers[i_shot]
            num_param_per_shot.append( len(mod.P))
        num_param_per_shot = COMM.reduce(num_param_per_shot)
        if COMM.rank==0:
            assert len(set(num_param_per_shot)) == 1
            num_param_per_shot = num_param_per_shot[0]
        num_param_per_shot = COMM.bcast(num_param_per_shot)

        self.num_param_per_shot = num_param_per_shot

    def mpi_get_ave_cell(self):
        all_ucell_p = []
        for i_shot, mod in self.data_modelers.items():
            ucell_p = mod.ucell_man.unit_cell_parameters
            all_ucell_p.append(ucell_p)

        all_ucell_p = COMM.reduce(all_ucell_p)

        ave_ucell = None
        if COMM.rank==0:
            ave_ucell = np.vstack(all_ucell_p).mean(0)
            print("Setting average unit cell=", ave_ucell)
        ave_ucell = COMM.bcast(ave_ucell)
        return ave_ucell

    def _mpi_get_shots_per_rank(self):
        """
        :return:  dictionary of (rank, number of data modelers on that rank)
        """
        rank_and_numShot = [(COMM.rank, self.num_modelers)]
        rank_and_numShot = COMM.reduce(rank_and_numShot)
        if COMM.rank == 0:
            rank_and_numShot = dict(rank_and_numShot)
        rank_and_numShot = COMM.bcast(rank_and_numShot)
        return rank_and_numShot

    def set_device_id(self):
        assert self.SIM is not None
        dev = COMM.rank % self.params.refiner.num_devices
        MAIN_LOGGER.info("will use device %d on host %s" % (dev, socket.gethostname()))
        self.SIM.D.device_Id = dev

    def mpi_set_x_slices(self):
        """
        x_slices is a dict that should have the same keys as the data_modelers
        Each slice slices through a global paramater array
        """
        self._mpi_sanity_check_num_params()
        self._mpi_compute_num_total_modelers()
        shots_per_rank = self._mpi_get_shots_per_rank()
        start = 0
        npar = self.num_param_per_shot
        for i_rank in range(COMM.size):
            rank_nshots = shots_per_rank[i_rank]

            if COMM.rank==i_rank:
                assert rank_nshots==self.num_modelers  # sanity test
                for i_shot in range(self.num_modelers):
                    x_slice = slice(start+i_shot*npar, start+(i_shot+1)*npar, 1)
                    self.x_slices[i_shot] = x_slice
                break

            rank_npar = npar*rank_nshots
            start += rank_npar

    def _mpi_compute_num_total_modelers(self):
        self.num_total_modelers = COMM.bcast(COMM.reduce(self.num_modelers))

    def __getitem__(self, item):
        assert item in self.data_modelers, "shot %d not in data modelers!" % item
        return self.data_modelers[item]

    def __iter__(self):
        return self.data_modelers.__iter__()

    def add_modeler(self, modeler):
        assert isinstance(modeler, hopper_utils.DataModeler)
        self.data_modelers[self.num_modelers] = modeler
        self.num_modelers += 1

    def prep_for_refinement(self):
        assert self.SIM is not None, "set the sim_data.SimData instance first. example shown in simtbx/command_line/hopper_ensemble.py method load_inputs"
        assert self.SIM.refining_Fhkl
        num_fhkl_param = self.SIM.Num_ASU*self.SIM.num_Fhkl_channels
        num_param_total = self.num_param_per_shot*self.num_total_modelers + num_fhkl_param
        vary = np.zeros(num_param_total)

        for i_shot, x_slice in self.x_slices.items():
            shot_vary = np.ones(self.num_param_per_shot)
            for p in self.data_modelers[i_shot].P.values():
                if not p.refine:
                    shot_vary[p.xpos] = 0
            np.add.at(vary, x_slice, shot_vary)

        vary = COMM.bcast(COMM.reduce(vary))
        # TODO: actually, if there are more than 1 Fhkl channels, then we want to fix the centric Fhkls in all but 1 of the channels
        vary[-num_fhkl_param:] = self._get_fhkl_vary_flags() #1  # we will always vary the fhkl params in ensemble refinement (current default)
        vary = vary.astype(bool)

        # use the first data modeler to set the diffBragg internal refinement flags
        P = self.data_modelers[0].P
        num_ucell_p = len(self.data_modelers[0].ucell_man.variables)
        if P["lambda_offset"].refine:
            for lam_id in hopper_utils.LAMBDA_IDS:
                self.SIM.D.refine(lam_id)
        if P["RotXYZ0_xtal0"].refine:
            self.SIM.D.refine(hopper_utils.ROTX_ID)
            self.SIM.D.refine(hopper_utils.ROTY_ID)
            self.SIM.D.refine(hopper_utils.ROTZ_ID)
        if P["Nabc0"].refine:
            self.SIM.D.refine(hopper_utils.NCELLS_ID)
        if P["Ndef0"].refine:
            self.SIM.D.refine(hopper_utils.NCELLS_ID_OFFDIAG)
        if P["ucell0"].refine:
            for i_ucell in range(num_ucell_p):
                self.SIM.D.refine(hopper_utils.UCELL_ID_OFFSET + i_ucell)
        if P["eta_abc0"].refine:
            self.SIM.D.refine(hopper_utils.ETA_ID)
        if P["detz_shift"].refine:
            self.SIM.D.refine(hopper_utils.DETZ_ID)
        if self.SIM.D.use_diffuse:
            self.SIM.D.refine(hopper_utils.DIFFUSE_ID)

        self._vary = vary

        self._set_mtz_data()
        self.set_device_id()

    def alloc_max_pix_per_shot(self):
        self._mpi_set_allocation_volume()

    def _get_fhkl_vary_flags(self):
        # we vary all Fhkl, however if there are more than 1 Fhkl channels
        # we only vary the centrics in the first channel, and then set those as the values in the other channels
        # (no anomalous scattering in centrics)

        num_fhkl_param = self.SIM.Num_ASU*self.SIM.num_Fhkl_channels
        fhkl_vary = np.ones(num_fhkl_param, int)

        if self.SIM.num_Fhkl_channels > 1:
            assert self.SIM.is_centric is not None
            for i_chan in range(1, self.SIM.num_Fhkl_channels):
                channel_slc = slice(i_chan*self.SIM.Num_ASU, (i_chan+1) *self.SIM.Num_ASU, 1)
                np.subtract.at(fhkl_vary, channel_slc, self.SIM.is_centric.astype(int))

        # only refine hkls that are present in the reflection tables
        all_nominal_hkl = set()
        for mod in self.data_modelers.values():
            all_nominal_hkl = all_nominal_hkl.union(mod.hi_asu_perpix)
        #TODO : is this memory intensive?
        all_nominal_hkl = COMM.gather(all_nominal_hkl)
        if COMM.rank == 0:
            # TODO: all_nominal_hkl is P1, asu_map_int is non-P1
            all_nominal_hkl = set(all_nominal_hkl[0]).union(*all_nominal_hkl[1:])
            all_nominal_hkl_sym = utils.map_hkl_list(all_nominal_hkl, True, self.SIM.crystal.symbol)
            asu_inds_to_vary = [self.SIM.asu_map_int[h] for h in all_nominal_hkl_sym]
        else:
            asu_inds_to_vary = None
        asu_inds_to_vary = set(COMM.bcast(asu_inds_to_vary))
        for i_chan in range(self.SIM.num_Fhkl_channels):
            for i_asu in asu_inds_to_vary:
                if i_asu not in asu_inds_to_vary:
                    fhkl_vary[i_asu + self.SIM.Num_ASU*i_chan] = 0

        return fhkl_vary


    @property
    def params(self):
        """
        all data_modelers should have the exact same phil extract, so we just grab the first one
        :return:  None or phil extract object
        """
        if not self.data_modelers:
            raise ValueError("No added data modelers! therefore no params")
        return self.data_modelers[0].params

    def Minimize(self, save=True):
        """
        :param save:  save an optimized MTZ file when finished
        """
        assert self._vary is not None, "call prep_for_refinement() first..."

        target = TargetFuncEnsemble(self._vary)
        x0_for_refinement = target.x0[self._vary]

        fhkl_is_varied = self._get_fhkl_vary_flags()
        num_fhkl_refined = int(np.sum(fhkl_is_varied))
        bounds = [(None, None)] * len(x0_for_refinement)
        for i in np.arange(num_fhkl_refined, 0, -1):
            bounds[-i] = (None, 8)
        min_kwargs = {
            "args": (self,),
            "method": "L-BFGS-B",
            "jac": target.jac,
            "bounds": bounds,
            "options" : {
                "ftol": self.params.ftol,
                "gtol": 1e-12,
                "maxfun": 1e5,
                "maxiter": self.params.lbfgs_maxiter,
                "eps": 1e-20
            }
        }

        # just to be consistent with the hopper_utils.py API, we call basinhopping
        # however we only expect to do a single round of descent mimimization here
        out = basinhopping(target, x0_for_refinement,
                     niter=self.params.niter,
                     minimizer_kwargs=min_kwargs,
                     T=self.params.temp,
                     callback=None,
                     disp=False,
                     stepsize=self.params.stepsize)
        target.x0[self._vary] = out.x
        if save:
            self.save_up(target.x0)
        return target.x0

    def _set_mtz_data(self):
        idx_to_asu = {idx: asu for asu, idx in self.SIM.asu_map_int.items()}
        asu_hkls = [idx_to_asu[i] for i in range(self.SIM.Num_ASU)]
        inds, amps = self.SIM.D.Fhkl_tuple
        amplitude_map = {h: amp for h, amp in zip(inds, amps)}
        assert set(asu_hkls).intersection(amplitude_map) == set(asu_hkls)
        self.initial_intens = np.array([amplitude_map[h]**2 for h in asu_hkls])
        self.flex_asu = flex.miller_index(asu_hkls)

    def save_up(self, x, ref_iter=None):
        """
        :param x: optimized parameters output by self.Minimize
        :param ref_iter: iteration number for optional saving during minimization (e.g. each X iterations)
        """
        assert self.outdir is not None

        cell_for_mtz = self.cell_for_mtz
        if self.cell_for_mtz is None:
            cell_for_mtz = tuple(self.mpi_get_ave_cell())
        sym = crystal.symmetry(cell_for_mtz, self.params.space_group)

        Fhkl_scale_hessian = np.zeros(self.SIM.Num_ASU * self.SIM.num_Fhkl_channels)
        for i_shot, mod in self.data_modelers.items():
            mod.best_model, _ = hopper_utils.model(x, mod, self.SIM, compute_grad=False, update_spectrum=True)
            mod.best_model_includes_background = False
            resid = mod.all_data - (mod.best_model+mod.all_background)
            V = mod.best_model + mod.all_sigma_rdout ** 2
            Gparam = mod.P["G_xtal0"]
            G = Gparam.get_val(x[Gparam.xpos])
            # here we must use the CPU method
            if i_shot % 100==0:
                MAIN_LOGGER.info("Getting Fhkl errors for shot %d/%d ... " % (i_shot+1, self.num_modelers))
            Fhkl_scale_hessian += self.SIM.D.add_Fhkl_gradients(
                mod.pan_fast_slow, resid, V, mod.all_trusted, mod.all_freq,
                self.SIM.num_Fhkl_channels, G, track=False, errors=True)
            # ------------

        Fhkl_scale_hessian = COMM.reduce(Fhkl_scale_hessian)

        if COMM.rank==0:
            # resitribute the Hessian for centrics
            if self.SIM.num_Fhkl_channels > 1:
                for i_chan in range(1, self.SIM.num_Fhkl_channels):
                    chan_start = i_chan * self.SIM.Num_ASU
                    chan_hess = Fhkl_scale_hessian[chan_start: chan_start + self.SIM.Num_ASU]
                    chan_centric_hess = chan_hess[self.SIM.is_centric]
                    np.add.at(Fhkl_scale_hessian, self.SIM.where_is_centric, chan_centric_hess)

                total_centric_hess = Fhkl_scale_hessian[self.SIM.where_is_centric]
                for i_chan in range(1, self.SIM.num_Fhkl_channels):
                    chan_start = i_chan * self.SIM.Num_ASU
                    where_to_put_hess = self.SIM.where_is_centric + chan_start
                    np.put(Fhkl_scale_hessian, where_to_put_hess, total_centric_hess)

            if not os.path.exists(self.outdir):
                os.makedirs(self.outdir)

            for i_chan in range(self.SIM.num_Fhkl_channels):

                mtz_prefix = "optimized_channel%d" % i_chan
                if ref_iter is not None:
                    mtz_prefix += "_iter%d" % ref_iter
                mtz_name = os.path.join(self.outdir, "%s.mtz" % mtz_prefix)

                fhkl_slice = slice(i_chan*self.SIM.Num_ASU, (i_chan+1)*self.SIM.Num_ASU,1)
                channel_scales = self.SIM.Fhkl_scales[fhkl_slice]
                channel_hessian = Fhkl_scale_hessian[fhkl_slice]
                with np.errstate(all='ignore'):
                    channel_scales_var = 1 / channel_hessian

                safe_vals = np.logical_and( channel_scales_var >= F32.min, channel_scales_var <= F32.max)
                is_finite = np.logical_and(safe_vals, ~np.isinf(channel_scales_var))

                #is_finite = ~np.isinf(channel_scales_var.astype(np.float32))  # should be finite float32
                is_reasonable = channel_scales_var < self.max_sigma
                is_positive = channel_hessian > 0
                sel = is_positive & is_finite & is_reasonable
                optimized_data = channel_scales[sel] * self.initial_intens[sel]
                optimized_sigmas = np.sqrt(channel_scales_var[sel]) * self.initial_intens[sel]
                channel_inds = self.flex_asu.select(flex.bool(sel))

                assert not np.any(np.isnan(optimized_sigmas)), "should be no nans here"
                mset = miller.set(sym, channel_inds, True)  # TODO optional anomalous flag

                ma = miller.array(mset, flex.double(optimized_data), flex.double(optimized_sigmas))
                ma = ma.set_observation_type_xray_intensity().as_amplitude_array()
                ma.as_mtz_dataset(column_root_label="F").mtz_object().write(mtz_name)

        if self.save_modeler_params:

            num_fhkl_params = self.SIM.Num_ASU * self.SIM.num_Fhkl_channels
            fcell_params = x[-num_fhkl_params:]
            for i_shot, mod in self.data_modelers.items():
                temp = mod.params.tag
                if ref_iter is not None:
                    mod.params.tag = mod.params.tag + ".iter%d" % ref_iter
                else:
                    mod.params.tag = mod.params.tag + ".final"
                # TODO: x should be for this particular modeler (fhkl_slice)
                shot_x_slice = self.x_slices[i_shot]
                per_shot_params = x[shot_x_slice]
                x_for_shot = np.hstack((per_shot_params, fcell_params))
                mod.save_up(x_for_shot, self.SIM, COMM.rank,
                            save_modeler_file=False,
                            save_fhkl_data=False,
                            save_sim_info=False,
                            save_refl=False)
                mod.params.tag = temp


def get_gather_name(exper_name, gather_dir):
    gathered_name = os.path.splitext(os.path.basename(exper_name))[0]
    gathered_name += "_withData.refl"
    gathered_name = os.path.join(gather_dir, gathered_name)
    return os.path.abspath(gathered_name)


def load_inputs(pandas_table, params, exper_key="exp_name", refls_key='predictions',
                gather_dir=None, exper_idx_key="exp_idx"):

    work_distribution = prep_dataframe(pandas_table, refls_key,
                                       res_ranges_string=params.refiner.res_ranges)
    COMM.barrier()
    num_exp = len(pandas_table)
    first_exper_file = pandas_table[exper_key].values[0]
    first_exper = ExperimentList.from_file(first_exper_file, check_format=False)[0]
    detector = first_exper.detector
    if detector is None and params.refiner.reference_geom is None:
        raise RuntimeError("No detector in experiment, must provide a reference geom.")
    # TODO verify all shots have the same detector ?
    if params.refiner.reference_geom is not None:
        detector = ExperimentList.from_file(params.refiner.reference_geom, check_format=False)[
            0].detector
        MAIN_LOGGER.debug("Using reference geom from expt %s" % params.refiner.reference_geom)

    if COMM.size > num_exp:
        raise ValueError("Requested %d MPI ranks to process %d shots. Reduce number of ranks to %d"
                         % (COMM.size, num_exp, num_exp))

    exper_names = pandas_table[exper_key]
    exper_ids = pandas_table[exper_idx_key]
    name_ids = list(zip(exper_names, exper_ids))
    assert len(name_ids) == len(set(name_ids))
    worklist = work_distribution[COMM.rank]
    MAIN_LOGGER.info("EVENT: begin loading inputs")

    Fhkl_model = utils.load_Fhkl_model_from_params_and_expt(params, first_exper)

    Fhkl_model = Fhkl_model.expand_to_p1().generate_bijvoet_mates()
    Fhkl_model_indices = set(Fhkl_model.indices())
    shot_modelers = hopper_ensemble_utils.DataModelers()
    for ii, i_df in enumerate(worklist):
        exper_name = exper_names[i_df]
        exper_id = int(exper_ids[i_df])
        MAIN_LOGGER.info("EVENT: BEGIN loading experiment list")
        check_format = not params.refiner.load_data_from_refl
        expt = hopper_utils.DataModeler.exper_json_single_file(exper_name, exper_id, check_format)
        expt_list = ExperimentList()
        expt_list.append(expt)
        MAIN_LOGGER.info("EVENT: DONE loading experiment list")
        expt.detector = detector  # in case of supplied ref geom

        exper_dataframe = pandas_table.query("%s=='%s'" % (exper_key, exper_name)).query("%s==%d" % (exper_idx_key, exper_id))

        refl_name = exper_dataframe[refls_key].values[0]
        refls = flex.reflection_table.from_file(refl_name)
        refls = refls.select(refls['id'] == exper_id)

        miller_inds = list( refls['miller_index'])
        is_not_000 = [h != (0, 0, 0) for h in miller_inds]
        is_in_Fhkl_model = [h in Fhkl_model_indices for h in miller_inds]
        MAIN_LOGGER.debug("Only refining %d/%d refls whose HKL are in structure factor model" %(np.sum(is_in_Fhkl_model), len(refls)))
        refl_sel = flex.bool(np.logical_and(is_not_000, is_in_Fhkl_model))
        refls = refls.select(refl_sel)

        exp_cry_sym = expt.crystal.get_space_group().type().lookup_symbol()
        if params.space_group is not None and exp_cry_sym.replace(" ", "") != params.space_group:
            gr = sgtbx.space_group_info(params.space_group).group()
            expt.crystal.set_space_group(gr)
            #raise ValueError("Crystals should all have the same space group symmetry")

        MAIN_LOGGER.info("EVENT: LOADING ROI DATA")
        shot_modeler = hopper_utils.DataModeler(params)
        shot_modeler.exper_name = exper_name
        shot_modeler.exper_idx = exper_id
        shot_modeler.refl_name = refl_name
        shot_modeler.rank = COMM.rank
        if params.refiner.load_data_from_refl:
            gathered = shot_modeler.GatherFromReflectionTable(expt, refls, sg_symbol=params.space_group)
            MAIN_LOGGER.debug("tried loading from reflection table")
        else:
            gathered = shot_modeler.GatherFromExperiment(expt, refls, sg_symbol=params.space_group)
            MAIN_LOGGER.debug("tried loading data from expt table")
        if not gathered:
            raise IOError("Failed to gather data from experiment %s", exper_name)
        else:
            MAIN_LOGGER.debug("successfully loaded data")
        MAIN_LOGGER.info("EVENT: DONE LOADING ROI")

        if gather_dir is not None:
            gathered_name = get_gather_name(exper_name, gather_dir)
            shot_modeler.dump_gathered_to_refl(gathered_name, do_xyobs_sanity_check=False)
            MAIN_LOGGER.info("SAVED ROI DATA TO %s" % gathered_name)
            all_data = shot_modeler.all_data.copy()
            all_roi_id = shot_modeler.roi_id.copy()
            all_bg = shot_modeler.all_background.copy()
            all_trusted = shot_modeler.all_trusted.copy()
            all_pids = np.array(shot_modeler.pids)
            all_rois = np.array(shot_modeler.rois)
            new_Modeler = hopper_utils.DataModeler(params)
            assert new_Modeler.GatherFromReflectionTable(exper_name, gathered_name, sg_symbol=params.space_group)
            assert np.allclose(new_Modeler.all_data, all_data)
            assert np.allclose(new_Modeler.all_background, all_bg)
            assert np.allclose(new_Modeler.rois, all_rois)
            assert np.allclose(new_Modeler.pids, all_pids)
            assert np.allclose(new_Modeler.all_trusted, all_trusted)
            assert np.allclose(new_Modeler.roi_id, all_roi_id)
            MAIN_LOGGER.info("Gathered file approved!")

        if gather_dir is not None:
            continue

        shot_modeler.set_parameters_for_experiment(best=exper_dataframe)
        shot_modeler.set_spectrum(spectra_file=exper_dataframe.spectrum_filename.values[0])
        MAIN_LOGGER.info("Will simulate %d energy channels" % len(shot_modeler.nanoBragg_beam_spectrum))

        # verify this
        shot_modeler.Umatrices = [shot_modeler.E.crystal.get_U()]

        MAIN_LOGGER.info(utils.memory_report('Rank 0 reporting memory usage'))
        if COMM.rank==0:
            print("Finished loading image %d / %d" % (ii + 1, len(worklist)), flush=True)

        shot_modelers.add_modeler(shot_modeler)

    if gather_dir is not None:
        if COMM.rank==0:
            pandas_table['ens.hopper.imported'] = [get_gather_name(f_exp, gather_dir) for f_exp in pandas_table[exper_key]]
            pd_name = os.path.join(params.outdir, "preImport_for_ensemble.pkl")
            pandas_table.to_pickle(pd_name)
            print("Wrote file %s to be used to re-run ens.hopper . Use optional ens.hopper arg '--refl ens.hopper.imported', and the phil param load_data_from_refl=True to load the imported data" % pd_name)
        COMM.barrier()
        sys.exit()
    shot_modelers.mpi_set_x_slices()

    assert shot_modelers.num_modelers > 0

    # use the first shot modeler to create a sim data instance:
    shot_modelers.SIM = hopper_utils.get_simulator_for_data_modelers(shot_modelers[0])

    shot_modelers.set_Fhkl_channels()

    return shot_modelers


 *******************************************************************************


 *******************************************************************************
simtbx/diffBragg/hopper_io.py
from __future__ import division
import pandas
from dxtbx.model import Experiment, ExperimentList
from copy import deepcopy
import logging
from simtbx.diffBragg import hopper_utils, utils
from scitbx.matrix import sqr , col
import os
import numpy as np


def save_expt_refl_file(filename, expts, refls, specs=None, check_exists=False, indices=None):
    """
    Save an input file for bg_and_probOri (the EMC initializer script)
    expt and refl names will be given absolute paths
    :param filename: input expt_refl name to be written (passable to script bg_and_probOri.py)
    :param expts: list of experiments
    :param refls: list of reflection tables
    :param specs: optional list of spectrum .lam files
    :param check_exists: ensure files actually exist
    :param indices: experiment indices if multiple images per experiment
    :return:
    """
    if specs is None:
        specs = [None]*len(expts)
    if indices is None:
        indices = [None] *len(indices)
    with open(filename, "w") as o:
        for expt, refl, spec, idx in zip(expts, refls, specs, indices):
            expt = os.path.abspath(expt)
            refl = os.path.abspath(refl)
            if spec is not None:
                spec = os.path.abspath(spec)
            if check_exists:
                assert os.path.exists(expt)
                assert os.path.exists(refl)
                if spec is not None:
                    assert os.path.exists(spec)
            if spec is None:
                spec = ""
            else:
                spec = " %s" %spec
            if idx is None:
                idx = ""
            else:
                idx = " %d" % idx
            o.write("%s %s%s%s\n" % (expt, refl, spec, idx))


def make_rank_outdir(root, subfolder, rank=0):
    rank_imgs_outdir = os.path.join(root, subfolder, "rank%d" % rank)
    if not os.path.exists(rank_imgs_outdir):
        os.makedirs(rank_imgs_outdir)
    return rank_imgs_outdir


def diffBragg_Umat(rotX, rotY, rotZ, U):
    xax = col((-1, 0, 0))
    yax = col((0, -1, 0))
    zax = col((0, 0, -1))
    ## update parameters:
    RX = xax.axis_and_angle_as_r3_rotation_matrix(rotX, deg=False)
    RY = yax.axis_and_angle_as_r3_rotation_matrix(rotY, deg=False)
    RZ = zax.axis_and_angle_as_r3_rotation_matrix(rotZ, deg=False)
    M = RX * RY * RZ
    U = M * sqr(U)
    return U


def save_to_pandas(x, Mod, SIM, orig_exp_name, params, expt, rank_exp_idx, stg1_refls, stg1_img_path=None,
                   rank=0, write_expt=True, write_pandas=True, exp_idx=0):
    """

    :param x: the optiized parameters used by hopper (output of Minimize)
    :param Mod: the instance of the hopper_utils.DataModeler that was used by hopper
    :param SIM: the instance of nanoBragg/sim_data.SimData that was used by hopper
    :param orig_exp_name: the name of the experiment list that was input to hopper
    :param params: the diffBragg hopper parameters
    :param expt: the data modeler experiment
    :param rank_exp_idx: order this shot was processed by this MPI rank #TODO rename this
    :param stg1_refls: path to the refls that were input to hopper
    :param stg1_img_path: leave as None, no longer used
    :param rank: MPI rank
    :param write_expt: whether to write the single shot experiment
    :param write_pandas: whether to write the single shot dataframe
    :param exp_idx: the index of the experiment within the experiment list (orig_exp_name)
    :return: the single shot dataframe
    """
    LOGGER = logging.getLogger("refine")
    opt_exp_path = None
    basename = os.path.splitext(os.path.basename(orig_exp_name))[0]
    if write_expt:
        rank_exper_outdir = make_rank_outdir(params.outdir, "expers",rank)
        opt_exp_path = os.path.join(rank_exper_outdir, "%s_%s_%d.expt" % (params.tag, basename, rank_exp_idx))

    scale, rotX, rotY, rotZ, Na, Nb, Nc, Nd, Ne, Nf,\
        diff_gam_a, diff_gam_b, diff_gam_c, diff_sig_a, \
        diff_sig_b, diff_sig_c, a,b,c,al,be,ga,detz_shift = \
        hopper_utils.get_param_from_x(x, Mod)

    scale_p = Mod.P["G_xtal0"]
    scale_init = scale_p.init

    Nabc_init = []
    for i in [0,1,2]:
        p = Mod.P["Nabc%d" % i]
        Nabc_init.append(p.init)
    Nabc_init = tuple(Nabc_init)

    if params.isotropic.diffuse_gamma:
        diff_gam_b = diff_gam_c = diff_gam_a
    if params.isotropic.diffuse_sigma:
        diff_sig_b = diff_sig_c = diff_sig_a

    if params.simulator.crystal.has_isotropic_ncells:
        Nb = Nc = Na

    eta_a, eta_b, eta_c = hopper_utils.get_mosaicity_from_x(x, Mod, SIM)
    a_init, b_init, c_init, al_init, be_init, ga_init = SIM.crystal.dxtbx_crystal.get_unit_cell().parameters()

    U = diffBragg_Umat(rotX, rotY, rotZ, SIM.crystal.dxtbx_crystal.get_U())
    new_cryst = deepcopy(SIM.crystal.dxtbx_crystal)
    new_cryst.set_U(U)

    ucparam = a, b, c, al, be, ga
    ucman = utils.manager_from_params(ucparam)
    new_cryst.set_B(ucman.B_recipspace)

    Amat = new_cryst.get_A()
    other_Umats = []
    other_spotscales = []
    if Mod.num_xtals > 1:
        for i_xtal in range(1,Mod.num_xtals,1):
            par = hopper_utils.get_param_from_x(x, Mod, i_xtal=i_xtal, as_dict=True)
            scale_xt = par['scale']
            rotX_xt = par['rotX']
            rotY_xt = par['rotY']
            rotZ_xt = par['rotZ']
            U_xt = diffBragg_Umat(rotX_xt, rotY_xt, rotZ_xt, SIM.Umatrices[i_xtal])
            other_Umats.append(U_xt)
            other_spotscales.append(scale_xt)

    eta = [0]
    lam_coefs = [0], [1]
    if hasattr(Mod, "P"):
        names = "lambda_offset", "lambda_scale"
        if names[0] in Mod.P and names[1] in Mod.P:
            lam_coefs = []
            for name in names:
                if name in Mod.P:
                    p = Mod.P[name]
                    val = p.get_val(x[p.xpos])
                    lam_coefs.append([val])
            lam_coefs = tuple(lam_coefs)

    new_expt = Experiment()
    new_expt.crystal = new_cryst
    new_expt.detector = expt.detector
    new_expt.beam = expt.beam
    new_expt.identifier = expt.identifier
    new_expt.imageset = expt.imageset
    # expt.detector = refiner.get_optimized_detector()
    new_exp_list = ExperimentList()
    new_exp_list.append(new_expt)
    if write_expt:
        new_exp_list.as_file(opt_exp_path)
        LOGGER.debug("saved opt_exp %s with wavelength %f" % (opt_exp_path, expt.beam.get_wavelength()))
    _,flux_vals = zip(*SIM.beam.spectrum)

    df = single_expt_pandas(xtal_scale=scale, Amat=Amat,
        ncells_abc=(Na, Nb, Nc), ncells_def=(Nd,Ne,Nf),
        eta_abc=(eta_a, eta_b, eta_c),
        diff_gamma=(diff_gam_a, diff_gam_b, diff_gam_c),
        diff_sigma=(diff_sig_a, diff_sig_b, diff_sig_c),
        detz_shift=detz_shift,
        use_diffuse=params.use_diffuse_models,
        gamma_miller_units=params.gamma_miller_units,
        eta=eta,
        rotXYZ=(rotX, rotY, rotZ),
        ucell_p = (a,b,c,al,be,ga),
        ucell_p_init=(a_init, b_init, c_init, al_init, be_init, ga_init),
        lam0_lam1 = lam_coefs,
        spec_file=params.simulator.spectrum.filename,
        spec_stride=params.simulator.spectrum.stride,
        flux=sum(flux_vals), beamsize_mm=SIM.beam.size_mm,
        orig_exp_name=orig_exp_name, opt_exp_name=opt_exp_path,
        spec_from_imageset=params.spectrum_from_imageset,
        oversample=params.simulator.oversample,
        opt_det=params.opt_det, stg1_refls=stg1_refls,
        stg1_img_path=stg1_img_path,
        ncells_init=Nabc_init, spot_scales_init=scale_init,
        other_Umats = other_Umats, other_spotscales = other_spotscales,
        num_mosaicity_samples=params.simulator.crystal.num_mosaicity_samples)

    df['exp_idx'] = exp_idx

    if hasattr(Mod, "sigz"):
        df['sigz'] = [Mod.sigz]
    if hasattr(Mod, "niter"):
        df['niter'] = [Mod.niter]
    df['phi_deg'] = SIM.D.phi_deg
    df['osc_deg'] = SIM.D.osc_deg
    if write_pandas:
        rank_pandas_outdir = make_rank_outdir(params.outdir, "pandas",rank)
        pandas_path = os.path.join(rank_pandas_outdir, "%s_%s_%d.pkl" % (params.tag, basename, rank_exp_idx))
        df.to_pickle(pandas_path)
    return df


def single_expt_pandas(xtal_scale, Amat, ncells_abc, ncells_def, eta_abc,
                       diff_gamma, diff_sigma, detz_shift, use_diffuse, gamma_miller_units, eta,
                       rotXYZ, ucell_p, ucell_p_init, lam0_lam1,
                       spec_file, spec_stride,flux, beamsize_mm,
                       orig_exp_name, opt_exp_name, spec_from_imageset, oversample,
                       opt_det, stg1_refls, stg1_img_path, ncells_init=None, spot_scales_init = None,
                       other_Umats=None, other_spotscales=None, num_mosaicity_samples=None):
    """

    :param xtal_scale:
    :param Amat:
    :param ncells_abc:
    :param ncells_def:
    :param eta_abc:
    :param diff_gamma:
    :param diff_sigma:
    :param detz_shift:
    :param use_diffuse:
    :param gamma_miller_units:
    :param eta:
    :param rotXYZ:
    :param ucell_p:
    :param ucell_p_init:
    :param lam0_lam1:
    :param spec_file:
    :param spec_stride:
    :param flux:
    :param beamsize_mm:
    :param orig_exp_name:
    :param opt_exp_name:
    :param spec_from_imageset:
    :param oversample:
    :param opt_det:
    :param stg1_refls:
    :param stg1_img_path:
    :num_mosaicity_samples:
    :return:
    """
    if other_Umats is None:
        other_Umats = []
    if other_spotscales is None:
        other_spotscales = []
    if ncells_init is None:
        ncells_init = np.nan, np.nan, np.nan
    if spot_scales_init is None:
        spot_scales_init = np.nan
    a,b,c,al,be,ga = ucell_p
    a_init, b_init, c_init, al_init, be_init, ga_init = ucell_p_init
    lam0,lam1 = lam0_lam1
    df = pandas.DataFrame({
        "spot_scales": [xtal_scale], "Amats": [Amat], "ncells": [ncells_abc],
        "spot_scales_init": [spot_scales_init],
        "ncells_init": [ncells_init],
        "eta_abc": [eta_abc],
        "detz_shift_mm": [detz_shift * 1e3],
        "ncells_def": [ncells_def],
        "diffuse_gamma": [diff_gamma],
        "diffuse_sigma": [diff_sigma],
        "fp_fdp_shift": [np.nan],
        "use_diffuse_models": [use_diffuse],
        "gamma_miller_units": [gamma_miller_units],
        "eta": eta,
        "rotX": rotXYZ[0],
        "rotY": rotXYZ[1],
        "rotZ": rotXYZ[2],
        "a": a, "b": b, "c": c, "al": al, "be": be, "ga": ga,
        "a_init": a_init, "b_init": b_init, "c_init": c_init, "al_init": al_init,
        "lam0": lam0, "lam1": lam1,
        "be_init": be_init, "ga_init": ga_init})
    if spec_file is not None:
        spec_file = os.path.abspath(spec_file)
    df["spectrum_filename"] = spec_file
    df["spectrum_stride"] = spec_stride
    if other_spotscales:
        df["other_spotscales"] = [tuple(other_spotscales)]
    if other_Umats:
        df["other_Umats"] = [tuple(map(tuple, other_Umats))]
    if num_mosaicity_samples is not None:
        df['num_mosaicity_samples'] = num_mosaicity_samples

    df["total_flux"] = flux
    df["beamsize_mm"] = beamsize_mm
    df["exp_name"] = os.path.abspath(orig_exp_name)

    if opt_exp_name is not None:
        opt_exp_name = os.path.abspath(opt_exp_name)
    df["opt_exp_name"] = opt_exp_name
    df["spectrum_from_imageset"] = spec_from_imageset
    df["oversample"] = oversample
    if opt_det is not None:
        df["opt_det"] = opt_det
    df["stage1_refls"] = stg1_refls
    df["stage1_output_img"] = stg1_img_path
    return df


 *******************************************************************************


 *******************************************************************************
simtbx/diffBragg/hopper_utils.py
from __future__ import absolute_import, division, print_function
import time
import os
import json
from dials.algorithms.shoebox import MaskCode
from copy import deepcopy
from dials.model.data import Shoebox

from simtbx.diffBragg import hopper_io
import numpy as np
from cctbx import crystal, miller
from scipy.optimize import dual_annealing, basinhopping
from collections import Counter
from scitbx.matrix import sqr, col
from scipy.ndimage import binary_dilation
from dxtbx.model.experiment_list import ExperimentListFactory
from dxtbx.model import Spectrum
try:  # TODO keep backwards compatibility until we close the nxmx_writer_experimental branch
    from serialtbx.detector.jungfrau import get_pedestalRMS_from_jungfrau
except ModuleNotFoundError:
    from xfel.util.jungfrau import get_pedestalRMS_from_jungfrau
from simtbx.nanoBragg.utils import downsample_spectrum
from dials.array_family import flex
from simtbx.diffBragg import utils
from simtbx.diffBragg.refiners.parameters import RangedParameter, Parameters, PositiveParameter
from simtbx.diffBragg.attr_list import NB_BEAM_ATTRS, NB_CRYST_ATTRS, DIFFBRAGG_ATTRS
from simtbx.diffBragg import psf

try:
    from line_profiler import LineProfiler
except ImportError:
    LineProfiler = None


import logging
MAIN_LOGGER = logging.getLogger("diffBragg.main")
PROFILE_LOGGER = logging.getLogger("diffBragg.profile")

ROTX_ID = 0
ROTY_ID = 1
ROTZ_ID = 2
ROTXYZ_IDS = ROTX_ID, ROTY_ID, ROTZ_ID
NCELLS_ID = 9
NCELLS_ID_OFFDIAG = 21
UCELL_ID_OFFSET = 3
DETZ_ID = 10
FHKL_ID = 11
ETA_ID = 19
DIFFUSE_ID = 23
LAMBDA_IDS = 12, 13

DEG = 180 / np.pi


def write_SIM_logs(SIM, log=None, lam=None):
    """
    Logs properties of SIM.D (diffBragg instance), and SIM.crystal, SIM.beam (nanoBragg beam and crystal)
    These are important for reproducing results
    :param SIM: sim_data instance used during hopper refinement, member of the data modeler
    :param log: optional log file to dump attributes of SIM
    :param lam: optional lambda file to dump the spectra to disk, can be read usint diffBragg.utils.load_spectra_file
    """
    if log is not None:
        with open(log, "w") as o:
            print("<><><><><>", file=o)
            print("DIFFBRAGG", file=o)
            print("<><><><><>", file=o)
            for attr in DIFFBRAGG_ATTRS:
                val = getattr(SIM.D, attr)
                print(attr+": ", val, file=o)
            if SIM.refining_Fhkl is not None and SIM.refining_Fhkl:
                channels = SIM.D.get_Fhkl_channels()
                channels = ",".join(map(str, channels))
                print("Fhkl channels: ", channels, file=o)
            print("\n<><><>", file=o)
            print("BEAM", file=o)
            print("<><><>", file=o)
            for attr in NB_BEAM_ATTRS:
                val = getattr(SIM.beam, attr)
                print(attr+": ", val, file=o)
            print("\n<><><><>", file=o)
            print("CRYSTAL", file=o)
            print("<><><><>", file=o)
            for attr in NB_CRYST_ATTRS:
                val = getattr(SIM.crystal, attr)
                print(attr+": ", val, file=o)
    if lam is not None:
        wavelen, wt = zip(*SIM.beam.spectrum)
        utils.save_spectra_file(lam, wavelen, wt)


def free_SIM_mem(SIM):
    """
    Frees memory allocated to host CPU (and GPU device, if applicable).
    Using this method is critical for serial applications!
    :param SIM: sim_data instance used during hopper refinement, member of the data modeler
    """
    SIM.D.free_all()
    SIM.D.free_Fhkl2()
    try:
        SIM.D.gpu_free()
    except TypeError:
        pass  # occurs on CPU-only builds


def finalize_SIM(SIM, log=None, lam=None):
    """
    thin wrapper to free_SIM_mem and write_SIM_logs
    :param SIM: sim_data instance used during hopper refinement, member of the data modeler
    :param log: optional log file to dump attributes of SIM
    :param lam: optional lambda file to dump the spectra to disk, can be read usint diffBragg.utils.load_spectra_file
    """
    write_SIM_logs(SIM, log, lam)
    free_SIM_mem(SIM)


class DataModeler:

    """
    The data modeler stores information in two ways:
    1- lists whose length is the number of pixels being modeled
    2- lists whose length is the number of shoeboxes being modeled

    for example if one is modeling 3 shoeboxes whose dimensions are 10x10, then
    the objects below like self.all_data will have length 300, and other objects like self.selection_flags
    will have length 3
    """

    def __init__(self, params):
        """ params is a simtbx.diffBragg.hopper phil"""
        self.Fhkl_channel_ids = None # this should be a list the same length as the spectrum. specifies which Fhkl to refine, depending on the energy (for eg. two color experiment)
        self.no_rlp_info = False  # whether rlps are stored in the refls table
        self.params = params  # phil params (see diffBragg/phil.py)
        self._abs_path_params()
        self.num_xtals = 1
        self.E = None  # placeholder for the dxtbx.model.Experiment instance
        self.pan_fast_slow =None  # (pid, fast, slow) per pixel
        self.all_background =None  # background model per pixel (photon units)
        self.roi_id =None  # region of interest ID per pixel
        self.u_id = None  # set of unique region of interest ids
        self.all_freq = None  # flag for the h,k,l frequency of the observed pixel
        self.best_model = None  # best model value at each pixel
        self.best_model_includes_background = False  # whether the best model includes the background scattering estimate
        self.all_nominal_hkl_p1 = None  # nominal p1 hkl at each pixel
        self.all_nominal_hkl = None  # nominal hkl at each pixel
        self.all_data =None  # data at each pixel (photon units)
        self.all_sigma_rdout = None  # this is either a float or an array. if the phil param use_perpixel_dark_rms=True, then these are different per pixel, per shot
        self.all_gain = None  # gain value per pixel (used during diffBragg/refiners/stage_two_refiner)
        self.all_sigmas =None  # error model for each pixel (photon units)
        self.all_trusted =None  # trusted pixel flags (True is trusted and therefore used during refinement)
        self.npix_total =None  # total number of pixels
        self.all_fast =None  # fast-scan coordinate per pixel
        self.all_slow =None  # slow-scan coordinate per pixel
        self.all_pid = None  # panel id per pixel
        self.all_zscore = None # the estimated z-score values for each pixel, updated each iteration in the Target class
        self.rois=None  # region of interest (per spot)
        self.pids=None  # panel id (per spot)
        self.tilt_abc=None  # background plane constants (per spot), a,b are fast,slow scan components, c is offset
        self.selection_flags=None  # whether the spot was selected for refinement (sometimes poorly conditioned spots are rejected)
        self.tilt_cov = None  # covariance estimates from background fitting (not used)
        self.simple_weights = None  # not used
        self.refls_idx = None  # position of modeled spot in original refl array
        self.refls = None  # reflection table
        self.nominal_sigma_rdout = None   # the value of the readout noise in photon units
        self.exper_name = None  # optional name specifying where dxtbx.model.Experiment was loaded from
        self.refl_name = None  # optional name specifying where dials.array_family.flex.reflection_table refls were loaded from
        self.spec_name = None  # optional name specifying spectrum file(.lam)
        self.exper_idx = 0  # optional number specifying the index of the experiment in the experiment list
        self.rank = 0  # in case DataModelers are part of an MPI program, have a rank attribute for record keeping

        self.Hi = None  # miller index (P1)
        self.Hi_asu = None  # miller index (high symmetry)
        self.target = None  # placeholder for the Target class instance
        self.nanoBragg_beam_spectrum = None  # see spectrun property of NBBeam (nanoBragg_beam.py)

        # which attributes to save when pickling a data modeler
        self.saves = ["all_data", "all_background", "all_trusted", "best_model", "nominal_sigma_rdout",
                      "rois", "pids", "tilt_abc", "selection_flags", "refls_idx", "pan_fast_slow",
                      "Hi", "Hi_asu", "roi_id", "params", "all_pid", "all_fast", "all_slow", "best_model_includes_background",
                      "all_q_perpix", "all_sigma_rdout"]

    def filter_pixels(self, thresh):
        assert self.roi_id is not None
        assert self.all_trusted is not None
        assert self.all_zscore is not None

        if not hasattr(self, 'roi_id_slices') or self.roi_id_slices is None:
            self.set_slices('roi_id')

        ntrust = self.all_trusted.sum()

        sigz_per_shoebox = []
        for roi_id in self.roi_id_unique:
            slcs = self.roi_id_slices[roi_id]
            Zs = []
            for slc in slcs:
                trusted = self.all_trusted[slc]
                Zs += list(self.all_zscore[slc][trusted])
            if not Zs:
                sigz = np.nan
            else:
                sigz = np.std(Zs)
            sigz_per_shoebox.append(sigz)
        if np.all(np.isnan(sigz_per_shoebox)):
            MAIN_LOGGER.debug("All shoeboxes are nan, nothing to filter")
            return
        med_sigz = np.median([sigz for sigz in sigz_per_shoebox if not np.isnan(sigz)])
        sigz_per_shoebox = np.nan_to_num(sigz_per_shoebox, nan=med_sigz)
        shoebox_is_bad = utils.is_outlier(sigz_per_shoebox, thresh)
        nbad_pix = 0
        for i_roi, roi_id in enumerate(self.roi_id_unique):
            if shoebox_is_bad[i_roi]:
                for slc in self.roi_id_slices[roi_id]:
                    nbad_pix += slc.stop - slc.start
                    self.all_trusted[slc] = False

        #inds = np.arange(len(self.all_trusted))
        #Zs = self.all_zscore[self.all_trusted]
        #bad = utils.is_outlier(Zs, thresh=thresh)
        #inds_trusted = inds[self.all_trusted]
        #self.all_trusted[inds_trusted[bad]] = False
        MAIN_LOGGER.debug("Added %d pixels from %d shoeboxes to the untrusted list (%d / %d trusted pixels remain)"
                          % (nbad_pix, shoebox_is_bad.sum(), self.all_trusted.sum(), len(self.all_trusted)))

    def set_spectrum(self, spectra_file=None, spectra_stride=None, total_flux=None):

        # note , the following 3 settings will only be used if spectrum_from_imageset is False and gause_spec is False
        if spectra_file is None:
            spectra_file = self.params.simulator.spectrum.filename
        if spectra_stride is None:
            spectra_stride = self.params.simulator.spectrum.stride  # will only be used if spectra_file is not None
        if total_flux is None:
            total_flux = self.params.simulator.total_flux

        if self.params.spectrum_from_imageset:
            self.nanoBragg_beam_spectrum = downsamp_spec_from_params(self.params, self.E)
        elif self.params.gen_gauss_spec:
            self.nanoBragg_beam_spectrum = set_gauss_spec(None, self.params, self.E)
        elif spectra_file is not None:
            self.nanoBragg_beam_spectrum = utils.load_spectra_file(spectra_file, total_flux, spectra_stride, as_spectrum=True)
        else:
            assert total_flux is not None
            self.nanoBragg_beam_spectrum = [(self.E.beam.get_wavelength(), total_flux)]

    def set_Fhkl_channels(self, SIM, set_in_diffBragg=True):
        if self.nanoBragg_beam_spectrum is None:
            raise AttributeError("Needs nanoBragg_beam_spectrum property first!")
        energies = np.array([utils.ENERGY_CONV / wave for wave, _ in self.nanoBragg_beam_spectrum])
        Fhkl_channel_ids = np.zeros(len(energies), int)
        for i_channel, (en1, en2) in enumerate(zip(SIM.Fhkl_channel_bounds, SIM.Fhkl_channel_bounds[1:])):
            sel = (energies >= en1) * (energies < en2)
            Fhkl_channel_ids[sel] = i_channel
        if set_in_diffBragg:
            SIM.D.update_Fhkl_channels(Fhkl_channel_ids)
        self.Fhkl_channel_ids = Fhkl_channel_ids

    def at_minimum(self, x, f, accept):
        self.target.iteration = 0
        self.target.all_x = []
        self.target.x0[self.target.vary] = x
        self.target.hop_iter += 1
        #self.target.minima.append((f,self.target.x0,accept))
        self.target.lowest_x = x
        try:
            # TODO get SIM and i_shot in here so we can save_up each new global minima!
            if f < self.target.lowest_f:
                 self.target.lowest_f = f
                 MAIN_LOGGER.info("New minimum found!")
                 self.save_up(self.target.x0, SIM, self.rank, i_shot=i_shot)
        except NameError:
            pass

    def _abs_path_params(self):
        """adds absolute path to certain params"""
        if self.params.simulator.structure_factors.mtz_name is not None:
            self.params.simulator.structure_factors.mtz_name = os.path.abspath(self.params.simulator.structure_factors.mtz_name)

    def __getstate__(self):
        # TODO cleanup/compress
        return {name: getattr(self, name) for name in self.saves}

    def __setstate__(self, state):
        for name in state:
            setattr(self, name, state[name])

    def set_slices(self, attr_name):
        """finds the boundaries for each attr in the 1-D array of per-shot data
        :params: attr_name, str

        For example, in a DataModeler, roi_id is set for every pixel,
        >> Modeler.roi_id returns
        0 0 0 0 1 1 2 2 2 2 3 3 3 3 4 4 4 4 4 ...

        A call Modeler.set_slices("roi_id") adds attributes
            roi_id_slices
            roi_id_unique
        where roi_id_slices is a dictionary whose keys are the unique roi ids and whose values
        are a list of slices corresponding to each connected regeion of the same roi ids.

        In the example,
        >> Modeler.roi_id_slices[0][0]
        slice(0,4,1)
        """
        vals = self.__getattribute__(attr_name)
        # find where the vals change
        splitter = np.where(np.diff(vals) != 0)[0] + 1
        npix = len(self.all_data)
        slices = [slice(V[0], V[-1] + 1, 1) for V in np.split(np.arange(npix), splitter)]
        vals_ids = [V[0] for V in np.split(np.array(vals), splitter)]
        vals_id_slices = {}
        for i_vals, slc in zip(vals_ids, slices):
            if i_vals not in vals_id_slices:
                vals_id_slices[i_vals] = [slc]
            else:
                vals_id_slices[i_vals].append(slc)
        unique_vals = set(vals)
        self.__setattr__("%s_unique" % attr_name, unique_vals)
        logging.debug("Modeler has data on %d unique vals from attribute %s" % (len(unique_vals) , attr_name))
        self.__setattr__("%s_slices" % attr_name, vals_id_slices)

    @property
    def sigma_rdout(self):
        print("WARNING ,this attribute will soon be deprecated, use nominal_sigma_rdout instead!")
        return self.nominal_sigma_rdout

    def clean_up(self, SIM):
        free_SIM_mem(SIM)

    @staticmethod
    def exper_json_single_file(exp_file, i_exp=0, check_format=True):
        """
        load a single experiment from an exp_file
        If working with large combined experiment files, we only want to load
        one image at a time on each MPI rank, otherwise at least one rank would need to
        load the entire file into memory.
        :param exp_file: experiment list file
        :param i_exp: experiment id
        :param check_format: bool, verifies the format class of the experiment, set to False if loading data from refls
        :return:
        """
        exper_json = json.load(open(exp_file))
        nexper = len(exper_json["experiment"])
        assert 0 <= i_exp < nexper

        this_exper = exper_json["experiment"][i_exp]

        new_json = {'__id__': "ExperimentList", "experiment": [deepcopy(this_exper)]}

        for model in ['beam', 'detector', 'crystal', 'imageset', 'profile', 'scan', 'goniometer', 'scaling_model']:
            if model in this_exper:
                model_index = this_exper[model]
                new_json[model] = [exper_json[model][model_index]]
                new_json["experiment"][0][model] = 0
            else:
                new_json[model] = []
        explist = ExperimentListFactory.from_dict(new_json, check_format=check_format)
        assert len(explist) == 1
        return explist[0]

    def set_experiment(self, exp, load_imageset=True, exp_idx=0):
        """
        :param exp: experiment or filename
        :param load_imageset: whether to load the imageset (usually True)
        :param exp_idx: index corresponding to experiment in experiment list
        """
        if isinstance(exp, str):
            if not load_imageset:
                self.E = ExperimentListFactory.from_json_file(exp, False)[exp_idx]
            else:
                self.E = self.exper_json_single_file(exp, exp_idx)
        else:
            self.E = exp
        if self.params.opt_det is not None:
            opt_det_E = ExperimentListFactory.from_json_file(self.params.opt_det, False)[0]
            self.E.detector = opt_det_E.detector
            MAIN_LOGGER.info("Set the optimal detector from %s" % self.params.opt_det)

        if self.params.opt_beam is not None:
            opt_beam_E = ExperimentListFactory.from_json_file(self.params.opt_beam, False)[0]
            self.E.beam = opt_beam_E.beam
            MAIN_LOGGER.info("Set the optimal beam from %s" % self.params.opt_beam)

    def load_refls(self, ref, exp_idx=0):
        """
        :param ref: reflection table or filename
        :param exp_idx: index corresponding to experiment in experiment list
        """
        if isinstance(ref, str):
            refls = flex.reflection_table.from_file(ref)
            # TODO: is this the proper way to select the id ?
            refls = refls.select(refls['id']==exp_idx)
        else:
            # assert is a reflection table. ..
            refls = ref
        return refls

    def is_duplicate_hkl(self, refls):
        nref = len(refls)
        is_duplicate = np.zeros(nref, bool)
        if len(set(refls['miller_index'])) < nref:
            hkls = refls['miller_index']
            dupe_hkl = {h for h, count in Counter(hkls).items() if count > 1}
            for i_ref in range(nref):
                hh = refls[i_ref]['miller_index']
                is_duplicate[i_ref] = hh in dupe_hkl

        return is_duplicate

    def GatherFromReflectionTable(self, exp, ref, sg_symbol=None):

        self.set_experiment(exp, load_imageset=False)
        self.refls = self.load_refls(ref)
        nref = len(self.refls)
        if nref ==0:
            return False
        self.refls_idx = list(range(nref))
        self.rois = [(x1, x2, y1, y2) for x1,x2,y1,y2,_,_ in self.refls["shoebox"].bounding_boxes()]
        self.pids = list(self.refls["panel"])

        npan = len(self.E.detector)
        nfast, nslow = self.E.detector[0].get_image_size()  # NOTE assumes all panels same shape
        img_data = np.zeros((npan, nslow, nfast))
        background = np.zeros_like(img_data)
        is_trusted = np.zeros((npan, nslow, nfast), bool)
        for i_ref in range(nref):
            ref = self.refls[i_ref]
            pid = ref['panel']
            x1, x2, y1, y2 = self.rois[i_ref]

            # these are the in-bounds limits (on the panel)
            x1_onPanel = max(x1,0)
            x2_onPanel = min(x2,nfast)
            y1_onPanel = max(y1,0)
            y2_onPanel = min(y2,nslow)

            xdim = x2_onPanel-x1_onPanel
            ydim = y2_onPanel-y1_onPanel

            sb = ref['shoebox']
            sb_ystart = y1_onPanel - y1
            sb_xstart = x1_onPanel - x1
            sb_sliceY = slice(sb_ystart, sb_ystart+ydim,1)
            sb_sliceX = slice(sb_xstart, sb_xstart+xdim,1)

            dat_sliceY = slice(y1_onPanel, y1_onPanel+ydim,1)
            dat_sliceX = slice(x1_onPanel, x1_onPanel+xdim,1)
            img_data[pid,   dat_sliceY, dat_sliceX] = sb.data.as_numpy_array()[0,sb_sliceY,sb_sliceX]
            sb_bkgrnd = sb.background.as_numpy_array()[0,sb_sliceY,sb_sliceX]
            background[pid, dat_sliceY, dat_sliceX] = sb_bkgrnd
            fg_code = MaskCode.Valid + MaskCode.Foreground  # 5
            bg_code = MaskCode.Valid + MaskCode.Background + MaskCode.BackgroundUsed  # 19
            mask = sb.mask.as_numpy_array()[0,sb_sliceY,sb_sliceX]
            if self.params.refiner.refldata_trusted=="allValid":
                sb_trust = mask > 0
            elif self.params.refiner.refldata_trusted=="fg":
                sb_trust = mask==fg_code
            else:
                sb_trust = np.logical_or(mask==fg_code, mask==bg_code)

            # below_zero = sb_bkgrnd <= 0
            below_zero = sb_bkgrnd < 0
            if np.any(below_zero):
                nbelow = np.sum(below_zero)
                ntot = sb_bkgrnd.size
                MAIN_LOGGER.debug("background <= zero in %d/%d pixels from shoebox %d! Marking those pixels as untrusted!" %  ( nbelow, ntot, i_ref ))
                sb_trust[below_zero] = False

            is_trusted[pid, dat_sliceY,dat_sliceX] = sb_trust

            self.rois[i_ref] = x1_onPanel, x2_onPanel, y1_onPanel, y2_onPanel


        if self.params.refiner.refldata_to_photons:
            MAIN_LOGGER.debug("Re-scaling reflection data to photon units: conversion factor=%f" % self.params.refiner.adu_per_photon)
            img_data /= self.params.refiner.adu_per_photon
            background /= self.params.refiner.adu_per_photon

        # can be used for Bfactor modeling
        self.Q = np.linalg.norm(self.refls["rlp"], axis=1)
        self.nominal_sigma_rdout = self.params.refiner.sigma_r / self.params.refiner.adu_per_photon

        self.Hi = list(self.refls["miller_index"])
        if sg_symbol is not None:
            self.Hi_asu = utils.map_hkl_list(self.Hi, True, sg_symbol)
        else:
            self.Hi_asu = self.Hi

        self.data_to_one_dim(img_data, is_trusted, background)
        return True

    def GatherFromExperiment(self, exp, ref, remove_duplicate_hkl=True, sg_symbol=None, exp_idx=0):
        """

        :param exp: experiment list filename , or experiment object
        :param ref: reflection table filename, or reflection table instance
        :param remove_duplicate_hkl: search for miller index duplicates and remove
        :param sg_symbol: space group lookup symbol P43212
        :param exp_idx: index of the experiment in the experiment list
        :return:
        """
        self.set_experiment(exp, load_imageset=True, exp_idx=exp_idx)

        refls = self.load_refls(ref, exp_idx=exp_idx)
        if len(refls)==0:
            MAIN_LOGGER.warning("no refls loaded!")
            return False

        if "rlp" not in list(refls[0].keys()):
            try:
                utils.add_rlp_column(refls, self.E)
                assert "rlp" in list(refls[0].keys())
            except KeyError:
                self.no_rlp_info = True
        img_data = utils.image_data_from_expt(self.E)
        img_data /= self.params.refiner.adu_per_photon
        is_trusted = np.ones(img_data.shape, bool)
        hotpix_mask = None
        if self.params.roi.hotpixel_mask is not None:
            is_trusted = utils.load_mask(self.params.roi.hotpixel_mask)
            hotpix_mask = ~is_trusted
        self.nominal_sigma_rdout = self.params.refiner.sigma_r / self.params.refiner.adu_per_photon

        roi_packet = utils.get_roi_background_and_selection_flags(
            refls, img_data, shoebox_sz=self.params.roi.shoebox_size,
            reject_edge_reflections=self.params.roi.reject_edge_reflections,
            reject_roi_with_hotpix=self.params.roi.reject_roi_with_hotpix,
            background_mask=None, hotpix_mask=hotpix_mask,
            bg_thresh=self.params.roi.background_threshold,
            use_robust_estimation=not self.params.roi.fit_tilt,
            set_negative_bg_to_zero=self.params.roi.force_negative_background_to_zero,
            pad_for_background_estimation=self.params.roi.pad_shoebox_for_background_estimation,
            sigma_rdout=self.nominal_sigma_rdout, deltaQ=self.params.roi.deltaQ, experiment=self.E,
            weighted_fit=self.params.roi.fit_tilt_using_weights,
            allow_overlaps=self.params.roi.allow_overlapping_spots,
            ret_cov=True, skip_roi_with_negative_bg=self.params.roi.skip_roi_with_negative_bg,
            only_high=self.params.roi.only_filter_zingers_above_mean, centroid=self.params.roi.centroid)

        if roi_packet is None:
            return False

        self.rois, self.pids, self.tilt_abc, self.selection_flags, background, self.tilt_cov = roi_packet

        if remove_duplicate_hkl and not self.no_rlp_info:
            is_not_a_duplicate = ~self.is_duplicate_hkl(refls)
            self.selection_flags = np.logical_and( self.selection_flags, is_not_a_duplicate)
        else:
            self.selection_flags  = np.array(self.selection_flags)

        if self.params.refiner.res_ranges is not None:
            # TODO add res ranges support for GatherFromReflectionTable
            if self.no_rlp_info:
                raise NotImplementedError("Cannot set resolution limits when processing refls that are missing the RLP column")
            res_flags = np.zeros(len(refls)).astype(bool)
            res = 1. / np.linalg.norm(refls["rlp"], axis=1)
            for dmin,dmax in utils.parse_reso_string(self.params.refiner.res_ranges):
                MAIN_LOGGER.debug("Parsing res range %.3f - %.3f Angstrom" % (dmin, dmax))
                in_resShell = np.logical_and(res >= dmin, res < dmax)
                res_flags[in_resShell] = True

            MAIN_LOGGER.info("Resolution filter removed %d/%d refls outside of all resolution ranges " \
                              % (sum(~res_flags), len(refls)))
            self.selection_flags[~res_flags] = False

        if "miller_index" in list(refls.keys()):
            self.Hi = list(refls["miller_index"])
            if sg_symbol is not None:
                self.Hi_asu = utils.map_hkl_list(self.Hi, True, sg_symbol)
            else:
                self.Hi_asu = self.Hi

        if sum(self.selection_flags) == 0:
            MAIN_LOGGER.info("No pixels slected, continuing")
            return False
        self.refls = refls
        self.refls_idx = [i_roi for i_roi in range(len(refls)) if self.selection_flags[i_roi]]

        self.rois = [roi for i_roi, roi in enumerate(self.rois) if self.selection_flags[i_roi]]
        self.tilt_abc = [abc for i_roi, abc in enumerate(self.tilt_abc) if self.selection_flags[i_roi]]
        self.pids = [pid for i_roi, pid in enumerate(self.pids) if self.selection_flags[i_roi]]
        self.tilt_cov = [cov for i_roi, cov in enumerate(self.tilt_cov) if self.selection_flags[i_roi]]
        self.Hi =[hi for i_roi, hi in enumerate(self.Hi) if self.selection_flags[i_roi]]
        self.Hi_asu =[hi_asu for i_roi, hi_asu in enumerate(self.Hi_asu) if self.selection_flags[i_roi]]

        if not self.no_rlp_info:
            self.Q = [np.linalg.norm(refls[i_roi]["rlp"]) for i_roi in range(len(refls)) if self.selection_flags[i_roi]]

        self.data_to_one_dim(img_data, is_trusted, background)
        return True

    def data_to_one_dim(self, img_data, is_trusted, background):
        all_data = []
        all_sigma_rdout = []
        all_pid = []
        all_fast = []
        all_slow = []
        all_fast_relative = []
        all_slow_relative = []
        all_trusted = []
        all_sigmas = []
        all_background = []
        roi_id = []
        all_q_perpix = []
        all_refls_idx = []
        pixel_counter = np.zeros_like(img_data)
        self.all_nominal_hkl = []
        self.hi_asu_perpix = []
        numOutOfRange = 0
        perpixel_dark_rms = None
        if self.params.use_perpixel_dark_rms:
            perpixel_dark_rms = get_pedestalRMS_from_jungfrau(self.E)
            perpixel_dark_rms /= self.params.refiner.adu_per_photon
        if self.params.try_strong_mask_only:
            strong_mask_img = utils.strong_spot_mask(self.refls, self.E.detector)

        for i_roi in range(len(self.rois)):
            pid = self.pids[i_roi]
            x1, x2, y1, y2 = self.rois[i_roi]
            Y, X = np.indices((y2 - y1, x2 - x1))
            data = img_data[pid, y1:y2, x1:x2].copy()
            pixel_counter[pid, y1:y2, x1:x2] += 1

            data = data.ravel()
            all_background += list(background[pid, y1:y2, x1:x2].ravel())
            trusted = is_trusted[pid, y1:y2, x1:x2].ravel()
            if perpixel_dark_rms is not None:
                sigma_rdout = perpixel_dark_rms[pid, y1:y2, x1:x2].ravel()
            else:
                sigma_rdout = np.ones_like(data)*self.nominal_sigma_rdout

            if self.params.roi.mask_outside_trusted_range:
                if self.params.roi.trusted_range is not None:
                    minDat, maxDat = self.params.roi.trusted_range
                    assert minDat < maxDat
                else:
                    minDat, maxDat = self.E.detector[pid].get_trusted_range()
                data_out_of_range = np.logical_or(data <= minDat, data >= maxDat)
                if self.params.roi.mask_all_if_any_outside_trusted_range:
                    if np.any(data_out_of_range):
                        data_out_of_range[:] = True

                trusted[data_out_of_range] = False
                numOutOfRange +=np.sum(data_out_of_range)

            if self.params.mask_highest_values is not None:
                trusted[np.argsort(data)[-self.params.mask_highest_values:]] = False

            if self.params.try_strong_mask_only:
                is_strong_spot = strong_mask_img[pid, y1:y2, x1:x2].ravel()
                if self.params.dilate_strong_mask is not None:
                    assert self.params.dilate_strong_mask >= 1
                    is_strong_spot = binary_dilation(is_strong_spot, iterations=self.params.dilate_strong_mask)
                trusted = np.logical_and(trusted, is_strong_spot)

            all_trusted += list(trusted)
            #TODO ignore invalid value warning (handled below), or else mitigate it!

            with np.errstate(invalid='ignore'):
                all_sigmas += list(np.sqrt(data + sigma_rdout ** 2))

            all_sigma_rdout += list(sigma_rdout)
            all_fast += list(X.ravel() + x1)
            all_fast_relative += list(X.ravel())
            all_slow += list(Y.ravel() + y1)
            all_slow_relative += list(Y.ravel())
            all_data += list(data)
            npix = len(data)  # np.sum(trusted)
            all_pid += [pid] * npix
            roi_id += [i_roi] * npix
            all_refls_idx += [self.refls_idx[i_roi]] * npix
            if not self.no_rlp_info:
                all_q_perpix += [self.Q[i_roi]]*npix
            if self.Hi is not None:
                self.all_nominal_hkl += [tuple(self.Hi[i_roi])]*npix
                self.hi_asu_perpix += [self.Hi_asu[i_roi]] * npix
                #self.all_nominal_hkl += [tuple(self.Hi[i_roi])]*npix
                #self.hi_asu_perpix += [self.Hi_asu[i_roi]] * npix

        if self.params.roi.mask_outside_trusted_range:
            MAIN_LOGGER.debug("Found %d pixels outside of trusted range" % numOutOfRange)
        all_freq = []
        for i_roi in range(len(self.rois)):
            pid = self.pids[i_roi]
            x1, x2, y1, y2 = self.rois[i_roi]
            freq = pixel_counter[pid, y1:y2, x1:x2].ravel()
            all_freq += list(freq)
        self.all_freq = np.array(all_freq, np.int32)  # if no overlapping pixels, this should be an array of 1's
        if not self.params.roi.allow_overlapping_spots:
            if not np.all(self.all_freq==1):
                print(set(self.all_freq))
                raise ValueError("There are overlapping regions of interest, despite the command to not allow overlaps")

        self.all_q_perpix = np.array(all_q_perpix)
        pan_fast_slow = np.ascontiguousarray((np.vstack([all_pid, all_fast, all_slow]).T).ravel())
        self.pan_fast_slow = flex.size_t(pan_fast_slow)
        self.all_background = np.array(all_background)
        self.roi_id = np.array(roi_id)
        self.all_data = np.array(all_data)
        if np.allclose(all_sigma_rdout, self.nominal_sigma_rdout):
            self.all_sigma_rdout = self.nominal_sigma_rdout
        else:
            self.all_sigma_rdout = np.array(all_sigma_rdout)
        self.all_sigmas = np.array(all_sigmas)
        # note rare chance for sigmas to be nan if the args of sqrt is below 0
        self.all_trusted = np.logical_and(np.array(all_trusted), ~np.isnan(all_sigmas))

        if self.params.roi.skip_roi_with_negative_bg:
            # Dont include pixels whose background model is below 0
            self.all_trusted[self.all_background < 0] = False

        self.npix_total = len(all_data)
        self.all_fast = np.array(all_fast)
        self.all_slow = np.array(all_slow)
        self.all_pid = np.array(all_pid)
        #self.simple_weights = 1/self.all_sigmas**2
        self.u_id = set(self.roi_id)
        self.all_refls_idx = np.array(all_refls_idx)

        MAIN_LOGGER.debug("Modeler has %d/ %d trusted pixels" % (self.all_trusted.sum() , self.npix_total))

    def dump_gathered_to_refl(self, output_name, do_xyobs_sanity_check=False):
        """after running GatherFromExperiment, dump the gathered results
        (data, background etc) to a new reflection file which can then be used to run
        diffBragg without the raw data in the experiment (this exists mainly for portability, and
        unit tests)"""
        shoeboxes = []
        R = flex.reflection_table()
        for i_roi, i_ref in enumerate(self.refls_idx):
            roi_sel = self.roi_id==i_roi
            x1, x2, y1, y2 = self.rois[i_roi]
            roi_shape = y2-y1, x2-x1
            roi_img = self.all_data[roi_sel].reshape(roi_shape).astype(np.float32)  #NOTE this has already been converted to photon units
            roi_bg = self.all_background[roi_sel].reshape(roi_shape).astype(np.float32)

            sb = Shoebox((x1, x2, y1, y2, 0, 1))
            sb.allocate()
            sb.data = flex.float(np.ascontiguousarray(roi_img[None]))
            sb.background = flex.float(np.ascontiguousarray(roi_bg[None]))

            dials_mask = np.zeros(roi_img.shape).astype(np.int32)
            mask = self.all_trusted[roi_sel].reshape(roi_shape)
            dials_mask[mask] = dials_mask[mask] + MaskCode.Valid
            sb.mask = flex.int(np.ascontiguousarray(dials_mask[None]))

            # quick sanity test
            if do_xyobs_sanity_check:
                ref = self.refls[i_ref]
                x,y,_ = ref['xyzobs.px.value']
                assert x1 <= x <= x2, "exp %s; refl %d, %f %f %f" % (output_name, i_ref, x1,x,x2)
                assert y1 <= y <= y2, "exp %s; refl %d, %f %f %f" % (output_name, i_ref, y1,y,y2)

            R.extend(self.refls[i_ref: i_ref+1])
            shoeboxes.append(sb)

        R['shoebox'] = flex.shoebox(shoeboxes)
        R['id'] = flex.int(len(R), 0)
        R.as_file(output_name)

    def set_parameters_for_experiment(self, best=None):
        if self.params.symmetrize_Flatt and not self.params.fix.eta_abc:
            if not self.params.simulator.crystal.has_isotropic_mosaicity:
                raise NotImplementedError("if fix.eta_abc=False and symmetrize_Flatt=True, then eta must be isotropic. Set simulator.crystal.has_isotropic_mosaicity=True")
        ParameterTypes = {"ranged": RangedParameter, "positive": PositiveParameter}
        ParameterType = RangedParameter  # most params currently only this type

        if self.params.centers.Nvol is not None:
            assert self.params.betas.Nvol is not None

        if best is not None:
            # set the crystal Umat (rotational displacement) and Bmat (unit cell)
            # Umatrix
            # NOTE: just set the best Amatrix here
            #C = deepcopy(self.E.crystal)
            #crystal = self.E.crystal
            #self.E.crystal = crystal

            ## TODO , currently need this anyway
            ucparam = best[["a","b","c","al","be","ga"]].values[0]
            ucman = utils.manager_from_params(ucparam)
            self.E.crystal.set_B(ucman.B_recipspace)
            self.E.crystal.set_A(best.Amats.values[0])

            # mosaic block
            self.params.init.Nabc = tuple(best.ncells.values[0])
            self.params.init.Ndef = tuple(best.ncells_def.values[0])
            # scale factor
            self.params.init.G = best.spot_scales.values[0]

            if "detz_shift_mm" in list(best):
                self.params.init.detz_shift = best.detz_shift_mm.values[0]

            # TODO: set best eta_abc params
            self.params.init.eta_abc = tuple(best.eta_abc.values[0])

            lam0, lam1 = get_lam0_lam1_from_pandas(best)
            self.params.init.spec = lam0, lam1

        init = self.params.init
        sigma = self.params.sigmas
        mins = self.params.mins
        maxs = self.params.maxs
        centers = self.params.centers
        betas = self.params.betas
        fix = self.params.fix
        types = self.params.types
        P = Parameters()
        if self.params.init.random_Gs is not None:
            init.G = np.random.choice(self.params.init.random_Gs)
        for i_xtal in range(self.num_xtals):
            for ii in range(3):

                p = ParameterType(init=0, sigma=sigma.RotXYZ[ii],
                                  minval=mins.RotXYZ[ii], maxval=maxs.RotXYZ[ii],
                                  fix=fix.RotXYZ, name="RotXYZ%d_xtal%d" % (ii,i_xtal),
                                  center=0 if betas.RotXYZ is not None else None,
                                  beta=betas.RotXYZ)
                P.add(p)

            p = ParameterTypes[types.G](init=init.G + init.G*0.01*i_xtal, sigma=sigma.G,
                              minval=mins.G, maxval=maxs.G,
                              fix=fix.G, name="G_xtal%d" %i_xtal,
                              center=centers.G, beta=betas.G)
            P.add(p)


        # these parameters are equal for all texture-domains within a crystal
        fix_Nabc = [fix.Nabc]*3
        if self.params.simulator.crystal.has_isotropic_ncells:
            fix_Nabc = [fix_Nabc[0], True, True]

        fix_difsig = [fix.diffuse_sigma]*3
        if self.params.isotropic.diffuse_sigma:
            fix_difsig = [fix_difsig[0], True, True]

        fix_difgam = [fix.diffuse_gamma]*3
        if self.params.isotropic.diffuse_gamma:
            fix_difgam = [fix_difgam[0], True, True]

        if not fix.eta_abc:
            assert all([eta> 0 for eta in init.eta_abc])
        if tuple(init.eta_abc ) == (0,0,0):
            mins.eta_abc=[-1e-10,-1e-10,-1e-10]

        fix_eta = [fix.eta_abc]*3
        if self.params.simulator.crystal.has_isotropic_mosaicity:
            fix_eta = [fix_eta[0], True, True]

        if self.params.init.random_Nabcs is not None:
            init.Nabc = np.random.choice(self.params.init.random_Nabcs, replace=True, size=3)
        for ii in range(3):
            # Mosaic domain tensor
            p = ParameterTypes[types.Nabc](init=init.Nabc[ii], sigma=sigma.Nabc[ii],
                              minval=mins.Nabc[ii], maxval=maxs.Nabc[ii],
                              fix=fix_Nabc[ii], name="Nabc%d" % (ii,),
                              center=centers.Nabc[ii] if centers.Nabc is not None else None,
                              beta=betas.Nabc[ii] if betas.Nabc is not None else None)
            P.add(p)

            p = ParameterType(init=init.Ndef[ii], sigma=sigma.Ndef[ii],
                              minval=mins.Ndef[ii], maxval=maxs.Ndef[ii],
                              fix=fix.Ndef, name="Ndef%d" % (ii,),
                              center=centers.Ndef[ii] if centers.Ndef is not None else None,
                              beta=betas.Ndef[ii] if betas.Ndef is not None else None)
            P.add(p)

            # diffuse gamma and sigma
            p = ParameterTypes[types.diffuse_gamma](init=init.diffuse_gamma[ii], sigma=sigma.diffuse_gamma[ii],
                              minval=mins.diffuse_gamma[ii], maxval=maxs.diffuse_gamma[ii],
                              fix=fix_difgam[ii], name="diffuse_gamma%d" % (ii,),
                              center=centers.diffuse_gamma[ii] if centers.diffuse_gamma is not None else None,
                              beta=betas.diffuse_gamma[ii] if betas.diffuse_gamma is not None else None)
            P.add(p)

            p = ParameterTypes[types.diffuse_sigma](init=init.diffuse_sigma[ii], sigma=sigma.diffuse_sigma[ii],
                              minval=mins.diffuse_sigma[ii], maxval=maxs.diffuse_sigma[ii],
                              fix=fix_difsig[ii], name="diffuse_sigma%d" % (ii,),
                              center=centers.diffuse_sigma[ii] if centers.diffuse_sigma is not None else None,
                              beta=betas.diffuse_sigma[ii] if betas.diffuse_sigma is not None else None)
            P.add(p)

            # mosaic spread (mosaicity)
            p = ParameterType(init=init.eta_abc[ii], sigma=sigma.eta_abc[ii],
                              minval=mins.eta_abc[ii], maxval=maxs.eta_abc[ii],
                              fix=fix_eta[ii], name="eta_abc%d" % (ii,),
                              center=centers.eta_abc[ii] if centers.eta_abc is not None else None,
                              beta=betas.eta_abc[ii] if betas.eta_abc is not None else None)
            P.add(p)

        ucell_man = utils.manager_from_crystal(self.E.crystal)
        ucell_vary_perc = self.params.ucell_edge_perc / 100.
        for i_uc, (name, val) in enumerate(zip(ucell_man.variable_names, ucell_man.variables)):
            if "Ang" in name:
                minval = val - ucell_vary_perc * val
                maxval = val + ucell_vary_perc * val
                if name == 'a_Ang':
                    cent = centers.ucell_a
                    beta = betas.ucell_a
                elif name== 'b_Ang':
                    cent = centers.ucell_b
                    beta = betas.ucell_b
                else:
                    cent = centers.ucell_c
                    beta = betas.ucell_c
            else:
                val_in_deg = val * 180 / np.pi
                minval = (val_in_deg - self.params.ucell_ang_abs) * np.pi / 180.
                maxval = (val_in_deg + self.params.ucell_ang_abs) * np.pi / 180.
                if name=='alpha_rad':
                    cent = centers.ucell_alpha
                    beta = betas.ucell_alpha
                elif name=='beta_rad':
                    cent = centers.ucell_beta
                    beta = betas.ucell_beta
                else:
                    cent = centers.ucell_gamma
                    beta = betas.ucell_gamma
                if cent is not None:
                    cent = cent*np.pi / 180.

            p = ParameterType(init=val, sigma=sigma.ucell[i_uc],
                              minval=minval, maxval=maxval, fix=fix.ucell,
                              name="ucell%d" % (i_uc,),
                              center=cent,
                              beta=beta)
            MAIN_LOGGER.info(
                "Unit cell variable %s (currently=%f) is bounded by %f and %f" % (name, val, minval, maxval))
            P.add(p)

        self.ucell_man = ucell_man

        p = ParameterType(init=init.detz_shift*1e-3, sigma=sigma.detz_shift,
                          minval=mins.detz_shift*1e-3, maxval=maxs.detz_shift*1e-3,
                          fix=fix.detz_shift,name="detz_shift",
                          center=centers.detz_shift,
                          beta=betas.detz_shift)
        P.add(p)

        if not self.params.fix.perRoiScale:
            self.set_slices("roi_id")  # this creates roi_id_unique
            refls_have_scales = "scale_factor" in list(self.refls.keys())
            for roi_id in self.roi_id_unique:
                slc = self.roi_id_slices[roi_id][0]
                if refls_have_scales:
                    refl_idx = int(self.all_refls_idx[slc][0])
                    init_scale = self.refls[refl_idx]["scale_factor"]
                else:
                    init_scale = 1
                p = RangedParameter(init=init_scale, sigma=self.params.sigmas.roiPerScale,
                                  minval=0, maxval=1e12,
                                  fix=fix.perRoiScale, name="scale_roi%d" % roi_id,
                                  center=1,
                                  beta=1e12)
                if isinstance(self.all_q_perpix, np.ndarray) and self.all_q_perpix.size:
                    q = self.all_q_perpix[slc][0]
                    reso = 1./q
                    hkl = self.all_nominal_hkl[slc][0]
                    p.misc_data = reso, hkl
                P.add(p)

        # two parameters for optimizing the spectrum
        p = RangedParameter(init=self.params.init.spec[0], sigma=self.params.sigmas.spec[0],
                            minval=mins.spec[0], maxval=maxs.spec[0], fix=fix.spec,
                            name="lambda_offset", center=centers.spec[0] if centers.spec is not None else None,
                            beta=betas.spec[0] if betas.spec is not None else None)
        P.add(p)
        p = RangedParameter(init=self.params.init.spec[1], sigma=self.params.sigmas.spec[1],
                            minval=mins.spec[1], maxval=maxs.spec[1], fix=fix.spec,
                            name="lambda_scale", center=centers.spec[1] if centers.spec is not None else None,
                            beta=betas.spec[1] if betas.spec is not None else None)
        P.add(p)

        # iterating over this dict is time-consuming when refinine Fhkl, so we split up the names here:
        self.non_fhkl_params = [name for name in P if not name.startswith("scale_roi") and not name.startswith("Fhkl_")]
        self.scale_roi_names = [name for name in P if name.startswith("scale_roi")]
        self.P = P

        for name in self.P:
            p = self.P[name]
            if (p.beta is not None and p.center is None) or (p.center is not None and p.beta is None):
                raise RuntimeError("To use restraints, must specify both center and beta for param %s" % name)

    def get_data_model_pairs(self, reorder=False):
        if self.best_model is None:
            raise ValueError("cannot get the best model, there is no best_model attribute")
        all_dat_img, all_mod_img = [], []
        all_trusted = []
        all_bragg = []
        all_sigma_rdout = []
        all_d_perpix = 1/self.all_q_perpix
        all_d = []
        for i_roi in range(len(self.rois)):
            x1, x2, y1, y2 = self.rois[i_roi]
            mod = self.best_model[self.roi_id == i_roi].reshape((y2 - y1, x2 - x1))
            try:
                res = all_d_perpix[self.roi_id==i_roi] .mean()
                all_d.append(res)
            except IndexError:
                pass
            if self.all_trusted is not None:
                trusted = self.all_trusted[self.roi_id == i_roi].reshape((y2 - y1, x2 - x1))
                all_trusted.append(trusted)
            else:
                all_trusted.append(None)

            dat = self.all_data[self.roi_id == i_roi].reshape((y2 - y1, x2 - x1))
            all_dat_img.append(dat)
            if isinstance(self.all_sigma_rdout, np.ndarray):
                sig = self.all_sigma_rdout[self.roi_id==i_roi].reshape((y2-y1, x2-x1))
                all_sigma_rdout.append(sig)
            if self.all_background is not None:
                bg = self.all_background[self.roi_id == i_roi].reshape((y2-y1, x2-x1))
                if self.best_model_includes_background:
                    all_bragg.append(mod-bg)
                    all_mod_img.append(mod)
                else:
                    all_bragg.append(mod)
                    all_mod_img.append(mod+bg)
            else:  # assume mod contains background
                all_mod_img.append(mod)
                all_bragg.append(None)
        ret_subimgs = [all_dat_img, all_mod_img, all_trusted, all_bragg]
        if all_sigma_rdout:
            ret_subimgs += [all_sigma_rdout]
        if reorder:
            order = np.argsort(all_d)[::-1]
            for i in range(len(ret_subimgs)):
                imgs = ret_subimgs[i]
                imgs = [imgs[i] for i in order]
                ret_subimgs[i] = imgs

        return ret_subimgs

    def Minimize(self, x0, SIM, i_shot=0):
        self.target = target = TargetFunc(SIM=SIM, niter_per_J=self.params.niter_per_J, profile=self.params.profile)

        # set up the refinement flags
        vary = np.ones(len(x0), bool)
        if SIM.refining_Fhkl:
            assert len(x0) == len(self.P)+SIM.Num_ASU*SIM.num_Fhkl_channels
        else:
            assert len(x0) == len(self.P)
        for p in self.P.values():
            if not p.refine:
                vary[p.xpos] = False

        target.vary = vary  # fixed flags
        target.x0 = np.array(x0, np.float64)  # initial full parameter list
        x0_for_refinement = target.x0[vary]

        if self.params.method is None:
            method = "Nelder-Mead"
        else:
            method = self.params.method

        maxfev = None
        if self.params.nelder_mead_maxfev is not None:
            maxfev = self.params.nelder_mead_maxfev * self.npix_total

        at_min = None
        if self.params.logging.show_params_at_minimum:
            at_min = target.at_minimum

        if self.params.niter >0:
            assert self.params.hopper_save_freq is None
            at_min = self.at_minimum

        callback_kwargs = {"SIM":SIM, "i_shot": i_shot, "save_freq": self.params.hopper_save_freq}
        callback = lambda x: self.callback(x, callback_kwargs)
        target.terminate_after_n_converged_iterations = self.params.terminate_after_n_converged_iter
        target.percent_change_of_converged = self.params.converged_param_percent_change
        if method in ["L-BFGS-B", "BFGS", "CG", "dogleg", "SLSQP", "Newton-CG", "trust-ncg", "trust-krylov", "trust-exact", "trust-ncg"]:
            if self.P["lambda_offset"].refine:
                for lam_id in LAMBDA_IDS:
                    SIM.D.refine(lam_id)
            if self.P["RotXYZ0_xtal0"].refine:
                SIM.D.refine(ROTX_ID)
                SIM.D.refine(ROTY_ID)
                SIM.D.refine(ROTZ_ID)
            if self.P["Nabc0"].refine:
                SIM.D.refine(NCELLS_ID)
            if self.P["Ndef0"].refine:
                SIM.D.refine(NCELLS_ID_OFFDIAG)
            if self.P["ucell0"].refine:
                for i_ucell in range(len(self.ucell_man.variables)):
                    SIM.D.refine(UCELL_ID_OFFSET + i_ucell)
            if self.P["eta_abc0"].refine:
                SIM.D.refine(ETA_ID)
            if self.P["detz_shift"].refine:
                SIM.D.refine(DETZ_ID)
            if SIM.D.use_diffuse:
                SIM.D.refine(DIFFUSE_ID)

            min_kwargs = {'args': (self,SIM, True), "method": method, "jac": target.jac,
                          'hess': self.params.hess, 'callback':callback}
            if method=="L-BFGS-B":
                min_kwargs["options"] = {"ftol": self.params.ftol, "gtol": 1e-12, "maxfun":1e5,
                                         "maxiter":self.params.lbfgs_maxiter, "eps":1e-20}

        else:
            min_kwargs = {'args': (self,SIM, False), "method": method,
                          'callback': callback,
                          'options': {'maxfev': maxfev,
                                      'fatol': self.params.nelder_mead_fatol}}

        if self.params.global_method=="basinhopping":
            HOPPER = basinhopping

            try:
                out = HOPPER(target, x0_for_refinement,
                                   niter=self.params.niter,
                                   minimizer_kwargs=min_kwargs,
                                   T=self.params.temp,
                                   callback=at_min,
                                   disp=False,
                                   stepsize=self.params.stepsize)
                target.x0[vary] = out.x
            except StopIteration:
                pass

        else:
            bounds = [(-100,100)] * len(x0_for_refinement)  # TODO decide about bounds, usually x remains close to 1 during refinement
            print("Beginning the annealing process")
            args = min_kwargs.pop("args")
            if self.params.dual.no_local_search:
                compute_grads = args[-1]
                if compute_grads:
                    print("Warning, parameters setup to compute gradients, swicthing off because no_local_search=True")
                args = list(args)
                args[-1] = False  # switch off grad
                args = tuple(args)
            out = dual_annealing(target, bounds=bounds, args=args,
                                 no_local_search=self.params.dual.no_local_search,
                                 x0=x0_for_refinement,
                                 accept=self.params.dual.accept,
                                 visit=self.params.dual.visit,
                                 maxiter=self.params.niter,
                                 local_search_options=min_kwargs,
                                 callback=at_min)
            target.x0[vary] = out.x

        return target.x0

    def callback(self, x, kwargs):
        save_freq = kwargs["save_freq"]
        i_shot = kwargs["i_shot"]
        SIM = kwargs["SIM"]
        target = self.target
        if save_freq is not None and target.iteration % save_freq==0 and target.iteration> 0:
            xall = target.x0.copy()
            xall[target.vary] = x
            self.save_up(xall, SIM, rank=self.rank, i_shot=i_shot)
        return

        rescaled_vals = np.zeros_like(xall)
        all_perc_change = []
        for name in self.P:
            if name.startswith("Fhkl_"):
                continue
            p = self.P[name]

            if not p.refine:
                continue
            xpos = p.xpos
            val = p.get_val(xall[xpos])
            log_s = "Iter %d: %s = %1.2g ." % (target.iteration, name, val)
            if name.startswith("scale_roi") and p.misc_data is not None:
                reso, (h, k, l) = p.misc_data
                log_s += "reso=%1.3f Ang. (h,k,l)=%d,%d,%d ." % (reso, h, k, l)
            rescaled_vals[xpos] = val
            if target.prev_iter_vals is not None:
                prev_val = target.prev_iter_vals[xpos]
                if val - prev_val == 0 and prev_val == 0:
                    val_percent_diff = 0
                elif prev_val == 0:
                    val_percent_diff = np.abs(val - prev_val) / val * 100.
                else:
                    val_percent_diff = np.abs(val - prev_val) / prev_val * 100.
                log_s += " Percent change = %1.2f%%" % val_percent_diff
                all_perc_change.append(val_percent_diff)

            if verbose:
                MAIN_LOGGER.debug(log_s)

        if all_perc_change:
            all_perc_change = np.abs(all_perc_change)
            ave_perc_change = np.mean(all_perc_change)
            num_perc_change_small = np.sum(all_perc_change < target.percent_change_of_converged)
            max_perc_change = np.max(all_perc_change)
            if num_perc_change_small == len(all_perc_change):
                target.all_converged_params += 1
            else:
                target.all_converged_params = 0
            if verbose:
                MAIN_LOGGER.info(
                    "Iter %d: Mean percent change = %1.2f%%. Num param with %%-change < %1.2f: %d/%d. Max %%-change=%1.2f%%"
                    % (target.iteration, ave_perc_change, target.percent_change_of_converged,
                       num_perc_change_small, len(all_perc_change), max_perc_change))

        target.prev_iter_vals = rescaled_vals
        if target.terminate_after_n_converged_iterations is not None and target.all_converged_params >= target.terminate_after_n_converged_iterations:
            # at this point prev_iter_vals are the converged parameters!
            raise StopIteration()  # Refinement has reached convergence!

    def save_up(self, x, SIM, rank=0, i_shot=0,
                save_fhkl_data=True, save_modeler_file=True,
                save_refl=True,
                save_sim_info=True,
                save_traces=True,
                save_pandas=True, save_expt=True):
        """

        :param x: l-bfgs refinement parameters (reparameterized, e.g. unbounded)
        :param SIM: sim_data.SimData instance
        :param rank: MPI rank Id
        :param i_shot: shot index for this rank (assuming each rank processes more than one shot, this should increment)
        :param save_fhkl_data: whether to write mtz files
        :param save_modeler_file: whether to write the DataModeler .npy file (a pickle file)
        :param save_refl: whether to write a reflection table for this shot with updated xyzcal.px from diffBragg models
        :param save_sim_info: whether to write a text file showing the diffBragg state
        :param save_traces: whether to write a text file showing the refinement target functional and sigmaZ per iter
        :param save_pandas: whether to write a single-shot pandas dataframe containing optimized diffBragg params
        :param save_expt: whether to save a single-shot experiment file for this shot with optimized crystal model
        :return: returns the single shot pandas dataframe (whether or not it was written)
        """
        assert self.exper_name is not None
        assert self.refl_name is not None
        Modeler = self
        LOGGER = logging.getLogger("refine")
        Modeler.best_model, _ = model(x, Modeler, SIM,  compute_grad=False)
        Modeler.best_model_includes_background = False
        LOGGER.info("Optimized values for i_shot %d:" % i_shot)

        basename = os.path.splitext(os.path.basename(self.exper_name))[0]

        if save_fhkl_data and SIM.refining_Fhkl:
            fhkl_scale_dir = hopper_io.make_rank_outdir(Modeler.params.outdir, "Fhkl_scale", rank)

            # ------------
            # here we run the command add_Fhkl_gradients one more time
            # , only this time we track all the asu indices that influence the model
            # This is a special call that requires openMP to have num_threads=1 because
            # I do not not how to interact with an unordered_set in openMP
            resid = self.all_data - (self.best_model + self.all_background)  # here best model is just the Bragg portion, hence we add background
            V = self.best_model + self.all_sigma_rdout ** 2
            Gparam = self.P["G_xtal0"]
            G = Gparam.get_val(x[Gparam.xpos])
            # here we must use the CPU method
            force_cpu = SIM.D.force_cpu
            SIM.D.force_cpu = True
            MAIN_LOGGER.info("Getting Fhkl errors (forcing CPUkernel usage)... might take some time")
            Fhkl_scale_errors = SIM.D.add_Fhkl_gradients(
                self.pan_fast_slow, resid, V, self.all_trusted, self.all_freq,
                SIM.num_Fhkl_channels, G, track=True, errors=True)
            SIM.D.force_gpu = force_cpu
            # ------------

            inds = np.sort(np.array(SIM.D.Fhkl_gradient_indices))

            num_asu = len(SIM.asu_map_int)
            idx_to_asu = {idx:asu for asu,idx in SIM.asu_map_int.items()}
            all_nominal_hkl = set(self.hi_asu_perpix)
            for i_chan in range(SIM.num_Fhkl_channels):
                sel = (inds >= i_chan*num_asu) * (inds < (i_chan+1)*num_asu)
                if not np.any(sel):
                    continue
                inds_chan= inds[sel] - i_chan*num_asu
                assert np.max(inds_chan) < num_asu
                asu_hkls = []
                is_nominal_hkl = []
                scale_facs = []
                scale_vars = []
                for i_hkl in inds_chan:
                    asu = idx_to_asu[i_hkl]
                    xpos = i_hkl + i_chan*num_asu
                    #xval = x[xpos]
                    scale_fac = SIM.Fhkl_scales[xpos]
                    hessian_term = Fhkl_scale_errors[xpos]
                    with np.errstate(all='ignore'):
                        scale_var = 1/hessian_term
                    asu_hkls.append(asu)
                    scale_facs.append(scale_fac)
                    scale_vars.append(scale_var)
                    is_nominal_hkl.append(asu in all_nominal_hkl)
                scale_fname = os.path.join(fhkl_scale_dir, "%s_%s_%d_%d_channel%d_scale.npz"\
                                     % (Modeler.params.tag, basename, i_shot, self.exper_idx, i_chan))
                np.savez(scale_fname, asu_hkl=asu_hkls, scale_fac=scale_facs, scale_var=scale_vars,
                         is_nominal_hkl=is_nominal_hkl)


        # TODO: pretty formatting ?
        if Modeler.target is not None:
            # hop number, gradient descent index (resets with each new hop), target functional
            trace0, trace1, trace2 = Modeler.target.all_hop_id, Modeler.target.all_f, Modeler.target.all_sigZ
            trace_data = np.array([trace0, trace1, trace2]).T

            if save_traces:
                rank_trace_outdir = hopper_io.make_rank_outdir(Modeler.params.outdir, "traces", rank)
                trace_path = os.path.join(rank_trace_outdir, "%s_%s_%d_%d_traces.txt"
                                          % (Modeler.params.tag, basename, i_shot, self.exper_idx))
                np.savetxt(trace_path, trace_data, fmt="%s")

            Modeler.niter = len(trace0)
            Modeler.sigz = trace2[-1]

        shot_df = hopper_io.save_to_pandas(x, Modeler, SIM, self.exper_name, Modeler.params, Modeler.E, i_shot,
                                           self.refl_name, None, rank, write_expt=save_expt, write_pandas=save_pandas,
                                           exp_idx=self.exper_idx)

        if isinstance(Modeler.all_sigma_rdout, np.ndarray):
            data_subimg, model_subimg, trusted_subimg, bragg_subimg, sigma_rdout_subimg = Modeler.get_data_model_pairs()
        else:
            data_subimg, model_subimg, trusted_subimg, bragg_subimg = Modeler.get_data_model_pairs()
            sigma_rdout_subimg = None

        wavelen_subimg = []
        if SIM.D.store_ave_wavelength_image:
            bm = Modeler.best_model.copy()
            Modeler.best_model = SIM.D.ave_wavelength_image().as_numpy_array()
            Modeler.best_model_includes_background = True
            _, wavelen_subimg, _, _ = Modeler.get_data_model_pairs()
            Modeler.best_model = bm
            Modeler.best_model_includes_background = False

        if save_refl:
            rank_refls_outdir = hopper_io.make_rank_outdir(Modeler.params.outdir, "refls", rank)
            new_refls_file = os.path.join(rank_refls_outdir, "%s_%s_%d_%d.refl"
                                          % (Modeler.params.tag, basename, i_shot, self.exper_idx))
            new_refls = deepcopy(Modeler.refls)
            has_xyzcal = 'xyzcal.px' in list(new_refls.keys())
            if has_xyzcal:
                new_refls['dials.xyzcal.px'] = deepcopy(new_refls['xyzcal.px'])
            per_refl_scales = flex.double(len(new_refls), 1)
            new_xycalcs = flex.vec3_double(len(Modeler.refls), (np.nan, np.nan, np.nan))
            sigmaZs = []
            for i_roi in range(len(data_subimg)):
                dat = data_subimg[i_roi]
                fit = model_subimg[i_roi]
                trust = trusted_subimg[i_roi]
                if sigma_rdout_subimg is not None:
                    sig = np.sqrt(fit + sigma_rdout_subimg[i_roi] ** 2)
                else:
                    sig = np.sqrt(fit + Modeler.nominal_sigma_rdout ** 2)
                Z = (dat - fit) / sig
                sigmaZ = np.nan
                if np.any(trust):
                    sigmaZ = Z[trust].std()

                sigmaZs.append(sigmaZ)
                if bragg_subimg[0] is not None:
                    if np.any(bragg_subimg[i_roi] > 0):
                        ref_idx = Modeler.refls_idx[i_roi]
                        ref = Modeler.refls[ref_idx]
                        I = bragg_subimg[i_roi]
                        Y, X = np.indices(bragg_subimg[i_roi].shape)
                        x1, x2, y1, y2 = Modeler.rois[i_roi]
                        com_x, com_y, _ = ref["xyzobs.px.value"]
                        com_x = int(com_x - x1 - 0.5)
                        com_y = int(com_y - y1 - 0.5)
                        # make sure at least some signal is at the centroid! otherwise this is likely a neighboring spot
                        try:
                            if I[com_y, com_x] == 0:
                                continue
                        except IndexError:
                            continue
                        X += x1
                        Y += y1
                        Isum = I.sum()
                        xcom = (X * I).sum() / Isum
                        ycom = (Y * I).sum() / Isum
                        com = xcom + .5, ycom + .5, 0
                        new_xycalcs[ref_idx] = com
                        if not Modeler.params.fix.perRoiScale:
                            scale_p = Modeler.P["scale_roi%d" % i_roi]
                            per_refl_scales[ref_idx] = scale_p.get_val(x[scale_p.xpos])

            new_refls["xyzcal.px"] = new_xycalcs
            if not Modeler.params.fix.perRoiScale:
                new_refls["scale_factor"] = per_refl_scales
            if Modeler.params.filter_unpredicted_refls_in_output:
                sel = [not np.isnan(x) for x, y, z in new_xycalcs]
                new_refls = new_refls.select(flex.bool(sel))
            new_refls.as_file(new_refls_file)

        if save_modeler_file:
            rank_imgs_outdir = hopper_io.make_rank_outdir(Modeler.params.outdir, "imgs", rank)
            modeler_file = os.path.join(rank_imgs_outdir,
                                        "%s_%s_%d_%d_modeler.npy"
                                        % (Modeler.params.tag, basename, i_shot, self.exper_idx))
            np.save(modeler_file, Modeler)
        if save_sim_info:
            spectrum_file = os.path.join(rank_imgs_outdir,
                                         "%s_%s_%d_%d_spectra.lam"
                                         % (Modeler.params.tag, basename, i_shot, self.exper_idx))
            rank_SIMlog_outdir = hopper_io.make_rank_outdir(Modeler.params.outdir, "simulator_state", rank)
            SIMlog_path = os.path.join(rank_SIMlog_outdir, "%s_%s_%d_%d.txt"
                                       % (Modeler.params.tag, basename, i_shot, self.exper_idx))
            write_SIM_logs(SIM, log=SIMlog_path, lam=spectrum_file)

        if Modeler.params.refiner.debug_pixel_panelfastslow is not None:
            # TODO separate diffBragg logger
            utils.show_diffBragg_state(SIM.D, Modeler.params.refiner.debug_pixel_panelfastslow)

        return shot_df


def convolve_model_with_psf(model_pix, J, mod, SIM, PSF=None, psf_args=None,
        roi_id_slices=None, roi_id_unique=None):
    if not SIM.use_psf:
        return model_pix, J
    if PSF is None:
        PSF = SIM.PSF
        assert PSF is not None
    if psf_args is None:
        psf_args = SIM.psf_args
        assert psf_args is not None
    if roi_id_slices is None:
        roi_id_slices = SIM.roi_id_slices
    if roi_id_unique is None:
        roi_id_unique = SIM.roi_id_unique

    coords = mod.pan_fast_slow.as_numpy_array()
    pid = coords[0::3]
    fid = coords[1::3]
    sid = coords[2::3]

    ref_xpos = []  # Jacobian index (J[xpos]) for the refined  parameters that arent roi scale factors
    if J is not None:
        for name in mod.P:
            if name.startswith("scale_roi") or name.startswith("Fhkl_"):
                continue
            p = mod.P[name]
            if p.refine:
                ref_xpos.append( p.xpos)

    for i in roi_id_unique:
        roi_p = mod.P["scale_roi%d" % i]
        for slc in roi_id_slices[i]:
            pvals = pid[slc]
            fvals = fid[slc]
            svals = sid[slc]
            f0 = fvals.min()
            s0 = svals.min()
            f1 = fvals.max()
            s1 = svals.max()
            fdim = int(f1-f0+1)
            sdim = int(s1-s0+1)
            img = model_pix[slc].reshape((sdim, fdim))
            img = psf.convolve_with_psf(img, psf=PSF, **psf_args)
            model_pix[slc] = img.ravel()
            if roi_p.refine and J is not None:
                deriv_img = J[roi_p.xpos, slc].reshape((sdim, fdim))
                deriv_img = psf.convolve_with_psf(deriv_img, psf=PSF, **psf_args)
                J[roi_p.xpos, slc] = deriv_img.ravel()

            for xpos in ref_xpos: # if J is None, then ref_xpos should be empty!
                deriv_img = J[xpos, slc].reshape((sdim, fdim))
                deriv_img = psf.convolve_with_psf(deriv_img, psf=PSF, **psf_args)
                J[xpos, slc] = deriv_img.ravel()

    return model_pix, J


def model(x, Mod, SIM,  compute_grad=True, dont_rescale_gradient=False, update_spectrum=False,
          update_Fhkl_scales=True):

    if Mod.params.logging.parameters:
        val_s = ""
        for p in Mod.P.values():
            if p.name.startswith("Fhkl_"):
                continue
            if p.refine:
                xval = x[p.xpos]
                val = p.get_val(xval)
                name = p.name
                if name == "detz_shift":
                    val = val * 1e3
                    name = p.name + "_mm"
                val_s += "%s=%.3f, " % (name, val)
        MAIN_LOGGER.debug(val_s)


    pfs = Mod.pan_fast_slow

    if update_spectrum:
        # update the photon energy spectrum for this shot
        SIM.beam.spectrum = Mod.nanoBragg_beam_spectrum
        SIM.D.xray_beams = SIM.beam.xray_beams
        # update Fhkl channels
        if Mod.Fhkl_channel_ids is not None:
            SIM.D.update_Fhkl_channels(Mod.Fhkl_channel_ids)

    if SIM.refining_Fhkl and update_Fhkl_scales:  # once per iteration
        nscales = SIM.Num_ASU*SIM.num_Fhkl_channels
        current_Fhkl_xvals = x[-nscales:]
        SIM.Fhkl_scales = SIM.Fhkl_scales_init * np.exp( Mod.params.sigmas.Fhkl *(current_Fhkl_xvals-1))
        SIM.D.update_Fhkl_scale_factors(SIM.Fhkl_scales, SIM.num_Fhkl_channels)

    # get the unit cell variables
    nucell = len(Mod.ucell_man.variables)
    ucell_params = [Mod.P["ucell%d" % i_uc] for i_uc in range(nucell)]
    ucell_xpos = [p.xpos for p in ucell_params]
    unitcell_var_reparam = [x[xpos] for xpos in ucell_xpos]
    unitcell_variables = [ucell_params[i].get_val(xval) for i, xval in enumerate(unitcell_var_reparam)]
    Mod.ucell_man.variables = unitcell_variables
    Bmatrix = Mod.ucell_man.B_recipspace
    SIM.D.Bmatrix = Bmatrix
    if compute_grad:
        for i_ucell in range(len(unitcell_variables)):
            SIM.D.set_ucell_derivative_matrix(
                i_ucell + UCELL_ID_OFFSET,
                Mod.ucell_man.derivative_matrices[i_ucell])

    # update the mosaicity here
    eta_params = [Mod.P["eta_abc%d" % i_eta] for i_eta in range(3)]
    if SIM.umat_maker is not None:
        # we are modeling mosaic spread
        eta_abc = [p.get_val(x[p.xpos]) for p in eta_params]
        if not SIM.D.has_anisotropic_mosaic_spread:
            eta_abc = eta_abc[0]
        SIM.update_umats_for_refinement(eta_abc)

#   detector parameters
    DetZ = Mod.P["detz_shift"]
    x_shiftZ = x[DetZ.xpos]
    shiftZ = DetZ.get_val(x_shiftZ)
    SIM.D.shift_origin_z(SIM.detector, shiftZ)

    if Mod.P["lambda_offset"].refine:
        p0 = Mod.P["lambda_offset"]
        p1 = Mod.P["lambda_scale"]
        lambda_coef = p0.get_val(x[p0.xpos]), p1.get_val(x[p1.xpos])
        SIM.D.lambda_coefficients = lambda_coef

    # Mosaic block
    Nabc_params = [Mod.P["Nabc%d" % (i_n,)] for i_n in range(3)]
    Na, Nb, Nc = [n_param.get_val(x[n_param.xpos]) for n_param in Nabc_params]
    if SIM.D.isotropic_ncells:
        Nb = Na
        Nc = Na
    SIM.D.set_ncells_values(tuple([Na, Nb, Nc]))

    Ndef_params = [Mod.P["Ndef%d" % (i_n,)] for i_n in range(3)]
    Nd, Ne, Nf = [n_param.get_val(x[n_param.xpos]) for n_param in Ndef_params]
    if SIM.D.isotropic_ncells:
        Ne = Nd
        Nf = Nd
    SIM.D.Ncells_def = Nd, Ne, Nf

    # diffuse signals
    if SIM.D.use_diffuse:
        diffuse_params_lookup = {}
        iso_flags = {'gamma':SIM.isotropic_diffuse_gamma, 'sigma':SIM.isotropic_diffuse_sigma}
        for diff_type in ['gamma', 'sigma']:
            diff_params = [Mod.P["diffuse_%s%d" % (diff_type,i_gam)] for i_gam in range(3)]
            diffuse_params_lookup[diff_type] = diff_params
            diff_vals = []
            for i_diff, param in enumerate(diff_params):
                val = param.get_val(x[param.xpos])
                if iso_flags[diff_type]:
                    diff_vals = [val]*3
                    break
                else:
                    diff_vals.append(val)
            if diff_type == "gamma":
                SIM.D.diffuse_gamma = tuple(diff_vals)
            else:
                SIM.D.diffuse_sigma = tuple(diff_vals)

    npix = int(len(pfs) / 3)
    nparam = len(x)
    J = None
    if compute_grad:
        # This should be all params save the Fhkl params
        J = np.zeros((nparam-SIM.Num_ASU*SIM.num_Fhkl_channels, npix))  # gradients

    model_pix = None
    #TODO check roiScales mode and if its broken, git rid of it!
    model_pix_noRoi = None

    # extract the scale factors per ROI, these might correspond to structure factor intensity scale factors, and quite possibly might result in overfits!
    roiScalesPerPix = 1
    if not Mod.params.fix.perRoiScale:
        perRoiParams = [Mod.P["scale_roi%d" % roi_id] for roi_id in Mod.roi_id_unique]
        perRoiScaleFactors = [p.get_val(x[p.xpos]) for p in perRoiParams]
        roiScalesPerPix = np.zeros(npix)
        for i_roi, roi_id in enumerate(Mod.roi_id_unique):
            slc = Mod.roi_id_slices[roi_id][0]
            roiScalesPerPix[slc] = perRoiScaleFactors[i_roi]

    for i_xtal in range(Mod.num_xtals):

        if hasattr(Mod, "Umatrices"):  # reflects new change for modeling multi-crystal experiments
            SIM.D.Umatrix = Mod.Umatrices[i_xtal]

        RotXYZ_params = [Mod.P["RotXYZ%d_xtal%d" % (i_rot, i_xtal)] for i_rot in range(3)]
        rotX,rotY,rotZ = [rot_param.get_val(x[rot_param.xpos]) for rot_param in RotXYZ_params]

        ## update parameters:
        # TODO: if not refining Umat, assert these are 0 , and dont set them here
        SIM.D.set_value(ROTX_ID, rotX)
        SIM.D.set_value(ROTY_ID, rotY)
        SIM.D.set_value(ROTZ_ID, rotZ)

        if Mod.params.symmetrize_Flatt:
            RXYZU = hopper_io.diffBragg_Umat(rotX, rotY, rotZ, SIM.D.Umatrix)
            Cryst = deepcopy(SIM.crystal.dxtbx_crystal)
            A = RXYZU * Mod.ucell_man.B_realspace
            A_recip = A.inverse().transpose()
            Cryst.set_A(A_recip)
            symbol = SIM.crystal.space_group_info.type().lookup_symbol()
            SIM.D.set_mosaic_blocks_sym(Cryst, symbol , Mod.params.simulator.crystal.num_mosaicity_samples,
                                        refining_eta=not Mod.params.fix.eta_abc)

        G = Mod.P["G_xtal%d" % i_xtal]
        scale = G.get_val(x[G.xpos])

        SIM.D.add_diffBragg_spots(pfs)

        pix_noRoiScale = SIM.D.raw_pixels_roi[:npix]
        pix_noRoiScale = pix_noRoiScale.as_numpy_array()

        pix = pix_noRoiScale * roiScalesPerPix

        if model_pix is None:
            model_pix = scale*pix
            model_pix_noRoi = scale*pix_noRoiScale
        else:
            model_pix += scale*pix
            model_pix_noRoi += scale*pix_noRoiScale

        if compute_grad:
            if G.refine:
                scale_grad = pix  # TODO double check multi crystal case
                scale_grad = G.get_deriv(x[G.xpos], scale_grad)
                J[G.xpos] += scale_grad

            if RotXYZ_params[0].refine:
                for i_rot in range(3):
                    rot_grad = scale * SIM.D.get_derivative_pixels(ROTXYZ_IDS[i_rot]).as_numpy_array()[:npix]
                    rot_p = RotXYZ_params[i_rot]
                    rot_grad = rot_p.get_deriv(x[rot_p.xpos], rot_grad)
                    J[rot_p.xpos] += rot_grad

            if Nabc_params[0].refine:
                Nabc_grads = SIM.D.get_ncells_derivative_pixels()
                for i_n in range(3):
                    N_grad = scale*(Nabc_grads[i_n][:npix].as_numpy_array())
                    p = Nabc_params[i_n]
                    N_grad = p.get_deriv(x[p.xpos], N_grad)
                    J[p.xpos] += N_grad
                    if SIM.D.isotropic_ncells:
                        break

            if Ndef_params[0].refine:
                Ndef_grads = SIM.D.get_ncells_def_derivative_pixels()
                for i_n in range(3):
                    N_grad = scale * (Ndef_grads[i_n][:npix].as_numpy_array())
                    p = Ndef_params[i_n]
                    N_grad = p.get_deriv(x[p.xpos], N_grad)
                    J[p.xpos] += N_grad

            if SIM.D.use_diffuse:
                for t in ['gamma','sigma']:
                    diffuse_grads = getattr(SIM.D, "get_diffuse_%s_derivative_pixels" % t)()
                    if diffuse_params_lookup[t][0].refine:
                        for i_diff in range(3):
                            diff_grad = scale*(diffuse_grads[i_diff][:npix].as_numpy_array())
                            p = diffuse_params_lookup[t][i_diff]
                            diff_grad = p.get_deriv(x[p.xpos], diff_grad)
                            J[p.xpos] += diff_grad

            if eta_params[0].refine:
                if SIM.D.has_anisotropic_mosaic_spread:
                    eta_derivs = SIM.D.get_aniso_eta_deriv_pixels()
                else:
                    eta_derivs = [SIM.D.get_derivative_pixels(ETA_ID)]
                num_eta = 3 if SIM.D.has_anisotropic_mosaic_spread else 1
                for i_eta in range(num_eta):
                    p = eta_params[i_eta]
                    eta_grad = scale * (eta_derivs[i_eta][:npix].as_numpy_array())
                    eta_grad = p.get_deriv(x[p.xpos], eta_grad)
                    J[p.xpos] += eta_grad

            if ucell_params[0].refine:
                for i_ucell in range(nucell):
                    p = ucell_params[i_ucell]
                    deriv = scale*SIM.D.get_derivative_pixels(UCELL_ID_OFFSET+i_ucell).as_numpy_array()[:npix]
                    deriv = p.get_deriv(x[p.xpos], deriv)
                    J[p.xpos] += deriv

            if DetZ.refine:
                d = SIM.D.get_derivative_pixels(DETZ_ID).as_numpy_array()[:npix]
                d = DetZ.get_deriv(x[DetZ.xpos], d)
                J[DetZ.xpos] += d

            if Mod.P["lambda_offset"].refine:
                lambda_derivs = SIM.D.get_lambda_derivative_pixels()
                lambda_param_names = "lambda_offset", "lambda_scale"
                for d,name in zip(lambda_derivs, lambda_param_names):
                    p = Mod.P[name]
                    d = d.as_numpy_array()[:npix]
                    d = p.get_deriv(x[p.xpos], d)
                    J[p.xpos] += d

    if not Mod.params.fix.perRoiScale and compute_grad:
        if compute_grad:
            for p in perRoiParams:
                roi_id = int(p.name.split("scale_roi")[1])
                slc = Mod.roi_id_slices[roi_id][0]
                if dont_rescale_gradient:
                    d = model_pix_noRoi[slc]
                else:
                    d = p.get_deriv(x[p.xpos], model_pix_noRoi[slc])
                J[p.xpos, slc] += d


    return model_pix, J


def look_at_x(x, Mod):
    for name, p in Mod.P.items():
        if name.startswith("scale_roi") and not p.refine:
            continue
        if name.startswith("Fhkl_"):
            continue
        val = p.get_val(x[p.xpos])
        print("%s: %f" % (name, val))


def get_param_from_x(x, Mod, i_xtal=0, as_dict=False):
    G = Mod.P['G_xtal%d' %i_xtal]
    scale = G.get_val(x[G.xpos])

    RotXYZ = [Mod.P["RotXYZ%d_xtal%d" % (i, i_xtal)] for i in range(3)]
    rotX, rotY, rotZ = [r.get_val(x[r.xpos]) for r in RotXYZ]

    Nabc = [Mod.P["Nabc%d" % (i, )] for i in range(3)]
    Na, Nb, Nc = [p.get_val(x[p.xpos]) for p in Nabc]

    Ndef = [Mod.P["Ndef%d" % (i, )] for i in range(3)]
    Nd, Ne, Nf = [p.get_val(x[p.xpos]) for p in Ndef]

    diff_gam_abc = [Mod.P["diffuse_gamma%d" % i] for i in range(3)]
    diff_gam_a, diff_gam_b, diff_gam_c = [p.get_val(x[p.xpos]) for p in diff_gam_abc]

    diff_sig_abc = [Mod.P["diffuse_sigma%d" % i] for i in range(3)]
    diff_sig_a, diff_sig_b, diff_sig_c = [p.get_val(x[p.xpos]) for p in diff_sig_abc]

    nucell = len(Mod.ucell_man.variables)
    ucell_p = [Mod.P["ucell%d" % i] for i in range(nucell)]
    ucell_var = [p.get_val(x[p.xpos]) for p in ucell_p]
    Mod.ucell_man.variables = ucell_var
    a,b,c,al,be,ga = Mod.ucell_man.unit_cell_parameters

    DetZ = Mod.P["detz_shift"]
    detz = DetZ.get_val(x[DetZ.xpos])

    if as_dict:
        vals = scale, rotX, rotY, rotZ, Na, Nb, Nc, Nd, Ne, Nf, diff_gam_a, diff_gam_b, diff_gam_c, diff_sig_a, diff_sig_b, diff_sig_c, a,b,c,al,be,ga, detz
        keys = 'scale', 'rotX', 'rotY', 'rotZ', 'Na', 'Nb', 'Nc', 'Nd', 'Ne', 'Nf', 'diff_gam_a', 'diff_gam_b', 'diff_gam_c', 'diff_sig_a', 'diff_sig_bvals = f_sig_c', 'a','b','c','al','be','ga', 'detz'
        param_dict = dict(zip(keys, vals))
        return param_dict
    else:
        return scale, rotX, rotY, rotZ, Na, Nb, Nc, Nd, Ne, Nf, diff_gam_a, diff_gam_b, diff_gam_c, diff_sig_a, diff_sig_b, diff_sig_c, a,b,c,al,be,ga, detz


class TargetFunc:
    def __init__(self, SIM, niter_per_J=1, profile=False):
        self.t_per_iter = []
        self.niter_per_J = niter_per_J
        self.prev_iter_vals = None
        self.global_x = []
        self.percent_change_of_converged = 0.1
        self.all_x = []
        self.terminate_after_n_converged_iterations = None
        self.vary = None #boolean numpy array specifying which params to refine
        self.x0 = None  # 1d array of parameters (should be numpy array, same length as vary)
        self.old_J = None
        self.old_model = None
        self.delta_x = None
        self.iteration = 0
        self.minima = []
        self.all_converged_params = 0
        self.SIM = SIM
        self.all_f = []  # store the target functionals here, 1 per iteration
        self.all_sigZ = []  # store the overall z-score sigmas here, 1 per iteration
        self.all_hop_id = []
        self.hop_iter = 0
        self.lowest_x = None
        self.lowest_f = np.inf

    def at_minimum(self, x, f, accept):
        self.iteration = 0
        self.all_x = []
        self.x0[self.vary] = x
        #look_at_x(self.x0,self)
        self.hop_iter += 1
        self.minima.append((f,self.x0,accept))
        self.lowest_x = x

    def jac(self, x, *args):
        if self.g is not None:
            return self.g[self.vary]

    def __call__(self, x, *args, **kwargs):
        self.x0[self.vary] = x
        if self.all_x:
            self.delta_x = self.x0 - self.all_x[-1]
        update_terms = None
        if not self.iteration % (self.niter_per_J) == 0:
            update_terms = (self.delta_x, self.old_J, self.old_model)
        self.all_x.append(self.x0)

        mod, SIM, compute_grad = args
        f, g, modelpix, J, sigZ, debug_s, zscore_perpix = target_func(self.x0, update_terms, mod, SIM, compute_grad,
                                                                      return_all_zscores=True)
        mod.all_zscore = zscore_perpix

        # filter during refinement?
        if mod.params.filter_during_refinement.enable and self.iteration > 0:
            if self.iteration % mod.params.filter_during_refinement.after_n == 0:
                mod.filter_pixels(thresh=mod.params.filter_during_refinement.threshold)


        self.t_per_iter.append(time.time())
        if len(self.t_per_iter) > 2:
            ave_t_per_it = np.mean([t2-t1 for t2,t1 in zip(self.t_per_iter[1:], self.t_per_iter[:-1])])
        else:
            ave_t_per_it = 0

        debug_s = "Hop=%d |it=%d | t/it=%.4fs" % (self.hop_iter, self.iteration, ave_t_per_it) + debug_s
        MAIN_LOGGER.debug(debug_s)
        self.all_f.append(f)
        self.all_sigZ.append(sigZ)
        self.all_hop_id.append(self.hop_iter)
        self.old_model = modelpix
        self.old_J = J
        self.iteration += 1
        self.g = g
        return f


def target_func(x, udpate_terms, mod, SIM, compute_grad=True, return_all_zscores=False):
    pfs = mod.pan_fast_slow
    data = mod.all_data
    sigma_rdout = mod.all_sigma_rdout
    trusted = mod.all_trusted
    background = mod.all_background
    params = mod.params
    if udpate_terms is not None:
        # if approximating the gradients, then fix the parameter refinment managers in diffBragg
        # so we dont waste time computing them
        _compute_grad = False
        SIM.D.fix(NCELLS_ID)
        SIM.D.fix(ROTX_ID)
        SIM.D.fix(ROTY_ID)
        SIM.D.fix(ROTZ_ID)
        for lam_id in LAMBDA_IDS:
            SIM.D.fix(lam_id)
        for i_ucell in range(len(mod.ucell_man.variables)):
            SIM.D.fix(UCELL_ID_OFFSET + i_ucell)
        SIM.D.fix(DETZ_ID)
        SIM.D.fix(ETA_ID)
        SIM.D.fix(DIFFUSE_ID)
    elif compute_grad:
        # actually compute the gradients
        _compute_grad = True
        if mod.P["Nabc0"].refine:
            SIM.D.let_loose(NCELLS_ID)
        if mod.P["RotXYZ0_xtal0"].refine:
            SIM.D.let_loose(ROTX_ID)
            SIM.D.let_loose(ROTY_ID)
            SIM.D.let_loose(ROTZ_ID)
        if mod.P["ucell0"].refine:
            for i_ucell in range(len(mod.ucell_man.variables)):
                SIM.D.let_loose(UCELL_ID_OFFSET + i_ucell)
        if mod.P["detz_shift"].refine:
            SIM.D.let_loose(DETZ_ID)
        if mod.P["eta_abc0"].refine:
            SIM.D.let_loose(ETA_ID)
        if mod.P["lambda_offset"].refine:
            for lam_id in LAMBDA_IDS:
                SIM.D.let_loose(lam_id)
    else:
        _compute_grad = False
    model_bragg, Jac = model(x, mod, SIM, compute_grad=_compute_grad)

    if udpate_terms is not None:
        # try a Broyden update ?
        # https://people.duke.edu/~hpgavin/ce281/lm.pdf  equation 19
        delta_x, prev_J, prev_model_bragg = udpate_terms
        if prev_J is not None:
            delta_y = model_bragg - prev_model_bragg

            delta_J = (delta_y - np.dot(prev_J.T, delta_x))
            delta_J /= np.dot(delta_x,delta_x)
            Jac = prev_J + delta_J
    # Jac has shape of num_param x num_pix

    model_pix = model_bragg + background

    if SIM.use_psf:
        model_pix, J = convolve_model_with_psf(model_pix, Jac, mod, SIM)

    resid = data - model_pix

    # data contributions to target function
    V = model_pix + sigma_rdout**2
    # TODO:what if V is allowed to be negative? The logarithm/sqrt will explore below
    resid_square = resid**2
    fLogLike = (.5*(np.log(2*np.pi*V) + resid_square / V))
    if params.roi.allow_overlapping_spots:
        fLogLike /= mod.all_freq
    fLogLike = fLogLike[trusted].sum()   # negative log Likelihood target

    # width of z-score should decrease as refinement proceeds
    zscore_per = resid/np.sqrt(V)
    zscore_sigma = np.std(zscore_per[trusted])

    restraint_terms = {}
    if params.use_restraints:
        # scale factor restraint
        for name in mod.non_fhkl_params:
            p = mod.P[name]
            if p.beta is not None:
                val = p.get_restraint_val(x[p.xpos])
                restraint_terms[name] = val

        if params.centers.Nvol is not None:
            na,nb,nc = SIM.D.Ncells_abc_aniso
            nd,ne,nf = SIM.D.Ncells_def
            Nmat = [na, nd, nf,
                    nd, nb, ne,
                    nf, ne, nc]
            Nmat = np.reshape(Nmat, (3,3))
            Nvol = np.linalg.det(Nmat)
            del_Nvol = params.centers.Nvol - Nvol
            fN_vol = .5*del_Nvol**2/params.betas.Nvol
            restraint_terms["Nvol"] = fN_vol
    if params.betas.Fhkl is not None:  # (experimental)
        fhkl_grad_channels = {}
        for i_chan in range(SIM.num_Fhkl_channels):
            fhkl_restraint_f, fhkl_restraint_grad = SIM.D.Fhkl_restraint_data(i_chan, params.betas.Fhkl, params.use_geometric_mean_Fhkl)
            fhkl_grad_channels[i_chan] = fhkl_restraint_grad
            restraint_terms["Fhkl_chan%d"% i_chan] = fhkl_restraint_f

#   accumulate target function
    f_restraints = 0
    if restraint_terms:
        f_restraints = np.sum(list(restraint_terms.values()))
    f = f_restraints + fLogLike

    restraint_debug_s = "LogLike: %.1f%%; " % (fLogLike / f *100.)
    for name, val in restraint_terms.items():
        if val > 0:
            frac_total = val / f *100.
            restraint_debug_s += "%s: %.2f%%; " % (name, frac_total)

    # fractions of the target function
    g = None  # gradient vector
    gnorm = -1  # norm of gradient vector
    if compute_grad:
        common_grad_term_all = (0.5 /V * (1-2*resid - resid_square / V))
        if params.roi.allow_overlapping_spots:
            common_grad_term_all /= mod.all_freq
        common_grad_term = common_grad_term_all[trusted]

        g = np.zeros(Jac.shape[0])
        for name in mod.non_fhkl_params:
            p = mod.P[name]
            Jac_p = Jac[p.xpos]
            g[p.xpos] += (Jac_p[trusted] * common_grad_term).sum()

        if not params.fix.perRoiScale:
            for name in mod.scale_roi_names:
                p = mod.P[name]
                Jac_p = Jac[p.xpos]
                roi_id = int(p.name.split("scale_roi")[1])
                slc = SIM.roi_id_slices[roi_id][0]
                common_grad_slc = common_grad_term_all[slc]
                Jac_slc = Jac_p[slc]
                trusted_slc = trusted[slc]
                g[p.xpos] += (Jac_slc * common_grad_slc)[trusted_slc].sum()

        # trusted pixels portion of Jacobian
        #  TODO: determine if this following method of summing over g is optimal in certain scenarios
        #Jac_t = Jac[:,trusted]
        # gradient vector
        #g = np.array([np.sum(common_grad_term*Jac_t[param_idx]) for param_idx in range(Jac_t.shape[0])])

        if params.use_restraints:
            # update gradients according to restraints
            for name in mod.non_fhkl_params:
                p = mod.P[name]
                if p.beta is not None:
                    g[p.xpos] += p.get_restraint_deriv(x[p.xpos])

            if not params.fix.perRoiScale:  # deprecated ?
                for name in mod.scale_roi_names:
                    p = mod.P[name]
                    if p.beta is not None:
                        g[p.xpos] += p.get_restraint_deriv(x[p.xpos])

            if params.betas.Nvol is not None:
                Nmat_inv = np.linalg.inv(Nmat)
                dVol_dN_vals = []
                for i_N in range(6):
                    if i_N ==0 :
                        dN = [1,0,0,
                              0,0,0,
                              0,0,0]
                    elif i_N == 1:
                        dN = [0,0,0,
                              0,1,0,
                              0,0,0]
                    elif i_N == 2:
                        dN = [0,0,0,
                              0,0,0,
                              0,0,1]
                    elif i_N == 3:
                        dN = [0,1,0,
                              1,0,0,
                              0,0,0]
                    elif i_N == 4:
                        dN = [0,0,0,
                              0,0,1,
                              0,1,0]
                    else:
                        dN = [0,0,1,
                              0,0,0,
                              1,0,0]
                    if i_N < 3:
                        p = mod.P["Nabc%d" % i_N]
                    else:
                        p = mod.P["Ndef%d" % (i_N-3)]
                    dN = np.reshape(dN, (3,3))
                    dVol_dN = Nvol * np.trace(np.dot(Nmat_inv, dN))
                    dVol_dN_vals.append( dVol_dN)
                    gterm = -del_Nvol / params.betas.Nvol * dVol_dN
                    g[p.xpos] += p.get_deriv(x[p.xpos], gterm)

        if SIM.refining_Fhkl:
            spot_scale_p = mod.P["G_xtal0"]
            G = spot_scale_p.get_val(x[spot_scale_p.xpos])
            fhkl_grad = SIM.D.add_Fhkl_gradients(pfs, resid, V, trusted,
                                                 mod.all_freq, SIM.num_Fhkl_channels, G)

            if params.betas.Fhkl is not None:
                for i_chan in range(SIM.num_Fhkl_channels):
                    restraint_contribution_to_grad = fhkl_grad_channels[i_chan]
                    fhkl_slice = slice(i_chan*SIM.Num_ASU, (i_chan+1)*SIM.Num_ASU, 1)
                    np.add.at(fhkl_grad, fhkl_slice, restraint_contribution_to_grad)

            fhkl_grad *= SIM.Fhkl_scales*params.sigmas.Fhkl  # sigma is always 1 for now..

            g = np.append(g, fhkl_grad)


        gnorm = np.linalg.norm(g)


    debug_s = "F=%10.7g sigZ=%10.7g (Fracs of F: %s), |g|=%10.7g" \
              % (f, zscore_sigma, restraint_debug_s, gnorm)

    return_data = f, g, model_bragg, Jac, zscore_sigma, debug_s
    if return_all_zscores:
        return_data += (zscore_per,)
    return return_data


def refine(exp, ref, params, spec=None, gpu_device=None, return_modeler=False, best=None, free_mem=True):
    if gpu_device is None:
        gpu_device = 0
    params.simulator.spectrum.filename = spec
    Modeler = DataModeler(params)
    if params.load_data_from_refls:
        Modeler.GatherFromReflectionTable(exp, ref, sg_symbol=params.space_group)
    else:
        assert Modeler.GatherFromExperiment(exp, ref, sg_symbol=params.space_group)

    SIM = get_simulator_for_data_modelers(Modeler)
    Modeler.set_parameters_for_experiment(best=best)
    SIM.D.device_Id = gpu_device

    nparam = len(Modeler.P)
    if SIM.refining_Fhkl:
        nparam += SIM.Num_ASU*SIM.num_Fhkl_channels
    x0 = [1] * nparam

    x = Modeler.Minimize(x0, SIM)
    Modeler.best_model, _ = model(x, Modeler, SIM, compute_grad=False)
    Modeler.best_model_includes_background = False

    new_crystal = update_crystal_from_x(Modeler, SIM, x)
    new_exp = deepcopy(Modeler.E)
    new_exp.crystal = new_crystal

    try:
        new_exp.beam.set_wavelength(SIM.dxtbx_spec.get_weighted_wavelength())
    except Exception: pass
    # if we strip the thickness from the detector, then update it here:
    #new_exp.detector. shift Z mm
    new_det = update_detector_from_x(Modeler, SIM, x)
    new_exp.detector = new_det

    new_refl = get_new_xycalcs(Modeler, new_exp)

    if free_mem:
        Modeler.clean_up(SIM)

    if return_modeler:
        return new_exp, new_refl, Modeler, SIM, x

    else:
        return new_exp, new_refl


def update_detector_from_x(Mod, SIM, x):
    scale, rotX, rotY, rotZ, Na, Nb, Nc, _,_,_,_,_,_,_,_,_,a, b, c, al, be, ga, detz_shift = get_param_from_x(x, Mod)
    detz_shift_mm = detz_shift*1e3
    det = SIM.detector
    det = utils.shift_panelZ(det, detz_shift_mm)
    return det


def new_cryst_from_rotXYZ_and_ucell(rotXYZ, ucparam, orig_crystal):
    """

    :param rotXYZ: tuple of rotation angles about princple axes (radians)
        Crystal will be rotated by rotX about (-1,0,0 ), rotY about (0,-1,0) and rotZ about
        (0,0,-1) . This is the convention used by diffBragg
    :param ucparam: unit cell parameter tuple (a,b,c, alpha, beta, gamma) in Angstrom,degrees)
    :param orig_crystal: dxtbx.model.Crystal object which will be copied and adjusted according to ucell and rotXYZ
    :return: new dxtbx.model.Crystal object
    """
    rotX, rotY, rotZ = rotXYZ
    xax = col((-1, 0, 0))
    yax = col((0, -1, 0))
    zax = col((0, 0, -1))
    ## update parameters:
    RX = xax.axis_and_angle_as_r3_rotation_matrix(rotX, deg=False)
    RY = yax.axis_and_angle_as_r3_rotation_matrix(rotY, deg=False)
    RZ = zax.axis_and_angle_as_r3_rotation_matrix(rotZ, deg=False)
    M = RX * RY * RZ
    U = M * sqr(orig_crystal.get_U())
    new_C = deepcopy(orig_crystal)
    new_C.set_U(U)

    ucman = utils.manager_from_params(ucparam)
    new_C.set_B(ucman.B_recipspace)
    return new_C


def update_crystal_from_x(Mod, SIM, x):
    """
    :param SIM: sim_data instance containing a nanoBragg crystal object
    :param x: parameters returned by hopper_utils (instance of simtbx.diffBragg.refiners.parameters.Parameters()
    :return: a new dxtbx.model.Crystal object with updated unit cell and orientation matrix
    """
    scale, rotX, rotY, rotZ, Na, Nb, Nc, _,_,_,_,_,_,_,_,_,a, b, c, al, be, ga, detz_shift = get_param_from_x(x, Mod)
    ucparam = a, b, c, al, be, ga
    return new_cryst_from_rotXYZ_and_ucell((rotX,rotY,rotZ), ucparam, SIM.crystal.dxtbx_crystal)


def get_new_xycalcs(Modeler, new_exp, old_refl_tag="dials"):
    """

    :param Modeler: data modeler instance after refinement
    :param new_exp: post-refinement dxtbx experiment obj
    :param old_refl_tag: exisiting columns will be renamed with this tag as a prefix
    :return: refl table with xyzcalcs derived from the modeling results
    """
    if isinstance(Modeler.all_sigma_rdout, np.ndarray):
        _, _, _, bragg_subimg, _ = Modeler.get_data_model_pairs()
    else:
        _,_,_, bragg_subimg = Modeler.get_data_model_pairs()
    new_refls = deepcopy(Modeler.refls)

    reflkeys = list(new_refls.keys())
    if "xyzcal.px" in reflkeys:
        new_refls['%s.xyzcal.px' % old_refl_tag] = deepcopy(new_refls['xyzcal.px'])
    if "xyzcal.mm" in reflkeys:
        new_refls['%s.xyzcal.mm' % old_refl_tag] = deepcopy(new_refls['xyzcal.mm'])
    if "xyzobs.mm.value" in list(new_refls.keys()):
        new_refls['%s.xyzobs.mm.value' % old_refl_tag] = deepcopy(new_refls['xyzobs.mm.value'])

    new_xycalcs = flex.vec3_double(len(Modeler.refls), (np.nan, np.nan, np.nan))
    new_xycalcs_mm = flex.vec3_double(len(Modeler.refls), (np.nan, np.nan, np.nan))
    new_xyobs_mm = flex.vec3_double(len(Modeler.refls), (np.nan, np.nan, np.nan))
    for i_roi in range(len(bragg_subimg)):

        ref_idx = Modeler.refls_idx[i_roi]

        #assert ref_idx==i_roi
        if np.any(bragg_subimg[i_roi] > 0):
            I = bragg_subimg[i_roi]
            assert np.all(I>=0)
            Y, X = np.indices(bragg_subimg[i_roi].shape)
            x1, _, y1, _ = Modeler.rois[i_roi]

            com_x, com_y, _ = new_refls[ref_idx]["xyzobs.px.value"]
            com_x = int(com_x - x1 - 0.5)
            com_y = int(com_y - y1 - 0.5)
            # make sure at least some signal is at the centroid! otherwise this is likely a neighboring spot
            try:
                if I[com_y, com_x] == 0:
                    continue
            except IndexError:
                continue

            X += x1
            Y += y1
            Isum = I.sum()
            xcom = (X * I).sum() / Isum + .5
            ycom = (Y * I).sum() / Isum + .5
            com = xcom, ycom, 0

            pid = Modeler.pids[i_roi]
            assert pid == new_refls[ref_idx]['panel']
            panel = new_exp.detector[pid]
            xmm, ymm = panel.pixel_to_millimeter((xcom, ycom))
            com_mm = xmm, ymm, 0
            xobs, yobs, _ = new_refls[ref_idx]["xyzobs.px.value"]
            xobs_mm, yobs_mm = panel.pixel_to_millimeter((xobs, yobs))
            obs_com_mm = xobs_mm, yobs_mm, 0

            new_xycalcs[ref_idx] = com
            new_xycalcs_mm[ref_idx] = com_mm
            new_xyobs_mm[ref_idx] = obs_com_mm

    new_refls["xyzcal.px"] = new_xycalcs
    new_refls["xyzcal.mm"] = new_xycalcs_mm
    new_refls["xyzobs.mm.value"] = new_xyobs_mm

    if Modeler.params.filter_unpredicted_refls_in_output:
        sel = [not np.isnan(x) for x,_,_ in new_refls['xyzcal.px']]
        nbefore = len(new_refls)
        new_refls = new_refls.select(flex.bool(sel))
        nafter = len(new_refls)
        MAIN_LOGGER.info("Filtered %d / %d reflections which did not show peaks in model" % (nbefore-nafter, nbefore))

    return new_refls


def get_mosaicity_from_x(x, Mod, SIM):
    """
    :param x: refinement parameters
    :param SIM: simulator used during refinement
    :return: float or 3-tuple, depending on whether mosaic spread was modeled isotropically
    """
    eta_params = [Mod.P["eta_abc%d"%i] for i in range(3)]
    eta_abc = [p.get_val(x[p.xpos]) for p in eta_params]
    if not SIM.D.has_anisotropic_mosaic_spread:
        eta_abc = [eta_abc[0]]*3
    return eta_abc


def generate_gauss_spec(central_en=9500, fwhm=10, res=1, nchan=20, total_flux=1e12, as_spectrum=True):
    """
    In cases where raw experimental data do not contain X-ray spectra, one can generate a gaussian spectra
    and use that as part of the diffBragg model.

    :param central_en: nominal beam energy in eV
    :param fwhm: fwhm of desired Gaussian spectrum in eV
    :param res: energy resolution of spectrum in eV
    :param nchan: number of energy channels in spectrum
    :param total_flux: total number of photons across whole spectrum
    :param as_spectrum: return as a spectrum object, a suitable attribute of nanoBragg_beam
    :return: either a nanoBragg_beam.NBbeam spectrum, or a tuple of numpy arrays spec_energies, spec_weights
    """
    sig = fwhm / 2.35482
    min_en = central_en - nchan / 2 * res
    max_en = central_en + nchan / 2 * res
    ens = np.linspace(min_en, max_en, nchan)
    wt = np.exp(-(ens - central_en) ** 2 / 2 / sig ** 2)
    wt /= wt.sum()
    wt *= total_flux
    if as_spectrum:
        wvs = utils.ENERGY_CONV / ens  # wavelengths
        spec = list(zip(list(wvs), list(wt)))
        return spec
    else:
        return ens, wt

def downsamp_spec_from_params(params, expt=None, imgset=None, i_img=0):
    """

    :param params:  hopper phil params extracted
    :param expt: a dxtbx experiment (optional)
    :param imgset: an dxtbx imageset (optional)
    :param i_img: index of the image in the imageset (only matters if imgset is not None)
    :return: dxtbx spectrum with parameters applied
    """
    if expt is not None:
        dxtbx_spec = expt.imageset.get_spectrum(0)
        starting_wave = expt.beam.get_wavelength()
    else:
        assert imgset is not None
        dxtbx_spec = imgset.get_spectrum(i_img)
        starting_wave = imgset.get_beam(i_img).get_wavelength()

    spec_en = dxtbx_spec.get_energies_eV()
    spec_wt = dxtbx_spec.get_weights()
    if params.downsamp_spec.skip:
        spec_wave = utils.ENERGY_CONV / spec_en.as_numpy_array()
        stride=params.simulator.spectrum.stride
        spec_wave = spec_wave[::stride]
        spec_wt = spec_wt[::stride]
        spectrum = list(zip(spec_wave, spec_wt))
    else:
        spec_en = dxtbx_spec.get_energies_eV()
        spec_wt = dxtbx_spec.get_weights()
        # ---- downsample the spectrum
        method2_param = {"filt_freq": params.downsamp_spec.filt_freq,
                         "filt_order": params.downsamp_spec.filt_order,
                         "tail": params.downsamp_spec.tail,
                         "delta_en": params.downsamp_spec.delta_en}
        downsamp_en, downsamp_wt = downsample_spectrum(spec_en.as_numpy_array(),
                                                       spec_wt.as_numpy_array(),
                                                       method=2, method2_param=method2_param)

        stride = params.simulator.spectrum.stride
        if stride > len(downsamp_en) or stride == 0:
            raise ValueError("Incorrect value for pinkstride")
        downsamp_en = downsamp_en[::stride]
        downsamp_wt = downsamp_wt[::stride]
        tot_fl = params.simulator.total_flux
        if tot_fl is not None:
            downsamp_wt = downsamp_wt / sum(downsamp_wt) * tot_fl

        downsamp_wave = utils.ENERGY_CONV / downsamp_en
        spectrum = list(zip(downsamp_wave, downsamp_wt))
    # the nanoBragg beam has an xray_beams property that is used internally in diffBragg
    waves, specs = map(np.array, zip(*spectrum))
    ave_wave = sum(waves*specs) / sum(specs)
    MAIN_LOGGER.debug("Starting wavelength=%f. Spectrum ave wavelength=%f" % (starting_wave, ave_wave))
    if expt is not None:
        expt.beam.set_wavelength(ave_wave)
        MAIN_LOGGER.debug("Shifting expt wavelength from %f to %f" % (starting_wave, ave_wave))
    MAIN_LOGGER.debug("USING %d ENERGY CHANNELS" % len(spectrum))
    return spectrum


# set the X-ray spectra for this shot
def downsamp_spec(SIM, params, expt, return_and_dont_set=False):
    SIM.dxtbx_spec = expt.imageset.get_spectrum(0)
    spec_en = SIM.dxtbx_spec.get_energies_eV()
    spec_wt = SIM.dxtbx_spec.get_weights()
    if params.downsamp_spec.skip:
        spec_wave = utils.ENERGY_CONV / spec_en.as_numpy_array()
        stride = params.simulator.spectrum.stride
        spec_wave = spec_wave[::stride]
        spec_wt = spec_wt[::stride]
        SIM.beam.spectrum = list(zip(spec_wave, spec_wt))
    else:
        spec_en = SIM.dxtbx_spec.get_energies_eV()
        spec_wt = SIM.dxtbx_spec.get_weights()
        # ---- downsample the spectrum
        method2_param = {"filt_freq": params.downsamp_spec.filt_freq,
                         "filt_order": params.downsamp_spec.filt_order,
                         "tail": params.downsamp_spec.tail,
                         "delta_en": params.downsamp_spec.delta_en}
        downsamp_en, downsamp_wt = downsample_spectrum(spec_en.as_numpy_array(),
                                                       spec_wt.as_numpy_array(),
                                                       method=2, method2_param=method2_param)

        stride = params.simulator.spectrum.stride
        if stride > len(downsamp_en) or stride == 0:
            raise ValueError("Incorrect value for pinkstride")
        downsamp_en = downsamp_en[::stride]
        downsamp_wt = downsamp_wt[::stride]
        tot_fl = params.simulator.total_flux
        if tot_fl is not None:
            downsamp_wt = downsamp_wt / sum(downsamp_wt) * tot_fl

        downsamp_wave = utils.ENERGY_CONV / downsamp_en
        SIM.beam.spectrum = list(zip(downsamp_wave, downsamp_wt))
    # the nanoBragg beam has an xray_beams property that is used internally in diffBragg
    starting_wave = expt.beam.get_wavelength()
    waves, specs = map(np.array, zip(*SIM.beam.spectrum))
    ave_wave = sum(waves*specs) / sum(specs)
    expt.beam.set_wavelength(ave_wave)
    MAIN_LOGGER.debug("Shifting wavelength from %f to %f" % (starting_wave, ave_wave))
    MAIN_LOGGER.debug("Using %d energy channels" % len(SIM.beam.spectrum))
    if return_and_dont_set:
        return SIM.beam.spectrum
    else:
        SIM.D.xray_beams = SIM.beam.xray_beams


def set_gauss_spec(SIM=None, params=None, E=None):
    """
    Generates a gaussian spectrum for the experiment E and attach it to SIM
    in a way that diffBragg understands

    This updates the xray_beams property of SIM.D and the spectrum property
    of SIM.beam

    :param SIM: simulator object, instance of nanoBragg/sim_data
    :param params: phil params , expected scope is defined in diffBragg/phil.py
    :param E: dxtbx Experiment
    :return: None
    """
    if SIM is not None and not hasattr(SIM, "D"):
        raise AttributeError("Cannot set the spectrum until diffBragg has been instantiated!")
    spec_mu = utils.ENERGY_CONV / E.beam.get_wavelength()
    spec_res = params.simulator.spectrum.gauss_spec.res
    spec_nchan = params.simulator.spectrum.gauss_spec.nchannels
    spec_fwhm = params.simulator.spectrum.gauss_spec.fwhm
    spec_ens, spec_wts = generate_gauss_spec(central_en=spec_mu, fwhm=spec_fwhm, res=spec_res,
                                             nchan=spec_nchan, as_spectrum=False)
    spec_wvs = utils.ENERGY_CONV / spec_ens
    nanoBragg_beam_spec = list(zip(spec_wvs, spec_wts))
    if SIM is not None:
        SIM.dxtbx_spec = Spectrum(spec_ens, spec_wts)
        SIM.beam.spectrum = nanoBragg_beam_spec
        SIM.D.xray_beams = SIM.beam.xray_beams
    else:
        return nanoBragg_beam_spec


def sanity_test_input_lines(input_lines):
    for line in input_lines:
        line_items = line.strip().split()
        if len(line_items) not in [2, 3, 4]:
            raise IOError("Input line %s is not formatted properly" % line)
        for item in line_items:
            if os.path.isfile(item) and not os.path.exists(item):
                raise FileNotFoundError("File %s does not exist" % item)


def full_img_pfs(img_sh):
    Panel_inds, Slow_inds, Fast_inds = map(np.ravel, np.indices(img_sh) )
    pfs_coords = np.vstack([Panel_inds, Fast_inds, Slow_inds]).T
    pfs_coords_flattened = pfs_coords.ravel()
    pfs_full = flex.size_t( np.ascontiguousarray(pfs_coords_flattened))
    return pfs_full


def print_profile(stats, timed_methods):
    for method in stats.timings.keys():
        filename, header_ln, name = method
        if name not in timed_methods:
            continue
        info = stats.timings[method]
        PROFILE_LOGGER.warning("\n")
        PROFILE_LOGGER.warning("FILE: %s" % filename)
        if not info:
            PROFILE_LOGGER.warning("<><><><><><><><><><><><><><><><><><><><><><><>")
            PROFILE_LOGGER.warning("METHOD %s : Not profiled because never called" % (name))
            PROFILE_LOGGER.warning("<><><><><><><><><><><><><><><><><><><><><><><>")
            continue
        unit = stats.unit

        line_nums, ncalls, timespent = zip(*info)
        fp = open(filename, 'r').readlines()
        total_time = sum(timespent)
        header_line = fp[header_ln-1][:-1]
        PROFILE_LOGGER.warning(header_line)
        PROFILE_LOGGER.warning("TOTAL FUNCTION TIME: %f ms" % (total_time*unit*1e3))
        PROFILE_LOGGER.warning("<><><><><><><><><><><><><><><><><><><><><><><>")
        PROFILE_LOGGER.warning("%5s%14s%9s%10s" % ("Line#", "Time", "%Time", "Line" ))
        PROFILE_LOGGER.warning("%5s%14s%9s%10s" % ("", "(ms)", "", ""))
        PROFILE_LOGGER.warning("<><><><><><><><><><><><><><><><><><><><><><><>")
        for i_l, l in enumerate(line_nums):
            frac_t = timespent[i_l] / total_time * 100.
            line = fp[l-1][:-1]
            PROFILE_LOGGER.warning("%5d%14.2f%9.2f%s" % (l, timespent[i_l]*unit*1e3, frac_t, line))


def get_lam0_lam1_from_pandas(df):
    lam0 = df.lam0.values[0]
    lam1 = df.lam1.values[0]
    assert not np.isnan(lam0)
    assert not np.isnan(lam1)
    assert lam0 != -1
    assert lam1 != -1
    return lam0, lam1


def get_simulator_for_data_modelers(data_modeler):
    self = data_modeler
    SIM = utils.simulator_for_refinement(self.E, self.params)

    if self.params.use_diffuse_models:
        if self.params.symmetrize_diffuse:
            assert self.params.space_group is not None
            SIM.D.laue_group_num = utils.get_laue_group_number(
                self.params.space_group)  # TODO this can also be retrieved from crystal model if params.space_group is None
            MAIN_LOGGER.debug("Set laue group number: %d (for diffuse models)" % SIM.D.laue_group_num)
        if self.params.diffuse_stencil_size > 0:
            SIM.D.stencil_size = self.params.diffuse_stencil_size
            MAIN_LOGGER.debug("Set diffuse stencil size: %d" % SIM.D.stencil_size)
        if self.params.diffuse_orientation == 1:
            ori = (1,0,0,0,1,0,0,0,1)
        else:
            a = 1/np.sqrt(2)
            ori = a, a, 0.0, a, a, 0.0, 0.0, 0.0, 1.0
        SIM.D.set_rotate_principal_axes(ori)
    SIM.D.gamma_miller_units = self.params.gamma_miller_units
    SIM.isotropic_diffuse_gamma = self.params.isotropic.diffuse_gamma
    SIM.isotropic_diffuse_sigma = self.params.isotropic.diffuse_sigma
    if self.params.record_device_timings:
        SIM.D.record_timings = True

    # TODO: use data_modeler.set_spectrum instead
    if self.params.spectrum_from_imageset:
        downsamp_spec(SIM, self.params, self.E)
    elif self.params.gen_gauss_spec:
        set_gauss_spec(SIM, self.params, self.E)
    data_modeler.nanoBragg_beam_spectrum = SIM.beam.spectrum

    # TODO: verify how slow always using lambda coefs is
    # TODO: ensure lam0/lam1 are not -1 and not np.nan
    SIM.D.use_lambda_coefficients = True
    SIM.D.lambda_coefficients = tuple(self.params.init.spec)
    _set_Fhkl_refinement_flags(self.params, SIM)
    data_modeler.set_Fhkl_channels(SIM)  # if doing ensemble refinement, do this on all modelers!

    return SIM


def _set_Fhkl_refinement_flags(params, SIM):

    SIM.refining_Fhkl = False
    SIM.Num_ASU = 0
    SIM.num_Fhkl_channels = 1
    SIM.Fhkl_channel_bounds = [0, np.inf]
    SIM.centric_flags = None
    if not params.fix.Fhkl:
        if params.Fhkl_channel_bounds is not None:
            assert params.Fhkl_channel_bounds == sorted(params.Fhkl_channel_bounds)
            SIM.Fhkl_channel_bounds = [0] + params.Fhkl_channel_bounds + [np.inf]

        SIM.num_Fhkl_channels = len(SIM.Fhkl_channel_bounds) - 1
        asu_map = SIM.D.get_ASUid_map()
        SIM.asu_map_int = {tuple(map(int, k.split(','))): v for k, v in asu_map.items()}

        num_unique_hkl = len(asu_map)
        SIM.Fhkl_scales_init = np.ones(num_unique_hkl * SIM.num_Fhkl_channels)
        SIM.refining_Fhkl = True
        SIM.Num_ASU = num_unique_hkl  # TODO replace with diffBragg property
        if params.betas.Fhkl is not None or params.betas.Finit is not None:
            MAIN_LOGGER.debug("Restraining to average Fhkl with %d bins" % params.Fhkl_dspace_bins)
            SIM.set_dspace_binning(params.Fhkl_dspace_bins)
        if params.betas.Friedel is not None:
            MAIN_LOGGER.debug("Restraining Friedel pairs")
            SIM.D.prep_Friedel_restraints()

        if SIM.num_Fhkl_channels > 1:
            assert params.space_group is not None
            sym = crystal.symmetry(SIM.D.unit_cell_tuple, params.space_group)
            hkl_flex = flex.miller_index(list(SIM.asu_map_int.keys()))
            mset = miller.set(sym, hkl_flex, True)
            cent = mset.centric_flags()
            cent_map = {h: flag for h, flag in zip(cent.indices(), cent.data())}
            SIM.is_centric = np.zeros(SIM.Num_ASU, bool)
            for h, idx in SIM.asu_map_int.items():
                is_centric = cent_map[h]
                SIM.is_centric[idx] = is_centric

            SIM.where_is_centric = np.where(SIM.is_centric)[0]


 *******************************************************************************
