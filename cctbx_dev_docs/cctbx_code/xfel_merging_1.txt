

 *******************************************************************************
xfel/merging/application/modify/factory.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.modify.polarization import polarization
from xfel.merging.application.modify.reindex_to_reference import reindex_to_reference
from xfel.merging.application.modify.cosym import cosym
from xfel.merging.application.modify.reindex_to_abc import reindex_to_abc
from xfel.merging.application.worker import factory as factory_base

class factory(factory_base):
  """ Factory class for modification of intensites. """
  @staticmethod
  def from_parameters(params, additional_info=None, mpi_helper=None, mpi_logger=None):
    if additional_info == "reindex_to_reference".split("_"):
      return [reindex_to_reference(params, mpi_helper, mpi_logger)]
    elif additional_info == "cosym".split("_"):
      return [cosym(params, mpi_helper, mpi_logger)]
    elif additional_info == "reindex_to_abc".split("_"):
      return [reindex_to_abc(params, mpi_helper, mpi_logger)]
    else:
      assert not additional_info, additional_info
      return [polarization(params, mpi_helper, mpi_logger)]


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/modify/polarization.py
from __future__ import absolute_import, division, print_function
from dials.array_family import flex
from scitbx import matrix
from xfel.merging.application.worker import worker
from xfel.merging.application.utils.memory_usage import get_memory_usage

class polarization(worker):
  """
  Computes the polarization correction as defined by Kahn 1982.
  Modifies the intensity.sum.value and intensity.sum.variance columns in place.
  """
  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(polarization, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Apply polarization correction'

  def run(self, experiments, reflections):

    self.logger.log_step_time("POLARIZATION_CORRECTION")

    result = flex.reflection_table()

    for expt_id, experiment in enumerate(experiments):
      refls = reflections.select(reflections['id'] == expt_id)
      if len(refls) == 0: continue
      beam = experiment.beam
      # Remove the need for pixel size within cxi.merge.  Allows multipanel detector with dissimilar panels.
      # Relies on new frame extractor code called by dials.stills_process that writes s0, s1 and polarization normal
      # vectors all to the integration pickle.  Future path (IE THIS CODE): use dials json and reflection file.
      s0_vec = matrix.col(beam.get_s0()).normalize()
      s0_polar_norm = beam.get_polarization_normal()
      s1_vec = refls['s1']
      Ns1 = len(s1_vec)
      # project the s1_vector onto the plane normal to s0.  Get result by subtracting the
      # projection of s1 onto s0, which is (s1.dot.s0_norm)s0_norm
      s0_norm = flex.vec3_double(Ns1,s0_vec)
      s1_proj = (s1_vec.dot(s0_norm))*s0_norm
      s1_in_normal_plane = s1_vec - s1_proj
      # Now want the polar angle between the projected s1 and the polarization normal
      s0_polar_norms = flex.vec3_double(Ns1,s0_polar_norm)
      dotprod = (s1_in_normal_plane.dot(s0_polar_norms))
      costheta = dotprod/(s1_in_normal_plane.norms())
      theta = flex.acos(costheta)
      cos_two_polar_angle = flex.cos(2.0*theta)
      # gives same as old answer to ~1% but not exact.  Not sure why, should not matter.

      tt_vec = experiment.crystal.get_unit_cell().two_theta(miller_indices = refls['miller_index'],
                                                            wavelength = beam.get_wavelength())
      cos_tt_vec = flex.cos(tt_vec)
      sin_tt_vec = flex.sin(tt_vec)
      cos_sq_tt_vec = cos_tt_vec * cos_tt_vec
      sin_sq_tt_vec = sin_tt_vec * sin_tt_vec
      P_nought_vec = 0.5 * (1. + cos_sq_tt_vec)

      F_prime = -1.0 # Hard-coded value defines the incident polarization axis
      P_prime = 0.5 * F_prime * cos_two_polar_angle * sin_sq_tt_vec

      # added as a diagnostic
      #prange=P_nought_vec - P_prime
      #other_F_prime = 1.0
      #otherP_prime = 0.5 * other_F_prime * cos_two_polar_angle * sin_sq_tt_vec
      #otherprange=P_nought_vec - otherP_prime
      #diff2 = flex.abs(prange - otherprange)
      #print >> out, "mean diff is",flex.mean(diff2), "range",flex.min(diff2), flex.max(diff2)
      # done

      correction = 1 / ( P_nought_vec - P_prime )
      refls['intensity.sum.value'] = refls['intensity.sum.value'] * correction
      refls['intensity.sum.variance'] = refls['intensity.sum.variance'] * correction**2 # propagated error
      # This corrects observations for polarization assuming 100% polarization on
      # one axis (thus the F_prime = -1.0 rather than the perpendicular axis, 1.0)
      # Polarization model as described by Kahn, Fourme, Gadet, Janin, Dumas & Andre
      # (1982) J. Appl. Cryst. 15, 330-337, equations 13 - 15.

      result.extend(refls)

    if len(reflections) > 0:
      self.logger.log("Applied polarization correction. Mean intensity changed from %.2f to %.2f"%(flex.mean(reflections['intensity.sum.value']), flex.mean(result['intensity.sum.value'])))

    self.logger.log_step_time("POLARIZATION_CORRECTION", True)
    self.logger.log("Memory usage: %d MB"%get_memory_usage())

    # Remove 's1' column from the reflection table
    from xfel.merging.application.reflection_table_utils import reflection_table_utils
    reflections = reflection_table_utils.prune_reflection_table_keys(reflections=result, keys_to_delete=['s1'],
                                                                     keys_to_ignore=self.params.input.persistent_refl_cols)
    self.logger.log("Pruned reflection table")
    self.logger.log("Memory usage: %d MB"%get_memory_usage())

    return experiments, reflections

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(polarization)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/modify/reindex_cosym.py
from __future__ import absolute_import, division, print_function
from scitbx.array_family import flex
from cctbx.sgtbx import change_of_basis_op
from cctbx import miller

"""The sole purpose here is to reindex a reflection table, once the coset assignments
   for each experiment have been determined"""

def reindex_refl_by_coset(refl,data,symms,uuids,co,anomalous_flag = False, verbose=True):
  if verbose:
    cache_miller = refl["miller_index"].deep_copy()
    cache_asu = refl["miller_index_asymmetric"].deep_copy()

  for icoset,partition in enumerate(co.partitions):
    if icoset==0: continue # no change of basis

    cb_op = change_of_basis_op(partition[0])
    mi_new = cb_op.apply(refl["miller_index"])
    mi_asu_new = mi_new.deep_copy()
    miller.map_to_asu(symms[0].space_group().info().type(), anomalous_flag, mi_asu_new)

    # now select only those expts assigned to that coset
    good_refls = flex.bool(len(refl), False)
    all_expt_id = list(data["experiment"])
    all_coset = list(data["coset"]) # would like to understand how to use pandas rather than Python list
    for iexpt in range(len(symms)):
        iexpt_id = uuids[iexpt]
        this_coset = all_coset[ all_expt_id.index(iexpt_id) ]
        if this_coset == icoset:
          good_refls |= refl["id"] == iexpt

    re_millers = mi_new.select(good_refls)
    re_asu = mi_asu_new.select(good_refls)

    refl["miller_index"].set_selected(good_refls, re_millers)
    refl["miller_index_asymmetric"].set_selected(good_refls, re_asu)
  if verbose:
    id_map = refl.experiment_identifiers()
    for x in range(len(refl)):
      print ("%3d"%x,id_map[refl["id"][x]],
             all_coset[ all_expt_id.index(id_map[refl["id"][x]]) ],
             "%10s"%(change_of_basis_op(co.partitions[   all_coset[ all_expt_id.index(id_map[refl["id"][x]]) ]   ][0]).as_hkl()),
             "(%4d%4d%4d)"%(cache_miller[x]), "(%4d%4d%4d)"%(cache_asu[x]),
             "(%4d%4d%4d)"%(refl["miller_index"][x]), "(%4d%4d%4d)"%(refl["miller_index_asymmetric"][x]))

  return refl

if __name__=="__main__":

  import pickle
  with open("refl.pickle","rb") as F:
    refl = pickle.load(F)
    print(refl)
    good_refls = flex.bool(len(refl), False)
    for x in range(0,len(refl),500):
      good_refls[x]=True
    refl = refl.select(good_refls)
    data = pickle.load(F)
    print(data)
    symms = pickle.load(F)
    print(symms)
    uuids = pickle.load(F)
    co = pickle.load(F)
    print(co)
    co.show()
    reindex_refl_by_coset(refl,data,symms,uuids,co)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/modify/reindex_to_abc.py
from __future__ import division
from xfel.merging.application.worker import worker
from rstbx.symmetry.constraints import parameter_reduction
from cctbx import sgtbx

def reindex_experiments(experiments, cb_op, space_group):
  for experiment in experiments:
    experiment.crystal.set_space_group(sgtbx.space_group("P 1"))
    experiment.crystal = experiment.crystal.change_basis(cb_op)
    experiment.crystal.set_space_group(space_group)
    S = parameter_reduction.symmetrize_reduce_enlarge(
        experiment.crystal.get_space_group()
    )
    S.set_orientation(experiment.crystal.get_B())
    S.symmetrize()
    experiment.crystal.set_B(S.orientation.reciprocal_matrix())
  return experiments

class reindex_to_abc(worker):
  """
  Reindex according to an a,b,c basis tranformation
  """

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(reindex_to_abc, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Reindex according to an a,b,c basis tranformation'

  def run(self, experiments, reflections):
    if len(experiments) == 0:
      return experiments, reflections
    cb_op_str = self.params.modify.reindex_to_abc.change_of_basis_op
    change_of_basis_op = sgtbx.change_of_basis_op(cb_op_str)

    space_group = self.params.modify.reindex_to_abc.space_group
    assert space_group is not None
    space_group = space_group.group()

    experiments = reindex_experiments(
        experiments, change_of_basis_op, space_group=space_group
    )

    miller_indices = reflections["miller_index"]
    miller_indices_reindexed = change_of_basis_op.apply(miller_indices)
    reflections["miller_index"] = miller_indices_reindexed

    return experiments, reflections


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/modify/reindex_to_reference.py
from __future__ import absolute_import, division, print_function
from dials.array_family import flex
from xfel.merging.application.worker import worker
from xfel.merging.application.scale.experiment_scaler import experiment_scaler
from xfel.merging.application.utils.memory_usage import get_memory_usage
from mmtbx.scaling.twin_analyses import twin_laws
from cctbx import sgtbx, miller
from cctbx.crystal import symmetry
from dxtbx.model.crystal import MosaicCrystalSauter2014

class reindex_to_reference(worker):
  """
  Resolve indexing ambiguity usin twin laws and correlation to a reference set
  """
  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(reindex_to_reference, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Resolve indexing ambiguity by comparison to a reference'

  def process_dataframe(self, raw, params):
    #two purposes: 1) write table to disk, 2) make plot
    import pandas as pd
    pd.set_option('display.max_rows', 300)
    pd.set_option('display.max_columns', 8)
    pd.set_option("display.width", None)
    from pandas import DataFrame as df
    data = df(raw)
    print (data)
    symops = list(data.keys())[1:]
    data["max"] = data[symops].max(axis=1)
    data["min"] = data[symops].min(axis=1)
    data['diff'] = data["max"] - data["min"]
    data["reindex_op"] = data[symops].idxmax(axis=1)

    # Now collect dataframes from all ranks and merge
    reports = self.mpi_helper.comm.gather(data,root=0)
    if self.mpi_helper.rank == 0:

      result = pd.concat(reports)

      # Best guess coset decomposition.  Works fine for P6 use case.
      change_of_basis_op_to_niggli_cell = \
      self.params.scaling.i_model.change_of_basis_op_to_niggli_cell()
      minimum_cell_symmetry = self.params.scaling.i_model.crystal_symmetry().change_basis(
      cb_op=change_of_basis_op_to_niggli_cell)
      lattice_group = sgtbx.lattice_symmetry.group(
      reduced_cell=minimum_cell_symmetry.unit_cell())
      Laue_group = self.params.scaling.i_model.space_group().build_derived_laue_group()
      LATG = lattice_group.change_basis(change_of_basis_op_to_niggli_cell.inverse())
      LAUG = Laue_group.build_derived_acentric_group()
      CO = sgtbx.cosets.left_decomposition(LATG,LAUG)
      partitions = CO.partitions

      # Initialize a coset column using best estimate
      working_coset = []
      working_reindex_op = list(result["reindex_op"])
      for iexpt in range(len(working_reindex_op)):
            this_coset = None
            for p_no, partition in enumerate(partitions):
              partition_ops = [sgtbx.change_of_basis_op(ip).as_hkl() for ip in partition]
              if working_reindex_op[iexpt] in partition_ops:
                this_coset = p_no; break
            assert this_coset is not None
            working_coset.append(this_coset)
      result["coset"]=working_coset
      print (result)

      import os
      result.to_pickle(path = os.path.join(params.output.output_dir, params.modify.reindex_to_reference.dataframe))
      #from matplotlib import pyplot as plt
      #plt.hist(data["min"], bins=24, range=[-0.5,1], )
      #plt.hist(data["max"], bins=24, range=[-0.5,1], alpha=0.75)
      #plt.show()
      # later change this plot to produce PDF file

  def run(self, experiments, reflections):

    self.logger.log_step_time("REINDEX")

    # Get list of twinning operators for this space group
    operators = twin_laws(self.params.scaling.i_model).operators
    if not operators:
      self.logger.log("No indexing ambiguity. Skipping this step.")
      return experiments, reflections
    self.logger.log("Resolving indexing ambiguity using operators h,k,l, %s"%", ".join( \
      [op.operator.r().as_hkl() for op in operators]))
    if self.params.modify.reindex_to_reference.dataframe:
      from collections import OrderedDict
      keyval = [("experiment",[]),("h,k,l", [])]
      for op in operators:
        keyval.append((op.operator.r().as_hkl(), []))
      raw = OrderedDict(keyval)
      keys = list(raw.keys())

    operators = [sgtbx.change_of_basis_op(op.operator.r().as_hkl()) for op in operators]

    result = flex.reflection_table()
    scaler = experiment_scaler(self.params, self.mpi_helper, self.logger)
    model_intensities = self.params.scaling.i_model
    target_symm = symmetry(unit_cell = self.params.scaling.unit_cell, space_group_info = self.params.scaling.space_group)

    def get_correlation(cb_op=None):
      """ Helper function to get CC to the reference given an operator """
      # Build a miller array for the experiment reflections
      exp_miller_indices = miller.set(target_symm, exp_reflections['miller_index_asymmetric'], True)
      exp_intensities = miller.array(exp_miller_indices, exp_reflections['intensity.sum.value'],
                                     flex.sqrt(exp_reflections['intensity.sum.variance']))
      if cb_op:
        exp_intensities = exp_intensities.change_basis(cb_op).map_to_asu()

      # Extract an array of HKLs from the model to match the experiment HKLs
      matching_indices = miller.match_multi_indices(miller_indices_unique = model_intensities.indices(), miller_indices = exp_intensities.indices())

      # Least squares
      scaling_result = scaler.fit_experiment_to_reference(model_intensities, exp_intensities, matching_indices)
      return scaling_result.correlation if scaling_result.correlation is not None else -1

    # Test each experiment to see if an operator gives a better CC to the reference, and if it does, apply it
    for expt_id, experiment in enumerate(experiments):
      exp_reflections = reflections.select(reflections['id'] == expt_id)
      all_correlations = []
      best_correlation = get_correlation()
      all_correlations.append(best_correlation)
      best_op = None
      for cb_op in operators:
        test_correlation = get_correlation(cb_op)
        all_correlations.append(test_correlation)
        if test_correlation > best_correlation:
          best_correlation = test_correlation
          best_op = cb_op
      if best_op:
        exp_miller_indices = miller.set(target_symm, exp_reflections['miller_index'], True).change_basis(best_op)
        exp_reflections['miller_index_asymmetric'] = exp_miller_indices.map_to_asu().indices()
        exp_reflections['miller_index'] = exp_miller_indices.indices()
        experiment.crystal = MosaicCrystalSauter2014(experiment.crystal.change_basis(best_op)) # need to use wrapper because of cctbx/dxtbx#5
      result.extend(exp_reflections)

      self.logger.log("Expt %d, reindexing op correlations: %s"%(expt_id, ", ".join(["%6.3f"%c for c in all_correlations])))
      if self.params.modify.reindex_to_reference.dataframe:
        raw["experiment"].append(experiment.identifier)
        for ix in range(len(all_correlations)):
          raw[keys[ix+1]].append(all_correlations[ix])

    if self.params.modify.reindex_to_reference.dataframe:
      self.process_dataframe(raw, self.params)

    self.logger.log_step_time("REINDEX", True)
    self.logger.log("Memory usage: %d MB"%get_memory_usage())

    from xfel.merging.application.utils.data_counter import data_counter
    data_counter(self.params).count(experiments, reflections)
    return experiments, result

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(reindex_to_reference)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/modify/token_passing_left_right.py
from __future__ import division, print_function

def token_passing_left_right(value, comm): # comm = MPI communicator
  comm.barrier()
  mpi_rank = comm.rank
  mpi_size = comm.size

  src = mpi_rank - 1 if mpi_rank != 0 else mpi_size - 1
  dst = mpi_rank + 1 if mpi_rank != mpi_size - 1 else 0
  print (src,dst)
  if mpi_rank % 2 == 0:
    comm.send(value, dest=dst)
    m = comm.recv(source=src)
  else:
    m = comm.recv(source=src)
    comm.send(value, dest=dst)
  tokens = [m,value]
  comm.barrier()

  src = mpi_rank + 1 if mpi_rank != mpi_size - 1 else 0
  dst = mpi_rank - 1 if mpi_rank != 0 else mpi_size - 1

  if mpi_rank % 2 == 0:
    comm.send(value, dest=dst)
    m = comm.recv(source=src)
  else:
    m = comm.recv(source=src)
    comm.send(value, dest=dst)
  tokens.append(m)
  comm.barrier()
  return tokens

def get_root_communicator():
  from libtbx.mpi4py import MPI
  return MPI.COMM_WORLD

def tst_token_passing():
  comm = get_root_communicator()
  mpi_rank = comm.Get_rank()
  mpi_size = comm.Get_size()
  if mpi_size==1: print("OK"); return
  T = token_passing_left_right(value = 100+mpi_rank, comm = comm)
  print("On Rank %d the tokens are: %s"%(mpi_rank, str(T)))
  if mpi_rank==0: print("OK")

def choose_without_replace(available, unavailable, mt):
  while 1:
    idx = int(mt.random_double() * len(available))
    if available[idx] not in unavailable: break
  return available.pop(idx)

def construct_src_to_dst_plan(icount, tranch_size, comm, verbose=True):
      mpi_rank = comm.Get_rank()
      mpi_size = comm.Get_size()
      COMPOSITE_MULTIPLICITY = 3
      from scitbx.array_family import flex
      imean = flex.mean(icount.as_double()); isum = flex.sum(icount)
      trial_comm_size = max(1, int(isum * COMPOSITE_MULTIPLICITY / tranch_size))
      trial_comm_size = min(trial_comm_size, mpi_size - 1) # new comm must have fewer than mpi_helper.size ranks
      if trial_comm_size in [2,3]: trial_comm_size = 1 # consensus communicator can run on 1,4,5,..., not 2,3
      trial_comm_size = max(1, trial_comm_size) # cannot be 0

      if verbose: print("recommended communicator size:",trial_comm_size)
      trial = [[] for itrank in range(trial_comm_size)]
      srcrk = [[] for itrank in range(trial_comm_size)]
      todst = {} # plan of action: src rank values to dst keys
      mt = flex.mersenne_twister(seed=0)
      order = mt.random_permutation(len(icount))
      available_dst = list(range(trial_comm_size))
      for idx in range(len(icount)):
        item = icount[order[idx]] ; rk = idx
        if trial_comm_size>3:
          unavailable = []
          for irx in range(COMPOSITE_MULTIPLICITY):
            if len(available_dst)==0:  available_dst = list(range(trial_comm_size))
            idx_st = choose_without_replace(available_dst, unavailable, mt)
            unavailable.append(idx_st)
            trial[(idx_st)%trial_comm_size].append(item); srcrk[(idx_st)%trial_comm_size].append(rk)
            todst[idx_st] = todst.get(idx_st,[]); todst[idx_st].append(order[idx])
        else:
          trial[0].append(item)
          idx_st = 0
          todst[idx_st] = todst.get(idx_st,[]); todst[idx_st].append(order[idx])
      if verbose:
        for itranch, item in enumerate(trial):
          print("tranch %2d:"%itranch, item, flex.sum(flex.int(item)))
        print()
        for item in srcrk:
          print(", ".join(["%02d"%i for i in item]))
        print(todst)
      return todst

def construct_anchor_src_to_dst_plan(min_anchor, icount, tranch_size, comm, verbose=True):
      """The overall purpose is to recruit enough total experiments from several ranks to perform anchor alignment"""
      mpi_rank = comm.Get_rank()
      mpi_size = comm.Get_size()
      todst = {} # plan of action: src rank values to dst keys
      todst[0] = []
      anchor_sum = 0
      rank = 0
      rank_count = []
      while anchor_sum < min_anchor and rank < mpi_size:
        todst[0].append(rank)
        rank_count.append(icount[rank])
        anchor_sum += icount[rank]
        rank += 1
      if verbose:
        print("anchor tranch:",rank_count, anchor_sum)
        print(todst)
      if anchor_sum < min_anchor:
        raise Exception("Cannot align to anchor with %d total experiments, need at least %d"%(anchor_sum, min_anchor))
      return todst

def get_experiment_counts_by_comm_size():
  return {
    64:[52, 48, 48, 48, 47, 47, 46, 45, 44, 43, 43, 42, 43, 42, 42, 40, 40, 38, 38, 38, 38, 38,
        37, 37, 37, 37, 37, 35, 35, 34, 33, 32, 32, 32, 31, 31, 30, 30, 29, 30, 30, 30, 29, 29,
        28, 29, 29, 28, 28, 28, 28, 27, 28, 27, 28, 27, 28, 28, 28, 28, 27, 28, 28, 28],
    41:[66, 66, 66, 66, 66, 65, 65, 65, 65, 63, 63, 62, 61, 60, 60, 60, 60, 58, 56, 56, 56, 56,
        55, 54, 50, 50, 49, 48, 48, 46, 45, 45, 43, 43, 43, 43, 42, 42, 40, 40, 38],
    32:[84, 81, 81, 79, 77, 77, 76, 75, 75, 73, 72, 72, 72, 71, 71, 68, 68, 66, 66, 66, 66, 66, 66, 65, 65, 65, 64, 61, 61, 60, 59, 57],
    16:[146, 144, 142, 142, 141, 141, 141, 141, 140, 140, 137, 137, 135, 134, 133, 131],
    13:[179, 178, 173, 173, 171, 171, 171, 171, 170, 167, 167, 167, 167],
    10:[226, 226, 225, 224, 222, 222, 221, 220, 220, 219],
     5:[447, 446, 446, 444, 442],
     4:[558, 557, 555, 555],
     3:[743, 741, 741],
     2:[1113, 1112],
     1:[2225]}

def tst_tranch_size():
  from scitbx.array_family import flex
  comm = get_root_communicator()
  mpi_rank = comm.Get_rank()
  mpi_size = comm.Get_size()
  tranch_size = 600 # target size for the composite tranch in this example
  if mpi_rank == 0:
      icount = flex.int(get_experiment_counts_by_comm_size()[mpi_size])
      plan = construct_src_to_dst_plan(icount, tranch_size, comm)
  else:
      plan = 0
  plan = comm.bcast(plan, root = 0)
  # plan = dictionary < key = destination composite tranch; value = list of src ranks contributing the unique data >
  dst_offset = 1 if mpi_size>1 else 0
  tokens = apply_all_to_all(plan=plan, dst_offset=dst_offset,
                   value=get_experiment_counts_by_comm_size()[mpi_size][mpi_rank], comm = comm)
  print ("rank",mpi_rank,plan.get(mpi_rank-dst_offset, None), flex.sum(flex.int(tokens)))
  if mpi_rank == 0: print("OK")

def apply_all_to_all(plan, dst_offset, value, comm):
  comm.barrier()
  mpi_rank = comm.rank
  mpi_size = comm.size

  dst_values = [None]*mpi_size # defaults
  for dst_key in plan:
    if mpi_rank in plan[dst_key]:
      dst_values[dst_key + dst_offset] = value
  tokens = comm.alltoall(dst_values)
  while None in tokens:
    tokens.pop(tokens.index(None))
  comm.barrier()
  return tokens

if __name__=="__main__":
  # Usage: mpirun -n 64 libtbx.python $MODULES/cctbx_project/xfel/merging/application/modify/token_passing_left_right.py
  #tst_token_passing()
  tst_tranch_size()


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/monitor/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/monitor/factory.py
from __future__ import absolute_import, division, print_function

from xfel.merging.application.monitor.monitor import MonitorWorker
from xfel.merging.application.worker import factory as factory_base


class factory(factory_base):
  """Factory class for monitoring CPU and GPU resources in the background"""
  @staticmethod
  def from_parameters(params, additional_info=None, mpi_helper=None, mpi_logger=None):
    return [MonitorWorker(params, mpi_helper, mpi_logger)]


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/monitor/monitor.py
from __future__ import absolute_import, division, print_function

from libtbx.resource_monitor import ResourceMonitor
from xfel.merging.application.worker import worker


resource_monitor = None


class MonitorWorker(worker):
  """Monitor and plot CPU and GPU resources during `cctbx.xfel.merge`"""

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    kwargs = dict(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)
    super(MonitorWorker, self).__init__(**kwargs)

  def __repr__(self):
    return 'Monitor CPU and GPU resources'

  def run(self, experiments, reflections):
    global resource_monitor
    if resource_monitor is None:
      resource_monitor = ResourceMonitor(
        detail=self.params.monitor.detail,
        period=self.params.monitor.period,
        plot=self.params.monitor.plot,
        prefix=self.params.monitor.prefix,
        write=self.params.monitor.write,
      )
    if resource_monitor.active:
      self.logger.log('Stopping resource monitor')
      resource_monitor.stop()
    else:  # if resource_monitor.active == False
      self.logger.log('Starting resource monitor')
      resource_monitor.start()
    return experiments, reflections


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/mpi_helper.py
from __future__ import absolute_import, division, print_function
from collections import Counter
from contextlib import contextmanager
from libtbx.mpi4py import MPI
import numpy as np


import sys
def system_exception_handler(exception_type, value, traceback):
  try:
    rank = MPI.COMM_WORLD.Get_rank()
    from traceback import print_exception
    sys.stderr.write("\nTrying to abort all MPI processes because of exception in process %d:\n"%rank)
    print_exception(exception_type, value, traceback)
    sys.stderr.write("\n")
    sys.stderr.flush()
  finally:
    try:
      MPI.COMM_WORLD.Abort(1)
    except Exception as e:
      sys.stderr.write("\nFailed to execute: MPI.COMM_WORLD.Abort(1)\n")
      sys.stderr.flush()
      raise e
sys.excepthook = system_exception_handler


@contextmanager
def adaptive_collective(rooted_variant, non_rooted_variant):
  """
  Declare and switch from rooted to non-rooted collective if root is None.
  Statement `with adaptive_collective(gather, allgather) as gather:` will yield
  `allgather` if root is None, and gather (with an appropriate root) otherwise.
  """
  def adaptive_collective_dispatcher(*args, **kwargs):
    root = kwargs.pop('root', 0)
    if root is None:
      collective = non_rooted_variant
    else:  # if root is an integer
      collective = rooted_variant
      kwargs['root'] = root
    return collective(*args, **kwargs)
  yield adaptive_collective_dispatcher


class mpi_helper(object):
  def __init__(self):
    self.MPI = MPI
    self.comm = self.MPI.COMM_WORLD
    self.rank = self.comm.Get_rank()
    self.size = self.comm.Get_size()
    self.error = (None,None) # (rank,description)

  def time(self):
    return self.MPI.Wtime()

  def finalize(self):
    self.MPI.Finalize()

  def cumulative_flex(self, flex_array, flex_type=None, root=0):
    """
    Build a cumulative sum flex array out of multiple same-size flex arrays.
    Example: (a1,a2,a3) + (b1, b2, b3) = (a1+b1, a2+b2, a3+b3)
    """
    flex_type = flex_type if flex_type is not None else type(flex_array)
    with adaptive_collective(self.comm.gather, self.comm.allgather) as gather:
      list_of_all_flex_arrays = gather(flex_array, root=root)
    if list_of_all_flex_arrays is None:
      return None
    cumulative = flex_type(flex_array.size(), 0)
    for flex_array in list_of_all_flex_arrays:
      if flex_array is not None:
        cumulative += flex_array
    return cumulative

  def aggregate_flex(self, flex_array, flex_type=None, root=0):
    """
    Build an aggregate flex array out of multiple flex arrays
    Example: (a1,a2,a3) + (b1, b2, b3) = (a1, a2, a3, b1, b2, b3)
    """
    flex_type = flex_type if flex_type is not None else type(flex_array)
    with adaptive_collective(self.comm.gather, self.comm.allgather) as gather:
      list_of_all_flex_arrays = gather(flex_array, root=root)
    if list_of_all_flex_arrays is None:
      return None
    aggregate = flex_type()
    for flex_array in list_of_all_flex_arrays:
      if flex_array is not None:
        aggregate.extend(flex_array)
    return aggregate

  def count(self, data, root=0):
    """
    Return total `Counter` of occurrences of each element in data across ranks.
    Example: (a1, a1, a2) + (a1, a2, a3) = {a1: 3, a2: 2, a1: 1}
    """
    with adaptive_collective(self.comm.gather, self.comm.allgather) as gather:
      counters = gather(Counter(data), root=root)
    return sum(counters, Counter()) if counters is not None else None

  def sum(self, data, root=0):
    """
    Sum values of data across all ranks.
    Example: a1 + a2 + a3 = a1+a2+a3
    """
    with adaptive_collective(self.comm.reduce, self.comm.allreduce) as reduce:
      return reduce(data, self.MPI.SUM, root=root)

  def set_error(self, description):
    self.error = (self.rank, description)

  def check_errors(self):
    all_errors = self.comm.allreduce([self.error], self.MPI.SUM)
    actual_errors = [error for error in all_errors if error != (None,None)]
    if len(actual_errors) > 0:
      sys.stderr.write("\nAborting MPI process %d because of the following error(s):"%self.rank)
      for error in actual_errors:
        sys.stderr.write("\nError reported by process %d: %s\n"%(error[0], error[1]))
      sys.stderr.flush()
      self.comm.Abort(1)

  def gather_variable_length_numpy_arrays(self, send_arrays, root=0, dtype=float):
    with adaptive_collective(self.comm.gather, self.comm.allgather) as gather:
      lengths = gather(send_arrays.size, root=root)
    gathered_array = np.empty(np.sum(lengths), dtype=dtype) if lengths else None
    with adaptive_collective(self.comm.Gatherv, self.comm.Allgatherv) as gather_v:
      gather_v(sendbuf=send_arrays, recvbuf=(gathered_array, lengths), root=root)
    return gathered_array


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/mpi_logger.py
from __future__ import absolute_import, division, print_function
import os, time

class mpi_logger(object):
  """A class to facilitate each rank writing to its own log file and (optionally) to a special log file for timing"""

  def __init__(self, params=None, mpi_helper=None):
    self.mpi_helper = mpi_helper
    if self.mpi_helper == None:
      from xfel.merging.application.mpi_helper import mpi_helper
      self.mpi_helper = mpi_helper()

    if params:
      self.set_log_file_paths(params)
    else:
      self.main_log_file_path = None
      self.rank_log_file_path = None
      self.timing_file_path = None

    self.log_buffer = self.main_log_buffer = '' # a buffer for handling a run-time case, when the file path isn't known yet, so we need to store the output somewhere

    self.timing_table = dict()

  def set_log_file_paths(self, params):

    if params.output.prefix:
      main_filename = params.output.prefix + "_main"
      aux_filename_prefix = params.output.prefix + "_"
    else:
      main_filename = "main"
      aux_filename_prefix = ""

    self.main_log_file_path = os.path.join(params.output.output_dir, main_filename + ".log")
    self.rank_log_file_path = os.path.join(params.output.output_dir, aux_filename_prefix + 'rank_%06d_%06d.log'%(self.mpi_helper.size, self.mpi_helper.rank))

    if params.output.do_timing:
      self.timing_file_path = os.path.join(params.output.output_dir, aux_filename_prefix + 'timing_%06d_%06d.log'%(self.mpi_helper.size, self.mpi_helper.rank))
    else:
      self.timing_file_path = None

    if params.output.log_level==0: # level 0 is most verbose, so for debug, catch err and out
      import sys, io
      expand_dir = os.path.expandvars(params.output.output_dir)
      os.makedirs(expand_dir, exist_ok=True)
      out_path = os.path.join(expand_dir,"rank_%d.out"%self.mpi_helper.rank)
      err_path = os.path.join(expand_dir,"rank_%d.err"%self.mpi_helper.rank)
      sys.stdout = io.TextIOWrapper(open(out_path,'ab', 0), write_through=True)
      sys.stderr = io.TextIOWrapper(open(err_path,'ab', 0), write_through=True)

  def log(self, message, rank_prepend=True):
    '''Log a rank message'''
    message = time.ctime(self.mpi_helper.time()) + " " + message

    # Prepend the message with the rank index - helpful for grep-ping
    if rank_prepend:
      rank_message = "Rank %d: %s\n"%(self.mpi_helper.rank, message)
    else:
      rank_message = message + "\n"
    self.log_buffer += rank_message

    if self.rank_log_file_path == None:
      return # the file path hasn't been parsed yet, so just keep the output in the buffer

    log_file_handle = open(self.rank_log_file_path, 'a')
    log_file_handle.write(self.log_buffer)
    self.log_buffer = ''
    log_file_handle.close()

  def info(self, message, *args):
    self.log("INFO: "+str(message) % args)

  def debug(self, message, *args):
    self.log("DEBUG: "+str(message) % args)

  def main_log(self, message):
    '''Log a message to the main log file'''

    self.main_log_buffer += (message + '\n')

    if self.main_log_file_path == None:
      return

    log_file_handle = open(self.main_log_file_path, 'a')
    log_file_handle.write(self.main_log_buffer)
    self.main_log_buffer = ''
    log_file_handle.close()

  def log_step_time(self, step, step_finished=False):
    '''Log elapsed time for an execution step'''

    step = step.replace(' ', '_') # for easier log file post-processing

    if not step_finished: # a step has started - cache its start time and return
      if not step in self.timing_table:
        self.timing_table[step] = dict({'single_step':dict({'start':self.mpi_helper.time(), 'elapsed':0.0}),
                                        'cumulative': 0.0
                                       }
                                      )
      else: # if a step is executed repeatedly - re-use the existent step key
        self.timing_table[step]['single_step']['start'] = self.mpi_helper.time()
      return

    # a step has finished - calculate its elapsed and cumulative time (the latter is needed when the step is executed repeatedly)
    #self.mpi_helper.comm.barrier()

    if not step in self.timing_table:
      assert False, "A step has finished, but doesn't have its starting time entry"
      return

    self.timing_table[step]['single_step']['elapsed'] = self.mpi_helper.time() - self.timing_table[step]['single_step']['start']
    self.timing_table[step]['cumulative'] += self.timing_table[step]['single_step']['elapsed']

    # log the elapsed time - single step and cumulative
    if self.timing_file_path == None:
      return

    log_file = open(self.timing_file_path,'a')
    log_file.write("RANK %d %s: %f s %f s\n"%(self.mpi_helper.rank, step, self.timing_table[step]['single_step']['elapsed'], self.timing_table[step]['cumulative']))
    log_file.close()


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/output/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/output/factory.py
from __future__ import division
from xfel.merging.application.output.output import output
from xfel.merging.application.worker import factory as factory_base

class factory(factory_base):
  """Factory class for outputting merged multiple measurements of symmetry-reduced HKLs"""
  @staticmethod
  def from_parameters(params, additional_info=None):
    return [output(params)]


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/phil/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/phil/phil.py
from __future__ import absolute_import, division, print_function

from iotbx.phil import parse

help_message = '''
Redesign script for merging xfel data
'''

dispatch_phil = """
dispatch {
  step_list = None
    .type = strings
    .help = List of steps to use. None means use the full set of steps to merge.
}
"""

input_phil = """
input {
  override_identifiers = False
    .type = bool
    .help = override whatever identifiers may be present in experiments, replacing with auto-generated hash
  alist {
    file = None
      .type = str
      .multiple = True
      .help = Path to a txt file containing experiment tags or experiment filenames to merge (1 per line, all of same type)
      .help = If contents are files: then only experiments whose filename is in the alist will be merged
      .help = If contents are tags: then only experiments whose filename contains one of the alist tags will be merged
    type = *tags files
      .type = choice
      .help = the contents of the input.alist_file, either dials.stills_process image_tags, or absolute expt file paths
      .help = actually the tag can be any unique substring of the expt filenames found under input.path
      .help = Note, for very large datasets, using type=files should perform better
    op = *keep reject
      .type = choice
      .help = whether we keep or reject experiments according to the alist(s)
  }
  persistent_refl_cols = None
    .type = str
    .multiple = True
    .help = Names of reflection table columns that will remain after all prune steps
    .help = If output.save_experiments_and_reflections=True, then these columns will be in the saved tables.
  keep_imagesets = True
    .type = bool
    .help = If True, keep imagesets attached to experiments
  read_image_headers = False
    .type = bool
    .help = If True, when loading data also read image headers. Not needed when merging integrated data.
    .help = Use when needing to read original image pixel data. Equivalent to check_format in other DIALS programs.
  path = None
    .type = str
    .multiple = True
    .help = paths are validated as a glob, directory or file.
    .help = however, validation is delayed until data are assigned to parallel ranks.
    .help = integrated experiments (.expt) and reflection tables (.refl) must both be
    .help = present as matching files.  Only one need be explicitly specified.
  reflections_suffix = _integrated.refl
    .type = str
    .help = Find file names with this suffix for reflections
  experiments_suffix = _integrated.expt
    .type = str
    .help = Find file names with this suffix for experiments

  parallel_file_load {
    method = *uniform node_memory
      .type = choice
      .help = uniform: distribute input experiments/reflections files uniformly over all available ranks
      .help = node_memory: distribute input experiments/reflections files over the nodes such that the node memory limit is not exceeded.
      .help = Within each node distribute the input files uniformly over all ranks of that node.
    node_memory {
      architecture = "Cori KNL"
        .type = str
        .help = node architecture name. Currently not used.
      limit = 90.0
        .type = float
        .help = node memory limit, GB. On Cori KNL each node has 96 GB of memory, but we use 6 GB as a cushion, so the default value is 90 GB.
      pickle_to_memory = 3.5
        .type = float
        .help = an empirical coefficient to convert pickle file size to anticipated run-time process memory required to load a file of that size
    }
    ranks_per_node = 68
        .type = int
        .help = number of MPI ranks per node
    balance = *global1 global2 per_node
      .type = choice
      .multiple = False
      .help = Balance the input file load by distributing experiments uniformly over all available ranks (global1/2) or over the ranks on each node (per_node)
      .help = The idea behind the "per_node" method is that it doesn't require MPI communications across nodes. But if the input file load varies strongly
      .help = between the nodes, "global1/2" is a much better option. "global1" is accomplished by reshuffling all data across all ranks while "global2" is
      .help = accomplished by sending the minimal necessary information between ranks to deterministically evenly balance the load.
    balance_verbose = False
      .type = bool
      .help = print load balancing details to the main log
    balance_mpi_alltoall_slices = 1
      .type = int
      .expert_level = 2
      .help = memory reduction factor for MPI alltoall.
      .help = Use mpi_alltoall_slices > 1, when available RAM is insufficient for doing MPI alltoall on all data at once.
      .help = The data will then be split into mpi_alltoall_slices parts and, correspondingly, alltoall will be performed in mpi_alltoall_slices iterations.
  }
}

mp {
  method = *mpi
    .type = choice
    .help = Muliprocessing method (only mpi at present)
  debug {
    cProfile = False
      .type = bool
      .help = Enable code profiling. Use (for example) runsnake to visualize processing performance
  }
}
"""

tdata_phil = """
tdata{
  output_path = None
    .type = path
    .help = If output_path is not None, the tdata worker writes out a list of unit cells to a file.
    .help = Generally speaking the program should then stop.  The tdata worker is not active by default, so it is necessary to have
    .help = the following phil configuration: dispatch.step_list=input,tdata.
    .help = The output_path assumes the *.tdata filename extension will be appended.
    .help = More information about using this option is given in the source code, xfel/merging/application/tdata/README.md
}
"""

filter_phil = """
filter
  .help = The filter section defines criteria to accept or reject whole experiments
  .help = or to modify the entire experiment by a reindexing operator
  .help = refer to the select section for filtering of individual reflections
  {
  algorithm = n_obs resolution unit_cell
    .type = choice(multi=True)
  n_obs {
    min = None
      .type = int
      .help = Minimum number of observations for subsequent processing
    max = None
      .type = int
      .help = Maximum number of observations for subsequent processing
  }
  resolution {
    d_min = None
      .type = float
      .help = Reject the experiment unless some reflections extend beyond this resolution limit
    model_or_image = model image
      .type = choice
      .help = Calculate resolution either using the scaling model unit cell or from the image itself
  }
  unit_cell
    .help = Various algorithms to restrict unit cell and space group
    {
    algorithm = range *value cluster
      .type = choice
    value
      .help = Discard lattices that are not close to the given target.
      .help = If the target is left as Auto, use the scaling model
      .help = (derived from either PDB file cryst1 record or MTZ header)
      {
      target_unit_cell = Auto
        .type = unit_cell
      relative_length_tolerance = 0.1
        .type = float
        .help = Fractional change in unit cell dimensions allowed (versus target cell).
      absolute_angle_tolerance = 2.
        .type = float
      target_space_group = Auto
        .type = space_group
      }
    cluster
      .help = CLUSTER implies an implementation (standalone program or fork?) where all the
      .help = unit cells are brought together prior to any postrefinement or merging,
      .help = and analyzed in a global sense to identify the isoforms.
      .help = the output of this program could potentially form the a_list for a subsequent
      .help = run where the pre-selected events are postrefined and merged.
      {
      algorithm = rodgriguez_laio dbscan *covariance
        .type = choice
      covariance
        .help = Read a pickle file containing the previously determined clusters,
        .help = represented by estimated covariance models for unit cell parameters.
        {
        file = None
          .type = path
        component = 0
          .type = int(value_min=0)
        skip_component = None
          .type = int(value_min=0)
          .multiple = True
          .help = If a lattice belongs to any of these components, exclude it from processing.
        mahalanobis = 4.0
          .type = float(value_min=0)
          .help = Is essentially the standard deviation cutoff. Given that the unit cells
          .help = are estimated to be distributed by a multivariate Gaussian, this is the
          .help = maximum deviation (in sigmas) from the central value that is allowable for the
          .help = unit cell to be considered part of the cluster.
        skip_mahalanobis = 4.0
          .type = float(value_min=0)
          .help = Cutoff distance for any components specified under skip_component.
        }
      isoform = None
        .type=str
        .help = unknown at present. if there is more than one cluster, such as in PSII,
        .help = perhaps the program should write separate a_lists.
        .help = Alternatively identify a particular isoform to carry forward for merging.
      }
  }
  outlier {
    mad_thresh = None
      .type = float
      .help = If provided, during  the actual merge step, symmetrically equivalent reflecitons (same ASU) are filtered
      .help = according to there median absolute deviation. mad_thresh=3 means a reflection is filtered
      .help = if its deviation is greater than 3 standard deviations of the median amongst ASU samples,
      .help = i.e. lower values of mad_thresh will filter more reflections.
    min_corr = 0.1
      .type = float
      .help = Correlation cutoff for rejecting individual experiments by comparing observed intensities to the model.
      .help = This filter is not applied if scaling.model==None. No experiments are rejected with min_corr=-1.
      .help = This either keeps or rejects the whole experiment.
    assmann_diederichs {}
  }
}
"""

modify_phil = """
modify
  .help = The MODIFY section defines operations on the integrated intensities
  {
  algorithm = *polarization
    .type = choice
    .multiple = True
  reindex_to_reference
    .help = An algorithm to match input experiments against a reference model to
    .help = break an indexing ambiguity
    {
    dataframe = None
      .type = path
      .help = if not None, save a list of which experiments were reindexed (requires pandas)
      .help = and plot a histogram of correlation coefficients (matplotlib)
    }
  cosym
    .help = Implement the ideas of Gildea and Winter doi:10.1107/S2059798318002978
    .help = to determine Laue symmetry from individual symops
    {
    include scope dials.command_line.cosym.phil_scope
    dataframe = None
      .type = path
      .help = if not None, save a list of which experiments were reindexed (requires pandas)
      .help = and plot a histogram of correlation coefficients (matplotlib)
    anchor = False
      .type = bool
      .help = Once the patterns are mutually aligned with the Gildea/Winter/Brehm/Diederichs methodology
      .help = flip the whole set so that it is aligned with a reference model.  For simplicity, the
      .help = reference model from scaling.model is used.  It should be emphasized that the scaling.model
      .help = is only used to choose the overall alignment, which may be chosen arbitrarily, it does not
      .help = bias the mutual alignment of the experimental diffraction patterns.
    tranch_size = 600
      .type = int(value_min=12)
      .help = Best guess as to the ideal tranch size for generating embedding plots.  Total number of
      .help = experiments will be divided among the MPI ranks so as to approximate the tranch size,
      .help = referring to the composite size (each experiment appears in 3 composite tranches).
      .help = Size represents a tradeoff,  larger size will take longer to compute but use fewer MPI ranks
      .help = and converge to more accurate coset assignment (higher % consensus).  Ideal value is data
      .help = dependent, default of 600 is good for a 200-Angstrom cell, but value should increase for smaller cell.
    twin_axis = None
      .type = ints(size=3)
      .multiple = True
      .help = Rotation axis to test as a reindexing operator. Given as a direct \
          axis, e.g. 1,0,0 is a rotation around the a axis.
    twin_rotation = None
      .type = int
      .multiple = True
      .help = Rotation corresponding to a value of twin_axis. Given as an \
          n-fold rotation, e.g. 2 denotes a twofold rotation.
    single_cb_op_to_minimum = False
      .type = bool
      .help = Internally the lattices are transformed to a primitive cell \
          before cosym analysis. Typically each cb_op_to_minimum is determined \
          individually, but if the options twin_axis and twin_rotation are \
          used, the transformation has to be the same for every lattice. \
          Forcing this is probably harmless but has not been tested extensively.
    plot
      {
      do_plot = True
        .type = bool
        .help = Generate embedding plots to assess quality of modify_cosym reindexing.
      n_max = 1
        .type = int
        .help = If shots were divided into tranches for alignment, generate embedding plots for
        .help = the first n_max tranches.
      interactive = False
        .type = bool
        .help = Open embedding plot in Matplotlib window instead of writing a file, overrides do_plot
      format = *png pdf
        .type = choice
        .multiple = False
      filename = cosym_embedding
        .type = str
      }
    }
  reindex_to_abc
    .help = Apply a reindexing operator
    {
      include scope dials.command_line.reindex.phil_scope
    }
}
"""

select_phil = """
select
  .help = The select section accepts or rejects specified reflections
  .help = refer to the filter section for filtering of whole experiments
  {
  algorithm = panel cspad_sensor significance_filter isolation_forest
    .type = choice(multi=True)
  cspad_sensor {
    number = None
      .type = int(value_min=0, value_max=31)
      .multiple = True
      .help = Index in the range(32) specifying sensor on the CSPAD to deselect from merging, for the purpose
      .help = of testing whether an individual sensor is poorly calibrated.
    operation = *deselect select
      .type = choice
      .multiple = True
  }
  significance_filter
    .help = If listed as an algorithm, apply a sigma cutoff (on unmerged data) to limit
    .help = the resolution from each diffraction pattern.
    .help = Implement an alternative filter for fuller-kapton geometry
    {
    n_bins = 12
      .type = int (value_min=2)
      .help = Initial target number of resolution bins for sigma cutoff calculation
    min_ct = 10
      .type = int
      .help = Decrease number of resolution bins to require mean bin population >= min_ct
    max_ct = 50
      .type = int
      .help = Increase number of resolution bins to require mean bin population <= max_ct
    sigma = 0.5
      .type = float
      .help = Remove highest resolution bins such that all accepted bins have <I/sigma> >= sigma
    d_min = None
      .type = float
      .help = Remove the entire lattice if the resolution is not at least this d_min
    }
  reflection_filter
    .help = Algorithms to identify single reflections as anomalies based on resolution and intensity. \
      Isolation Forest and Local Outlier Factor are available algorithms. \
      Refer to scikit-learn for more information: \
        https://scikit-learn.org/stable/modules/outlier_detection.html \
        https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html \
        https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html
    {
    n_bins = 100
      .type = int
      .help = Number of bins used to determine the mean intensity for normalization. Also tails \
        are taken from within bins instead of over the entire dataset.
    tail_percentile = 0.01
      .type = float
      .help = Percentile used to select the tails of the data.
    apply_lower = True
      .type = bool
      .help = Apply reflection filter to the lower tail of the data.
    apply_upper = True
      .type = bool
      .help = Apply reflection filter to the upper tail of the data.
    do_diagnostics = False
      .type = bool
      .help = Make plots of the reflections selected as outliers
    contamination_lower = 0.0001
      .type = float
      .help = Fraction of lower tail reflections that are outliers
    contamination_upper = 0.0001
      .type = float
      .help = Fraction of upper tail reflections that are outliers
    n_estimators = 100
      .type = int
      .help = Number of decision trees in random forest model
    sampling_fraction = 0.1
      .type = float
      .help = Fraction of total dataset subsampled to train each decision tree.
    random_seed = 0
      .type = int
      .help = seed for the random forest model
    }
}
"""

scaling_phil = """
scaling {
  model = None
    .type = str
    .help = PDB filename containing atomic coordinates & isomorphous cryst1 record
    .help = or MTZ filename from a previous cycle. If MTZ, specify mtz.mtz_column_F.
  unit_cell = None
    .type = unit_cell
    .help = Unit cell to be used during scaling and merging. Used if model is not provided
    .help = (e.g. mark1).
  space_group = None
    .type = space_group
    .help = Space group to be used during scaling and merging. Used if model is not provided
    .help = (e.g. mark1).
  model_reindex_op = h,k,l
    .type = str
    .help = Kludge for cases with an indexing ambiguity, need to be able to adjust scaling model
  resolution_scalar = 0.969
    .type = float
    .help = Accommodates a few more miller indices at the high resolution limit to account for
    .help = unit cell variation in the sample. merging.d_min is multiplied by resolution_scalar
    .help = when computing which reflections are within the resolution limit.
  mtz {
    mtz_column_F = fobs
      .type = str
      .help = scaling reference column name containing reference structure factors. Can be
      .help = intensities or amplitudes
    minimum_common_hkls = -1
      .type = int
      .help = minimum required number of common hkls between mtz reference and data
      .help = used to validate mtz-based model. No validation with -1.
  }
  pdb {
    include_bulk_solvent = True
      .type = bool
      .help = Whether to simulate bulk solvent
    k_sol = 0.35
      .type = float
      .help = If model is taken from coordinates, use k_sol for the bulk solvent scale factor
      .help = default is approximate mean value in PDB (according to Pavel)
    b_sol = 46.00
      .type = float
      .help = If model is taken from coordinates, use b_sol for bulk solvent B-factor
      .help = default is approximate mean value in PDB (according to Pavel)
    solvent_algorithm = *mosaic flat
      .type = choice
      .help = Mosaic solvent model is as in https://doi.org/10.1101/2021.12.09.471976
  }
  algorithm = *mark0 mark1
    .type = choice
    .help = "mark0: original per-image scaling by reference to isomorphous PDB model"
    .help = "mark1: no scaling, just averaging (i.e. Monte Carlo
             algorithm).  Individual image scale factors are set to 1."
  weights = *unit icalc icalc_sigma
    .type = choice
    .help = "Sigmas applied in the linear fit of Iobs vs Icalc. unit: sigmas"
            "equal to 1. icalc: Sigmas are proportional to the square root of"
            "Icalc (this relation is totally empirical). icalc_sigma: Error"
            "due to partiality is proportional to Icalc; this term is added to"
            "the sigma(Iobs) determined in integration so that sigma ="
            "sqrt(Icalc**2 + sigma(Iobs)**2)."
}
"""

postrefinement_phil = """
postrefinement {
  enable = True
    .type = bool
    .help = enable the preliminary postrefinement algorithm (monochromatic)
    .expert_level = 3
  algorithm = *rs rs2 rs_hybrid eta_deff
    .type = choice
    .help = rs only, eta_deff protocol 7
    .expert_level = 3
  partiality_threshold_hcfix = 0.2
    .type = float ( value_min = 0.0001 )
    .help = Throw out observations below this value. Hard coded as 0.2 for rs2
    .help = Minimum positive value is required because partiality appears in the denominator
  rs {
    fix = thetax thetay *RS G BFACTOR
      .type = choice(multi=True)
      .help = Which parameters to fix during postrefinement
  }
  rs2
    .help = Reimplement postrefinement with the following (Oct 2016):
    .help = Refinement engine now work on analytical derivatives instead of finite differences
    .help = Better convergence using "traditional convergence test"
    {}
  rs_hybrid
    .help = More aggressive postrefinement with the following (Oct 2016):
    .help = One round of 'rs2' using LBFGS minimizer as above to refine G,B,rotx,roty
    .help = Gentle weighting rather than unit weighting for the postrefinement target
    .help = Second round of LevMar adding an Rs refinement parameter
    .help = Option of weighting the merged terms by partiality
    {
    partiality_threshold = 0.2
      .type = float ( value_min = 0.01 )
      .help = throw out observations below this value. Hard coded as 0.2 for rs2, allow value for hybrid
      .help = must enforce minimum positive value because partiality appears in the denominator
    }
  target_weighting = *unit variance gentle extreme
    .type = choice
    .help = weights for the residuals in the postrefinement target (for rs2 or rs_hybrid)
    .help = Unit: each residual weighted by 1.0
    .help = Variance: weighted by 1/sigma**2.  Doesn't seem right, constructive feedback invited
    .help = Gentle: weighted by |I|/sigma**2.  Seems like best option
    .help = Extreme: weighted by (I/sigma)**2.  Also seems right, but severely downweights weak refl
  merge_weighting = *variance
    .type = choice
    .help = assumed that individual reflections are weighted by the counting variance
  merge_partiality_exponent = 0
    .type = float
    .help = additionally weight each measurement by partiality**exp when merging
    .help = 0 is no weighting, 1 is partiality weighting, 2 is weighting by partiality-squared
  lineshape = *lorentzian gaussian
    .type = choice
    .help = Soft sphere RLP modeled with Lorentzian radial profile as in prime
    .help = or Gaussian radial profile. (for rs2 or rs_hybrid)
  show_trumpet_plot = False
    .type = bool
    .help = each-image trumpet plot showing before-after plot. Spot color warmth indicates I/sigma
    .help = Spot radius for lower plot reflects partiality. Only implemented for rs_hybrid
  delta_corr_limit = None
    .type = float(allow_none=True)
    .help = Experimental: Reject experiments where correlation to reference decreases by more \
        than this value in postrefinement.
}
"""
merging_phil = """
merging {
  minimum_multiplicity = 2
    .type = int(value_min=2)
    .help = If defined, merged structure factors not produced for the Miller indices below this threshold.
  error {
    model = ha14 *ev11 mm24 errors_from_sample_residuals
      .type = choice
      .multiple = False
      .help = ha14, formerly sdfac_auto, apply sdfac to each-image data assuming negative
      .help = intensities are normally distributed noise
      .help = errors_from_sample_residuals, use the distribution of intensities in a given miller index
      .help = to compute the error for each merged reflection
    ev11
      .help = formerly sdfac_refine, correct merged sigmas refining sdfac, sdb and sdadd as Evans 2011.
      {
      random_seed = None
        .help = Random seed. May be int or None. Only used for the simplex minimizer
        .type = int
        .expert_level = 1
      minimizer = *lbfgs LevMar
        .type = choice
        .help = Which minimizer to use while refining the Sdfac terms
      refine_propagated_errors = False
        .type = bool
        .help = If True then during sdfac refinement, also \
                refine the estimated error used for error propagation.
      show_finite_differences = False
        .type = bool
        .help = If True and minimizer is lbfgs, show the finite vs. analytical differences
      plot_refinement_steps = False
        .type = bool
        .help = If True, plot refinement steps during refinement.
    }
    mm24
      .help = Maximum log-likelihood from Mittan-Moreau 2024
      {
      expected_gain = None
        .help = Expected gain used for s_fac initialization.\
                If None, initialize s_fac using routine.
        .type = float
      number_of_intensity_bins = 100
        .help = Number of intensity bins
        .type = int
      tuning_param = 10
        .help = Tuning param for t-dist in maximum log likelihood
        .type = float
      n_max_differences = 100
        .help = Maximum number of pairwise differences per reflection.\
                If None, then do not limit the maximum number of differences
        .type = int
      random_seed = 50298
        .help = Seed used to establish the random number generator for\
                subsampling the pairwise differences.
        .type = int
      tuning_param_opt = False
        .type = bool
        .help = If True, optimize the t-distribution's tuning parameter
      likelihood = normal *t-dist
        .help = Choice for likelihood function.
        .type = choice
        .multiple = False
      cc_after_pr = True
        .type = bool
        .help = If True - use correlation coefficient determined after post-refinement.\
                If False - use correlation coefficient determined before. \
                If post-refinement is not performed, must be False.
      constant_sadd = False
        .type = bool
        .help = If False, parameterize sadd with the correlation coefficient.\
                If True, use a constant sadd.
      do_diagnostics = False
        .type = bool
        .help = Make diagnostic plots.
    }
  }
  plot_single_index_histograms = False
    .type = bool
  set_average_unit_cell = True
    .type = bool
    .help = Output file adopts the unit cell of the data rather than of the reference model.
    .help = How is it determined?  Not a simple average, use a cluster-driven method for
    .help = deriving the best unit cell value.
  d_min = None
    .type = float
    .help = limiting resolution for scaling and merging
  d_max = None
    .type = float
    .help = limiting resolution for scaling and merging.  Implementation currently affects only the CCiso cal
  merge_anomalous = False
    .type = bool
    .help = Merge anomalous contributors
  include_multiplicity_column = False
    .type = bool
    .help = If True, save multiplicity to output mtz as separate column
}
"""

output_phil = """
output {
  expanded_bookkeeping = False
    .type = bool
    .help = if True, and if save_experiments_and_reflections=True, then include in the saved refl tabls:
    .help = 1- modified experiment identifier that contains the image number and lattice number
    .help = 2- index corresponding to the particular reflection in the input file (usually something_integrated.refl)
    .help = 3- the is_odd flag
    .help = 4- the original exp id for the reflection
    .help = 5- a unique number mapping the reflection to its input expFile, refFile pair (see output_dir/file_list_mapping.json)
  prefix = iobs
    .type = str
    .help = Prefix for all output file names
  title = None
    .type = str
    .help = Title for run - will appear in MTZ file header
  output_dir = .
    .type = str
    .help = output file directory
  tmp_dir = None
    .type = str
    .help = temporary file directory
  do_timing = False
    .type = bool
    .help = When True, calculate and log elapsed time for execution steps
  log_level = 1
    .type = int
    .help = determines how much information to log. Level 0 means: log all, while a non-zero level reduces the logging amount.
  save_experiments_and_reflections = False
    .type = bool
    .help = If True, dump the final set of experiments and reflections from the last worker
}
"""

statistics_phil = """
statistics {
  shuffle_ids = False
    .type = bool
    .help = shuffle the IDs when dividing into even/odd. This adds variation to half dataset stats like CC1/2
  n_bins = 10
    .type = int(value_min=1)
    .help = Number of resolution bins in statistics table
  cc1_2 {
    hash_filenames = False
      .type = bool
      .help = For CC1/2, instead of using odd/even filenames to split images into two sets,
      .help = hash the filename using md5 and split the images using odd/even hashes.
  }
  cciso {
    mtz_file = None
      .type = str
      .help = The isomorphous reference structure factors for Riso/ CCiso.
      .help = a fake file is written out to this file name if model is None
      .help = The experimental reference file can be old type *.mtz or new *sf.cif
    mtz_column_F = fobs
      .type = str
      .help = For Riso/ CCiso, the array name containing reference structure factors
      .help = As an example, could be intensity from *sf.cif
      .help = Specifically this is a tag searched for in the array name, could be 'tensity' from 'intensity'
  }
  deltaccint
    .help = Parameters used when computing CC (aka CC internal), a means of filtering out lattices that
    .help = degrade the overall CC. Uses the - method from Assmann 2016 to avoid splitting the data into
    .help = odd/even datasets. Enable by adding the deltaccint worker after the group worker.
  {
    iqr_ratio = 10
      .type = float
      .help = If the CC filter is enabled, first compute CC when removing every image one at a time, then
      .help = compute the IQR of these CCs. Remove all lattices whose contribution degrades CC by more than
      .help = IQR * iqr_ratio above the median. You can discover a good IQR by running the program once,
      .help = examining the log file where possible values are listed, and running it again with the best value.
    verbose = False
      .type = bool
      .help = If True, include the CC for every lattice in the main log.
  }
  predictions_to_edge {
    apply = False
      .type = bool
      .help = If True and key 'indices_to_edge' not found in integration pickles, predictions
      .help = will be made to the edge of the detector based on current unit cell, orientation,
      .help = and mosaicity.
    image = None
      .type = path
      .help = Path to an example image from which to extract active areas and pixel size.
    detector_phil = None
      .type = path
      .help = Path to the detector version phil file used to generate the selected data.
  }
  report_ML = True
    .type = bool
    .help = Report statistics on per-frame attributes modeled by max-likelihood fit (expert only).
  uc_precision = 2
    .type = int
    .help = Decimal places for unit cell statistics
}
"""

group_phil = """
parallel {
  a2a = 1
    .type = int
    .expert_level = 2
    .help = memory reduction factor for MPI alltoall.
    .help = Use a2a > 1, when available RAM is insufficient for doing MPI alltoall on all data at once.
    .help = The data will be split into a2a parts and, correspondingly, alltoall will be performed in a2a iterations.
}
"""

publish_phil = """
publish {
  include scope xfel.command_line.upload_mtz.phil_scope
}
"""

lunus_phil = """
lunus {
  deck_file = None
    .type = path
}
"""

diffbragg_phil = """
diffBragg {
  include scope simtbx.diffBragg.phil.phil_scope
}
"""

monitor_phil = """
monitor {
  detail = *rank node rank0 none
    .type = choice
    .help = Detail of data to be collected: from every rank, from rank 0 only,
    .help = from first rank on every node, or none.
  period = 5.0
    .type = float
    .help = Interval between subsequent resource statistics checks in seconds.
    .help = Short periods might lead to inconsistent logging.
  plot = True
    .type = bool
    .help = Plot the resource usage history after the monitor is stopped.
  prefix = monitor
    .type = str
    .help = Filename prefix for log files and summary plot.
  write = True
    .type = bool
    .help = Write collected resource information to log files.
}
"""


filter_global_phil = """
filter_global {
  intensity_extrema_iqr_dist_threshold = 1000.0
    .type = float(value_min=0, value_max=None)
    .help = Maximum tolerated deviation of max(intensity.sum.value)
    .help = and min(intensity.sum.value) from expts' population's respective
    .help = medians, expressed in population's interquartile range units.
}
"""


# A place to override any defaults included from elsewhere
program_defaults_phil_str = """
modify.cosym.use_curvatures=False
"""

master_phil = dispatch_phil + input_phil + tdata_phil + filter_phil + modify_phil + \
              select_phil + scaling_phil + postrefinement_phil + merging_phil + \
              output_phil + statistics_phil + group_phil + lunus_phil + \
              publish_phil + diffbragg_phil + monitor_phil + filter_global_phil

import os, importlib
custom_phil_pathstr = os.environ.get('XFEL_CUSTOM_WORKER_PATH')
if custom_phil_pathstr is not None and os.path.isdir(custom_phil_pathstr):
  for dir in os.listdir(custom_phil_pathstr):
    path = os.path.join(custom_phil_pathstr, dir, 'phil.py')
    if not os.path.isfile(path): continue
    spec = importlib.util.spec_from_file_location('_', path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    master_phil += module.phil_str


phil_scope = parse(master_phil, process_includes = True)
phil_scope = phil_scope.fetch(parse(program_defaults_phil_str))

class Script(object):
  '''A class for running the script.'''

  def __init__(self):
    # The script usage
    import libtbx.load_env
    self.usage = "usage: %s [options] [param.phil] " % libtbx.env.dispatcher_name
    self.parser = None

  def initialize(self):
    '''Initialise the script.'''
    from dials.util.options import ArgumentParser
    # Create the parser
    self.parser = ArgumentParser(
      usage=self.usage,
      phil=phil_scope,
      epilog=help_message)
    self.parser.add_option(
        '--plots',
        action='store_true',
        default=False,
        dest='show_plots',
        help='Show some plots.')

    # Parse the command line. quick_parse is required for MPI compatibility
    params, options = self.parser.parse_args(show_diff_phil=True,quick_parse=True)
    self.params = params
    self.options = options

  def validate(self):
    from xfel.merging.application.validation.application import application
    application(self.params)

  def modify(self, experiments, reflections):
    return experiments, reflections #nop

  def run(self):
    print('''Initializing and validating phil...''')

    self.initialize()
    self.validate()

    # do other stuff
    return

if __name__ == '__main__':
  script = Script()
  result = script.run()
  print ("OK")


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/postrefine/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/postrefine/factory.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.postrefine.postrefinement_rs import postrefinement_rs
from xfel.merging.application.postrefine.postrefinement_rs2 import postrefinement_rs2
from xfel.merging.application.worker import factory as factory_base

class factory(factory_base):
  """ Factory class for post-refining experiments. """
  @staticmethod
  def from_parameters(params, additional_info=None, mpi_helper=None, mpi_logger=None):
    """ """
    if params.postrefinement.algorithm in ['rs','eta_deff']:
      return [postrefinement_rs(params, mpi_helper, mpi_logger)]
    elif params.postrefinement.algorithm in ['rs2']:
      return [postrefinement_rs2(params, mpi_helper, mpi_logger)]

    '''
    FROM CXI-MERGE
    if params.postrefinement.enable:
      if params.postrefinement.algorithm in ['rs','eta_deff']:
        from xfel.cxi.postrefinement_legacy_rs import legacy_rs as postrefinement
      elif params.postrefinement.algorithm in ['rs2']:
        from xfel.cxi.postrefinement_updated_rs import updated_rs as postrefinement_algorithm
      elif self.params.postrefinement.algorithm in ['rs_hybrid']:
        from xfel.cxi.postrefinement_hybrid_rs import rs_hybrid as postrefinement_algorithm
      return [postrefinement_algorithm]
    return None
    '''


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/postrefine/postrefinement_rs.py
from __future__ import absolute_import, division, print_function
import six
from six.moves import range
from six.moves import cStringIO as StringIO
from collections import Counter
import math
from xfel.merging.application.worker import worker
from libtbx import adopt_init_args, group_args
from dials.array_family import flex
from dxtbx.model.experiment_list import ExperimentList
from cctbx import miller
from cctbx.crystal import symmetry
from scitbx import matrix
from scitbx.math.tests.tst_weighted_correlation import simple_weighted_correlation
from cctbx.crystal_orientation import crystal_orientation, basis_type

class postrefinement_rs(worker):

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(postrefinement_rs, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Postrefinement'

  def run(self, experiments, reflections):
    self.logger.log_step_time("POSTREFINEMENT")
    if (not self.params.postrefinement.enable) or (self.params.scaling.algorithm != "mark0"): # mark1 implies no scaling/post-refinement
      self.logger.log("No post-refinement was done")
      if self.mpi_helper.rank == 0:
        self.logger.main_log("No post-refinement was done")
      return experiments, reflections

    target_symm = symmetry(unit_cell = self.params.scaling.unit_cell, space_group_info = self.params.scaling.space_group)
    i_model = self.params.scaling.i_model
    miller_set = self.params.scaling.miller_set

    # Ensure that match_multi_indices() will return identical results
    # when a frame's observations are matched against the
    # pre-generated Miller set, miller_set, and the reference
    # data set, i_model.  The implication is that the same match
    # can be used to map Miller indices to array indices for intensity
    # accumulation, and for determination of the correlation
    # coefficient in the presence of a scaling reference.
    assert len(i_model.indices()) == len(miller_set.indices())
    assert (i_model.indices() == miller_set.indices()).count(False) == 0

    new_experiments = ExperimentList()
    new_reflections = flex.reflection_table()

    experiments_rejected_by_reason = Counter()  # reason:how_many_rejected

    for expt_id, experiment in enumerate(experiments):

      exp_reflections = reflections.select(reflections['id'] == expt_id)

      # Build a miller array with _original_ miller indices of the experiment reflections
      exp_miller_indices_original = miller.set(target_symm, exp_reflections['miller_index'], not self.params.merging.merge_anomalous)
      observations_original_index = miller.array(exp_miller_indices_original, exp_reflections['intensity.sum.value'], flex.sqrt(exp_reflections['intensity.sum.variance']))

      assert exp_reflections.size() == exp_miller_indices_original.size()
      assert observations_original_index.size() == exp_miller_indices_original.size()

      # Build a miller array with _asymmetric_ miller indices of the experiment reflections
      exp_miller_indices_asu = miller.set(target_symm, exp_reflections['miller_index_asymmetric'], True)
      observations = miller.array(exp_miller_indices_asu, exp_reflections['intensity.sum.value'], flex.sqrt(exp_reflections['intensity.sum.variance']))

      matches = miller.match_multi_indices(miller_indices_unique = miller_set.indices(), miller_indices = observations.indices())

      pair1 = flex.int([pair[1] for pair in matches.pairs()]) # refers to the observations
      pair0 = flex.int([pair[0] for pair in matches.pairs()]) # refers to the model

      # narrow things down to the set that matches, only
      observations_pair1_selected = observations.customized_copy(indices = flex.miller_index([observations.indices()[p] for p in pair1]),
                                                                 data = flex.double([observations.data()[p] for p in pair1]),
                                                                 sigmas = flex.double([observations.sigmas()[p] for p in pair1]))

      observations_original_index_pair1_selected = observations_original_index.customized_copy(indices = flex.miller_index([observations_original_index.indices()[p] for p in pair1]),
                                                                                               data = flex.double([observations_original_index.data()[p] for p in pair1]),
                                                                                               sigmas = flex.double([observations_original_index.sigmas()[p] for p in pair1]))
      I_observed = observations_pair1_selected.data()
      MILLER = observations_original_index_pair1_selected.indices()

      ORI = crystal_orientation(experiment.crystal.get_A(), basis_type.reciprocal)
      Astar = matrix.sqr(ORI.reciprocal_matrix())
      Astar_from_experiment = matrix.sqr(experiment.crystal.get_A())
      assert Astar == Astar_from_experiment

      WAVE = experiment.beam.get_wavelength()
      BEAM = matrix.col((0.0,0.0,-1./WAVE))
      BFACTOR = 0.
      MOSAICITY_DEG = experiment.crystal.get_half_mosaicity_deg()
      DOMAIN_SIZE_A = experiment.crystal.get_domain_size_ang()

      # calculation of correlation here
      I_reference = flex.double([i_model.data()[pair[0]] for pair in matches.pairs()])
      I_invalid = flex.bool([i_model.sigmas()[pair[0]] < 0. for pair in matches.pairs()])
      use_weights = False # New facility for getting variance-weighted correlation

      if use_weights:
        # variance weighting
        I_weight = flex.double([1./(observations_pair1_selected.sigmas()[pair[1]])**2 for pair in matches.pairs()])
      else:
        I_weight = flex.double(len(observations_pair1_selected.sigmas()), 1.)

      I_weight.set_selected(I_invalid, 0.)

      """Explanation of 'include_negatives' semantics as originally implemented in cxi.merge postrefinement:
         include_negatives = True
         + and - reflections both used for Rh distribution for initial estimate of RS parameter
         + and - reflections both used for calc/obs correlation slope for initial estimate of G parameter
         + and - reflections both passed to the refinery and used in the target function (makes sense if
                             you look at it from a certain point of view)

         include_negatives = False
         + and - reflections both used for Rh distribution for initial estimate of RS parameter
         +       reflections only used for calc/obs correlation slope for initial estimate of G parameter
         + and - reflections both passed to the refinery and used in the target function (makes sense if
                             you look at it from a certain point of view)

         NOTE: by the new design, "include negatives" is always True
      """

      SWC = simple_weighted_correlation(I_weight, I_reference, I_observed)
      if self.params.output.log_level == 0:
        self.logger.log("Old correlation is: %f"%SWC.corr)

      if self.params.postrefinement.algorithm == "rs":

        Rhall = flex.double()

        for mill in MILLER:
          H = matrix.col(mill)
          Xhkl = Astar*H
          Rh = ( Xhkl + BEAM ).length() - (1./WAVE)
          Rhall.append(Rh)

        Rs = math.sqrt(flex.mean(Rhall*Rhall))

        RS = 1./10000. # reciprocal effective domain size of 1 micron
        RS = Rs        # try this empirically determined approximate, monochrome, a-mosaic value
        current = flex.double([SWC.slope, BFACTOR, RS, 0., 0.])

        parameterization_class = rs_parameterization
        refinery = rs_refinery(ORI=ORI, MILLER=MILLER, BEAM=BEAM, WAVE=WAVE, ICALCVEC = I_reference, IOBSVEC = I_observed)

      elif self.params.postrefinement.algorithm == "eta_deff":

        eta_init = 2. * MOSAICITY_DEG * math.pi/180.
        D_eff_init = 2. * DOMAIN_SIZE_A
        current = flex.double([SWC.slope, BFACTOR, eta_init, 0., 0., D_eff_init])

        parameterization_class = eta_deff_parameterization
        refinery = eta_deff_refinery(ORI=ORI, MILLER=MILLER, BEAM=BEAM, WAVE=WAVE, ICALCVEC = I_reference, IOBSVEC = I_observed)

      func = refinery.fvec_callable(parameterization_class(current))
      functional = flex.sum(func * func)

      if self.params.output.log_level == 0:
        self.logger.log("functional: %f"%functional)

      self.current = current;
      self.parameterization_class = parameterization_class
      self.refinery = refinery;

      self.observations_pair1_selected = observations_pair1_selected;
      self.observations_original_index_pair1_selected = observations_original_index_pair1_selected

      error_detected = False

      try:
        self.run_plain()

        result_observations_original_index, result_observations, result_matches = self.result_for_cxi_merge()

        assert result_observations_original_index.size() == result_observations.size()
        assert result_matches.pairs().size() == result_observations_original_index.size()
        # Calculate the correlation of each frame after corrections.
        # This is used in the MM24 error model to determine a per frame level of error
        # These are the added to the reflection table
        I_observed = result_observations.data()
        matches = miller.match_multi_indices(
          miller_indices_unique = miller_set.indices(),
          miller_indices = result_observations.indices()
        )
        I_reference = flex.double([i_model.data()[pair[0]] for pair in matches.pairs()])
        I_invalid = flex.bool([i_model.sigmas()[pair[0]] < 0. for pair in matches.pairs()])
        I_weight = flex.double(len(result_observations.sigmas()), 1.)
        I_weight.set_selected(I_invalid, 0.)
        SWC_after_post = simple_weighted_correlation(I_weight, I_reference, I_observed)
        dcl = self.params.postrefinement.delta_corr_limit
        if dcl is not None and SWC_after_post.corr - SWC.corr < -(dcl):
          raise ValueError("CC decreased by > {} in postrefinement".format(dcl))
      except (AssertionError, ValueError, RuntimeError) as e:
        error_detected = True
        reason = repr(e)
        if not reason:
          reason = "Unknown error"
        experiments_rejected_by_reason[reason] += 1

      if not error_detected:
        new_experiments.append(experiment)

        new_exp_reflections = flex.reflection_table()
        new_exp_reflections['miller_index_asymmetric']  = result_observations.indices()
        new_exp_reflections['intensity.sum.value']      = result_observations.data()
        new_exp_reflections['intensity.sum.variance']   = flex.pow(result_observations.sigmas(),2)
        new_exp_reflections['id']                       = flex.int(len(new_exp_reflections), len(new_experiments)-1)
        new_exp_reflections.experiment_identifiers()[len(new_experiments)-1] = experiment.identifier

        # The original reflection table, i.e. the input to this run() method, has more columns than those used
        # for the postrefinement ("data" and "sigma" in the miller arrays). The problems is: some of the input reflections may have been rejected by now.
        # So to bring those extra columns over to the new reflection table, we have to create a subset of the original exp_reflections table,
        # which would match (by original miller indices) the miller array results of the postrefinement.
        match_original_indices = miller.match_multi_indices(miller_indices_unique = exp_miller_indices_original.indices(), miller_indices = result_observations_original_index.indices())
        exp_reflections_match_results = exp_reflections.select(match_original_indices.pairs().column(0))
        assert (exp_reflections_match_results['intensity.sum.value'] == result_observations_original_index.data()).count(False) == 0
        new_exp_reflections['intensity.sum.value.unmodified'] = exp_reflections_match_results['intensity.sum.value.unmodified']
        new_exp_reflections['intensity.sum.variance.unmodified'] = exp_reflections_match_results['intensity.sum.variance.unmodified']
        for key in self.params.input.persistent_refl_cols:
          if not key in new_exp_reflections.keys() and key in exp_reflections_match_results.keys():
            new_exp_reflections[key] = exp_reflections_match_results[key]
        new_exp_reflections["correlation_after_post"] = flex.double(len(new_exp_reflections), SWC_after_post.corr)
        new_reflections.extend(new_exp_reflections)

    # report rejected experiments, reflections
    experiments_rejected_by_postrefinement = len(experiments) - len(new_experiments)
    reflections_rejected_by_postrefinement = reflections.size() - new_reflections.size()

    self.logger.log("Experiments rejected by post-refinement: %d"%experiments_rejected_by_postrefinement)
    self.logger.log("Reflections rejected by post-refinement: %d"%reflections_rejected_by_postrefinement)

    for reason, count in six.iteritems(experiments_rejected_by_reason):
      self.logger.log("Experiments rejected due to %s: %d"%(reason,count))

    # Now that each rank has all reasons from all ranks, we can treat the reasons in a uniform way.
    total_experiments_rejected_by_reason = self.mpi_helper.count(experiments_rejected_by_reason)
    total_accepted_experiment_count = self.mpi_helper.sum(len(new_experiments))

    # how many reflections have we rejected due to post-refinement?
    rejected_reflections = len(reflections) - len(new_reflections);
    total_rejected_reflections = self.mpi_helper.sum(rejected_reflections)

    if self.mpi_helper.rank == 0:
      for reason, count in six.iteritems(total_experiments_rejected_by_reason):
        self.logger.main_log("Total experiments rejected due to %s: %d"%(reason,count))
      self.logger.main_log("Total experiments accepted: %d"%total_accepted_experiment_count)
      self.logger.main_log("Total reflections rejected due to post-refinement: %d"%total_rejected_reflections)

    self.logger.log_step_time("POSTREFINEMENT", True)

    # Do we have any data left?
    from xfel.merging.application.utils.data_counter import data_counter
    data_counter(self.params).count(new_experiments, new_reflections)

    return new_experiments, new_reflections

  def run_plain(self):

    out = StringIO()
    self.MINI = lbfgs_minimizer_base(self.params,
                                     current_x = self.current,
                                     parameterization = self.parameterization_class,
                                     refinery = self.refinery,
                                     out = out)
    if self.params.output.log_level == 0:
      self.logger.log("\n" + out.getvalue())

  def result_for_cxi_merge(self):

    scaler = self.refinery.scaler_callable(self.parameterization_class(self.MINI.x))

    if self.params.postrefinement.algorithm == "rs":
      fat_selection = (self.refinery.lorentz_callable(self.parameterization_class(self.MINI.x)) > self.params.postrefinement.partiality_threshold_hcfix)
      fats = self.refinery.lorentz_callable(self.parameterization_class(self.MINI.x))
    else:
      fat_selection = (self.refinery.lorentz_callable(self.parameterization_class(self.MINI.x)) < 0.9)

    fat_count = fat_selection.count(True)

    # reject an experiment with insufficient number of near-full reflections
    if fat_count < 3:

      if self.params.output.log_level == 0:
        self.logger.log("Rejected experiment, because: On total %5d the fat selection is %5d"%(len(self.observations_pair1_selected.indices()), fat_count))

      '''
      # debugging
      rejected_fat_max = 0.0
      for fat in fats:
        if fat <= 0.2:
          if fat > rejected_fat_max:
            rejected_fat_max = fat
      self.logger.log("MAXIMUM FAT VALUE AMONG REJECTED REFLECTIONS IS: %f"%rejected_fat_max)
      '''

      raise ValueError("< 3 near-fulls after refinement")

    if self.params.output.log_level == 0:
      self.logger.log("On total %5d the fat selection is %5d"%(len(self.observations_pair1_selected.indices()), fat_count))

    observations_original_index = self.observations_original_index_pair1_selected.select(fat_selection)

    observations = self.observations_pair1_selected.customized_copy(indices = self.observations_pair1_selected.indices().select(fat_selection),
                                                                    data = (self.observations_pair1_selected.data()/scaler).select(fat_selection),
                                                                    sigmas = (self.observations_pair1_selected.sigmas()/scaler).select(fat_selection))

    matches = miller.match_multi_indices(miller_indices_unique = self.params.scaling.miller_set.indices(),
                                         miller_indices = observations.indices())

    return observations_original_index, observations, matches

  def get_parameter_values(self):
    values = self.parameterization_class(self.MINI.x)
    return values

  def result_for_samosa(self):
    values = self.parameterization_class(self.MINI.x)
    return self.refinery.get_eff_Astar(values), values.RS

class refinery_base(group_args):
    def __init__(self, **kwargs):
      group_args.__init__(self,**kwargs)
      mandatory = ["ORI","MILLER","BEAM","WAVE","ICALCVEC","IOBSVEC"]
      for key in mandatory: getattr(self,key)
      self.DSSQ = self.ORI.unit_cell().d_star_sq(self.MILLER)

    """Refinery class takes reference and observations, and implements target
    functions and derivatives for a particular model paradigm."""
    def get_Rh_array(self, values):
      eff_Astar = self.get_eff_Astar(values)
      h = self.MILLER.as_vec3_double()
      x = flex.mat3_double(len(self.MILLER), eff_Astar) * h
      Svec = x + self.BEAM
      Rh = Svec.norms() - (1./self.WAVE)
      return Rh

    def get_s1_array(self, values):
      miller_vec = self.MILLER.as_vec3_double()
      ref_ori = matrix.sqr(self.ORI.reciprocal_matrix())
      Rx = matrix.col((1,0,0)).axis_and_angle_as_r3_rotation_matrix(values.thetax)
      Ry = matrix.col((0,1,0)).axis_and_angle_as_r3_rotation_matrix(values.thetay)
      s_array = flex.mat3_double(len(self.MILLER),Ry * Rx * ref_ori) * miller_vec
      s1_array = s_array + flex.vec3_double(len(self.MILLER), self.BEAM)
      return s1_array

    def get_eff_Astar(self, values):
      thetax = values.thetax; thetay = values.thetay;
      effective_orientation = self.ORI.rotate_thru((1,0,0),thetax
         ).rotate_thru((0,1,0),thetay
         )
      return matrix.sqr(effective_orientation.reciprocal_matrix())

    def scaler_callable(self, values):
      PB = self.get_partiality_array(values)
      EXP = flex.exp(-2.*values.BFACTOR*self.DSSQ)
      terms = values.G * EXP * PB
      return terms

    def fvec_callable(self, values):
      PB = self.get_partiality_array(values)
      EXP = flex.exp(-2.*values.BFACTOR*self.DSSQ)
      terms = (values.G * EXP * PB * self.ICALCVEC - self.IOBSVEC)
      # Ideas for improvement
      #   straightforward to also include sigma weighting
      #   add extra terms representing rotational excursion: terms.concatenate(1.e7*Rh)
      return terms

class rs_refinery(refinery_base):
    def lorentz_callable(self,values):
      return self.get_partiality_array(values)

    def get_partiality_array(self,values):
      rs = values.RS
      Rh = self.get_Rh_array(values)
      rs_sq = rs*rs
      PB = rs_sq / ((2. * (Rh * Rh)) + rs_sq)
      return PB

class eta_deff_refinery(refinery_base):
    def __init__(self, **kwargs):
      refinery_base.__init__(self,**kwargs)
      self.DVEC = self.ORI.unit_cell().d(self.MILLER)

    def lorentz_callable(self,values):
      Rh = self.get_Rh_array(values)
      Rs = flex.double(len(self.MILLER),1./values.DEFF)+flex.double(len(self.MILLER),values.ETA/2.)/self.DVEC
      ratio = Rh / Rs
      ratio_abs = flex.abs(ratio)
      return ratio_abs

    def get_partiality_array(self,values):
      Rh = self.get_Rh_array(values)
      Rs = flex.double(len(self.MILLER),1./values.DEFF)+flex.double(len(self.MILLER),values.ETA/2.)/self.DVEC
      Rs_sq = Rs * Rs
      Rh_sq = Rh * Rh
      numerator = Rs_sq - Rh_sq
      denominator = values.DEFF * Rs * Rs_sq
      partiality = numerator / denominator
      return partiality

class unpack_base(object):
  "abstract interface"
  def __init__(YY,values):
    YY.reference = values # simply the flex double list of parameters
    YY.keys = None # override
  def __getattr__(YY,item):
    if item not in YY.keys:
      raise AttributeError(item)
    return YY.reference[YY.keys.index(item)]
  def show(values,out):
    raise NotImplementedError

class rs_parameterization(unpack_base):
  def __init__(YY,values):
    super(rs_parameterization, YY).__init__(values)
    YY.keys = ['G', 'BFACTOR','RS','thetax','thetay']

  def show(YY, out):
    print ("G: %10.7f; B: %10.7f; RS: %10.7f; THETAX: %7.3f deg; THETAY: %7.3f deg"\
          %(YY.G, YY.BFACTOR, YY.RS, 180.*YY.thetax/math.pi, 180.*YY.thetay/math.pi)\
          , file=out)

class eta_deff_parameterization(unpack_base):
  def __init__(YY,values):
    super(eta_deff_parameterization, YY).__init__(values)
    YY.keys = ['G', 'BFACTOR','ETA','thetax','thetay','DEFF']

  def show(YY, out):
    print ("G: %10.7f; B: %10.7f; eta: %10.7f; Deff %10.2f; THETAX: %7.3f deg; THETAY: %7.3f deg"\
          %(YY.G, YY.BFACTOR, YY.ETA, YY.DEFF, 180.*YY.thetax/math.pi, 180.*YY.thetay/math.pi)\
          , file=out)

class lbfgs_minimizer_base:

  def __init__(self, params, current_x=None, parameterization=None, refinery=None, out=None,
               min_iterations=0, max_calls=1000, max_drop_eps=1.e-5):
    adopt_init_args(self, locals())
    self.n = current_x.size()
    self.x = current_x
    from scitbx import lbfgs
    self.minimizer = lbfgs.run(
      target_evaluator=self,
      termination_params=lbfgs.termination_parameters(
        traditional_convergence_test=False,
        drop_convergence_test_max_drop_eps=max_drop_eps,
        min_iterations=min_iterations,
        max_iterations = None,
        max_calls=max_calls),
      exception_handling_params=lbfgs.exception_handling_parameters(
         ignore_line_search_failed_rounding_errors=True,
         ignore_line_search_failed_step_at_lower_bound=True,#the only change from default
         ignore_line_search_failed_step_at_upper_bound=False,
         ignore_line_search_failed_maxfev=False,
         ignore_line_search_failed_xtol=False,
         ignore_search_direction_not_descent=False)
      )

  def compute_functional_and_gradients(self):
    values = self.parameterization(self.x)
    assert -150. < values.BFACTOR < 150. # limits on the exponent, please
    self.func = self.refinery.fvec_callable(values)
    functional = flex.sum(self.func*self.func)
    self.f = functional
    DELTA = 1.E-7
    self.g = flex.double()
    for x in range(self.n):
      templist = list(self.x)
      templist[x]+=DELTA
      dvalues = flex.double(templist)

      dfunc = self.refinery.fvec_callable(self.parameterization(dvalues))
      dfunctional = flex.sum(dfunc*dfunc)
      #calculate by finite_difference
      self.g.append( ( dfunctional-functional )/DELTA )

    if self.params.postrefinement.algorithm == 'rs':
      for p in self.params.postrefinement.rs.fix:
        self.g[values.keys.index(p)] = 0

    print ("rms %10.3f; "%math.sqrt(flex.mean(self.func*self.func)), file=self.out, end='')
    values.show(self.out)

    return self.f, self.g

  def __del__(self):
    values = self.parameterization(self.x)
    print ("FINALMODEL", file=self.out)
    print ("rms %10.3f; "%math.sqrt(flex.mean(self.func*self.func)), file=self.out, end='')
    values.show(self.out)

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(postrefinement_rs)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/postrefine/postrefinement_rs2.py
from __future__ import absolute_import, division, print_function
import six
from six.moves import range
from six.moves import cStringIO as StringIO
from collections import Counter
import math
from libtbx import adopt_init_args
from dials.array_family import flex
from dxtbx.model.experiment_list import ExperimentList
from cctbx import miller
from cctbx.crystal import symmetry
from scitbx import matrix
from scitbx.math.tests.tst_weighted_correlation import simple_weighted_correlation
from cctbx.crystal_orientation import crystal_orientation, basis_type
from xfel.merging.application.postrefine.postrefinement_rs import postrefinement_rs, rs_refinery, rs_parameterization, lbfgs_minimizer_base

def chosen_weights(observation_set, params):
    data = observation_set.data()
    sigmas = observation_set.sigmas()
    return {
      "unit": flex.double(len(data),1.),
      "variance": 1./(sigmas*sigmas),
      "gentle": flex.pow(flex.sqrt(flex.abs(data))/sigmas,2),
      "extreme": flex.pow(data/sigmas,2)
    } [ params.postrefinement.target_weighting ]

class postrefinement_rs2(postrefinement_rs):

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(postrefinement_rs2, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Postrefinement'

  def run(self, experiments, reflections):
    self.logger.log_step_time("POSTREFINEMENT")
    if (not self.params.postrefinement.enable) or (self.params.scaling.algorithm != "mark0"): # mark1 implies no scaling/post-refinement
      self.logger.log("No post-refinement was done")
      if self.mpi_helper.rank == 0:
        self.logger.main_log("No post-refinement was done")
      return experiments, reflections

    target_symm = symmetry(unit_cell = self.params.scaling.unit_cell, space_group_info = self.params.scaling.space_group)
    i_model = self.params.scaling.i_model
    miller_set = self.params.scaling.miller_set

    # Ensure that match_multi_indices() will return identical results
    # when a frame's observations are matched against the
    # pre-generated Miller set, miller_set, and the reference
    # data set, i_model.  The implication is that the same match
    # can be used to map Miller indices to array indices for intensity
    # accumulation, and for determination of the correlation
    # coefficient in the presence of a scaling reference.
    assert len(i_model.indices()) == len(miller_set.indices())
    assert (i_model.indices() == miller_set.indices()).count(False) == 0

    new_experiments = ExperimentList()
    new_reflections = flex.reflection_table()

    experiments_rejected_by_reason = Counter()  # reason:how_many_rejected

    for expt_id, experiment in enumerate(experiments):

      exp_reflections = reflections.select(reflections['id'] == expt_id)

      # Build a miller array with _original_ miller indices of the experiment reflections
      exp_miller_indices_original = miller.set(target_symm, exp_reflections['miller_index'], not self.params.merging.merge_anomalous)
      observations_original_index = miller.array(exp_miller_indices_original, exp_reflections['intensity.sum.value'], flex.sqrt(exp_reflections['intensity.sum.variance']))

      assert exp_reflections.size() == exp_miller_indices_original.size()
      assert observations_original_index.size() == exp_miller_indices_original.size()

      # Build a miller array with _asymmetric_ miller indices of the experiment reflections
      exp_miller_indices_asu = miller.set(target_symm, exp_reflections['miller_index_asymmetric'], True)
      observations = miller.array(exp_miller_indices_asu, exp_reflections['intensity.sum.value'], flex.sqrt(exp_reflections['intensity.sum.variance']))

      matches = miller.match_multi_indices(miller_indices_unique = miller_set.indices(), miller_indices = observations.indices())

      pair1 = flex.int([pair[1] for pair in matches.pairs()]) # refers to the observations
      pair0 = flex.int([pair[0] for pair in matches.pairs()]) # refers to the model

      # narrow things down to the set that matches, only
      observations_pair1_selected = observations.customized_copy(indices = flex.miller_index([observations.indices()[p] for p in pair1]),
                                                                 data = flex.double([observations.data()[p] for p in pair1]),
                                                                 sigmas = flex.double([observations.sigmas()[p] for p in pair1]))

      observations_original_index_pair1_selected = observations_original_index.customized_copy(indices = flex.miller_index([observations_original_index.indices()[p] for p in pair1]),
                                                                                               data = flex.double([observations_original_index.data()[p] for p in pair1]),
                                                                                               sigmas = flex.double([observations_original_index.sigmas()[p] for p in pair1]))
###################
      I_observed = observations_pair1_selected.data()
      chosen = chosen_weights(observations_pair1_selected, self.params)

      MILLER = observations_original_index_pair1_selected.indices()
      ORI = crystal_orientation(experiment.crystal.get_A(), basis_type.reciprocal)
      Astar = matrix.sqr(ORI.reciprocal_matrix())
      Astar_from_experiment = matrix.sqr(experiment.crystal.get_A())
      assert Astar == Astar_from_experiment

      WAVE = experiment.beam.get_wavelength()
      BEAM = matrix.col((0.0,0.0,-1./WAVE))
      BFACTOR = 0.
      MOSAICITY_DEG = experiment.crystal.get_half_mosaicity_deg()
      DOMAIN_SIZE_A = experiment.crystal.get_domain_size_ang()

      # calculation of correlation here
      I_reference = flex.double([i_model.data()[pair[0]] for pair in matches.pairs()])
      I_invalid = flex.bool([i_model.sigmas()[pair[0]] < 0. for pair in matches.pairs()])
      use_weights = False # New facility for getting variance-weighted correlation

      if use_weights:
        # variance weighting
        I_weight = flex.double([1./(observations_pair1_selected.sigmas()[pair[1]])**2 for pair in matches.pairs()])
      else:
        I_weight = flex.double(len(observations_pair1_selected.sigmas()), 1.)

      I_weight.set_selected(I_invalid, 0.)
      chosen.set_selected(I_invalid,0.)

      """Explanation of 'include_negatives' semantics as originally implemented in cxi.merge postrefinement:
         include_negatives = True
         + and - reflections both used for Rh distribution for initial estimate of RS parameter
         + and - reflections both used for calc/obs correlation slope for initial estimate of G parameter
         + and - reflections both passed to the refinery and used in the target function (makes sense if
                             you look at it from a certain point of view)

         include_negatives = False
         + and - reflections both used for Rh distribution for initial estimate of RS parameter
         +       reflections only used for calc/obs correlation slope for initial estimate of G parameter
         + and - reflections both passed to the refinery and used in the target function (makes sense if
                             you look at it from a certain point of view)

         NOTE: by the new design, "include negatives" is always True
      """

      SWC = simple_weighted_correlation(I_weight, I_reference, I_observed)
      if self.params.output.log_level == 0:
        self.logger.log("Old correlation is: %f"%SWC.corr)

      Rhall = flex.double()
      for mill in MILLER:
        H = matrix.col(mill)
        Xhkl = Astar*H
        Rh = ( Xhkl + BEAM ).length() - (1./WAVE)
        Rhall.append(Rh)
      Rs = math.sqrt(flex.mean(Rhall*Rhall))

      RS = 1./10000. # reciprocal effective domain size of 1 micron
      RS = Rs        # try this empirically determined approximate, monochrome, a-mosaic value
      current = flex.double([SWC.slope, BFACTOR, RS, 0., 0.])

      parameterization_class = rs_parameterization
      refinery = rs2_refinery(ORI=ORI, MILLER=MILLER, BEAM=BEAM, WAVE=WAVE, ICALCVEC = I_reference, IOBSVEC = I_observed, WEIGHTS = chosen)
      refinery.set_profile_shape(self.params.postrefinement.lineshape)

      func = refinery.fvec_callable(parameterization_class(current))
      functional = flex.sum(func * func)

      if self.params.output.log_level == 0:
        self.logger.log("functional: %f"%functional)

      self.current = current;
      self.parameterization_class = parameterization_class
      self.refinery = refinery;

      self.observations_pair1_selected = observations_pair1_selected;
      self.observations_original_index_pair1_selected = observations_original_index_pair1_selected

      error_detected = False

      try:
        self.run_plain()

        result_observations_original_index, result_observations, result_matches = self.result_for_cxi_merge()

        assert result_observations_original_index.size() == result_observations.size()
        assert result_matches.pairs().size() == result_observations_original_index.size()
        # Calculate the correlation of each frame after corrections.
        # This is used in the MM24 error model to determine a per frame level of error
        # These are the added to the reflection table
        I_observed = result_observations.data()
        matches = miller.match_multi_indices(
          miller_indices_unique = miller_set.indices(),
          miller_indices = result_observations.indices()
        )
        I_reference = flex.double([i_model.data()[pair[0]] for pair in matches.pairs()])
        I_invalid = flex.bool([i_model.sigmas()[pair[0]] < 0. for pair in matches.pairs()])
        I_weight = flex.double(len(result_observations.sigmas()), 1.)
        I_weight.set_selected(I_invalid, 0.)
        SWC_after_post = simple_weighted_correlation(I_weight, I_reference, I_observed)
      except (AssertionError, ValueError, RuntimeError) as e:
        error_detected = True
        reason = repr(e)
        if not reason:
          reason = "Unknown error"
        experiments_rejected_by_reason[reason] += 1

      if not error_detected:
        new_experiments.append(experiment)

        new_exp_reflections = flex.reflection_table()
        new_exp_reflections['miller_index_asymmetric']  = result_observations.indices()
        new_exp_reflections['intensity.sum.value']      = result_observations.data()
        new_exp_reflections['intensity.sum.variance']   = flex.pow(result_observations.sigmas(),2)
        new_exp_reflections['id']                       = flex.int(len(new_exp_reflections), len(new_experiments)-1)
        new_exp_reflections.experiment_identifiers()[len(new_experiments)-1] = experiment.identifier

        # The original reflection table, i.e. the input to this run() method, has more columns than those used
        # for the postrefinement ("data" and "sigma" in the miller arrays). The problems is: some of the input reflections may have been rejected by now.
        # So to bring those extra columns over to the new reflection table, we have to create a subset of the original exp_reflections table,
        # which would match (by original miller indices) the miller array results of the postrefinement.
        match_original_indices = miller.match_multi_indices(miller_indices_unique = exp_miller_indices_original.indices(), miller_indices = result_observations_original_index.indices())
        exp_reflections_match_results = exp_reflections.select(match_original_indices.pairs().column(0))
        assert (exp_reflections_match_results['intensity.sum.value'] == result_observations_original_index.data()).count(False) == 0
        new_exp_reflections['intensity.sum.value.unmodified'] = exp_reflections_match_results['intensity.sum.value.unmodified']
        new_exp_reflections['intensity.sum.variance.unmodified'] = exp_reflections_match_results['intensity.sum.variance.unmodified']
        for key in self.params.input.persistent_refl_cols:
          if key not in new_exp_reflections.keys():
            new_exp_reflections[key] = exp_reflections_match_results[key]
        new_exp_reflections["correlation_after_post"] = flex.double(len(new_exp_reflections), SWC_after_post.corr)
        new_reflections.extend(new_exp_reflections)

    # report rejected experiments, reflections
    experiments_rejected_by_postrefinement = len(experiments) - len(new_experiments)
    reflections_rejected_by_postrefinement = reflections.size() - new_reflections.size()

    self.logger.log("Experiments rejected by post-refinement: %d"%experiments_rejected_by_postrefinement)
    self.logger.log("Reflections rejected by post-refinement: %d"%reflections_rejected_by_postrefinement)

    for reason, count in six.iteritems(experiments_rejected_by_reason):
      self.logger.log("Experiments rejected due to %s: %d"%(reason,count))

    # Now that each rank has all reasons from all ranks, we can treat the reasons in a uniform way.
    total_experiments_rejected_by_reason = self.mpi_helper.count(experiments_rejected_by_reason)
    total_accepted_experiment_count = self.mpi_helper.sum(len(new_experiments))

    # how many reflections have we rejected due to post-refinement?
    rejected_reflections = len(reflections) - len(new_reflections);
    total_rejected_reflections = self.mpi_helper.sum(rejected_reflections)

    if self.mpi_helper.rank == 0:
      for reason, count in six.iteritems(total_experiments_rejected_by_reason):
        self.logger.main_log("Total experiments rejected due to %s: %d"%(reason,count))
      self.logger.main_log("Total experiments accepted: %d"%total_accepted_experiment_count)
      self.logger.main_log("Total reflections rejected due to post-refinement: %d"%total_rejected_reflections)

    self.logger.log_step_time("POSTREFINEMENT", True)

    # Do we have any data left?
    from xfel.merging.application.utils.data_counter import data_counter
    data_counter(self.params).count(new_experiments, new_reflections)

    return new_experiments, new_reflections

  def run_plain(self):

    out = StringIO()
    self.MINI = lbfgs_minimizer_derivatives(self.params,
                                           current_x = self.current,
                                           parameterization = self.parameterization_class,
                                           refinery = self.refinery,
                                           out = out)
    self.refined_mini = self.MINI

    if self.params.output.log_level == 0:
      self.logger.log("\n" + out.getvalue())

  def rs2_parameter_range_assertions(self, values):
    # New range assertions for refined variables
    assert 0 < values.G, "G-scale value out of range ( < 0 ) after rs2 refinement"
    assert -25 < values.BFACTOR and values.BFACTOR < 25, "B-factor value out of range ( |B|>25 ) after rs2 refinement"
    assert -0.5<180.*values.thetax/math.pi<0.5,"thetax value out of range ( |rotx|>.5 degrees ) after rs2 refinement"
    assert -0.5<180.*values.thetay/math.pi<0.5,"thetay value out of range ( |roty|>.5 degrees ) after rs2 refinement"

  def result_for_cxi_merge(self):
    values = self.get_parameter_values()
    self.rs2_parameter_range_assertions(values)
    scaler = self.refinery.scaler_callable(self.parameterization_class(self.MINI.x))

    partiality_array = self.refinery.get_partiality_array(values)
    p_scaler = flex.pow(partiality_array,
                        0.5*self.params.postrefinement.merge_partiality_exponent)
    fat_selection = (partiality_array > self.params.postrefinement.partiality_threshold_hcfix)
    fat_count = fat_selection.count(True)
    scaler_s = scaler.select(fat_selection)
    p_scaler_s = p_scaler.select(fat_selection)

    # reject an experiment with insufficient number of near-full reflections
    if fat_count < 3:
      if self.params.output.log_level == 0:
        self.logger.log("Rejected experiment, because: On total %5d the fat selection is %5d"%(len(self.observations_pair1_selected.indices()), fat_count))
      raise ValueError("< 3 near-fulls after refinement")
    if self.params.output.log_level == 0:
      self.logger.log("On total %5d the fat selection is %5d"%(len(self.observations_pair1_selected.indices()), fat_count))

    observations_original_index = self.observations_original_index_pair1_selected.select(fat_selection)

    observations = self.observations_pair1_selected.customized_copy(
      indices = self.observations_pair1_selected.indices().select(fat_selection),
      data = (self.observations_pair1_selected.data().select(fat_selection)/scaler_s),
      sigmas = (self.observations_pair1_selected.sigmas().select(fat_selection)/(scaler_s * p_scaler_s))
    )
    matches = miller.match_multi_indices(
      miller_indices_unique=self.params.scaling.miller_set.indices(),
      miller_indices=observations.indices())

    I_weight = flex.double(len(observations.sigmas()), 1.)
    I_reference = flex.double([self.params.scaling.i_model.data()[pair[0]] for pair in matches.pairs()])
    I_invalid = flex.bool([self.params.scaling.i_model.sigmas()[pair[0]] < 0. for pair in matches.pairs()])
    I_weight.set_selected(I_invalid,0.)
    SWC = simple_weighted_correlation(I_weight, I_reference, observations.data())

    if self.params.output.log_level == 0:
      self.logger.log("CORR: NEW correlation is: %f"%SWC.corr)
      self.logger.log("ASTAR: ")
      self.logger.log(tuple(self.refinery.get_eff_Astar(values)))

    self.final_corr = SWC.corr
    self.refined_mini = self.MINI

    #another range assertion
    assert self.final_corr > 0.1,"correlation coefficient out of range (<= 0.1) after rs2 refinement"

    return observations_original_index, observations, matches

  def get_parameter_values(self):
    return self.refined_mini.parameterization(self.refined_mini.x)

class rs2_refinery(rs_refinery):

    def set_profile_shape(self, shape):
      self.profile_shape = shape
      self.get_partiality_array = {
        "lorentzian":super(rs2_refinery, self).get_partiality_array,
        "gaussian": self.get_gaussian_partiality_array
      }[shape]

    def get_gaussian_partiality_array(self,values):
      rs = values.RS
      Rh = self.get_Rh_array(values)
      immersion = Rh/rs
      gaussian = flex.exp(-2. * math.log(2) * (immersion*immersion))
      return gaussian

    def jacobian_callable(self,values):
      PB = self.get_partiality_array(values)
      EXP = flex.exp(-2.*values.BFACTOR*self.DSSQ)
      G_terms = (EXP * PB * self.ICALCVEC)
      B_terms = (values.G * EXP * PB * self.ICALCVEC)*(-2.*self.DSSQ)
      P_terms = (values.G * EXP * self.ICALCVEC)

      thetax = values.thetax; thetay = values.thetay;
      Rx = matrix.col((1,0,0)).axis_and_angle_as_r3_rotation_matrix(thetax)
      dRx_dthetax = matrix.col((1,0,0)).axis_and_angle_as_r3_derivative_wrt_angle(thetax)
      Ry = matrix.col((0,1,0)).axis_and_angle_as_r3_rotation_matrix(thetay)
      dRy_dthetay = matrix.col((0,1,0)).axis_and_angle_as_r3_derivative_wrt_angle(thetay)
      ref_ori = matrix.sqr(self.ORI.reciprocal_matrix())
      miller_vec = self.MILLER.as_vec3_double()
      ds1_dthetax = flex.mat3_double(len(self.MILLER),Ry * dRx_dthetax * ref_ori) * miller_vec
      ds1_dthetay = flex.mat3_double(len(self.MILLER),dRy_dthetay * Rx * ref_ori) * miller_vec

      s1vec = self.get_s1_array(values)
      s1lenvec = flex.sqrt(s1vec.dot(s1vec))
      dRh_dthetax = s1vec.dot(ds1_dthetax)/s1lenvec
      dRh_dthetay = s1vec.dot(ds1_dthetay)/s1lenvec
      rs = values.RS
      Rh = self.get_Rh_array(values)
      rs_sq = rs*rs
      denomin = (2. * Rh * Rh + rs_sq)
      dPB_dRh = { "lorentzian": -PB * 4. * Rh / denomin,
                  "gaussian": -PB * 4. * math.log(2) * Rh / rs_sq }[self.profile_shape]
      dPB_dthetax = dPB_dRh * dRh_dthetax
      dPB_dthetay = dPB_dRh * dRh_dthetay
      Px_terms = P_terms * dPB_dthetax; Py_terms = P_terms * dPB_dthetay

      return [G_terms,B_terms,0,Px_terms,Py_terms]

class lbfgs_minimizer_derivatives(lbfgs_minimizer_base):

  def __init__(self, params, current_x=None, parameterization=None, refinery=None, out=None,
               min_iterations=0, max_calls=1000, max_drop_eps=1.e-5):
    adopt_init_args(self, locals())
    self.n = current_x.size()
    self.x = current_x
    from scitbx import lbfgs
    self.minimizer = lbfgs.run(
      target_evaluator=self,
      termination_params=lbfgs.termination_parameters(
        traditional_convergence_test=True,
        drop_convergence_test_max_drop_eps=max_drop_eps,
        min_iterations=min_iterations,
        max_iterations = None,
        max_calls=max_calls),
      exception_handling_params=lbfgs.exception_handling_parameters(
         ignore_line_search_failed_rounding_errors=True,
         ignore_line_search_failed_step_at_lower_bound=True,#the only change from default
         ignore_line_search_failed_step_at_upper_bound=False,
         ignore_line_search_failed_maxfev=False,
         ignore_line_search_failed_xtol=False,
         ignore_search_direction_not_descent=False)
      )

  def compute_functional_and_gradients(self):
    values = self.parameterization(self.x)
    assert -150. < values.BFACTOR < 150,"B-factor out of range (+/-150) within rs2 functional and gradients"
    self.func = self.refinery.fvec_callable(values)
    functional = flex.sum(self.refinery.WEIGHTS*self.func*self.func)
    self.f = functional
    jacobian = self.refinery.jacobian_callable(values)
    self.g = flex.double(self.n)
    for ix in range(self.n):
      self.g[ix] = flex.sum(2. * self.refinery.WEIGHTS * self.func * jacobian[ix])

    print("rms %10.3f"%math.sqrt(flex.sum(self.refinery.WEIGHTS*self.func*self.func)/flex.sum(self.refinery.WEIGHTS)), file=self.out, end='')

    values.show(self.out)

    return self.f, self.g

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(postrefinement_rs2)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/preference/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/preference/factory.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.preference.preference import PreferenceWorker
from xfel.merging.application.worker import factory as factory_base


class factory(factory_base):
  """Factory for PreferenceWorker which evaluates preferential orientation"""
  @staticmethod
  def from_parameters(params, additional_info=None, mpi_helper=None, mpi_logger=None):
    return [PreferenceWorker(params, mpi_helper, mpi_logger)]


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/preference/preference.py
from __future__ import absolute_import, division, print_function

from functools import wraps

import numpy as np

from xfel.merging.application.worker import worker
from xfel.util.preference import ascii_plot, DirectSpaceBases, \
  find_preferential_distribution, space_group_auto


def return_input_expts_refls_if_error(run_method):
  @wraps(run_method)
  def run_method_wrapper(worker_instance, experiments, reflections):
    try:
      return run_method(worker_instance, experiments, reflections)
    except Exception:  # noqa - this is meant to catch everything
      worker_instance.logger.log('Error when running worker, returning input')
      return experiments, reflections
  return run_method_wrapper


class PreferenceWorker(worker):
  """Read lattice vectors, apply symmetry, look for preferential orientation"""

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    kwargs = dict(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)
    super(PreferenceWorker, self).__init__(**kwargs)

  def __repr__(self):
    return 'Evaluate preferential orientation'

  @return_input_expts_refls_if_error
  def run(self, experiments, reflections):
    sg, message_ = space_group_auto(experiments, comm=self.mpi_helper.comm)
    if self.mpi_helper.rank == 0:
      for message_line in message_.split('\n'):
        self.logger.main_log(message_line)
      self.logger.main_log('')
    abc_stack = DirectSpaceBases.from_expts(experiments, sg)
    abc_stack = abc_stack.symmetrize(sg.build_derived_point_group())
    abc_stacks = self.mpi_helper.comm.gather(abc_stack)
    if self.mpi_helper.comm.rank != 0:
      return experiments, reflections
    abc_stack = DirectSpaceBases(np.concatenate(abc_stacks, axis=0))
    distributions = find_preferential_distribution(abc_stack, sg)
    self.logger.main_log(distributions.table)
    self.logger.main_log('')
    pref_direction, pref_distribution = distributions.best
    self.logger.main_log('Vector distribution for zone axes family '
                         + pref_direction)
    self.logger.main_log(ascii_plot(pref_distribution.vectors))
    self.logger.main_log('')
    return experiments, reflections


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/publish/factory.py
from __future__ import division
from xfel.merging.application.publish.publisher import publisher
from xfel.merging.application.worker import factory as factory_base

class factory(factory_base):
  @staticmethod
  def from_parameters(params, additional_info=None, mpi_helper=None, mpi_logger=None):
    return [publisher(params, mpi_helper, mpi_logger)]


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/publish/publisher.py
from __future__ import division
from xfel.merging.application.worker import worker
from xfel.command_line.upload_mtz import run_with_preparsed as run_publish
import os

class publisher(worker):

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(publisher, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return "Publish output mtz and main log to a Google Drive folder"

  def run(self, experiments, reflections):

    mtz_fname = self.params.output.prefix + "_all.mtz"
    mtz_path = os.path.join(self.params.output.output_dir, mtz_fname)
    self.params.publish.input.mtz_file = mtz_path
    self.params.publish.input.log_file = None
    self.params.publish.input.version = None

    if self.mpi_helper.rank==0: run_publish(self.params.publish)

    return experiments, reflections


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/reflection_table_utils.py
from __future__ import absolute_import, division, print_function
import numpy as np
from six.moves import range
from dials.array_family import flex
import math
from simtbx.diffBragg.utils import is_outlier

class reflection_table_utils(object):

  @staticmethod
  def get_next_hkl_reflection_table(reflections):
    '''Generate asu hkl slices from an asu hkl-sorted reflection table'''
    if reflections.size() == 0:
      yield reflections

    i_begin = 0
    hkl_ref = reflections['miller_index_asymmetric'][0]
    for i in range(reflections.size()):
      hkl = reflections['miller_index_asymmetric'][i]
      if hkl == hkl_ref:
        continue
      else:
        yield reflections[i_begin:i]
        i_begin = i
        hkl_ref = hkl

    yield reflections[i_begin:i+1]

  @staticmethod
  def select_odd_experiment_reflections(reflections, col='id'):
    'Select reflections from experiments with odd ids.'
    sel = reflections[col] % 2 == 1
    reflections["is_odd_experiment"] = sel  # store this for later use, NOTE this is un-prunable if expanded_bookkeeping=True
    return reflections.select(sel)

  @staticmethod
  def select_even_experiment_reflections(reflections, col='id'):
    'Select reflections from experiments with even ids'
    sel = reflections[col] % 2 == 0
    return reflections.select(sel)

  @staticmethod
  def merged_reflection_table():
    '''Create a reflection table for storing merged HKLs'''
    table = flex.reflection_table()
    table['miller_index'] = flex.miller_index()
    table['intensity'] = flex.double()
    table['sigma'] = flex.double()
    table['multiplicity'] = flex.int()
    return table

  @staticmethod
  def merge_reflections(reflections, min_multiplicity, nameprefix=None, thresh=None):
    '''Merge intensities of multiply-measured symmetry-reduced HKLs. The input reflection table must be sorted by symmetry-reduced HKLs.'''
    merged_reflections = reflection_table_utils.merged_reflection_table()
    for i_refls,refls in enumerate(reflection_table_utils.get_next_hkl_reflection_table(reflections=reflections)):
      if refls.size() == 0:
        break # unless the input "reflections" list is empty, generated "refls" lists cannot be empty

      hkl = refls[0]['miller_index_asymmetric']
      # This assert is timeconsuming when using a small number of cores
      #assert not (hkl in merged_reflections['miller_index']) # i.e. assert that the input reflection table came in sorted

      refls = refls.select(refls['intensity.sum.variance'] > 0.0)

      if refls.size() >= min_multiplicity:
        weighted_intensity_array = refls['intensity.sum.value'] / refls['intensity.sum.variance']
        weights_array = flex.double(refls.size(), 1.0) / refls['intensity.sum.variance']

        weighted_mean_intensity = flex.sum(weighted_intensity_array) / flex.sum(weights_array)
        standard_error_of_weighted_mean_intensity = 1.0/math.sqrt(flex.sum(weights_array))

        if thresh is not None:
          vals = refls["intensity.sum.value"].as_numpy_array()
          good = ~is_outlier(vals, thresh)
          good_vals = vals[good]
          weighted_mean_intensity = np.mean(good_vals)
          vals_var = refls["intensity.sum.variance"].as_numpy_array()
          num_good = good.sum()
          standard_error_of_weighted_mean_intensity = np.sqrt(vals_var[good].sum())/num_good
        merged_reflections.append(
                                  {'miller_index' : hkl,
                                  'intensity' : weighted_mean_intensity,
                                  'sigma' : standard_error_of_weighted_mean_intensity,
                                  'multiplicity' : refls.size()})
    return merged_reflections

  @staticmethod
  def prune_reflection_table_keys(reflections, keys_to_delete=None, keys_to_keep=None,
                                  keys_to_ignore=None):
    '''Remove reflection table keys: either inclusive or exclusive, columns in keys_to_ignore will always remain'''
    # These columns were created by the merging application, and we want to retain them
    if keys_to_delete is not None:
      keys_to_delete = [k for k in keys_to_delete if k not in keys_to_ignore]
    if keys_to_keep is not None:
      keys_to_keep += [k for k in keys_to_ignore if k not in keys_to_keep]

    if len(reflections) != 0:
      all_keys = list()
      for key in reflections.keys():
        all_keys.append(key)
      if keys_to_delete != None:
        for key in keys_to_delete:
          if key in all_keys:
            del reflections[key]
      elif keys_to_keep != None:
        for key in all_keys:
          if not key in keys_to_keep:
            del reflections[key]
    return reflections

  @staticmethod
  def get_next_reflection_table_slice(reflections, n_slices, reflection_table_stub):
    '''Generate an exact number of slices from a reflection table. Make slices as even as possible. If not enough reflections, generate empty tables'''
    assert n_slices >= 0

    if n_slices == 1:
      yield reflections
    else:
      import math

      generated_slices = 0
      count = len(reflections)

      if count > 0:
        # how many non-empty slices should we generate and with what stride?
        nonempty_slices = min(count, n_slices)
        stride = int(math.ceil(count / nonempty_slices))

        # generate all non-empty slices
        for i in range(0, count, stride):
          generated_slices += 1
          i2 = i + stride
          if generated_slices == nonempty_slices:
            i2 = count
          yield reflections[i:i2]

      # generate some empty slices if necessary
      empty_slices = max(0, n_slices - generated_slices)
      for i in range(empty_slices):
        yield reflection_table_stub(reflections)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/scale/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/scale/experiment_scaler.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.worker import worker
from dials.array_family import flex
from dxtbx.model.experiment_list import ExperimentList
from cctbx import miller
from cctbx.crystal import symmetry
from scipy.optimize import curve_fit
from scipy.stats import pearsonr
import numpy as np

class scaling_result(object):
  '''Stores results of scaling of an experiment'''
  err_low_signal = 1
  err_low_correlation = 2 # low correlation between the observed and reference, or model, intensities

  def __init__(self):
    self.error = None
    self.data_count = None
    self.slope = None
    self.slope_error = None
    self.offset = None
    self.offset_error = None
    self.correlation = None

class experiment_scaler(worker):
  '''Scales experiment reflection intensities to the reference, or model, intensities'''

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(experiment_scaler, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Scaling; cross-correlation'

  def run(self, experiments, reflections):
    self.logger.log_step_time("SCALE_FRAMES")
    if self.params.scaling.algorithm != "mark0": # mark1 implies no scaling/post-refinement
      self.logger.log("No scaling was done")
      if self.mpi_helper.rank == 0:
        self.logger.main_log("No scaling was done")
      return experiments, reflections

    new_experiments = ExperimentList()
    new_reflections = flex.reflection_table()

    # scale experiments, one at a time. Reject experiments that do not correlate with the reference or fail to scale.
    results = []
    slopes = []
    correlations = []
    high_res_experiments = 0
    experiments_rejected_because_of_low_signal = 0
    experiments_rejected_because_of_low_correlation_with_reference = 0

    target_symm = symmetry(unit_cell = self.params.scaling.unit_cell, space_group_info = self.params.scaling.space_group)
    for expt_id, experiment in enumerate(experiments):
      exp_reflections = reflections.select(reflections['id'] == expt_id)

      # Build a miller array for the experiment reflections
      exp_miller_indices = miller.set(target_symm, exp_reflections['miller_index_asymmetric'], True)
      exp_intensities = miller.array(exp_miller_indices, exp_reflections['intensity.sum.value'], flex.sqrt(exp_reflections['intensity.sum.variance']))

      model_intensities = self.params.scaling.i_model

      # Extract an array of HKLs from the model to match the experiment HKLs
      matching_indices = miller.match_multi_indices(miller_indices_unique = model_intensities.indices(), miller_indices = exp_intensities.indices())

      # Least squares
      weights = self.params.scaling.weights
      result = self.fit_experiment_to_reference(model_intensities, exp_intensities, matching_indices, weights)

      if result.error == scaling_result.err_low_signal:
        experiments_rejected_because_of_low_signal += 1
        continue
      elif result.error == scaling_result.err_low_correlation:
        experiments_rejected_because_of_low_correlation_with_reference += 1
        continue

      slopes.append(result.slope)
      correlations.append(result.correlation)

      if self.params.output.log_level == 0:
        self.logger.log("Experiment ID: %s; Slope: %f; Correlation %f"%(experiment.identifier, result.slope, result.correlation))

      # count high resolution experiments
      if exp_intensities.d_min() <= self.params.merging.d_min:
        high_res_experiments += 1

      # apply scale factors
      if (
          not self.params.postrefinement.enable or
          'postrefine' not in self.params.dispatch.step_list
      ):
        exp_reflections['intensity.sum.value'] *= result.slope
        exp_reflections['intensity.sum.variance'] *= (result.slope**2)
      exp_reflections['correlation'] = flex.double(len(exp_reflections), result.correlation)
      new_experiments.append(experiment)
      new_reflections.extend(exp_reflections)

    new_reflections.reset_ids()
    rejected_experiments = len(experiments) - len(new_experiments)
    assert rejected_experiments == experiments_rejected_because_of_low_signal + \
                                    experiments_rejected_because_of_low_correlation_with_reference

    reflections_removed_because_of_rejected_experiments = reflections.size() - new_reflections.size()

    self.logger.log("Experiments rejected because of low signal: %d"%experiments_rejected_because_of_low_signal)
    self.logger.log("Experiments rejected because of low correlation with reference: %d"%experiments_rejected_because_of_low_correlation_with_reference)
    self.logger.log("Reflections rejected because of rejected experiments: %d"%reflections_removed_because_of_rejected_experiments)
    self.logger.log("High resolution experiments: %d"%high_res_experiments)
    if self.params.postrefinement.enable and 'postrefine' in self.params.dispatch.step_list:
      self.logger.log("Note: scale factors were not applied, because postrefinement is enabled")

    # MPI-reduce all counts
    comm = self.mpi_helper.comm
    MPI = self.mpi_helper.MPI
    total_experiments_rejected_because_of_low_signal                            = comm.reduce(experiments_rejected_because_of_low_signal, MPI.SUM, 0)
    total_experiments_rejected_because_of_low_correlation_with_reference        = comm.reduce(experiments_rejected_because_of_low_correlation_with_reference, MPI.SUM, 0)
    total_reflections_removed_because_of_rejected_experiments                   = comm.reduce(reflections_removed_because_of_rejected_experiments, MPI.SUM, 0)
    total_high_res_experiments                                                  = comm.reduce(high_res_experiments, MPI.SUM, 0)
    all_slopes                                                                  = comm.reduce(slopes, MPI.SUM, 0)
    all_correlations                                                            = comm.reduce(correlations, MPI.SUM, 0)

    # rank 0: log data statistics
    if self.mpi_helper.rank == 0:
      self.logger.main_log('Experiments rejected because of low signal: %d'%total_experiments_rejected_because_of_low_signal)
      self.logger.main_log('Experiments rejected because of low correlation with reference: %d'%total_experiments_rejected_because_of_low_correlation_with_reference)
      self.logger.main_log('Reflections rejected because of rejected experiments: %d'%total_reflections_removed_because_of_rejected_experiments)
      self.logger.main_log('Experiments with high resolution of %5.2f Angstrom or better: %d'%(self.params.merging.d_min, total_high_res_experiments))

      if len(all_slopes) > 0:
        stats_slope = flex.mean_and_variance(flex.double(all_slopes))
        self.logger.main_log('Average experiment scale factor wrt reference: %f'%(stats_slope.mean()))
      if len(all_correlations) > 1:
        stats_correlation = flex.mean_and_variance(flex.double(all_correlations))
        self.logger.main_log('Average experiment correlation with reference: %f +/- %f'%(
            stats_correlation.mean(), stats_correlation.unweighted_sample_standard_deviation()))

      if self.params.postrefinement.enable and 'postrefine' in self.params.dispatch.step_list:
        self.logger.main_log("Note: scale factors were not applied, because postrefinement is enabled")

    self.logger.log_step_time("SCALE_FRAMES", True)

    # Do we have any data left?
    from xfel.merging.application.utils.data_counter import data_counter
    data_counter(self.params).count(new_experiments, new_reflections)

    return new_experiments, new_reflections

  def fit_experiment_to_reference(self,
      model_intensities,
      experiment_intensities,
      matching_indices,
      weights='unit'
  ):
    'Scale the observed intensities to the reference, or model, using a linear least squares fit.'
     # Y = offset + slope * X, where Y is I_r and X is I_o

    result = scaling_result()
    result.data_count = matching_indices.pairs().size()
    if result.data_count < 3:
      result.error = scaling_result.err_low_signal
      return result

    model_subset = []
    exp_subset = []
    exp_sigmas = []
    for pair in matching_indices.pairs():
      model_subset.append(model_intensities.data()[pair[0]])
      exp_subset.append(experiment_intensities.data()[pair[1]])
      exp_sigmas.append(experiment_intensities.sigmas()[pair[1]])
    model_subset = np.array(model_subset)
    exp_subset = np.array(exp_subset)
    exp_sigmas = np.array(exp_sigmas)

    correlation = pearsonr(exp_subset, model_subset)[0]
    if correlation < self.params.filter.outlier.min_corr:
      result.error = scaling_result.err_low_correlation
      return result
    result.correlation = correlation

    def linfunc(x, m): return x*m
    # For weighting, we need to put the Icalc values on the scale of Iobs, so
    # we do a first-pass scale with unit weights and apply it to model_subset.
    slope_unwt = curve_fit(linfunc, exp_subset, model_subset)[0][0]
    model_subset_scaled = model_subset / slope_unwt
    if weights == 'unit':
      slope = slope_unwt
    elif weights == 'icalc_sigma':
      sigma = (model_subset_scaled**2 + exp_sigmas**2)**.5
      slope = curve_fit(linfunc, exp_subset, model_subset, sigma=sigma)[0][0]
    elif weights == 'icalc':
      sigma = model_subset_scaled**.5
      slope = curve_fit(linfunc, exp_subset, model_subset, sigma=sigma)[0][0]

    result.slope = slope
    return result

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(experiment_scaler)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/scale/factory.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.scale.experiment_scaler import experiment_scaler
from xfel.merging.application.worker import factory as factory_base

class factory(factory_base):
  """ Factory class for scaling frames of data. """
  @staticmethod
  def from_parameters(params, additional_info=None, mpi_helper=None, mpi_logger=None):
    """ """
    return [experiment_scaler(params, mpi_helper, mpi_logger)]


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/statistics/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/statistics/beam_statistics.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.worker import worker
from dials.array_family import flex

class beam_statistics(worker):

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(beam_statistics, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Beam statistics'

  def run(self, experiments, reflections):
    self.logger.log_step_time("BEAM_STATISTICS")
    f_wavelengths = flex.double([b.get_wavelength() for b in experiments.beams()])

    flex_all_wavelengths = self.mpi_helper.aggregate_flex(f_wavelengths, flex.double, root=None)
    average_wavelength = flex.mean(flex_all_wavelengths)
    if self.mpi_helper.rank == 0:
      self.logger.main_log("Wavelength: %f"%average_wavelength)

    # save the average wavelength to the phil parameters
    if self.mpi_helper.rank == 0:
      self.logger.main_log("Average wavelength (%f A) is saved to phil parameters"%average_wavelength)
    if not 'average_wavelength' in (self.params.statistics).__dict__:
      self.params.statistics.__inject__('average_wavelength', average_wavelength)
    else:
      self.params.statistics.__setattr__('average_wavelength', average_wavelength)

    self.logger.log_step_time("BEAM_STATISTICS", True)

    return experiments, reflections

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(beam_statistics)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/statistics/deltaccint.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from dxtbx import flumpy
from dials.array_family import flex
import itertools
import numpy as np
from scitbx.math import five_number_summary
from xfel.merging.application.reflection_table_utils import reflection_table_utils
from xfel.merging.application.worker import worker


class deltaccint(worker):
  '''Calculates CC using the - method from Assmann 2016'''

  def __repr__(self):
    return 'CC statistics (- method)'

  def run(self, experiments, reflections):
    self.logger.log_step_time("STATISTICS_DELTA_CCINT")

    comm = self.mpi_helper.comm
    MPI = self.mpi_helper.MPI

    assert experiments == None, "Must be run after the group worker"
    # need at least 3 reflections to keep multiplicty > 2 when removing an image
    filtered = flex.reflection_table()
    min_mult = max(3, self.params.merging.minimum_multiplicity)
    for refls in reflection_table_utils.get_next_hkl_reflection_table(reflections=reflections):
      if len(set(refls['id'])) >= min_mult:
        filtered.extend(refls)

    expt_map = filtered.experiment_identifiers()

    all_expt_ids = sorted(set(itertools.chain.from_iterable(comm.allgather(expt_map.values()))))
    all_expts_map = {v: k for k, v in enumerate(all_expt_ids)}

    if self.mpi_helper.rank == 0:
      self.logger.main_log("Beginning CC analysis (- method from Assmann 2016)")
      self.logger.main_log("Removing reflections with less than %d measurements"%min_mult)
      self.logger.main_log("N experiments after filtering: %d"%len(all_expt_ids))
      self.logger.main_log("")

    # We need to compute
    # 1) variance of the average intensities -> compute averages of all intensities and then compute variance per bin
    # 2) average variance of the intensities -> compute variances of all intensities and then compute average per bin
    # In reality for 2), we compute the average of the standard error of the mean squared instead of the variance (semsq)

    hkl_resolution_bins = self.params.statistics.hkl_resolution_bins
    hkl_set = [hkl for hkl in set(filtered['miller_index_asymmetric']) if hkl in hkl_resolution_bins]
    n_hkl = len(hkl_set)
    hkl_map = {v: k for k, v in enumerate(hkl_set)}

    # N expts (all ranks) x N hkl (this rank)
    sums   = np.zeros((len(all_expt_ids), n_hkl), float)
    n      = np.zeros((len(all_expt_ids), n_hkl), int)
    merged = np.zeros((len(all_expt_ids), n_hkl), float)
    semsq  = np.zeros((len(all_expt_ids), n_hkl), float)

    for refls in reflection_table_utils.get_next_hkl_reflection_table(reflections=filtered):
      hkl = refls['miller_index_asymmetric'][0]
      if hkl not in hkl_map: continue
      hkl_idx = hkl_map[hkl]
      intensity = flumpy.to_numpy(refls['intensity.sum.value'])

      # set the sum and n for all expts for this hkl
      sums[:,hkl_idx] = np.sum(intensity)
      n   [:,hkl_idx] = len(refls)

      # For each reflection, subtract it off the experiment it came from, leaving it for the rest of the experiments
      for i in range(len(refls)):
        expt_idx = all_expts_map[expt_map[refls['id'][i]]]
        sums[expt_idx,hkl_idx] -= intensity[i]
        n   [expt_idx,hkl_idx] -= 1
      merged[:,hkl_idx] = sums[:,hkl_idx]/n[:,hkl_idx]

      # compute semsq for each hkl (less the intensity from each experiment)
      diff_sq_ = np.full(len(all_expt_ids), -1, dtype=float)

      for i in range(len(refls)):
        expt_idx = all_expts_map[expt_map[refls['id'][i]]]
        mean_intensity_modified = merged[expt_idx,hkl_idx]
        # Handle the case where a single image contains same hkl twice.
        if diff_sq_[expt_idx] <0:
          diff_sq_modified = np.sum((intensity-mean_intensity_modified)**2)
          diff_sq_[expt_idx] = diff_sq_modified - (intensity[i] - mean_intensity_modified)**2
        else:
          diff_sq_[expt_idx] -= (intensity[i] - mean_intensity_modified)**2

      mean_intensity_unmodified = np.mean(intensity)
      diff_sq_unmodified = np.sum((intensity-mean_intensity_unmodified)**2)
      diff_sq_[diff_sq_<0] = diff_sq_unmodified
      semsq[:,hkl_idx] = diff_sq_ / (n[:,hkl_idx]-1) / n[:,hkl_idx]

    # N expts (all ranks)
    all_i_sums     = np.zeros(len(all_expt_ids), float) # sum of the averaged intensities
    all_i_n        = np.zeros(len(all_expt_ids), int)   # count of the averaged intensities
    all_semsq_sums = np.zeros(len(all_expt_ids), float) # sums of the semsq of the intensities

    # Sum up and reduce the bins
    for hkl in hkl_set:
      all_i_sums     += merged[:,hkl_map[hkl]]
      all_i_n        += 1
      all_semsq_sums += semsq[:,hkl_map[hkl]]

    # Broadcast the semsq and average intensities
    total_i_sums     = comm.allreduce(all_i_sums, op=MPI.SUM)
    total_i_n        = comm.allreduce(all_i_n,    op=MPI.SUM)
    total_semsq_sums = comm.reduce(all_semsq_sums,  op=MPI.SUM)
    total_i_average  = total_i_sums / total_i_n

    # Compute the variance of the average intensities
    # First, the numerator, the difference between each hkl and the average for that hkl's bin (ommiting each experiment once)
    diff_sq = np.zeros(merged.shape, float)
    for hkl in hkl_set:
      diff_sq[:,hkl_map[hkl]] = (merged[:,hkl_map[hkl]] - total_i_average) ** 2

    # N expts (all ranks) x N bins
    diff_sq_sum = np.zeros(all_i_sums.shape, float)

    # Complete the numerator for this rank
    for hkl in hkl_set:
      diff_sq_sum += diff_sq[:,hkl_map[hkl]]

    total_diff_sq_sum = comm.reduce(diff_sq_sum, MPI.SUM, 0)

    # Report
    if self.mpi_helper.rank == 0:
      sigma_sq_y = total_diff_sq_sum / (total_i_n-1) # variance of the average intensities
      sigma_sq_e = 2 * total_semsq_sums / total_i_n    # average semsq of the intensities
      deltaccint_st = (sigma_sq_y - (0.5 * sigma_sq_e)) / (sigma_sq_y + (0.5 * sigma_sq_e))

      data = flex.double(deltaccint_st) * 100
      sorted_data = data.select(flex.sort_permutation(data))

      mini, q1, med, q3, maxi = five_number_summary(data)
      self.logger.main_log("Five number summary")
      self.logger.main_log("% 8.4f%% min"%mini)
      self.logger.main_log("% 8.4f%% q1"%q1)
      self.logger.main_log("% 8.4f%% median"%med)
      self.logger.main_log("% 8.4f%% q3"%q3)
      self.logger.main_log("% 8.4f%% max"%maxi)
      self.logger.main_log("")

      if self.params.statistics.deltaccint.verbose:
        self.logger.main_log("Showing CC for all lattices")
        for e, identifier in enumerate(all_expt_ids):
          self.logger.main_log("%s %f"%(identifier, data[e]))

      n_worst = min(len(data), 30)
      worst = sorted_data[-n_worst:]
      iqr = q3-q1

      self.logger.main_log("Showing CC of the worst %d lattices and IQR ratio needed to remove them"%n_worst)
      self.logger.main_log(" CC (%) IQR ratio Lattices removed")
      for i in range(len(worst)):
        self.logger.main_log("% 8.4f % 10.1f %d"%(worst[i], (worst[i]-med)/iqr, n_worst-i))
      self.logger.main_log("")

      if self.params.statistics.deltaccint.iqr_ratio:
        cut = q3 + iqr * self.params.statistics.deltaccint.iqr_ratio
        sel = data > cut
        worst_expts_ = flex.std_string(all_expt_ids).select(sel)

        self.logger.main_log("Removing %d experiments out of %d using IQR ratio %.1f"%(len(worst_expts_), len(all_expt_ids), self.params.statistics.deltaccint.iqr_ratio))

    # Broadcast the worst experiments to cut
    else:
      worst_expts_ = None

    if self.params.statistics.deltaccint.iqr_ratio:
      worst_expts = comm.bcast(worst_expts_, 0)
      self.logger.log("Starting number of reflections: %d"%len(reflections))
      self.logger.log("Reflections after filtering by minimum multiplicity of %d: %d"%(min_mult, len(filtered)))
      reflections.remove_on_experiment_identifiers(worst_expts)
      reflections.reset_ids()
      self.logger.log("Reflections after filtering by CC: %d"%len(reflections))

    self.logger.log_step_time("STATISTICS_DELTA_CCINT", True)

    return experiments, reflections

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(deltaccint)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/statistics/experiment_resolution_statistics.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from dials.array_family import flex
from libtbx import table_utils
from xfel.merging.application.worker import worker
from xfel.merging.application.reflection_table_utils import reflection_table_utils

class experiment_resolution_statistics(worker):
  '''Calculates experiments accepted vs resolution bins'''

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(experiment_resolution_statistics, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Lattices resolution'

  def run(self, experiments, reflections):

    self.logger.log_step_time("EXPERIMENT_RESOLUTION_STATS")

    # Get pre-created resolution binning objects from the parameters
    self.resolution_binner = self.params.statistics.resolution_binner
    self.hkl_resolution_bins = self.params.statistics.hkl_resolution_bins

    # How many bins do we have?
    self.n_bins = self.resolution_binner.n_bins_all() # (self.params.statistics.n_bins + 2), 2 - to account for the hkls outside of the binner resolution range

    # To enable MPI all-rank reduction, every rank must initialize statistics array(s), even if the rank doesn't have any reflections.
    self.experiment_count_per_resolution_bins = flex.int(self.n_bins, 0)

    # Calculate, format and output statistics for each rank
    if reflections.size() > 0:
      self.count_experiments_per_resolution_bins(reflections)
      Experiment_Table_text = self.get_formatted_table(self.experiment_count_per_resolution_bins, len(experiments))
      self.logger.log(Experiment_Table_text)

    # Accumulate statistics from all ranks
    all_ranks_experiment_count_per_resolution_bins = self.mpi_helper.cumulative_flex(self.experiment_count_per_resolution_bins, flex.int)
    all_ranks_total_experiment_count = self.mpi_helper.sum(len(experiments))

    # Format and output all-rank total statistics
    if self.mpi_helper.rank == 0:
      Experiment_Table_text = self.get_formatted_table(all_ranks_experiment_count_per_resolution_bins, all_ranks_total_experiment_count)
      self.logger.main_log(Experiment_Table_text)

    self.logger.log_step_time("EXPERIMENT_RESOLUTION_STATS", True)

    return experiments, reflections

  def get_formatted_table(self, experiment_count_per_bin, total_experiment_count):
    '''Produce a table with experiment count over resolution bins'''

    table_data = [["Bin", "Resolution Range", "Lattices", "Accepted (%)"]]

    for i_bin in self.resolution_binner.range_used():
      col_legend = '%-13s' % self.resolution_binner.bin_legend(
                                                               i_bin=i_bin,
                                                               show_bin_number=False,
                                                               show_bin_range=False,
                                                               show_d_range=True,
                                                               show_counts=False)
      exp_count_abs = '%8d' % experiment_count_per_bin[i_bin]
      exp_count_percent = '%5.2f'% (100. * experiment_count_per_bin[i_bin] / total_experiment_count)
      table_data.append(['%3d' % i_bin, col_legend, exp_count_abs, exp_count_percent])

    table_data.append([""] * len(table_data[0]))
    table_data.append(["All", "", '%8d' % total_experiment_count])

    return "\n          Image Statistics\n" + table_utils.format(table_data, has_header=1, justify='center', delim=' ')

  def count_experiments_per_resolution_bins(self, reflections):
    '''For each resolution bin, count experiments that contributed reflections to that bin'''

    # Sort all reflections on asu hkls
    self.logger.log_step_time("SORT")
    self.logger.log("Sorting reflection table...")
    reflections.sort('miller_index_asymmetric')
    self.logger.log_step_time("SORT", True)

    # Initialize a dictionary to store unique experiment ids in resolution bins
    experiments_per_resolution_bins = {}
    for i_bin in range(self.n_bins):
      experiments_per_resolution_bins[i_bin] = set()

    # Accumulate experiment ids in the resolution bins where those experiments contributed reflections
    for refls in reflection_table_utils.get_next_hkl_reflection_table(reflections=reflections):
      if refls.size() == 0:
        break # unless the input "reflections" list is empty, generated "refls" lists cannot be empty
      hkl = refls[0]['miller_index_asymmetric']
      if hkl in self.hkl_resolution_bins:
        i_bin = self.hkl_resolution_bins[hkl]
        for refl in refls.rows():
          experiments_per_resolution_bins[i_bin].add(refls.experiment_identifiers()[refl['id']])

    # For each bin, reduce the sets of unique experiment ids to their count
    for i_bin in range(self.resolution_binner.n_bins_all()):
      self.experiment_count_per_resolution_bins[i_bin] = len(experiments_per_resolution_bins[i_bin])

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(experiment_resolution_statistics)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/statistics/factory.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.statistics.unit_cell_statistics import unit_cell_statistics
from xfel.merging.application.statistics.beam_statistics import beam_statistics
from xfel.merging.application.statistics.intensity_resolution_statistics import intensity_resolution_statistics
from xfel.merging.application.statistics.intensity_resolution_statistics_cxi import intensity_resolution_statistics_cxi
from xfel.merging.application.statistics.deltaccint import deltaccint
from xfel.merging.application.statistics.experiment_resolution_statistics import experiment_resolution_statistics
from xfel.merging.application.statistics.intensity_histogram import intensity_histogram
from xfel.merging.application.worker import factory as factory_base

class factory(factory_base):
  """ Factory class for calculating statistics of merged measurements. """
  @staticmethod
  def from_parameters(params, additional_info=[], mpi_helper=None, mpi_logger=None):
    """ """
    info_count = len(additional_info)
    assert info_count > 0
    if additional_info[0] == 'unitcell':
      return [unit_cell_statistics(params, mpi_helper, mpi_logger)]
    elif additional_info[0] == 'beam':
      return [beam_statistics(params, mpi_helper, mpi_logger)]
    elif additional_info[0] == 'resolution':
      return [experiment_resolution_statistics(params, mpi_helper, mpi_logger)]
    elif additional_info[0] == 'intensity':
      if info_count == 1:
        return [intensity_resolution_statistics(params, mpi_helper, mpi_logger)]
      elif info_count > 1 and additional_info[1] == 'cxi':
        return [intensity_resolution_statistics_cxi(params, mpi_helper, mpi_logger)]
      elif info_count > 1 and additional_info[1] == 'histogram':
        return [intensity_histogram(params, mpi_helper, mpi_logger)]
    elif additional_info[0] == 'deltaccint':
      return [deltaccint(params, mpi_helper, mpi_logger)]



 *******************************************************************************


 *******************************************************************************
xfel/merging/application/statistics/intensity_histogram.py
from __future__ import division
from xfel.merging.application.worker import worker
from dials.array_family import flex
from six.moves import cStringIO as StringIO

class intensity_histogram(worker):
  '''Builds a histogram of reflection intensities'''

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(intensity_histogram, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Intensity histogram'

  def run(self, experiments, reflections):
    comm = self.mpi_helper.comm
    MPI = self.mpi_helper.MPI
    if self.mpi_helper.rank == 0:
      self.logger.log_step_time("INTENSITY_HISTOGRAM")
      self.histogram(reflections['intensity'])
      self.logger.log_step_time("INTENSITY_HISTOGRAM", True)

    return experiments, reflections

  def histogram(self, data):
    from matplotlib import pyplot as plt
    nslots = 100
    histogram = flex.histogram(
                               data=data,
                               n_slots=nslots)
    out = StringIO()
    histogram.show(f=out, prefix="    ", format_cutoffs="%6.2f")
    self.logger.main_log(out.getvalue() + '\n' + "Total: %d"%data.size() + '\n')

    if False:
      fig = plt.figure()
      plt.bar(histogram.slot_centers(), histogram.slots(), align="center", width=histogram.slot_width())
      plt.show()

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(intensity_histogram)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/statistics/intensity_resolution_statistics.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from xfel.merging.application.worker import worker
from dials.array_family import flex
import math
from libtbx import adopt_init_args
from libtbx.str_utils import format_value
from libtbx import table_utils
from xfel.merging.application.reflection_table_utils import reflection_table_utils
from six.moves import cStringIO as StringIO
from cctbx.crystal import symmetry
from cctbx import miller
from scitbx.math import basic_statistics

class intensity_table_bin(object):
  '''Storage class for parameters of a resolution bin used by the hkl intensity statistics table'''
  def __init__(self,
               i_bin=None,
               d_range=None,
               d_min=None,
               redundancy_asu=None,
               redundancy_obs=None,
               redundancy_to_edge=None,
               absent=None,
               complete_tag=None,
               completeness=None,
               measurements=None,
               multiply_measured_asu=None,
               predictions=None,
               mean_I=None,
               mean_I_sigI=None,
               sigmaa=None,
               unmerged_meanIsig=None,
               unmerged_stddevIsig=None,
               unmerged_skewIsig=None):
    adopt_init_args(self, locals())

class intensity_table(object):
  '''Represents a table of hkl intensity statistics for resolution bins'''
  def __init__(self):
    self.table = []
    self.cumulative_theor_asu_count = 0
    self.cumulative_observed_asu_count = 0
    self.cumulative_observed_count = 0
    self.cumulative_multiply_observed_asu_count = 0
    self.cumulative_I = 0.0
    self.cumulative_Isigma = 0.0

  def get_table_text(self):
    '''Produce formatted table text'''
    table_header = ["","","","","<asu","<obs","<pred","","","","Merged","Merged","Unmerged","Unmerged","Unmerged"]
    table_header2 = ["Bin","Resolution Range","Completeness","%","multi>","multi>","multi>",
                      "n_meas", "asu_m_meas", "n_pred","<I>","<I/sig(I)>", "mean(I/sig(I))", "stddev(I/sig(I))", "skew(I/sig(I))"]

    use_preds = False # TODO?

    include_columns = [True, True, True, True, True, True, use_preds, True, True, use_preds, True, True, True, True, True]

    table_data = []
    table_data.append(table_header)
    table_data.append(table_header2)

    for bin in self.table:
      table_row = []
      table_row.append("%3d" % bin.i_bin)
      table_row.append("%-13s" % bin.d_range)
      table_row.append("%13s" % bin.complete_tag)
      table_row.append("%5.2f" % (100*bin.completeness))
      table_row.append("%6.2f" % bin.redundancy_asu)
      table_row.append("%6.2f" % bin.redundancy_obs)
      table_row.append("%6.2f" % (0)) # if redundancy_to_edge is None else bin.redundancy_to_edge))
      table_row.append("%6d" % bin.measurements)
      table_row.append("%6d" % bin.multiply_measured_asu)
      table_row.append("%6d" % (0)) # if redundancy_to_edge is None else flex.sum(bin.predictions)))
      table_row.append("%8.0f" % bin.mean_I)
      table_row.append("%8.3f" % bin.mean_I_sigI)
      table_row.append("%8.3f" % bin.unmerged_meanIsig)
      table_row.append("%8.3f" % bin.unmerged_stddevIsig)
      table_row.append("%8.3f" % bin.unmerged_skewIsig)
      table_data.append(table_row)

    if len(table_data) <= 2:
      return ("Intensity statistics table could not be constructed -- no bins accepted.")

    table_data.append([""] * len(table_header))

    total_completeness = 0
    total_asu_multiplicity = 0
    if self.cumulative_theor_asu_count > 0:
      total_completeness = 100 * (self.cumulative_observed_asu_count / self.cumulative_theor_asu_count)
      total_asu_multiplicity = self.cumulative_observed_count / self.cumulative_theor_asu_count

    total_obs_multiplicity = 0
    if self.cumulative_observed_asu_count:
      total_obs_multiplicity = self.cumulative_observed_count / self.cumulative_observed_asu_count

    total_mean_I = 0
    total_mean_Isigma = 0
    total_unmerged_mean_Isigma = 0
    total_unmerged_stddev_Isigma = 0
    total_unmerged_skew_Isigma = 0
    if self.cumulative_multiply_observed_asu_count > 0:
      total_mean_I        = self.cumulative_I / self.cumulative_multiply_observed_asu_count
      total_mean_Isigma   = self.cumulative_Isigma / self.cumulative_multiply_observed_asu_count

    table_data.append(  [
        format_value("%3s",   "All"),
        format_value("%-13s", "                 "),
        format_value("%13s",  "[%d/%d]"%(self.cumulative_observed_asu_count, self.cumulative_theor_asu_count)),
        format_value("%5.2f", total_completeness),
        format_value("%6.2f", total_asu_multiplicity),
        format_value("%6.2f", total_obs_multiplicity),
        format_value("%6.2f", (0)), # if redundancy_to_edge is None else cumulative_n_pred/cumulative_theor)),
        format_value("%6d",   self.cumulative_observed_count),
        format_value("%6d",   self.cumulative_multiply_observed_asu_count),
        format_value("%6d",   (0)), #if redundancy_to_edge is None else flex.sum(redundancy_to_edge))),
        format_value("%8.0f", total_mean_I),
        format_value("%8.3f", total_mean_Isigma),
        format_value("%8.3f", total_unmerged_mean_Isigma),
        format_value("%8.3f", total_unmerged_stddev_Isigma),
        format_value("%8.3f", total_unmerged_skew_Isigma),
    ])
    table_data = table_utils.manage_columns(table_data, include_columns)

    return table_utils.format(table_data, has_header = 2, justify ='center', delim = " ")

class cross_correlation_resolution_bin(object):
  '''Storage class for parameters of a cross-correlation resolution bin'''
  def __init__(self,
               i_bin=None,
               theor_asu_count=0,
               observed_matching_asu_count=0,
               cross_correlation=None):
    adopt_init_args(self, locals())

class cross_correlation_table(object):
  '''Represents a table of cross-correlations for resolution bins'''
  def __init__(self):
    self.table = []
    self.cumulative_observed_matching_asu_count = 0
    self.cumulative_theor_asu_count = 0
    self.cumulative_cross_correlation = 0.0

class intensity_resolution_statistics(worker):
  '''Calculates hkl intensity statistics for resolution bins'''

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(intensity_resolution_statistics, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Intensity resolution statistics'

  def run(self, experiments, reflections):
    self.last_bin_incomplete = False
    self.suggested_resolution_scalar = -1.0

    reflections_before_stats_count = reflections.size()
    total_reflections_before_stats_count = self.mpi_helper.sum(reflections_before_stats_count)
    if self.mpi_helper.rank == 0:
      self.logger.main_log("Total reflections before doing intensity statistics: %d"%(total_reflections_before_stats_count))

    self.logger.log_step_time("INTENSITY_STATISTICS")

    title = "\n                     Intensity Statistics (odd accepted experiments)\n"
    self.logger.log(title, rank_prepend=False)
    if self.mpi_helper.rank == 0:
      self.logger.main_log(title)
    reflections_odd = reflection_table_utils.select_odd_experiment_reflections(reflections)
    self.run_detail(reflections_odd)

    title = "\n                     Intensity Statistics (even accepted experiments)\n"
    self.logger.log(title, rank_prepend=False)
    if self.mpi_helper.rank == 0:
      self.logger.main_log(title)
    reflections_even = reflection_table_utils.select_even_experiment_reflections(reflections)
    self.run_detail(reflections_even)
    assert len(reflections_even) + len(reflections_odd) == len(reflections)

    title = "\n                     Intensity Statistics (all accepted experiments)\n"
    self.logger.log(title, rank_prepend=False)
    if self.mpi_helper.rank == 0:
      self.logger.main_log(title)
    self.run_detail(reflections)

    title = "\n                     CC 1/2, CC ISO\n"
    self.logger.log(title, rank_prepend=False)
    self.calculate_cc_int(reflections_odd, reflections_even)
    self.calculate_cc_iso(reflections)

    if self.mpi_helper.rank == 0:
      for entry in [(self.Total_CC_OneHalf_Table, "\n                     CC 1/2\n"),
                    (self.Total_CC_Iso_Table,     "\n                     CC ISO\n")]:
        Table = entry[0]
        Title = entry[1]
        if Table is not None:
          self.logger.main_log(Title)
          for row in Table.table:
            self.logger.main_log("%d/%d\t\t%f"%(row.observed_matching_asu_count, row.theor_asu_count, row.cross_correlation))
          self.logger.main_log("--------------------------------------------------------")
          self.logger.main_log("%d/%d\t\t%f\n"%(Table.cumulative_observed_matching_asu_count,
                                                     Table.cumulative_theor_asu_count,
                                                     Table.cumulative_cross_correlation))

    self.logger.log_step_time("INTENSITY_STATISTICS", True)

    return experiments, reflections

  def calculate_cc_iso(self, reflections):
    if self.params.statistics.cciso.mtz_file == None:
      self.Total_CC_Iso_Table = None
      return

    reflections_merged = reflection_table_utils.merge_reflections(reflections, self.params.merging.minimum_multiplicity)

    # Create target symmetry
    if self.params.merging.set_average_unit_cell:
      assert 'average_unit_cell' in (self.params.statistics).__dict__
      unit_cell = self.params.statistics.__phil_get__('average_unit_cell')
    else:
      unit_cell = self.params.scaling.unit_cell
    target_symm = symmetry(unit_cell=unit_cell, space_group_info = self.params.scaling.space_group)

    # Build miller array for experimental data
    miller_indices = miller.set(target_symm, reflections_merged['miller_index'], True)
    exp_intensities = miller.array(miller_indices,
                                   reflections_merged['intensity'],
                                   flex.double(reflections_merged.size(), 1.0))

    self.Total_CC_Iso_Table = self.calculate_cross_correlation(self.params.scaling.i_model, exp_intensities)

  def calculate_cc_int(self, odd_reflections, even_reflections):
    odd_reflections_merged = reflection_table_utils.merge_reflections(odd_reflections, self.params.merging.minimum_multiplicity)
    even_reflections_merged = reflection_table_utils.merge_reflections(even_reflections, self.params.merging.minimum_multiplicity)

    # Create target symmetry
    if self.params.merging.set_average_unit_cell:
      assert 'average_unit_cell' in (self.params.statistics).__dict__
      unit_cell = self.params.statistics.__phil_get__('average_unit_cell')
    else:
      unit_cell = self.params.scaling.unit_cell
    target_symm = symmetry(unit_cell=unit_cell, space_group_info = self.params.scaling.space_group)

    # Build miller arrays
    miller_indices_odd = miller.set(target_symm, odd_reflections_merged['miller_index'], True)
    intensities_odd = miller.array(miller_indices_odd,
                                   odd_reflections_merged['intensity'],
                                   flex.double(odd_reflections_merged.size(), 1.0))

    miller_indices_even = miller.set(target_symm, even_reflections_merged['miller_index'], True)
    intensities_even = miller.array(miller_indices_even, even_reflections_merged['intensity'],
                                    flex.double(even_reflections_merged.size(), 1.0))

    # Calculate crosss-correlation
    self.Total_CC_OneHalf_Table = self.calculate_cross_correlation(intensities_odd, intensities_even)

  def calculate_cross_correlation(self, miller_array_1, miller_array_2):
    # Get pre-created resolution binning objects from the parameters
    self.resolution_binner = self.params.statistics.resolution_binner
    self.hkl_resolution_bins = self.params.statistics.hkl_resolution_bins

    # How many bins do we have?
    n_bins = self.resolution_binner.n_bins_all() # (self.params.statistics.n_bins + 2), 2 - to account for the hkls outside of the binner resolution range

    # To enable MPI all-rank reduction, every rank must initialize statistics array(s), even if the rank doesn't have any reflections.
    self.cc_N         = flex.int(n_bins, 0)
    self.cc_sum_xx    = flex.double(n_bins, 0.0)
    self.cc_sum_xy    = flex.double(n_bins, 0.0)
    self.cc_sum_yy    = flex.double(n_bins, 0.0)
    self.cc_sum_x     = flex.double(n_bins, 0.0)
    self.cc_sum_y     = flex.double(n_bins, 0.0)

    # Find matching indices in the two data sets
    matching_indices = miller.match_multi_indices(miller_indices_unique = miller_array_1.indices(),
                                                  miller_indices = miller_array_2.indices())

    # Perform binned summations for all components of the cross-correlation formula
    for pair in matching_indices.pairs():

      hkl = miller_array_1.indices()[pair[0]]
      assert hkl == miller_array_2.indices()[pair[1]]

      if hkl in self.hkl_resolution_bins:
        i_bin = self.hkl_resolution_bins[hkl]

        I_x = miller_array_1.data()[pair[0]]
        I_y = miller_array_2.data()[pair[1]]

        self.cc_N[i_bin]        += 1
        self.cc_sum_xx[i_bin]   += I_x**2
        self.cc_sum_yy[i_bin]   += I_y**2
        self.cc_sum_xy[i_bin]   += I_x * I_y
        self.cc_sum_x[i_bin]    += I_x
        self.cc_sum_y[i_bin]    += I_y

    # Accumulate binned counts (cc_N) and sums (cc_sum) from all ranks
    all_ranks_cc_N          = self.mpi_helper.cumulative_flex(self.cc_N,      flex.int)
    all_ranks_cc_sum_xx     = self.mpi_helper.cumulative_flex(self.cc_sum_xx, flex.double)
    all_ranks_cc_sum_yy     = self.mpi_helper.cumulative_flex(self.cc_sum_yy, flex.double)
    all_ranks_cc_sum_xy     = self.mpi_helper.cumulative_flex(self.cc_sum_xy, flex.double)
    all_ranks_cc_sum_x      = self.mpi_helper.cumulative_flex(self.cc_sum_x,  flex.double)
    all_ranks_cc_sum_y      = self.mpi_helper.cumulative_flex(self.cc_sum_y,  flex.double)

    # Reduce all binned counts (cc_N) and sums (cc_sum) from all ranks
    if self.mpi_helper.rank == 0:
      return self.build_cross_correlation_table(
                                                all_ranks_cc_N,
                                                all_ranks_cc_sum_xx,
                                                all_ranks_cc_sum_yy,
                                                all_ranks_cc_sum_xy,
                                                all_ranks_cc_sum_x,
                                                all_ranks_cc_sum_y)
    else:
      return None

  def run_detail(self, reflections):
    # Get pre-created resolution binning objects from the parameters
    self.resolution_binner = self.params.statistics.resolution_binner
    self.hkl_resolution_bins = self.params.statistics.hkl_resolution_bins

    # How many bins do we have?
    n_bins = self.resolution_binner.n_bins_all() # (self.params.statistics.n_bins + 2), 2 - to account for the hkls outside of the binner resolution range

    # To enable MPI all-rank reduction, every rank must initialize statistics array(s), even if the rank doesn't have any reflections.
    self.I_sum      = flex.double(n_bins, 0.0) # a sum of the weighted mean intensities from all asu HKLs
    self.Isig_sum   = flex.double(n_bins, 0.0) # a sum of I/sigma from all asu HKLs
    self.Isig_list  = [flex.double() for _ in range(n_bins)]
    self.n_sum      = flex.int(n_bins, 0) # number of theoretically prediced asu hkls
    self.m_sum      = flex.int(n_bins, 0) # number of observed asu hkls
    self.mm_sum     = flex.int(n_bins, 0) # number of observed asu hkls with multiplicity > 1

    # Calculate, format and output statistics for each rank
    self.logger.log("Calculating intensity statistics...")
    self.calculate_intensity_statistics(reflections)
    Intensity_Table = self.build_intensity_table(
                                                I_sum = self.I_sum,
                                                Isig_sum = self.Isig_sum,
                                                n_sum = self.n_sum,
                                                m_sum = self.m_sum,
                                                mm_sum = self.mm_sum,
                                                unmerged_meanIsig = None,
                                                unmerged_stddevIsig = None,
                                                unmerged_skewIsig = None)
    if self.params.output.log_level == 0:
      self.logger.log(Intensity_Table.get_table_text(), rank_prepend=False)

    # Accumulate statistics from all ranks
    all_ranks_I_sum       = self.mpi_helper.cumulative_flex(self.I_sum, flex.double)
    all_ranks_Isig_sum    = self.mpi_helper.cumulative_flex(self.Isig_sum, flex.double)
    all_ranks_n_sum       = self.mpi_helper.cumulative_flex(self.n_sum, flex.int)
    all_ranks_m_sum       = self.mpi_helper.cumulative_flex(self.m_sum, flex.int)
    all_ranks_mm_sum      = self.mpi_helper.cumulative_flex(self.mm_sum, flex.int)

    all_ranks_unmerged_meanIsig = []
    all_ranks_unmerged_stddevIsig = []
    all_ranks_unmerged_skewIsig = []
    for bin_id in range(n_bins):
      all_ranks_isigi_list = self.mpi_helper.comm.gather(self.Isig_list[bin_id], 0)
      if self.mpi_helper.rank == 0:
        all_isigi = flex.double()
        for ranklist in all_ranks_isigi_list:
          all_isigi.extend(ranklist)
        stats = basic_statistics(all_isigi)
        all_ranks_unmerged_meanIsig.append(stats.mean)
        all_ranks_unmerged_stddevIsig.append(stats.bias_corrected_standard_deviation)
        all_ranks_unmerged_skewIsig.append(stats.skew)

    # Calculate, format and output all-rank total statistics
    if self.mpi_helper.rank == 0:
      Intensity_Table = self.build_intensity_table(
                                                  I_sum      = all_ranks_I_sum,
                                                  Isig_sum   = all_ranks_Isig_sum,
                                                  n_sum      = all_ranks_n_sum,
                                                  m_sum      = all_ranks_m_sum,
                                                  mm_sum     = all_ranks_mm_sum,
                                                  unmerged_meanIsig   = all_ranks_unmerged_meanIsig,
                                                  unmerged_stddevIsig = all_ranks_unmerged_stddevIsig,
                                                  unmerged_skewIsig   = all_ranks_unmerged_skewIsig)
      self.logger.main_log(Intensity_Table.get_table_text())
      if self.last_bin_incomplete:
        self.logger.main_log("Warning: the last resolution shell is incomplete. If your data was integrated to that resolution,\nconsider using scaling.resolution_scalar=%f or lower."%self.suggested_resolution_scalar)

  def calculate_intensity_statistics(self, reflections):
    '''Calculate statistics for hkl intensities distributed over resolution bins'''
    # The count of all reflections in the input reflection list, not restricted by any resolution limits, multiplicity thresholds, etc.
    zero_intensity_count_all      = (reflections['intensity.sum.value'] == 0.0).count(True)
    positive_intensity_count_all  = (reflections['intensity.sum.value'] >  0.0).count(True)
    negative_intensity_count_all  = (reflections['intensity.sum.value'] <  0.0).count(True)

    # The count of reflections filtered out from the input list by resolution limits
    zero_intensity_count_resolution_limited = 0
    positive_intensity_count_resolution_limited = 0
    negative_intensity_count_resolution_limited = 0

    # a list of reflections _actually_ used for the intenisty statistics calculations
    #used_reflections = flex.reflection_table()

    # How many bins do we have?
    n_bins = self.resolution_binner.n_bins_all() # (self.params.statistics.n_bins + 2), 2 - to account for the HKLs outside of the binner resolution range

    # Calculate auxiliary per-bin sums needed for the intensity statistics
    for refls in reflection_table_utils.get_next_hkl_reflection_table(reflections=reflections):
      if refls.size() == 0:
        break # unless the input "reflections" list is empty, generated "refls" lists cannot be empty

      hkl = refls[0]['miller_index_asymmetric']
      if hkl in self.hkl_resolution_bins:

        i_bin = self.hkl_resolution_bins[hkl]

        if i_bin > 0 and i_bin < n_bins - 1:
          zero_intensity_count_resolution_limited     += (refls['intensity.sum.value'] == 0.0).count(True)
          positive_intensity_count_resolution_limited += (refls['intensity.sum.value'] >  0.0).count(True)
          negative_intensity_count_resolution_limited += (refls['intensity.sum.value'] <  0.0).count(True)

          refls = refls.select(refls['intensity.sum.variance'] > 0.0)

          multiplicity = refls.size()

          if multiplicity >= self.params.merging.minimum_multiplicity:
            self.n_sum[i_bin] += 1
            self.m_sum[i_bin] += multiplicity
            self.mm_sum[i_bin] += 1
            weighted_intensity_array = refls['intensity.sum.value'] / refls['intensity.sum.variance']
            weights_array = flex.double(refls.size(), 1.0) / refls['intensity.sum.variance']
            self.I_sum[i_bin]     += flex.sum(weighted_intensity_array) / flex.sum(weights_array)
            self.Isig_sum[i_bin]  += flex.sum(weighted_intensity_array) / math.sqrt(flex.sum(weights_array))
            self.Isig_list[i_bin].extend(refls['intensity.sum.value.unmodified']/flex.sqrt(refls['intensity.sum.variance.unmodified']))
            #used_reflections.extend(refls)

    self.logger.log_step_time("INTENSITY_HISTOGRAM")

    # Accumulate intensities, which were used in the above statistics table, from all ranks
    #all_used_intensities = self.mpi_helper.extend_flex(used_reflections['intensity.sum.value'], flex.double)

    total_zero_intensity_count_all = self.mpi_helper.sum(zero_intensity_count_all)
    total_zero_intensity_count_resolution_limited = self.mpi_helper.sum(zero_intensity_count_resolution_limited)

    total_positive_intensity_count_all = self.mpi_helper.sum(positive_intensity_count_all)
    total_positive_intensity_count_resolution_limited = self.mpi_helper.sum(positive_intensity_count_resolution_limited)

    total_negative_intensity_count_all = self.mpi_helper.sum(negative_intensity_count_all)
    total_negative_intensity_count_resolution_limited = self.mpi_helper.sum(negative_intensity_count_resolution_limited)

    # Build a histogram of all intensities
    if self.mpi_helper.rank == 0:

      self.logger.main_log("Total reflections (I == 0.0): \t\t%d"%(total_zero_intensity_count_all))
      self.logger.main_log("Total reflections (I > 0.0): \t\t%d"%(total_positive_intensity_count_all))
      self.logger.main_log("Total reflections (I < 0.0): \t\t%d\n"%(total_negative_intensity_count_all))

      self.logger.main_log("Resolution-limited reflections (I == 0.0): \t\t%d"%(total_zero_intensity_count_resolution_limited))
      self.logger.main_log("Resolution-limited reflections (I > 0.0): \t\t%d"%(total_positive_intensity_count_resolution_limited))
      self.logger.main_log("Resolution-limited reflections (I < 0.0): \t\t%d\n"%(total_negative_intensity_count_resolution_limited))

      #self.histogram(all_used_intensities)
    self.logger.log_step_time("INTENSITY_HISTOGRAM", True)

  def build_intensity_table(self,
                            n_sum,
                            m_sum,
                            mm_sum,
                            I_sum,
                            Isig_sum,
                            unmerged_meanIsig,
                            unmerged_stddevIsig,
                            unmerged_skewIsig):
    '''Produce a table with hkl intensity statistics for resolution bins'''
    Intensity_Table = intensity_table()

    for i_bin in self.resolution_binner.range_used():
      theor_asu_count             = self.resolution_binner.counts()[i_bin]
      observed_asu_count          = n_sum[i_bin]
      observed_count              = m_sum[i_bin]
      multiply_observed_asu_count = mm_sum[i_bin]
      intensity_sum               = I_sum[i_bin]
      intensity_to_sigma_sum      = Isig_sum[i_bin]

      if observed_asu_count > 0:
        mean_I = mean_Isig = 0
        if multiply_observed_asu_count > 0:
          mean_I      = intensity_sum / multiply_observed_asu_count
          mean_Isig   = intensity_to_sigma_sum / multiply_observed_asu_count

        res_bin = intensity_table_bin(i_bin                 = i_bin,
                                  d_range                   = self.resolution_binner.bin_legend(i_bin=i_bin, show_bin_number=False, show_counts=False),
                                  d_min                     = self.resolution_binner.bin_d_min(i_bin),
                                  redundancy_asu            = observed_count / theor_asu_count,
                                  redundancy_obs            = observed_count / observed_asu_count,
                                  redundancy_to_edge        = 0, # TODO
                                  complete_tag              = "[%d/%d]" % (observed_asu_count, theor_asu_count),
                                  completeness              = observed_asu_count / theor_asu_count,
                                  measurements              = observed_count,
                                  multiply_measured_asu     = multiply_observed_asu_count,
                                  predictions               = None, # TODO
                                  mean_I                    = mean_I,
                                  mean_I_sigI               = mean_Isig,
                                  unmerged_meanIsig         = unmerged_meanIsig[i_bin] if unmerged_meanIsig is not None else 0,
                                  unmerged_stddevIsig       = unmerged_stddevIsig[i_bin] if unmerged_stddevIsig is not None else 0,
                                  unmerged_skewIsig         = unmerged_skewIsig[i_bin] if unmerged_skewIsig is not None else 0)

        Intensity_Table.table.append(res_bin)

      Intensity_Table.cumulative_observed_asu_count                   += observed_asu_count
      Intensity_Table.cumulative_observed_count                       += observed_count
      Intensity_Table.cumulative_theor_asu_count                      += theor_asu_count
      Intensity_Table.cumulative_multiply_observed_asu_count          += multiply_observed_asu_count
      Intensity_Table.cumulative_I                                    += intensity_sum
      Intensity_Table.cumulative_Isigma                               += intensity_to_sigma_sum

    '''
    # TODO
    if redundancy_to_edge is not None:
      cumulative_n_pred += flex.sum(sel_redundancy_pred)
      cumulative_pred   += redundancy_to_edge
    '''
    return Intensity_Table

  def histogram(self, data):
    from matplotlib import pyplot as plt
    nslots = 100
    histogram = flex.histogram(
                               data=data,
                               n_slots=nslots)
    out = StringIO()
    histogram.show(f=out, prefix="    ", format_cutoffs="%6.2f")
    self.logger.main_log(out.getvalue() + '\n' + "Total: %d"%data.size() + '\n')

    if False:
      fig = plt.figure()
      plt.bar(histogram.slot_centers(), histogram.slots(), align="center", width=histogram.slot_width())
      plt.show()

  def build_cross_correlation_table(self,
                                    count_array,
                                    sum_xx_array,
                                    sum_yy_array,
                                    sum_xy_array,
                                    sum_x_array,
                                    sum_y_array):

    Cross_Correlation_Table = cross_correlation_table()

    cumulative_observed_matching_asu_count = 0
    cumulative_theor_asu_count = 0
    cumulative_sum_xx  = 0.0
    cumulative_sum_yy  = 0.0
    cumulative_sum_xy  = 0.0
    cumulative_sum_x   = 0.0
    cumulative_sum_y   = 0.0

    for i_bin in self.resolution_binner.range_used():
      count   = count_array[i_bin]
      sum_xx  = sum_xx_array[i_bin]
      sum_yy  = sum_yy_array[i_bin]
      sum_xy  = sum_xy_array[i_bin]
      sum_x   = sum_x_array[i_bin]
      sum_y   = sum_y_array[i_bin]

      cross_correlation = self.cross_correlation_formula(count,
                                                         sum_xx,
                                                         sum_yy,
                                                         sum_xy,
                                                         sum_x,
                                                         sum_y);

      Cross_Correlation_Table.table.append(
                    cross_correlation_resolution_bin(
                            i_bin = i_bin,
                            theor_asu_count = self.resolution_binner.counts()[i_bin],
                            observed_matching_asu_count = count,
                            cross_correlation = cross_correlation))

      cumulative_observed_matching_asu_count += count
      cumulative_theor_asu_count += self.resolution_binner.counts()[i_bin]
      cumulative_sum_xx  += sum_xx
      cumulative_sum_yy  += sum_yy
      cumulative_sum_xy  += sum_xy
      cumulative_sum_x   += sum_x
      cumulative_sum_y   += sum_y

    Cross_Correlation_Table.cumulative_observed_matching_asu_count = cumulative_observed_matching_asu_count
    Cross_Correlation_Table.cumulative_theor_asu_count = cumulative_theor_asu_count
    Cross_Correlation_Table.cumulative_cross_correlation = self.cross_correlation_formula(cumulative_observed_matching_asu_count,
                                                                                          cumulative_sum_xx,
                                                                                          cumulative_sum_yy,
                                                                                          cumulative_sum_xy,
                                                                                          cumulative_sum_x,
                                                                                          cumulative_sum_y)
    return Cross_Correlation_Table


  def cross_correlation_formula(self,
                                count,
                                sum_xx,
                                sum_yy,
                                sum_xy,
                                sum_x,
                                sum_y):

    numerator = (count * sum_xy - sum_x * sum_y)
    denominator = (math.sqrt(count * sum_xx - sum_x**2) * math.sqrt(count * sum_yy - sum_y**2))
    cross_correlation = 0.0
    if denominator != 0.0:
      cross_correlation = numerator / denominator

    return cross_correlation

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(intensity_resolution_statistics)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/statistics/intensity_resolution_statistics_cxi.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from xfel.merging.application.worker import worker
from dials.array_family import flex
import math
from libtbx.str_utils import format_value
from libtbx import table_utils
from cctbx.crystal import symmetry
from cctbx.miller import binned_data
import os
from iotbx import mtz

class intensity_resolution_statistics_cxi(worker):
  '''Calculates hkl intensity statistics for resolution bins using adapted cxi-xmerge code'''

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(intensity_resolution_statistics_cxi, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Intensity resolution statistics (cxi-xmerge method)'

  def run(self, experiments, reflections):
    self.logger.log_step_time("INTENSITY_STATISTICS CXI")

    reflections_before_stats_count = reflections.size()
    total_reflections_before_stats_count = self.mpi_helper.sum(reflections_before_stats_count)
    if self.mpi_helper.rank == 0:
      self.logger.main_log("Total reflections before doing intensity statistics (cxi-xmerge method): %d"%(total_reflections_before_stats_count))

    if self.mpi_helper.rank == 0:
      self.run_cc()

    self.logger.log_step_time("INTENSITY_STATISTICS CXI", True)

    return experiments, reflections

  def run_cc(self):
    uniform, selected_uniform, have_iso_ref = self.load_cc_data()

    include_negatives = True
    if have_iso_ref:
      slope, offset, corr_iso, N_iso = self.correlation(
        selected_uniform[1], selected_uniform[0], include_negatives)
      self.logger.main_log("C.C. iso is %.1f%% on %d indices"%(100 * corr_iso, N_iso))

    slope, offset, corr_int, N_int = self.correlation(selected_uniform[2], selected_uniform[3], include_negatives)
    self.logger.main_log("C.C. int is %.1f%% on %d indices"%(100.*corr_int, N_int))

    if have_iso_ref:
      binned_cc_ref, binned_cc_ref_N = self.binned_correlation(
        selected_uniform[1], selected_uniform[0], include_negatives)
      #binned_cc_ref.show(f=output)

      ref_scale = self.scale_factor(
        selected_uniform[1], selected_uniform[0],
        weights=flex.pow(selected_uniform[1].sigmas(), -2),
        use_binning=True)
      #ref_scale.show(f=output)

      ref_riso = self.r1_factor(
        selected_uniform[1], selected_uniform[0],
        scale_factor=ref_scale, use_binning=True)
      #ref_riso.show(f=output)

      ref_scale_all = self.scale_factor(
        selected_uniform[1], selected_uniform[0],
        weights=flex.pow(selected_uniform[1].sigmas(), -2))

      ref_riso_all = self.r1_factor(
        selected_uniform[1], selected_uniform[0],
        scale_factor=ref_scale_all)

    binned_cc_int,binned_cc_int_N = self.binned_correlation(
      #selected_uniform[2], selected_uniform[3], params.include_negatives)
      selected_uniform[2], selected_uniform[3], True)
    #binned_cc_int.show(f=output)

    oe_scale = self.scale_factor(selected_uniform[2],selected_uniform[3],
      weights = flex.pow(selected_uniform[2].sigmas(),-2)
              + flex.pow(selected_uniform[3].sigmas(),-2),
      use_binning=True)
    #oe_scale.show(f=output)

    oe_rint = self.r1_factor(selected_uniform[2],selected_uniform[3],
                         scale_factor = oe_scale, use_binning=True)
    #oe_rint.show(f=output)

    oe_rsplit = self.r_split(selected_uniform[2], selected_uniform[3],
                        use_binning=True)

    oe_scale_all = self.scale_factor(selected_uniform[2],selected_uniform[3],
      weights = flex.pow(selected_uniform[2].sigmas(),-2)
              + flex.pow(selected_uniform[3].sigmas(),-2),)

    oe_rint_all = self.r1_factor(selected_uniform[2],selected_uniform[3],
                         scale_factor = oe_scale_all)
    oe_rsplit_all = self.r_split(selected_uniform[2], selected_uniform[3])
    if have_iso_ref:
      self.logger.main_log("R factors Riso = %.1f%%, Rint = %.1f%%"%(100.*ref_riso_all, 100.*oe_rint_all))
    else:
      self.logger.main_log("R factor Rint = %.1f%%"%(100.*oe_rint_all))

    split_sigma_data = self.split_sigma_test(selected_uniform[2],selected_uniform[3],
                                        scale=oe_scale,use_binning=True,show_plot=False)
    split_sigma_data_all = self.split_sigma_test(selected_uniform[2],selected_uniform[3],
                                            scale=oe_scale_all,use_binning=False,show_plot=False)

    self.logger.main_log('')
    if self.params.scaling.model_reindex_op == "h,k,l":
      self.logger.main_log("Table of Scaling Results:")
    else:
      self.logger.main_log("Table of Scaling Results with Model Reindexing as %s:"%reindexing_op)

    from libtbx import table_utils
    table_header = ["","","","CC"," N","CC"," N","R","R","R","Scale","Scale","SpSig"]
    table_header2 = ["Bin","Resolution Range","Completeness","int","int","iso","iso","int","split","iso","int","iso","Test"]
    table_data = []
    table_data.append(table_header)
    table_data.append(table_header2)

    items = binned_cc_int.binner.range_used()

    # XXX Make it clear what the completeness here actually is!
    cumulative_counts_given = 0
    cumulative_counts_complete = 0
    for bin in items:
      table_row = []
      table_row.append("%3d"%bin)
      table_row.append("%-13s"%binned_cc_int.binner.bin_legend(i_bin=bin,show_bin_number=False,show_bin_range=False,
                                                   show_d_range=True, show_counts=False))
      table_row.append("%13s"%binned_cc_int.binner.bin_legend(i_bin=bin,show_bin_number=False,show_bin_range=False,
                                                   show_d_range=False, show_counts=True))
      cumulative_counts_given += binned_cc_int.binner._counts_given[bin]
      cumulative_counts_complete += binned_cc_int.binner._counts_complete[bin]
      table_row.append("%.1f%%"%(100.*binned_cc_int.data[bin]))
      table_row.append("%7d"%(binned_cc_int_N.data[bin]))

      if have_iso_ref and binned_cc_ref.data[bin] is not None:
        table_row.append("%.1f%%" % (100 * binned_cc_ref.data[bin]))
      else:
        table_row.append("--")

      if have_iso_ref and binned_cc_ref_N.data[bin] is not None:
        table_row.append("%6d" % (binned_cc_ref_N.data[bin]))
      else:
        table_row.append("--")

      if oe_rint.data[bin] is not None:
        table_row.append("%.1f%%"%(100.*oe_rint.data[bin]))
      else:
        table_row.append("--")

      if oe_rsplit.data[bin] is not None:
        table_row.append("%.1f%%" % (100 * oe_rsplit.data[bin]))
      else:
        table_row.append("--")

      if have_iso_ref and ref_riso.data[bin] is not None:
        table_row.append("%.1f%%" % (100 * ref_riso.data[bin]))
      else:
        table_row.append("--")

      if oe_scale.data[bin] is not None:
        table_row.append("%.3f"%oe_scale.data[bin])
      else:
        table_row.append("--")

      if have_iso_ref and ref_scale.data[bin] is not None:
        table_row.append("%.3f" % ref_scale.data[bin])
      else:
        table_row.append("--")

      if split_sigma_data.data[bin] is not None:
        table_row.append("%.4f" % split_sigma_data.data[bin])
      else:
        table_row.append("--")

      table_data.append(table_row)
    table_data.append([""]*len(table_header))

    table_row = [format_value("%3s",   "All"),
                 format_value("%-13s", "                 "),
                 format_value("%13s",  "[%d/%d]"%(cumulative_counts_given,
                                                  cumulative_counts_complete)),
                 format_value("%.1f%%", 100 * corr_int),
                 format_value("%7d", N_int)]

    if have_iso_ref:
      table_row.extend((format_value("%.1f%%", 100 * corr_iso),
                        format_value("%6d", N_iso)))
    else:
      table_row.extend(("--", "--"))

    table_row.extend((format_value("%.1f%%", 100 * oe_rint_all),
                      format_value("%.1f%%", 100 * oe_rsplit_all)))
    if have_iso_ref:
      table_row.append(format_value("%.1f%%", 100 * ref_riso_all))
    else:
      table_row.append("--")

    table_row.append(format_value("%.3f", oe_scale_all))
    if have_iso_ref:
      table_row.append(format_value("%.3f", ref_scale_all))
    else:
      table_row.append("--")

    if split_sigma_data_all is not None:
      table_row.append("%.1f" % split_sigma_data_all)
    else:
      table_row.append("--")

    table_data.append(table_row)

    self.logger.main_log(' ')
    self.logger.main_log(table_utils.format(table_data,has_header=2,justify='center',delim=" "))
    self.logger.main_log("""CCint is the CC-1/2 defined by Diederichs; correlation between odd/even images.
    Similarly, Scale int and R int are the scaling factor and scaling R factor between odd/even images.
    "iso" columns compare the whole XFEL dataset to the isomorphous reference.""")

    self.logger.main_log("Niso: result vs. reference common set")

    if have_iso_ref:
      assert N_iso == flex.sum(flex.double([x for x in binned_cc_ref_N.data if x is not None]))
    assert N_int == flex.sum(flex.double([x for x in binned_cc_int_N.data if x is not None]))

    # TODO: how is plotting handled in the new phil design?
    '''
    if params.scaling.show_plots:
      from matplotlib import pyplot as plt
      plt.plot(flex.log(selected_uniform[-2].data()),
               flex.log(selected_uniform[-1].data()), 'r.')
      plt.show()
      if have_iso_ref:
        plt.plot(flex.log(selected_uniform[0].data()),
                 flex.log(selected_uniform[1].data()), 'r.')
        plt.show()
    '''
    self.logger.main_log(' ')

  def scale_factor(self, this, other, weights=None, cutoff_factor=None,
                     use_binning=False):
      """
      The analytical expression for the least squares scale factor.

      K = sum(w * yo * yc) / sum(w * yc^2)

      If the optional cutoff_factor argument is provided, only the reflections
      whose magnitudes are greater than cutoff_factor * max(yo) will be included
      in the calculation.
      """
      assert not use_binning or this.binner() is not None
      if use_binning: assert cutoff_factor is None
      assert other.size() == this.data().size()
      if not use_binning:
        if this.data().size() == 0: return None
        obs = this.data()
        calc = other.data()
        if cutoff_factor is not None:
          assert cutoff_factor < 1
          sel = obs >= flex.max(this.data()) * cutoff_factor
          obs = obs.select(sel)
          calc = calc.select(sel)
          if weights is not None:
            weights = weights.select(sel)
        if weights is None:
          return flex.sum(obs*calc) / flex.sum(flex.pow2(calc))
        else:
          return flex.sum(weights * obs * calc) \
               / flex.sum(weights * flex.pow2(calc))
      results = []
      for i_bin in this.binner().range_all():
        sel = this.binner().selection(i_bin)
        weights_sel = None
        if weights is not None:
          weights_sel = weights.select(sel)
        results.append(
          self.scale_factor(this.select(sel),other.select(sel), weights_sel))
      return binned_data(binner=this.binner(), data=results, data_fmt="%7.4f")

  def split_sigma_test(self, this, other, scale, use_binning=False, show_plot=False):
    """
    Calculates the split sigma ratio test by Peter Zwart:
    ssr = sum( (Iah-Ibh)^2 ) / sum( sigma_ah^2 + sigma_bh^2)

    where Iah and Ibh are merged intensities for a given hkl from two halves of
    a dataset (a and b). Likewise for sigma_ah and sigma_bh.

    ssr (split sigma ratio) should approximately equal 1 if the errors are correctly estimated.
    """

    assert other.size() == this.data().size()
    assert (this.indices() == other.indices()).all_eq(True)
    assert not use_binning or this.binner() is not None

    if use_binning:
      results = []
      for i_bin in this.binner().range_all():
        sel = this.binner().selection(i_bin)
        i_this = this.select(sel)
        i_other = other.select(sel)
        scale_rel = scale.data[i_bin]
        if i_this.size() == 0:
          results.append(None)
        else:
          results.append(self.split_sigma_test(i_this,i_other,scale=scale_rel,show_plot=show_plot))
      return binned_data(binner=this.binner(), data=results, data_fmt="%7.4f")

    a_data = this.data(); b_data = scale * other.data()
    a_sigmas = this.sigmas(); b_sigmas = scale * other.sigmas()

    if show_plot:
      """
      # Diagnostic use of the (I - <I>) / sigma distribution, should have mean=0, std=1
      a_variance = a_sigmas * a_sigmas
      b_variance = b_sigmas * b_sigmas
      mean_num = (a_data/ (a_variance) ) + (b_data/ (b_variance) )
      mean_den = (1./ (a_variance) ) + (1./ (b_variance) )
      mean_values = mean_num / mean_den

      delta_I_a = a_data - mean_values
      normal_a = delta_I_a / (a_sigmas)
      stats_a = flex.mean_and_variance(normal_a)
      print "\nA mean %7.4f std %7.4f"%(stats_a.mean(),stats_a.unweighted_sample_standard_deviation())
      order_a = flex.sort_permutation(normal_a)

      delta_I_b = b_data - mean_values
      normal_b = delta_I_b / (b_sigmas)
      stats_b = flex.mean_and_variance(normal_b)
      print "B mean %7.4f std %7.4f"%(stats_b.mean(),stats_b.unweighted_sample_standard_deviation())
      order_b = flex.sort_permutation(normal_b)
      # plots for debugging
      from matplotlib import pyplot as plt
      plt.plot(range(len(order_a)),normal_a.select(order_a),"b.")
      plt.plot(range(len(order_b)),normal_b.select(order_b),"r.")
      plt.show()
      """
      from cctbx.examples.merging.sigma_correction import ccp4_model
      Correction = ccp4_model()
      Correction.plots(a_data, b_data, a_sigmas, b_sigmas)
      #a_new_variance,b_new_variance = Correction.optimize(a_data, b_data, a_sigmas, b_sigmas)
      #Correction.plots(a_data, b_data, flex.sqrt(a_new_variance), flex.sqrt(b_new_variance))

    n = flex.pow(a_data - b_data,2)
    d = flex.pow(a_sigmas,2)+flex.pow(b_sigmas,2)

    return flex.sum(n)/flex.sum(d)

  def r1_factor(self, this, other, scale_factor=None, assume_index_matching=False,
                  use_binning=False):
      r"""Get the R1 factor according to this formula

      .. math::
         R1 = \dfrac{\sum{||F| - k|F'||}}{\sum{|F|}}

      where F is this.data() and F' is other.data() and
      k is the factor to put F' on the same scale as F"""
      assert not use_binning or this.binner() is not None
      assert other.indices().size() == this.indices().size()
      if not use_binning:
        if this.data().size() == 0: return None
        if (assume_index_matching):
          o, c = this, other
        else:
          o, c = this.common_sets(other=other, assert_no_singles=True)
        o  = flex.abs(o.data())
        c = flex.abs(c.data())
        if (scale_factor is None):
          den = flex.sum(c * c)
          if (den != 0):
            c *= (flex.sum(o * c) / den)
        elif (scale_factor is not None):
          c *= scale_factor
        return flex.sum(flex.abs(o - c)) / flex.sum(o)
      results = []
      for i_bin in this.binner().range_all():
        sel = this.binner().selection(i_bin)
        results.append(self.r1_factor(this.select(sel),
          other.select(sel), scale_factor.data[i_bin], assume_index_matching))
      return binned_data(binner=this.binner(), data=results, data_fmt="%7.4f")

  def r_split(self, this, other, assume_index_matching=False, use_binning=False):
      # Used in Boutet et al. (2012), which credit it to Owen et al
      # (2006).  See also R_mrgd_I in Diederichs & Karplus (1997)?
      # Barends cites Collaborative Computational Project Number 4. The
      # CCP4 suite: programs for protein crystallography. Acta
      # Crystallogr. Sect. D-Biol. Crystallogr. 50, 760-763 (1994) and
      # White, T. A. et al. CrystFEL: a software suite for snapshot
      # serial crystallography. J. Appl. Cryst. 45, 335-341 (2012).
      if not use_binning:
        assert other.indices().size() == this.indices().size()
        if this.data().size() == 0:
          return None

        if assume_index_matching:
          (o, c) = (this, other)
        else:
          (o, c) = this.common_sets(other=other, assert_no_singles=True)

        # The case where the denominator is less or equal to zero is
        # pathological and should never arise in practice.
        den = flex.sum(flex.abs(o.data() + c.data()))
        assert den > 0
        return math.sqrt(2) * flex.sum(flex.abs(o.data() - c.data())) / den

      assert this.binner is not None
      results = []
      for i_bin in this.binner().range_all():
        sel = this.binner().selection(i_bin)
        results.append(self.r_split(this.select(sel), other.select(sel),
          assume_index_matching=assume_index_matching,
          use_binning=False))
      return binned_data(binner=this.binner(), data=results, data_fmt='%7.4f')

  def binned_correlation(self, this, other, include_negatives=False):
      results = []
      bin_count = []
      for i_bin in this.binner().range_all():
        sel = this.binner().selection(i_bin)
        if sel.count(True)==0:
          results.append(0.)
          bin_count.append(0.)
          continue
        result_tuple = self.correlation(this.select(sel), other.select(sel), include_negatives)
        results.append(result_tuple[2])
        bin_count.append(result_tuple[3])
        # plots for debugging
        #from matplotlib import pyplot as plt
        #plt.plot(flex.log(this.select(sel).data()),flex.log(other.select(sel).data()),"b.")
        #plt.show()

      return binned_data(binner=this.binner(), data=results, data_fmt="%7.4f"),\
             binned_data(binner=this.binner(), data=bin_count, data_fmt="%7d")

  def correlation(self, this, other, include_negatives=False):
      N = 0
      sum_xx = 0
      sum_xy = 0
      sum_yy = 0
      sum_x = 0
      sum_y = 0
      for idx in range(this.indices().size()):

        assert this.indices()[idx]==other.indices()[idx]
        I_r = other.data()[idx]
        I_o = this.data()[idx]
        #assert I_r >= 0. or I_o >= 0.
        #why does this go from 81% to 33% when uniform selection is made?

        if not include_negatives and (I_r < 0. or I_o < 0.): continue
        #print "%15s %15s %10.0f %10.0f"%(
      #this.indices()[idx], this.indices()[idx],
      #this.data()[idx], other.data()[idx],)
        N      += 1
        sum_xx += I_r**2
        sum_yy += I_o**2
        sum_xy += I_r * I_o
        sum_x  += I_r
        sum_y  += I_o
      # Linearly fit I_r to I_o, i.e. find slope and offset such that
      # I_o = slope * I_r + offset, optimal in a least-squares sense.
      if N < 2:  return 0,0,0,N
      slope = (N * sum_xy - sum_x * sum_y) / (N * sum_xx - sum_x**2)
      offset = (sum_xx * sum_y - sum_x * sum_xy) / (N * sum_xx - sum_x**2)
      corr  = (N * sum_xy - sum_x * sum_y) / (math.sqrt(N * sum_xx - sum_x**2) *
               math.sqrt(N * sum_yy - sum_y**2))
      return slope,offset,corr,N

  def load_mpi_merge_data(self, filename_postfix):
    filepath = os.path.join(self.params.output.output_dir, "%s_%s.mtz"%(self.params.output.prefix, filename_postfix))
    data = mtz.object(filepath)
    return data

  def load_cc_data(self):
    if self.params.scaling.model_reindex_op != "h,k,l":
      self.logger.main_log('''Recalculating after reindexing the new data with %s
       (it is necessary to pick which indexing choice gives the sensible CC iso):'''%self.params.scaling.model_reindex_op)

    try:
      model_file_path = self.params.statistics.cciso.mtz_file
      assert model_file_path.endswith(("mtz", "sf.cif"))
      # support both old-style *.mtz and structure factor *-sf.cif
      from iotbx import reflection_file_reader
      data_SR = reflection_file_reader.any_reflection_file(
                file_name = model_file_path)
      have_iso_ref = True
    except (RuntimeError, AttributeError): # Attribute error if model_file_path is None
      data_SR = None
      have_iso_ref = False

    data_d0 = self.load_mpi_merge_data('all')
    data_d1 = self.load_mpi_merge_data('odd')
    data_d2 = self.load_mpi_merge_data('even')

    uniform = []
    for idx, item in enumerate([data_SR, data_d0, data_d1, data_d2]):
      if not have_iso_ref and idx == 0:
        uniform.append(None)
        continue
      #item.show_summary()
      self.logger.main_log("-------------------------------")
      for array in item.as_miller_arrays():
         this_label = array.info().label_string().lower()
         self.logger.main_log(this_label + ' ' + self.params.statistics.cciso.mtz_column_F)
         if this_label.find("fobs") >= 0:
           self.logger.main_log(this_label + ' ' + str(array.observation_type()))
           uniform.append(array.as_intensity_array())
           break
         if this_label.find("iobs") >= 0:
           """This test is added for the use case of unmerged anomalous data without
              an isomorphous reference (in other words, mark1).  Without this,
              the unmerged reflections are not picked up by the cc comparison.
              Indicates that this section probably has to be reanalyzed and redesigned.
           """
           self.logger.main_log(this_label + ' ' + str(array.observation_type()))
           uniform.append(array.as_intensity_array())
           break
         if this_label.find("imean") >= 0:
           self.logger.main_log(this_label + ' ' + str(array.observation_type()))
           uniform.append(array.as_intensity_array())
           break
         if self.params.statistics.cciso.mtz_column_F.lower() in this_label:
           self.logger.main_log(this_label + ' ' + str(array.observation_type()))
           uniform.append(array.as_intensity_array())
           break

    # If necesssary, generate Bijvoet mates for the isomorphous reference.
    if have_iso_ref \
       and not self.params.merging.merge_anomalous \
       and not uniform[0].anomalous_flag():
        uniform[0] = uniform[0].generate_bijvoet_mates()

    # TODO: Shouldn't reindexing be done as part of filtering?

    #for x in [1,2,3]:
    #  # reindex the experimental data
    #  uniform[x] = uniform[x].change_basis(reindexing_op).map_to_asu()

    d_max_min = uniform[1].d_max_min()
    if have_iso_ref:
      sgi = uniform[0].space_group_info()
    else:
      sgi = self.params.scaling.space_group

    unit_cell = uniform[1].unit_cell()
    unit_cell_formatted = "(%.6f, %.6f, %.6f, %.3f, %.3f, %.3f)"\
                      %(unit_cell.parameters()[0], unit_cell.parameters()[1], unit_cell.parameters()[2], \
                        unit_cell.parameters()[3], unit_cell.parameters()[4], unit_cell.parameters()[5])
    self.logger.main_log("\nUnit cell: %s\n"%unit_cell_formatted)

    for x in [0,1,2,3]:
      if not have_iso_ref and x == 0:
        continue
      self.logger.main_log("%6d indices:"%uniform[x].size() + ' ' + {0:"Reference intensities",
                       1:"Merged structure factors",
                       2:"Semi-dataset 1",
                       3:"Semi-dataset 2"}[x])

      uniform[x] = uniform[x].customized_copy(
        crystal_symmetry = symmetry(unit_cell=uniform[1].unit_cell(), space_group_info=sgi),
        ).resolution_filter(d_min = d_max_min[1], d_max = self.params.merging.d_max,
        ).complete_array(d_min = d_max_min[1], d_max = self.params.merging.d_max).map_to_asu()

    self.logger.main_log("%6d indices: An asymmetric unit in the resolution interval %.2f - %.2f Angstrom"%(
       uniform[1].size(), d_max_min[0], d_max_min[1]))

    if have_iso_ref:
      uniform[0] = uniform[0].common_set(uniform[1])
      assert len(uniform[0].indices()) == len(uniform[1].indices())

    uniform[2] = uniform[2].common_set(uniform[1])
    uniform[3] = uniform[3].common_set(uniform[1])

    self.logger.main_log("-------------------------------")

    NBIN = self.params.statistics.n_bins

    for x in [0, 1, 2, 3]:
      if not have_iso_ref and x == 0:
        continue
      uniform[x].setup_binner(n_bins=NBIN)

    for x in range(len(uniform[1].indices())):
      if have_iso_ref:
        assert uniform[0].indices()[x] == uniform[1].indices()[x]
      assert uniform[1].indices()[x] == uniform[2].indices()[x]
      assert uniform[2].indices()[x] == uniform[3].indices()[x]

    selected_uniform = []
    if have_iso_ref:
      # quickly circumvent the odd case where the reference intensities have no sigmas
      # (which is the case for model F's)
      if uniform[0].sigmas() is None:
        uniform[0].set_sigmas( uniform[0].data() )
      uniformA = (uniform[0].sigmas() > 0.).__and__(uniform[1].sigmas() > 0.)

      for x in [0, 1]:
        selected_uniform.append(uniform[x].select(uniformA))
        selected_uniform[x].setup_binner(
          d_max=(self.params.merging.d_max or 100000), d_min=self.params.merging.d_min, n_bins=NBIN)

    else:
      selected_uniform = [None, None]

    uniformB = (uniform[2].sigmas() > 0.).__and__(uniform[3].sigmas() > 0.)

    for x in [2, 3]:
      selected_uniform.append(uniform[x].select(uniformB))
      selected_uniform[x].setup_binner(
        d_max=(self.params.merging.d_max or 100000), d_min=self.params.merging.d_min, n_bins=NBIN)

    return uniform, selected_uniform, have_iso_ref

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(intensity_resolution_statistics_cxi)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/statistics/unit_cell_statistics.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.worker import worker
from dials.array_family import flex
from six.moves import cStringIO as StringIO
from six.moves import zip
from cctbx import uctbx

histogram_slots = 20

class unit_cell_distribution(object):
  """Container for collecting unit cell statistics"""
  # TODO make this more general - currently assumes that angles are fixed,
  # which is true for the systems studied so far
  def __init__(self, reference_unit_cell, logger, mpi_helper, precision=None):
    self.reference_unit_cell = reference_unit_cell
    self.logger = logger
    self.mpi_helper = mpi_helper
    if precision is None: self.uc_precision = 2
    else: self.uc_precision = precision

    # this rank cell values
    self.uc_a_values = flex.double()
    self.uc_b_values = flex.double()
    self.uc_c_values = flex.double()

    self.uc_alpha_values  = flex.double()
    self.uc_beta_values   = flex.double()
    self.uc_gamma_values  = flex.double()

    # all ranks cell values
    self.all_uc_a_values = flex.double()
    self.all_uc_b_values = flex.double()
    self.all_uc_c_values = flex.double()

    self.all_uc_alpha_values  = flex.double()
    self.all_uc_beta_values   = flex.double()
    self.all_uc_gama_values   = flex.double()

  def add_cell(self, unit_cell):
    if unit_cell is None:
      return
    (a,b,c,alpha,beta,gamma) = unit_cell.parameters()
    self.uc_a_values.append(a)
    self.uc_b_values.append(b)
    self.uc_c_values.append(c)

    self.uc_alpha_values.append(alpha)
    self.uc_beta_values.append(beta)
    self.uc_gamma_values.append(gamma)

  def collect_from_all_ranks(self):
    self.all_uc_a_values = self.mpi_helper.aggregate_flex(self.uc_a_values, flex.double, root=None)
    self.all_uc_b_values = self.mpi_helper.aggregate_flex(self.uc_b_values, flex.double, root=None)
    self.all_uc_c_values = self.mpi_helper.aggregate_flex(self.uc_c_values, flex.double, root=None)

    self.all_uc_alpha_values  = self.mpi_helper.aggregate_flex(self.uc_alpha_values, flex.double, root=None)
    self.all_uc_beta_values   = self.mpi_helper.aggregate_flex(self.uc_beta_values, flex.double, root=None)
    self.all_uc_gamma_values  = self.mpi_helper.aggregate_flex(self.uc_gamma_values, flex.double, root=None)

  def is_valid(self):
    return len(self.all_uc_a_values) > 0 and len(self.all_uc_b_values) > 0 and len(self.all_uc_c_values) > 0 and \
           len(self.all_uc_alpha_values) > 0 and len(self.all_uc_beta_values) > 0 and len(self.all_uc_gamma_values) > 0

  def show_histograms(self, n_slots=histogram_slots):
    assert self.mpi_helper.rank == 0

    if self.reference_unit_cell is None:
      a0 = b0 = c0 = alpha0 = beta0 = gamma0 = None
    else:
      a0,b0,c0,alpha0,beta0,gamma0 = self.reference_unit_cell.parameters()

    self.logger.main_log("")

    labels = ["a","b","c"]

    ref_edges = [a0,b0,c0]

    def _show_each(edges):
      for edge, ref_edge, label in zip(edges, ref_edges, labels):
        h = flex.histogram(edge, n_slots=n_slots)
        smin, smax = flex.min(edge), flex.max(edge)
        stats = flex.mean_and_variance(edge)

        self.logger.main_log("  %s edge"%label)
        range_template = "     range:     %6.{0}f - %.{0}f".format(self.uc_precision)
        self.logger.main_log(range_template %(smin, smax))
        mean_template = "     mean:      %6.{0}f +/- %6.{0}f on N = %d".format(self.uc_precision)
        self.logger.main_log(mean_template %(stats.mean(), stats.unweighted_sample_standard_deviation(), edge.size()))
        if ref_edge is not None:
          ref_template = "     reference: %6.{}f".format(self.uc_precision)
          self.logger.main_log(ref_template %ref_edge)

        out = StringIO()
        h.show(f=out, prefix="    ", format_cutoffs="%6.{}f".format(self.uc_precision))
        self.logger.main_log(out.getvalue() + '\n')

    edges = [self.all_uc_a_values, self.all_uc_b_values, self.all_uc_c_values]
    _show_each(edges)

  def get_average_cell(self):
    a = flex.mean(self.all_uc_a_values)
    b = flex.mean(self.all_uc_b_values)
    c = flex.mean(self.all_uc_c_values)

    alpha = flex.mean(self.all_uc_alpha_values)
    beta  = flex.mean(self.all_uc_beta_values)
    gamma = flex.mean(self.all_uc_gamma_values)

    return uctbx.unit_cell([a,b,c,alpha,beta,gamma])

class unit_cell_statistics(worker):

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(unit_cell_statistics, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Unit cell statistics'

  def run(self, experiments, reflections):
    self.logger.log_step_time("UNIT_CELL_STATISTICS")
    ucd = unit_cell_distribution(
        self.params.scaling.unit_cell,
        self.logger,
        self.mpi_helper,
        precision=self.params.statistics.uc_precision
        )
    for experiment in experiments:
      ucd.add_cell(experiment.crystal.get_unit_cell())
    ucd.collect_from_all_ranks()

    average_unit_cell = None
    if ucd.is_valid():
      if self.mpi_helper.rank == 0:
        ucd.show_histograms()
      average_unit_cell = ucd.get_average_cell()

    # save the average unit cell to the phil parameters
    if self.mpi_helper.rank == 0:
      self.logger.main_log("Average unit_cell %s is saved to phil parameters"%str(average_unit_cell))
    if not 'average_unit_cell' in (self.params.statistics).__dict__:
      self.params.statistics.__inject__('average_unit_cell', average_unit_cell)
    else:
      self.params.statistics.__setattr__('average_unit_cell', average_unit_cell)

    self.logger.log_step_time("UNIT_CELL_STATISTICS", True)

    return experiments, reflections

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(unit_cell_statistics)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/tdata/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/tdata/cell_listing.py
from __future__ import absolute_import, division, print_function

from xfel.merging.application.worker import worker
class simple_cell_listing(worker):
  '''A class that reads the experiments and writes the unit cell / space group list in text format.'''

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(simple_cell_listing, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Output unit cells in tdata format'

  def run(self, all_experiments, all_reflections):
    if self.mpi_helper.rank == 0:
      self.logger.log("Writing out a list of all unit cells")
  # START OUTPUT ALL UNIT CELLS
    all_results = []
    for experiment_id, experiment in enumerate(all_experiments):
          if experiment.identifier is None or len(experiment.identifier) == 0:
            experiment.identifier = create_experiment_identifier(experiment, experiments_filename, experiment_id)

          uc = experiment.crystal.get_unit_cell()
          sg = "".join(experiment.crystal.get_space_group().type().lookup_symbol().split())

          uc_ps = uc.parameters()
          result = "%f %f %f %f %f %f %s"%(uc_ps[0], uc_ps[1], uc_ps[2], uc_ps[3], uc_ps[4], uc_ps[5], sg)
          all_results.append(result)

    comm = self.mpi_helper.comm
    MPI = self.mpi_helper.MPI
    self.global_results = comm.reduce(all_results, MPI.SUM, 0)
    if self.mpi_helper.rank == 0:
      file_cells = open("%s.tdata"%(self.params.tdata.output_path), "w")
      for result in self.global_results:
        line =  str(result) + "\n"
        file_cells.write(line)
      file_cells.close()
      self.logger.main_log("output a list of %d unit cells"%len(self.global_results))
  # END OUTPUT ALL UNIT CELLS

    return all_experiments, all_reflections

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(simple_cell_listing)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/tdata/factory.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.tdata.cell_listing import simple_cell_listing
from xfel.merging.application.worker import factory as factory_base

""" Factory class for tdata cell listing """

class factory(factory_base):
  @staticmethod
  def from_parameters(params, additional_info=None, mpi_helper=None, mpi_logger=None):
     if params.tdata.output_path is not None:
      return [simple_cell_listing(params, mpi_helper, mpi_logger)]



 *******************************************************************************


 *******************************************************************************
xfel/merging/application/utils/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/utils/data_counter.py
from __future__ import absolute_import, division, print_function
from math import log10


class data_counter(object):
  def __init__(self, params, mpi_helper=None, mpi_logger=None):

    self.mpi_helper = mpi_helper
    if self.mpi_helper == None:
      from xfel.merging.application.mpi_helper import mpi_helper
      self.mpi_helper = mpi_helper()

    self.logger = mpi_logger
    if self.logger == None:
      from xfel.merging.application.mpi_logger import mpi_logger
      self.logger = mpi_logger(params)

  def count_each(self, experiments, reflections, verbose=False):
    self.logger.log_step_time("CALC_LOAD_STATISTICS")

    # MPI-gather individual counts
    comm = self.mpi_helper.comm

    # count experiments and reflections
    experiment_count = len(experiments) if experiments != None else 0
    reflection_count = len(reflections) if reflections != None else 0

    # count images
    if experiments != None:
      image_count = sum(len(iset) for iset in experiments.imagesets())
    else:
      image_count = 0

    experiment_count_list   = comm.gather(experiment_count, root=0)
    image_count_list        = comm.gather(image_count, root=0)
    reflection_count_list   = comm.gather(reflection_count, root=0)

    # rank 0: log data statistics
    if self.mpi_helper.rank == 0 and verbose:
      max_count = int(log10(max(image_count_list + reflection_count_list)))
      count_fmt = '%' + str(max_count + 1) + 'd'
      self.logger.main_log('Experiments by rank: ' + ', '.join([count_fmt % i for i in experiment_count_list]))
      self.logger.main_log('Images by rank:      ' + ', '.join([count_fmt % i for i in image_count_list]))
      self.logger.main_log('Reflections by rank: ' + ', '.join([count_fmt % i for i in reflection_count_list]))

    self.logger.log_step_time("CALC_LOAD_STATISTICS", True)

    return (experiment_count_list, image_count_list, reflection_count_list) # for load balancing purposes

  def count(self, experiments, reflections):
    self.logger.log_step_time("CALC_LOAD_STATISTICS")

    # count experiments and reflections
    experiment_count = len(experiments) if experiments != None else 0
    reflection_count = len(reflections) if reflections != None else 0

    # count images
    if experiments != None:
      image_count = sum(len(iset) for iset in experiments.imagesets())
    else:
      image_count = 0

    # MPI-reduce all counts
    comm = self.mpi_helper.comm
    MPI = self.mpi_helper.MPI

    total_experiment_count  = comm.reduce(experiment_count, MPI.SUM)
    min_experiment_count    = comm.reduce(experiment_count, MPI.MIN)
    max_experiment_count    = comm.reduce(experiment_count, MPI.MAX)

    total_image_count       = comm.reduce(image_count, MPI.SUM)
    min_image_count         = comm.reduce(image_count, MPI.MIN)
    max_image_count         = comm.reduce(image_count, MPI.MAX)

    total_reflection_count  = comm.reduce(reflection_count, MPI.SUM)
    min_reflection_count    = comm.reduce(reflection_count, MPI.MIN)
    max_reflection_count    = comm.reduce(reflection_count, MPI.MAX)

    # rank 0: log data statistics
    if self.mpi_helper.rank == 0:
      self.logger.main_log('Experiments: (total,min,max): %d, %d, %d'%(total_experiment_count, min_experiment_count, max_experiment_count))
      self.logger.main_log('Images:      (total,min,max): %d, %d, %d'%(total_image_count, min_image_count, max_image_count))
      self.logger.main_log('Reflections: (total,min,max): %d, %d, %d'%(total_reflection_count, min_reflection_count, max_reflection_count))

      if total_reflection_count == 0:
        self.mpi_helper.set_error("Zero reflection count.")

    self.mpi_helper.check_errors()

    self.logger.log_step_time("CALC_LOAD_STATISTICS", True)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/utils/memory_usage.py
from __future__ import absolute_import, division, print_function

def get_memory_usage():
  '''Return memory used by the process in MB'''
  import resource
  import platform
  # getrusage returns kb on linux, bytes on mac
  units_per_mb = 1024
  if platform.system() == "Darwin":
    units_per_mb = 1024*1024
  return int(resource.getrusage(resource.RUSAGE_SELF).ru_maxrss) / units_per_mb


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/validation/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/validation/application.py
from __future__ import absolute_import, division, print_function

class application:
  def __init__(self,params):

    self.params = params
    self.application_level_validation()

  def application_level_validation(self):
    assert [self.params.scaling.model, self.params.scaling.unit_cell].count(None) == 1, 'Provide only the model or a unit cell, not both'
    assert [self.params.scaling.model, self.params.scaling.space_group].count(None) == 1, 'Provide only the model or a space group, not both'


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/worker.py
from __future__ import absolute_import, division, print_function

"""
Base classes for the merging application
"""

class worker(object):
  """ Base class for the worker objects. Performs validation and does work using the run method """
  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    self.params = params

    self.mpi_helper = mpi_helper
    if self.mpi_helper == None:
      from xfel.merging.application.mpi_helper import mpi_helper
      self.mpi_helper = mpi_helper()

    self.logger = mpi_logger
    if self.logger == None:
      from xfel.merging.application.mpi_logger import mpi_logger
      self.logger = mpi_logger(self.params)

  def __repr__(self):
    return 'Unknown'

  def validate(self):
    """ Override to perform any validation of the input parameters """
    pass

  def run(self, experiments, reflections):
    """ Process the data """
    pass

class factory(object):
  """ Constructs worker objects """

  @staticmethod
  def from_parameters(param, additional_info=None, mpi_helper=None, mpi_logger=None):
    """ Construct a list of workers given the params object. The list contains all workers
        that comprise a single step, in the order that they will be executed """
    pass

def exercise_worker(worker_class):
  """ Boilerplate code for testing a worker class """
  from xfel.merging.application.phil.phil import phil_scope
  from dials.util.options import ArgumentParser
  # Create the parser
  parser = ArgumentParser(phil=phil_scope)

  # Parse the command line. quick_parse is required for MPI compatibility
  params, options = parser.parse_args(show_diff_phil=True,quick_parse=True)

  # Load the data for the worker
  if 'simple_file_loader' in str(worker_class):
    experiments = reflections = None
  else:
    from xfel.merging.application.input.file_loader import simple_file_loader
    loader = simple_file_loader(params)
    loader.validate()
    experiments, reflections = loader.run(None, None)

  worker = worker_class(params)
  worker.validate()
  experiments, reflections = worker.run(experiments, reflections)

  prefix = worker_class.__name__
  reflections.as_msgpack_file(prefix + ".mpack")
  experiments.as_file(prefix + ".expt")


 *******************************************************************************


 *******************************************************************************
xfel/merging/command_line/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/command_line/merge.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.merge

import os, sys

# vvv Temporary fix for https://github.com/dials/dials/issues/1998
# Remove this when https://github.com/pydata/numexpr/pull/400 and
# https://github.com/dials/dials/pull/2000 have been merged and released
if os.environ.get("CCTBX_NO_UUID", None) is not None:
  def mp_system(): return sys.platform.capitalize()
  def mp_machine(): return 'x86_64'
  import platform
  platform.system = mp_system
  platform.machine = mp_machine
# ^^^

from xfel.merging.application.mpi_helper import mpi_helper
from xfel.merging.application.mpi_logger import mpi_logger
from libtbx.mpi4py import mpi_abort_on_exception

# Note, when modifying this list, be sure to modify the README in xfel/merging

default_steps = [
  'input',
  'balance', # balance input load
  'model_scaling', # build full Miller list, model intensities, and resolution binner - for scaling and post-refinement
  'modify', # apply polarization correction, etc.
  'filter', # reject whole experiments or individual reflections
  'scale',
  'postrefine',
  'statistics_unitcell', # if required, save the average unit cell to the phil parameters
  'statistics_beam', # save the average wavelength to the phil parameters
  'model_statistics', # build full Miller list, model intensities, and resolution binner - for statistics. May use average unit cell.
  'statistics_resolution', # calculate resolution statistics for experiments
  'group', # re-distribute reflections over the ranks, so that all measurements of every HKL are gathered at the same rank
  'errors_merge', # correct errors using a per-HKL algorithm, e.g. errors_from_sample_residuals
  'statistics_intensity', # calculate resolution statistics for intensities
  'merge', # merge HKL intensities, MPI-gather all HKLs at rank 0, output "odd", "even" and "all" HKLs as mtz files
  'statistics_intensity_cxi', # run cxi_cc code ported from cxi-xmerge
]

class Script(object):
  '''A class for running the script.'''

  def __init__(self):
    self.mpi_helper = mpi_helper()
    self.mpi_logger = mpi_logger()

  def __del__(self):
    self.mpi_helper.finalize()

  def parse_input(self, args=None):
    '''Parse input at rank 0 and broadcast the input parameters and options to all ranks'''

    if self.mpi_helper.rank == 0:
      from xfel.merging.application.phil.phil import phil_scope
      help_message = '''Merge xfel data.'''

      # The script usage
      import libtbx.load_env
      self.usage = "usage: %s [options] [param.phil] " % libtbx.env.dispatcher_name
      self.parser = None

      '''Initialize the script.'''
      from dials.util.options import ArgumentParser
      # Create the parser
      self.parser = ArgumentParser(
        usage=self.usage,
        phil=phil_scope,
        epilog=help_message)

      # Parse the command line. quick_parse is required for MPI compatibility
      params, options = self.parser.parse_args(args, show_diff_phil=True,quick_parse=True)

      # Log the modified phil parameters
      diff_phil_str = self.parser.diff_phil.as_str()
      if diff_phil_str != "":
        self.mpi_logger.main_log("The following parameters have been modified:\n%s"%diff_phil_str)

      # prepare for transmitting input parameters to all ranks
      transmitted = dict(params = params, options = options)

      # make the output folders
      try:
        os.mkdir(params.output.output_dir)
      except FileExistsError:
        pass

    else:
      transmitted = None

    # broadcast parameters and options to all ranks
    self.mpi_logger.log("Broadcasting input parameters...")
    self.mpi_logger.log_step_time("BROADCAST_INPUT_PARAMS")

    transmitted = self.mpi_helper.comm.bcast(transmitted, root = 0)

    self.params = transmitted['params']
    self.options = transmitted['options']

    self.mpi_logger.set_log_file_paths(self.params)

    self.mpi_logger.log("Received input parameters and options")
    self.mpi_logger.log_step_time("BROADCAST_INPUT_PARAMS", True)

  @mpi_abort_on_exception
  def run(self, args=None):
    import datetime
    time_now = datetime.datetime.now()

    self.mpi_logger.log(str(time_now))
    if self.mpi_helper.rank == 0:
      self.mpi_logger.main_log(str(time_now))

    self.mpi_logger.log_step_time("TOTAL")

    self.mpi_logger.log_step_time("PARSE_INPUT_PARAMS")
    self.parse_input(args)
    self.mpi_logger.log_step_time("PARSE_INPUT_PARAMS", True)

    if self.params.mp.debug.cProfile:
      import cProfile
      pr = cProfile.Profile()
      pr.enable()

    # Create the workers using the factories
    self.mpi_logger.log_step_time("CREATE_WORKERS")
    from xfel.merging import application
    import importlib, copy

    self._resolve_persistent_columns()

    workers = []
    self.params.dispatch.step_list = self.params.dispatch.step_list or default_steps
    for step in self.params.dispatch.step_list:
      step_factory_name = step
      step_additional_info = []

      step_info = step.split('_')
      assert len(step_info) > 0
      if len(step_info) > 1:
        step_factory_name = step_info[0]
        step_additional_info = step_info[1:]

      try:
        factory = importlib.import_module('xfel.merging.application.' + step_factory_name + '.factory')
      except ModuleNotFoundError:
        custom_worker_path = os.environ.get('XFEL_CUSTOM_WORKER_PATH')
        if custom_worker_path is None: raise
        # remember the system path so the custom worker can temporarily modify it
        sys_path = copy.deepcopy(sys.path)
        pathstr = os.path.join(
            custom_worker_path, step_factory_name, 'factory.py'
        )

        modulename = 'xfel.merging.application.' + step_factory_name + '.factory'
        spec = importlib.util.spec_from_file_location(modulename, pathstr)
        factory = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(factory)
        # reset the path
        sys.path = sys_path

      workers.extend(factory.factory.from_parameters(self.params, step_additional_info, mpi_helper=self.mpi_helper, mpi_logger=self.mpi_logger))

    # Perform phil validation up front
    for worker in workers:
      worker.validate()
    self.mpi_logger.log_step_time("CREATE_WORKERS", True)

    # Do the work
    experiments = reflections = None
    step = 0
    while(workers):
      worker = workers.pop(0)
      self.mpi_logger.log_step_time("STEP_" + worker.__repr__())
      # Log worker name, i.e. execution step name
      step += 1
      if step > 1:
        self.mpi_logger.log('')
      step_desc = "STEP %d: %s"%(step, worker)
      self.mpi_logger.log(step_desc)

      if self.mpi_helper.rank == 0:
        if step > 1:
          self.mpi_logger.main_log('')
        self.mpi_logger.main_log(step_desc)

      # Execute worker
      experiments, reflections = worker.run(experiments, reflections)
      self.mpi_logger.log_step_time("STEP_" + worker.__repr__(), True)
      if experiments:
        self.mpi_logger.log("Ending step with %d experiments"%len(experiments))

    if self.params.output.save_experiments_and_reflections:
      if self.mpi_helper.size == 1:
        filename_suffix = ""
      else:
        filename_suffix = "_%06d"%self.mpi_helper.rank

      if len(reflections):
        assert 'id' in reflections
        reflections.as_file(os.path.join(self.params.output.output_dir, "%s%s.refl"%(self.params.output.prefix, filename_suffix)))
      if experiments:
        experiments.as_file(os.path.join(self.params.output.output_dir, "%s%s.expt"%(self.params.output.prefix, filename_suffix)))

    self.mpi_logger.log_step_time("TOTAL", True)

    if self.params.mp.debug.cProfile:
      pr.disable()
      pr.dump_stats(os.path.join(self.params.output.output_dir, "cpu_%s_%d.prof"%(self.params.output.prefix, self.mpi_helper.rank)))

  def _resolve_persistent_columns(self):
    if self.params.output.expanded_bookkeeping:
      if self.params.input.persistent_refl_cols is None:
        self.params.input.persistent_refl_cols = []
      keysCreatedByMerge = ["input_refl_index", "original_id", "file_list_mapping", "is_odd_experiment"]
      for key in keysCreatedByMerge:
        if key not in self.params.input.persistent_refl_cols:
          self.params.input.persistent_refl_cols.append(key)
    if "correlation_after_post" not in self.params.input.persistent_refl_cols:
      self.params.input.persistent_refl_cols.append("correlation_after_post")
    if "correlation" not in self.params.input.persistent_refl_cols:
      self.params.input.persistent_refl_cols.append("correlation")

if __name__ == '__main__':
  script = Script()

  result = script.run()


 *******************************************************************************


 *******************************************************************************
xfel/merging/command_line/mpi_integrate.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.mpi_integrate
from libtbx.phil import parse

default_steps = [
  'input',
  'balance', # balance input load
  'integrate',
]

from xfel.merging.application.input.file_loader import simple_file_loader
#class noprune_file_loader(simple_file_loader):
def prune_reflection_table_keys(self, reflections):
  return reflections # noop
#import xfel.merging.application.input.file_loader
#xfel.merging.application.input.file_loader.simple_file_loader = noprune_file_loader
simple_file_loader.prune_reflection_table_keys = prune_reflection_table_keys

integrate_phil_str = '''
  include scope dials.algorithms.integration.integrator.phil_scope
  include scope dials.algorithms.profile_model.factory.phil_scope
  include scope dials.algorithms.spot_prediction.reflection_predictor.phil_scope
  include scope dials.algorithms.integration.stills_significance_filter.phil_scope

  dispatch {
    coset = False
      .expert_level = 2
      .type = bool
      .help = Within the integrate dispatcher, integrate a sublattice coset intended to represent \
              negative control spots with no Bragg diffraction.
  }

  integration {
    include scope dials.algorithms.integration.kapton_correction.absorption_phil_scope
    coset {
      transformation = 6
        .type = int(value_min=0, value_max=6)
        .multiple = False
        .help = The index number(s) of the modulus=2 sublattice transformation(s) used to produce distince coset results. \
                0=Double a, 1=Double b, 2=Double c, 3=C-face centering, 4=B-face centering, 5=A-face centering, 6=Body centering \
                See Sauter and Zwart, Acta D (2009) 65:553
    }
    debug {
      delete_shoeboxes = True
        .type = bool
        .help = "Delete shoeboxes immediately before saving files. This option"
                "in combination with debug.output=True enables intermediate"
                "processing steps to make use of shoeboxes."
    }
    integration_only_overrides {
      trusted_range = None
        .type = floats(size=2)
        .help = "Override the panel trusted range (underload and saturation) during integration."
        .short_caption = "Panel trusted range"
    }
  }

  output {
    composite_output = None
      .type = bool
      .expert_level = 4
      .help = Unused
    integrated_filename = None
      .type = str
      .expert_level = 4
      .help = Unused
    integrated_experiments_filename = None
      .type = str
      .expert_level = 4
      .help = Unused
  }
  profile {
    gaussian_rs {
      parameters {
        sigma_b_cutoff = 0.1
          .type = float
          .help = Maximum sigma_b before the image is rejected
      }
    }
  }
'''

program_defaults_phil_str = """
input.keep_imagesets=True
"""

from dials.command_line.stills_process import program_defaults_phil_str as dials_program_defaults_phil_str
from xfel.merging.application.phil.phil import dispatch_phil, input_phil, output_phil
phil_scope = parse(integrate_phil_str + dispatch_phil + input_phil + output_phil, process_includes=True).fetch(parse(dials_program_defaults_phil_str)).fetch(parse(program_defaults_phil_str))
import xfel.merging.application.phil.phil
xfel.merging.application.phil.phil.phil_scope = phil_scope

if __name__ == '__main__':
  from xfel.merging.command_line.merge import Script
  script = Script()

  result = script.run()


 *******************************************************************************


 *******************************************************************************
xfel/merging/database/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/database/merging_database.py
from __future__ import absolute_import, division, print_function
from cctbx.array_family import flex
from six.moves import zip

mysql_master_phil = """
backend = FS *MySQL SQLite Flex
  .type = choice
  .help = "Back end database; FS for flat-file ASCII data storage,
           MySQL and SQLite for the respective proper database
           backends. Flex gives in-memory flex arrays instead of disk-based storage,
           which are ultimately written as pickle files at the final join()"
mysql {
  # MySQL database v5.1 data store.
  # mysql -u root -p # Steps to be taken by the database administrator
  # CREATE DATABASE database;
  # GRANT ALL ON database.* to 'user' IDENTIFIED BY 'passwd';
  # SET GLOBAL max_allowed_packet=512*1024*1024;
  # installation of MySQLdb, download from http://sourceforge.net/projects/mysql-python
  # install into the cctbx python with libtbx.python setup.py install
  # Maintenance and cleanup by user with mysql -u user -p
  # SHOW TABLES FROM database;
  # DROP TABLE *;
  host = localhost
    .type = str
    .help = persistent data tables to MySQL database using mysql-server on this host
    .help = concurrent client connections OK, can use nproc > 1
  port = 3306
    .type = int
    .help = port number for connecting on this host
  runtag = None
    .type = str
    .help = "Identifier is for this run, signifies database to use.
             Dumps old data when applicable and writes new tables."
  user = None
    .type = str
    .help = mysql username provided by the database administrator
  passwd = None
    .type = str
    .help = mysql password provided by the database administrator
  database = None
    .type = str
    .help = mysql user's working database name, provided by the database administrator
}
"""
class manager_base (object):
  def insert_frame_legacy(self,result,wavelength,corr,slope,offset,data):
    from scitbx import matrix
    """Legacy compatibility with cxi.merge; insert frame-data to backend.
    XXX needs to be backported to SQLite backend (use this base class)
    result: an unpickled dictionary from an integration pickle
    wavelength, beam_x, beam_y, distance: parameters from the model
    data: an instance of a "frame_data" container class
    postx: an instance of the legacy_cxi_merge_postrefinement results, or None
    """
    have_sa_params = ( type(result.get("sa_parameters")[0]) == type(dict()) )

    kwargs = {'wavelength': wavelength,
              'beam_x': result['xbeam'],
              'beam_y': result['ybeam'],
              'distance': result['distance'],
              'c_c': corr,
              'slope': slope,
              'offset': offset,
              'unique_file_name': data.file_name}
    if have_sa_params:
      sa_parameters = result['sa_parameters'][0]
      res_ori_direct = sa_parameters['reserve_orientation'].direct_matrix().elems

      kwargs['res_ori_1'] = res_ori_direct[0]
      kwargs['res_ori_2'] = res_ori_direct[1]
      kwargs['res_ori_3'] = res_ori_direct[2]
      kwargs['res_ori_4'] = res_ori_direct[3]
      kwargs['res_ori_5'] = res_ori_direct[4]
      kwargs['res_ori_6'] = res_ori_direct[5]
      kwargs['res_ori_7'] = res_ori_direct[6]
      kwargs['res_ori_8'] = res_ori_direct[7]
      kwargs['res_ori_9'] = res_ori_direct[8]

      kwargs['rotation100_rad'] = sa_parameters.rotation100_rad
      kwargs['rotation010_rad'] = sa_parameters.rotation010_rad
      kwargs['rotation001_rad'] = sa_parameters.rotation001_rad

      kwargs['half_mosaicity_deg'] = sa_parameters.half_mosaicity_deg
      kwargs['wave_HE_ang'] = sa_parameters.wave_HE_ang
      kwargs['wave_LE_ang'] = sa_parameters.wave_LE_ang
      kwargs['domain_size_ang'] = sa_parameters.domain_size_ang

    else:
      res_ori_direct = matrix.sqr(
        data.indexed_cell.orthogonalization_matrix()).transpose().elems

      kwargs['res_ori_1'] = res_ori_direct[0]
      kwargs['res_ori_2'] = res_ori_direct[1]
      kwargs['res_ori_3'] = res_ori_direct[2]
      kwargs['res_ori_4'] = res_ori_direct[3]
      kwargs['res_ori_5'] = res_ori_direct[4]
      kwargs['res_ori_6'] = res_ori_direct[5]
      kwargs['res_ori_7'] = res_ori_direct[6]
      kwargs['res_ori_8'] = res_ori_direct[7]
      kwargs['res_ori_9'] = res_ori_direct[8]
      if self.params.scaling.report_ML:
        kwargs['half_mosaicity_deg'] = result["ML_half_mosaicity_deg"][0]
        kwargs['domain_size_ang'] = result["ML_domain_size_ang"][0]
      else:
        kwargs['half_mosaicity_deg'] =float("NaN")
        kwargs['domain_size_ang'] =float("NaN")
    return self.insert_frame(**kwargs)

class manager (manager_base):
  def __init__(self,params):
    self.params = params

  def connection(self):
    try:
      import MySQLdb
      db = MySQLdb.connect(passwd=self.params.mysql.passwd,
                           user = self.params.mysql.user,
                           host = self.params.mysql.host,
                           port = self.params.mysql.port,
                           db = self.params.mysql.database,compress=False)
      cursor = db.cursor()
      cursor.execute("use %s;"%self.params.mysql.database)
      db.commit()

      return db
    except Exception:
      raise RuntimeError("Couldn't connect to mysql database")


  def initialize_db(self, indices):
    db = self.connection()
    print("testing for tables")
    cursor = db.cursor()
    cursor.execute("SHOW TABLES from %s;"%self.params.mysql.database)
    all_tables = cursor.fetchall()

    # Beware of SQL injection vulnerability (here and elsewhere).
    new_tables = self.merging_schema_tables(self.params.mysql.runtag)
    for table in new_tables:
      cursor.execute("DROP TABLE IF EXISTS %s;"%table[0])
      cursor.execute("CREATE TABLE %s "%table[0]+table[1].replace("\n"," ")+" ;")
    from six.moves import cStringIO as StringIO
    query = StringIO()
    query.write("INSERT INTO `%s_miller` (h,k,l) VALUES "%self.params.mysql.runtag)
    firstcomma = ""
    for item in indices:
      query.write(firstcomma); firstcomma=","
      query.write("('%d','%d','%d')"%(item[0],item[1],item[2]))
    query.write(" ;")
    cursor.execute( query.getvalue() )
    db.commit()

  def _insert(self, table, **kwargs):
    """The _insert() function generates the SQL command and parameter
    argument for the _execute() function.
    """

    # Note MySQL uses "%s", whereas SQLite uses "?".
    sql = ("INSERT INTO %s (" % table) \
          + ", ".join(kwargs.keys()) + ") VALUES (" \
          + ", ".join(["%s"] * len(kwargs.keys())) + ")"

    # If there are more than one rows to insert, "unpack" the keyword
    # argument iterables and zip them up.  This effectively rearranges
    # a list of columns into a list of rows.
    try:
      parameters = list(zip(*kwargs.values()))
    except TypeError:
      parameters = [kwargs.values()]

    return (sql, parameters)


  def insert_frame(self, **kwargs):
    db = self.connection()
    cursor = db.cursor()

    (sql, parameters) = self._insert(
      table='`%s_frame`' % self.params.mysql.runtag,
      **kwargs)

    cursor.execute(sql, parameters[0])
    db.commit()

    # Entry in the observation table is zero-based.
    return cursor.lastrowid - 1


  def insert_observation(self, **kwargs):
    return
    db = self.connection()
    cursor = db.cursor()

    # For MySQLdb executemany() is six times slower than a single big
    # execute() unless the "values" keyword is given in lowercase
    # (http://sourceforge.net/p/mysql-python/bugs/305).
    #
    # See also merging_database_sqlite3._insert()
    query = ("INSERT INTO `%s_observation` (" % self.params.mysql.runtag) \
            + ", ".join(kwargs.keys()) + ") values (" \
            + ", ".join(["%s"] * len(kwargs.keys())) + ")"
    try:
      parameters = list(zip(*kwargs.values()))
    except TypeError:
      parameters = [kwargs.values()]
    cursor.executemany(query, parameters)
    db.commit()


  def join(self):
    pass


  def read_indices(self):
    db = self.connection()
    cursor = db.cursor()
    from cctbx.array_family import flex
    millers = dict(merged_asu_hkl=flex.miller_index())
    cursor.execute("SELECT h,k,l FROM `%s_miller` ORDER BY hkl_id_1_base"%self.params.mysql.runtag)
    for item in cursor.fetchall():
      millers["merged_asu_hkl"].append((item[0],item[1],item[2]))
    return millers

  def read_observations(self):
    db = self.connection()
    cursor = db.cursor()
    cursor.execute("SELECT hkl_id_0_base,i,sigi,frame_id_0_base,original_h,original_k,original_l FROM `%s_observation`"%self.params.mysql.runtag)
    ALL = cursor.fetchall()

    return dict(hkl_id = flex.int([a[0] for a in ALL]), #as MySQL indices are 1-based
               i = flex.double([a[1] for a in ALL]),
               sigi = flex.double([a[2] for a in ALL]),
               frame_id = flex.int([a[3] for a in ALL]),
               original_h = flex.int([a[4] for a in ALL]),
               original_k = flex.int([a[5] for a in ALL]),
               original_l = flex.int([a[6] for a in ALL]),
               )

  def read_frames(self):
    from xfel.cxi.util import is_odd_numbered
    db = self.connection()
    cursor = db.cursor()
    cursor.execute("""SELECT
    frame_id_1_base,wavelength,c_c,slope,offset,res_ori_1,res_ori_2,res_ori_3,
    res_ori_4,res_ori_5,res_ori_6,res_ori_7,res_ori_8,res_ori_9,
    unique_file_name
    FROM `%s_frame`"""%self.params.mysql.runtag)
    ALL = cursor.fetchall()
    from cctbx.crystal_orientation import crystal_orientation
    orientations = [crystal_orientation(
     (a[5],a[6],a[7],a[8],a[9],a[10],a[11],a[12],a[13]),False) for a in ALL]
    return dict( frame_id = flex.int( [a[0]-1 for a in ALL] ),
               wavelength = flex.double( [a[1] for a in ALL] ),
                       cc = flex.double( [a[2] for a in ALL] ),
                    slope = flex.double( [a[3] for a in ALL] ),
                   offset = flex.double( [a[4] for a in ALL] ),
             odd_numbered = flex.bool( [is_odd_numbered(a[14], use_hash = self.params.hash_filenames) for a in ALL] ),
              orientation = orientations,
                unit_cell = [CO.unit_cell() for CO in orientations],
         unique_file_name = [a[14] for a in ALL] )

  def merging_schema_tables(self,runtag):
    return [("`"+runtag+"_observation`","""
            (
              hkl_id_0_base INT,
              i DOUBLE NOT NULL,
              sigi DOUBLE NOT NULL,
              detector_x DOUBLE NOT NULL,
              detector_y DOUBLE NOT NULL,
              frame_id_0_base INT,
              overload_flag INTEGER,
              original_h INT NOT NULL,
              original_k INT NOT NULL,
              original_l INT NOT NULL
            )
            """),
            ("`"+runtag+"_frame`","""
            (
              frame_id_1_base INT UNSIGNED AUTO_INCREMENT NOT NULL PRIMARY KEY,
              wavelength DOUBLE NOT NULL,
              beam_x DOUBLE NOT NULL,
              beam_y DOUBLE NOT NULL,
              distance DOUBLE NOT NULL,
              c_c DOUBLE NOT NULL,
              slope DOUBLE NOT NULL,
              offset DOUBLE NOT NULL,
              res_ori_1 DOUBLE NOT NULL,
              res_ori_2 DOUBLE NOT NULL,
              res_ori_3 DOUBLE NOT NULL,
              res_ori_4 DOUBLE NOT NULL,
              res_ori_5 DOUBLE NOT NULL,
              res_ori_6 DOUBLE NOT NULL,
              res_ori_7 DOUBLE NOT NULL,
              res_ori_8 DOUBLE NOT NULL,
              res_ori_9 DOUBLE NOT NULL,
              rotation100_rad DOUBLE,
              rotation010_rad DOUBLE,
              rotation001_rad DOUBLE,
              half_mosaicity_deg DOUBLE,
              wave_HE_ang DOUBLE,
              wave_LE_ang DOUBLE,
              domain_size_ang DOUBLE,
              unique_file_name MEDIUMTEXT
              ) AUTO_INCREMENT = 1
            """
            ),
            ("`"+runtag+"_miller`","""(
              hkl_id_1_base INT AUTO_INCREMENT PRIMARY KEY,
              h INT NOT NULL,
              k INT NOT NULL,
              l INT NOT NULL
              ) AUTO_INCREMENT = 1
            """
            ),
              ]
  def positional_refinement_schema_tables(self,runtag):
    return [("`"+runtag+"_spotfinder`","""
            (
              frame_id INT, itile INT,
              beam1x DOUBLE NOT NULL,
              beam1y DOUBLE NOT NULL,
              beamrx DOUBLE NOT NULL,
              beamry DOUBLE NOT NULL,
              spotfx DOUBLE NOT NULL,
              spotfy DOUBLE NOT NULL,
              spotcx DOUBLE NOT NULL,
              spotcy DOUBLE NOT NULL,
              h INT NOT NULL,
              k INT NOT NULL,
              l INT NOT NULL,
              radialpx DOUBLE NOT NULL DEFAULT 0.0,
              azimutpx DOUBLE NOT NULL DEFAULT 0.0
            )
            """),
              ]


 *******************************************************************************


 *******************************************************************************
xfel/merging/database/merging_database_flex.py
from __future__ import absolute_import, division, print_function
from cctbx.array_family import flex

def _execute(db_commands_queue, db_results_queue, output_prefix, semaphore, X):
  """The _execute() function defines a consumer process that executes
  commands on the SQL database in serial.
  """
  # Acquire the semaphore when the consumer process is starting, and
  # release it on return.
  semaphore.acquire()

  rows_frame = 0 # a.k.a. frame_id

  # Process commands from the commands queue and mark them as done.
  while True:
    command = db_commands_queue.get()
    if command is None:
      break
    table = command[0]
    data = command[1]
    lastrowid_key = command[2]

    if table == 'frame':
      items = [0]*len(order_dict)
      for key,val in data.items():
        items[order_dict[key]]=val
      characters = ' '.join([str(i) for i in items])+'\n'
      X["xtal_proxy"].get_obj()[rows_frame*600:rows_frame*600+len(characters)]=characters
      rows_frame += 1
      lastrowid_value = rows_frame

    elif table == 'observation':
      rows_observation = X["rows"].get_obj()[0]
      new_rows_observation = rows_observation+len(data[list(data.keys())[0]]) # XXX FIXME
      X["intensity_proxy"].get_obj()[rows_observation:new_rows_observation]=data["i"]
      X["sigma_proxy"].get_obj()[rows_observation:new_rows_observation]=data["sigi"]
      X["miller_proxy"].get_obj()[rows_observation:new_rows_observation]=data["hkl_id_0_base"]
      X["frame_proxy"].get_obj()[rows_observation:new_rows_observation]=data["frame_id_0_base"]
      X["H_proxy"].get_obj()[rows_observation:new_rows_observation]=data["original_h"]
      X["K_proxy"].get_obj()[rows_observation:new_rows_observation]=data["original_k"]
      X["L_proxy"].get_obj()[rows_observation:new_rows_observation]=data["original_l"]
      X["rows"].get_obj()[0] = new_rows_observation
      lastrowid_value = new_rows_observation

    else:
      raise RuntimeError("Unknown table '%s'" % command[0])
    print("FRAME",rows_frame,"OBS",X['rows'].get_obj()[0])
    if lastrowid_key is not None:
      db_results_queue.put((lastrowid_key, lastrowid_value))
    db_commands_queue.task_done()

  # Mark the terminating None command as done.
  db_commands_queue.task_done()

  db_commands_queue.join()
  semaphore.release()

class manager:
  # The manager

  def __init__(self, params, data_proxy):
    import multiprocessing

    self.params = params
    mgr = multiprocessing.Manager()
    self._db_commands_queue = mgr.JoinableQueue()
    self._db_results_queue = mgr.JoinableQueue()
    self._semaphore = mgr.Semaphore()

    multiprocessing.Process(
        target=_execute,
        args=(self._db_commands_queue,
              self._db_results_queue,
              self.params.output.prefix,
              self._semaphore,
              data_proxy)).start()

  def initialize_db(self, indices):
    from os import remove

    for suffix in '_frame.pickle', '_miller.pickle', '_observation.pickle':
      try:
        remove(self.params.output.prefix + suffix)
      except OSError as e:
        pass # deliberate - file does not exist

    self.miller = indices

  def insert_frame(self, **kwargs):
    # Pick up the index of the row just added.  The string is
    # assumed to to serve as a unique key.
    lastrowid_key = kwargs['unique_file_name']
    self._db_commands_queue.put(('frame', kwargs, lastrowid_key))
    while True:
      item = self._db_results_queue.get()
      self._db_results_queue.task_done()
      if item[0] == kwargs['unique_file_name']:
        # Entry in the observation table is zero-based.
        return item[1] - 1
      else:
        # If the key does not match, put it back in the queue for
        # someone else to pick up.
        self._db_results_queue.put(item)

  def insert_observation(self, **kwargs):
    print("inserting obs:")
    self._db_commands_queue.put(('observation', kwargs, None))

  def join(self,data_dict):
    """The join() function closes the database.
    """
    # Terminate the consumer process by feeding it a None command and
    # wait for it to finish.
    self._db_commands_queue.put(None)
    self._db_commands_queue.join()
    self._db_results_queue.join()
    self._semaphore.acquire()
    nrows = data_dict["rows"].get_obj()[0]
    print("writing observation pickle with %d rows"%nrows)
    kwargs = dict(
      miller_lookup =      flex.size_t(data_dict["miller_proxy"].get_obj()[:nrows]),
      observed_intensity = flex.double(data_dict["intensity_proxy"].get_obj()[:nrows]),
      observed_sigI =      flex.double(data_dict["sigma_proxy"].get_obj()[:nrows]),
      frame_lookup =       flex.size_t(data_dict["frame_proxy"].get_obj()[:nrows]),
      original_H =         flex.int   (data_dict["H_proxy"].get_obj()[:nrows]),
      original_K =         flex.int   (data_dict["K_proxy"].get_obj()[:nrows]),
      original_L =         flex.int   (data_dict["L_proxy"].get_obj()[:nrows]),
    )
    from six.moves import cPickle as pickle
    pickle.dump(kwargs, open(self.params.output.prefix+"_observation.pickle","wb"),
                pickle.HIGHEST_PROTOCOL)
    pickle.dump(self.miller, open(self.params.output.prefix+"_miller.pickle","wb"),
                pickle.HIGHEST_PROTOCOL)
    pickle.dump(data_dict["xtal_proxy"].get_obj().raw.replace('\0','').strip(),
                        open(self.params.output.prefix+"_frame.pickle","wb"),pickle.HIGHEST_PROTOCOL)
    return kwargs

order_dict = {'wavelength': 0,
                  'beam_x': 1,
                  'beam_y': 2,
                  'distance': 3,
                  'res_ori_1': 4,
                  'res_ori_2': 5,
                  'res_ori_3': 6,
                  'res_ori_4': 7,
                  'res_ori_5': 8,
                  'res_ori_6': 9,
                  'res_ori_7': 10,
                  'res_ori_8': 11,
                  'res_ori_9': 12,
                  'half_mosaicity_deg': 13,
                  'domain_size_ang':14,
                  'unique_file_name': 15}
class read_experiments(object):
  def __init__(self,params):
    from six.moves import cPickle as pickle
    from dxtbx.model import BeamFactory
    from dxtbx.model import DetectorFactory
    from dxtbx.model.crystal import CrystalFactory
    from cctbx.crystal_orientation import crystal_orientation,basis_type
    from dxtbx.model import Experiment, ExperimentList
    from scitbx import matrix
    self.experiments = ExperimentList()
    self.unique_file_names = []

    self.params = params
    data = pickle.load(open(self.params.output.prefix+"_frame.pickle","rb"))
    frames_text = data.split("\n")

    for item in frames_text:
      tokens = item.split(' ')
      wavelength = float(tokens[order_dict["wavelength"]])

      beam = BeamFactory.simple(wavelength = wavelength)

      detector = DetectorFactory.simple(
        sensor = DetectorFactory.sensor("PAD"), # XXX shouldn't hard code for XFEL
        distance = float(tokens[order_dict["distance"]]),
        beam_centre = [float(tokens[order_dict["beam_x"]]), float(tokens[order_dict["beam_y"]])],
        fast_direction = "+x",
        slow_direction = "+y",
        pixel_size = [self.params.pixel_size,self.params.pixel_size],
        image_size = [1795,1795],  # XXX obviously need to figure this out
        )

      reciprocal_matrix = matrix.sqr([float(tokens[order_dict[k]]) for k in [
'res_ori_1','res_ori_2','res_ori_3','res_ori_4','res_ori_5','res_ori_6','res_ori_7','res_ori_8','res_ori_9']])
      ORI = crystal_orientation(reciprocal_matrix, basis_type.reciprocal)
      direct = matrix.sqr(ORI.direct_matrix())
      transfer_dict = dict(__id__="crystal",
                           ML_half_mosaicity_deg=float(tokens[order_dict["half_mosaicity_deg"]]),
                           ML_domain_size_ang=float(tokens[order_dict["domain_size_ang"]]),
                           real_space_a = matrix.row(direct[0:3]),
                           real_space_b = matrix.row(direct[3:6]),
                           real_space_c = matrix.row(direct[6:9]),
                           space_group_hall_symbol = self.params.target_space_group.type().hall_symbol(),
                           )
      crystal = CrystalFactory.from_dict(transfer_dict)
      """ old code reflects python-based crystal model
      crystal = Crystal(
        real_space_a = matrix.row(direct[0:3]),
        real_space_b = matrix.row(direct[3:6]),
        real_space_c = matrix.row(direct[6:9]),
        space_group_symbol = self.params.target_space_group.type().lookup_symbol(),
        mosaicity = float(tokens[order_dict["half_mosaicity_deg"]]),
      )
      crystal.domain_size = float(tokens[order_dict["domain_size_ang"]])
      """
      #if isoform is not None:
      #  newB = matrix.sqr(isoform.fractionalization_matrix()).transpose()
      #  crystal.set_B(newB)

      self.experiments.append(Experiment(beam=beam,
                                  detector=None, #dummy for now
                                  crystal=crystal))
      self.unique_file_names.append(tokens[order_dict["unique_file_name"]])

    self.show_summary()

  def get_experiments(self):
    return self.experiments
  def get_files(self):
    return self.unique_file_names

  def show_summary(self):
    w = flex.double([e.beam.get_wavelength() for e in self.experiments])
    stats=flex.mean_and_variance(w)
    print("Wavelength mean and standard deviation:",stats.mean(),stats.unweighted_sample_standard_deviation())
    uc = [e.crystal.get_unit_cell().parameters() for e in self.experiments]
    a = flex.double([u[0] for u in uc])
    stats=flex.mean_and_variance(a)
    print("Unit cell a mean and standard deviation:",stats.mean(),stats.unweighted_sample_standard_deviation())
    b = flex.double([u[1] for u in uc])
    stats=flex.mean_and_variance(b)
    print("Unit cell b mean and standard deviation:",stats.mean(),stats.unweighted_sample_standard_deviation())
    c = flex.double([u[2] for u in uc])
    stats=flex.mean_and_variance(c)
    print("Unit cell c mean and standard deviation:",stats.mean(),stats.unweighted_sample_standard_deviation())
    d = flex.double([e.crystal.get_domain_size_ang() for e in self.experiments])
    stats=flex.mean_and_variance(d)
    # NOTE XXX FIXME:  cxi.index seems to record the half-domain size; report here the full domain size
    print("Domain size mean and standard deviation:",2.*stats.mean(),2.*stats.unweighted_sample_standard_deviation())


 *******************************************************************************


 *******************************************************************************
xfel/merging/database/merging_database_fs.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# $Id$

from __future__ import absolute_import, division, print_function

from cctbx.array_family import flex
from six.moves import range
from six.moves import zip


# XXX Use proper field separators (e.g. ASCII 31), see
# http://en.wikipedia.org/wiki/Delimiter.  Alternatively, use the csv
# module?
MISSING_STRING = '#'

def _execute(db_commands_queue, db_results_queue, output_prefix, semaphore):
  """The _execute() function defines a consumer process that executes
  commands on the SQL database in serial.
  """

  # Acquire the semaphore when the consumer process is starting, and
  # release it on return.
  semaphore.acquire()

  rows_frame = 0 # a.k.a. frame_id
  rows_miller = 0 # a.k.a. hkl_id
  rows_observation = 0

  # Process commands from the commands queue and mark them as done.
  while True:
    command = db_commands_queue.get()
    if command is None:
      break

    table = command[0]
    order = command[1]
    parameters = command[2]
    lastrowid_key = command[3]

    # Break race condition w.r.t. initialisation!
    if rows_frame == 0:
      stream_frame = open(output_prefix + '_frame.db', 'a')
    if rows_miller == 0:
      stream_miller = open(output_prefix + '_miller.db', 'a')
    if rows_observation == 0:
      stream_observation = open(output_prefix + '_observation.db', 'a')

    if table == 'frame':
      for row in parameters:
        items = [repr(rows_frame)] + [MISSING_STRING] * 24
        for j in range(len(order)):
          items[order[j]] = repr(row[j])
        print(' '.join(items), file=stream_frame)
        rows_frame += 1
      lastrowid_value = rows_frame

    elif table == 'miller':
      for row in parameters:
        items = [repr(rows_miller)] + [MISSING_STRING] * 3
        for j in range(len(order)):
          items[order[j]] = repr(row[j])
        print(' '.join(items), file=stream_miller)
        rows_miller += 1
      lastrowid_value = rows_miller

    elif table == 'observation':
      for row in parameters:
        items = [MISSING_STRING] * 10
        for j in range(len(order)):
          items[order[j]] = repr(row[j])
        print(' '.join(items), file=stream_observation)
        rows_observation += 1
      lastrowid_value = rows_observation


    else:
      raise RuntimeError("Unknown table '%s'" % command[0])

    if lastrowid_key is not None:
      db_results_queue.put((lastrowid_key, lastrowid_value))
    db_commands_queue.task_done()

  # Mark the terminating None command as done.
  db_commands_queue.task_done()

  # Commit all the processed commands and join the commands queue.
  if rows_frame > 0:
    stream_frame.close()
  if rows_miller > 0:
    stream_miller.close()
  if rows_observation > 0:
    stream_observation.close()

  db_commands_queue.join()
  semaphore.release()

from xfel.merging.database.merging_database import manager_base
class manager (manager_base):
  # The manager

  def __init__(self, params):
    import multiprocessing

    self.params = params

    mgr = multiprocessing.Manager()
    self._db_commands_queue = mgr.JoinableQueue()
    self._db_results_queue = mgr.JoinableQueue()
    self._semaphore = mgr.Semaphore()

    multiprocessing.Process(
        target=_execute,
        args=(self._db_commands_queue,
              self._db_results_queue,
              self.params.output.prefix,
              self._semaphore)).start()

  def initialize_db(self, indices):
    from os import remove
    print(self.params.postrefinement.algorithm)
    for suffix in '_frame.db', '_miller.db', '_observation.db':
      try:
        remove(self.params.output.prefix + suffix)
      except OSError as e:
        pass # deliberate - file does not exist

    self._db_commands_queue.put(('miller', (1, 2, 3), indices, None))

  def insert_frame_updated(self,result,wavelength,data,postx):
    from scitbx import matrix # implicit import
    """New compatibility with postrefinement container
    XXX needs to be backported to the MySQL and SQLite backends (put into a base class)
    """
    kwargs = {'wavelength': wavelength,
              'beam_x': result['xbeam'],
              'beam_y': result['ybeam'],
              'distance': result['distance'],
              'c_c': postx.final_corr,
              'unique_file_name': data.file_name}
    values = postx.get_parameter_values()
    kwargs["G"]=values.G
    kwargs["BFACTOR"]=values.BFACTOR
    kwargs["RS"]=values.RS
    kwargs["thetax"]=values.thetax
    kwargs["thetay"]=values.thetay
    Astar=postx.refinery.ORI.reciprocal_matrix()
    kwargs['Astar_1'], kwargs['Astar_2'], kwargs['Astar_3'],\
    kwargs['Astar_4'], kwargs['Astar_5'], kwargs['Astar_6'],\
    kwargs['Astar_7'], kwargs['Astar_8'], kwargs['Astar_9'] = Astar
    return self.insert_frame(**kwargs)

  def insert_frame(self, **kwargs):
    order = []
    if self.params.postrefinement.enable==True and \
       self.params.postrefinement.algorithm in ["rs2","rs_hybrid"]:
      order_dict = {'wavelength': 1,
                  'beam_x': 2,
                  'beam_y': 3,
                  'distance': 4,
                  'G': 5,
                  'BFACTOR': 6,
                  'RS': 7,
                  'Astar_1': 8,
                  'Astar_2': 9,
                  'Astar_3': 10,
                  'Astar_4': 11,
                  'Astar_5': 12,
                  'Astar_6': 13,
                  'Astar_7': 14,
                  'Astar_8': 15,
                  'Astar_9': 16,
                  'thetax': 17,
                  'thetay': 18,
                  'unique_file_name': 19,
                  'c_c': 20}
    else:
      order_dict = {'wavelength': 1,
                  'beam_x': 2,
                  'beam_y': 3,
                  'distance': 4,
                  'c_c': 5,
                  'slope': 6,
                  'offset': 7,
                  'res_ori_1': 8,
                  'res_ori_2': 9,
                  'res_ori_3': 10,
                  'res_ori_4': 11,
                  'res_ori_5': 12,
                  'res_ori_6': 13,
                  'res_ori_7': 14,
                  'res_ori_8': 15,
                  'res_ori_9': 16,
                  'rotation100_rad': 17,
                  'rotation010_rad': 18,
                  'rotation001_rad': 19,
                  'half_mosaicity_deg': 20,
                  'wave_HE_ang': 21,
                  'wave_LE_ang': 22,
                  'domain_size_ang':23,
                  'unique_file_name': 24}
    for key in kwargs.keys():
      order.append(order_dict[key])
    parameters = [list(kwargs.values())]

    # Pick up the index of the row just added.  The file name is
    # assumed to to serve as a unique key.
    lastrowid_key = kwargs['unique_file_name']
    self._db_commands_queue.put(('frame', order, parameters, lastrowid_key))
    while True:
      item = self._db_results_queue.get()
      self._db_results_queue.task_done()
      if item[0] == kwargs['unique_file_name']:
        # Entry in the observation table is zero-based.
        return item[1] - 1
      else:
        # If the key does not match, put it back in the queue for
        # someone else to pick up.
        self._db_results_queue.put(item)

  def insert_observation(self, **kwargs):
    order = []
    order_dict = {'hkl_id_0_base': 0,
                  'i': 1,
                  'sigi': 2,
                  'detector_x': 3,
                  'detector_y': 4,
                  'frame_id_0_base': 5,
                  'overload_flag': 6,
                  'original_h': 7,
                  'original_k': 8,
                  'original_l': 9}
    for key in kwargs.keys():
      order.append(order_dict[key])
    parameters = list(zip(*list(kwargs.values()))) # FIXME
    self._db_commands_queue.put(('observation', order, parameters, None))


  def join(self):
    """The join() function closes the database.
    """

    # Terminate the consumer process by feeding it a None command and
    # wait for it to finish.
    self._db_commands_queue.put(None)
    self._db_commands_queue.join()
    self._db_results_queue.join()
    self._semaphore.acquire()


  def read_indices(self):
    millers = dict(merged_asu_hkl=flex.miller_index())
    stream = open(self.params.output.prefix + '_miller.db', 'r')
    for row in stream:
      millers['merged_asu_hkl'].append(
        tuple(int(t) for t in row.split()[1:4]))
    stream.close()
    return millers


  def read_observations(self):
    observations = {'hkl_id': flex.int(),
                    'i': flex.double(),
                    'sigi': flex.double(),
                    'frame_id': flex.int(),
                    'original_h': flex.int(),
                    'original_k': flex.int(),
                    'original_l': flex.int()}
    stream = open(self.params.output.prefix + '_observation.db', 'r')
    for row in stream:
      items = row.split()
      observations['hkl_id'].append(int(items[0]))
      observations['i'].append(float(items[1]))
      observations['sigi'].append(float(items[2]))
      observations['frame_id'].append(int(items[5]))
      observations['original_h'].append(int(items[7]))
      observations['original_k'].append(int(items[8]))
      observations['original_l'].append(int(items[9]))
    stream.close()
    return observations


  def read_frames(self):
    if self.params.postrefinement.enable==True and \
       self.params.postrefinement.algorithm in ["rs2","rs_hybrid"]:
      return self.read_frames_updated_detail()
    else:
      return self.read_frames_legacy_detail()

  def read_frames_updated_detail(self):
    from cctbx.crystal_orientation import crystal_orientation
    from xfel.cxi.util import is_odd_numbered

    frames = {'frame_id': flex.int(),
              'wavelength': flex.double(),
              'cc': flex.double(),
              'G': flex.double(),
              'BFACTOR': flex.double(),
              'RS': flex.double(),
              'odd_numbered': flex.bool(),
              'thetax': flex.double(),
              'thetay': flex.double(),
              'orientation': [],
              'unit_cell': [],
              'unique_file_name': []}
    stream = open(self.params.output.prefix + '_frame.db', 'r')
    for row in stream:
      items = row.split()
      CO = crystal_orientation([float(t) for t in items[8:17]], True)
      unique_file_name = eval(items[19])
      frames['frame_id'].append(int(items[0]))
      frames['wavelength'].append(float(items[1]))
      frames['cc'].append(float(items[20]))
      frames['G'].append(float(items[5]))
      frames['BFACTOR'].append(float(items[6]))
      frames['RS'].append(float(items[7]))
      frames['thetax'].append(float(items[17]))
      frames['thetay'].append(float(items[18]))
      frames['odd_numbered'].append(is_odd_numbered(unique_file_name, use_hash = self.params.hash_filenames))
      frames['orientation'].append(CO)
      frames['unit_cell'].append(CO.unit_cell())
      frames['unique_file_name'].append(unique_file_name)
    stream.close()
    return frames

  def read_frames_legacy_detail(self):
    from cctbx.crystal_orientation import crystal_orientation
    from xfel.cxi.util import is_odd_numbered

    # XXX issues with spaces in the file name, and leading and
    # trailing single quotes (stripped below).

    frames = {'frame_id': flex.int(),
              'wavelength': flex.double(),
              'cc': flex.double(),
              'slope': flex.double(),
              'offset': flex.double(),
              'odd_numbered': flex.bool(),
              'domain_size_ang': flex.double(),
              'half_mosaicity_deg': flex.double(),
              'orientation': [],
              'unit_cell': [],
              'unique_file_name': []}
    stream = open(self.params.output.prefix + '_frame.db', 'r')
    for row in stream:
      items = row.split()
      CO = crystal_orientation([float(t) for t in items[8:17]], False)
      unique_file_name = eval(items[24])
      frames['frame_id'].append(int(items[0]))
      frames['wavelength'].append(float(items[1]))
      frames['cc'].append(float(items[5]))
      frames['slope'].append(float(items[6]))
      frames['offset'].append(float(items[7]))
      frames['domain_size_ang'].append(float(items[23]))
      frames['half_mosaicity_deg'].append(float(items[20]))
      frames['odd_numbered'].append(is_odd_numbered(unique_file_name, use_hash = self.params.hash_filenames))
      frames['orientation'].append(CO)
      frames['unit_cell'].append(CO.unit_cell())
      frames['unique_file_name'].append(unique_file_name)
    stream.close()
    return frames

class manager2 (manager_base):
  # Worker threads collect their INSERT requests in a sequencer list but take no
  # other action until master (rank=0) gathers all the requests.

  def __init__(self, params):
    self.params = params
    self.sequencer = []

  def insert_frame(self, **kwargs):
    self.sequencer.append(dict(call="insert_frame",data=kwargs))
    return 99999 # dummy value for frame_id_zero_base

  def insert_observation(self, **kwargs):
    self.sequencer.append(dict(call="insert_observation",data=kwargs))

  def join(self):
    """The join() function closes the database.
    """
    pass


 *******************************************************************************


 *******************************************************************************
xfel/merging/database/merging_database_sqlite3.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# $Id$

from __future__ import absolute_import, division, print_function

from cctbx.array_family import flex
from six.moves import zip


def _execute(db_commands_queue, db_results_queue, db, semaphore):
  """The _execute() function defines a consumer process that executes
  commands on the SQL database in serial.
  """

  # Acquire the semaphore when the consumer process is starting, and
  # release it on return.
  semaphore.acquire()

  # Process commands from the commands queue and mark them as done.
  cursor = db.cursor()
  while True:
    command = db_commands_queue.get()
    if command is None:
      break

    parameters = command[1]
    if len(parameters) > 1:
      cursor.executemany(command[0], parameters)
    else:
      cursor.execute(command[0], parameters[0])
      lastrowid_key = command[2]
      if lastrowid_key is not None:
        db_results_queue.put((lastrowid_key, cursor.lastrowid))
    db_commands_queue.task_done()

  # Mark the terminating None command as done.
  db_commands_queue.task_done()

  # Commit all the processed commands.
  db.commit()
  semaphore.release()


class manager:
  # The manager

  def __init__(self, params):
    import multiprocessing
    import sqlite3

    self.params = params

    mgr = multiprocessing.Manager()
    self._db_commands_queue = mgr.JoinableQueue()
    self._db_results_queue = mgr.JoinableQueue()
    self._semaphore = mgr.Semaphore()

    self._db = sqlite3.connect('%s.sqlite' % self.params.output.prefix)
    multiprocessing.Process(
        target=_execute,
        args=(self._db_commands_queue,
              self._db_results_queue,
              self._db,
              self._semaphore)).start()


  def initialize_db(self, indices):
    cursor = self._db.cursor()
    for table in self.merging_schema_tables(''):
      cursor.execute("DROP TABLE IF EXISTS %s;"%table[0])
      cursor.execute("CREATE TABLE %s " %
                     table[0] + table[1].replace("\n", " ") + " ;")
    cursor.executemany("INSERT INTO _miller VALUES (NULL, ?, ?, ?)", indices)
    self._db.commit()


  def _insert(self, table, **kwargs):
    """The _insert() function generates the SQL command and parameter
    argument for the _execute() function.
    """

    sql = ("INSERT INTO %s (" % table) \
          + ", ".join(kwargs) + ") VALUES (" \
          + ", ".join(["?"] * len(kwargs)) + ")"

    # If there are more than one rows to insert, "unpack" the keyword
    # argument iterables and zip them up.  This effectively rearranges
    # a list of columns into a list of rows.
    try:
      parameters = list(zip(*list(kwargs.values()))) # FIXME
    except TypeError:
      parameters = [list(kwargs.values())]

    return (sql, parameters)


  def insert_frame(self, **kwargs):
    # Explicitly add the auto-increment column for SQLite.
    (sql, parameters) = self._insert(
        table='_frame',
        frame_id_1_base=None,
        **kwargs)

    # Pick up the index of the row just added.  The file name is
    # assumed to to serve as a unique key.
    lastrowid_key = kwargs['unique_file_name']
    self._db_commands_queue.put((sql, parameters, lastrowid_key))
    while True:
      item = self._db_results_queue.get()
      self._db_results_queue.task_done()
      if item[0] == kwargs['unique_file_name']:
        # Entry in the observation table is zero-based.
        return item[1] - 1
      else:
        # If the key does not match, put it back in the queue for
        # someone else to pick up.
        self._db_results_queue.put(item)


  def insert_observation(self, **kwargs):
    (sql, parameters) = self._insert(
        table='_observation',
        **kwargs)

    self._db_commands_queue.put((sql, parameters, None))


  def join(self):
    """The join() function closes the database.
    """

    # Terminate the consumer process by feeding it a None command and
    # wait for it to finish.
    self._db_commands_queue.put(None)
    self._db_commands_queue.join()
    self._db_results_queue.join()
    self._semaphore.acquire()

    self._db.close()


  def read_indices(self):
    from cctbx.array_family import flex

    cursor = self._db.cursor()
    millers = dict(merged_asu_hkl=flex.miller_index())
    cursor.execute("SELECT h,k,l FROM _miller ORDER BY hkl_id_1_base")
    for item in cursor.fetchall():
      millers["merged_asu_hkl"].append((item[0],item[1],item[2]))
    return millers


  def read_observations(self):
    cursor = self._db.cursor()
    cursor.execute("SELECT hkl_id_0_base,i,sigi,frame_id_0_base,original_h,original_k,original_l FROM _observation")
    ALL = cursor.fetchall()

    return dict(hkl_id = flex.int([a[0] for a in ALL]), #as MySQL indices are 1-based
               i = flex.double([a[1] for a in ALL]),
               sigi = flex.double([a[2] for a in ALL]),
               frame_id = flex.int([a[3] for a in ALL]),
               original_h = flex.int([a[4] for a in ALL]),
               original_k = flex.int([a[5] for a in ALL]),
               original_l = flex.int([a[6] for a in ALL]),
               )

  def read_frames(self):
    from xfel.cxi.util import is_odd_numbered

    cursor = self._db.cursor()
    cursor.execute("""SELECT
    frame_id_1_base,wavelength,c_c,slope,offset,res_ori_1,res_ori_2,res_ori_3,
    res_ori_4,res_ori_5,res_ori_6,res_ori_7,res_ori_8,res_ori_9,
    unique_file_name
    FROM _frame""")
    ALL = cursor.fetchall()
    from cctbx.crystal_orientation import crystal_orientation
    orientations = [crystal_orientation(
     (a[5],a[6],a[7],a[8],a[9],a[10],a[11],a[12],a[13]),False) for a in ALL]
    return dict( frame_id = flex.int( [a[0]-1 for a in ALL] ),
               wavelength = flex.double( [a[1] for a in ALL] ),
                       cc = flex.double( [a[2] for a in ALL] ),
                    slope = flex.double( [a[3] for a in ALL] ),
                   offset = flex.double( [a[4] for a in ALL] ),
             odd_numbered = flex.bool( [is_odd_numbered(a[14], use_hash = self.params.hash_filenames) for a in ALL] ),
              orientation = orientations,
                unit_cell = [CO.unit_cell() for CO in orientations],
         unique_file_name = [a[14] for a in ALL] )

  def merging_schema_tables(self,runtag):

    # http://www.sqlite.org/faq.html#q1
    return [(runtag+"_observation","""
            (
              hkl_id_0_base INTEGER,
              i DOUBLE NOT NULL,
              sigi DOUBLE NOT NULL,
              detector_x DOUBLE NOT NULL,
              detector_y DOUBLE NOT NULL,
              frame_id_0_base INTEGER,
              overload_flag INTEGER,
              original_h INTEGER NOT NULL,
              original_k INTEGER NOT NULL,
              original_l INTEGER NOT NULL
            )
            """),
            (runtag+"_frame","""
            (
              frame_id_1_base INTEGER PRIMARY KEY,
              wavelength DOUBLE NOT NULL,
              beam_x DOUBLE NOT NULL,
              beam_y DOUBLE NOT NULL,
              distance DOUBLE NOT NULL,
              c_c DOUBLE NOT NULL,
              slope DOUBLE NOT NULL,
              offset DOUBLE NOT NULL,
              res_ori_1 DOUBLE NOT NULL,
              res_ori_2 DOUBLE NOT NULL,
              res_ori_3 DOUBLE NOT NULL,
              res_ori_4 DOUBLE NOT NULL,
              res_ori_5 DOUBLE NOT NULL,
              res_ori_6 DOUBLE NOT NULL,
              res_ori_7 DOUBLE NOT NULL,
              res_ori_8 DOUBLE NOT NULL,
              res_ori_9 DOUBLE NOT NULL,
              rotation100_rad DOUBLE,
              rotation010_rad DOUBLE,
              rotation001_rad DOUBLE,
              half_mosaicity_deg DOUBLE,
              wave_HE_ang DOUBLE,
              wave_LE_ang DOUBLE,
              domain_size_ang DOUBLE,
              unique_file_name MEDIUMTEXT
              )
            """
            ),
            (runtag+"_miller","""(
              hkl_id_1_base INTEGER PRIMARY KEY,
              h INTEGER NOT NULL,
              k INTEGER NOT NULL,
              l INTEGER NOT NULL
              )
            """
            ),
              ]
  def positional_refinement_schema_tables(self,runtag):
    return [(runtag+"_spotfinder","""
            (
              frame_id INTEGER, itile INTEGER,
              beam1x DOUBLE NOT NULL,
              beam1y DOUBLE NOT NULL,
              beamrx DOUBLE NOT NULL,
              beamry DOUBLE NOT NULL,
              spotfx DOUBLE NOT NULL,
              spotfy DOUBLE NOT NULL,
              spotcx DOUBLE NOT NULL,
              spotcy DOUBLE NOT NULL,
              h INTEGER NOT NULL,
              k INTEGER NOT NULL,
              l INTEGER NOT NULL,
              radialpx DOUBLE NOT NULL DEFAULT 0.0,
              azimutpx DOUBLE NOT NULL DEFAULT 0.0
            )
            """),
              ]


 *******************************************************************************
