

 *******************************************************************************
cctbx/dispersion/example.py
"""
Example optimization using functions in kramers_kronig_optimze.py.
In this script, the following is completed:
1. f" is simulated based on a very simple model of the K-edge (a ramp function that saturates).
2. The Hilbert tranform is used to calculate f'.
3. f" and f' are contaminated with Gaussian noise.
4. f" and f' are subsampled.
5. The optimization attempts to determine f" and f' by both trying to match the subsampled values and
    minimize a penalty that restrains f" and f' to be the Hilbert transform and negative Hilbert transform
    of each other respectively.
6. The final results are plotted in matplotlib.
"""
from __future__ import division

import kramers_kronig.kramers_kronig_optimize as kramers_kronig_optimize

known_response_energy = None
known_response_f_p = None
known_response_f_dp = None

energy,\
f_p,\
f_dp,\
energy_ss,\
f_p_noisy_ss,\
f_dp_noisy_ss,\
f_p_pred_0,\
f_dp_pred_0,\
f_p_opt,\
f_dp_opt,\
loss_vec,\
actual_loss_vec  = kramers_kronig_optimize.run_example_opt(width=5,
                                                           padn=10,
                                                           trim=30,
                                                           spacing=10,
                                                           noise_loc=[0,0],
                                                           noise_scale=[1e-3,1e-3],
                                                           learning_rate=1e-1,
                                                           num_iter=1000,
                                                           uniform_energy=True,
                                                           known_response_energy=known_response_energy,
                                                           known_response_f_p=known_response_f_p,
                                                           known_response_f_dp=known_response_f_dp,
                                                           )

kramers_kronig_optimize.visualize(energy,
                                  f_p,
                                  f_dp,
                                  energy_ss,
                                  f_p_noisy_ss,
                                  f_dp_noisy_ss,
                                  f_p_pred_0,
                                  f_dp_pred_0,
                                  f_p_opt,
                                  f_dp_opt,
                                  loss_vec,
                                  actual_loss_vec,
                                  )


 *******************************************************************************


 *******************************************************************************
cctbx/dispersion/kramers_kronig/__init__.py


 *******************************************************************************


 *******************************************************************************
cctbx/dispersion/kramers_kronig/kramers_kronig.py
"""Functions to compute penalty for f' and f" violating Kramers Kronig relations, written in PyTorch"""

from __future__ import division

import numpy as np
import torch

from scipy.signal.windows import get_window
from . import kramers_kronig_helper

def get_hilbert_transform(x, axis=-1):
    """
    Perform the Hilbert transform on a real-valued input.
    This is a PyTorch implementation of the Hilbert transform in scipy.signal
    Reference: https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.hilbert.html
    Like the scipy version, this version takes as input a real-valued function.
    Assuming the function is analytic, the full complex function is computed.
    Unlike the scipy version, this version only returns the imaginary part of the analytic function.
    scipy returns a complex vector where the real part is the input and the imaginary part is the same as
    the output of this function.
    Saying that the real and imaginary part of a function are related by Kramers-Kronig is the same as saying
    that the imaginary part of a function is the Hilbert transform of the real part.
    """

    N = x.shape[axis]
    if N <= 0:
        raise ValueError("N must be positive.")

    Xf = torch.fft.fft(x, N, axis=axis)
    h = torch.zeros(N, dtype=Xf.dtype, requires_grad=False)
    if N % 2 == 0:
        h[0] = h[N // 2] = 1
        h[1:N // 2] = 2
    else:
        h[0] = 1
        h[1:(N + 1) // 2] = 2

    if x.ndim > 1:
        ind = [np.newaxis] * x.ndim
        ind[axis] = slice(None)
        h = h[tuple(ind)]
    x = torch.fft.ifft(Xf * h, axis=axis)
    return(x.imag)

def get_f_p(energy,
            f_dp,
            padn=5000,
            trim=0,
            window_type='cosine',
            known_response_energy=None,
            known_response_f_p=None,
            known_response_f_dp=None,
            ):
    """
    This function calculates f' (f_p) from f" (f_dp).
    The input is f" as a function of energy. If the function is not on a uniform grid, f"
    is interpolated, using the smallest spacing of the given nonuniform grid.
    The Hilbert transform is linear. Thus, if a pair of f' and f" has already been calculated from the
    Hilbert transform, the known f" can be subtracted off of the input f". After f' is calculated from the input f"
    through the Hilbert transform, the known f' can be added back to the calculated f' for the final value.
    The Hilbert transform requires knowledge of the function at all energies. As we generally only have a truncated
    region, this subtraction and addition procedure allows the user to make the tails of the input function go to zero
    at the edges, getting a better approximation of the Hilbert transform.
    The subtracted input function is padded by zeros (of number padn) and windowed by a window of window_type.
    A number of start and endpoints of the input function (of number trim) are included in this window.
    """

    denergy = energy[1:]-energy[:-1]
    if torch.any(torch.abs(denergy-denergy[0])>1e-3): # Energy spacing is not constant.
        energy, f_dp = kramers_kronig_helper.interpolate(energy, f_dp, mode="torch")

    if known_response_energy is not None:
        known_response_f_p_interp = kramers_kronig_helper.INTERP_FUNC(known_response_energy, known_response_f_p)(energy)
        known_response_f_dp_interp = kramers_kronig_helper.INTERP_FUNC(known_response_energy, known_response_f_dp)(energy)
    else:
        known_response_f_p_interp = torch.zeros_like(f_dp)
        known_response_f_dp_interp = torch.zeros_like(f_dp)

    f_in = f_dp - torch.Tensor(known_response_f_dp_interp)
    f_in = apply_window(f_in, padn, trim=trim, window_type=window_type)

    f_p_pred_padded = get_hilbert_transform(f_in)
    f_p_pred_padded[padn:len(f_p_pred_padded)-padn] += torch.Tensor(known_response_f_p_interp)

    if padn != 0:
        f_p_pred = f_p_pred_padded[padn:-padn]
        dE = energy[1] - energy[0]
        start_energy = energy[0]-padn*dE
        if len(energy)%2: # odd
            end_energy = energy[-1]+(padn+1)*dE
        else:
            end_energy = energy[-1]+(padn+0)*dE
        energy_padded = torch.arange(start_energy,end_energy,dE)
    else:
        f_p_pred = f_p_pred_padded
        energy_padded = energy

    return(energy_padded,f_p_pred,f_p_pred_padded,f_in)

def get_f_dp(energy,
             f_p,
             padn=5000,
             trim=0,
             window_type='cosine',
             known_response_energy=None,
             known_response_f_p=None,
             known_response_f_dp=None,
             ):

    """Derive f" from f'. This is calculated from the negative of the Hilbert transform."""

    if known_response_f_p is not None:
        known_response_f_p = -known_response_f_p


    energy_padded,f_dp_pred,f_dp_pred_padded,f_in = get_f_p(energy, -f_p, padn=padn,
                                                            trim=trim,
                                                            window_type=window_type,
                                                            known_response_energy=known_response_energy,
                                                            known_response_f_p=known_response_f_dp,
                                                            known_response_f_dp=known_response_f_p,
                                                            )
    return(energy_padded,f_dp_pred,f_dp_pred_padded,f_in)

def apply_window(f_in, padn,
                 trim=0,
                 window_type='cosine'):
    """
    Apply windowing to f_in
    padn # of points are added to the beginning and end of f_in
    trim # of points are modified at the beginning and end of f_in

    Possible window_type:
    boxcar, triang, blackman, hamming, hann, bartlett, flattop, parzen, bohman,
    blackman, harris, nuttall, barthann, cosine, exponential, tukey, taylor,
    lanczos
    """

    window = get_window(window_type,(padn+trim)*2)
    window = window[0:padn+trim]
    window = torch.tensor(np.expand_dims(window, 0))

    window_start = f_in[0]*torch.ones(padn+trim)
    window_start[padn:]=f_in[0:trim]

    window_end = f_in[-1]*torch.ones(padn+trim)
    window_end[0:trim]=f_in[len(f_in)-trim:]

    windowed_func = torch.hstack((window_start * torch.squeeze(window),
                                  f_in[trim:len(f_in)-trim],
                                  window_end * torch.squeeze(torch.fliplr(window))))

    return(windowed_func)

def get_penalty(energy, f_p, f_dp, trim=0, padn=5000,window_type='cosine',
                known_response_energy=None,
                known_response_f_p=None,
                known_response_f_dp=None,
                ):
    """How close are f' and f" are to obeying the Kramers Kronig relation?"""

    #Going from f_dp to f_p

    energy_padded,f_p_pred,f_p_pred_padded,f_in = get_f_p(energy, f_dp, trim=trim, padn=padn,
                                                          window_type=window_type,
                                                          known_response_energy=known_response_energy,
                                                          known_response_f_p=known_response_f_p,
                                                          known_response_f_dp=known_response_f_dp,
                                                          )

    # The Hilbert transform filters out any DC term. We add back the DC term.
    F_p_pred = torch.fft.fft(f_p_pred_padded)
    F_p_pred[0] = torch.fft.fft(f_p)[0]
    f_p_pred_padded = torch.fft.ifft(F_p_pred).real

    f_p_pred_padded = f_p_pred_padded[padn:len(f_p_pred_padded)-padn]
    f_p_pred = f_p_pred_padded[trim:len(f_p_pred_padded)-trim]

    # Going from f_p to f_dp
    energy_padded,f_dp_pred,f_dp_pred_padded, f_in = get_f_dp(energy, f_p, trim=trim, padn=padn,
                                                              window_type=window_type,
                                                              known_response_energy=known_response_energy,
                                                              known_response_f_p=known_response_f_p,
                                                              known_response_f_dp=known_response_f_dp,
                                                              )

    # Add back DC term
    F_dp_pred = torch.fft.fft(f_dp_pred_padded)
    F_dp_pred[0] = torch.fft.fft(f_dp)[0]
    f_dp_pred_padded = torch.fft.ifft(F_dp_pred).real

    f_dp_pred_padded = f_dp_pred_padded[padn:len(f_dp_pred_padded)-padn]
    f_dp_pred = f_dp_pred_padded[trim:len(f_dp_pred_padded)-trim]

    # trim f_p and f_dp as the start and endpoints (of number trim) are
    # windowed during computation of the Hilbert transform
    f_p = f_p[trim:len(energy)-trim]
    f_dp = f_dp[trim:len(energy)-trim]
    mse = torch.mean((f_p - f_p_pred)**2) + torch.mean((f_dp - f_dp_pred)**2)
    return(mse)


 *******************************************************************************


 *******************************************************************************
cctbx/dispersion/kramers_kronig/kramers_kronig_helper.py
"""Penalty for f' and f" violating Kramers Kronig relations"""

from __future__ import division

import os
import pathlib
import numpy as np
import torch

import libtbx.load_env

from scipy.interpolate import interp1d

INTERP_FUNC = lambda x,y: interp1d(x,y,fill_value="extrapolate")

SAMPLE_DATA_PATH = USER = os.path.join(libtbx.env.find_in_repositories('ls49_big_data'), 'data_sherrell')

def parse_data(path, remove_first_line=False):
    """Load and parse input."""
    data_input=pathlib.Path(path).read_text().rstrip()
    lines = data_input.split('\n')
    if remove_first_line:
        start_ind=1
    else:
        start_ind=0
    sf = np.array([[float(p) for p in line.split()] for line in lines[start_ind:]])

    return(sf)

def interpolate(x_original, y_original, mode="scipy"):
    """
    Interpolate y_original onto a uniform grid, matching the smallest spacing of x_original
    mode can be "scipy" or "torch", torch allows for automatic differentiation to propagate by using PyTorch
    """

    if mode=="scipy":
        numerical_package = np
    elif mode=="torch":
        numerical_package = torch

    dx_original = x_original[1:]-x_original[:-1]
    dx = numerical_package.min(dx_original)

    x = numerical_package.arange(x_original[0],x_original[-1],dx)

    if mode == "scipy":
        interp = INTERP_FUNC(x_original, y_original)
        y = interp(x) # interpolated f_dp
    elif mode == "torch":
        y = interpolate_torch(x_original, y_original, x)
    return(x,y)

def interpolate_torch(x_original, y_original, x_new):
    """Linearly interpolate y_original(x_original), returning y = y_original(x_new), using PyTorch"""

    if torch.abs(x_original[-1]-x_new[-1])<1e-5:
      x_new = x_new[:-1]
      same_end = True
    else:
      same_end = False

    x_new_upper_position = torch.searchsorted(x_original, x_new)
    x_new_lower_position = x_new_upper_position - 1

    x_new_dist_lower_position = x_new - x_original[x_new_lower_position]
    x_new_dist_upper_position = x_original[x_new_upper_position] - x_new

    x_new_dist_lower_position_norm = x_new_dist_upper_position / (x_new_dist_lower_position + x_new_dist_upper_position)
    x_new_dist_upper_position_norm = x_new_dist_lower_position / (x_new_dist_lower_position + x_new_dist_upper_position)

    y_new = y_original[x_new_upper_position]*x_new_dist_upper_position_norm + y_original[x_new_lower_position]*x_new_dist_lower_position_norm

    if same_end:
      y_new = torch.concat((y_new,y_original[-1].expand(1)))

    return(y_new)


 *******************************************************************************


 *******************************************************************************
cctbx/dispersion/kramers_kronig/kramers_kronig_optimize.py
"""Functions to create test optimizations with a kramers-kronig penalty."""

from __future__ import division

import numpy as np
import torch
import matplotlib.pyplot as plt

from . import kramers_kronig
from . import kramers_kronig_helper

def create_f(width=10,
             dE=.1,
             trim=0,
             slope = 1,
             padn=5000,
             uniform_energy=True
             ):
    """Create a simulated absorption edge"""

    if uniform_energy:
        energy = np.arange(-width,width,dE)
    else:
        energy_0 = np.arange(-width,0,dE)
        energy_1 = np.arange(0,width, 2*dE)
        energy = np.concatenate((energy_0, energy_1))

    # ramp/unit step function for f" to emulate a simple K-edge
    ramp = energy*slope
    ramp_start = np.argmin(np.abs(ramp))
    ramp_end = np.argmin(np.abs(ramp-1))
    ramp_size = ramp_end-ramp_start
    f_dp = np.heaviside(energy, 0.5)
    mid_ind = np.argmin(np.abs(energy))
    f_dp[mid_ind:mid_ind+ramp_size] = ramp[ramp_start:ramp_end]

    energy = torch.Tensor(energy)
    f_dp = torch.Tensor(f_dp)

    # get f' from the Hilbert transform
    energy_padded,_,f_p_padded,f_dp_padded = \
    kramers_kronig.get_f_p(energy, f_dp, padn=padn,
                           trim=trim,
                           )
    return(energy_padded,f_p_padded,f_dp_padded)

def sample(f_p,
           f_dp,
           loc=[0,0],
           scale=[1e-3,1e-3],
           ):
    """Add Gaussian noise to the f' and f" curves and sample"""
    f_p_dist = torch.distributions.normal.Normal(f_p + loc[0], scale[0])
    f_dp_dist = torch.distributions.normal.Normal(f_dp + loc[1], scale[1])
    return(f_p_dist.sample(),f_dp_dist.sample())

def subsample(energy,
              f_p,
              f_dp,
              spacing=2,
              ):
    """Subsample the f' and f" curves at an input spacing"""
    inds = np.arange(0,len(energy),spacing)
    energy = energy[inds]
    f_p = f_p[inds]
    f_dp = f_dp[inds]
    return(energy,f_p,f_dp,inds)

def get_loss(energy,
             f_p_opt,
             f_dp_opt,
             f_p_noisy_ss,
             f_dp_noisy_ss,
             inds,
             known_response_energy=None,
             known_response_f_p=None,
             known_response_f_dp=None,
             ):
    """Optimization loss is the MSE of the match to the noisy subsampled data summed with the penalty for
    violating the Kramers-Kronig relations"""
    data_loss = torch.mean((f_p_opt[inds]-f_p_noisy_ss)**2 + (f_dp_opt[inds]-f_dp_noisy_ss)**2)
    kk_loss = kramers_kronig.get_penalty(energy, f_p_opt, f_dp_opt, padn=0, trim=0,
                                         known_response_energy=known_response_energy,
                                         known_response_f_p=known_response_f_p,
                                         known_response_f_dp=known_response_f_dp,
                                         )

    return(data_loss + kk_loss)

def run_example_opt(width=5,
                    padn=100,
                    trim=30,
                    spacing=5,
                    noise_loc=[0,0],
                    noise_scale=[1e-1,1e-1],
                    learning_rate=1e-1,
                    num_iter=10000,
                    uniform_energy=True,
                    known_response_energy=None,
                    known_response_f_p=None,
                    known_response_f_dp=None,
                    ):
    """
    Run an example optimization with the following steps:
    1. Simulate f" based on a very simple model of the K-edge.
    2. Use the dispersion relations to calculate fâ€².
    3. Sample both of these curves with Gaussian noise to simulate experimental measurement of the two curves.
    4. Use restraint to optimize the parameters. Use automatic differentiation for first-derivatives.
    """

    energy,f_p,f_dp = create_f(width=width,
                               padn=padn,
                               trim=trim,
                               uniform_energy=uniform_energy)

    f_p_noisy,f_dp_noisy = sample(f_p,f_dp,
                                  loc=noise_loc,
                                  scale=noise_scale,
                                  )
    energy_ss,f_p_noisy_ss,f_dp_noisy_ss,inds = subsample(energy,f_p_noisy,f_dp_noisy,
                                                          spacing=spacing)

    # From energy_ss,f_p_noisy_ss, and f_dp_noisy_ss, determine f_p and f_dp, energy is given
    f_p_pred_0 = kramers_kronig_helper.INTERP_FUNC(energy_ss,f_p_noisy_ss)(energy)
    f_dp_pred_0 = kramers_kronig_helper.INTERP_FUNC(energy_ss,f_dp_noisy_ss)(energy)

    f_p_opt = torch.tensor(f_p_pred_0,requires_grad=True)
    f_dp_opt = torch.tensor(f_dp_pred_0, requires_grad=True)

    optimizer = torch.optim.SGD([f_p_opt,f_dp_opt],lr=learning_rate)

    loss_vec = []
    actual_loss_vec = []
    for i in range(num_iter):
        loss = get_loss(energy,
                        f_p_opt,
                        f_dp_opt,
                        f_p_noisy_ss,
                        f_dp_noisy_ss,
                        inds,
                        known_response_energy=known_response_energy,
                        known_response_f_p=known_response_f_p,
                        known_response_f_dp=known_response_f_dp,
                        )
        actual_loss = torch.mean((f_p_opt-f_p)**2 + (f_dp_opt-f_dp)**2)
        loss_vec.append(loss)
        actual_loss_vec.append(actual_loss)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    return(energy,
           f_p,
           f_dp,
           energy_ss,
           f_p_noisy_ss,
           f_dp_noisy_ss,
           f_p_pred_0,
           f_dp_pred_0,
           f_p_opt,
           f_dp_opt,
           loss_vec,
           actual_loss_vec,
           )

def visualize(energy,
              f_p,
              f_dp,
              energy_ss,
              f_p_noisy_ss,
              f_dp_noisy_ss,
              f_p_pred_0,
              f_dp_pred_0,
              f_p_opt,
              f_dp_opt,
              loss_vec,
              actual_loss_vec,
              ):

    """Compare the optimized model to the initial ground truth
    and show result in matplotlib."""

    plt.figure()
    plt.title("Subsampled curves with noise")
    plt.plot(energy,f_dp,energy_ss,f_dp_noisy_ss,'.')
    plt.plot(energy,f_p,energy_ss,f_p_noisy_ss,'.')
    plt.xlim([energy[0],energy[-1]])
    plt.show()

    plt.figure()
    plt.title('Initial Guess for actual curves')
    plt.plot(energy,f_dp,energy,f_dp_pred_0,'.')
    plt.plot(energy,f_p,energy,f_p_pred_0,'.')
    plt.xlim([energy[0],energy[-1]])
    plt.show()

    plt.figure()
    plt.title('Final Guess for actual curves')
    plt.plot(energy,f_dp,energy,f_dp_opt.detach().numpy(),'.')
    plt.plot(energy,f_p,energy,f_p_opt.detach().numpy(),'.')
    plt.xlim([energy[0],energy[-1]])
    plt.show()

    plt.figure()
    plt.title('Objective Loss')
    plt.plot([loss.detach().numpy() for loss in loss_vec])
    plt.show()

    plt.figure()
    plt.title('Ground Truth Loss')
    plt.plot([actual_loss.detach().numpy() for actual_loss in actual_loss_vec])
    plt.show()


 *******************************************************************************


 *******************************************************************************
cctbx/dispersion/tests/tst_kramers_kronig.py
"""Tests for kramers_kroning.py"""

from __future__ import division

import numpy as np
import torch

import cctbx.dispersion.kramers_kronig.kramers_kronig_helper as kramers_kronig_helper
import cctbx.dispersion.kramers_kronig.kramers_kronig as kramers_kronig

# Get constants
Fe3 = kramers_kronig_helper.parse_data(kramers_kronig_helper.SAMPLE_DATA_PATH + "/pf-rd-ox_fftkk.out")[6064:6165,:]
Fe2 = kramers_kronig_helper.parse_data(kramers_kronig_helper.SAMPLE_DATA_PATH + "/pf-rd-red_fftkk.out")[6064:6165,:]

path = kramers_kronig_helper.SAMPLE_DATA_PATH + "/Fe_fake.dat"
sf = kramers_kronig_helper.parse_data(path)
ind_0 = np.argmin(np.abs(sf[:,0]-7070))
ind_1 = np.argmin(np.abs(sf[:,0]-7170))
sf = sf[ind_0:ind_1,:]
Fe0 = sf


def get_f_p_get_f_dp(sf, padn=10):
    """Helper for test that finding f_p and then finding f_dp yields the input f_dp."""

    energy = torch.Tensor(sf[:,0])
    f_dp = torch.Tensor(sf[:,2])

    energy_padded,f_p_pred,f_p_pred_padded,f_dp_padded = kramers_kronig.get_f_p(energy,
                                                                                f_dp,
                                                                                padn=padn,
                                                                                )


    energy_padded,f_dp_pred,f_dp_pred_padded,_ = kramers_kronig.get_f_dp(energy_padded,
                                                                         f_p_pred_padded,
                                                                         padn=0,
                                                                         )


    # add back DC term
    F_dp_pred = np.fft.fft(f_dp_pred)
    F_dp_pred[0] = np.fft.fft(f_dp_padded)[0]
    f_dp_pred = np.fft.ifft(F_dp_pred).real

    if padn != 0:
        f_dp_padded = f_dp_padded[padn:-padn]
        f_dp_pred = f_dp_pred[padn:-padn]

    return(f_dp_padded, f_dp_pred)


def test_get_f_p_get_f_dp_Fe3(Fe3):
    """Test that finding f_p and then finding f_dp yields the input f_dp."""
    f_dp, f_dp_pred = get_f_p_get_f_dp(Fe3)

    np.testing.assert_allclose(f_dp, f_dp_pred, atol=1e-4, rtol=1e-4)


def test_get_f_p_get_f_dp_Fe2(Fe2):
    """Test that finding f_p and then finding f_dp yields the input f_dp."""
    f_dp, f_dp_pred = get_f_p_get_f_dp(Fe2)

    np.testing.assert_allclose(f_dp, f_dp_pred, atol=1e-4, rtol=1e-4)


def get_penalty(sf, padn=10):
    """Helper for test that finding f_p and then calculating the penalty yields 0 penalty"""

    energy = torch.Tensor(sf[:,0])
    f_dp = torch.Tensor(sf[:,2])

    energy_padded,f_p_pred,f_p_pred_padded,f_dp_padded  = kramers_kronig.get_f_p(energy,
                                                                                 f_dp,
                                                                                 padn=padn,
                                                                                 )

    mse = kramers_kronig.get_penalty(energy_padded, f_p_pred_padded,
                                     f_dp_padded,
                                     padn=0, trim=0)
    return(mse)


def test_get_penalty_Fe3(Fe3):
    """Test that finding f_p and then calculating the penalty yields 0 penalty"""
    mse = get_penalty(Fe3)
    np.testing.assert_allclose(0,mse,atol=1e-8, rtol=1e-8)


def test_get_penalty_Fe2(Fe2):
    """Test that finding f_p and then calculating the penalty yields 0 penalty"""
    mse = get_penalty(Fe2)
    np.testing.assert_allclose(0,mse,atol=1e-8, rtol=1e-8)


def test_get_penalty_known_response(Fe2):
    """Test that the penalty of Fe2 is 0 when the known response is the same as the input function"""

    energy = torch.Tensor(Fe2[:,0])
    f_p = torch.Tensor(Fe2[:,1])
    f_dp = torch.Tensor(Fe2[:,2])


    mse = kramers_kronig.get_penalty(energy, f_p,
                                     f_dp,
                                     padn=0, trim=0,
                                     known_response_energy=energy,
                                     known_response_f_p=f_p,
                                     known_response_f_dp=f_dp,
                                     )

    np.testing.assert_allclose(0,mse,atol=1e-8, rtol=1e-8)

def test_cos_wave():
    """Test that finding f_p when f_dp is cos(energy) yields sin(energy)"""
    energy = torch.Tensor(np.arange(-np.pi,np.pi,.05))

    u = torch.Tensor(np.cos(energy))
    h_u = np.sin(energy)

    _,f_p_pred,_,_ = kramers_kronig.get_f_p(energy, u,
                                            padn=0, trim=0,
                                            )

    np.testing.assert_allclose(h_u, f_p_pred, rtol=1e-2, atol=1e-2)


def test_get_f_p_cos_wave_0():
    """Test that finding f_p with an added cos wave is same as subtracting the cos wave,
    transforming, and adding the known response"""

    padn=0
    energy = torch.Tensor(np.arange(7070,7170))

    T = energy[-1]-energy[0]
    w = 2*np.pi/T
    u = torch.Tensor(np.cos(w*energy))

    _,h_u,_,_ = kramers_kronig.get_f_p(energy, u, padn=padn,
                                            known_response_energy=None,
                                            known_response_f_p=None,
                                            known_response_f_dp=None,)

    _,f_p_pred_subtract,_,_ = kramers_kronig.get_f_p(energy, u, padn=padn,
                                                      known_response_energy=energy,
                                                      known_response_f_p=h_u,
                                                      known_response_f_dp=u,
                                                      )

    np.testing.assert_allclose(f_p_pred_subtract, h_u, rtol=1e-4, atol=1e-4)



def test_get_f_p_cos_wave(Fe3):
    """Test that finding f_p with an added cos wave is same as subtracting the cos wave,
    transforming, and adding the known response"""

    padn=0
    energy = torch.Tensor(Fe3[:,0])
    f_dp = torch.Tensor(Fe3[:,2])

    T = energy[-1]-energy[0]
    w = 2*np.pi/T
    u = torch.Tensor(np.cos(w*energy))

    _,h_u,_,_ = kramers_kronig.get_f_p(energy, u, padn=padn,
                                            known_response_energy=None,
                                            known_response_f_p=None,
                                            known_response_f_dp=None,)

    _,f_p_pred_subtract,_,_ = kramers_kronig.get_f_p(energy, f_dp+u, padn=padn,
                                                     known_response_energy=energy,
                                                     known_response_f_p=h_u,
                                                     known_response_f_dp=u,
                                                     )



    _,f_p_pred,_,_ = kramers_kronig.get_f_p(energy, f_dp+u, padn=padn,
                                            known_response_energy=None,
                                            known_response_f_p=None,
                                            known_response_f_dp=None,
                                            )

    _,f_p_pred_subtract,_,_ = kramers_kronig.get_f_p(energy, u, padn=padn,
                                                      known_response_energy=energy,
                                                      known_response_f_p=h_u,
                                                      known_response_f_dp=u,
                                                      )

    _,f_p_pred,_,_ = kramers_kronig.get_f_p(energy, u, padn=padn,
                                            known_response_energy=None,
                                            known_response_f_p=None,
                                            known_response_f_dp=None,
                                            )

    np.testing.assert_allclose(f_p_pred_subtract, f_p_pred, rtol=1e-4, atol=1e-4)

def test_get_f_dp_cos_wave(Fe3):
    """Test that finding f_dp with an added cos wave is same as subtracting the cos wave,
    transforming, and adding the known response"""

    padn=0
    energy = torch.Tensor(Fe3[:,0])
    f_p = torch.Tensor(Fe3[:,1])

    T = energy[-1]-energy[0]
    w = 2*np.pi/T
    u = torch.Tensor(np.cos(w*energy))
    _,h_u,_,_ = kramers_kronig.get_f_p(energy, u, padn=padn)

    _,f_dp_pred_subtract,_,_ = kramers_kronig.get_f_dp(energy, f_p+h_u, padn=padn,
                                                       known_response_energy=energy,
                                                       known_response_f_p=h_u,
                                                       known_response_f_dp=u,
                                                       )

    _,f_dp_pred,_,_ = kramers_kronig.get_f_dp(energy, f_p+h_u, padn=padn,
                                              known_response_energy=None,
                                              known_response_f_p=None,
                                              known_response_f_dp=None,
                                              )

    np.testing.assert_allclose(f_dp_pred_subtract, f_dp_pred, rtol=1e-2, atol=1e-2)

def test_get_f_p_nonuniform(Fe0, padn=100):
    """Test that finding f_p for Fe_nonuniform is the same as finding it for Fe_uniform"""

    # remove some values to make non-uniform
    energy_0 = torch.Tensor(np.concatenate((Fe0[0:2,0],Fe0[2::2,0],Fe0[-1:,0]),axis=0))
    f_dp_0 = torch.Tensor(np.concatenate((Fe0[0:2,2],Fe0[2::2,2], Fe0[-1:,2]),axis=0))

    # original vector
    energy_1 = torch.Tensor(Fe0[:-1,0])
    f_dp_1 = torch.Tensor(Fe0[:-1,2])

    energy_padded_0,f_p_pred_0,_,_ = kramers_kronig.get_f_p(energy_0,
                                              f_dp_0,
                                              padn=padn,
                                              )

    energy_padded_1,f_p_pred_1,_,_ = kramers_kronig.get_f_p(energy_1,
                                              f_dp_1,
                                              padn=padn,
                                              )

    np.testing.assert_allclose(energy_padded_0, energy_padded_1)
    np.testing.assert_allclose(f_p_pred_0, f_p_pred_1, rtol=1e-1, atol=1e-1)

def run():
    """Run all tests"""
    test_get_f_p_get_f_dp_Fe3(Fe3)
    test_get_f_p_get_f_dp_Fe2(Fe2)
    test_get_penalty_Fe3(Fe3)
    test_get_penalty_Fe2(Fe2)
    test_get_penalty_known_response(Fe2)
    test_cos_wave()
    test_get_f_p_cos_wave_0()
    test_get_f_p_cos_wave(Fe3)
    test_get_f_dp_cos_wave(Fe3)
    test_get_f_p_nonuniform(Fe0, padn=100)
    print("OK")

if __name__ == '__main__':
    run()


 *******************************************************************************


 *******************************************************************************
cctbx/dispersion/tests/tst_kramers_kronig_helper.py
"""Tests for kramers_kronig_helper.py"""

from __future__ import division

import numpy as np
import torch

import cctbx.dispersion.kramers_kronig.kramers_kronig_helper as kramers_kronig_helper

# Get constants
Fe3 = kramers_kronig_helper.parse_data(kramers_kronig_helper.SAMPLE_DATA_PATH + "/pf-rd-ox_fftkk.out")
Fe2 = kramers_kronig_helper.parse_data(kramers_kronig_helper.SAMPLE_DATA_PATH + "/pf-rd-red_fftkk.out")

def test_parse_Fe3_beginning(Fe3):
    """Test that input is parsed properly."""
    np.testing.assert_equal(Fe3[:4], np.array([[1006.0,-1.95442043311, 5.92009170594],
                                               [1007.0, -2.81223119888, 8.52503033764],
                                               [1008.0, -3.33235759037, 10.1156545543],
                                               [1009.0, -3.56113395273, 10.8295300422]]))

def test_parse_Fe3_end(Fe3):
    """Test that input is parsed properly."""
    np.testing.assert_equal(Fe3[-4:], np.array([[24902.0,0.244888423888, 0.418575531827],
                                                [24903.0,0.237396106135, 0.405759333696],
                                                [24904.0,0.220802089543, 0.377388435958],
                                                [24905.0,0.18454034239, 0.315405661584]]))

def test_interpolate_scipy(Fe3):
    """Test that interpolate with scipy mode results in a uniform x array."""
    Fe3 = Fe3[np.array([0,100,101,102,103,104]),:]
    x_new, y_new = kramers_kronig_helper.interpolate(Fe3[:,0],Fe3[:,1], mode="scipy")
    dx_new = x_new[1:]-x_new[:-1]
    np.testing.assert_equal(False,np.any(dx_new-dx_new[0]))

def test_interpolate_torch_uniformity(Fe3):
    """Test that interpolate with torch mode results in a uniform x array."""
    Fe3 = Fe3[np.array([0,100,101,102,103,104]),:]
    x_new, y_new = kramers_kronig_helper.interpolate(torch.tensor(Fe3[:,0]),torch.tensor(Fe3[:,1]), mode="torch")
    dx_new = x_new[1:]-x_new[:-1]
    np.testing.assert_equal(False,np.any(np.array(dx_new-dx_new[0])))

def test_interpolate_torch_0(Fe3):
    """Test that interpolate_torch yields correct result on linear example."""
    x_original = torch.tensor([1,5,10])
    y_original = torch.tensor([2,10,20])
    x_new = torch.tensor([4,7])
    y_new = kramers_kronig_helper.interpolate_torch(x_original, y_original, x_new)
    np.testing.assert_equal(np.array(y_new),np.array([8,14]))

def test_interpolate_torch_1(Fe3):
    """Test that interpolate_torch yields correct result on linear example."""
    x_original = torch.tensor([1,2,3,4,5,6,7,8,9,10])
    y_original = 2*torch.tensor([1,2,3,4,5,6,7,8,9,10])
    x_new = torch.tensor([4,7])
    y_new = kramers_kronig_helper.interpolate_torch(x_original, y_original, x_new)
    np.testing.assert_equal(np.array(y_new),np.array([8,14]))

def run():
    """Run all tests"""
    test_parse_Fe3_beginning(Fe3)
    test_parse_Fe3_end(Fe3)
    test_interpolate_scipy(Fe3)
    test_interpolate_torch_uniformity(Fe3)
    test_interpolate_torch_0(Fe3)
    test_interpolate_torch_1(Fe3)
    print("OK")

if __name__ == '__main__':
    run()


 *******************************************************************************


 *******************************************************************************
cctbx/dispersion/tests/tst_kramers_kronig_optimize.py
"""Tests for optimizations using the kramers_kronig_optimize.py"""

from __future__ import division

import numpy as np
import cctbx.dispersion.kramers_kronig.kramers_kronig_optimize as kramers_kronig_optimize

def test_run_example_opt_params_0():
    """Test optimization with Kramers-Kronig penalty yields decreasing actual loss"""
    _,_,_,_,_,_,_,_,_,_,_,\
    actual_loss_vec  = kramers_kronig_optimize.run_example_opt(width=5,
                                                               padn=10,
                                                               trim=30,
                                                               spacing=20,
                                                               noise_loc=[0,0],
                                                               noise_scale=[1e-3,1e-3],
                                                               learning_rate=1e-1,
                                                               num_iter=100,
                                                               )
    np.testing.assert_array_less(actual_loss_vec[-1].detach().numpy(),actual_loss_vec[0].detach().numpy())

def test_run_example_opt_params_1():
    """Test optimization with Kramers-Kronig penalty yields decreasing actual loss"""
    _,_,_,_,_,_,_,_,_,_,_,\
    actual_loss_vec  = kramers_kronig_optimize.run_example_opt(width=5,
                                                               padn=10,
                                                               trim=30,
                                                               spacing=20,
                                                               noise_loc=[0,0],
                                                               noise_scale=[1e-2,1e-2],
                                                               learning_rate=1e-1,
                                                               num_iter=100,
                                                               )
    np.testing.assert_array_less(actual_loss_vec[-1].detach().numpy(),actual_loss_vec[0].detach().numpy())

def test_run_example_opt_params_2():
    """Test optimization with Kramers-Kronig penalty yields decreasing actual loss"""
    _,_,_,_,_,_,_,_,_,_,_,\
    actual_loss_vec  = kramers_kronig_optimize.run_example_opt(width=5,
                                                               padn=10,
                                                               trim=30,
                                                               spacing=5,
                                                               noise_loc=[0,0],
                                                               noise_scale=[1e-3,1e-3],
                                                               learning_rate=1e-1,
                                                               num_iter=100,
                                                               )
    np.testing.assert_array_less(actual_loss_vec[-1].detach().numpy(),actual_loss_vec[0].detach().numpy())

def test_run_example_opt_params_3():
    """Test optimization with Kramers-Kronig penalty yields decreasing actual loss"""
    _,_,_,_,_,_,_,_,_,_,_,\
    actual_loss_vec  = kramers_kronig_optimize.run_example_opt(width=5,
                                                               padn=10,
                                                               trim=30,
                                                               spacing=5,
                                                               noise_loc=[0,0],
                                                               noise_scale=[1e-3,1e-3],
                                                               learning_rate=1e-1,
                                                               num_iter=100,
                                                               uniform_energy=False,
                                                               )
    np.testing.assert_array_less(actual_loss_vec[-1].detach().numpy(),actual_loss_vec[0].detach().numpy())

def run():
    """Run all tests"""
    test_run_example_opt_params_0()
    test_run_example_opt_params_1()
    test_run_example_opt_params_2()
    test_run_example_opt_params_3()
    print("OK")

if __name__ == '__main__':
    run()


 *******************************************************************************
