

 *******************************************************************************
xfel/command_line/make_mask.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.make_mask
#
# $Id: make_mask.py 411 2013-10-16 22:17:45Z aaron $
#
# This code reads three cctbx.xfel pickle format images and builds a mask from
# them.  The first image should be an average from a dark run, the second the
# standard deviation from that run.  The third image should be a maximum projection
# from a run with the beam on.
#
# The result is an image with all pixels which are valid for use set to 0, and
# those that are invalid set to -2 by default, or the value of the option passed in
# to mask_pix_val.
#

import dxtbx.format.Registry
from xfel.cxi.cspad_ana.cspad_tbx import dpack, dwritef2
from scitbx.array_family import flex
import sys

def point_in_polygon(point, poly):
  """ Determine if a point is inside a given polygon or not.  Polygon is a list of (x,y) pairs.
  Code adapted from a dials polygon clipping test algorithm"""
  if len(poly) < 3: return False

  inside = False
  for i in range(len(poly)):
    j = (i+1) % len(poly)
    if (((poly[i][1] > point[1]) != (poly[j][1] > point[1])) and
      (point[0] < (poly[j][0] - poly[i][0]) * (point[1] - poly[i][1]) /
                  (poly[j][1] - poly[i][1]) + poly[i][0])):
      inside = not inside
  return inside

def point_inside_circle(x,y,center_x,center_y,radius):
  """Determine if a given point (x,y) is inside a circle whose center is at
  (center_x,center_y) with radius x."""
  return (x-center_x)**2 + (y - center_y)**2 < radius**2

def run(argv=None):
  import libtbx.option_parser

  if (argv is None):
    argv = sys.argv

  command_line = (libtbx.option_parser.option_parser(
    usage="%s [-v] [-p poly_mask] [-c circle_mask] [-a avg_max] [-s stddev_max] [-m maxproj_min] [-x mask_pix_val] [-o output] avg_path stddev_path max_path" % libtbx.env.dispatcher_name)
                  .option(None, "--verbose", "-v",
                          action="store_true",
                          default=False,
                          dest="verbose",
                          help="Print more information about progress")
                  .option(None, "--poly_mask", "-p",
                          type="string",
                          default=None,
                          dest="poly_mask",
                          help="Polygon to mask out.  Comma-seperated string of xy pairs.")
                  .option(None, "--circle_mask", "-c",
                          type="string",
                          default=None,
                          dest="circle_mask",
                          help="Circle to mask out.  Comma-seperated string of x, y, and radius.")
                  .option(None, "--avg_max", "-a",
                          type="float",
                          default=2000.0,
                          dest="avg_max",
                          help="Maximum ADU that pixels in the average image are allowed to have before masked out")
                  .option(None, "--stddev_max", "-s",
                          type="float",
                          default=10.0,
                          dest="stddev_max",
                          help="Maximum ADU that pixels in the standard deviation image are allowed to have before masked out")
                  .option(None, "--maxproj_min", "-m",
                          type="float",
                          default=300.0,
                          dest="maxproj_min",
                          help="Minimum ADU that pixels in the maximum projection image are allowed to have before masked out")
                  .option(None, "--mask_pix_val", "-x",
                          type="int",
                          default=-2,
                          dest="mask_pix_val",
                          help="Value for masked out pixels")
                  .option(None, "--detector_format_version", "-d",
                          type="string",
                          default=None,
                          dest="detector_format_version",
                          help="detector format version string")
                  .option(None, "--output", "-o",
                          type="string",
                          default="mask_.pickle",
                          dest="destpath",
                          help="output file path, should be *.pickle")
                  ).process(args=argv[1:])

  # Must have exactly three remaining arguments.
  paths = command_line.args
  if (len(paths) != 3):
    command_line.parser.print_usage(file=sys.stderr)
    return

  if command_line.options.detector_format_version is None:
    address = timestamp = None
  else:
    from xfel.cxi.cspad_ana.cspad_tbx import evt_timestamp
    from iotbx.detectors.cspad_detector_formats import address_and_timestamp_from_detector_format_version
    address, timestamp = address_and_timestamp_from_detector_format_version(command_line.options.detector_format_version)
    timestamp = evt_timestamp((timestamp,0))

  poly_mask = None
  if not command_line.options.poly_mask == None:
    poly_mask = []
    poly_mask_tmp = command_line.options.poly_mask.split(",")
    if len(poly_mask_tmp) % 2 != 0:
      command_line.parser.print_usage(file=sys.stderr)
      return
    odd = True
    for item in poly_mask_tmp:
      try:
        if odd:
          poly_mask.append(int(item))
        else:
          poly_mask[-1] = (poly_mask[-1],int(item))
      except ValueError:
        command_line.parser.print_usage(file=sys.stderr)
        return
      odd = not odd

  circle_mask = None
  if command_line.options.circle_mask is not None:
    circle_mask_tmp = command_line.options.circle_mask.split(",")
    if len(circle_mask_tmp) != 3:
      command_line.parser.print_usage(file=sys.stderr)
      return
    try:
      circle_mask = (int(circle_mask_tmp[0]),int(circle_mask_tmp[1]),int(circle_mask_tmp[2]))
    except ValueError:
      command_line.parser.print_usage(file=sys.stderr)
      return

  avg_path    = paths[0]
  stddev_path = paths[1]
  max_path    = paths[2]

  # load the three images
  format_class = dxtbx.format.Registry.get_format_class_for_file(avg_path)
  avg_f = format_class(avg_path)
  avg_i = avg_f.get_detectorbase()
  avg_d = avg_i.get_raw_data()

  stddev_f = format_class(stddev_path)
  stddev_i = stddev_f.get_detectorbase()
  stddev_d = stddev_i.get_raw_data()

  max_f = format_class(max_path)
  max_i = max_f.get_detectorbase()
  max_d = max_i.get_raw_data()

  # first find all the pixels in the average that are less than zero or greater
  # than a cutoff and set them to the masking value
  avg_d.set_selected((avg_d <= 0) | (avg_d > command_line.options.avg_max), command_line.options.mask_pix_val)

  # set all the rest of the pixels to zero.  They will be accepted
  avg_d.set_selected(avg_d != command_line.options.mask_pix_val, 0)

  # mask out the overly noisy or flat pixels
  avg_d.set_selected(stddev_d <= 0, command_line.options.mask_pix_val)
  avg_d.set_selected(stddev_d >= command_line.options.stddev_max, command_line.options.mask_pix_val)

  # these are the non-bonded pixels
  avg_d.set_selected(max_d < command_line.options.maxproj_min, command_line.options.mask_pix_val)

  # calculate the beam center
  panel = avg_f.get_detector()[0]
  bcx, bcy = panel.get_beam_centre(avg_f.get_beam().get_s0())

  if poly_mask is not None or circle_mask is not None:
    minx = miny = 0
    maxx = avg_d.focus()[0]
    maxy = avg_d.focus()[1]
    if poly_mask is not None:
      minx = min([x[0] for x in poly_mask])
      miny = min([y[1] for y in poly_mask])
      maxx = max([x[0] for x in poly_mask])
      maxy = max([y[1] for y in poly_mask])
    if circle_mask is not None:
      circle_x, circle_y, radius = circle_mask

      if circle_x - radius < minx: minx = circle_x - radius
      if circle_y - radius < miny: miny = circle_y - radius
      if circle_x + radius > maxx: maxx = circle_x + radius
      if circle_y + radius > maxy: maxy = circle_y + radius

    sel = avg_d == command_line.options.mask_pix_val
    for j in range(miny, maxy):
      for i in range(minx, maxx):
        idx = j * avg_d.focus()[0] + i
        if not sel[idx]:
          if poly_mask is not None and point_in_polygon((i,j),poly_mask):
            sel[idx] = True
          elif circle_mask is not None and point_inside_circle(i,j,circle_x,circle_y,radius):
            sel[idx] = True
    avg_d.set_selected(sel,command_line.options.mask_pix_val)

  # have to re-layout the data to match how it was stored originally
  shifted_int_data_old = avg_d
  shifted_int_data_new = shifted_int_data_old.__class__(
    flex.grid(shifted_int_data_old.focus()))
  shifted_int_data_new += command_line.options.mask_pix_val

  phil = avg_i.horizons_phil_cache
  manager = avg_i.get_tile_manager(phil)

  for i,shift in enumerate(manager.effective_translations()):
    shift_slow = shift[0]
    shift_fast = shift[1]

    ur_slow = phil.distl.detector_tiling[4 * i + 0] + shift_slow
    ur_fast = phil.distl.detector_tiling[4 * i + 1] + shift_fast
    ll_slow = phil.distl.detector_tiling[4 * i + 2] + shift_slow
    ll_fast = phil.distl.detector_tiling[4 * i + 3] + shift_fast

    #print "Shifting tile at (%d, %d) by (%d, %d)" % (ur_slow-shift_slow, ur_fast-shift_fast, -shift_slow, -shift_fast)

    shifted_int_data_new.matrix_paste_block_in_place(
      block = shifted_int_data_old.matrix_copy_block(
        i_row=ur_slow,i_column=ur_fast,
        n_rows=ll_slow-ur_slow, n_columns=ll_fast-ur_fast),
      i_row = ur_slow - shift_slow,
      i_column = ur_fast - shift_fast
    )

  d = dpack(
    active_areas=avg_i.parameters['ACTIVE_AREAS'],
    address=address,
    beam_center_x=bcx,
    beam_center_y=bcy,
    data=shifted_int_data_new,
    distance=avg_i.distance,
    timestamp=timestamp,
    wavelength=avg_i.wavelength,
    xtal_target=None,
    pixel_size=avg_i.pixel_size,
    saturated_value=avg_i.saturation)

  dwritef2(d, command_line.options.destpath)

  #the minimum number of pixels to mask out cooresponding to the interstitial regions for the CS-PAD
  min_count  = 818265 # (1765 * 1765) - (194 * 185 * 64)
  masked_out = len(avg_d.as_1d().select((avg_d == command_line.options.mask_pix_val).as_1d()))
  assert masked_out >= min_count

  print("Masked out %d pixels out of %d (%.2f%%)"% \
    (masked_out-min_count,len(avg_d)-min_count,(masked_out-min_count)*100/(len(avg_d)-min_count)))

if (__name__ == "__main__"):
  sys.exit(run())


 *******************************************************************************


 *******************************************************************************
xfel/command_line/metrology.py
#
# LIBTBX_SET_DISPATCHER_NAME cspad.metrology
#

from __future__ import absolute_import, division, print_function
from scipy.optimize import leastsq # special import
from xfel.metrology.mark10 import fit_translation4
from libtbx.utils import Sorry
import os.path

master_phil="""
data = None
  .type = path
  .multiple = True
  .help = Directory containing integrated data in pickle format.
  .help = Repeat to specify additional directories, as many as desired.
  .help = Single entry can be a valid unix glob like /pathto/run000[3-8]/integration
bravais_setting_id = None
  .type = int
  .help = ID number for the Bravais setting of interest (Labelit format).
  .help = triclinic=1 monoclinic=2 orthorhombic=5 rhombohedral=5
  .help = tetragonal=9 hexagonal=12 cubic=22
max_frames = None
  .type = int(value_min=2)
  .help = From all the input images, only use the first max_frames for metrology analysis.
min_count = 25
  .type = int(value_min=3)
  .help = On each sensor, require this minimum number of spots before proposing unit pixel translations.
  .help = On the CSPAD, 1 sensor is a pair of 2 ASICs.
detector_format_version = None
  .type = str
  .help = CSPAD format version, as defined in spotfinder/applications/xfel/cxi_phil.py
  .help = Case I: If given, program will print out old and new unit pixel translations, to be pasted back into code.
  .help = The new replaces the old.
  .help = Case II: If not given, program will print increments to the whatever translations were used for integration,
  .help = also to be pasted back into the code.  The new increments the old, and this must be explicitly coded.
show_plots = False
  .type = bool
  .help = Show graphical plots using matplotlib (expert only)
show_consistency = False
  .type = bool
  .help = Run the consistency controls (expert only)
effective_tile_boundaries = None
  .type = ints
  .help = Effective integer tile boundaries applied to convert xtc stream to pickled image files. Must be 64 * 4 integers.
  .help = Boundaries should not be under user control, they are provided by the input data (pickle files).
diff_cutoff = 5
  .type = float
  .help = throw out image if any correction vector exceeds this length in pixels
"""

def get_phil(args):
  print(args)

  import iotbx.phil
  phil = iotbx.phil.process_command_line(args=args, master_string=master_phil).show()
  work_params = phil.work.extract()

  if work_params.show_plots is True:
    from matplotlib import pyplot as plt # special import
  return work_params

def validate_phil(wp):
  if wp.data == []:
    raise Sorry("need one or more data directories")

  #handle globs at this level:
  import glob
  provisional = []
  for glob_item in wp.data:
    provisional = provisional + glob.glob(glob_item)
  wp.data = provisional

  for item in wp.data:
    if not os.path.isdir(item):
      raise Sorry("can't stat directory",item)

  if wp.bravais_setting_id is None or \
     wp.bravais_setting_id not in [1,2,5,9,12,22]:
    raise Sorry("""must set the bravais_setting_id
  triclinic=1 monoclinic=2 orthorhombic=5 rhombohedral=5
  tetragonal=9 hexagonal=12 cubic=22""")

def run(args):

  work_params = get_phil(args)
  try:
    validate_phil(work_params)
  except Sorry as s:
    raise s
  except Exception as s:
    raise s

  C = fit_translation4(work_params)
  print("done constructor")

  #parse a phil file from the args
  for item in args:
    if os.path.isfile(item):
      from xfel.phil_preferences import load_cxi_phil
      C.optional_params = load_cxi_phil(item,[])
      break

  print()
  C.run_cycle_a()
  C.run_cycle_b(0)
  print()
  C.run_cycle_a()
  C.run_cycle_b(1)
  print()
  C.run_cycle_a()

if (__name__ == "__main__"):
  import sys
  run(args=sys.argv[1:])

#cspad.metrology data=/reg/d/psdm/CXI/cxid9114/ftc/sauter/results/r010[2-9]/018/integration max_frames=300 bravais_setting_id=9
#cspad.metrology data=/reg/d/psdm/CXI/cxid9114/ftc/brewster/results/r00[3-4]*/003/integration max_frames=300 bravais_setting_id=9
#cspad.metrology data=/reg/d/psdm/CXI/cxid9114/ftc/brewster/results/r00[3-4]*/003/integration max_frames=300 bravais_setting_id=9 min_count=20


 *******************************************************************************


 *******************************************************************************
xfel/command_line/monitor_detectors.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.monitor_detectors
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1

from xfel.cxi.gfx import wx_detectors
import sys

if (__name__ == "__main__") :
  wx_detectors.run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/monitor_trials.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.monitor_trials
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1

from xfel.cxi.gfx import trials_plot
import sys

if (__name__ == "__main__") :
  trials_plot.run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/mpi_average.py
from __future__ import absolute_import, division, print_function
from six.moves import range
#-*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.mpi_average
#

import psana
import numpy as np
from xfel.cxi.cspad_ana import cspad_tbx, parse_calib
import libtbx
import libtbx.load_env
from libtbx import easy_pickle
import libtbx.option_parser
from scitbx.array_family import flex
import sys, os
from libtbx.utils import Sorry
from six.moves import zip

def average(argv=None):
  if argv == None:
    argv = sys.argv[1:]

  try:
    from libtbx.mpi4py import MPI
  except ImportError:
    raise Sorry("MPI not found")

  command_line = (libtbx.option_parser.option_parser(
    usage="""
%s [-p] -c config -x experiment -a address -r run -d detz_offset [-o outputdir] [-A averagepath] [-S stddevpath] [-M maxpath] [-n numevents] [-s skipnevents] [-v] [-m] [-b bin_size] [-X override_beam_x] [-Y override_beam_y] [-D xtc_dir] [-f] [-g gain_mask_value] [--min] [--minpath minpath]

To write image pickles use -p, otherwise the program writes CSPAD CBFs.
Writing CBFs requires the geometry to be already deployed.

Examples:
cxi.mpi_average -c cxi49812/average.cfg -x cxi49812 -a CxiDs1.0:Cspad.0 -r 25 -d 571

Use one process on the current node to process all the events from run 25 of
experiment cxi49812, using a detz_offset of 571.

mpirun -n 16 cxi.mpi_average -c cxi49812/average.cfg -x cxi49812 -a CxiDs1.0:Cspad.0 -r 25 -d 571

As above, using 16 cores on the current node.

bsub -a mympi -n 100 -o average.out -q psanaq cxi.mpi_average -c cxi49812/average.cfg -x cxi49812 -a CxiDs1.0:Cspad.0 -r 25 -d 571 -o cxi49812

As above, using the psanaq and 100 cores, putting the log in average.out and
the output images in the folder cxi49812.
""" % libtbx.env.dispatcher_name)
                .option(None, "--as_pickle", "-p",
                        action="store_true",
                        default=False,
                        dest="as_pickle",
                        help="Write results as image pickle files instead of cbf files")
                .option(None, "--raw_data", "-R",
                        action="store_true",
                        default=False,
                        dest="raw_data",
                        help="Disable psana corrections such as dark pedestal subtraction or common mode (cbf only)")
                .option(None, "--background_pickle", "-B",
                        default=None,
                        dest="background_pickle",
                        help="")
                .option(None, "--config", "-c",
                        type="string",
                        default=None,
                        dest="config",
                        metavar="PATH",
                        help="psana config file")
                .option(None, "--experiment", "-x",
                        type="string",
                        default=None,
                        dest="experiment",
                        help="experiment name (eg cxi84914)")
                .option(None, "--run", "-r",
                        type="int",
                        default=None,
                        dest="run",
                        help="run number")
                .option(None, "--address", "-a",
                        type="string",
                        default="CxiDs2.0:Cspad.0",
                        dest="address",
                        help="detector address name (eg CxiDs2.0:Cspad.0)")
                .option(None, "--detz_offset", "-d",
                        type="float",
                        default=None,
                        dest="detz_offset",
                        help="offset (in mm) from sample interaction region to back of CSPAD detector rail (CXI), or detector distance (XPP)")
                .option(None, "--outputdir", "-o",
                        type="string",
                        default=".",
                        dest="outputdir",
                        metavar="PATH",
                        help="Optional path to output directory for output files")
                .option(None, "--averagebase", "-A",
                        type="string",
                        default="{experiment!l}_avg-r{run:04d}",
                        dest="averagepath",
                        metavar="PATH",
                        help="Path to output average image without extension. String substitution allowed")
                .option(None, "--stddevbase", "-S",
                        type="string",
                        default="{experiment!l}_stddev-r{run:04d}",
                        dest="stddevpath",
                        metavar="PATH",
                        help="Path to output standard deviation image without extension. String substitution allowed")
                .option(None, "--maxbase", "-M",
                        type="string",
                        default="{experiment!l}_max-r{run:04d}",
                        dest="maxpath",
                        metavar="PATH",
                        help="Path to output maximum projection image without extension. String substitution allowed")
                .option(None, "--numevents", "-n",
                        type="int",
                        default=None,
                        dest="numevents",
                        help="Maximum number of events to process. Default: all")
                .option(None, "--skipevents", "-s",
                        type="int",
                        default=0,
                        dest="skipevents",
                        help="Number of events in the beginning of the run to skip. Default: 0")
                .option(None, "--verbose", "-v",
                        action="store_true",
                        default=False,
                        dest="verbose",
                        help="Print more information about progress")
                .option(None, "--pickle-optical-metrology", "-m",
                        action="store_true",
                        default=False,
                        dest="pickle_optical_metrology",
                        help="If writing pickle files, use the optical metrology in the experiment's calib directory")
                .option(None, "--bin_size", "-b",
                        type="int",
                        default=None,
                        dest="bin_size",
                        help="Rayonix detector bin size")
                .option(None, "--override_beam_x", "-X",
                        type="float",
                        default=None,
                        dest="override_beam_x",
                        help="Rayonix detector beam center x coordinate")
                .option(None, "--override_beam_y", "-Y",
                        type="float",
                        default=None,
                        dest="override_beam_y",
                        help="Rayonix detector beam center y coordinate")
                .option(None, "--calib_dir", "-C",
                        type="string",
                        default=None,
                        dest="calib_dir",
                        metavar="PATH",
                        help="calibration directory")
                .option(None, "--pickle_calib_dir", "-P",
                        type="string",
                        default=None,
                        dest="pickle_calib_dir",
                        metavar="PATH",
                        help="pickle calibration directory specification. Replaces --calib_dir functionality.")
                .option(None, "--xtc_dir", "-D",
                        type="string",
                        default=None,
                        dest="xtc_dir",
                        metavar="PATH",
                        help="xtc stream directory")
                .option(None, "--use_ffb", "-f",
                        action="store_true",
                        default=False,
                        dest="use_ffb",
                        help="Use the fast feedback filesystem at LCLS. Only for the active experiment!")
                .option(None, "--gain_mask_value", "-g",
                        type="float",
                        default=None,
                        dest="gain_mask_value",
                        help="Ratio between low and high gain pixels, if CSPAD in mixed-gain mode. Only used in CBF averaging mode.")
                .option(None, "--min", None,
                        action="store_true",
                        default=False,
                        dest="do_minimum_projection",
                        help="Output a minimum projection")
                .option(None, "--minpath", None,
                        type="string",
                        default="{experiment!l}_min-r{run:04d}",
                        dest="minpath",
                        metavar="PATH",
                        help="Path to output minimum image without extension. String substitution allowed")
                ).process(args=argv)


  if len(command_line.args) > 0 or \
      command_line.options.as_pickle is None or \
      command_line.options.experiment is None or \
      command_line.options.run is None or \
      command_line.options.address is None or \
      command_line.options.detz_offset is None or \
      command_line.options.averagepath is None or \
      command_line.options.stddevpath is None or \
      command_line.options.maxpath is None or \
      command_line.options.pickle_optical_metrology is None:
    command_line.parser.show_help()
    return

  # set this to sys.maxint to analyze all events
  if command_line.options.numevents is None:
    maxevents = sys.maxsize
  else:
    maxevents = command_line.options.numevents

  comm = MPI.COMM_WORLD
  rank = comm.Get_rank()
  size = comm.Get_size()

  if command_line.options.config is not None:
    psana.setConfigFile(command_line.options.config)
  dataset_name = "exp=%s:run=%d:smd"%(command_line.options.experiment, command_line.options.run)
  if command_line.options.xtc_dir is not None:
    if command_line.options.use_ffb:
      raise Sorry("Cannot specify the xtc_dir and use SLAC's ffb system")
    dataset_name += ":dir=%s"%command_line.options.xtc_dir
  elif command_line.options.use_ffb:
    # as ffb is only at SLAC, ok to hardcode /reg/d here
    dataset_name += ":dir=/reg/d/ffb/%s/%s/xtc"%(command_line.options.experiment[0:3],command_line.options.experiment)
  if command_line.options.calib_dir is not None:
    psana.setOption('psana.calib-dir',command_line.options.calib_dir)
  ds = psana.DataSource(dataset_name)
  address = command_line.options.address
  src = psana.Source('DetInfo(%s)'%address)
  nevent = np.array([0.])

  if command_line.options.background_pickle is not None:
    background = easy_pickle.load(command_line.options.background_pickle)['DATA'].as_numpy_array()

  for run in ds.runs():
    runnumber = run.run()

    if not command_line.options.as_pickle:
      psana_det = psana.Detector(address, ds.env())

    # list of all events
    if command_line.options.skipevents > 0:
      print("Skipping first %d events"%command_line.options.skipevents)
    elif "Rayonix" in command_line.options.address:
      print("Skipping first image in the Rayonix detector") # Shuttering issue
      command_line.options.skipevents = 1

    for i, evt in enumerate(run.events()):
      if i%size != rank: continue
      if i < command_line.options.skipevents: continue
      if i >= maxevents: break
      if i%10==0: print('Rank',rank,'processing event',i)
      #print "Event #",rank*mylength+i," has id:",evt.get(EventId)
      if 'Rayonix' in command_line.options.address or 'FeeHxSpectrometer' in command_line.options.address or 'XrayTransportDiagnostic' in command_line.options.address:
        data = evt.get(psana.Camera.FrameV1,src)
        if data is None:
          print("No data")
          continue
        data=data.data16().astype(np.float64)
      elif command_line.options.as_pickle:
        data = evt.get(psana.ndarray_float64_3, src, 'image0')
      else:
        # get numpy array, 32x185x388
        from xfel.cftbx.detector.cspad_cbf_tbx import get_psana_corrected_data
        if command_line.options.raw_data:
          data = get_psana_corrected_data(psana_det, evt, use_default=False, dark=False, common_mode=None,
                                          apply_gain_mask=False, per_pixel_gain=False)
        else:
          if command_line.options.gain_mask_value is None:
            data = get_psana_corrected_data(psana_det, evt, use_default=True)
          else:
            data = get_psana_corrected_data(psana_det, evt, use_default=False, dark=True, common_mode=None,
                                            apply_gain_mask=True, gain_mask_value=command_line.options.gain_mask_value,
                                            per_pixel_gain=False)

      if data is None:
        print("No data")
        continue

      if command_line.options.background_pickle is not None:
        data -= background

      if 'FeeHxSpectrometer' in command_line.options.address or 'XrayTransportDiagnostic' in command_line.options.address:
        distance = np.array([0.0])
        wavelength = np.array([1.0])
      else:
        d = cspad_tbx.env_distance(address, run.env(), command_line.options.detz_offset)
        if d is None:
          print("No distance, using distance", command_line.options.detz_offset)
          assert command_line.options.detz_offset is not None
          if 'distance' not in locals():
            distance = np.array([command_line.options.detz_offset])
          else:
            distance += command_line.options.detz_offset
        else:
          if 'distance' in locals():
            distance += d
          else:
            distance = np.array([float(d)])

        w = cspad_tbx.evt_wavelength(evt)
        if w is None:
          print("No wavelength")
          if 'wavelength' not in locals():
            wavelength = np.array([1.0])
        else:
          if 'wavelength' in locals():
            wavelength += w
          else:
            wavelength = np.array([w])

      t = cspad_tbx.evt_time(evt)
      if t is None:
        print("No timestamp, skipping shot")
        continue
      if 'timestamp' in locals():
        timestamp += t[0] + (t[1]/1000)
      else:
        timestamp = np.array([t[0] + (t[1]/1000)])

      if 'sum' in locals():
        sum+=data
      else:
        sum=np.array(data, copy=True)
      if 'sumsq' in locals():
        sumsq+=data*data
      else:
        sumsq=data*data
      if 'maximum' in locals():
        maximum=np.maximum(maximum,data)
      else:
        maximum=np.array(data, copy=True)

      if command_line.options.do_minimum_projection:
        if 'minimum' in locals():
          minimum=np.minimum(minimum,data)
        else:
          minimum=np.array(data, copy=True)

      nevent += 1

  #sum the images across mpi cores
  if size > 1:
    print("Synchronizing rank", rank)
  totevent = np.zeros(nevent.shape)
  comm.Reduce(nevent,totevent)

  if rank == 0 and totevent[0] == 0:
    raise Sorry("No events found in the run")

  sumall = np.zeros(sum.shape).astype(sum.dtype)
  comm.Reduce(sum,sumall)

  sumsqall = np.zeros(sumsq.shape).astype(sumsq.dtype)
  comm.Reduce(sumsq,sumsqall)

  maxall = np.zeros(maximum.shape).astype(maximum.dtype)
  comm.Reduce(maximum,maxall, op=MPI.MAX)

  if command_line.options.do_minimum_projection:
    minall = np.zeros(maximum.shape).astype(minimum.dtype)
    comm.Reduce(minimum,minall, op=MPI.MIN)

  waveall = np.zeros(wavelength.shape).astype(wavelength.dtype)
  comm.Reduce(wavelength,waveall)

  distall = np.zeros(distance.shape).astype(distance.dtype)
  comm.Reduce(distance,distall)

  timeall = np.zeros(timestamp.shape).astype(timestamp.dtype)
  comm.Reduce(timestamp,timeall)

  if rank==0:
    if size > 1:
      print("Synchronized")

    # Accumulating floating-point numbers introduces errors,
    # which may cause negative variances.  Since a two-pass
    # approach is unacceptable, the standard deviation is
    # clamped at zero.
    mean = sumall / float(totevent[0])
    variance = (sumsqall / float(totevent[0])) - (mean**2)
    variance[variance < 0] = 0
    stddev = np.sqrt(variance)

    wavelength = waveall[0] / totevent[0]
    distance = distall[0] / totevent[0]
    pixel_size = cspad_tbx.pixel_size
    saturated_value = cspad_tbx.cspad_saturated_value
    timestamp = timeall[0] / totevent[0]
    timestamp = (int(timestamp), timestamp % int(timestamp) * 1000)
    timestamp = cspad_tbx.evt_timestamp(timestamp)


    if command_line.options.as_pickle:
      extension = ".pickle"
    else:
      extension = ".cbf"

    dest_paths = [cspad_tbx.pathsubst(command_line.options.averagepath + extension, evt, ds.env()),
                  cspad_tbx.pathsubst(command_line.options.stddevpath  + extension, evt, ds.env()),
                  cspad_tbx.pathsubst(command_line.options.maxpath     + extension, evt, ds.env())]
    if command_line.options.do_minimum_projection:
      dest_paths.append(cspad_tbx.pathsubst(command_line.options.minpath + extension, evt, ds.env()))

    dest_paths = [os.path.join(command_line.options.outputdir, path) for path in dest_paths]
    if 'Rayonix' in command_line.options.address:
      all_data = [mean, stddev, maxall]
      if command_line.options.do_minimum_projection:
        all_data.append(minall)
      from xfel.cxi.cspad_ana import rayonix_tbx
      pixel_size = rayonix_tbx.get_rayonix_pixel_size(command_line.options.bin_size)
      beam_center = [command_line.options.override_beam_x,command_line.options.override_beam_y]
      active_areas = flex.int([0,0,mean.shape[1],mean.shape[0]])
      split_address = cspad_tbx.address_split(address)
      old_style_address = split_address[0] + "-" + split_address[1] + "|" + split_address[2] + "-" + split_address[3]
      for data, path in zip(all_data, dest_paths):
        print("Saving", path)
        d = cspad_tbx.dpack(
            active_areas=active_areas,
            address=old_style_address,
            beam_center_x=pixel_size * beam_center[0],
            beam_center_y=pixel_size * beam_center[1],
            data=flex.double(data),
            distance=distance,
            pixel_size=pixel_size,
            saturated_value=rayonix_tbx.rayonix_saturated_value,
            timestamp=timestamp,
            wavelength=wavelength)
        easy_pickle.dump(path, d)
    elif 'FeeHxSpectrometer' in command_line.options.address or 'XrayTransportDiagnostic' in command_line.options.address:
      all_data = [mean, stddev, maxall]
      split_address = cspad_tbx.address_split(address)
      old_style_address = split_address[0] + "-" + split_address[1] + "|" + split_address[2] + "-" + split_address[3]
      if command_line.options.do_minimum_projection:
        all_data.append(minall)
      for data, path in zip(all_data, dest_paths):
        d = cspad_tbx.dpack(
          address = old_style_address,
          data = flex.double(data),
          distance = distance,
          pixel_size = 0.1,
          timestamp=timestamp,
          wavelength=wavelength
        )
        print("Saving", path)
        easy_pickle.dump(path, d)
    elif command_line.options.as_pickle:
      split_address = cspad_tbx.address_split(address)
      old_style_address = split_address[0] + "-" + split_address[1] + "|" + split_address[2] + "-" + split_address[3]

      xpp = 'xpp' in address.lower()
      if xpp:
        evt_time = cspad_tbx.evt_time(evt) # tuple of seconds, milliseconds
        timestamp = cspad_tbx.evt_timestamp(evt_time) # human readable format
        from iotbx.detectors.cspad_detector_formats import detector_format_version, reverse_timestamp
        from xfel.cxi.cspad_ana.cspad_tbx import xpp_active_areas
        version_lookup = detector_format_version(old_style_address, reverse_timestamp(timestamp)[0])
        assert version_lookup is not None
        active_areas = xpp_active_areas[version_lookup]['active_areas']
        beam_center = [1765 // 2, 1765 // 2]
      else:
        if command_line.options.pickle_calib_dir is not None:
          metro_path = command_line.options.pickle_calib_dir
        elif command_line.options.pickle_optical_metrology:
          from xfel.cftbx.detector.cspad_cbf_tbx import get_calib_file_path
          metro_path = get_calib_file_path(run.env(), address, run)
        else:
          metro_path = libtbx.env.find_in_repositories("xfel/metrology/CSPad/run4/CxiDs1.0_Cspad.0")
        sections = parse_calib.calib2sections(metro_path)
        beam_center, active_areas = cspad_tbx.cbcaa(
          cspad_tbx.getConfig(address, ds.env()), sections)

      class fake_quad(object):
        def __init__(self, q, d):
          self.q = q
          self.d = d

        def quad(self):
          return self.q

        def data(self):
          return self.d

      if xpp:
        quads = [fake_quad(i, mean[i*8:(i+1)*8,:,:]) for i in range(4)]
        mean = cspad_tbx.image_xpp(old_style_address, None, ds.env(), active_areas, quads = quads)
        mean = flex.double(mean.astype(np.float64))

        quads = [fake_quad(i, stddev[i*8:(i+1)*8,:,:]) for i in range(4)]
        stddev = cspad_tbx.image_xpp(old_style_address, None, ds.env(), active_areas, quads = quads)
        stddev = flex.double(stddev.astype(np.float64))

        quads = [fake_quad(i, maxall[i*8:(i+1)*8,:,:]) for i in range(4)]
        maxall = cspad_tbx.image_xpp(old_style_address, None, ds.env(), active_areas, quads = quads)
        maxall = flex.double(maxall.astype(np.float64))

        if command_line.options.do_minimum_projection:
          quads = [fake_quad(i, minall[i*8:(i+1)*8,:,:]) for i in range(4)]
          minall = cspad_tbx.image_xpp(old_style_address, None, ds.env(), active_areas, quads = quads)
          minall = flex.double(minall.astype(np.float64))
      else:
        quads = [fake_quad(i, mean[i*8:(i+1)*8,:,:]) for i in range(4)]
        mean = cspad_tbx.CsPadDetector(
          address, evt, ds.env(), sections, quads=quads)
        mean = flex.double(mean.astype(np.float64))

        quads = [fake_quad(i, stddev[i*8:(i+1)*8,:,:]) for i in range(4)]
        stddev = cspad_tbx.CsPadDetector(
          address, evt, ds.env(), sections, quads=quads)
        stddev = flex.double(stddev.astype(np.float64))

        quads = [fake_quad(i, maxall[i*8:(i+1)*8,:,:]) for i in range(4)]
        maxall = cspad_tbx.CsPadDetector(
          address, evt, ds.env(), sections, quads=quads)
        maxall = flex.double(maxall.astype(np.float64))

        if command_line.options.do_minimum_projection:
          quads = [fake_quad(i, minall[i*8:(i+1)*8,:,:]) for i in range(4)]
          minall = cspad_tbx.CsPadDetector(
            address, evt, ds.env(), sections, quads=quads)
          minall = flex.double(minall.astype(np.float64))

      all_data = [mean, stddev, maxall]
      if command_line.options.do_minimum_projection:
        all_data.append(minall)

      for data, path in zip(all_data, dest_paths):
        print("Saving", path)

        d = cspad_tbx.dpack(
          active_areas=active_areas,
          address=old_style_address,
          beam_center_x=pixel_size * beam_center[0],
          beam_center_y=pixel_size * beam_center[1],
          data=data,
          distance=distance,
          pixel_size=pixel_size,
          saturated_value=saturated_value,
          timestamp=timestamp,
          wavelength=wavelength)

        easy_pickle.dump(path, d)
    else:
      # load a header only cspad cbf from the slac metrology
      from xfel.cftbx.detector import cspad_cbf_tbx
      import pycbf
      base_dxtbx = cspad_cbf_tbx.env_dxtbx_from_slac_metrology(run, address)
      if base_dxtbx is None:
        raise Sorry("Couldn't load calibration file for run %d"%run.run())

      all_data = [mean, stddev, maxall]
      if command_line.options.do_minimum_projection:
        all_data.append(minall)

      for data, path in zip(all_data, dest_paths):
        print("Saving", path)
        cspad_img = cspad_cbf_tbx.format_object_from_data(base_dxtbx, data, distance, wavelength, timestamp, address, round_to_int=False)
        cspad_img._cbf_handle.write_widefile(path.encode(), pycbf.CBF,\
          pycbf.MIME_HEADERS|pycbf.MSG_DIGEST|pycbf.PAD_4K, 0)


if (__name__ == "__main__"):
  sys.exit(average(sys.argv[1:]))


 *******************************************************************************


 *******************************************************************************
xfel/command_line/or_mask.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.or_mask
from libtbx import easy_pickle
from scitbx.array_family import flex
import sys,os
files = sys.argv[1:]
srcs = files[:-1]
dest = files[-1]
print(srcs,">",dest)

data1= easy_pickle.load(srcs[0])
ddata = data1["DATA"]
try:
  idata1 = ddata.iround()
except AttributeError:
  idata1 = ddata.as_double().iround()
discover_mask_pix_val = flex.sum(idata1.as_long())//(idata1!=0).count(True)
print("I think the mask pixel value is", discover_mask_pix_val)
bdata1 = (idata1!=0)
for item in srcs[1:]:
  dataN= easy_pickle.load(item)
  try:
    bdata1 |= (dataN["DATA"].iround()!=0)
  except AttributeError:
    bdata1 |= (dataN["DATA"].as_double().iround()!=0)
dirname = os.path.dirname(dest)
if dirname != "" and not os.path.isdir(dirname):
  os.makedirs(dirname)
debug_fixer = flex.bool(list(bdata1)).as_int().as_double()*discover_mask_pix_val
debug_fixer.reshape(flex.grid(ddata.focus()))
data1["DATA"]=debug_fixer
easy_pickle.dump(dest,data1)


 *******************************************************************************


 *******************************************************************************
xfel/command_line/overlay_spectra.py
from __future__ import absolute_import, division, print_function
import sys

from matplotlib import pyplot

import iotbx.phil
from scitbx.array_family import flex

from scitbx import smoothing
from xfel.command_line import smooth_spectrum
from six.moves import zip

master_phil_str = """\
x_offsets = None
  .type = floats
legend = None
  .type = strings
  .help = Labels for each plot.
bg_range = 100, 200
  .type = floats(size=2)
  .help = Range in pixels to align background of spectra.
plot_range = None
  .type = floats(size=2)
  .help = Range in pixels to plot
smoothing {
  method = savitzky_golay fourier_filter
    .type = choice
  savitzky_golay {
    half_window = 16
      .type = int
      .help = The number of values either side of a data point to use.
    degree = 4
      .type = int
      .help = The degree of polynomial to fit to data points.
  }
  fourier_filter_cutoff = None
    .type = int
    .help = Cutoff frequency for Fourier filtering.
}
"""

def run(args):
  master_phil = iotbx.phil.parse(master_phil_str)
  processed = iotbx.phil.process_command_line(
    args=args, master_string=master_phil_str)
  args = processed.remaining_args
  work_params = processed.work.extract()

  x_offsets = work_params.x_offsets
  bg_range_min, bg_range_max = work_params.bg_range
  if work_params.plot_range is not None:
    x_min, x_max = work_params.plot_range
  else:
    x_min, x_max = (0, 385)

  print(bg_range_min, bg_range_max)
  if x_offsets is None:
    x_offsets = [0]*len(args)
  legend = work_params.legend
  linewidth = 2
  fontsize = 26
  xy_pairs = []
  colours = ["cornflowerblue", "darkmagenta", "darkgreen", "black", "red", "blue", "pink"]
  colours[2] = "orangered"
  colours[1] = "olivedrab"
  min_background = 1e16
  #x_min, x_max = (0, 391)
  #x_min, x_max = (0, 360)
  #x_min, x_max = (200, 360)

  for i, filename in enumerate(args):
    print(filename)
    f = open(filename, 'rb')
    x, y = zip(*[line.split() for line in f.readlines() if not line.startswith("#")])
    x = flex.double(flex.std_string(x))
    y = flex.double(flex.std_string(y))

    if work_params.smoothing.method is not None:
      savitzky_golay_half_window = work_params.smoothing.savitzky_golay.half_window
      savitzky_golay_degree = work_params.smoothing.savitzky_golay.degree
      fourier_cutoff = work_params.smoothing.fourier_filter_cutoff

      method = work_params.smoothing.method
      if method == "fourier_filter":
        assert work_params.smoothing.fourier_filter_cutoff is not None

      if method == "savitzky_golay":
        x, y = smoothing.savitzky_golay_filter(
          x, y, savitzky_golay_half_window, savitzky_golay_degree)

      elif method == "fourier_filter":
        x, y = smooth_spectrum.fourier_filter(x, y, cutoff_frequency=fourier_cutoff)


    x += x_offsets[i]
    y = y.select((x <= x_max) & (x > 0))
    x = x.select((x <= x_max) & (x > 0))
    bg_sel = (x > bg_range_min) & (x < bg_range_max)
    xy_pairs.append((x,y))
    min_background = min(min_background, flex.mean(y.select(bg_sel))/flex.max(y))
    y -= min_background
    print("Peak maximum at: %i" %int(x[flex.max_index(y)]))
  for i, filename in enumerate(args):
    if legend is None:
      label = filename
    else:
      print(legend)
      assert len(legend) == len(args)
      label = legend[i]
    x, y = xy_pairs[i]
    if i == -1:
      x, y = interpolate(x, y)
      x, y = savitzky_golay_filter(x, y)
    #if i == 0:
      #y -= 10
    bg_sel = (x > bg_range_min) & (x < bg_range_max)
    y -= (flex.mean(y.select(bg_sel)) - min_background*flex.max(y))
    #y -= flex.min(y)
    y_min = flex.min(y.select(bg_sel))
    if i == -2:
      y += 0.2 * flex.max(y)
    print("minimum at: %i" %int(x[flex.min_index(y)]), flex.min(y))
    #print "fwhm: %.2f" %full_width_half_max(x, y)
    y /= flex.max(y)
    if len(colours) > i:
      pyplot.plot(x, y, label=label, linewidth=linewidth, color=colours[i])
    else:
      pyplot.plot(x, y, label=label, linewidth=linewidth)
  pyplot.ylabel("Intensity", fontsize=fontsize)
  pyplot.xlabel("Pixel column", fontsize=fontsize)
  if i > 0:
    # For some reason the line below causes a floating point error if we only
    # have one plot (i.e. i==0)
    legend = pyplot.legend(loc=2)
    for t in legend.get_texts():
      t.set_fontsize(fontsize)
  axes = pyplot.axes()
  for tick in axes.xaxis.get_ticklabels():
    tick.set_fontsize(20)
  for tick in axes.yaxis.get_ticklabels():
    tick.set_fontsize(20)
  pyplot.ylim(0,1)
  pyplot.xlim(x_min, x_max)
  ax = pyplot.axes()
  #ax.xaxis.set_minor_locator(pyplot.MultipleLocator(5))
  #ax.yaxis.set_major_locator(pyplot.MultipleLocator(0.1))
  #ax.yaxis.set_minor_locator(pyplot.MultipleLocator(0.05))
  pyplot.show()

def full_width_half_max(x, y):
  y = y/flex.max(y)
  perm = flex.sort_permutation(x)
  y = y.select(perm)
  x = x.select(perm)
  x_lower = None
  x_upper = None
  for x_i, y_i in zip(x, y):
    if x_lower is None:
      if y_i >= 0.5:
        x_lower = x_i
    elif x_upper is None:
      if y_i <= 0.5:
        x_upper = x_i
    else:
      break
  return (x_upper - x_lower)


def estimate_signal_to_noise(x, y):
  raise
  if 1:
    x, y = interpolate(x, y)
    #x, y_tr = fourier_filter(x, y)
    x, y_tr = savitzky_golay_filter(x, y)
    noise = y - y_tr
  else:

    from scitbx.math import chebyshev_polynome
    from scitbx.math import chebyshev_lsq_fit

    x_obs, y_obs = x, y
    w_obs = flex.double(y_obs.size(), 1)
    w_obs[0] = 1e16
    w_obs[-1] = 1e16
    ## determining the number of terms takes much, much longer than the fit
    n_terms = chebyshev_lsq_fit.cross_validate_to_determine_number_of_terms(
      x_obs, y_obs, w_obs,
      min_terms=2, max_terms=30,
      n_goes=20, n_free=20)
    #n_terms = 7
    print("n_terms:", n_terms)
    fit = chebyshev_lsq_fit.chebyshev_lsq_fit(n_terms, x_obs, y_obs, w_obs)
    fit_funct = chebyshev_polynome(
      n_terms, fit.low_limit, fit.high_limit, fit.coefs)
    y_fitted = fit_funct.f(x)
    y_tr = y_fitted
    n = y_tr.size()
    noise = y - y_tr


  noise_sq = flex.pow2(noise)
  from xfel.command_line.view_pixel_histograms import sliding_average
  #sigma_sq = sliding_average(noise_sq, n=31)
  sigma_sq = sliding_average(noise_sq, n=15)
  #sigma_sq = sliding_average(sigma_sq)
  #signal_to_noise = y/flex.sqrt(sigma_sq)
  import math
  signal_to_noise = y/math.sqrt(flex.mean(noise_sq[50:200]))
  #pyplot.plot(noise)
  #pyplot.plot(x,y)
  #pyplot.show()
  offset = 0.2 * flex.max(y)
  offset = 0
  pyplot.plot(x, y, linewidth=2)
  pyplot.plot(x, offset+y_tr, linewidth=2)
  pyplot.show()
  pyplot.plot(x, noise, linewidth=2)
  #pyplot.plot(x, flex.sqrt(sigma_sq), linewidth=2)
  #ax2 = pyplot.twinx()
  #ax2.plot(x, y)
  pyplot.show()
  pyplot.plot(x[:375], signal_to_noise[:375])
  #pyplot.xlim(
  #ax2 = pyplot.twinx()
  #ax2.plot(x, y)
  pyplot.show()

if __name__ == '__main__':
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/plot_to_subpixel.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# LIBTBX_SET_DISPATCHER_NAME cxi.plotcv_parse

import sys
from scitbx.array_family import flex

def run (input=None) :
  if (input is None) :
    input = sys.__stdin__
  tile = flex.int()
  delx = flex.double()
  dely = flex.double()
  for line in input.readlines():
    if line.find("Tile")==0:
      tokens = line.split()
      #print tokens[1], int(tokens[1][:-1]),tokens
      tile.append( int(tokens[1][:-1]) )
      delx.append( float(tokens[7]) )
      dely.append( float(tokens[9][:-1]) )
      if len(tile)==64: break

  order = flex.sort_permutation(tile)
  for x in range(64):
    #print tile[order[x]], delx[order[x]], dely[order[x]]
    pass

  for x in range(8):
    for y in range(8):
      print("%5.2f,"%delx[order[x*8+y]], "%5.2f,"%dely[order[x*8+y]], end=' ')
    print()
  print()

  for x in range(8):
    for y in range(8):
      print("%5.2f,"%dely[order[x*8+y]], "%5.2f,"%delx[order[x*8+y]], end=' ')
    print()
  print()

  for x in range(8):
    for y in range(8):
      print("%5.2f,"%-delx[order[x*8+y]], "%5.2f,"%-dely[order[x*8+y]], end=' ')
    print()
  print()

  for x in range(8):
    for y in range(8):
      print("%5.2f,"%-dely[order[x*8+y]], "%5.2f,"%-delx[order[x*8+y]], end=' ')
    print()
  print()

if (__name__ == "__main__") :
  run(sys.__stdin__)


 *******************************************************************************


 *******************************************************************************
xfel/command_line/postrefine.py
# LIBTBX_SET_DISPATCHER_NAME cxi.postrefine
from __future__ import absolute_import, division, print_function
import os, sys
from libtbx.easy_mp import pool_map
import numpy as np
from cctbx.array_family import flex
from datetime import datetime, time
import logging
from six.moves import range

FORMAT = '%(levelname)s %(module)s.%(funcName)s: %(message)s'
logging.basicConfig(level=logging.WARNING, format=FORMAT)

def determine_mean_I_mproc(frame_no, frame_files, iph):
  from xfel.cxi.postrefine import postref_handler
  prh = postref_handler()
  mean_I = prh.calc_mean_intensity(frame_files[frame_no], iph)
  return mean_I

def scale_frame_by_mean_I_mproc(frame_no, frame_files, iph, mean_of_mean_I):
  from xfel.cxi.postrefine import postref_handler
  prh = postref_handler()
  pres = prh.scale_frame_by_mean_I(frame_no,frame_files[frame_no], iph, mean_of_mean_I)
  return pres

def postrefine_by_frame_mproc(frame_no, frame_files, iph, miller_array_ref):
  from xfel.cxi.postrefine import postref_handler
  prh = postref_handler()
  pres = prh.postrefine_by_frame(frame_no, frame_files[frame_no], iph, miller_array_ref)
  return pres

def read_input(args):
  file_name_input = ''
  frame_files = []
  for i in range(len(args)):
    pair=args[i].split('=')
    if pair[0]=='input':
      file_name_input = pair[1]

    #other args are considered as the pickle directory
    if len(pair) == 1:
      if pair[0].endswith('.pickle'):
        frame_files.append(pair[0])

  if file_name_input == '':
    print("Please provide input-parameters file (usage: input=yourinput.inp)")
    exit()

  from xfel.cxi.postrefine import postref_handler
  prh = postref_handler()
  iph = prh.read_input_parameters(file_name_input)

  #make run_no folder
  if not os.path.exists(iph.run_no):
    os.makedirs(iph.run_no)

  #check if pickle_dir is given in input file instead of from cmd arguments.
  if len(frame_files)==0:
    print('Path to pickle files is missing, please specify it at command line.')
    print('Usage: cxi.postrefine input=myinp.inp /path/to/pickles/*')
    exit()

  return iph, frame_files


if (__name__ == "__main__"):
  logging.info("Starting process.")
  #capture starting time
  time_global_start=datetime.now()

  #0 .read input parameters and frames (pickle files)
  iph, frame_files = read_input(args = sys.argv[1:])
  frames = list(range(len(frame_files)))

  #1. prepare reference miller array
  if iph.file_name_ref_mtz == '':
    #if iso. ref. is not given, use the <I> to scale each frame.

    #calculate mean intensity for each frame and determine median of the
    #distribution
    def determine_mean_I_mproc_wrapper(arg):
      return determine_mean_I_mproc(arg, frame_files, iph)

    determine_mean_I_result = pool_map(
            args=frames,
            func=determine_mean_I_mproc_wrapper,
            processes=None)

    frames_mean_I = flex.double()
    for result in determine_mean_I_result:
      if result is not None:
        frames_mean_I.append(result)

    mean_of_mean_I = np.median(frames_mean_I)

    #use the calculate <mean_I> to scale each frame
    def scale_frame_by_mean_I_mproc_wrapper(arg):
      return scale_frame_by_mean_I_mproc(arg, frame_files, iph, mean_of_mean_I)

    scale_frame_by_mean_I_result = pool_map(
            args=frames,
            func=scale_frame_by_mean_I_mproc_wrapper,
            processes=None)

    observations_merge_mean_set = []
    for pres in scale_frame_by_mean_I_result:
      if pres is not None:
        observations_merge_mean_set.append(pres)

    if len(observations_merge_mean_set) > 0:
      from xfel.cxi.postrefine import merge_observations
      miller_array_merge_mean, txt_merge_mean, csv_merge_mean = merge_observations(observations_merge_mean_set, iph, iph.run_no+'/mean_scaled','average')
      miller_array_ref = miller_array_merge_mean.expand_to_p1().generate_bijvoet_mates()
      if iph.flag_force_no_postrefine:
        txt_out = iph.txt_out + txt_merge_mean
        f = open(iph.run_no+'/log.txt', 'w')
        f.write(txt_out)
        f.close()
        exit()
    else:
      print("No frames merged as a reference set - exit without post-refinement")
      exit()
  else:
    miller_array_ref = iph.miller_array_iso
    txt_merge_mean = ''

  #2. Post-refinement
  n_iters = iph.n_postref_cycle
  txt_merge_postref = ''
  for i in range(n_iters):
    txt_merge_postref += 'Start post-refinement cycle '+str(i+1)+'\n'
    print(txt_merge_postref)
    def postrefine_by_frame_mproc_wrapper(arg):
      return postrefine_by_frame_mproc(arg, frame_files, iph, miller_array_ref)

    postrefine_by_frame_result = pool_map(
            args=frames,
            func=postrefine_by_frame_mproc_wrapper,
            processes=None)

    postrefine_by_frame_good = []
    for pres in postrefine_by_frame_result:
      if pres is not None:
        postrefine_by_frame_good.append(pres)

    if len(postrefine_by_frame_good) > 0:
      from xfel.cxi.postrefine import merge_observations
      miller_array_merge_postref, txt_merge_out, csv_merge_PR = merge_observations(postrefine_by_frame_good, iph, iph.run_no+'/postref_cycle_'+str(i+1),'weighted')
      miller_array_ref = miller_array_merge_postref.generate_bijvoet_mates()
      txt_merge_postref += txt_merge_out
    else:
      print("No frames merged as a reference set - exit without post-refinement")
      exit()

  #collect caculating time
  time_global_end=datetime.now()
  time_global_spent=time_global_end-time_global_start
  txt_out_time_spent = 'Total calculation time: '+'{0:.2f}'.format(time_global_spent.seconds)+' seconds\n'
  print(txt_out_time_spent)

  txt_out = iph.txt_out + txt_merge_mean + txt_merge_postref + txt_out_time_spent
  f = open(iph.run_no+'/log.txt', 'w')
  f.write(txt_out)
  f.close()

  with open("{}/mean_stats.csv".format(iph.run_no), 'wb') as mean_stats:
    mean_stats.write(csv_merge_mean)

  with open("{}/PostRef_stats.csv".format(iph.run_no), 'wb') as PR_stats:
    PR_stats.write(csv_merge_PR)



 *******************************************************************************


 *******************************************************************************
xfel/command_line/preference.py
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.preference

from __future__ import division
import sys
from xfel.util.preference import params_from_phil, phil_scope, run, message


if __name__ == '__main__':
  if '--help' in sys.argv[1:] or '-h' in sys.argv[1:]:
    print(message)
    exit()
  params = params_from_phil(phil_scope, sys.argv[1:])
  run(params)


 *******************************************************************************


 *******************************************************************************
xfel/command_line/print_pickle.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from six.moves import zip
#-*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.print_pickle
#

"""
Simple utility for printing the contents of a cctbx.xfel pickle file
"""

import sys, os, six
from iotbx.detectors.cspad_detector_formats import detector_format_version as detector_format_function
from iotbx.detectors.cspad_detector_formats import reverse_timestamp
from cctbx import sgtbx # import dependency
from cctbx.array_family import flex

def keywise_printout(data):
  for key in data:
    if key == 'ACTIVE_AREAS':
      print(int(len(data[key])/4), "active areas, first one: ", list(data[key][0:4]))
    elif key == 'observations':
      print(key, data[key], "Showing unit cell/spacegroup:")
      obs = data[key][0]
      uc = obs.unit_cell()
      uc.show_parameters()
      obs.space_group().info().show_summary()
      d = uc.d(obs.indices())
      print("Number of observations:", len(obs.indices()))
      print("Max resolution: %f"%flex.min(d))
      print("Mean I/sigma:", flex.mean(obs.data())/flex.mean(obs.sigmas()))
      print("I/sigma > 1 count:", (obs.data()/obs.sigmas() > 1).count(True))
      print("I <= 0:", len(obs.data().select(obs.data() <= 0)))

      from cctbx.crystal import symmetry
      sym = symmetry(unit_cell = uc, space_group = obs.space_group())
      mset = sym.miller_set(indices = obs.indices(), anomalous_flag=False)
      binner = mset.setup_binner(n_bins=20)
      acceptable_resolution_bins = []
      binned_avg_i_sigi = []
      for i in binner.range_used():
        d_max, d_min = binner.bin_d_range(i)
        sel = (d <= d_max) & (d > d_min)
        sel &= obs.data() > 0
        intensities = obs.data().select(sel)
        sigmas = obs.sigmas().select(sel)
        n_refls = len(intensities)
        avg_i = flex.mean(intensities) if n_refls > 0 else 0
        avg_i_sigi = flex.mean(intensities / sigmas) if n_refls > 0 else 0
        acceptable_resolution_bins.append(avg_i_sigi >= 1.0)

      acceptable_resolution_bins = [acceptable_resolution_bins[i] if False not in acceptable_resolution_bins[:i+1] else False
                                    for i in range(len(acceptable_resolution_bins))]
      best_res = None
      for i, ok in zip(binner.range_used(), acceptable_resolution_bins):
        d_max, d_min = binner.bin_d_range(i)
        if ok:
          best_res = d_min
        else:
          break
      if best_res is None:
        print("Highest resolution with I/sigI >= 1.0: None")
      else:
        print("Highest resolution with I/sigI >= 1.0: %f"%d_min)

    elif key == 'mapped_predictions':
      print(key, data[key][0][0], "(only first shown of %d)"%len(data[key][0]))
    elif key == 'correction_vectors' and data[key] is not None and data[key][0] is not None:
      if data[key][0] is None:
        print(key, "None")
      else:
        print(key, data[key][0][0], "(only first shown)")
    elif key == "DATA":
      print(key,"len=%d max=%f min=%f dimensions=%s"%(data[key].size(),flex.max(data[key]),flex.min(data[key]),str(data[key].focus())))
    elif key == "WAVELENGTH":
      print("WAVELENGTH", data[key], ", converted to eV:", 12398.4187/data[key])
    elif key == "fuller_kapton_absorption_correction":
      print(key, data[key])
      if doplots:
        c = data[key][0]
        hist = flex.histogram(c, n_slots=30)
        from matplotlib import pyplot as plt
        plt.scatter(hist.slot_centers(), hist.slots())
        plt.show()

        obs = data['observations'][0]
        preds = data['mapped_predictions'][0]
        p1 = preds.select(c == 1.0)
        p2 = preds.select((c != 1.0) & (c <= 1.5))
        plt.scatter(preds.parts()[1], preds.parts()[0], c='g')
        plt.scatter(p1.parts()[1], p1.parts()[0], c='b')
        plt.scatter(p2.parts()[1], p2.parts()[0], c='r')
        plt.show()

    else:
      print(key, data[key])

def generate_streams_from_path(tar_or_other):
    from tarfile import ReadError
    import tarfile
    try:
      T = tarfile.open(name=tar_or_other, mode='r')
      K = T.getmembers()
      NT = len(K)
      for nt in range(NT):
        k = os.path.basename(K[nt].path)
        fileIO = T.extractfile(member=K[nt])
        yield fileIO,K[nt].path
    except ReadError as e:
      fileIO = open(tar_or_other,'rb')
      yield fileIO,tar_or_other

def generate_data_from_streams(args, verbose=False):
  from six.moves import cPickle as pickle
  for path in args:
    if not os.path.isfile(path):
      if verbose: print("Not a file:", path)
      continue

    # interpret the object as a tar of pickles, a pickle, or none of the above
    for fileIO,path in generate_streams_from_path(path):
      try:
        if six.PY3:
          from iotbx.detectors.npy import image_dict_to_unicode
          data = image_dict_to_unicode(pickle.load(fileIO, encoding='bytes'))
        else:
          data = pickle.load(fileIO)
        if not isinstance(data, dict):
          if verbose: print("\nNot a dictionary pickle",path)
          continue
        else:
          if verbose: print("\nPrinting contents of", path)
          data["path"] = path
          yield data, path

      except pickle.UnpicklingError as e:
        if verbose: print("\ndoesn't unpickle",path)
      except EOFError as e:
        if verbose: print("\nEOF error",path)

if __name__=="__main__":
  args = sys.argv[1:]
  if "--break" in args:
    args.remove("--break")
    dobreak = True
  else:
    dobreak = False

  if "--plots" in args:
    args.remove("--plots")
    doplots = True
  else:
    doplots = False

  for data, path in generate_data_from_streams(args, verbose=True):
    if 'TIMESTAMP' in data:
      # this is how FormatPYunspecified guesses the address
      if not "DETECTOR_ADDRESS" in data:
        # legacy format; try to guess the address
        LCLS_detector_address = 'CxiDs1-0|Cspad-0'
        if "DISTANCE" in data and data["DISTANCE"] > 1000:
          # downstream CS-PAD detector station of CXI instrument
          LCLS_detector_address = 'CxiDsd-0|Cspad-0'
      else:
        LCLS_detector_address = data["DETECTOR_ADDRESS"]

      detector_format_version = detector_format_function(
        LCLS_detector_address, reverse_timestamp(data['TIMESTAMP'])[0])
      print("Detector format version:", detector_format_version)
      image_pickle = True
    else:
      print("Not an image pickle")
      image_pickle = False

    keywise_printout(data)

    if image_pickle:
      import dxtbx
      image = dxtbx.load(path)
      tile_manager = image.detectorbase.get_tile_manager(image.detectorbase.horizons_phil_cache)
      tiling = tile_manager.effective_tiling_as_flex_int(reapply_peripheral_margin = True)
      print(int(len(tiling)/4), "translated active areas, first one: ", list(tiling[0:4]))

    if dobreak:
      print("Entering break. The pickle is loaded in the variable 'data'")
      try:
        from IPython import embed
      except ImportError:
        import pdb; pdb.set_trace()
      else:
        embed()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/quadrants.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cspad.quadrants
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1

from xfel.cxi.display_spots import ImageFactory,cxi_phil
from xfel.cxi import display_spots
import sys,os,copy
from scitbx.array_family import flex
from libtbx.utils import Sorry

def view_raw_image(path, *command_line, **kwargs):
  args = [path,
          "viewer.powder_arcs.show=False",
          "viewer.powder_arcs.code=3n9c",
         ]

  horizons_phil = cxi_phil.cxi_versioned_extract(
                    copy.deepcopy(args),list(command_line))

  #global parameters
  display_spots.parameters.horizons_phil = horizons_phil
  return horizons_phil


if (__name__ == "__main__"):
  files = [arg for arg in sys.argv[1:] if os.path.isfile(arg)]
  arguments = [arg for arg in sys.argv[1:] if not os.path.isfile(arg)]

  for file in files:
    message="""Based on the file %s,
    this program will compute incremental quadrant translations to circularize
    powder rings on the inner four sensors.  The algorithm treats each quadrant
    independently and scores based on self-correlation upon 45-degree rotation.
    Quad-translations and per-tile unit_translations that are already defined in
    spotfinder/applications/xfel/cxi_phil.py are applied first, and increments
    are determined ON TOP OF those values.  Output is given in the form of
    a new quad_translation array."""%file
    print(message)
    phil_params = view_raw_image(file, *arguments, **({'display':True}))
    image = ImageFactory(file)
    image.show_header()

    if image.horizons_phil_cache.distl.detector_format_version != \
       phil_params.distl.detector_format_version:
         raise Sorry(
         '''it is necessary to put distl.detector_format_version="%s" on the command line.'''%(
       image.horizons_phil_cache.distl.detector_format_version))

    M = image.get_tile_manager(phil = phil_params)
    tiling = M.effective_tiling_as_flex_int(encode_inactive_as_zeroes=True)

    # somebodys stupid private cache
    #print image.horizons_phil_cache.distl.detector_format_version
    #print image.horizons_phil_cache.distl.quad_translations
    #print image.horizons_phil_cache.distl.tile_translations

    dfv = image.horizons_phil_cache.distl.detector_format_version
    if dfv is None or dfv.find("CXI")>=0:
      key_sensors =[(2,3),(18,19),(50,51),(34,35)] # UL, UR, LL, LR
    elif dfv.find("XPP")>=0:
      key_sensors =[(34,35),(50,51),(18,19),(2,3)] # UL, UR, LL, LR

    if dfv is None:
      old_quad_trans = flex.int(8)
      new_quad_trans = flex.int(8) #initialize
    else:
      old_quad_trans = flex.int(image.horizons_phil_cache.distl.quad_translations)
      new_quad_trans = old_quad_trans + flex.int(len(old_quad_trans)) #initialize

    from xfel.metrology.quadrant import one_sensor
    for isensor,sensor in enumerate(key_sensors):
      Q = one_sensor(image,sensor,M)
      print(Q.coordmax[0], Q.coordmax[1])
      new_quad_trans[isensor*2]-=Q.coordmax[0]
      new_quad_trans[isensor*2 + 1]-=Q.coordmax[1]

    print("The OLD QUAD translations are:",list(old_quad_trans))
    print("\nThe NEW QUAD translations are:",list(new_quad_trans))
    print("""These should be pasted into spotfinder/applications/xfel/cxi_phil.py
    with NEW replacing the OLD.""")


#cspad.quadrants /reg/d/psdm/cxi/cxie1414/ftc/brewster/averages/r0006/000/out/max-Ds2-r0006.pickle


 *******************************************************************************


 *******************************************************************************
xfel/command_line/quadrants_cbf.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cspad.quadrants_cbf
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1

import sys,os
from scitbx.matrix import col
from serialtbx.detector import center
import dxtbx
import libtbx.load_env
from libtbx.utils import Usage
from scitbx.array_family import flex
from libtbx.phil import parse
from libtbx.utils import Sorry

phil_scope = parse("""
  show_plots = False
    .type = bool
    .help = Show CC gridmap plots
  multi_angle = True
    .type = bool
    .help = If true, compute CC over many angles at each gridpoint (20-70 degrees \
            in increments of 2.5 degrees). Otherwise, just compute CC at 45 \
            degrees at each grid point (much faster but not as robust)
  plot_range = None
    .type = floats(size=2)
    .help = Min and max CC values for gridmap plots
  pdf_file = None
    .type = path
    .help = If not None and show_plots is True, then save CC plots as multi- \
            page cbf file
  save_cbf = True
    .type = bool
    .help = If True, write cbf with best corrections applied
""")

def run(args):
  if len(args) == 0 or '-h' in args or '--help' in args or '-c' in args:
    print("Usage: %s [-p] files"%libtbx.env.dispatcher_name)
    phil_scope.show(attributes_level = 2)
    return

  files = [arg for arg in args if os.path.isfile(arg)]
  arguments = [arg for arg in args if not os.path.isfile(arg)]
  user_phil = []
  for arg in arguments:
    if arg == '-p':
      user_phil.append(parse("show_plots=True"))
    else:
      try:
        user_phil.append(parse(arg))
      except Exception as e:
        raise Sorry("Unrecognized argument: %s"%arg)
  params = phil_scope.fetch(sources = user_phil).extract()

  for file in files:
    message="""Based on the file %s,
    this program will compute incremental quadrant translations to circularize
    powder rings on the inner four sensors.  The algorithm treats each quadrant
    independently and scores based on self-correlation upon 45-degree rotation.
    """%file
    print(message)

    image = dxtbx.load(file)
    detector = image.get_detector()
    beam = image.get_beam()

    from xfel.metrology.quadrant import one_panel
    ccs = flex.double()
    for i_quad, quad in enumerate(detector.hierarchy()):
      # find panel closest to the beam center
      panels = []
      def recursive_get_panels(group):
        if group.is_group():
          for child in group:
            recursive_get_panels(child)
        else:
          panels.append(group)
      recursive_get_panels(quad)

      smallest_dist = float("inf")

      for panel in panels:
        p_w, p_h = panel.get_image_size()
        c = center([col(panel.get_pixel_lab_coord((0    ,0  ))),
                    col(panel.get_pixel_lab_coord((p_w-1,0  ))),
                    col(panel.get_pixel_lab_coord((p_w-1,p_h-1))),
                    col(panel.get_pixel_lab_coord((0    ,p_h-1)))])
        beam_center = col(panel.get_beam_centre_lab(beam.get_s0()))

        dist = (c-beam_center).length()
        if dist < smallest_dist:
          smallest_dist = dist
          key_panel = panel

      print("Doing cross-correlation on panel", key_panel.get_name())
      Q = one_panel(image,key_panel,i_quad,quad,params.show_plots,params.multi_angle,params.plot_range,params.pdf_file is None)
      delta = panel.pixel_to_millimeter((Q.coordmax[0], Q.coordmax[1]))

      quad.set_frame(quad.get_fast_axis(),
                     quad.get_slow_axis(),
                     col(quad.get_origin())-col((delta[0],delta[1],0)))
      ccs.append(Q.ccmax)
    print("Average CC: %7.4f"%flex.mean(ccs))
    if params.pdf_file is not None:
      print("Saving plots to", params.pdf_file)
      from matplotlib import pyplot as plt
      from matplotlib.backends.backend_pdf import PdfPages
      pp = PdfPages(params.pdf_file)
      for i in plt.get_fignums():
        pp.savefig(plt.figure(i), dpi=300)
      pp.close()

    if params.save_cbf:
      import pycbf
      image.sync_detector_to_cbf()
      dest_path = os.path.splitext(file)[0]+"_cc.cbf"
      print("Saving result to", dest_path)
      image._cbf_handle.write_widefile(dest_path.encode(),pycbf.CBF,\
          pycbf.MIME_HEADERS|pycbf.MSG_DIGEST|pycbf.PAD_4K,0)

if (__name__ == "__main__"):
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/radial_average.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# LIBTBX_SET_DISPATCHER_NAME cxi.radial_average
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1

import libtbx.phil
from libtbx.utils import Usage, Sorry
import sys
import os
import math
import numpy as np

master_phil = libtbx.phil.parse("""
  file_path = None
    .type = str
  beam_x = None
    .type = float
  beam_y = None
    .type = float
  handedness = 0
    .type = int
  xfel_target = None
    .type = str
  n_bins = 0
    .type = int
  verbose = True
    .type = bool
  output_bins = True
    .type = bool
  output_file = None
    .type = str
  plot_y_max = None
    .type = int
""")
# Array of handedness possibilities.  Input 0 for no subpixel
# metrology correction
                         # id  x/y   theta    x       y
handednesses = \
 [[False, 1, 1, 1],      #  1 normal  pos     x       y
  [False, 1, 1,-1],      #  2 normal  pos     x   neg y
  [False, 1,-1, 1],      #  3 normal  pos neg x       y
  [False, 1,-1,-1],      #  4 normal  pos neg x   neg y
  [False,-1, 1, 1],      #  5 normal  neg     x       y
  [False,-1, 1,-1],      #  6 normal  neg     x   neg y
  [False,-1,-1, 1],      #  7 normal  neg neg x       y
  [False,-1,-1,-1],      #  8 normal  neg neg x   neg y
  [True , 1, 1, 1],      #  9 swapped pos     x       y
  [True , 1, 1,-1],      # 10 swapped pos     x   neg y
  [True , 1,-1, 1],      # 11 swapped pos neg x       y
  [True , 1,-1,-1],      # 12 swapped pos neg x   neg y
  [True ,-1, 1, 1],      # 13 swapped neg     x       y
  [True ,-1, 1,-1],      # 14 swapped neg     x   neg y
  [True ,-1,-1, 1],      # 15 swapped neg neg x       y
  [True ,-1,-1,-1]]      # 16 swapped neg neg x   neg y
h_swapped = 0
h_theta = 1
h_x = 2
h_y = 3

def run (args, source_data = None) :
  from dxtbx.ext import radial_average
  from scitbx.array_family import flex
  from iotbx.detectors.cspad_detector_formats import reverse_timestamp
  from iotbx.detectors.cspad_detector_formats import detector_format_version as detector_format_function
  from spotfinder.applications.xfel import cxi_phil
  from iotbx.detectors.npy import NpyImage
  import os, sys
  from iotbx.detectors.npy import NpyImage

  user_phil = []
  # TODO: replace this stuff with iotbx.phil.process_command_line_with_files
  # as soon as I can safely modify it
  for arg in args :
    if (not "=" in arg) :
      try :
        user_phil.append(libtbx.phil.parse("""file_path=%s""" % arg))
      except ValueError as e :
        raise Sorry("Unrecognized argument '%s'" % arg)
    else :
      try :
        user_phil.append(libtbx.phil.parse(arg))
      except RuntimeError as e :
        raise Sorry("Unrecognized argument '%s' (error: %s)" % (arg, str(e)))
  params = master_phil.fetch(sources=user_phil).extract()
  if params.file_path is None or not os.path.isfile(params.file_path) and source_data is None:
    master_phil.show()
    raise Usage("file_path must be defined (either file_path=XXX, or the path alone).")
  assert params.handedness is not None
  assert params.n_bins is not None
  assert params.verbose is not None
  assert params.output_bins is not None

  if source_data is None:
    from libtbx import easy_pickle
    source_data = easy_pickle.load(params.file_path)

  if params.output_file is None:
    logger = sys.stdout
  else:
    logger = open(params.output_file, 'w')
    logger.write("%s "%params.output_file)

  if not "DETECTOR_ADDRESS" in source_data:
    # legacy format; try to guess the address
    LCLS_detector_address = 'CxiDs1-0|Cspad-0'
    if "DISTANCE" in source_data and source_data["DISTANCE"] > 1000:
      # downstream CS-PAD detector station of CXI instrument
      LCLS_detector_address = 'CxiDsd-0|Cspad-0'
  else:
    LCLS_detector_address = source_data["DETECTOR_ADDRESS"]
  timesec = reverse_timestamp( source_data["TIMESTAMP"] )[0]
  version_lookup = detector_format_function(LCLS_detector_address,timesec)
  args = [
          "distl.detector_format_version=%s"%version_lookup,
          "viewer.powder_arcs.show=False",
          "viewer.powder_arcs.code=3n9c",
         ]

  horizons_phil = cxi_phil.cxi_versioned_extract(args).persist.commands

  img = NpyImage(params.file_path, source_data)
  img.readHeader(horizons_phil)
  img.translate_tiles(horizons_phil)
  if params.verbose:
    img.show_header()

  the_tiles = img.get_tile_manager(horizons_phil).effective_tiling_as_flex_int(
        reapply_peripheral_margin=False,encode_inactive_as_zeroes=True)

  if params.beam_x is None:
    params.beam_x = img.beamx / img.pixel_size
  if params.beam_y is None:
    params.beam_y = img.beamy / img.pixel_size
  if params.verbose:
    logger.write("I think the beam center is (%s,%s)\n"%(params.beam_x, params.beam_y))

  bc = (int(params.beam_x),int(params.beam_y))

  extent = int(math.ceil(max(distance((0,0),bc),
                             distance((img.size1,0),bc),
                             distance((0,img.size2),bc),
                             distance((img.size1,img.size2),bc))))

  if params.n_bins < extent:
    params.n_bins = extent

  extent_in_mm = extent * img.pixel_size
  extent_two_theta = math.atan(extent_in_mm/img.distance)*180/math.pi

  sums    = flex.double(params.n_bins) * 0
  sums_sq = flex.double(params.n_bins) * 0
  counts  = flex.int(params.n_bins) * 0
  data    = img.get_raw_data()

  if hasattr(data,"as_double"):
    data = data.as_double()

  logger.write("Average intensity: %9.3f\n"%flex.mean(data))

  if params.verbose:
    logger.write("Generating average...tile:")
    logger.flush()
  for tile in range(len(the_tiles)//4):
    if params.verbose:
      logger.write(" %d"%tile)
      logger.flush()

    x1,y1,x2,y2 = get_tile_coords(the_tiles,tile)

    radial_average(data,bc,sums,sums_sq,counts,img.pixel_size,img.distance,
                   (x1,y1),(x2,y2))

  if params.verbose:
    logger.write(" Finishing...\n")

  # average, avoiding division by zero
  results = sums.set_selected(counts <= 0, 0)
  results /= counts.set_selected(counts <= 0, 1).as_double()

  # calculte standard devations
  std_devs = [math.sqrt((sums_sq[i]-sums[i]*results[i])/counts[i])
              if counts[i] > 0 else 0 for i in range(len(sums))]

  xvals = flex.double(len(results))
  max_twotheta = float('-inf')
  max_result   = float('-inf')

  for i in range(len(results)):
    twotheta = i * extent_two_theta/params.n_bins
    xvals[i] = twotheta

    if params.output_bins and "%.3f"%results[i] != "nan":
     #logger.write("%9.3f %9.3f\n"%     (twotheta,results[i]))        #.xy  format for Rex.cell.
      logger.write("%9.3f %9.3f %9.3f\n"%(twotheta,results[i],std_devs[i])) #.xye format for GSASII
     #logger.write("%.3f %.3f %.3f\n"%(twotheta,results[i],ds[i]))  # include calculated d spacings
    if results[i] > max_result:
      max_twotheta = twotheta
      max_result = results[i]

  logger.write("Maximum 2theta for %s, TS %s: %f, value: %f\n"%(params.file_path, source_data['TIMESTAMP'], max_twotheta, max_result))

  if params.verbose:
    from pylab import scatter, show, xlabel, ylabel, ylim
    scatter(xvals,results)
    xlabel("2 theta")
    ylabel("Avg ADUs")
    if params.plot_y_max is not None:
      ylim(0, params.plot_y_max)
    show()

  return xvals, results

def get_tile_id(tiles, x, y):
    for tile in range(len(tiles)//4):
      x1,y1,x2,y2 = get_tile_coords(tiles, tile)
      if x <= x2 and x >= x1 and y <= y2 and y >= y1:
        return tile
    return -1

def get_tile_center(tiles, tile):
  x1,y1,x2,y2 = get_tile_coords(tiles, tile)

  cx = x1 + (distance((x2,y1),(x1,y1))/2)
  cy = y1 + (distance((x1,y2),(x1,y1))/2)
  return (cx, cy)

def distance (a,b): return math.sqrt((math.pow(b[0]-a[0],2)+math.pow(b[1]-a[1],2)))

def get_tile_coords(tiles, tile):
  """ returns x1, y1, x2, y2 """
  y1 = tiles[tile*4 + 0]
  x1 = tiles[tile*4 + 1]
  y2 = tiles[tile*4 + 2]
  x2 = tiles[tile*4 + 3]
  return (x1,y1,x2,y2)

from scitbx.matrix import col, sqr

def apply_sub_pixel_metrology(tile, x, y, tcx, tcy, phil,handedness=0):
  handedness -= 1
  if handedness < 0:
    return (x,y)

  if tile < 0: return None

  r = col((x, y))      # point of interest
  Ti = col((tcx,tcy))  # center of tile i

  # sub pixel translation/rotation
  if handednesses[handedness][h_swapped]:
    ti = col((phil.integration.subpixel_joint_model.translations[(tile*2)+1] * handednesses[handedness][h_y],
              phil.integration.subpixel_joint_model.translations[tile*2]     * handednesses[handedness][h_x]))
  else:
    ti = col((phil.integration.subpixel_joint_model.translations[tile*2]     * handednesses[handedness][h_x],
              phil.integration.subpixel_joint_model.translations[(tile*2)+1] * handednesses[handedness][h_y]))
  theta = phil.integration.subpixel_joint_model.rotations[tile] * (math.pi/180) * handednesses[handedness][h_theta]

  Ri = sqr((math.cos(theta),-math.sin(theta),math.sin(theta),math.cos(theta)))

  # apply sub-pixel translation to point of interest and tile center
  rp = r + ti     # p: prime
  Tip = Ti + ti

  result = (Ri*(rp-Tip))+Tip

  return (result[0], result[1])

# Debugging jiffy function to show a few tiles and verify metrology visually
def show_tiles(the_tiles, img, phil, bc, handedness=0):
  import numpy as np
  import matplotlib.pyplot as plt

  #tiles_list = (0, 1)
  #tiles_list = (2, 18)
  #tiles_list = (48, 49)
  tiles_list = (35, 48, 49, 51)
  #tiles_list = range(64)
  arraysx = np.array([])
  arraysy = np.array([])
  arraysz = np.array([])
  for tile in tiles_list:
    x1,y1,x2,y2 = get_tile_coords(the_tiles, tile)
    w = x2-x1
    h = y2-y1
    cx,cy = get_tile_center(the_tiles, tile)

    print("tile %d, x1 %d, y1 %d, x2 %d, y2 %d, w %d, h %d, cx %s, cy %s"%(tile, x1, y1, x2, y2, w, h, cx, cy))

    x = np.array([0]*(w*h))
    y = np.array([0]*(w*h))
    z = np.array([0]*(w*h))

    for j in range(h):
      for i in range(w):
        t_id = get_tile_id(the_tiles,x1+i,y1+j)
        if tile != t_id:
          print("bug! tile: %d, t_id %d, x %d, y %d"%(tile,t_id,x1+i,y1+j))
          return
        xt, yt = apply_sub_pixel_metrology(tile,x1+i,y1+j,cx,cy,phil,handedness)
        x[(j*w)+i] = xt
        y[(j*w)+i] = yt
        c = img.get_pixel_intensity((j+y1,i+x1))
        z[(j*w)+i] = c

    arraysx = np.concatenate([arraysx, x])
    arraysy = np.concatenate([arraysy, y])
    arraysz = np.concatenate([arraysz, z])

  arraysz -= min(arraysz)
  arraysz /= max(arraysz)


  plt.scatter(arraysx, arraysy, c=arraysz, s=1, marker='s', cmap="gray_r", lw=0)
  plt.gca().invert_yaxis()
  circ = plt.Circle((bc[0], bc[1]), radius=377, facecolor='none', edgecolor='red')
  plt.gca().add_patch(circ)
  circ = plt.Circle((bc[0], bc[1]), radius=365, facecolor='none', edgecolor='red')
  plt.gca().add_patch(circ)

  plt.title("Handedness: %s"%(handedness))

  plt.show()
  return

if (__name__ == "__main__") :
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/recompute_mosaicity.py
from __future__ import absolute_import, division, print_function
from six.moves import range
#!/usr/bin/env python
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# recompute_mosaicity.py
#
#  Copyright (C) 2017 Lawrence Berkeley National Laboratory (LBNL)
#
#  Author: Aaron Brewster
#
#  This code is distributed under the X license, a copy of which is
#  included in the root directory of this package.
#
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.recompute_mosaicity
#
from dials.algorithms.indexing.nave_parameters import NaveParameters
from dials.array_family import flex
from dials.util import show_mail_on_error
import libtbx.load_env
from libtbx.phil import parse

help_message = '''
Recompute mosaic parameters for a set of experiments and apply outlier rejection.

Example:

  %s refined.expt refined.refl
''' % libtbx.env.dispatcher_name

# Create the phil parameters
phil_scope = parse("""
plot_changes = False
  .type = bool
  .help = If True, plot the change in the mosaic parameters
output {
  experiments = refined.expt
    .type = str
    .help = Name of output  experiments file
  reflections = refined.refl
    .type = str
    .help = Name of output reflections file
}
""")

from dials.command_line.stills_process import phil_scope as dsp_phil

class Script(object):
  ''' Class to parse the command line options. '''

  def __init__(self):
    ''' Set the expected options. '''
    from dials.util.options import ArgumentParser
    import libtbx.load_env

    # Create the option parser
    usage = "usage: %s refined.expt refined.refl" % libtbx.env.dispatcher_name
    self.parser = ArgumentParser(
      usage=usage,
      sort_options=True,
      phil=phil_scope,
      read_experiments=True,
      read_reflections=True,
      check_format=False,
      epilog=help_message)

  def run(self, args=None):
    """Execute the script."""
    params, options = self.parser.parse_args(args, show_diff_phil=True)
    self.run_with_preparsed(params, options)

  def run_with_preparsed(self, params, options):
    """Run recompute_mosaicity, but allow passing in of parameters"""
    from dials.util.options import flatten_experiments, flatten_reflections
    experiments = flatten_experiments(params.input.experiments)
    reflections = flatten_reflections(params.input.reflections)
    assert len(reflections) == 1
    reflections = reflections[0]

    nvparams = dsp_phil.extract()

    domain_size = flex.double()
    mosaic_angle = flex.double()
    filtered_reflections = flex.reflection_table()

    for i in range(len(experiments)):
      refls = reflections.select(reflections['id'] == i)
      tmp = refls['id']
      refls['id'] = flex.int(len(refls), 0)
      try:
        nv = NaveParameters(params = nvparams, experiments=experiments[i:i+1], reflections=refls, refinery=None, graph_verbose=False)
        crystal_model_nv = nv()[0]
      except Exception as e:
        print("Error recomputing mosaicity for experiment %d: %s"%(i, str(e)))
        continue
      domain_size.append(experiments[i].crystal.get_domain_size_ang() - crystal_model_nv.get_domain_size_ang())
      mosaic_angle.append(experiments[i].crystal.get_half_mosaicity_deg() - crystal_model_nv.get_half_mosaicity_deg())
      experiments[i].crystal = crystal_model_nv

      refls['id'] = tmp
      refls = refls.select(nv.nv_acceptance_flags)
      filtered_reflections.extend(refls)

    print("Saving new experiments as %s"%params.output.experiments)
    experiments.as_file(params.output.experiments)

    print("Removed %d out of %d reflections as outliers"%(len(reflections) - len(filtered_reflections), len(reflections)))
    print("Saving filtered reflections as %s"%params.output.experiments)
    filtered_reflections.as_pickle(params.output.reflections)

    if params.plot_changes:
      from matplotlib import pyplot as plt
      domain_size = domain_size.select((domain_size >= -10) & (domain_size <= 10))
      mosaic_angle = mosaic_angle.select((mosaic_angle >= -0.1) & (mosaic_angle <= 0.1))

      for d in [domain_size, mosaic_angle]:
        f = plt.figure()
        plt.hist(d, bins=30)
      plt.show()

if __name__ == '__main__':
  with show_mail_on_error():
    script = Script()
    script.run()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/riso.py
from __future__ import absolute_import, division, print_function
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cctbx.riso
#
# $Id: riso.py idyoung $

import iotbx.phil
from dials.util.options import ArgumentParser
from iotbx import mtz
from cctbx.array_family import flex
from libtbx.str_utils import format_value
from xfel.cxi.cxi_cc import r1_factor, scale_factor
from cctbx.crystal import symmetry
from six.moves import zip

phil_scope = iotbx.phil.parse("""
input {
  data_1 = None
    .type = path
    .help = Reference dataset for Riso calculation.
  data_2 = None
    .type = path
    .help = Comparison dataset for Riso calculation.
  labels_1 = Iobs
    .type = str
    .help = Selected column in data_1.
  labels_2 = Iobs
    .type = str
    .help = Selected column in data_2.
}
d_min = None
  .type = float
  .help = High resolution cutoff for Riso calculation.
d_max = None
  .type = float
  .help = Low resolution cutoff for Riso calculation.
anomalous_flag = False
  .type = bool
  .help = Compare anomalous datasets.
output {
  n_bins = 20
    .type = int
    .help = Number of bins for the Riso calculation.
}
""")

def riso(data_1, data_2, params, show_tables=True):
  uniform = []

  # construct a list of intensity arrays to compare between the two datasets
  for item, label in zip(
    [data_1, data_2],
    [params.input.labels_1, params.input.labels_2]):
    for array in item.as_miller_arrays():
      this_label = array.info().labels[0]
      if this_label != label: continue
      # print this_label, array.observation_type()
      uniform.append(array.as_intensity_array())
  assert len(uniform) == 2, "Could not identify the two arrays to compare. "+\
    "Please check that columns %s and %s are available in the files provided."%\
    (params.labels_1, params.labels_2)

  # if anomalous data, generate Bijvoet mates for any arrays lacking them
  if params.anomalous_flag:
    for i in (0,1):
      if not uniform[i].anomalous_flag():
        uniform[i] = uniform[i].generate_bijvoet_mates()

  # reindex
  for i in (0,1):
    uniform[i] = uniform[i].change_basis("h,k,l").map_to_asu()

  assert uniform[0].space_group_info().symbol_and_number() == \
    uniform[1].space_group_info().symbol_and_number(),\
    "Incompatible space groups between the datasets provided."

  # copy the second array with the unit cell of the first
  d_min = max(params.d_min or 0, uniform[0].d_min(), uniform[1].d_min())
  d_max = min(params.d_max or 10000, 10000)
  common_set_1 = uniform[1].customized_copy(
    crystal_symmetry = symmetry(
      unit_cell=uniform[0].unit_cell(),
      space_group_info = uniform[0].space_group_info()),
    ).resolution_filter(d_min=d_min, d_max=d_max).map_to_asu()
  common_set_2 = uniform[0].common_set(common_set_1)
  common_set_1 = uniform[1].common_set(common_set_2)
  # set 1 intentionally repeated in case of low res missing reflections
  assert len(common_set_1.indices()) == len(common_set_2.indices())
  common = (common_set_1, common_set_2)
  print("%6d indices in common in the range %.2f-%.2f Angstroms"%\
    (common_set_1.size(),d_min, d_max))

  # bin for comparison
  for array in common:
    array.setup_binner(d_min=d_min, d_max=d_max,
      n_bins=params.output.n_bins)

  # calculate scale factor and Riso
  # XXX TODO: riso_scale_factor is not set up right yet
  riso_scale_factor = scale_factor(
    common_set_2, common_set_1,
    weights=flex.pow(common_set_2.sigmas(), -2),
    use_binning=True)
  riso_binned = r1_factor(
    common_set_2, common_set_1,
    scale_factor=riso_scale_factor,
    use_binning=True)
  riso_scale_factor_all = scale_factor(
    common_set_2, common_set_1,
    weights=flex.pow(common_set_2.sigmas(), -2),
    use_binning=False)
  riso_all = r1_factor(
    common_set_2, common_set_1,
    scale_factor=riso_scale_factor_all,
    use_binning=False)

  if show_tables:
    from libtbx import table_utils
    table_header = ["","","","R"]
    table_header2 = ["Bin","Resolution Range","Completeness","iso"]
    table_data = []
    table_data.append(table_header)
    table_data.append(table_header2)

    items = riso_binned.binner.range_used()
    cumulative_counts_given = 0
    cumulative_counts_complete = 0
    for bin in items:
      table_row = []
      table_row.append("%3d"%bin)
      table_row.append("%-13s"%riso_binned.binner.bin_legend(
        i_bin=bin,show_bin_number=False,show_bin_range=False,
        show_d_range=True, show_counts=False))
      table_row.append("%13s"%riso_binned.binner.bin_legend(
        i_bin=bin,show_bin_number=False,show_bin_range=False,
        show_d_range=False, show_counts=True))
      cumulative_counts_given += riso_binned.binner._counts_given[bin]
      cumulative_counts_complete += riso_binned.binner._counts_complete[bin]
      table_row.append("%.1f%%" % (100 * riso_binned.data[bin]))
      table_data.append(table_row)

    table_row = [format_value("%3s",   "All"),
                 format_value("%-13s", "                 "),
                 format_value("%13s",  "[%d/%d]"%(cumulative_counts_given,
                                                  cumulative_counts_complete)),
                 format_value("%.1f%%", 100 * riso_all)]
    table_data.append(table_row)
    print(table_utils.format(
      table_data, has_header=2, justify='center', delim=" "))
    print("Riso is the R1 factor between the two datasets supplied.")

  return riso_binned, riso_all

def run(args):
  import os
  if ("--help" in args) or ("-h" in args) or (len(args) == 0):
    print("""cctbx.riso: a command line script for calculating an R1 factor
    between two datasets.

    Example usage:

    cctbx.riso data_1=5kaf-sf.mtz data_2=5kai-sf.mtz \\
    labels_1=Iobs labels_2=Iobs
    """)
    return
  elif ("--config" in args) or ("-c" in args):
    iotbx.phil.parse(master_phil).show(attributes_level=2)
    return
  parser = ArgumentParser(phil=phil_scope)
  params, options = parser.parse_args(show_diff_phil=True)
  # XXX TODO: find out how to auto recognize .mtz files as data_X
  extracted_data = []
  for data, label in [(params.input.data_1, params.input.labels_1),
                      (params.input.data_2, params.input.labels_2)]:
    assert data is not None, "Please supply two mtz files."
    assert os.path.exists(data), "Cannot access data: %s"%data
    extracted = mtz.object(data)
    assert extracted.has_column(label), "%s does not contain column %s"%\
      (data, label)
    extracted_data.append(extracted)
  riso(extracted_data[0], extracted_data[1], params, show_tables=True)

if __name__ == "__main__":
  import sys
  result = run(sys.argv[1:])
  if not result:
    sys.exit(1)


 *******************************************************************************


 *******************************************************************************
xfel/command_line/runs_sentinel.py
# LIBTBX_SET_DISPATCHER_NAME xpp.runs_sentinel

"""
Program to monitor runs using SLACs dataexport webservice and to log them in the
experiment managment database when they arrive. Default is to check for and log
new runs at 0.1 Hz.

Example usage:
xpp.runs_sentinel db.name=xppi6115 db.user=xppi6115 experiment=xppi6115 experiment_tag=debug web.user=<username> web.password=<password>
"""
from __future__ import absolute_import, division, print_function

import iotbx.phil
from libtbx.utils import Usage, Sorry
import sys, time

master_phil = """
  experiment = None
    .type = str
  experiment_tag = None
    .type = str
  run_tags = None
    .type = str
  db {
    host = psdb.slac.stanford.edu
      .type = str
    name = None
      .type = str
    user = None
      .type = str
    password = None
      .type = str
  }
  web {
    user = None
      .type = str
    password = None
      .type = str
  }
"""

def run(args):
  try:
    from cxi_xdr_xes.cftbx.cspad_ana import db as db
  except ImportError:
    raise Sorry("Trial logging not supported for this installation. Contact the developers for access.")

  phil = iotbx.phil.process_command_line(args=args, master_string=master_phil)
  params = phil.work.extract()

  if params.db.host is None:
    raise Usage("Please provide a host name")
  if params.db.name is None:
    raise Usage("Please provide a database name")
  if params.db.user is None:
    raise Usage("Please provide a user name")
  if params.db.password is None:
    import getpass
    password = getpass.getpass()
  else:
    password = params.db.password

  try:
    dbobj = db.dbconnect(host=params.db.host, db=params.db.name, username=params.db.user, password=password)
  except Exception as e:
    raise Sorry(e)

  from xfel.xpp.simulate import file_table
  query = "https://pswww.slac.stanford.edu/ws-auth/dataexport/placed?exp_name=%s"%(params.experiment)

  # set up extra run tags, if provided
  if params.run_tags is not None:
    extra1 = ", tags"
    extra2 = ",'%s'"%params.run_tags
  else:
    extra1 = ""
    extra2 = ""

  while True:
    # Get the set of known runs in the experiment database
    cmd = "SELECT run from %s_runs"%params.experiment_tag
    cursor = dbobj.cursor()
    cursor.execute(cmd)

    # Get the set of runs from SLAC's database
    FT = file_table(params,query)

    # Find the delta
    known_runs = [int(entry[0]) for entry in cursor.fetchall()]
    unknown_runs = [run for run in FT.rundict if run not in known_runs]

    print("%d new runs"%len(unknown_runs))

    # Enter any new runs into the experiment database
    if len(unknown_runs) > 0:
      cmd = "INSERT INTO %s_runs (run%s) VALUES "%(params.experiment_tag, extra1)
      comma = ""
      for run in unknown_runs:
        cmd += comma + "(%d%s)"%(run, extra2)
        comma = ", "

      cursor = dbobj.cursor()
      cursor.execute(cmd)
      dbobj.commit()

    time.sleep(10)

if __name__ == "__main__":
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/show_spot_separation.py
from __future__ import division
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.show_spot_separation

from xfel.util.show_spot_separation import run
import sys

if __name__ == '__main__':
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/smooth_spectrum.py
from __future__ import absolute_import, division, print_function
import math
import os
import sys

#from libtbx.option_parser import option_parser
import iotbx.phil
from scitbx.array_family import flex
from scitbx import smoothing
from six.moves import range
from six.moves import zip

def pyplot_label_axes(xlabel="Pixel column", ylabel="Intensity", fontsize=20):
  from matplotlib import pyplot
  pyplot.ylabel("Intensity", fontsize=fontsize)
  pyplot.xlabel("Pixel column", fontsize=fontsize)
  axes = pyplot.axes()
  for tick in axes.xaxis.get_ticklabels():
    tick.set_fontsize(20)
  for tick in axes.yaxis.get_ticklabels():
    tick.set_fontsize(20)


master_phil_str = """\
smoothing {
  method = *savitzky_golay fourier_filter
    .type = choice
  savitzky_golay {
    half_window = 16
      .type = int
      .help = The number of values either side of a data point to use.
    degree = 4
      .type = int
      .help = The degree of polynomial to fit to data points.
  }
  fourier_filter_cutoff = None
    .type = int
    .help = Cutoff frequency for Fourier filtering.
}
"""

master_phil = iotbx.phil.parse(master_phil_str)


def run(args):
  processed = iotbx.phil.process_command_line(
    args=args, master_string=master_phil_str)
  args = processed.remaining_args
  work_params = processed.work.extract().smoothing
  savitzky_golay_half_window = work_params.savitzky_golay.half_window
  savitzky_golay_degree = work_params.savitzky_golay.degree
  fourier_cutoff = work_params.fourier_filter_cutoff

  #assert (fourier_cutoff is not None or
          #[savitzky_golay_degree, savitzky_golay_half_window].count(None) == 0)

  method = work_params.method
  if method == "fourier_filter":
    assert work_params.fourier_filter_cutoff is not None

  for i, filename in enumerate(args):
    print(filename)
    f = open(filename, 'rb')
    x, y = zip(*[line.split() for line in f.readlines() if not line.startswith("#")])
    x = flex.double(flex.std_string(x))
    y = flex.double(flex.std_string(y))
    x = x[:-2]
    y = y[:-2]
    x_orig = x.deep_copy()
    y_orig = y.deep_copy()

    x, y = interpolate(x, y)

    if method == "savitzky_golay":
      x, y_smoothed = smoothing.savitzky_golay_filter(
        x, y, savitzky_golay_half_window, savitzky_golay_degree)

    elif method == "fourier_filter":
      x, y_smoothed = fourier_filter(x, y, cutoff_frequency=fourier_cutoff)

    from matplotlib import pyplot
    fontsize = 20
    pyplot.plot(x_orig, y_orig, color='black', linestyle='dotted', linewidth=2)
    #pyplot.plot(x_orig, y_orig, color='black', linewidth=2)
    pyplot.plot(x, y_smoothed, linewidth=2, color='red')
    pyplot_label_axes()
    pyplot.show()
    #pyplot.plot(x[:385], (y - y_smoothed)[:385], linewidth=2)
    #pyplot_label_axes()
    #pyplot.show()

    filename = os.path.join(os.path.dirname(filename), "smoothed_spectrum.txt")
    f = open(filename, "wb")
    print("\n".join(["%i %f" %(xi, yi)
                           for xi, yi in zip(x, y_smoothed)]), file=f)
    f.close()
    print("Smoothed spectrum written to %s" %filename)

    x_interp_size = x.size()
    for i, x_i in enumerate(reversed(x)):
      if x_i not in x_orig:
        assert x[x_interp_size - i - 1] == x_i
        del x[x_interp_size - i - 1]
        del y[x_interp_size - i - 1]
        del y_smoothed[x_interp_size - i - 1]

    x = x[10:-10]
    y = y[10:-10]
    y_smoothed = y_smoothed[10:-10]

    signal_to_noise = estimate_signal_to_noise(x, y, y_smoothed, plot=False)

    #pyplot.plot(x[:375], signal_to_noise[:375])
    #pyplot.show()


def interpolate(x, y, half_window=10):
  perm = flex.sort_permutation(x)
  x = x.select(perm)
  y = y.select(perm)
  x_all = flex.double()
  y_all = flex.double()
  for i in range(x.size()):
    x_all.append(x[i])
    y_all.append(y[i])
    if i < x.size()-1 and (x[i+1] - x[i]) > 1:
      window_left = min(half_window, i)
      window_right = min(half_window, x.size() - i)
      x_ = x[i-window_left:i+window_right]
      y_ = y[i-window_left:i+window_right]
      from scitbx.math import curve_fitting
      # fit a 2nd order polynomial through the missing points
      polynomial = curve_fitting.univariate_polynomial(1, 1)
      fit = curve_fitting.lbfgs_minimiser([polynomial], x_,y_).functions[0]
      missing_x = flex.double(range(int(x[i]+1), int(x[i+1])))
      x_all.extend(missing_x)
      y_all.extend(fit(missing_x))

  perm = flex.sort_permutation(x_all)
  x_all = x_all.select(perm)
  y_all = y_all.select(perm)
  return x_all, y_all


def fourier_filter(x, y, cutoff_frequency):
  assert cutoff_frequency < len(y)
  from scitbx import fftpack
  fft = fftpack.real_to_complex(len(y))
  n = fft.n_real()
  m = fft.m_real()
  y_tr = y.deep_copy()
  y_tr.extend(flex.double(m-n, 0))
  fft.forward(y_tr)
  for i in range(cutoff_frequency, m):
    y_tr[i] = 0
  fft.backward(y_tr)
  y_tr = y_tr[:n]
  scale = 1 / fft.n_real()
  y_tr *= scale
  return x, y_tr[:n]


def estimate_signal_to_noise(x, y_noisy, y_smoothed, plot=False):
  """Estimate noise in spectra by subtracting a smoothed spectrum from the
     original noisy unsmoothed spectrum.

     See:
       The extraction of signal to noise values in x-ray absorption spectroscopy
       A. J. Dent, P. C. Stephenson, and G. N. Greaves
       Rev. Sci. Instrum. 63, 856 (1992); https://doi.org/10.1063/1.1142627
  """
  noise = y_noisy - y_smoothed
  noise_sq = flex.pow2(noise)
  from xfel.command_line.view_pixel_histograms import sliding_average
  sigma_sq = sliding_average(noise_sq, n=31)
  sigma_sq = smoothing.savitzky_golay_filter(
    x.as_double(), flex.pow2(noise), half_window=20, degree=1)[1]
  sigma_sq.set_selected(sigma_sq <= 0, flex.mean(sigma_sq))
  # or do this instead to use the background region as the source of noise:
  #signal_to_noise = y_smoothed/math.sqrt(flex.mean(noise_sq[50:190]))
  signal_to_noise = y_smoothed/flex.sqrt(sigma_sq)
  #signal_to_noise.set_selected(x < 50, 0)
  #signal_to_noise.set_selected(x > 375, 0)
  if plot:
    from matplotlib import pyplot
    linewidth=2
    pyplot.plot(x, y_noisy, linewidth=linewidth)
    pyplot.plot(x, y_smoothed, linewidth=linewidth)
    pyplot_label_axes()
    pyplot.show()
    pyplot.plot(x, noise, linewidth=linewidth, label="noise")
    pyplot.plot(x, flex.sqrt(sigma_sq), linewidth=linewidth, label="sigma")
    pyplot_label_axes()
    pyplot.legend(loc=2, prop={'size':20})
    pyplot.show()
    pyplot.plot(x, signal_to_noise, linewidth=linewidth)
    pyplot_label_axes()
    pyplot.show()

  return signal_to_noise


if __name__ == '__main__':
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/stills_merge.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# LIBTBX_SET_DISPATCHER_NAME stills.merge
#
# $Id: stills_merge.py 20545 2014-08-25 22:22:15Z idyoung $

from __future__ import absolute_import, division, print_function
from six.moves import range

import iotbx.phil
from cctbx.array_family import flex
from cctbx.sgtbx.bravais_types import bravais_lattice
from libtbx.utils import Sorry
from dials.array_family import flex
from xfel.command_line import cxi_merge
from xfel.command_line import frame_extractor
from xfel.command_line.cxi_merge import OutlierCellError, WrongBravaisError
import os
import math
import sys
op = os.path

def eval_ending (file_name):
  ordered_endings_mapping = [
    ("refined_experiments.json", "integrated.pickle"),
    ("experiments.json", "integrated.pickle"),
    ("refined.expt", "integrated.refl"),
    ("indexed.expt", "integrated.refl"),
    ]
  dir_name = os.path.dirname(file_name)
  basename = os.path.basename(file_name)
  for pair in ordered_endings_mapping:
    if basename.endswith(pair[0]):
      refl_name = os.path.join(dir_name, basename.split(pair[0])[0] + pair[1])
      fragment = basename.split(pair[0])[0]
      digit = "0" # if fragment contains no digit
      for i in range(len(fragment)):
        if fragment[-i-1].isdigit():
          digit = fragment[-i-1]
          break
      return (digit, file_name, refl_name)
  return None

def get_observations (data_dirs,data_subset):
  print("Step 1.  Get a list of all files")
  file_names = []
  for dir_name in data_dirs :
    if not os.path.isdir(dir_name):
      continue
    for file_name in os.listdir(dir_name):
      if eval_ending(file_name) is not None: # only accept jsons
        if data_subset == 0 or \
          (data_subset == 1 and int(eval_ending(file_name)[0][-1]) % 2 == 1 or \
          (data_subset == 2 and int(eval_ending(file_name)[0][-1]) % 2 == 0)):
          file_names.append(os.path.join(dir_name, file_name))
  print("Number of frames found:", len(file_names))
  return file_names

cxi_merge.get_observations = get_observations

def load_result (file_name,
                 ref_bravais_type,
                 reference_cell,
                 params,
                 reindex_op,
                 out) :
  # Pull relevant information from integrated.refl and refined.expt
  # files to construct the equivalent of a single integration pickle (frame).
  try:
    frame = frame_extractor.ConstructFrameFromFiles(eval_ending(file_name)[2], eval_ending(file_name)[1]).make_frame()
  except Exception:
    return None

  # If @p file_name cannot be read, the load_result() function returns
  # @c None.

  print("Step 2.  Load frame obj and filter on lattice & cell with",reindex_op)
  """
  Take a frame with all expected contents of an integration pickle, confirm
  that it contains the appropriate data, and check the lattice type and unit
  cell against the reference settings - if rejected, raises an exception
  (for tracking statistics).
  """
  # Ignore frames with no integrated reflections.
  obj = frame
  if ("observations" not in obj) :
    return None

  if reindex_op == "h,k,l":
    pass
  else:
    obj["observations"][0].apply_change_of_basis(reindex_op)
    pass

  result_array = obj["observations"][0]
  unit_cell = result_array.unit_cell()
  sg_info = result_array.space_group_info()
  print("", file=out)
  print("-" * 80, file=out)
  print(file_name, file=out)
  print(sg_info, file=out)
  print(unit_cell, file=out)

  #Check for pixel size (at this point we are assuming we have square pixels, all experiments described in one
  #refined.expt file use the same detector, and all panels on the detector have the same pixel size)

  if params.pixel_size is not None:
    pixel_size = params.pixel_size
  elif "pixel_size" in obj:
    pixel_size = obj["pixel_size"]
  else:
    raise Sorry("Cannot find pixel size. Specify appropriate pixel size in mm for your detector in phil file.")

  #Calculate displacements based on pixel size
  assert obj['mapped_predictions'][0].size() == obj["observations"][0].size()
  mm_predictions = pixel_size*(obj['mapped_predictions'][0])
  mm_displacements = flex.vec3_double()
  cos_two_polar_angle = flex.double()
  for pred in mm_predictions:
    mm_displacements.append((pred[0]-obj["xbeam"],pred[1]-obj["ybeam"],0.0))
    cos_two_polar_angle.append( math.cos( 2. * math.atan2(pred[1]-obj["ybeam"],pred[0]-obj["xbeam"]) ) )
  obj["cos_two_polar_angle"] = cos_two_polar_angle
  #then convert to polar angle and compute polarization correction

  if (not bravais_lattice(sg_info.type().number()) == ref_bravais_type) :
    raise WrongBravaisError("Skipping cell in different Bravais type (%s)" %
      str(sg_info))
  if (not unit_cell.is_similar_to(
      other=reference_cell,
      relative_length_tolerance=params.unit_cell_length_tolerance,
      absolute_angle_tolerance=params.unit_cell_angle_tolerance)) :
    raise OutlierCellError(
      "Skipping cell with outlier dimensions (%g %g %g %g %g %g" %
      unit_cell.parameters())
  print("Integrated data:", file=out)
  result_array.show_summary(f=out, prefix="  ")
  # XXX don't force reference setting here, it will be done later, after the
  # original unit cell is recorded
  return obj

cxi_merge.load_result = load_result


if (__name__ == "__main__"):
  show_plots = False
  if ("--plots" in sys.argv) :
    sys.argv.remove("--plots")
    show_plots = True
  result = cxi_merge.run(args=sys.argv[1:])
  if result is None:
    sys.exit(1)
  if (show_plots) :
    try :
      result.plots.show_all_pyplot()
      from wxtbx.command_line import loggraph
      loggraph.run([result.loggraph_file])
    except Exception as e :
      print("Can't display plots")
      print("You should be able to view them by running this command:")
      print("  wxtbx.loggraph %s" % result.loggraph_file)
      raise e


 *******************************************************************************


 *******************************************************************************
xfel/command_line/striping.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.stripe_experiment
#
# Given an LCLS experiment results directory and a trial, group results by
# run group and then distrbute each run group's results into subgroups and run
# dials.combine_experiments (optionally with clustering and selecting clusters).
#
from dials.util import show_mail_on_error
from libtbx.phil import parse
from libtbx.utils import Sorry
from libtbx import easy_run
from xfel.util.dials_file_matcher import match_dials_files
from xfel.util.mp import mp_phil_str as multiprocessing_str
from xfel.util.mp import get_submit_command_chooser
import sys

import os, math
import six

multiprocessing_override_str = '''
mp {
  use_mpi = False
}
'''

striping_str = '''
striping {
  results_dir = None
    .type = path
    .help = "LCLS results directory containint runs starting with r."
  rungroup = None
    .type = int
    .multiple = True
    .help = "Selected rungroups to stripe. If None, all rungroups are accepted."
  run = None
    .type = str
    .multiple = True
    .help = "Selected runs to stripe. If None, all runs are accepted."
  trial = None
    .type = int
    .help = "Trial identifier for an XFEL GUI formatted processing trial."
  stripe = False
    .type = bool
    .help = "Enable to select results evenly spaced across each rungroup"
            "(stripes) as opposed to contiguous chunks."
  chunk_size = 1000
    .type = float
    .help = "Maximum number of images per chunk or stripe."
  respect_rungroup_barriers = True
    .type = bool
    .help = "Enforce separation by rungroup at time of striping (default)."
            "Turn off to allow multiple rungroups to share a detector model."
  dry_run = False
    .type = bool
    .help = "Only set up jobs but do not execute them"
  output_folder = None
    .type = path
    .help = "Path for output data. If None, use current directory"
}
'''

combining_str = '''
combine_experiments {
  clustering {
    dendrogram = False
      .type = bool
      .help = "Overrides any multiprocessing parameters to allow interactive"
      .help = "run. Clustering dendrograms can only be displayed in this mode."
    }
  keep_integrated = False
    .type = bool
    .help = "Combine refined.expt and integrated.refl files."
    .help = "If False, ignore integrated.refl files in favor of"
    .help = "indexed.refl files in preparation for reintegrating."
  include scope dials.command_line.combine_experiments.phil_scope
}
'''

combining_override_str = '''
combine_experiments {
  output {
    experiments_filename = FILENAME_combined.expt
    reflections_filename = FILENAME_combined.refl
    delete_shoeboxes = False
  }
  reference_from_experiment {
    detector = 0
  }
  clustering {
    use = True
  }
}
'''

# future feature: filter experiments by rmsd after combining/clustering
filtering_str = '''
filtering {
  enable = False
}
'''

refinement_str = '''
refinement {
  include scope dials.command_line.refine.phil_scope
  input {
    experiments = None
    reflections = None
  }
}
'''

refinement_override_str = '''
refinement {
  output {
    experiments = FILENAME_refined_CLUSTER.expt
    reflections = FILENAME_refined_CLUSTER.refl
    include_unused_reflections = False
    log = FILENAME_refine_CLUSTER.log
    debug_log = FILENAME_refine_CLUSTER.debug.log
  }
  refinement {
    parameterisation {
      auto_reduction {
        action = remove
      }
      beam {
        fix = all
      }
    }
    refinery {
      engine = SparseLevMar
    }
    reflections {
      outlier {
        algorithm = sauter_poon
        minimum_number_of_reflections = 3
        separate_experiments = False
        separate_panels = False
      }
    }
  }
  input {
    experiments = FILENAME_combined_CLUSTER.expt
    reflections = FILENAME_combined_CLUSTER.refl
  }
}
'''

recompute_mosaicity_str = '''
recompute_mosaicity {
  include scope xfel.command_line.recompute_mosaicity.phil_scope
  input {
    experiments = None
    reflections = None
  }
}
'''

recompute_mosaicity_override_str = '''
recompute_mosaicity {
  input {
    experiments = FILENAME_refined_CLUSTER.expt
    reflections = FILENAME_refined_CLUSTER.refl
  }
  output {
    experiments = FILENAME_refined_CLUSTER.expt
    reflections = FILENAME_refined_CLUSTER.refl
  }
}
'''


# reintegration after dials refinement
reintegration_str = '''
reintegration {
  enable = True
  include scope xfel.merging.command_line.mpi_integrate.phil_scope
  input {
    experiments = None
    reflections = None
  }
}
'''

reintegration_override_str = '''
reintegration{
  dispatch {
    step_list = input balance integrate
  }
  output {
    prefix = FILENAME_reintegrated_CLUSTER
    save_experiments_and_reflections = True
  }
  input {
    path = .
    experiments_suffix = FILENAME_refined_CLUSTER.expt
    reflections_suffix = FILENAME_refined_CLUSTER.refl
  }
}
'''

# split results and coerce to integration pickle for merging
postprocessing_str = '''
postprocessing {
  enable = False
  include scope xfel.command_line.frame_extractor.phil_scope
}
'''

postprocessing_override_str = """
postprocessing {
  input {
    experiments = FILENAME_reintegrated_CLUSTER*.expt
    reflections = FILENAME_reintegrated_CLUSTER*.refl
  }
  output {
    filename = FILENAME_CLUSTER_ITER_extracted.refl
    dirname = %s
  }
}
"""

master_defaults_str = multiprocessing_str + striping_str + combining_str + filtering_str + \
                        refinement_str + recompute_mosaicity_str + reintegration_str + postprocessing_str

# initialize a master scope from the multiprocessing phil string
master_defaults_scope = parse(master_defaults_str, process_includes=True)
# update master scope with customized and local phil scopes
phil_scope = master_defaults_scope.fetch(parse(postprocessing_override_str, process_includes=True))
phil_scope = phil_scope.fetch(parse(reintegration_override_str, process_includes=True))
phil_scope = phil_scope.fetch(parse(recompute_mosaicity_override_str, process_includes=True))
phil_scope = phil_scope.fetch(parse(refinement_override_str, process_includes=True))
phil_scope = phil_scope.fetch(parse(combining_override_str, process_includes=True))
phil_scope = phil_scope.fetch(parse(multiprocessing_override_str, process_includes=True))

helpstring = """cctbx.xfel.stripe_experiment: parallel processing of an XFEL UI-generated trial.

usage: cctbx.xfel.stripe_experiment striping.results_dir=/path/to/results striping.trial=000

for interactive unit cell clustering, use combine_experiments.clustering.dendrogram=True
"""

def allocate_chunks(results_dir,
                    trial_no,
                    rgs_selected=None,
                    respect_rungroup_barriers=True,
                    runs_selected=None,
                    stripe=False,
                    max_size=1000,
                    integrated=False):
  refl_ending = "_integrated" if integrated else "_indexed"
  expt_ending = "_refined.expt"
  trial = "%03d" % trial_no
  print("processing trial %s" % trial)
  if rgs_selected:
    rg_condition = lambda rg: rg in rgs_selected
  else:
    rg_condition = lambda rg: True
  rgs = {} # rungroups and associated runs
  for run in os.listdir(results_dir):
    if runs_selected and run not in runs_selected:
      continue
    trgs = [trg for trg in os.listdir(os.path.join(results_dir, run))
            if (trg[:6] == trial + "_rg") and rg_condition(trg[-5:])]
    if not trgs:
      continue
    rungroups = set([n.split("_")[1] for n in trgs])
    for rg in rungroups:
      if rg not in rgs:
        rgs[rg] = [run]
      else:
        rgs[rg].append(run)
  batch_chunk_nums_sizes = {}
  batch_contents = {}
  if respect_rungroup_barriers:
    batchable = {rg:{rg:runs} for rg, runs in six.iteritems(rgs)}
  else:
    batchable = {"all":rgs}
  # for either grouping, iterate over the top level keys in batchable and
  # distribute the events within those "batches" in stripes or chunks
  extension = None
  for batch, rungroups in six.iteritems(batchable):
    rg_by_run = {}
    for rungroup, runs in six.iteritems(rungroups):
      for run in runs:
        rg_by_run[run] = rungroup
    n_img = 0
    batch_contents[batch] = []
    for run, rg in six.iteritems(rg_by_run):
      try:
        trg = trial + "_" + rg
        contents = sorted(os.listdir(os.path.join(results_dir, run, trg, "out")))
      except OSError:
        print("skipping run %s missing out directory" % run)
        continue
      abs_contents = [os.path.join(results_dir, run, trg, "out", c)
                      for c in contents]
      batch_contents[batch].extend(abs_contents)
      expts = [c for c in contents if c.endswith(expt_ending)]
      n_img += len(expts)
      if extension is None:
        if any(c.endswith(".mpack") for c in contents):
          extension = ".mpack"
        elif any(c.endswith(".refl") for c in contents):
          extension = ".refl"
        else:
         extension = ".pickle"
    if n_img == 0:
      print("no images found for %s" % batch)
      del batch_contents[batch]
      continue
    n_chunks = int(math.ceil(n_img/max_size))
    chunk_size = int(math.ceil(n_img/n_chunks))
    batch_chunk_nums_sizes[batch] = (n_chunks, chunk_size)
  if len(batch_contents) == 0:
    raise Sorry("no DIALS integration results found.")
  refl_ending += extension
  batch_chunks = {}
  for batch, num_size_tuple in six.iteritems(batch_chunk_nums_sizes):
    num, size = num_size_tuple
    batch_chunks[batch] = []
    contents = batch_contents[batch]
    expts = [c for c in contents if c.endswith(expt_ending)]
    refls = [c for c in contents if c.endswith(refl_ending)]
    expts, refls = match_dials_files(expts, refls, expt_ending, refl_ending)
    if stripe:
      for i in range(num):
        expts_stripe = expts[i::num]
        refls_stripe = refls[i::num]
        batch_chunks[batch].append((expts_stripe, refls_stripe))
      print("striped %d experiments in %s with %d experiments per stripe and %d stripes" % \
        (len(expts), batch, len(batch_chunks[batch][0][0]), len(batch_chunks[batch])))
    else:
      for i in range(num):
        expts_chunk = expts[i*size:(i+1)*size]
        refls_chunk = refls[i*size:(i+1)*size]
        batch_chunks[batch].append((expts_chunk, refls_chunk))
      print("chunked %d experiments in %s with %d experiments per chunk and %d chunks" % \
        (len(expts), batch, len(batch_chunks[batch][0][0]), len(batch_chunks[batch])))
  return batch_chunks

def parse_retaining_scope(args, phil_scope=phil_scope):
  if "-c" in args:
    phil_scope.show(attributes_level=2)
    return
  file_phil = []
  cmdl_phil = []
  for arg in args:
    if os.path.isfile(arg):
      try:
        file_phil.append(parse(file_name=arg))
      except Exception as e:
        raise Sorry("Unrecognized file: %s" % arg)
    else:
      try:
        cmdl_phil.append(parse(arg))
      except Exception as e:
        raise Sorry("Unrecognized argument: %s" % arg)

  run_scope, unused1 = phil_scope.fetch(sources=file_phil, track_unused_definitions=True)
  run_scope, unused2 = run_scope.fetch(sources=cmdl_phil, track_unused_definitions=True)
  if any([unused1, unused2]):
    msg = "\n".join([str(loc) for loc in unused1 + unused2])
    raise Sorry("Unrecognized argument(s): " + msg)

  return run_scope

def script_to_expand_over_clusters(clustered_json_name,
                                   phil_template_name, command, location):
  """
  Write a bash script to find results of a clustering step and produce customized
  phils and commands to run with each of them. For example, run the command
  dials.refine ...cluster8.expt ...cluster8.refl ...cluster8.phil followed by
  dials.refine ...cluster9.expt ...cluster9.refl ...cluster9.phil.
  clustered_json_name, clustered_refl_name and phil_template_name must each
  contain an asterisk, and substitution in phil_template itself will occur at
  each instance of CLUSTER.
  """
  clj_part_first, clj_part_last = clustered_json_name.split("CLUSTER")
  clustered_template_name = clj_part_first + "*" + clj_part_last
  ph_part_first, ph_part_last = phil_template_name.split("CLUSTER")

  bash_str = '''
#! /bin/sh

for file in `ls {clname}`
  do export cluster=`echo $file | sed "s:{cljfirst}::; s:{cljlast}::"`
  export philname="{phfirst}${cluster}{phlast}"
  export outname=`echo $philname | sed "s:.phil:.out:"`
  sed "s:CLUSTER:${cluster}:g" {phtempl} > $philname
  {command} $philname > $outname
done
'''.format(clname=clustered_template_name, phtempl=phil_template_name,
           cljfirst=clj_part_first, cljlast=clj_part_last,
           phfirst=ph_part_first, phlast=ph_part_last,
           command=command, cluster="{cluster}")

  bash_name = "generator".join([ph_part_first, ph_part_last]).split(".phil")[0] + ".sh"
  with open(os.path.join(location, bash_name), "w") as script:
    script.write(bash_str)
  return bash_name

class Script(object):

  def __init__(self, args = None):
    '''Initialise the script.'''

    # The script usage
    self.master_defaults_scope = master_defaults_scope
    if args is None: args = sys.argv[1:]
    self.run_scope = parse_retaining_scope(args)
    self.diff_scope = self.master_defaults_scope.fetch_diff(self.run_scope)
    self.params = self.run_scope.extract()

    # Validation
    if self.params.reintegration.enable:
      if self.params.combine_experiments.output.delete_shoeboxes:
        raise Sorry("Keep shoeboxes during combine_experiments and joint refinement when reintegrating."+
          "Set combine_experiments.output.delete_shoeboxes = False when using reintegration.")

    # Setup
    self.clustering = self.params.combine_experiments.clustering.use

  def set_up_section(self, section_tag, dispatcher_name,
    clustering=False, custom_parts=None, lambda_diff_str=None):
    diff_str = self.diff_scope.get(section_tag).as_str().replace("FILENAME", self.filename)
    if lambda_diff_str is not None:
      diff_str = lambda_diff_str(diff_str)
    if not clustering:
      diff_str = diff_str.replace("_CLUSTER", "")
    diff_parts = diff_str.split("\n")[1:-2]
    if custom_parts is not None:
      for part in custom_parts:
        diff_parts.append(part)
    diff_str = "\n".join(diff_parts)
    phil_filename = "%s_%s_CLUSTER.phil" % (self.filename, section_tag) if clustering else \
      "%s_%s.phil" % (self.filename, section_tag)
    phil_path = os.path.join(self.params.striping.output_folder, self.intermediates, phil_filename)
    if os.path.isfile(phil_path):
      os.remove(phil_path)
    with open(phil_path, "w") as phil_outfile:
      phil_outfile.write(diff_str + "\n")
    if clustering:
      script = script_to_expand_over_clusters(
        self.params.refinement.input.experiments[0].replace("FILENAME", self.filename),
        phil_filename,
        dispatcher_name,
        self.intermediates)
      command = ". %s" % os.path.join(self.params.striping.output_folder, self.intermediates, script)
    else:
      command = "%s_phil=%s" % (dispatcher_name, phil_filename)
    self.argument_sequence.append(command)

  def run(self):
    '''Execute the script.'''
    runs = ["r%04d" % int(r) if r.isnumeric() else r for r in self.params.striping.run]
    if self.params.striping.run:
      print("processing runs " + ", ".join(runs))
    if self.params.striping.rungroup:
      print("processing rungroups " + ", ".join(["rg%03d" % rg for rg in self.params.striping.rungroup]))
    batch_chunks = allocate_chunks(self.params.striping.results_dir,
                                   self.params.striping.trial,
                                   rgs_selected=["rg%03d" % rg for rg in self.params.striping.rungroup],
                                   respect_rungroup_barriers=self.params.striping.respect_rungroup_barriers,
                                   runs_selected=runs,
                                   stripe=self.params.striping.stripe,
                                   max_size=self.params.striping.chunk_size,
                                   integrated=self.params.combine_experiments.keep_integrated)
    self.dirname = os.path.join(self.params.striping.output_folder, "combine_experiments_t%03d" % self.params.striping.trial)
    self.intermediates = os.path.join(self.dirname, "intermediates")
    self.extracted = os.path.join(self.dirname, "final_extracted")
    for d in self.dirname, self.intermediates, self.extracted:
      if not os.path.isdir(d):
        os.mkdir(d)
    if self.params.striping.output_folder is None:
      self.params.striping.output_folder = os.getcwd()
    tag = "stripe" if self.params.striping.stripe else "chunk"
    all_commands = []
    for batch, ch_list in six.iteritems(batch_chunks):
      for idx in range(len(ch_list)):
        chunk = ch_list[idx]

        # reset for this chunk/stripe
        self.filename = "t%03d_%s_%s%03d" % (self.params.striping.trial, batch, tag, idx)
        self.argument_sequence = []

        # set up the file containing input expts and refls (logging)
        chunk_path = os.path.join(self.params.striping.output_folder, self.intermediates, self.filename)
        if os.path.isfile(chunk_path):
          os.remove(chunk_path)
        with open(chunk_path, "w") as outfile:
          for i in (0, 1): # expts then refls
            outfile.write("\n".join(chunk[i]) + "\n")

        # set up the params for dials.combine_experiments
        custom_parts = ["  input {"]
        for expt_path in chunk[0]:
          custom_parts.append("    experiments = %s" % expt_path)
        for refl_path in chunk[1]:
          custom_parts.append("    reflections = %s" % refl_path)
        custom_parts.append("  }")
        self.set_up_section("combine_experiments", "combine_experiments",
          clustering=False, custom_parts=custom_parts)

        # refinement of the grouped experiments
        self.set_up_section("refinement", "refine",
          clustering=self.clustering)

        # refinement of the grouped experiments
        self.set_up_section("recompute_mosaicity", "recompute_mosaicity",
          clustering=self.clustering)

        # reintegration
        if self.params.reintegration.enable:
          self.set_up_section("reintegration", "integration", clustering=self.clustering)

        # extract results to integration pickles for merging
        if self.params.postprocessing.enable:
          pass # disabled
          #lambda_diff_str = lambda diff_str: (diff_str % \
          #  (os.path.join("..", "final_extracted"))).replace("ITER", "%04d")
          #self.set_up_section("postprocessing", "cctbx.xfel.frame_extractor",
          #  lambda_diff_str=lambda_diff_str, clustering=self.clustering)

        # submit queued job from appropriate directory
        os.chdir(self.intermediates)
        command = "cctbx.xfel.ensemble_refinement_pipeline " + " ".join(self.argument_sequence)

        if self.params.mp.method != 'shifter' and self.params.mp.mpi_command:
          command = "%s %s"%(self.params.mp.mpi_command, command)

        if self.params.combine_experiments.clustering.dendrogram:
          easy_run.fully_buffered(command).raise_if_errors().show_stdout()
        else:
          submit_folder = os.path.join(self.params.striping.output_folder, self.intermediates)
          submit_path = os.path.join(submit_folder, "combine_%s.sh" % self.filename)
          submit_command = get_submit_command_chooser(command, submit_path, self.intermediates, self.params.mp,
            log_name=os.path.splitext(os.path.basename(submit_path))[0] + ".out",
            err_name=os.path.splitext(os.path.basename(submit_path))[0] + ".err",
            root_dir = submit_folder)
          all_commands.append(submit_command)
          if not self.params.striping.dry_run:
            print("executing command: %s" % submit_command)
            try:
              easy_run.fully_buffered(submit_command).raise_if_errors().show_stdout()
            except Exception as e:
              if not "Warning: job being submitted without an AFS token." in str(e):
                raise e
    return all_commands

if __name__ == "__main__":
  import sys
  if "-h" in sys.argv[1:] or "--help" in sys.argv[1:]:
    print(helpstring)
    exit()
  if "-c" in sys.argv[1:]:
    expert_level = int(sys.argv[sys.argv.index("-e") + 1]) if "-e" in sys.argv[1:] else 0
    attr_level = int(sys.argv[sys.argv.index("-a") + 1]) if "-a" in sys.argv[1:] else 0
    phil_scope.show(expert_level=expert_level, attributes_level=attr_level)
    with open("striping_defaults.phil", "w") as defaults:
      defaults.write(phil_scope.as_str())
    exit()
  with show_mail_on_error():
    script = Script()
    script.run()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/subtract_background.py
from __future__ import absolute_import, division, print_function
import os
import sys

from libtbx.option_parser import option_parser
from scitbx.array_family import flex
from scitbx import smoothing
from xfel.command_line import smooth_spectrum
from six.moves import zip


def run(args):
  command_line = (option_parser()
                  .option("--output_dirname", "-o",
                          type="string",
                          help="Directory for output files.")
                  ).process(args=args)
  args = command_line.args
  output_dirname = command_line.options.output_dirname
  if output_dirname is None:
    output_dirname = os.path.dirname(args[0])
  assert len(args) == 2
  xy_pairs = []
  for i, filename in enumerate(args):
    print("Reading data from: %s" %filename)
    f = open(filename, 'rb')
    x, y = zip(*[line.split() for line in f.readlines() if not line.startswith("#")])
    x = flex.double(flex.std_string(x))
    y = flex.double(flex.std_string(y))
    xy_pairs.append((x,y))

  signal = xy_pairs[0]
  background = xy_pairs[1]

  signal_x, background_subtracted = subtract_background(signal, background, plot=True)
  filename = os.path.join(output_dirname, "background_subtracted.txt")
  f = open(filename, "wb")
  print("\n".join(["%i %f" %(x, y)
                         for x, y in zip(signal_x, background_subtracted)]), file=f)
  f.close()
  print("Background subtracted spectrum written to %s" %filename)


def subtract_background(signal, background, plot=False):

  x, y = smooth_spectrum.interpolate(background[0], background[1])
  y_fitted = smoothing.savitzky_golay_filter(x, y, half_window=32, degree=3)[1]
  signal_x, signal_y = signal
  signal_x, signal_y = smooth_spectrum.interpolate(signal[0], signal[1])

  x_interp_size = x.size()
  for i, x_i in enumerate(reversed(x)):
    if x_i not in signal[0]:
      assert x[x_interp_size - i - 1] == x_i
      del signal_x[x_interp_size - i - 1]
      del signal_y[x_interp_size - i - 1]
      del y_fitted[x_interp_size - i - 1]

  background_subtracted = signal_y - y_fitted

  if plot:
    from matplotlib import pyplot
    pyplot.plot(signal[0], signal[1], linewidth=2, label="signal+background")
    pyplot.plot(background[0], background[1], linewidth=2, label="background")
    pyplot.plot(signal_x, y_fitted, linewidth=2, label="background_fit")
    pyplot.plot(signal_x, background_subtracted, linewidth=2, label="signal")
    pyplot.legend(loc=2)
    pyplot.ylabel("Intensity", fontsize=15)
    pyplot.xlabel("Pixel column", fontsize=15)
    pyplot.show()

  return signal_x, background_subtracted


def signal_to_noise_statistical(signal, background):
  "M.F. Koenig and J.T. Grant, Surface and Interface Analysis, Vol. 7, No.5, 1985, 217"
  snr = signal/flex.pow(signal + 2 * background, 0.5)
  return snr


if __name__ == '__main__':
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/trial_stats.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.trial_stats
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1

import libtbx.phil
from libtbx.utils import Usage, Sorry
import sys

master_phil = libtbx.phil.parse("""
  trial_id = None
    .type = int
  hit_cutoff = 16
    .type = int
  run_start = None
    .type = int
  run_end = None
    .type = int
  db {
    host = None
      .type = str
    name = None
      .type = str
    table_name = None
      .type = str
    user = None
      .type = str
    password = None
      .type = str
    tags = None
      .type = str
  }
""")

def run (args) :
  try:
    from cxi_xdr_xes.cftbx.cspad_ana import db as db
  except ImportError:
    raise Sorry("Trial logging not supported for this installation. Conact the developers for access.")

  user_phil = []
  # TODO: replace this stuff with iotbx.phil.process_command_line_with_files
  # as soon as I can safely modify it
  for arg in args :
    #if (os.path.isdir(arg)) :
      #user_phil.append(libtbx.phil.parse("""status_dir=\"%s\"""" % arg))
    #elif (not "=" in arg) :
    if (not "=" in arg) :
      try :
        user_phil.append(libtbx.phil.parse("""trial_id=%d""" % int(arg)))
      except ValueError as e :
        raise Sorry("Unrecognized argument '%s'" % arg)
    else :
      try :
        user_phil.append(libtbx.phil.parse(arg))
      except RuntimeError as e :
        raise Sorry("Unrecognized argument '%s' (error: %s)" % (arg, str(e)))
  params = master_phil.fetch(sources=user_phil).extract()
  if (params.trial_id is None) :
    master_phil.show()
    raise Usage("trial_id must be defined (either trial_id=XXX, or the integer "+
      "ID alone).")
  assert (params.hit_cutoff is not None) and (params.hit_cutoff > 0)

  extra_cmd = ""
  if params.run_start is not None:
    extra_cmd += "AND run >= %d" % params.run_start
  if params.run_end is not None:
    extra_cmd += "AND run <= %d" % params.run_end

  dbobj = db.dbconnect(host=params.db.host, db=params.db.name, username=params.db.user, password=params.db.password)


  cursor = dbobj.cursor()
  cmd = "SELECT DISTINCT(run) FROM %s WHERE trial = %%s %s ORDER BY run"%(params.db.table_name, extra_cmd)
  cursor.execute(cmd, params.trial_id)

  frames_total = 0
  hits_total = 0
  indexed_total = 0

  for runId in cursor.fetchall():
    run = int(runId[0])
    cmd = "SELECT id, eventstamp, hitcount, distance, sifoil, wavelength, indexed FROM %s \
        WHERE trial = %s AND run = %s"
    if params.db.tags is not None:
      for tag in params.db.tags.split(','):
        cmd += """ AND tags LIKE "%%{0}%%" """.format(tag)
    cursor.execute(cmd%(params.db.table_name,params.trial_id,run))

    numframes = numhits = numindexed = 0
    for id, eventstamp, hitcount, distance, sifoil, wavelength, indexed in cursor.fetchall():
      numframes +=1
      if hitcount >= params.hit_cutoff:
        numhits += 1
      if indexed:
        numindexed += 1

    if numhits == 0:
      hitrate = 0
    else:
      hitrate = 100*numhits/numframes
    if numindexed == 0:
      indexingrate = 0
    else:
      indexingrate = 100*numindexed/numframes

    print("Run: %3d, number of hits: %6d, number of frames: %6d, hitrate: %4.1f%%. Number indexed: %6d (%4.1f%%)"%(run,numhits,numframes,hitrate,numindexed,indexingrate))
    frames_total += numframes
    hits_total += numhits
    indexed_total += numindexed

  if hits_total == 0:
    hitrate = 0
  else:
    hitrate = 100*hits_total/frames_total
  if indexed_total == 0:
    indexingrate = 0
  else:
    indexingrate = 100*indexed_total/frames_total

  print("Totals: frames: %d, hits: %d (%4.1f%%), indexed: %d (%4.1f%%)"%(frames_total,hits_total,hitrate,indexed_total,indexingrate))
  dbobj.close()

if (__name__ == "__main__") :
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/trumpet_plots.py
# -*- coding: utf-8 -*-
#!/usr/bin/env python
#
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.trumpet_plots
#
from __future__ import absolute_import, division, print_function
from dials.util import show_mail_on_error
from matplotlib import pyplot as plt
from libtbx.phil import parse
from six.moves import zip

help_message = '''

Make trumpet plots from dials output, one per experiment

See Sauter 2014 (https://doi.org/10.1107/S1399004714024134)

Example:

  cctbx.xfel.trumpet_plots experiment.expt reflections.refl
'''

# Create the phil parameters
phil_scope = parse('''
max_plots = 10
  .type = int
  .help = Maximum number of plots to make
repredict_input_reflections = True
  .type = bool
  .help = Whether to use the input models to repredict reflection positions \
          prior to making plots
''', process_includes=True)

from xfel.command_line.detector_residuals import setup_stats, trumpet_plot

class Script(object):
  ''' Class to parse the command line options. '''

  def __init__(self):
    ''' Set the expected options. '''
    from dials.util.options import OptionParser
    import libtbx.load_env

    # Create the option parser
    usage = "usage: %s [options] /path/to/refined/json/file" % libtbx.env.dispatcher_name
    self.parser = OptionParser(
      usage=usage,
      sort_options=True,
      phil=phil_scope,
      read_experiments=True,
      read_reflections=True,
      check_format=False,
      epilog=help_message)

  def run(self):
    ''' Parse the options. '''
    # Parse the command line arguments
    params, options = self.parser.parse_args(show_diff_phil=True)
    self.params = params

    total = 0
    for ewrap, rwrap in zip(params.input.experiments, params.input.reflections):
      experiments = ewrap.data
      reflections = rwrap.data
      if params.repredict_input_reflections:
        from dials.algorithms.refinement.prediction.managed_predictors import ExperimentsPredictorFactory
        ref_predictor = ExperimentsPredictorFactory.from_experiments(experiments, force_stills=experiments.all_stills())
        reflections = ref_predictor(reflections)
      reflections = setup_stats(experiments, reflections)
      for expt_id, expt in enumerate(experiments):
        refls = reflections.select(reflections['id'] == expt_id)
        if len(refls) == 0: continue
        trumpet_plot(expt, refls)

        total += 1
        if params.max_plots and total >= params.max_plots:
          break
      if params.max_plots and total >= params.max_plots:
        break
    plt.show()

if __name__ == '__main__':
  with show_mail_on_error():
    script = Script()
    script.run()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/upload_mtz.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.upload_mtz

from libtbx.phil import parse
from dials.util import Sorry
import os, sys
import re
try:
  import fcntl
except ImportError:
  fcntl = None


help_message = """

Upload an .mtz file and merging log to a shared Google Drive folder.

"""

phil_str = """
drive {
  credential_file = None
    .type = path
    .help = Credential file (json format) for a Google Cloud service account
  shared_folder_id = None
    .type = str
    .help = Id string of the destination folder. If the folder url is \
https://drive.google.com/drive/u/0/folders/1NlJkfL6CMd1NZIl6Duy23i4G1RM9cNH- , \
then the id is 1NlJkfL6CMd1NZIl6Duy23i4G1RM9cNH- .
}
input {
  mtz_file = None
    .type = path
    .help = Location of the mtz file to upload
  disregard_mtz = False
    .type = bool
    .help = Flag to bypass the upload of mtz file entirely
  log_file = None
    .type = path
    .help = Location of the log file to upload. If None, guess from mtz name.
  other_files = None
    .type = path
    .multiple = True
    .help = Any additional files to upload.
  dataset_root = None
    .type = path
    .help = Path to folder with dataset versions. Can be None if guess_root_and_version = True
  version = None
    .type = int
    .help = Dataset version number. If None, guess from mtz name.
  guess_root_and_version = True
    .type = bool
    .help = Whether to guess the file paths from the mtz_file path
}
"""
phil_scope = parse(phil_str)

def _get_root_and_version(mtz_fname):
  """
  find and return the dataset name and version string from an mtz filename
  """
  regex = re.compile(r'(.*)_(v\d{3})_all.mtz$')
  hit = regex.search(mtz_fname)
  assert hit is not None
  assert len(hit.groups()) == 2
  return hit.groups()

def _get_log_fname(mtz_fname):
  """
  convert an mtz filename to the corresponding main log filename
  """
  regex = re.compile(r'(.*)_all.mtz$')
  hit = regex.search(mtz_fname)
  assert hit is not None
  assert len(hit.groups()) == 1
  return hit.groups()[0] + '_main.log'

class Locker:
  """ See https://stackoverflow.com/a/60214222
  """
  def __enter__(self):
    try:
      self.fp = open(os.path.expanduser('~/.upload_mtz.lock'), 'wb')
    except FileNotFoundError:
      self.fp = None
    if fcntl and self.fp is not None:
      fcntl.flock(self.fp.fileno(), fcntl.LOCK_EX)
  def __exit__(self, *args, **kwargs):
    if self.fp is not None:
      if fcntl:
        fcntl.flock(self.fp.fileno(), fcntl.LOCK_UN)
      self.fp.close()

class pydrive2_interface:
  """
  Wrapper for uploading versioned mtzs and logs using Pydrive2. Constructed from
  a service account credentials file and the Google Drive id of the top-level
  destination folder.
  """

  def __init__(self, cred_file, folder_id):
    try:
      from pydrive2.auth import ServiceAccountCredentials, GoogleAuth
      from pydrive2.drive import GoogleDrive
    except ImportError:
      raise Sorry("Pydrive2 not found. Try:\n$ conda install pydrive2 -c conda-forge")
    gauth = GoogleAuth()
    scope = ['https://www.googleapis.com/auth/drive']
    gauth.credentials = ServiceAccountCredentials.from_json_keyfile_name(
        cred_file, scope
    )
    self.drive = GoogleDrive(gauth)
    self.top_folder_id = folder_id



  def _fetch_or_create_folder(self, fname, parent_id):
    with Locker():
      query = {
          "q": "'{}' in parents and title='{}'".format(parent_id, fname),
          "supportsTeamDrives": "true",
          "includeItemsFromAllDrives": "true",
          "corpora": "allDrives"
      }
      hits = self.drive.ListFile(query).GetList()
      if hits:
        assert len(hits)==1
        return hits[0]['id']
      else:
        query = {
          "title": fname,
          "mimeType": "application/vnd.google-apps.folder",
          "parents": [{"kind": "drive#fileLink", "id": parent_id}]
        }
        f = self.drive.CreateFile(query)
        f.Upload()
        return f['id']

  def _upload_detail(self, file_path, parent_id):
    title = os.path.split(file_path)[1]
    query = {
        "title": title,
        "parents": [{"kind": "drive#fileLink", "id": parent_id}]
    }
    f = self.drive.CreateFile(query)
    f.SetContentFile(file_path)
    f.Upload()


  def upload(self, folder_list, files):
    """
    Upload from the given file paths to a folder defined by the hierarchy in
    folder_list. So if `folders` is ['a', 'b'] and `files` is [f1, f2], then
    inside the folder defined by self.folder_id, we create nested folder a/b/
    and upload f1 and f2 to that folder.
    """
    current_folder_id = self.top_folder_id
    for fname in folder_list:
      current_folder_id = self._fetch_or_create_folder(fname, current_folder_id)
    for file in files:
      self._upload_detail(file, current_folder_id)

def run(args):

  user_phil = []
  if '--help' in args or '-h' in args:
    print(help_message)
    phil_scope.show()
    return

  for arg in args:
    try:
      if os.path.isfile(arg):
        user_phil.append(parse(file_name=arg))
      else:
        user_phil.append(parse(arg))
    except Exception as e:
      raise Sorry("Unrecognized argument %s"%arg)
  params = phil_scope.fetch(sources=user_phil).extract()
  run_with_preparsed(params)



def run_with_preparsed(params):
  assert params.drive.credential_file is not None
  assert params.drive.shared_folder_id is not None
  assert params.input.mtz_file is not None
  if params.input.other_files is None: params.input.other_files = []


  mtz_dirname, mtz_fname = os.path.split(params.input.mtz_file)
  mtz_path = params.input.mtz_file

  if params.input.guess_root_and_version:
    if params.input.version is not None:
      dataset_root = _get_root_and_version(mtz_fname)[0]
      version_str = "v{:03d}".format(params.input.version)
    else:
      dataset_root, version_str = _get_root_and_version(mtz_fname)
  else:
    dataset_root = params.input.dataset_root
    version_str = "v{:03d}".format(params.input.version)

  if params.input.log_file is not None:
    log_path = params.input.log_file
  else:
    log_fname = _get_log_fname(mtz_fname)
    log_path = os.path.join(mtz_dirname, log_fname)

  drive = pydrive2_interface(
      params.drive.credential_file,
      params.drive.shared_folder_id
  )
  folders = [dataset_root, version_str]
  files = [log_path]
  if not params.input.disregard_mtz:
    files.append(mtz_path)
  files.extend(params.input.other_files)
  drive.upload(folders, files)


if __name__=="__main__":
  from libtbx.mpi4py import MPI
  comm = MPI.COMM_WORLD
  rank = comm.Get_rank() # each process in MPI has a unique id, 0-indexed
  size = comm.Get_size() # size: number of processes running in this job
  if rank == 0:
    run(sys.argv[1:])
  comm.barrier()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/view_pixel_histograms.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.pixel_histograms
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1

import sys

from libtbx import easy_pickle
from libtbx.option_parser import option_parser
from scitbx.array_family import flex
from scitbx.math import curve_fitting

from xfel.cxi.cspad_ana import cspad_tbx
from six.moves import range

def run(args):
  assert len(args) > 0
  command_line = (option_parser()
                  .option("--roi",
                          type="string",
                          help="Region of interest for summing up histograms"
                          "from neighbouring pixels.")
                  .option("--log_scale",
                          action="store_true",
                          default=False,
                          help="Draw y-axis on a log scale.")
                  .option("--normalise",
                          action="store_true",
                          default=False,
                          help="Normalise by number of member images.")
                  .option("--save",
                          action="store_true",
                          default=False,
                          help="Save each plot as a png.")
                  .option("--start",
                          type="string",
                          help="Starting pixel coordinates")
                  .option("--fit_gaussians",
                          action="store_true",
                          default=False,
                          help="Fit gaussians to the peaks.")
                  .option("--n_gaussians",
                          type="int",
                          default=2,
                          help="Number of gaussians to fit.")
                  .option("--estimated_gain",
                          type="float",
                          default=30,
                          help="The approximate position of the one photon peak.")
                  ).process(args=args)
  log_scale = command_line.options.log_scale
  fit_gaussians = command_line.options.fit_gaussians
  roi = cspad_tbx.getOptROI(command_line.options.roi)
  normalise = command_line.options.normalise
  save_image = command_line.options.save
  starting_pixel = command_line.options.start
  n_gaussians = command_line.options.n_gaussians
  estimated_gain = command_line.options.estimated_gain
  if starting_pixel is not None:
    starting_pixel = eval(starting_pixel)
    assert isinstance(starting_pixel, tuple)
  args = command_line.args

  path = args[0]
  window_title = path
  d = easy_pickle.load(path)
  args = args[1:]
  pixels = None
  if len(args) > 0:
    pixels = [eval(arg) for arg in args]
    for pixel in pixels:
      assert isinstance(pixel, tuple)
      assert len(pixel) == 2
  if roi is not None:

    summed_hist = {}
    for i in range(roi[2], roi[3]):
      for j in range(roi[0], roi[1]):
        if (i,j) not in summed_hist:
          summed_hist[(0,0)] = d[(i,j)]
        else:
          summed_hist[(0,0)].update(d[(i,j)])
    d = summed_hist

  #if roi is not None:

    #summed_hist = None
    #for i in range(roi[2], roi[3]):
      #for j in range(roi[0], roi[1]):
        #if summed_hist is None:
          #summed_hist = d[(i,j)]
        #else:
          #summed_hist.update(d[(i,j)])

    #title = str(roi)
    #plot(hist, window_title=window_title, title=title,log_scale=log_scale,
         #normalise=normalise, save_image=save_image, fit_gaussians=fit_gaussians)
    #return


  histograms = pixel_histograms(d, estimated_gain=estimated_gain)
  histograms.plot(
     pixels=pixels, starting_pixel=starting_pixel, fit_gaussians=fit_gaussians,
    n_gaussians=n_gaussians, window_title=window_title, log_scale=log_scale,
    save_image=save_image)


class pixel_histograms(object):

  def __init__(self, histograms, estimated_gain=30):
    self.histograms = histograms
    self.estimated_gain = estimated_gain

  def plot(self, pixels=None, starting_pixel=None, fit_gaussians=True,
           n_gaussians=2, window_title=None, log_scale=False, save_image=False):
    from matplotlib import pyplot
    normalise=False # XXX
    assert [pixels, starting_pixel].count(None) > 0
    print("n_images:", flex.sum(self.histograms.values()[0].slots()))
    print("n_pixels:", len(self.histograms))
    if pixels is None:
      pixels = sorted(self.histograms.keys())
      if starting_pixel is not None:
        pixels = pixels[pixels.index(starting_pixel):]
    for pixel in pixels:
      hist = self.histograms[pixel]
      print(pixel)
      title = str(pixel)
      if fit_gaussians:
        pyplot.subplot(211)
      self.plot_one_histogram(
        hist, window_title=window_title, title=title,log_scale=log_scale,
        normalise=normalise, save_image=save_image)
      fontsize = 18
      pyplot.ylabel("Counts", fontsize=fontsize)
      pyplot.xlabel("ADUs", fontsize=fontsize)
      if fit_gaussians:
        self.plot_gaussians(pixel, n_gaussians=n_gaussians, log_scale=log_scale)
      pyplot.ylabel("Counts", fontsize=fontsize)
      pyplot.xlabel("ADUs", fontsize=fontsize)
      #axes = pyplot.axes()
      #for tick in axes.xaxis.get_ticklabels():
        #tick.set_fontsize(fontsize)
      #for tick in axes.yaxis.get_ticklabels():
        #tick.set_fontsize(fontsize)

      if save_image:
        pyplot.savefig("%s.png" %title)
      else:
        pyplot.show()

  def plot_gaussians(self, pixel, n_gaussians=2, log_scale=False):
    from matplotlib import pyplot
    if log_scale:
      pyplot.ylim(ymin=0.1)
    hist = self.histograms[pixel]
    #pyplot.ylim(ymax=flex.max(hist.slots()))
    gaussians = self.fit_one_histogram(pixel, n_gaussians=n_gaussians)
    x = hist.slot_centers()
    y_calc = flex.double(x.size(), 0)
    for g in gaussians:
      print(g.params)
      y = g(x)
      y_calc += y
      pyplot.plot(x, y, linewidth=2)

    #print "Peak height ratio: %.2f" %(gaussians[0].params[0]/gaussians[1].params[0])
    pyplot.plot(x, y_calc)
    # Plot the fit residuals
    xlim = pyplot.xlim() # store for reuse below
    pyplot.subplot(212)
    residual = hist.slots().as_double() - y_calc
    pyplot.plot(x, residual, linewidth=2)
    pyplot.xlim(xlim)

  def plot_one_histogram(self, histogram,
                         window_title=None, title=None,
                         log_scale=False, normalise=False, save_image=False):
    from matplotlib import pyplot
    slots = histogram.slots().as_double()
    if normalise:
      normalisation = (flex.sum(slots) + histogram.n_out_of_slot_range()) / 1e5
      print("normalising by factor: ", normalisation)
      slots /= normalisation
    bins, data = hist_outline(histogram)
    if log_scale:
      data.set_selected(data == 0, 0.1) # otherwise lines don't get drawn when we have some empty bins
      pyplot.yscale("log")
    pyplot.plot(bins, data, '-k', linewidth=2)
    pyplot.suptitle(title)
    data_min = min([slot.low_cutoff for slot in histogram.slot_infos() if slot.n > 0])
    data_max = max([slot.low_cutoff for slot in histogram.slot_infos() if slot.n > 0])
    pyplot.xlim(data_min, data_max+histogram.slot_width())

  def fit_one_histogram(self, pixel, n_gaussians=2):
    histogram = self.histograms[pixel]
    fitted_gaussians = []

    slot_centers = histogram.slot_centers()
    slots = histogram.slots().as_double()

    zero_peak_gaussian = None
    for i in range(n_gaussians):
      if i == 0:
        lower_threshold = -1000
        upper_threshold = 0.4 * self.estimated_gain
        mean = 0
        fit = self.single_peak_fit(histogram, lower_threshold, upper_threshold, mean,
                                   zero_peak_gaussian=zero_peak_gaussian)
        hist_max = flex.max(histogram.slots())
        if abs(fit.functions[0].params[0] - hist_max)/hist_max > 0.1:
          upper_threshold = 0.3 * self.estimated_gain
          fit = self.single_peak_fit(histogram, lower_threshold, upper_threshold, mean,
                                     zero_peak_gaussian=zero_peak_gaussian)
      else:
        y_obs = histogram.slots().as_double()
        x = histogram.slot_centers()
        y_calc = flex.double(y_obs.size(), 0)
        for g in fitted_gaussians:
          y_calc += g(x)
        residual = y_obs - y_calc
        # triangular smoothing of residual to find peak position
        residual = sliding_average(residual)
        residual = sliding_average(residual)
        for n in (4, 5, 6, 7, 8):
        #for n in (5, 6, 7, 8):
          # we assume that the peaks are separated by at least n sigma
          n_sigma = abs(n * fitted_gaussians[0].params[2])
          slot_i = histogram.get_i_slot(fitted_gaussians[i-1].params[1]+n_sigma)
          max_slot_i = flex.max_index(residual[slot_i:]) + slot_i
          mean = slot_centers[max_slot_i]
          lower_threshold = mean - 0.3 * (mean - fitted_gaussians[0].params[1])
          upper_threshold = mean + 0.4 * (mean - fitted_gaussians[0].params[1])
          #print lower_threshold, mean, upper_threshold
          #zero_peak_gaussian = None
          fit = self.single_peak_fit(histogram, lower_threshold, upper_threshold, mean,
                                     zero_peak_gaussian=zero_peak_gaussian)
          if (fit.functions[0].params[1] > fitted_gaussians[-1].params[1]
              and fit.functions[0].sigma > 0.5 * fitted_gaussians[-1].sigma
              and (fit.functions[0].params[1] - fitted_gaussians[-1].params[1]) > n_sigma):
            break
      fitted_gaussians += fit.functions
      if i == 0: zero_peak_gaussian = fit.functions[0]


    if len(fitted_gaussians) > 1:
      try:
        check_pixel_histogram_fit(histogram, fitted_gaussians)
      except PixelFitError as e:
        print("PixelFitError:", str(e))
      gain = fitted_gaussians[1].params[1] - fitted_gaussians[0].params[1]
      print("gain: %s" %gain)
      zero_peak = fitted_gaussians[0].params[1]
      photon_threshold = 2/3
      n_single_photons = flex.sum(
        histogram.slots()[histogram.get_i_slot(photon_threshold * gain + zero_peak):])
      n_double_photons = flex.sum(
        histogram.slots()[histogram.get_i_slot((1+photon_threshold) * gain + zero_peak):])
      n_single_photons -= n_double_photons
      print("n_single_photons: %i" %n_single_photons)
      print("n_double_photons: %i" %n_double_photons)
      #print n_double_photons/(n_single_photons+n_double_photons)
    return fitted_gaussians

  def single_peak_fit(self, hist, lower_threshold, upper_threshold, mean,
                      zero_peak_gaussian=None):
    lower_slot = 0
    for slot in hist.slot_centers():
      lower_slot += 1
      if slot > lower_threshold: break
    upper_slot = 0
    for slot in hist.slot_centers():
      upper_slot += 1
      if slot > upper_threshold: break

    x = hist.slot_centers()
    y = hist.slots().as_double()
    starting_gaussians = [curve_fitting.gaussian(
      a=flex.max(y[lower_slot:upper_slot]), b=mean, c=3)]
   # print starting_gaussians
    #mamin: fit gaussian will take the maximum between starting point (lower_slot) and ending (upper_slot) as a
    if zero_peak_gaussian is not None:
      y -= zero_peak_gaussian(x)
    if 1:
      fit = curve_fitting.lbfgs_minimiser(
        starting_gaussians, x[lower_slot:upper_slot], y[lower_slot:upper_slot])
      sigma = abs(fit.functions[0].params[2])
      if sigma < 1 or sigma > 10:
        if flex.sum(y[lower_slot:upper_slot]) < 15: #mamin I changed 15 to 5
          # No point wasting time attempting to fit a gaussian if there aren't any counts
          #raise PixelFitError("Not enough counts to fit gaussian")
          return fit
        print("using cma_es:", sigma)
        fit = curve_fitting.cma_es_minimiser(
          starting_gaussians, x[lower_slot:upper_slot], y[lower_slot:upper_slot])
    else:
      fit = curve_fitting.cma_es_minimiser(
        starting_gaussians, x[lower_slot:upper_slot], y[lower_slot:upper_slot])
    return fit

  def pixels(self):
    for pixel in self.histograms.keys():
      yield pixel

def sliding_average(y, n=3):
  assert n % 2 == 1 # n must be odd
  n_either_side = n//2
  summed = y[n_either_side:-n_either_side]
  for i in range(n_either_side):
    summed += y[n_either_side-i-1:-(n_either_side+i+1)]
    end = -(n_either_side-i-1)
    if end == 0: end = None
    summed += y[(n_either_side+i+1):end]
  averaged = summed / n
  for i in range(n_either_side):
    averaged.insert(i, y[i])
  for i in range(n_either_side, 0, -1):
    averaged.append(y[-i])
  return averaged

def hist_outline(hist):

  step_size = hist.slot_width()
  half_step_size = 0.5 * step_size
  n_slots = len(hist.slots())

  bins = flex.double(n_slots * 2 + 2, 0)
  data = flex.double(n_slots * 2 + 2, 0)
  for i in range(n_slots):
    bins[2 * i + 1] = hist.slot_centers()[i] - half_step_size
    bins[2 * i + 2] = hist.slot_centers()[i] + half_step_size
    data[2 * i + 1] = hist.slots()[i]
    data[2 * i + 2] = hist.slots()[i]

  bins[0] = bins[1] - step_size
  bins[-1] = bins[-2] + step_size
  data[0] = 0
  data[-1] = 0

  return (bins, data)


class PixelFitError(RuntimeError):
  pass

def check_pixel_histogram_fit(hist, gaussians):
  assert gaussians is not None
  #if gaussians is None:
    ## Presumably the peak fitting failed in some way
    #print "Skipping pixel %s" %str(pixel)
    #continue
  zero_peak_diff = gaussians[0].params[1]
  if len(gaussians) < 2:
    raise PixelFitError("Only one gaussian!")
  y_obs = hist.slots().as_double()
  x = hist.slot_centers()
  y_calc = flex.double(y_obs.size(), 0)
  for g in gaussians:
    y_calc += g(x)
  residual = y_obs - y_calc

  # check the overall residual
  if flex.max(residual)/flex.sum(hist.slots()) > 0.015:
    raise PixelFitError("Bad fit residual: %f" %(flex.max(residual)/flex.sum(hist.slots())))

  # check the residual around the zero photon peak
  zero_gaussian = gaussians[0]
  selection = ((x < zero_gaussian.params[1] + 1 * zero_gaussian.sigma))
  #if ((flex.max(residual.select(selection))/flex.sum(hist.slots()) > 0.008)
      #or (flex.min(residual.select(selection))/flex.sum(hist.slots()) < -0.0067)):
    #raise PixelFitError("Bad fit residual around zero photon peak")

  # check the residual around the one photon peak
  one_gaussian = gaussians[1]
  selection = ((x > one_gaussian.params[1] - 1.4 * one_gaussian.sigma)) # &
  if selection.count(True) == 0:
    raise PixelFitError("Bad fit residual around one photon peak")
  max_residual_sel = flex.max(residual.select(selection))
  if max_residual_sel > 20 and max_residual_sel > 1.2 * one_gaussian.params[0]:
    raise PixelFitError("Bad fit residual: %f" %max_residual_sel)

  gain = gaussians[1].params[1] - gaussians[0].params[1]
  if 0 and estimated_gain is not None and abs(gain - estimated_gain) > 0.5 * estimated_gain:
    print("bad gain!!!!!", pixel, gain)
  #elif (one_gaussian.sigma / zero_gaussian.sigma) > 1.9:
    #raise PixelFitError("Bad sigma ratio: %.1f, %.1f" %(one_gaussian.sigma, zero_gaussian.sigma))
  elif gaussians[1].sigma < (0.5 * gaussians[0].sigma):
    raise PixelFitError("Bad sigma: %f" %gaussians[1].sigma)
  elif gain < (4 * gaussians[0].sigma):
    raise PixelFitError("Bad gain: %f" %gain)
  elif gain > (20 * gaussians[0].sigma): # XXX is 20 to low?
    raise PixelFitError("Bad gain: %f" %gain)

if __name__ == '__main__':
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/weather.py
from __future__ import division
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.weather

from xfel.util.weather import params_from_phil, run, message
import sys

if __name__ == '__main__':
  if '--help' in sys.argv[1:] or '-h' in sys.argv[1:]:
    print(message)
    exit()
  params = params_from_phil(sys.argv[1:])
  run(params)


 *******************************************************************************


 *******************************************************************************
xfel/command_line/xes_finalise.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.xes_finalise
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT

import os
import sys

from libtbx.option_parser import option_parser
from xfel.cxi.cspad_ana.xes_finalise import xes_finalise

if (__name__ == "__main__"):
  args = sys.argv[1:]
  assert len(args) > 0
  command_line = (option_parser()
                  .option("--roi",
                          type="string",
                          help="Region of interest for summing up spectrum.")
                  .option("--output_dirname", "-o",
                          type="string",
                          help="Directory for output files.")
                  ).process(args=args)
  roi = command_line.options.roi
  output_dirname = command_line.options.output_dirname
  runs = command_line.args
  if output_dirname is None:
    output_dirname = os.path.join(runs[0], "finalise")
  print("Output directory: %s" %output_dirname)
  if not len(runs) > 0:
    print("Usage: cxi.finalise [-o result directory] [data directories as r0xxx/nnn ...]")
  xes_finalise(runs, output_dirname=output_dirname, roi=roi)


 *******************************************************************************


 *******************************************************************************
xfel/command_line/xes_histograms.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME xes.histograms
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT

import sys

from xfel.cxi.cspad_ana import xes_histograms

if __name__ == '__main__':
  xes_histograms.run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/xfel_process.py
#!/usr/bin/env python
#
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.process

from __future__ import absolute_import, division, print_function
from libtbx.mpi4py import mpi_abort_on_exception

help_message = '''

See dials.stills_process. This version adds mysql database logging for each event.

'''

from dials.util import show_mail_on_error
from libtbx.phil import parse
control_phil_str = '''
  input {
    run_num = None
      .type = str
      .help = Run name (not necessarily a number)
    trial = None
      .type = int
      .help = Trial number for this run.
    rungroup = None
      .type = int
      .help = Useful for organizing runs with similar parameters into logical \
              groupings.
   }
'''

delete_shoeboxes_override_str = '''
  integration {
    debug {
      delete_shoeboxes = True
        .type = bool
        .help = "Delete shoeboxes immediately before saving files. This option"
                "in combination with debug.output=True enables intermediate"
                "processing steps to make use of shoeboxes."
    }
  }
'''

radial_average_phil_str = '''
  radial_average {
    enable = False
      .type = bool
      .help = If True, perform a radial average on each image
    two_theta_low = None
      .type = float
      .help = If not None and database logging is enabled, for each image \
              compute the radial average at this two theta position and log \
              it in the database
    two_theta_high = None
      .type = float
      .help = If not None and database logging is enabled, for each image \
              compute the radial average at this two theta position and log \
              it in the database
    include scope dxtbx.command_line.radial_average.master_phil
  }
'''

from xfel.ui.db.frame_logging import DialsProcessorWithLogging
from dials.command_line.stills_process import dials_phil_str, program_defaults_phil_str, Script as DialsScript, control_phil_str as dials_control_phil_str
from xfel.ui import db_phil_str

phil_scope = parse(dials_control_phil_str + control_phil_str + dials_phil_str + db_phil_str + radial_average_phil_str, process_includes=True).fetch(parse(program_defaults_phil_str))
phil_scope = phil_scope.fetch(parse(delete_shoeboxes_override_str))

class Script(DialsScript):
  '''A class for running the script.'''
  def __init__(self):
    '''Initialise the script.'''
    from dials.util.options import ArgumentParser
    import libtbx.load_env

    # The script usage
    usage = "usage: %s [options] [param.phil] filenames" % libtbx.env.dispatcher_name

    self.tag = None
    self.reference_detector = None

    # Create the parser
    self.parser = ArgumentParser(
      usage=usage,
      phil=phil_scope,
      epilog=help_message
      )

  @mpi_abort_on_exception
  def run(self):
    super().run()



if __name__ == '__main__':
  import dials.command_line.stills_process
  dials.command_line.stills_process.Processor = DialsProcessorWithLogging

  with show_mail_on_error():
    script = Script()
    script.run()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/xpp_isoform.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# LIBTBX_SET_DISPATCHER_NAME xpp.isoform
#

from __future__ import absolute_import, division, print_function
import iotbx.phil
import sys

from xfel.command_line.experiment_manager import master_phil as ex_master_phil
master_phil=ex_master_phil + """

isoform
  .help=Constrain the unit cell to specific values during refinement
  .help=As presently implemented, applies only to dials_refinement_preceding_integration
  .help=and applies only to the higher-symmetry integration trial, not the initial triclinic
  .multiple=False
  {
    name=None
      .type=str
    cell=None
      .type=unit_cell
    lookup_symbol=None
      .type=str
      .help=The sgtbx lookup symbol of the reflections pointgroup
    resolution_limit=10.
      .type = float
      .help = an outer resolution limit considerably beyond anything possible in this experiment
  }
"""

#-----------------------------------------------------------------------
def run(args):
  phil = iotbx.phil.process_command_line(args=args, master_string=master_phil).show()
  work_params = phil.work.extract()
  from xfel.xpp.isoform import phil_validation
  phil_validation(work_params)
  if ("--help" in args) :
    libtbx.phil.parse(master_phil.show())
    return

  from xfel.xpp.isoform import application
  application(work_params)

if (__name__ == "__main__"):
  result = run(args=sys.argv[1:])
  if result is None:
    sys.exit(1)

# typical usage for experiment LI61:
# xpp.isoform experiment=xppi6115 experiment_tag=xppi6115 db.user=xxxx isoform.name=A isoform.cell=1,2,3,90,90,90 isoform.lookup_symbol="P m m m"


 *******************************************************************************


 *******************************************************************************
xfel/command_line/xpp_progress_bar.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# LIBTBX_SET_DISPATCHER_NAME xpp.progress_bar
#
'''
This is a wxpython GUI application for reading a database and
displaying a plot showing multiplicity by trial.
Currently it features:
* contains the matplotlib navigation toolbar
* allows user input of desired experiment trial number
* finds the multiplicity of data currently in the trial
* Displays this information graphically
* automatically updates the plot on a timer
* The plot can be saved to a file from the file menu

'''
from __future__ import absolute_import, division, print_function
import os
import wx
import numpy as np
import sys
import time
import threading
# The recommended way to use wx with mpl is with the WXAgg
# backend.
#
import matplotlib
matplotlib.use('WXAgg')
from matplotlib.figure import Figure
from matplotlib.backends.backend_wxagg import \
    FigureCanvasWxAgg as FigCanvas, \
    NavigationToolbar2WxAgg as NavigationToolbar

from xfel.command_line.xpp_progress_detail import master_phil
import iotbx.phil

myEVT_DB_STATS = wx.NewEventType()
EVT_DB_STATS = wx.PyEventBinder(myEVT_DB_STATS, 1)
class StatsEvent(wx.PyCommandEvent):
  """Event to signal that a count value is ready"""
  def __init__(self, etype, eid, value=None):
      """Creates the event object"""
      wx.PyCommandEvent.__init__(self, etype, eid)
      self._value = value

  def GetValue(self):
      """Returns the value from the event.
      @return: the value of this event
      """
      return self._value

class DB_thread(threading.Thread):
    def __init__(self, parent):
       """
         @param parent: The gui object that should recieve the value
         @param value: value to 'calculate' to

       """
       threading.Thread.__init__(self)
       self.parent = parent

    def run(self):
       while True:
         from xfel.xpp.progress_utils import application
         stats = application(self.parent.params, loop = False)
         evt = StatsEvent(myEVT_DB_STATS, -1, stats)
         wx.PostEvent(self.parent, evt)
         import time
         time.sleep(5)

class BarsFrame(wx.Frame):
    """ The main frame of the application
    """
    title = 'Multiplicity Progress'

    def __init__(self, params):
        wx.Frame.__init__(self, None, -1, self.title)
        self.params = params
        self.stats = None

        self.create_menu()
        self.create_status_bar()
        self.create_main_panel()


        #set up the thread
        self.Bind(EVT_DB_STATS, self.on_stats_update)
        self.worker = DB_thread(self)
        self.worker.start()

    def create_menu(self):
        self.menubar = wx.MenuBar()

        menu_file = wx.Menu()
        m_expt = menu_file.Append(-1, "&Save plot\tCtrl-S", "Save plot to file")
        self.Bind(wx.EVT_MENU, self.on_save_plot, m_expt)
        menu_file.AppendSeparator()
        m_exit = menu_file.Append(-1, "E&xit\tCtrl-X", "Exit")
        self.Bind(wx.EVT_MENU, self.on_exit, m_exit)

        menu_help = wx.Menu()
        m_help = menu_help.Append(-1, "&About\tF1", "About the data")
        self.Bind(wx.EVT_MENU, self.on_about, m_help)

        self.menubar.Append(menu_file, "&File")
        self.menubar.Append(menu_help, "&Help")
        self.SetMenuBar(self.menubar)

    def create_main_panel(self):
        """ Creates the main panel with all the controls on it:
             * mpl canvas
             * mpl navigation toolbar
             * Control panel for interaction
        """
        self.panel = wx.Panel(self)

        # Create the mpl Figure and FigCanvas objects.
        # 10.5x5.5 inches, 100 dots-per-inch
        #
        self.dpi = 100
        self.fig = Figure((10.5, 5.5), dpi=self.dpi)
        self.canvas = FigCanvas(self.panel, -1, self.fig)

        # set up trial text box
        self.textbox = wx.TextCtrl(self.panel, -1, '')
        if self.params.trial is not None:
          self.textbox.SetValue(str(self.params.trial))
        self.fnameLabel = wx.StaticText(self.panel, wx.ID_ANY, 'Trial Number ')
        self.fnameLabel.SetFont(wx.Font (12, wx.SWISS, wx.NORMAL, wx.BOLD))

        # set up resolution text box
        self.restextbox = wx.TextCtrl(self.panel, -1, '')
        if self.params.resolution is not None:
          self.restextbox.SetValue(str(self.params.resolution))
        self.resLabel = wx.StaticText(self.panel, wx.ID_ANY, 'Resolution Shell')
        self.resLabel.SetFont(wx.Font (12, wx.SWISS, wx.NORMAL, wx.BOLD))

        # set up tags text box
        # this determines which crystal isoforms in which illumination stats are plotted
        self.tagstextbox = wx.TextCtrl(self.panel, -1, '')
        if self.params.run_tags is not None:
          self.tagstextbox.SetValue(self.params.run_tags)
        self.tagsLabel = wx.StaticText(self.panel, wx.ID_ANY, 'Tags')
        self.tagsLabel.SetFont(wx.Font (12, wx.SWISS, wx.NORMAL, wx.BOLD))

        # set up the submit button
        self.SubmitButton = wx.Button(self.panel, wx.ID_ANY, 'Submit')
        self.Bind(wx.EVT_BUTTON, self.on_submit, self.SubmitButton)

        # Since we have only one plot, we can use add_axes
        # instead of add_subplot, but then the subplot
        # configuration tool in the navigation toolbar wouldn't
        # work.
        #
        self.axes = self.fig.add_subplot(111)
        #

        # Create the navigation toolbar, tied to the canvas
        #
        self.toolbar = NavigationToolbar(self.canvas)

        #
        # Layout with box sizers
        #
        self.vbox = wx.BoxSizer(wx.VERTICAL)
        self.vbox.Add(self.canvas, 1, wx.LEFT | wx.TOP | wx.GROW )
        self.vbox.Add(self.toolbar, 0, wx.EXPAND)
        self.vbox.AddSpacer(10)

        self.hbox = wx.BoxSizer(wx.HORIZONTAL)
        flags = wx.ALIGN_LEFT | wx.ALL | wx.ALIGN_CENTER_VERTICAL

        self.DataSizer = wx.BoxSizer(wx.HORIZONTAL)

        # create a box for entering the trial number
        self.DataSizer.Add(self.fnameLabel, 0 , wx.ALL | wx.ALIGN_CENTER, 5)
        self.DataSizer.Add(self.textbox, 1, wx.ALL | wx.ALIGN_CENTER , 5)

        # create a box for entering the resolution shell
        self.DataSizer.Add(self.resLabel, 0, wx.ALIGN_CENTER | wx.ALL, 5)
        self.DataSizer.Add(self.restextbox, 1, wx.ALIGN_CENTER| wx.ALL, 5)

        self.DataSizer.Add(self.tagsLabel, 0, wx.ALIGN_CENTER | wx.ALL, 5)
        self.DataSizer.Add(self.tagstextbox, 1, wx.ALIGN_CENTER| wx.ALL, 5)

        self.DataSizer.Add(self.SubmitButton, 0, wx.ALL | wx.EXPAND , 5)

        self.vbox.Add(self.DataSizer, 0 , wx.ALL |wx.CENTER, 5)

        self.panel.SetSizer(self.vbox)
        self.vbox.Fit(self)

    def create_status_bar(self):
        self.statusbar = self.CreateStatusBar()

    def draw_figure(self):
        """ Redraws the figure
        """
        stats = self.stats
        if stats is None: return
        if len(stats) == 0:
          return

        print(stats)
        res = self.restextbox.GetValue()
        trial = self.textbox.GetValue()
        self.mult_highest = [stats[key]['multiplicity_highest'] for key in stats.keys()]
        self.mult = [stats[key]['multiplicity'] for key in stats.keys()]
        plot_max = max(max(self.mult), max(self.mult_highest)) + 1
        pos = np.arange(len(self.mult))+0.5 # the bar centers on the y-axis
        labels = list(stats.keys())
        n = len(labels)
        # clear the axes and redraw the plot anew
        #
        self.axes.clear()
        self.axes.barh(
            pos,
            self.mult,
            align='center',
            color='lightskyblue')
        self.axes.barh(
            pos,
            self.mult_highest,
            align='center',
            color='blue')
        title = 'Trial: %(tr)s Overall multplicity (light blue) and multiplicity at %(ang)s angstroms (blue).\n' \
          % {"tr": trial, "ang": res} \
          + 'Updated %(HH)02d:%(MM)02d:%(SS)02d' \
          % {"HH":time.localtime().tm_hour, "MM":time.localtime().tm_min, "SS":time.localtime().tm_sec}
        self.axes.set_title(title)
        self.axes.set_yticks(pos)
        self.axes.set_yticklabels(labels)
        self.axes.set_xlabel('Multiplicity')
        self.axes.set_xlim(0.0,plot_max)
        self.canvas.draw()

    def on_submit(self, event):
        self.params.trial = int(self.textbox.GetValue())
        self.params.resolution = float(self.restextbox.GetValue())
        self.params.run_tags = self.tagstextbox.GetValue()

    def on_stats_update(self,event):
        self.stats = event.GetValue()
        self.draw_figure()

    def on_save_plot(self, event):
        file_choices = "PNG (*.png)|*.png"

        dlg = wx.FileDialog(
            self,
            message="Save plot as...",
            defaultDir=os.getcwd(),
            defaultFile="plot.png",
            wildcard=file_choices,
            style=wx.SAVE)

        if dlg.ShowModal() == wx.ID_OK:
            path = dlg.GetPath()
            self.canvas.print_figure(path, dpi=self.dpi)
            self.flash_status_message("Saved to %s" % path)

    def on_exit(self, event):
        self.redraw_timer.Stop()
        self.Destroy()

    def on_about(self, event):
        msg = """ A progress plot using wxPython with matplotlib:

         * Use the text box to enter experiment trial number
         * Save the plot to a file using the File menu

        """
        dlg = wx.MessageDialog(self, msg, "About", wx.OK)
        dlg.ShowModal()
        dlg.Destroy()

    def flash_status_message(self, msg, flash_len_ms=1500):
        self.statusbar.SetStatusText(msg)
        self.timeroff = wx.Timer(self)
        self.Bind(
            wx.EVT_TIMER,
            self.on_flash_status_off,
            self.timeroff)
        self.timeroff.Start(flash_len_ms, oneShot=True)

    def on_flash_status_off(self, event):
        self.statusbar.SetStatusText('')

if __name__ == '__main__':
  args = sys.argv[1:]
  phil = iotbx.phil.process_command_line(args=args, master_string=master_phil).show()
  work_params = phil.work.extract()
  from xfel.xpp.progress_utils import phil_validation
  phil_validation(work_params)
  if ("--help" in args) :
    libtbx.phil.parse(master_phil.show())
  else:
    app = wx.PySimpleApp()
    app.frame = BarsFrame(work_params)
    app.frame.Show()
    app.MainLoop()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/xpp_progress_detail.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# LIBTBX_SET_DISPATCHER_NAME xpp.progress_detail
#
# $Id: cxi_merge.py 22906 2015-09-15 22:32:08Z phyy-nx $

from __future__ import absolute_import, division, print_function
import iotbx.phil
import sys

master_phil="""
experiment = None
  .type = str
experiment_tag = None
  .type = str
db {
  host = None
    .type=str
  name = None
    .type = str
  user = None
    .type=str
  password = None
    .type = str
}
trial = None
  .type = int
resolution = 2.5
  .type = float
n_bins = 15
  .type = int
run_tags = None
  .type = str
include_negatives = False
  .type = bool
"""
#xpp.progress_detail db.host= db.user= data=xppi6115 trial=12
#-----------------------------------------------------------------------
def run(args):
  phil = iotbx.phil.process_command_line(args=args, master_string=master_phil).show()
  work_params = phil.work.extract()
  from xfel.xpp.progress_utils import phil_validation
  phil_validation(work_params)
  if ("--help" in args) :
    libtbx.phil.parse(master_phil.show())
    return

  from xfel.xpp.progress_utils import application
  application(work_params)

if (__name__ == "__main__"):
  result = run(args=sys.argv[1:])
  if result is None:
    sys.exit(1)

# typical usage for experiment LI61:
# xpp.progress_detail password=XXXXXXXX db.host=psdb db.user=xppi6115 data=xppi6115 trial=12


 *******************************************************************************


 *******************************************************************************
xfel/command_line/xpp_simulate.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# LIBTBX_SET_DISPATCHER_NAME xpp.simulate
#
# $Id: cxi_merge.py 22906 2015-09-15 22:32:08Z phyy-nx $

from __future__ import absolute_import, division, print_function
import iotbx.phil
import sys

master_phil="""
data = None
  .type = str
  .multiple = True
  .help = LCLS experiment, like xppi6113
web {
  user = None
    .type = str
  password = None
    .type = str
}
speedup {
  factor = 100
    .type = float
}
runlimits = None
  .type = ints(2)
enforce80 = False
  .type = bool
  .help = report only on stream 80, FEE spectrometer
enforce81 = False
  .type = bool
  .help = report only on stream 81, FEE spectrometer
"""

#-----------------------------------------------------------------------
def run(args):
  phil = iotbx.phil.process_command_line(args=args, master_string=master_phil).show()
  work_params = phil.work.extract()
  from xfel.xpp.simulate import phil_validation
  phil_validation(work_params)
  if ("--help" in args) :
    libtbx.phil.parse(master_phil.show())
    return

  from xfel.xpp.simulate import application
  application(work_params)

if (__name__ == "__main__"):
  result = run(args=sys.argv[1:])
  if result is None:
    sys.exit(1)

# typical usage for experiment LI61:
# xpp.simulate web.user=<LCLSuser> web.password=XXXXXXXX data=xppi6115 runlimits=86,144


 *******************************************************************************


 *******************************************************************************
xfel/command_line/xtc_dump.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.xtc_dump
#
import errno
import psana
from xfel.cftbx.detector import cspad_cbf_tbx
from xfel.cxi.cspad_ana import cspad_tbx, rayonix_tbx
import os, sys
import libtbx.load_env
from libtbx.utils import Sorry, Usage
from dials.util import show_mail_on_error
from dials.util.options import ArgumentParser
from libtbx.phil import parse
from libtbx import easy_pickle

phil_scope = parse('''
  dispatch {
    max_events = None
      .type = int
      .help = If not specified, process all events. Otherwise, only process this many
    selected_events = False
      .type = bool
      .help = If True, only dump events specified in input.event scopes
  }
  input {
    cfg = None
      .type = str
      .help = Path to psana config file. Genearlly not needed for CBFs. For image pickles, \
              the psana config file should have a mod_image_dict module.
    experiment = None
      .type = str
      .help = Experiment identifier, e.g. cxi84914
    run_num = None
      .type = int
      .help = Run number or run range to process
    address = None
      .type = str
      .help = Detector address, e.g. CxiDs2.0:Cspad.0 or detector alias, e.g. Ds1CsPad
    calib_dir = None
      .type = str
      .help = Non-standard calib directory location
    xtc_dir = None
      .type = str
      .help = Non-standard xtc directory location
    timestamp = None
      .type = str
      .multiple = True
      .help = Event timestamp(s) of event(s) in human-readable format of images to
      .help = dump (must also specify dispatch.selected_events=True.)
  }
  format {
    file_format = *cbf pickle
      .type = choice
      .help = Output file format, 64 tile segmented CBF or image pickle
    pickle {
      out_key = cctbx.xfel.image_dict
        .type = str
        .help = Key name that mod_image_dict uses to put image data in each psana event
    }
    cbf {
      detz_offset = None
        .type = float
        .help = Distance from back of detector rail to sample interaction region (CXI) \
                or actual detector distance (XPP/MFX)
      override_energy = None
        .type = float
        .help = If not None, use the input energy for every event instead of the energy \
                from the XTC stream
      mode = *cspad rayonix
        .type = choice
      cspad {
        gain_mask_value = None
          .type = float
          .help = If not None, use the gain mask for the run to multiply the low-gain pixels by this number
      }
      rayonix {
        bin_size = 2
          .type = int
          .help = Detector binning mode
        override_beam_x = None
          .type = float
          .help = If set, override the beam X position
        override_beam_y = None
          .type = float
          .help = If set, override the beam Y position
      }
    }
  }
  output {
    output_dir = .
      .type = str
      .help = Directory output files will be placed
    tmp_output_dir = None
      .type = str
      .help = Directory for CBFlib tmp output files
  }
''', process_includes=True)

class Script(object):
  """ Script to process dump XFEL data at LCLS """
  def __init__(self):
    """ Set up the option parser. Arguments come from the command line or a phil file """
    self.usage = """
%s input.experiment=experimentname input.run_num=N input.address=address
 format.file_format=cbf format.cbf.detz_offset=N
%s input.experiment=experimentname input.run_num=N input.address=address
 format.file_format=pickle format.pickle.cfg=path
    """%(libtbx.env.dispatcher_name, libtbx.env.dispatcher_name)

    self.parser = ArgumentParser(
      usage = self.usage,
      phil = phil_scope)

  def run(self):
    """ Process all images assigned to this thread """
    params, options = self.parser.parse_args(
      show_diff_phil=True)

    if params.input.experiment is None or \
       params.input.run_num is None or \
       params.input.address is None:
      raise Usage(self.usage)

    if params.format.file_format == "cbf":
      if params.format.cbf.detz_offset is None:
        raise Usage(self.usage)
    elif params.format.file_format == "pickle":
      if params.input.cfg is None:
        raise Usage(self.usage)
    else:
      raise Usage(self.usage)

    if not os.path.exists(params.output.output_dir):
      raise Sorry("Output path not found:" + params.output.output_dir)

    #Environment variable redirect for CBFLib temporary CBF_TMP_XYZ file output
    if params.format.file_format == "cbf":
      if params.output.tmp_output_dir is None:
        tmp_dir = os.path.join(params.output.output_dir, '.tmp')
      else:
        tmp_dir = os.path.join(params.output.tmp_output_dir, '.tmp')
      if not os.path.exists(tmp_dir):
        with show_mail_on_error():
          try:
            os.makedirs(tmp_dir)
            # Can fail if running multiprocessed - that's OK if the folder was created
          except OSError as e:  # In Python 2, a FileExistsError is just an OSError
            if e.errno != errno.EEXIST:  # If this OSError is not a FileExistsError
              raise
      os.environ['CBF_TMP_DIR'] = tmp_dir

    # Save the paramters
    self.params = params
    self.options = options

    from libtbx.mpi4py import MPI
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank() # each process in MPI has a unique id, 0-indexed
    size = comm.Get_size() # size: number of processes running in this job

    # set up psana
    if params.input.cfg is not None:
      psana.setConfigFile(params.input.cfg)

    if params.input.calib_dir is not None:
      psana.setOption('psana.calib-dir',params.input.calib_dir)

    dataset_name = "exp=%s:run=%s:idx"%(params.input.experiment,params.input.run_num)
    if params.input.xtc_dir is not None:
      dataset_name = "exp=%s:run=%s:idx:dir=%s"%(params.input.experiment,params.input.run_num,params.input.xtc_dir)

    ds = psana.DataSource(dataset_name)

    if params.format.file_format == "cbf":
      src = psana.Source('DetInfo(%s)'%params.input.address)
      psana_det = psana.Detector(params.input.address, ds.env())

    # set this to sys.maxint to analyze all events
    if params.dispatch.max_events is None:
      max_events = sys.maxsize
    else:
      max_events = params.dispatch.max_events

    for run in ds.runs():
      if params.format.file_format == "cbf":
        if params.format.cbf.mode == "cspad":
          # load a header only cspad cbf from the slac metrology
          base_dxtbx = cspad_cbf_tbx.env_dxtbx_from_slac_metrology(run, params.input.address)
          if base_dxtbx is None:
            raise Sorry("Couldn't load calibration file for run %d"%run.run())
        elif params.format.cbf.mode == "rayonix":
          # load a header only rayonix cbf from the input parameters
          detector_size = rayonix_tbx.get_rayonix_detector_dimensions(ds.env())
          base_dxtbx = rayonix_tbx.get_dxtbx_from_params(params.format.cbf.rayonix, detector_size)

      # list of all events
      times = run.times()
      if params.dispatch.selected_events:
        times = [t for t in times if cspad_tbx.evt_timestamp((t.seconds(),t.nanoseconds()/1e6)) in params.input.timestamp]
      nevents = min(len(times),max_events)
      # chop the list into pieces, depending on rank.  This assigns each process
      # events such that the get every Nth event where N is the number of processes
      mytimes = [times[i] for i in range(nevents) if (i+rank)%size == 0]

      for i in range(len(mytimes)):
        evt = run.event(mytimes[i])
        id = evt.get(psana.EventId)
        print("Event #",i," has id:",id)

        timestamp = cspad_tbx.evt_timestamp(cspad_tbx.evt_time(evt)) # human readable format
        if timestamp is None:
          print("No timestamp, skipping shot")
          continue

        if evt.get("skip_event") or "skip_event" in [key.key() for key in evt.keys()]:
          print("Skipping event",timestamp)
          continue

        t = timestamp
        s = t[0:4] + t[5:7] + t[8:10] + t[11:13] + t[14:16] + t[17:19] + t[20:23]
        print("Processing shot", s)

        if params.format.file_format == "pickle":
          if evt.get("skip_event"):
            print("Skipping event",id)
            continue
          # the data needs to have already been processed and put into the event by psana
          data = evt.get(params.format.pickle.out_key)
          if data is None:
            print("No data")
            continue

          # set output paths according to the templates
          path = os.path.join(params.output.output_dir, "shot-" + s + ".pickle")

          print("Saving", path)
          easy_pickle.dump(path, data)

        elif params.format.file_format == "cbf":
          if params.format.cbf.mode == "cspad":
            # get numpy array, 32x185x388
            data = cspad_cbf_tbx.get_psana_corrected_data(psana_det, evt, use_default=False, dark=True,
                                                          common_mode=None,
                                                          apply_gain_mask=params.format.cbf.cspad.gain_mask_value is not None,
                                                          gain_mask_value=params.format.cbf.cspad.gain_mask_value,
                                                          per_pixel_gain=False)

            distance = cspad_tbx.env_distance(params.input.address, run.env(), params.format.cbf.detz_offset)
          elif params.format.cbf.mode == "rayonix":
            data = rayonix_tbx.get_data_from_psana_event(evt, params.input.address)
            distance = params.format.cbf.detz_offset

          if distance is None:
            print("No distance, skipping shot")
            continue

          if self.params.format.cbf.override_energy is None:
            wavelength = cspad_tbx.evt_wavelength(evt)
            if wavelength is None:
              print("No wavelength, skipping shot")
              continue
          else:
            wavelength = 12398.4187/self.params.format.cbf.override_energy

          # stitch together the header, data and metadata into the final dxtbx format object
          if params.format.cbf.mode == "cspad":
            image = cspad_cbf_tbx.format_object_from_data(base_dxtbx, data, distance, wavelength, timestamp, params.input.address, round_to_int=False)
          elif params.format.cbf.mode == "rayonix":
            image = rayonix_tbx.format_object_from_data(base_dxtbx, data, distance, wavelength, timestamp, params.input.address)
          path = os.path.join(params.output.output_dir, "shot-" + s + ".cbf")
          print("Saving", path)

          # write the file
          import pycbf
          image._cbf_handle.write_widefile(path.encode(), pycbf.CBF,\
            pycbf.MIME_HEADERS|pycbf.MSG_DIGEST|pycbf.PAD_4K, 0)

      run.end()
    ds.end()

if __name__ == "__main__":
  with show_mail_on_error():
    script = Script()
    script.run()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/xtc_process.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from six.moves import zip
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.xtc_process
#

try:
  import psana
except ImportError:
  pass # for running at home without psdm build

import errno
from xfel.cftbx.detector import cspad_cbf_tbx
from xfel.cxi.cspad_ana import cspad_tbx, rayonix_tbx
import pycbf, os, sys, copy, socket
import libtbx.load_env
from libtbx.utils import Sorry, Usage
from dials.util import show_mail_on_error
from dials.util.options import ArgumentParser
from libtbx.phil import parse
from dxtbx.model.experiment_list import ExperimentListFactory
from dials.array_family import flex
import numpy as np
from libtbx import easy_pickle
from dxtbx.model.experiment_list import ExperimentList
from serialtbx import detector as serialtbx_detector

import io # fix buffering py2/3

# check version of psana
from xfel.cftbx.detector.cspad_cbf_tbx import PSANA2_VERSION

xtc_phil_str = '''
  dispatch {
    max_events = None
      .type = int
      .help = If not specified, process all events. Otherwise, only process this many
    process_percent = None
      .type = int(value_min=1, value_max=100)
      .help = Percent of events to process
    estimate_gain_only = False
      .type = bool
      .help = Use to print estimated gain parameters for each event, then exit without attempting \
              further processing.
    find_spots = True
      .type = bool
      .help = Whether to do spotfinding. Needed for indexing/integration
    datasource = None
      .type = str
      .expert_level = 2
      .help = This is to specify which datasource should be used for processing data at LCLS \
              Format is exp=<experiment_name>:run=<run_number>:<mode> \
              eg. exp=mfxo1916:run=20:xtc \
              More info at https://confluence.slac.stanford.edu/display/PSDM/Manual#Manual-Datasetspecification
    hit_finder{
      enable = True
        .type = bool
        .help = Whether to do hitfinding. hit_finder=False: process all images
      minimum_number_of_reflections = 16
        .type = int
        .help = If the number of strong reflections on an image is less than this, and \
                 the hitfinder is enabled, discard this image.
      maximum_number_of_reflections = None
       .type = int
       .help = If specified, ignores images with more than this many number of reflections
    }
    index = True
      .type = bool
      .help = Attempt to index images
    refine = False
      .type = bool
      .help = If True, after indexing, refine the experimental models
    integrate = True
      .type = bool
      .help = Integrated indexed images. Ignored if index=False
    coset = False
      .expert_level = 2
      .type = bool
      .help = Within the integrate dispatcher, integrate a sublattice coset intended to represent \
              negative control spots with no Bragg diffraction.
    dump_strong = False
      .type = bool
      .help = Save strongly diffracting images to cbf format
    dump_indexed = True
      .type = bool
      .help = Save indexed images to cbf format
    dump_all = False
      .type = bool
      .help = All frames will be saved to cbf format if set to True
    reindex_strong = False
      .type = bool
      .help = If true, after indexing and refinement, re-index the strong reflections with \
              no outlier rejection
  }
  debug
    .help = Use these flags to track down problematic events that cause unhandled exceptions. \
            Here, a bad event means it caused an unhandled exception, not that the image \
            failed to index. \
            Examples: \
            Process only unprocessed events (excluding bad events): \
              skip_processed_events=True, skip_unprocessed_events=False skip_bad_events=True \
            Process only bad events (for debugging): \
              skip_processed_events=True, skip_unprocessed_events=True skip_bad_events=False \
            Note, due to how MPI works, if an unhandled exception occurrs, some bad events \
            will be marked as bad that were simply in process when the program terminated \
            due to a bad event. Try processing only bad events single process to find the \
            culprit and alert the program authors.
  {
    skip_processed_events = False
      .type = bool
      .help = If True, will look for diagnostic files in the output directory and use \
              them to skip events that had already been processed (succesfully or not)
    skip_unprocessed_events = False
      .type = bool
      .help = If True, will look for diagnostic files in the output directory and use \
              them to skip events that had haven't been processed
    skip_bad_events = False
      .type = bool
      .help = If True, will look for diagnostic files in the output directory and use \
              them to skip events that had caused unhandled exceptions previously
    event_timestamp = None
      .type = str
      .multiple = True
      .help = List of timestamps. If set, will only process the events that match them
  }
  input {
    cfg = None
      .type = str
      .help = Path to psana config file. Genearlly not needed for CBFs. For image pickles, \
              the psana config file should have a mod_image_dict module.
    experiment = None
      .type = str
      .help = Experiment identifier, e.g. cxi84914
    run_num = None
      .type = int
      .help = Run number or run range to process
    address = None
      .type = str
      .help = Detector address, e.g. CxiDs2.0:Cspad.0, or detector alias, e.g. Ds1CsPad
    stream = None
      .type = ints
      .expert_level = 2
      .help = Stream number to read from. Usually not necessary as psana will read the data \
              from all streams by default
    override_spotfinding_trusted_max = None
      .type = int
      .help = During spot finding, override the saturation value for this data. \
              Overloads will not be integrated, but they can assist with indexing.
    override_spotfinding_trusted_min = None
      .type = int
      .help = During spot finding, override the minimum pixel value \
              for this data. This does not affect integration.
    override_integration_trusted_max = None
      .type = int
      .help = During integration, override the saturation value for this data.
    override_integration_trusted_min = None
      .type = int
      .help = During integration, override the minimum pixel value \
              for this data.
    use_ffb = False
      .type = bool
      .help = Run on the ffb if possible. Only for active users!
    xtc_dir = None
      .type = str
      .help = Optional path to data directory if it's non-standard. Only needed if xtc \
              streams are not in the standard location for your PSDM installation.
    calib_dir = None
      .type = str
      .help = Optional path to calib directory if it's non-standard. Only needed if calib \
              data are not in the standard location for your PSDM installation.
    trial = None
      .type = int
      .help = Optional. Trial number for this run.
    rungroup = None
      .type = int
      .help = Optional. Useful for organizing runs with similar parameters into logical \
              groupings.
    known_orientations_folder = None
      .type = str
      .expert_level = 2
      .help = Folder with previous processing results including crystal orientations. \
              If specified, images will not be re-indexed, but instead the known \
              orientations will be used.
    ignore_gain_mismatch = False
      .type = bool
      .expert_level = 3
      .help = Detector gain should be set on the detector models loaded from the images or in the \
              processing parameters, not both. Override the check that this is true with this flag. \
 }
  format {
    file_format = *cbf pickle
      .type = choice
      .help = Output file format, 64 tile segmented CBF or image pickle
    pickle {
      out_key = cctbx.xfel.image_dict
        .type = str
        .help = Key name that mod_image_dict uses to put image data in each psana event
    }
    cbf {
      detz_offset = None
        .type = float
        .help = Distance from back of detector rail to sample interaction region (CXI) \
                or actual detector distance (XPP/MFX)
      override_energy = None
        .type = float
        .help = If not None, use the input energy for every event instead of the energy \
                from the XTC stream
      override_distance = None
        .type = float
        .help = If not None, use the input distance for every event instead of the distance \
                from the XTC stream
      invalid_pixel_mask = None
        .type = str
        .help = Path to invalid pixel mask, in the dials.generate_mask format. If not set, use the \
                psana computed invalid pixel mask. Regardless, pixels outside of the trusted range \
                for each image will also be masked out. See cxi.make_dials_mask.
      mode = *cspad rayonix
        .type = choice
        .help = CBFs output in the designated mode
      cspad {
        mask_nonbonded_pixels = False
          .type = bool
          .help = If true, try to get non-bonded pixels from psana calibrations and apply them. Includes \
                  the 4 pixels on each side of each pixel. Only used if a custom invalid_pixel_mask is \
                  provided (otherwise the psana calibration will mask these out automatically).
        gain_mask_value = None
          .type = float
          .help = If not None, use the gain mask for the run to multiply the low-gain pixels by this number
        per_pixel_gain = False
          .type = bool
          .help = If True, use a per pixel gain from the run's calib folder, if available
        additional_gain_factor = None
          .type = float
          .help = If set, pixels counts are divided by this number after all other corrections.
        common_mode {
          algorithm = default custom
            .type = choice
            .help = Choice of SLAC's common mode correction algorithms. If not specified, use no common \
                    mode correction, only dark pedestal subtraction. Default: use the default common_mode \
                    correction. Custom, see \
                    https://confluence.slac.stanford.edu/display/PSDM/Common+mode+correction+algorithms
          custom_parameterization = None
            .type = ints
            .help = Parameters to control SLAC's common mode correction algorithms. Should be None if \
                    common_mode.algorithm is default or None.  See \
                    https://confluence.slac.stanford.edu/display/PSDM/Common+mode+correction+algorithms
        }
      }
      rayonix {
        bin_size = 2
          .type = int
          .help = Detector binning mode
        override_beam_x = None
          .type = float
          .help = If set, override the beam X position
        override_beam_y = None
          .type = float
          .help = If set, override the beam Y position
      }
    }
    per_pixel_absorption_correction
      .multiple = True {
      apply = False
        .type = bool
      algorithm = *fuller_kapton
        .type = choice
      fuller_kapton {
        xtal_height_above_kapton_mm {
          value = 0.02
            .type = float
            .help = height of the beam (or the irradiated crystal) above the kapton tape
        }
        rotation_angle_deg {
          value = 1.15
            .type = float
            .help = angle of the tape from vertical
        }
        kapton_half_width_mm {
          value = 1.5875
            .type = float
            .help = forward distance from irradiated crystal to edge of tape nearest detector
        }
        kapton_thickness_mm {
          value = 0.05
            .type = float
            .help = tape thickness
        }
      }
    }
  }
  output {
    output_dir = .
      .type = str
      .help = Directory output files will be placed
    composite_output = True
      .type = bool
      .help = If True, save one set of json/pickle files per process, where each is a \
              concatenated list of all the successful events examined by that process. \
              If False, output a separate json/pickle file per image (generates a \
              lot of files).
    delete_integration_shoeboxes = True
      .type = bool
      .help = Delete integration shoeboxes when finished with each image.
    logging_dir = None
      .type = str
      .help = Directory output log files will be placed
    experiments_filename = %s.expt
      .type = str
      .help = The filename for output experiment list
    strong_filename = %s_strong.refl
      .type = str
      .help = The filename for strong reflections from spot finder output.
    indexed_filename = %s_indexed.refl
      .type = str
      .help = The filename for indexed reflections.
    refined_experiments_filename = %s_refined.expt
      .type = str
      .help = The filename for saving refined experimental models
    integrated_filename = %s_integrated.refl
      .type = str
      .help = The filename for final experimental modls
    integrated_experiments_filename = %s_integrated.expt
      .type = str
      .help = The filename for final integrated reflections.
    coset_filename = %s_coset%d.refl
      .type = str
      .help = The filename for final coset reflections.
    coset_experiments_filename = %s_coset%d.expt
      .type = str
      .help = The filename for saving final coset experimental models.
    profile_filename = None
      .type = str
      .help = The filename for output reflection profile parameters
    integration_pickle = int-%d-%s.pickle
      .type = str
      .help = Filename for cctbx.xfel-style integration pickle files
    reindexedstrong_filename = %s_reindexedstrong.refl
      .type = str
      .help = The file name for re-indexed strong reflections
    tmp_output_dir = "(NONE)"
      .type = str
      .help = Directory for CBFlib temporary output files
  }
  mp {
    method = *mpi sge
      .type = choice
      .help = Muliprocessing method
    mpi {
      method = *client_server striping
        .type = choice
        .help = Method of serving data to child processes in MPI. client_server:    \
                use one process as a server that sends timestamps to each process.  \
                All processes will stay busy at all times at the cost of MPI send/  \
                recieve overhead. striping: each process uses its rank to determine \
                which events to process. Some processes will finish early and go    \
                idle, but no MPI overhead is incurred.
    }
    composite_stride = None
      .type = int
      .help = For MPI, if using composite mode, specify how many ranks to    \
              aggregate data from.  For example, if you have 100 processes,  \
              composite mode will output N*100 files, where N is the number  \
              of file types (json, pickle, etc). If you specify stride = 25, \
              then each group of 25 process will send their results to 4     \
              processes and only N*4 files will be created. Ideally, match   \
              stride to the number of processors per node.
  }
'''

from dials.command_line.stills_process import dials_phil_str, program_defaults_phil_str

extra_dials_phil_str = '''
  border_mask {
    include scope dials.util.masking.phil_scope
  }

  joint_reintegration {
    enable = False
      .type = bool
      .help = If enabled, after processing the data, do a joint refinement and \
              re-integration
    minimum_results = 30
      .type = int
      .help = Minimum number of integration results needed for joint reintegration
    maximum_results_per_chunk = 500
      .type = int

    include scope dials.algorithms.refinement.refiner.phil_scope
    include scope dials.algorithms.integration.integrator.phil_scope
  }
'''

def filter(evt):
    return True

def run_psana2(ims, params, comm):
    """" Begins psana2
    This setup a DataSource psana2 style. The parallelization is determined within
    the generation of the DataSource.

    ims: InMemScript (cctbx driver class)
    params: input parameters
    comm: mpi comm for broadcasting per run calibration files"""
    ds = psana.DataSource(exp=params.input.experiment, run=params.input.run_num, \
            dir=params.input.xtc_dir, max_events=params.dispatch.max_events, \
            det_name=params.input.address)

    for run in ds.runs():
      det = run.Detector(ds.det_name)
      # broadcast cctbx per run calibration
      if comm.Get_rank() == 0:
        PS_CALIB_DIR = os.environ.get('PS_CALIB_DIR')
        assert PS_CALIB_DIR
        dials_mask = easy_pickle.load(params.format.cbf.invalid_pixel_mask)
      else:
        dials_mask = None
      dials_mask = comm.bcast(dials_mask, root=0)

      for evt in run.events():
        ims.base_dxtbx = cspad_cbf_tbx.env_dxtbx_from_slac_metrology(run, params.input.address)
        ims.dials_mask = dials_mask
        ims.spotfinder_mask = None
        ims.integration_mask = None
        ims.psana_det = det
        if params.format.file_format == 'cbf':
          if params.format.cbf.cspad.common_mode.algorithm == "custom":
            ims.common_mode = params.format.cbf.cspad.common_mode.custom_parameterization
            assert ims.common_mode is not None
          else:
            ims.common_mode = params.format.cbf.cspad.common_mode.algorithm # could be None or default
        ims.process_event(run, evt)

    ims.finalize()

class EventOffsetSerializer(object):
  """ Pickles python object """
  def __init__(self,psanaOffset):
    self.filenames = psanaOffset.filenames()
    self.offsets = psanaOffset.offsets()
    self.lastBeginCalibCycleDgram = psanaOffset.lastBeginCalibCycleDgram()

from xfel.ui import db_phil_str
from xfel.command_line.xfel_process import radial_average_phil_str

phil_scope = parse(xtc_phil_str + dials_phil_str + extra_dials_phil_str + db_phil_str + radial_average_phil_str, process_includes=True).fetch(parse(program_defaults_phil_str))

from xfel.command_line.xfel_process import Script as DialsProcessScript
from xfel.ui.db.frame_logging import DialsProcessorWithLogging
from xfel.ui.db.dxtbx_db import dxtbx_xfel_db_application

class InMemScript(DialsProcessScript, DialsProcessorWithLogging):
  """ Script to process XFEL data at LCLS """
  def __init__(self):
    """ Set up the option parser. Arguments come from the command line or a phil file """
    self.usage = """
%s input.experiment=experimentname input.run_num=N input.address=address
 format.file_format=cbf format.cbf.detz_offset=N
%s input.experiment=experimentname input.run_num=N input.address=address
 format.file_format=pickle input.cfg=filename
    """%(libtbx.env.dispatcher_name, libtbx.env.dispatcher_name)
    self.parser = ArgumentParser(
      usage = self.usage,
      phil = phil_scope)

    self.debug_file_path = None
    self.debug_str = None
    self.mpi_log_file_path = None

    self.reference_detector = None

    self.composite_tag = None
    self.all_imported_experiments = None
    self.all_strong_reflections = None
    self.all_indexed_experiments = None
    self.all_indexed_reflections = None
    self.all_integrated_experiments = None
    self.all_integrated_reflections = None
    self.all_int_pickle_filenames = []
    self.all_int_pickles = []

    self.cached_ranges = None

    self.tt_low = None
    self.tt_high = None
    self.db_app = None

  def debug_start(self, ts):
    self.debug_str = "%s,%s"%(socket.gethostname(), ts)
    self.debug_str += ",%s,%s,%s\n"
    self.debug_write("start")

  def debug_write(self, string, state = None):
    ts = cspad_tbx.evt_timestamp() # Now
    debug_file_handle = open(self.debug_file_path, 'a')
    if string == "":
      debug_file_handle.write("\n")
    else:
      if state is None:
        state = "    "
      debug_file_handle.write(self.debug_str%(ts, state, string))
    debug_file_handle.close()

  def mpi_log_write(self, string):
    print(string)
    mpi_log_file_handle = open(self.mpi_log_file_path, 'a')
    mpi_log_file_handle.write(string)
    mpi_log_file_handle.close()

  def psana_mask_to_dials_mask(self, psana_mask):
    if psana_mask.dtype == np.bool:
      psana_mask = flex.bool(psana_mask)
    else:
      psana_mask = flex.bool(psana_mask == 1)
    assert psana_mask.focus() == (32, 185, 388)
    dials_mask = []
    for i in range(32):
      dials_mask.append(psana_mask[i:i+1,:,:194])
      dials_mask[-1].reshape(flex.grid(185,194))
      dials_mask.append(psana_mask[i:i+1,:,194:])
      dials_mask[-1].reshape(flex.grid(185,194))
    return dials_mask

  def run(self):
    """ Process all images assigned to this thread """

    try:
      params, options = self.parser.parse_args(
        show_diff_phil=True, quick_parse=True)
    except Exception as e:
      if "Unknown command line parameter definition" in str(e) or \
          "The following definitions were not recognised" in str(e):
        deprecated_params = ['mask_nonbonded_pixels','gain_mask_value','algorithm','custom_parameterization']
        deprecated_strs = ['%s','%s','common_mode.%s','common_mode.%s']
        for i in range(len(deprecated_params)):
          if deprecated_params[i] in str(e):
            print("format.cbf.%s"%(deprecated_strs[i]%deprecated_params[i]), "has changed to format.cbf.cspad.%s"%(deprecated_strs[i]%deprecated_params[i]))
      raise

    if params.dispatch.coset:
      self.all_coset_experiments = ExperimentList()
      self.all_coset_reflections = flex.reflection_table()

    # Check inputs
    if params.input.experiment is None or \
       params.input.run_num is None or \
       (params.input.address is None and params.format.file_format != 'pickle'):
      raise Usage(self.usage)

    if params.format.file_format == "cbf":
      if params.format.cbf.detz_offset is None:
        raise Usage(self.usage)
    elif params.format.file_format == "pickle":
      if params.input.cfg is None:
        raise Usage(self.usage)
    else:
      raise Usage(self.usage)

    if not os.path.exists(params.output.output_dir):
      raise Sorry("Output path not found:" + params.output.output_dir)

    if params.format.file_format == "cbf":
      if params.output.tmp_output_dir == "(NONE)":
        tmp_dir = params.output.tmp_output_dir
      else:
        #Environment variable redirect for CBFLib temporary CBF_TMP_XYZ file output
        if params.output.tmp_output_dir is None:
          tmp_dir = os.path.join(params.output.output_dir, '.tmp')
        else:
          tmp_dir = os.path.join(params.output.tmp_output_dir, '.tmp')
        if not os.path.exists(tmp_dir):
          with show_mail_on_error():
            try:
              os.makedirs(tmp_dir)
              # Can fail if running multiprocessed - that's OK if the folder was created
            except OSError as e:  # In Python 2, a FileExistsError is just an OSError
              if e.errno != errno.EEXIST:  # If this OSError is not a FileExistsError
                raise
      os.environ['CBF_TMP_DIR'] = tmp_dir

    for abs_params in params.integration.absorption_correction:
      if abs_params.apply and abs_params.algorithm == "fuller_kapton":
        if not (params.integration.debug.output and not params.integration.debug.separate_files):
          raise Sorry('Shoeboxes must be saved to integration intermediates to apply an absorption correction. '\
            +'Set integration.debug.output=True and integration.debug.separate_files=False to save shoeboxes.')

    self.params = params
    self.load_reference_geometry()

    if params.output.composite_output:
      self.all_imported_experiments = ExperimentList()
      self.all_strong_reflections = flex.reflection_table()
      self.all_indexed_experiments = ExperimentList()
      self.all_indexed_reflections = flex.reflection_table()
      self.all_integrated_experiments = ExperimentList()
      self.all_integrated_reflections = flex.reflection_table()
    else:
      # The convention is to put %s in the phil parameter to add a time stamp to
      # each output datafile. Save the initial templates here.
      self.strong_filename_template                 = params.output.strong_filename
      self.indexed_filename_template                = params.output.indexed_filename
      self.refined_experiments_filename_template    = params.output.refined_experiments_filename
      self.integrated_filename_template             = params.output.integrated_filename
      self.integrated_experiments_filename_template = params.output.integrated_experiments_filename
      self.coset_filename_template                  = params.output.coset_filename
      self.coset_experiments_filename_template      = params.output.coset_experiments_filename
      self.reindexedstrong_filename_template        = params.output.reindexedstrong_filename

    # Don't allow the strong reflections to be written unless there are enough to
    # process
    params.output.strong_filename = None

    # Save the paramters
    self.params_cache = copy.deepcopy(params)
    self.options = options

    if params.mp.method == "mpi":
      from libtbx.mpi4py import MPI
      comm = MPI.COMM_WORLD
      rank = comm.Get_rank() # each process in MPI has a unique id, 0-indexed
      size = comm.Get_size() # size: number of processes running in this job
    elif params.mp.method == "sge" and \
        'SGE_TASK_ID'    in os.environ and \
        'SGE_TASK_FIRST' in os.environ and \
        'SGE_TASK_LAST'  in os.environ:
      if 'SGE_STEP_SIZE' in os.environ:
        assert int(os.environ['SGE_STEP_SIZE']) == 1
      if os.environ['SGE_TASK_ID'] == 'undefined' or os.environ['SGE_TASK_ID'] == 'undefined' or os.environ['SGE_TASK_ID'] == 'undefined':
        rank = 0
        size = 1
      else:
        rank = int(os.environ['SGE_TASK_ID']) - int(os.environ['SGE_TASK_FIRST'])
        size = int(os.environ['SGE_TASK_LAST']) - int(os.environ['SGE_TASK_FIRST']) + 1
    else:
      rank = 0
      size = 1
    self.composite_tag = "%04d"%rank

    # Configure the logging
    if params.output.logging_dir is None:
      logfile = ''
    elif params.output.logging_dir == 'DevNull':
      print("Redirecting stdout, stderr and other DIALS logfiles to /dev/null")
      logfile = os.devnull
      try:
        # Python 3
        sys.stdout = io.TextIOWrapper(open(os.devnull,'wb', 0), write_through=True)
        sys.stderr = io.TextIOWrapper(open(os.devnull,'wb', 0), write_through=True)
      except TypeError:
        # Python 2
        sys.stdout = open(os.devnull,'w', buffering=0)
        sys.stderr = open(os.devnull,'w',buffering=0)

      info_path = os.devnull
      debug_path = os.devnull
    else:
      log_path = os.path.join(params.output.logging_dir, "log_rank%04d.out"%rank)
      error_path = os.path.join(params.output.logging_dir, "error_rank%04d.out"%rank)
      print("Redirecting stdout to %s"%log_path)
      print("Redirecting stderr to %s"%error_path)
      try:
        # Python 3
        sys.stdout = io.TextIOWrapper(open(log_path,'ab', 0), write_through=True)
        sys.stderr = io.TextIOWrapper(open(error_path,'ab', 0), write_through=True)
      except TypeError:
        # Python 2
        sys.stdout = open(log_path,'a', buffering=0)
        sys.stderr = open(error_path,'a',buffering=0)

      print("Should be redirected now")

      logfile = os.path.join(params.output.logging_dir, "info_rank%04d.out"%rank)

    from dials.util import log
    log.config(options.verbose, logfile=logfile)

    debug_dir = os.path.join(params.output.output_dir, "debug")
    if not os.path.exists(debug_dir):
      try:
        os.makedirs(debug_dir)
      except OSError as e:
        pass # due to multiprocessing, makedirs can sometimes fail
    assert os.path.exists(debug_dir)

    if params.debug.skip_processed_events or params.debug.skip_unprocessed_events or params.debug.skip_bad_events:
      print("Reading debug files...")
      self.known_events = {}
      for filename in os.listdir(debug_dir):
        # format: hostname,timestamp_event,timestamp_now,status,detail
        for line in open(os.path.join(debug_dir, filename)):
          vals = line.strip().split(',')
          if len(vals) != 5:
            continue
          _, ts, _, status, detail = vals
          if status in ["done", "stop", "fail"]:
            self.known_events[ts] = status
          else:
            self.known_events[ts] = "unknown"

    self.debug_file_path = os.path.join(debug_dir, "debug_%d.txt"%rank)
    write_newline = os.path.exists(self.debug_file_path)
    if write_newline: # needed if the there was a crash
      self.debug_write("")

    if params.mp.method != 'mpi' or params.mp.mpi.method == 'client_server':
      if rank == 0:
        self.mpi_log_file_path = os.path.join(debug_dir, "mpilog.out")
        write_newline = os.path.exists(self.mpi_log_file_path)
        if write_newline: # needed if the there was a crash
          self.mpi_log_write("\n")

    # FIXME MONA: psana 2 has pedestals and geometry hardcoded for cxid9114.
    # We can remove after return code when all interfaces are ready.
    if PSANA2_VERSION:
        print(("PSANA2_VERSION", PSANA2_VERSION))
        run_psana2(self, params, comm)
        return

    # set up psana
    if params.input.cfg is not None:
      psana.setConfigFile(params.input.cfg)
    # all cores in stripe mode and the master in client-server mode read smd
    if params.dispatch.datasource is None:
      datasource = "exp=%s:run=%s:%s"%(params.input.experiment,params.input.run_num,'smd')
      if params.input.xtc_dir is not None:
        if params.input.use_ffb:
          raise Sorry("Cannot specify the xtc_dir and use SLAC's ffb system")
        datasource += ":dir=%s"%params.input.xtc_dir
      elif params.input.use_ffb:
      # as ffb is only at SLAC, ok to hardcode /reg/d here
        datasource += ":dir=/reg/d/ffb/%s/%s/xtc"%(params.input.experiment[0:3],params.input.experiment)
      if params.input.stream is not None and len(params.input.stream) > 0:
        datasource += ":stream=%s"%(",".join(["%d"%stream for stream in params.input.stream]))
      if params.input.calib_dir is not None:
        psana.setOption('psana.calib-dir',params.input.calib_dir)
      if params.mp.method == "mpi" and params.mp.mpi.method == 'client_server' and size > 2:
        dataset_name_client = datasource.replace(":smd",":rax")
      # for client-server, master reads smd - clients read rax
        if rank == 0:
          ds = psana.DataSource(datasource)
        else:
          ds = psana.DataSource(dataset_name_client)

      else:
      # for stripe, all cores read smd
        ds = psana.DataSource(datasource)
    else:
      datasource = params.dispatch.datasource
      ds = psana.DataSource(datasource)

    if params.format.file_format == "cbf":
      self.psana_det = psana.Detector(params.input.address, ds.env())

    # set this to sys.maxint to analyze all events
    if params.dispatch.max_events is None:
      max_events = sys.maxsize
    else:
      max_events = params.dispatch.max_events

    # Set up db connection if being used
    if self.params.experiment_tag is not None:
      self.db_app = dxtbx_xfel_db_application(params)

    for run in ds.runs():
      if params.format.file_format == "cbf":
        if params.format.cbf.mode == "cspad":
          # load a header only cspad cbf from the slac metrology
          try:
            self.base_dxtbx = cspad_cbf_tbx.env_dxtbx_from_slac_metrology(run, params.input.address)
          except Exception as e:
            raise Sorry("Couldn't load calibration file for run %d, %s"%(run.run(), str(e)))
        elif params.format.cbf.mode == "rayonix":
          # load a header only rayonix cbf from the input parameters
          detector_size = rayonix_tbx.get_rayonix_detector_dimensions(ds.env())
          self.base_dxtbx = rayonix_tbx.get_dxtbx_from_params(params.format.cbf.rayonix, detector_size)

        if self.base_dxtbx is None:
          raise Sorry("Couldn't load calibration file for run %d"%run.run())

        if params.format.file_format == 'cbf':
          if params.format.cbf.cspad.common_mode.algorithm == "custom":
            self.common_mode = params.format.cbf.cspad.common_mode.custom_parameterization
            assert self.common_mode is not None
          else:
            self.common_mode = params.format.cbf.cspad.common_mode.algorithm # could be None or default

        if params.format.cbf.invalid_pixel_mask is not None:
          self.dials_mask = easy_pickle.load(params.format.cbf.invalid_pixel_mask)
          if params.format.cbf.mode == "cspad":
            assert len(self.dials_mask) == 64
            if self.params.format.cbf.cspad.mask_nonbonded_pixels:
              psana_mask = self.psana_det.mask(run.run(),calib=False,status=False,edges=False,central=False,unbond=True,unbondnbrs=True)
              dials_mask = self.psana_mask_to_dials_mask(psana_mask)
              self.dials_mask = [self.dials_mask[i] & dials_mask[i] for i in range(len(dials_mask))]
        else:
          if params.format.cbf.mode == "cspad":
            psana_mask = self.psana_det.mask(run.run(),calib=True,status=True,edges=True,central=True,unbond=True,unbondnbrs=True)
            self.dials_mask = self.psana_mask_to_dials_mask(psana_mask)
          else:
            self.dials_mask = None

      if self.params.spotfinder.lookup.mask is not None:
        self.spotfinder_mask = easy_pickle.load(self.params.spotfinder.lookup.mask)
      else:
        self.spotfinder_mask = None
      if self.params.integration.lookup.mask is not None:
        self.integration_mask = easy_pickle.load(self.params.integration.lookup.mask)
      else:
        self.integration_mask = None

      # prepare fractions of process_percent, if given
      process_fractions = None
      if params.dispatch.process_percent:
        import fractions
        percent = params.dispatch.process_percent / 100
        process_fractions = fractions.Fraction(percent).limit_denominator(100)
      # list of all events
      # only cycle through times in client_server mode
      if params.mp.method == "mpi" and params.mp.mpi.method == 'client_server' and size > 2:
        # process fractions only works in idx-striping mode
        if params.dispatch.process_percent:
          raise Sorry("Process percent only works in striping mode.")
        print("Using MPI client server in rax mode")
        # use a client/server approach to be sure every process is busy as much as possible
        # only do this if there are more than 2 processes, as one process will be a server
        try:
          if rank == 0:
            # server process
            self.mpi_log_write("MPI START\n")
            for nevt, evt in enumerate(run.events()):
              if nevt == max_events: break
              self.mpi_log_write("Getting next available process\n")
              offset = evt.get(psana.EventOffset)
              rankreq = comm.recv(source=MPI.ANY_SOURCE)
              t = evt.get(psana.EventId).time()
              ts = cspad_tbx.evt_timestamp((t[0],t[1]/1e6))
              self.mpi_log_write("Process %s is ready, sending ts %s\n"%(rankreq, ts))
              comm.send(EventOffsetSerializer(offset),dest=rankreq)
            # send a stop command to each process
            self.mpi_log_write("MPI DONE, sending stops\n")
            for rankreq in range(size-1):
              self.mpi_log_write("Getting next available process\n")
              rankreq = comm.recv(source=MPI.ANY_SOURCE)
              self.mpi_log_write("Sending stop to %d\n"%rankreq)
              comm.send('endrun',dest=rankreq)
            self.mpi_log_write("All stops sent.")
          else:
            # client process
            while True:
              # inform the server this process is ready for an event
              print("Rank %d getting next task"%rank)
              comm.send(rank,dest=0)
              print("Rank %d waiting for response"%rank)
              offset = comm.recv(source=0)
              if offset == 'endrun':
                print("Rank %d recieved endrun"%rank)
                break
              evt = ds.jump(offset.filenames, offset.offsets, offset.lastBeginCalibCycleDgram)
              print("Rank %d beginning processing"%rank)
              try:
                self.process_event(run, evt)
              except Exception as e:
                print("Rank %d unhandled exception processing event"%rank, str(e))
              print("Rank %d event processed"%rank)
        except Exception as e:
          print("Error caught in main loop")
          print(str(e))
        print("Synchronizing rank %d"%rank)
        comm.Barrier()
        print("Rank %d done with main loop"%rank)
      else:
        import resource
        # chop the list into pieces, depending on rank.  This assigns each process
        # events such that the get every Nth event where N is the number of processes
        print("Striping events")

        nevent = mem = first = last = 0
        if process_fractions:
          def process_this_event(nevent):
            # nevent modulo the denominator gives us which fraction we're in
            n_mod_denom = nevent % process_fractions.denominator
            # compare the 0-indexed modulo against the 1-indexed numerator (intentionally not <=)
            n_accept = n_mod_denom < process_fractions.numerator
            return n_accept
        for nevent, evt in enumerate(run.events()):
          if nevent%size != rank: continue
          if nevent >= max_events: break
          if process_fractions and not process_this_event(nevent): continue

          self.process_event(run, evt)

          mem = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
          if nevent < 50:
            #print "Mem test rank %03d"%rank, i, mem
            continue
          #print "Mem test rank %03d"%rank, 'Cycle %6d total %7dkB increase %4dkB' % (i, mem, mem - last)
          if not first:
            first = mem
          last = mem
        print('Total memory leaked in %d cycles: %dkB' % (nevent+1-50, mem - first))

    print("Rank %d finalizing"%rank)
    try:
      self.finalize()
    except Exception as e:
      print("Rank %d, exception caught in finalize"%rank)
      print(str(e))

    if params.format.file_format == "cbf" and params.output.tmp_output_dir == "(NONE)":
      try:
        os.rmdir(tmp_dir)
      except Exception as e:
        pass

    if params.joint_reintegration.enable:
      if params.output.composite_output:
        raise NotImplementedError("Joint reintegration not implemented for composite output yet")
      assert self.params.dispatch.dump_indexed, "Cannot do joint reintegration unless indexed files were dumped"
      if rank == 0:
        reint_dir = os.path.join(params.output.output_dir, "reint")
        if not os.path.exists(reint_dir):
          os.makedirs(reint_dir)
        images = []
        experiment_jsons = []
        indexed_tables = []
        for filename in os.listdir(params.output.output_dir):
          if not filename.endswith("_indexed.refl"):
            continue
          experiment_jsons.append(os.path.join(params.output.output_dir, filename.split("_indexed.refl")[0] + "_refined.expt"))
          indexed_tables.append(os.path.join(params.output.output_dir, filename))
          if params.format.file_format == "cbf":
            images.append(os.path.join(params.output.output_dir, filename.split("_indexed.refl")[0] + ".cbf"))
          elif params.format.file_format == "pickle":
            images.append(os.path.join(params.output.output_dir, filename.split("_indexed.refl")[0] + ".pickle"))

        if len(images) < params.joint_reintegration.minimum_results:
          pass # print and return

        # TODO: maximum_results_per_chunk = 500
        combo_input = os.path.join(reint_dir, "input.phil")
        f = open(combo_input, 'w')
        for json, indexed in zip(experiment_jsons, indexed_tables):
          f.write("input {\n")
          f.write("  experiments = %s\n"%json)
          f.write("  reflections = %s\n"%indexed)
          f.write("}\n")
        f.close()

        combined_experiments_file = os.path.join(reint_dir, "combined.expt")
        combined_reflections_file = os.path.join(reint_dir, "combined.refl")
        command = "dials.combine_experiments reference_from_experiment.average_detector=True %s output.reflections=%s output.experiments=%s"% \
          (combo_input, combined_reflections_file, combined_experiments_file)
        print(command)
        from libtbx import easy_run
        easy_run.fully_buffered(command).raise_if_errors().show_stdout()

        from dxtbx.model.experiment_list import ExperimentListFactory

        combined_experiments = ExperimentListFactory.from_json_file(combined_experiments_file, check_format=False)
        combined_reflections = flex.reflection_table.from_file(combined_reflections_file)

        from dials.algorithms.refinement import RefinerFactory

        refiner = RefinerFactory.from_parameters_data_experiments(
          params.joint_reintegration, combined_reflections, combined_experiments)

        refiner.run()
        experiments = refiner.get_experiments()
        reflections = combined_reflections.select(refiner.selection_used_for_refinement())

        experiments.as_file(os.path.join(reint_dir, "refined.expt"))
        reflections.as_pickle(os.path.join(reint_dir, "refined.refl"))

        for expt_id, (expt, img_file) in enumerate(zip(experiments, images)):
          try:
            refls = reflections.select(reflections['id'] == expt_id)
            refls['id'] = flex.int(len(refls), 0)
            base_name = os.path.splitext(os.path.basename(img_file))[0]
            self.params.output.integrated_filename = os.path.join(reint_dir, base_name + "_integrated.refl")

            expts = ExperimentList([expt])
            self.integrate(expts, refls)
            expts.as_file(os.path.join(reint_dir, base_name + "_refined.expt"))
          except Exception as e:
            print("Couldn't reintegrate", img_file, str(e))
    print("Rank %d signing off"%rank)

  def get_run_and_timestamp(self, obj):
    # Used by database logger
    return self.run.run(), self.timestamp

  def process_event(self, run, evt):
    """
    Process a single event from a run
    @param run psana run object
    @param timestamp psana timestamp object
    """
    if PSANA2_VERSION:
      sec  = evt._seconds
      nsec = evt._nanoseconds
    else:
      time = evt.get(psana.EventId).time()
      fid = evt.get(psana.EventId).fiducials()
      sec  = time[0]
      nsec = time[1]

    ts = cspad_tbx.evt_timestamp((sec,nsec/1e6))
    if ts is None:
      print("No timestamp, skipping shot")
      return

    if len(self.params_cache.debug.event_timestamp) > 0 and ts not in self.params_cache.debug.event_timestamp:
      return
    self.run = run

    if self.params_cache.debug.skip_processed_events or self.params_cache.debug.skip_unprocessed_events or self.params_cache.debug.skip_bad_events:
      if ts in self.known_events:
        if self.known_events[ts] not in ["stop", "done", "fail"]:
          if self.params_cache.debug.skip_bad_events:
            print("Skipping event %s: possibly caused an unknown exception previously"%ts)
            return
        elif self.params_cache.debug.skip_processed_events:
          print("Skipping event %s: processed successfully previously"%ts)
          return
      else:
        if self.params_cache.debug.skip_unprocessed_events:
          print("Skipping event %s: not processed previously"%ts)
          return

    self.debug_start(ts)

    # FIXME MONA: below will be replaced with filter() callback
    if not PSANA2_VERSION:
      if evt.get("skip_event") or "skip_event" in [key.key() for key in evt.keys()]:
        print("Skipping event",ts)
        self.debug_write("psana_skip", "skip")
        return

    print("Accepted", ts)
    self.params = copy.deepcopy(self.params_cache)

    # the data needs to have already been processed and put into the event by psana
    if self.params.format.file_format == 'cbf':
      if self.params.format.cbf.mode == "cspad":
        # get numpy array, 32x185x388
        data = serialtbx_detector.cspad.get_psana_corrected_data(self.psana_det, evt, use_default=False, dark=True,
                                                    common_mode=self.common_mode,
                                                    apply_gain_mask=self.params.format.cbf.cspad.gain_mask_value is not None,
                                                    gain_mask_value=self.params.format.cbf.cspad.gain_mask_value,
                                                    per_pixel_gain=self.params.format.cbf.cspad.per_pixel_gain,
                                                    additional_gain_factor=self.params.format.cbf.cspad.additional_gain_factor)
      elif self.params.format.cbf.mode == "rayonix":
        data = rayonix_tbx.get_data_from_psana_event(evt, self.params.input.address)
      if data is None:
        print("No data")
        self.debug_write("no_data", "skip")
        return

      if self.params.format.cbf.override_distance is None:
        if self.params.format.cbf.mode == "cspad":
          distance = cspad_tbx.env_distance(self.params.input.address, run.env(), self.params.format.cbf.detz_offset)
        elif self.params.format.cbf.mode == "rayonix":
          distance = self.params.format.cbf.detz_offset
        if distance is None:
          print("No distance, skipping shot")
          self.debug_write("no_distance", "skip")
          return
      else:
        distance = self.params.format.cbf.override_distance

      if self.params.format.cbf.override_energy is None:
        if PSANA2_VERSION:
          wavelength = 12398.4187/self.psana_det.raw.photonEnergy(evt)
        else:
          wavelength = cspad_tbx.evt_wavelength(evt)
        if wavelength is None:
          print("No wavelength, skipping shot")
          self.debug_write("no_wavelength", "skip")
          return
      else:
        wavelength = 12398.4187/self.params.format.cbf.override_energy

    if self.params.format.file_format == 'pickle':
      image_dict = evt.get(self.params.format.pickle.out_key)
      data = image_dict['DATA']

    self.timestamp = timestamp = t = ts
    s = t[0:4] + t[5:7] + t[8:10] + t[11:13] + t[14:16] + t[17:19] + t[20:23]
    print("Processing shot", s)

    def build_dxtbx_image():
      if self.params.format.file_format == 'cbf':
        # stitch together the header, data and metadata into the final dxtbx format object
        if self.params.format.cbf.mode == "cspad":
          dxtbx_img = cspad_cbf_tbx.format_object_from_data(self.base_dxtbx, data, distance, wavelength, timestamp, self.params.input.address, round_to_int=False)
        elif self.params.format.cbf.mode == "rayonix":
          dxtbx_img = rayonix_tbx.format_object_from_data(self.base_dxtbx, data, distance, wavelength, timestamp, self.params.input.address)

        if self.params.input.reference_geometry is not None:
          from dxtbx.model import Detector
          # copy.deep_copy(self.reference_detctor) seems unsafe based on tests. Use from_dict(to_dict()) instead.
          dxtbx_img._detector_instance = Detector.from_dict(self.reference_detector.to_dict())
          if self.params.format.cbf.mode == "cspad":
            dxtbx_img.sync_detector_to_cbf() #FIXME need a rayonix version of this??

      elif self.params.format.file_format == 'pickle':
        from dxtbx.format.FormatPYunspecifiedStill import FormatPYunspecifiedStillInMemory
        dxtbx_img = FormatPYunspecifiedStillInMemory(image_dict)
      return dxtbx_img

    dxtbx_img = build_dxtbx_image()
    for correction in self.params.format.per_pixel_absorption_correction:
      if correction.apply:
        if correction.algorithm == "fuller_kapton":
          from dials.algorithms.integration.kapton_correction import all_pixel_image_data_kapton_correction
          data = all_pixel_image_data_kapton_correction(image_data=dxtbx_img, params=correction.fuller_kapton)()
          dxtbx_img = build_dxtbx_image() # repeat as necessary to update the image pixel data and rebuild the image

    self.tag = s # used when writing integration pickle

    if self.params.dispatch.dump_all:
      self.save_image(dxtbx_img, self.params, os.path.join(self.params.output.output_dir, "shot-" + s))

    self.cache_ranges(dxtbx_img, self.params.input.override_spotfinding_trusted_min, self.params.input.override_spotfinding_trusted_max)

    from dxtbx.imageset import ImageSet, ImageSetData, MemReader
    imgset = ImageSet(ImageSetData(MemReader([dxtbx_img]), None))
    imgset.set_beam(dxtbx_img.get_beam())
    imgset.set_detector(dxtbx_img.get_detector())

    if self.params.dispatch.estimate_gain_only:
      from dials.command_line.estimate_gain import estimate_gain
      estimate_gain(imgset)
      return

    # FIXME MONA: radial avg. is currently disabled
    if not PSANA2_VERSION:
      # Two values from a radial average can be stored by mod_radial_average. If present, retrieve them here
      key_low = 'cctbx.xfel.radial_average.two_theta_low'
      key_high = 'cctbx.xfel.radial_average.two_theta_high'
      tt_low = evt.get(key_low)
      tt_high = evt.get(key_high)

    if self.params.radial_average.enable:
      if tt_low is not None or tt_high is not None:
        print("Warning, mod_radial_average is being used while also using xtc_process radial averaging. mod_radial_averaging results will not be logged to the database.")

    from dxtbx.model.experiment_list import ExperimentListFactory
    experiments = ExperimentListFactory.from_imageset_and_crystal(imgset, None)

    try:
      self.pre_process(experiments)
    except Exception as e:
      self.debug_write("preprocess_exception", "fail")
      return

    if not self.params.dispatch.find_spots:
      self.debug_write("data_loaded", "done")
      return

    # before calling DIALS for processing, set output paths according to the templates
    if not self.params.output.composite_output:
      if self.indexed_filename_template is not None and "%s" in self.indexed_filename_template:
        self.params.output.indexed_filename = os.path.join(self.params.output.output_dir, self.indexed_filename_template%("idx-" + s))
      if "%s" in self.refined_experiments_filename_template:
        self.params.output.refined_experiments_filename = os.path.join(self.params.output.output_dir, self.refined_experiments_filename_template%("idx-" + s))
      if "%s" in self.integrated_filename_template:
        self.params.output.integrated_filename = os.path.join(self.params.output.output_dir, self.integrated_filename_template%("idx-" + s))
      if "%s" in self.integrated_experiments_filename_template:
        self.params.output.integrated_experiments_filename = os.path.join(self.params.output.output_dir, self.integrated_experiments_filename_template%("idx-" + s))
      if "%s" in self.coset_filename_template:
        self.params.output.coset_filename = os.path.join(self.params.output.output_dir, self.coset_filename_template%("idx-" + s,
          self.params.integration.coset.transformation))
      if "%s" in self.coset_experiments_filename_template:
        self.params.output.coset_experiments_filename = os.path.join(self.params.output.output_dir, self.coset_experiments_filename_template%("idx-" + s,
          self.params.integration.coset.transformation))
      if "%s" in self.reindexedstrong_filename_template:
        self.params.output.reindexedstrong_filename = os.path.join(self.params.output.output_dir, self.reindexedstrong_filename_template%("idx-" + s))

    if self.params.input.known_orientations_folder is not None:
      expected_orientation_path = os.path.join(self.params.input.known_orientations_folder, os.path.basename(self.params.output.refined_experiments_filename))
      if os.path.exists(expected_orientation_path):
        print("Known orientation found")
        from dxtbx.model.experiment_list import ExperimentListFactory
        self.known_crystal_models = ExperimentListFactory.from_json_file(expected_orientation_path, check_format=False).crystals()
      else:
        print("Image not previously indexed, skipping.")
        self.debug_write("not_previously_indexed", "stop")
        return

    # Load a dials mask from the trusted range and psana mask
    import dials.util.masking
    mask = dials.util.masking.generate_mask(imgset, self.params.border_mask)
    if self.params.format.file_format == "cbf" and self.dials_mask is not None:
      mask = tuple([a&b for a, b in zip(mask,self.dials_mask)])
    if self.spotfinder_mask is None:
      self.params.spotfinder.lookup.mask = mask
    else:
      self.params.spotfinder.lookup.mask = tuple([a&b for a, b in zip(mask,self.spotfinder_mask)])

    self.debug_write("spotfind_start")
    try:
      observed = self.find_spots(experiments)
    except Exception as e:
      import traceback; traceback.print_exc()
      print(str(e), "event", timestamp)
      self.debug_write("spotfinding_exception", "fail")
      return

    print("Found %d bright spots"%len(observed))

    if self.params.dispatch.hit_finder.enable and len(observed) < self.params.dispatch.hit_finder.minimum_number_of_reflections:
      print("Not enough spots to index")
      self.debug_write("not_enough_spots_%d"%len(observed), "stop")
      return
    if self.params.dispatch.hit_finder.maximum_number_of_reflections is not None:
      if self.params.dispatch.hit_finder.enable and len(observed) > self.params.dispatch.hit_finder.maximum_number_of_reflections:
        print("Too many spots to index - Possibly junk")
        self.debug_write("too_many_spots_%d"%len(observed), "stop")
        return

    self.restore_ranges(dxtbx_img)

    # save cbf file
    if self.params.dispatch.dump_strong:
      self.save_image(dxtbx_img, self.params, os.path.join(self.params.output.output_dir, "hit-" + s))

      # save strong reflections.  self.find_spots() would have done this, but we only
      # want to save data if it is enough to try and index it
      if self.strong_filename_template:
        if "%s" in self.strong_filename_template:
          strong_filename = self.strong_filename_template%("hit-" + s)
        else:
          strong_filename = self.strong_filename_template
        strong_filename = os.path.join(self.params.output.output_dir, strong_filename)

        from dials.util.command_line import Command
        Command.start('Saving {0} reflections to {1}'.format(
            len(observed), os.path.basename(strong_filename)))
        observed.as_pickle(strong_filename)
        Command.end('Saved {0} observed to {1}'.format(
            len(observed), os.path.basename(strong_filename)))

    if not self.params.dispatch.index:
      self.debug_write("strong_shot_%d"%len(observed), "done")
      return

    # index and refine
    self.debug_write("index_start")
    try:
      experiments, indexed = self.index(experiments, observed)
    except Exception as e:
      import traceback; traceback.print_exc()
      print(str(e), "event", timestamp)
      self.debug_write("indexing_failed_%d"%len(observed), "stop")
      return

    if self.params.dispatch.dump_indexed:
      img_path = self.save_image(dxtbx_img, self.params, os.path.join(self.params.output.output_dir, "idx-" + s))
      imgset = ExperimentListFactory.from_filenames([img_path]).imagesets()[0]
      assert len(experiments.detectors()) == 1;   imgset.set_detector(experiments[0].detector)
      assert len(experiments.beams()) == 1;       imgset.set_beam(experiments[0].beam)
      assert len(experiments.scans()) <= 1;       imgset.set_scan(experiments[0].scan)
      assert len(experiments.goniometers()) <= 1; imgset.set_goniometer(experiments[0].goniometer)
      for expt_id, expt in enumerate(experiments):
        expt.imageset = imgset

    self.debug_write("refine_start")

    try:
      experiments, indexed = self.refine(experiments, indexed)
    except Exception as e:
      import traceback; traceback.print_exc()
      print(str(e), "event", timestamp)
      self.debug_write("refine_failed_%d"%len(indexed), "fail")
      return

    if self.params.dispatch.reindex_strong:
      self.debug_write("reindex_start")
      try:
        self.reindex_strong(experiments, observed)
      except Exception as e:
        import traceback; traceback.print_exc()
        print(str(e), "event", timestamp)
        self.debug_write("reindexstrong_failed_%d"%len(indexed), "fail")
        return

    if not self.params.dispatch.integrate:
      self.debug_write("index_ok_%d"%len(indexed), "done")
      return

    # integrate
    self.debug_write("integrate_start")
    self.cache_ranges(dxtbx_img, self.params.input.override_integration_trusted_min, self.params.input.override_integration_trusted_max)

    if self.cached_ranges is not None:
      # Load a dials mask from the trusted range and psana mask
      imgset = ImageSet(ImageSetData(MemReader([dxtbx_img]), None))
      imgset.set_beam(dxtbx_img.get_beam())
      imgset.set_detector(dxtbx_img.get_detector())
      import dials.util.masking
      mask = dials.util.masking.generate_mask(imgset, self.params.border_mask)
      if self.params.format.file_format == "cbf" and self.dials_mask is not None:
        mask = tuple([a&b for a, b in zip(mask,self.dials_mask)])
    if self.integration_mask is None:
      self.params.integration.lookup.mask = mask
    else:
      self.params.integration.lookup.mask = tuple([a&b for a, b in zip(mask,self.integration_mask)])

    try:
      integrated = self.integrate(experiments, indexed)
    except Exception as e:
      import traceback; traceback.print_exc()
      print(str(e), "event", timestamp)
      self.debug_write("integrate_failed_%d"%len(indexed), "fail")
      return
    self.restore_ranges(dxtbx_img)

    self.debug_write("integrate_ok_%d"%len(integrated), "done")

  def save_image(self, image, params, root_path):
    """ Save an image, in either cbf or pickle format.
    @param image dxtbx format object
    @param params phil scope object
    @param root_path output file path without extension
    """

    if params.format.file_format == 'cbf':
      dest_path = root_path + ".cbf"
    elif params.format.file_format == 'pickle':
      dest_path = root_path + ".pickle"

    try:
      if params.format.file_format == 'cbf':
        image._cbf_handle.write_widefile(dest_path.encode(), pycbf.CBF,\
          pycbf.MIME_HEADERS|pycbf.MSG_DIGEST|pycbf.PAD_4K, 0)
      elif params.format.file_format == 'pickle':
        easy_pickle.dump(dest_path, image._image_file)
    except Exception:
      print("Warning, couldn't save image:", dest_path)

    return dest_path

  def cache_ranges(self, dxtbx_img, min_val, max_val):
    """ Save the current trusted ranges, and replace them with the given overrides, if present.
    @param cspad_image dxtbx format object
    """
    if min_val is None and max_val is None:
      return

    detector = dxtbx_img.get_detector()
    self.cached_ranges = []
    for panel in detector:
      new_range = cached_range = panel.get_trusted_range()
      self.cached_ranges.append(cached_range)
      if max_val is not None:
        new_range = new_range[0], max_val
      if min_val is not None:
        new_range = min_val, new_range[1]

      panel.set_trusted_range(new_range)

  def restore_ranges(self, dxtbx_img):
    """ Restore the previously cached trusted ranges, if present.
    @param cspad_image dxtbx format object
    """
    if self.cached_ranges is None:
      return

    detector = dxtbx_img.get_detector()
    for cached_range, panel in zip(self.cached_ranges, detector):
      panel.set_trusted_range(cached_range)

    self.cached_ranges = None

  def reindex_strong(self, experiments, strong):
    print("Reindexing strong reflections using refined experimental models and no outlier rejection...")
    from dials.algorithms.indexing.stills_indexer import StillsIndexerKnownOrientation
    indexer = StillsIndexerKnownOrientation(strong, experiments.imagesets(), self.params, experiments.crystals())
    indexed_reflections = indexer.reflections.select(indexer.indexed_reflections)

    print("Indexed %d strong reflections out of %d"%(len(indexed_reflections), len(strong)))
    self.save_reflections(indexed_reflections, self.params.output.reindexedstrong_filename)

  def finalize(self):
    if self.params.output.composite_output:
      # Each process will write its own set of output files
      s = self.composite_tag
      self.params.output.indexed_filename                = os.path.join(self.params.output.output_dir, self.params.output.indexed_filename%("idx-" + s))
      self.params.output.refined_experiments_filename    = os.path.join(self.params.output.output_dir, self.params.output.refined_experiments_filename%("idx-" + s))
      self.params.output.integrated_filename             = os.path.join(self.params.output.output_dir, self.params.output.integrated_filename%("idx-" + s))
      self.params.output.integrated_experiments_filename = os.path.join(self.params.output.output_dir, self.params.output.integrated_experiments_filename%("idx-" + s))
      self.params.output.coset_filename                  = os.path.join(self.params.output.output_dir, self.params.output.coset_filename%("idx-" + s,
        self.params.integration.coset.transformation))
      self.params.output.coset_experiments_filename      = os.path.join(self.params.output.output_dir, self.params.output.coset_experiments_filename%("idx-" + s,
        self.params.integration.coset.transformation))
      self.params.output.reindexedstrong_filename        = os.path.join(self.params.output.output_dir, self.params.output.reindexedstrong_filename%("idx-" + s))

    super(InMemScript, self).finalize()

if __name__ == "__main__":
  with show_mail_on_error():
    script = InMemScript()
    script.run()


 *******************************************************************************
