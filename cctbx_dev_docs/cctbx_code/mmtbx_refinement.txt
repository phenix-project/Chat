

 *******************************************************************************
mmtbx/refinement/__init__.py
from __future__ import absolute_import, division, print_function
from libtbx import adopt_init_args

class monitors(object):
  def __init__(
        self,
        params,
        model,
        fmodels,
        log = None,
        neutron_refinement = None,
        call_back_handler = None):
    adopt_init_args(self, locals())
    from mmtbx.refinement import print_statistics
    self.monitor_xray = print_statistics.refinement_monitor(
      params    = params,
      neutron_refinement = self.neutron_refinement,
      out       = log,
      call_back_handler = call_back_handler,
      is_neutron_monitor = False)
    self.monitor_neutron = None
    if(fmodels.fmodel_n is not None):
      self.monitor_neutron = print_statistics.refinement_monitor(
        params    = params,
        neutron_refinement = self.neutron_refinement,
        out       = log,
        call_back_handler = call_back_handler,
        is_neutron_monitor = True)

  def collect(
        self,
        step,
        fmodels = None,
        model = None,
        rigid_body_shift_accumulator = None):
    from mmtbx import utils
    if(model is not None): self.model = model
    if(fmodels is not None): self.fmodels = fmodels
    utils.assert_xray_structures_equal(
      x1 = self.fmodels.fmodel_xray().xray_structure,
      x2 = self.model.get_xray_structure())
    self.monitor_xray.collect(
      model                        = self.model,
      fmodel                       = self.fmodels.fmodel_xray(),
      step                         = step,
      wilson_b                     = self.fmodels.fmodel_xray().wilson_b(),
      rigid_body_shift_accumulator = rigid_body_shift_accumulator)
    if(self.monitor_neutron is not None):
      utils.assert_xray_structures_equal(
        x1 = self.fmodels.fmodel_neutron().xray_structure,
        x2 = self.model.get_xray_structure())
      self.monitor_neutron.collect(
        model                        = self.model,
        fmodel                       = self.fmodels.fmodel_neutron(),
        step                         = step,
        wilson_b                     = self.fmodels.fmodel_neutron().wilson_b(),
        rigid_body_shift_accumulator = rigid_body_shift_accumulator)


 *******************************************************************************


 *******************************************************************************
mmtbx/refinement/adp_refinement.py
from __future__ import absolute_import, division, print_function
import mmtbx.refinement.group
from mmtbx.refinement import minimization
from mmtbx.refinement import print_statistics
from mmtbx import utils
from mmtbx.tls import tools
import iotbx.phil
from cctbx import adptbx
from cctbx.array_family import flex
import scitbx.lbfgs
from libtbx.test_utils import approx_equal
from libtbx import adopt_init_args, Auto
from libtbx.utils import user_plus_sys_time

time_adp_refinement_py = 0.0

def show_times(out = None):
  if(out is None): out = sys.stdout
  total = time_adp_refinement_py
  if(total > 0.01):
     print("ADP refinement:", file=out)
     print("  time spent in adp_refinement.py          = %-7.2f" % time_adp_refinement_py, file=out)
  return total

group_adp_master_params = iotbx.phil.parse("""\
  number_of_macro_cycles   = 3
    .type = int
    .expert_level = 1
  max_number_of_iterations = 25
    .type = int
    .expert_level = 1
  convergence_test         = False
    .type = bool
    .expert_level = 3
  run_finite_differences_test = False
    .type = bool
    .expert_level = 3
  use_restraints = True
    .type = bool
    .expert_level = 0
  restraints_weight = None
    .type = float
    .expert_level = 0
""")

tls_master_params = iotbx.phil.parse("""\
  one_residue_one_group       = None
    .type = bool
    .style = tribool
  refine_T                    = True
    .type = bool
  refine_L                    = True
    .type = bool
  refine_S                    = True
    .type = bool
  number_of_macro_cycles      = 2
    .type = int
  max_number_of_iterations    = 25
    .type = int
  start_tls_value             = None
    .type = float
  run_finite_differences_test = False
    .type = bool
    .help = Test with finite differences instead of gradients.  FOR \
      DEVELOPMENT PURPOSES ONLY.
    .expert_level = 3
  eps                         = 1.e-6
    .type = float
    .help = Finite difference setting.
    .expert_level = 3
  min_tls_group_size = 5
    .type = int
    .help = min number of atoms allowed per TLS group
  verbose = True
    .type = bool
""")

individual_adp_master_params = iotbx.phil.parse("""\
  iso {
    max_number_of_iterations = 25
      .type = int
    scaling {
      scale_max       = 3.0
        .type = float
      scale_min       = 10.0
        .type = float
    }
  }
""")

adp_restraints_master_params = iotbx.phil.parse("""\
  iso {
    use_u_local_only = False
      .type = bool
    sphere_radius = 5.0
      .type = float
    distance_power = 1.69
      .type = float
    average_power = 1.03
      .type = float
    wilson_b_weight_auto = False
      .type = bool
    wilson_b_weight = None
      .type = float
    plain_pairs_radius = 5.0
      .type = float
    refine_ap_and_dp = False
      .type = bool
  }
""")

class manager(object):
  def __init__(
            self,
            fmodels,
            model,
            all_params,
            group_adp_selections   = None,
            group_adp_selections_h = None,
            group_adp_params       = None,
            tls_selections         = None,
            tls_params             = tls_master_params.extract(),
            individual_adp_params  = individual_adp_master_params.extract(),
            adp_restraints_params  = adp_restraints_master_params.extract(),
            refine_adp_individual  = None,
            refine_adp_group       = None,
            refine_tls             = None,
            tan_b_iso_max          = None,
            restraints_manager     = None,
            target_weights         = None,
            macro_cycle            = None,
            log                    = None,
            h_params               = None,
            nproc                  = None):
    global time_adp_refinement_py
    if(group_adp_params is None):
      group_adp_params = group_adp_master_params.extract()
    scatterers = fmodels.fmodel_xray().xray_structure.scatterers()
    timer = user_plus_sys_time()
    if(log is None): log = sys.stdout
    tan_u_iso = False
    param = 0
    if(tan_b_iso_max > 0.0):
       tan_u_iso = True
       param = int(tan_b_iso_max)
    if(macro_cycle == 1):
       offset = True
    else:
       offset = False

    if(refine_tls):
       print_statistics.make_sub_header(text = "TLS refinement",
                                        out  = log)
       tls_sel_st = flex.size_t()
       for ts in tls_selections:
         tls_sel_st.extend(ts)
       tls_sel_bool = flex.bool(scatterers.size(), flex.size_t(tls_sel_st))
       ### totally ad hoc fix
       tmp_site_t = flex.size_t()
       for gs in group_adp_selections:
         for gs_ in gs:
           tmp_site_t.append(gs_)
       ###
       if(macro_cycle == 1 or tmp_site_t.size() != scatterers.size()):
          gbr_selections = []
          for s in tls_selections:
            gbr_selections.append(s)
       else:
          gbr_selections = []
          for gs in group_adp_selections:
            gbr_selection = flex.size_t()
            for gs_ in gs:
              if(tls_sel_bool[gs_]):
                gbr_selection.append(gs_)
            if(gbr_selection.size() > 0):
              gbr_selections.append(gbr_selection)
       gbr_selections_one_arr = flex.size_t()
       for gbs in gbr_selections:
         gbr_selections_one_arr.extend(gbs)
       scatterers = fmodels.fmodel_xray().xray_structure.scatterers()
       for gbr_selection in gbr_selections_one_arr:
         scatterers[gbr_selection].flags.set_use_u_iso(True)
       group_b_manager = mmtbx.refinement.group.manager(
          fmodel                   = fmodels.fmodel_xray(),
          selections               = gbr_selections,
          convergence_test         = group_adp_params.convergence_test,
          max_number_of_iterations = 50,
          number_of_macro_cycles   = 1,
          refine_adp               = True,
          use_restraints           = False, #XXX do not use in TLS refinement for now
          log                      = log)
       scatterers = fmodels.fmodel_xray().xray_structure.scatterers()
       for tls_selection_ in tls_selections:
         for tls_selection__ in tls_selection_:
           scatterers[tls_selection__].flags.set_use_u_aniso(True)
       model.show_groups(tls = True, out = log)
       current_target_name = fmodels.fmodel_xray().target_name
       fmodels.fmodel_xray().update(target_name = "ls_wunit_k1")
       tools.split_u(fmodels.fmodel_xray().xray_structure, tls_selections, offset)
       self.tls_refinement_manager = tools.tls_refinement(
          fmodel                      = fmodels.fmodel_xray(),
          model                       = model,
          selections                  = tls_selections,
          selections_1d               = tls_sel_st,
          refine_T                    = tls_params.refine_T,
          refine_L                    = tls_params.refine_L,
          refine_S                    = tls_params.refine_S,
          number_of_macro_cycles      = tls_params.number_of_macro_cycles,
          max_number_of_iterations    = tls_params.max_number_of_iterations,
          start_tls_value             = tls_params.start_tls_value,
          run_finite_differences_test = tls_params.run_finite_differences_test,
          eps                         = tls_params.eps,
          out                         = log,
          macro_cycle = macro_cycle,
          verbose = tls_params.verbose)
       fmodels.fmodel_xray().update(target_name = current_target_name)
       fmodels.update_xray_structure(
            xray_structure = self.tls_refinement_manager.fmodel.xray_structure,
            update_f_calc  = True)
       model.set_xray_structure(fmodels.fmodel_xray().xray_structure)

    if(refine_adp_individual):
       refine_adp(
         model                 = model,
         fmodels               = fmodels,
         target_weights        = target_weights,
         individual_adp_params = individual_adp_params,
         adp_restraints_params = adp_restraints_params,
         h_params              = h_params,
         log                   = log,
         all_params            = all_params,
         nproc                 = nproc)

    if(refine_adp_group):
      print_statistics.make_sub_header(
        text= "group isotropic ADP refinement", out = log)
      group_b_manager = mmtbx.refinement.group.manager(
        fmodel                   = fmodels.fmodel_xray(),
        selections               = group_adp_selections,
        convergence_test         = group_adp_params.convergence_test,
        max_number_of_iterations = group_adp_params.max_number_of_iterations,
        number_of_macro_cycles   = group_adp_params.number_of_macro_cycles,
        run_finite_differences_test = group_adp_params.run_finite_differences_test,
        use_restraints           = group_adp_params.use_restraints,
        restraints_weight        = group_adp_params.restraints_weight,
        refine_adp               = True,
        log                      = log)
    time_adp_refinement_py += timer.elapsed()

class refine_adp(object):

  def __init__(
            self,
            model,
            fmodels,
            target_weights,
            individual_adp_params,
            adp_restraints_params,
            h_params,
            log,
            all_params,
            nproc=None):
    adopt_init_args(self, locals())
    d_min = fmodels.fmodel_xray().f_obs().d_min()
    #
    # Figure out if need to optimize weights or skip it
    #
    optimize_adp_weight = self.target_weights.twp.optimize_adp_weight
    if(optimize_adp_weight):
      r_work = self.fmodels.fmodel_xray().r_work()
      r_free = self.fmodels.fmodel_xray().r_free()
      if ((r_free < r_work or (r_free-r_work)<0.01) and
          (not all_params.target_weights.force_optimize_weights)) :
        optimize_adp_weight = False
    # initialize with defaults...
    if(self.fmodels.fmodel_xray().f_obs().d_min()<3): # This logic is only good for high_res
      if(target_weights is not None):
        import mmtbx.refinement.weights_params
        wcp = mmtbx.refinement.weights_params.tw_customizations_params.extract()
        for w_s_c in wcp.weight_selection_criteria:
          if(d_min >= w_s_c.d_min and d_min < w_s_c.d_max):
            r_free_range_width = w_s_c.r_free_range_width
            r_free_r_work_gap = w_s_c.r_free_minus_r_work
            mean_diff_b_iso_bonded_fraction = w_s_c.mean_diff_b_iso_bonded_fraction
            min_diff_b_iso_bonded = w_s_c.min_diff_b_iso_bonded
            break
        # ...then customize
        wsc = all_params.target_weights.weight_selection_criteria
        if(wsc.r_free_minus_r_work is not None):
          r_free_r_work_gap = wsc.r_free_minus_r_work
        if(wsc.r_free_range_width is not None):
          r_free_range_width = wsc.r_free_range_width
        if(wsc.mean_diff_b_iso_bonded_fraction is not None):
          mean_diff_b_iso_bonded_fraction = wsc.mean_diff_b_iso_bonded_fraction
        if(wsc.min_diff_b_iso_bonded is not None):
          min_diff_b_iso_bonded = wsc.min_diff_b_iso_bonded
    else: # Worse than 3A: better yet ad hoc criteria.
      r_free_r_work_gap = 6
      r_free_range_width = 1.5
      mean_diff_b_iso_bonded_fraction = 0.2
      min_diff_b_iso_bonded = 20
    #
    print_statistics.make_sub_header(text="Individual ADP refinement", out = log)
    assert fmodels.fmodel_xray().xray_structure is model.get_xray_structure()
    #
    fmodels.create_target_functors()
    assert approx_equal(self.fmodels.fmodel_xray().target_w(),
      self.fmodels.target_functor_result_xray(
        compute_gradients=False).target_work())
    rw     = flex.double()
    rf     = flex.double()
    rfrw   = flex.double()
    deltab = flex.double()
    w      = flex.double()
    if(self.target_weights is not None):
      fmth ="    R-FACTORS      <Bi-Bj>  <B>   WEIGHT       TARGETS"
      print(fmth, file=self.log)
      print(" work  free  delta                           data restr", file=self.log)
    else:
      print("Unresrained refinement...", file=self.log)
    self.save_scatterers = self.fmodels.fmodel_xray().xray_structure.\
        deep_copy_scatterers().scatterers()
    if(self.target_weights is not None):
      default_weight = self.target_weights.adp_weights_result.wx*\
          self.target_weights.adp_weights_result.wx_scale
      if(optimize_adp_weight):
        wx_scale = [0.03,0.125,0.5,1.,1.5,2.,2.5,3.,3.5,4.,4.5,5.]

        trial_weights = list( flex.double(wx_scale)*self.target_weights.adp_weights_result.wx )
        self.wx_scale = 1
      else:
        trial_weights = [self.target_weights.adp_weights_result.wx]
        self.wx_scale = self.target_weights.adp_weights_result.wx_scale
    else:
      default_weight = 1
      trial_weights = [1]
      self.wx_scale = 1
    self.show(weight=default_weight)
    trial_results = []
    if nproc is None:
      nproc =  all_params.main.nproc
    parallel = False
    if (len(trial_weights) > 1) and ((nproc is Auto) or (nproc > 1)):
      parallel = True
      from libtbx import easy_mp
      stdout_and_results = easy_mp.pool_map(
        processes=nproc,
        fixed_func=self.try_weight,
        args=trial_weights,
        func_wrapper="buffer_stdout_stderr") # XXX safer for phenix GUI
      trial_results = [ r for so, r in stdout_and_results ]
    else :
      for weight in trial_weights:
        result = self.try_weight(weight, print_stats=True)
        trial_results.append(result)
    for result in trial_results :
      if(result is not None) and (result.r_work is not None):
        if (parallel):
          result.show(out=self.log)
        rw     .append(result.r_work)
        rf     .append(result.r_free)
        rfrw   .append(result.r_gap)
        deltab .append(result.delta_b)
        w      .append(result.weight)
    #
    if(len(trial_weights)>1 and rw.size()>0):
      # filter by rfree-rwork
      rw,rf,rfrw,deltab,w = self.score(rw=rw,rf=rf,rfrw=rfrw,deltab=deltab,w=w,
        score_target=rfrw,score_target_value=r_free_r_work_gap,
        secondary_target=deltab)
      # filter by rfree
      rw,rf,rfrw,deltab,w = self.score(rw=rw,rf=rf,rfrw=rfrw,deltab=deltab,w=w,
        score_target=rf,score_target_value=flex.min(rf)+r_free_range_width)
      # filter by <Bi-Bj>
      delta_b_target = max(min_diff_b_iso_bonded, flex.mean(self.fmodels.
        fmodel_xray().xray_structure.extract_u_iso_or_u_equiv()*
          adptbx.u_as_b(1))*mean_diff_b_iso_bonded_fraction)
      print("  max suggested <Bi-Bj> for this run: %7.2f"%delta_b_target, file=log)
      print("  max allowed Rfree-Rwork gap: %5.1f"%r_free_r_work_gap, file=log)
      print("  range of equivalent Rfree: %5.1f"%r_free_range_width, file=log)
      rw,rf,rfrw,deltab,w = self.score(rw=rw,rf=rf,rfrw=rfrw,deltab=deltab,w=w,
        score_target=deltab,score_target_value=delta_b_target)
      # select the result with lowest rfree
      sel = flex.sort_permutation(rf)
      rw,rf,rfrw,deltab,w= self.select(
        rw=rw,rf=rf,rfrw=rfrw,deltab=deltab,w=w,sel=sel)
      #
      w_best = w[0]
      rw_best = rw[0]
      print("Best ADP weight: %8.3f"%w_best, file=self.log)
      #
      self.target_weights.adp_weights_result.wx = w_best
      self.target_weights.adp_weights_result.wx_scale = 1
      best_u_star = None
      best_u_iso = None
      for result in trial_results :
        if(abs(result.weight-w_best)<=1.e-8):
          best_u_star = result.u_star
          best_u_iso = result.u_iso
          break
      if(best_u_iso is None) : # XXX this probably shouldn't happen...
        self.fmodels.fmodel_xray().xray_structure.replace_scatterers(
          self.save_scatterers.deep_copy())
      else :
        assert (best_u_star is not None)
        xrs = self.fmodels.fmodel_xray().xray_structure
        xrs.set_u_iso(values=best_u_iso)
        xrs.scatterers().set_u_star(best_u_star)
        new_u_iso = xrs.scatterers().extract_u_iso()
        assert (new_u_iso.all_eq(best_u_iso))
      self.fmodels.update_xray_structure(
        xray_structure = self.fmodels.fmodel_xray().xray_structure,
        update_f_calc  = True)
      print("Accepted refinement result:", file=self.log)
      # reset alpha/beta parameters - if this is not done, the assertion
      # below will fail
      fmodels.create_target_functors()
      if(self.fmodels.fmodel_neutron() is None):
        assert approx_equal(self.fmodels.fmodel_xray().r_work()*100, rw_best,
          eps=0.001)
        # this needs to be done again again, just in case
        fmodels.create_target_functors()
      self.show(weight=w_best)
    self.fmodels.fmodel_xray().xray_structure.tidy_us()
    self.fmodels.update_xray_structure(
      xray_structure = self.fmodels.fmodel_xray().xray_structure,
      update_f_calc  = True)
    fmodels.create_target_functors()
    assert approx_equal(self.fmodels.fmodel_xray().target_w(),
       self.fmodels.target_functor_result_xray(
         compute_gradients=False).target_work())
    self.model.set_xray_structure(self.fmodels.fmodel_xray().xray_structure)

  # XXX parallelized
  def try_weight(self, weight, print_stats=False):
    if(self.target_weights is not None):
      self.fmodels.fmodel_xray().xray_structure.replace_scatterers(
        self.save_scatterers.deep_copy())
      self.fmodels.update_xray_structure(
        xray_structure = self.fmodels.fmodel_xray().xray_structure,
        update_f_calc  = True)
      self.target_weights.adp_weights_result.wx = weight
      self.target_weights.adp_weights_result.wx_scale = self.wx_scale
    minimized = self.minimize()
    wt = weight*self.wx_scale
    result = self.show(weight=wt, print_stats=print_stats)
    return result

  def show(self, weight = None, prefix = "", show_neutron=True,
      print_stats=True):
    deltab = self.model.rms_b_iso_or_b_equiv_bonded()
    r_work = self.fmodels.fmodel_xray().r_work()*100.
    r_free = self.fmodels.fmodel_xray().r_free()*100.
    mean_b = flex.mean(
      self.model.get_xray_structure().extract_u_iso_or_u_equiv())*adptbx.u_as_b(1)
    if(deltab is None):
      print("  r_work=%5.2f r_free=%5.2f"%(r_work, r_free), file=self.log)
      return None
    neutron_r_work = neutron_r_free = None
    if (show_neutron) and (self.fmodels.fmodel_neutron() is not None):
      neutron_r_work = self.fmodels.fmodel_neutron().r_work()*100.
      neutron_r_free = self.fmodels.fmodel_neutron().r_free()*100.
    xrs = self.fmodels.fmodel_xray().xray_structure
    result = weight_result(
      r_work=r_work,
      r_free=r_free,
      delta_b=deltab,
      mean_b=mean_b,
      weight=weight,
      xray_target=self.fmodels.fmodel_xray().target_w(),
      neutron_r_work=neutron_r_work,
      neutron_r_free=neutron_r_free,
      u_star=xrs.scatterers().extract_u_star(),
      u_iso=xrs.scatterers().extract_u_iso())
    if (print_stats):
      result.show(out=self.log)
    return result

  def score(self, rw, rf, rfrw, deltab, w, score_target, score_target_value,
            secondary_target=None):
    sel  = score_target < score_target_value
    sel &= score_target > 0
    if(sel.count(True)>0):
      rw,rf,rfrw,deltab,w = self.select(
        rw=rw,rf=rf,rfrw=rfrw,deltab=deltab,w=w, sel=sel)
    else:
      if(secondary_target is None):
        sel = flex.sort_permutation(score_target)
      else:
        sel = flex.sort_permutation(secondary_target)
      rw,rf,rfrw,deltab,w = self.select(
        rw=rw,rf=rf,rfrw=rfrw,deltab=deltab,w=w, sel=sel)
      #
      rw     = flex.double([rw    [0]])
      rf     = flex.double([rf    [0]])
      rfrw   = flex.double([rfrw  [0]])
      deltab = flex.double([deltab[0]])
      w      = flex.double([w     [0]])
    return rw, rf, rfrw, deltab, w

  def select(self, rw, rf, rfrw, deltab, w, sel):
    rw     = rw    .select(sel)
    rf     = rf    .select(sel)
    rfrw   = rfrw  .select(sel)
    deltab = deltab.select(sel)
    w      = w     .select(sel)
    return rw, rf, rfrw, deltab, w

  def minimize(self):
    utils.assert_xray_structures_equal(
      x1 = self.fmodels.fmodel_xray().xray_structure,
      x2 = self.model.get_xray_structure())
    self.model.set_refine_individual_adp()
    self.run_lbfgs()
    self.model.set_xray_structure(self.fmodels.fmodel_xray().xray_structure)
    #assert minimized.xray_structure is self.model.get_xray_structure()
    #utils.assert_xray_structures_equal(
    #  x1 = minimized.xray_structure,
    #  x2 = self.model.get_xray_structure())
    #return minimized

  def run_lbfgs(self):
    if(self.model.get_ncs_groups() is None or
       not self.all_params.ncs.constraints.apply_to_adp):
      lbfgs_termination_params = scitbx.lbfgs.termination_parameters(
        max_iterations = self.individual_adp_params.iso.max_number_of_iterations)
      is_neutron_scat_table = False
      if(self.all_params.main.scattering_table == "neutron"):
        is_neutron_scat_table = True
      minimized = minimization.lbfgs(
        restraints_manager       = self.model.restraints_manager,
        fmodels                  = self.fmodels,
        model                    = self.model,
        refine_adp               = True,
        is_neutron_scat_table    = is_neutron_scat_table,
        lbfgs_termination_params = lbfgs_termination_params,
        iso_restraints           = self.adp_restraints_params.iso,
        verbose                  = 0,
        target_weights           = self.target_weights,
        h_params                 = self.h_params)
    elif(self.all_params.ncs.constraints.apply_to_coordinates):
      fmodel = self.fmodels.fmodel_xray()
      # update NCS groups
      import mmtbx.ncs.ncs_utils as nu
      nu.get_list_of_best_ncs_copy_map_correlation(
        ncs_groups = self.model.get_ncs_groups(),
        fmodel     = fmodel)
      assert "individual_adp" in self.all_params.refine.strategy
      minimized = minimization.run_constrained(
        model         = self.model,
        fmodel        = fmodel,
        target_weight = self.target_weights.xyz_weights_result.wx,
        log           = self.log,
        params        = self.all_params,
        refine_u_iso  = True,
        prefix        = "NCS constrained ADP refinement").minimized
      self.model.set_xray_structure(fmodel.xray_structure)
    else: raise RuntimeError("Bad ncs options.")

class weight_result(object):
  def __init__(self, r_work, r_free, delta_b, mean_b, weight, xray_target,
      neutron_r_work, neutron_r_free, u_star, u_iso):
    adopt_init_args(self, locals())
    self.r_gap = r_free - r_work

  def show(self, out, prefix=""):
    if (out is None) : return
    if(len(prefix.strip())>0): prefix += " "
    format = prefix+"%5.2f %5.2f %6.2f %6.3f  %6.3f %6.3f   %6.3f"
    print(format % (self.r_work, self.r_free, self.r_gap, self.delta_b,
      self.mean_b, self.weight, self.xray_target), file=out)
    if (self.neutron_r_work is not None):
      print("", file=out)
      print("Neutron data: r_work=%5.2f r_free=%5.2f"%(
        self.neutron_r_work, self.neutron_r_free), file=out)


 *******************************************************************************


 *******************************************************************************
mmtbx/refinement/anomalous_scatterer_groups.py
from __future__ import absolute_import, division, print_function
from cctbx.array_family import flex
from scitbx import lbfgs
from libtbx.str_utils import make_sub_header
from libtbx.test_utils import approx_equal
from libtbx import adopt_init_args
import time
import sys
from six.moves import zip
from six.moves import range

class minimizer(object):

  def __init__(self,
        fmodel,
        groups,
        call_back_after_minimizer_cycle=None,
        number_of_minimizer_cycles=3,
        lbfgs_max_iterations=20,
        number_of_finite_difference_tests=0):
    adopt_init_args(self, locals())
    self.x = flex.double()
    for group in groups:
      if (group.refine_f_prime): self.x.append(group.f_prime)
      if (group.refine_f_double_prime): self.x.append(group.f_double_prime)
    fmodel.xray_structure.scatterers().flags_set_grads(state=False)
    for group in groups:
      if (group.refine_f_prime):
        fmodel.xray_structure.scatterers().flags_set_grad_fp(
          iselection=group.iselection)
      if (group.refine_f_double_prime):
        fmodel.xray_structure.scatterers().flags_set_grad_fdp(
          iselection=group.iselection)
    self.target_functor = fmodel.target_functor()
    self.target_functor.prepare_for_minimization()
    for self.i_cycle in range(number_of_minimizer_cycles):
      self.lbfgs = lbfgs.run(
        target_evaluator=self,
        termination_params=lbfgs.termination_parameters(
          max_iterations=lbfgs_max_iterations),
        exception_handling_params=lbfgs.exception_handling_parameters(
          ignore_line_search_failed_step_at_lower_bound = True))
      if (call_back_after_minimizer_cycle is not None):
        self.unpack()
        if (not call_back_after_minimizer_cycle(minimizer=self)):
          break
    if (call_back_after_minimizer_cycle is None):
      self.unpack()
    del self.i_cycle
    del self.lbfgs
    del self.x
    del self.target_functor
    del self.fmodel
    del self.groups

  def unpack(self):
    xi = iter(self.x)
    for group in self.groups:
      if (group.refine_f_prime): group.f_prime = next(xi)
      if (group.refine_f_double_prime): group.f_double_prime = next(xi)
    for group in self.groups:
      group.copy_to_scatterers_in_place(
        scatterers=self.fmodel.xray_structure.scatterers())
    self.fmodel.update_xray_structure(update_f_calc=True)

  def compute_functional_and_gradients(self):
    self.unpack()
    t_r = self.target_functor(compute_gradients=True)
    fmodel = self.fmodel
    f = t_r.target_work()
    d_target_d_f_calc = t_r.d_target_d_f_calc_work()
    sfg = fmodel.structure_factor_gradients_w(
      u_iso_refinable_params=None,
      d_target_d_f_calc=d_target_d_f_calc.data(),
      xray_structure=fmodel.xray_structure,
      n_parameters=0,
      miller_set=d_target_d_f_calc,
      algorithm=fmodel.sfg_params.algorithm)
    d_t_d_fp = sfg.d_target_d_fp()
    d_t_d_fdp = sfg.d_target_d_fdp()
    del sfg
    g = flex.double()
    for group in self.groups:
      if (group.refine_f_prime):
        g.append(flex.sum(d_t_d_fp.select(group.iselection)))
      if (group.refine_f_double_prime):
        g.append(flex.sum(d_t_d_fdp.select(group.iselection)))
    if (self.number_of_finite_difference_tests != 0):
      self.number_of_finite_difference_tests -= 1
      g_fin = []
      eps = 1.e-5
      x = self.x
      for i in range(x.size()):
        fs = []
        xi0 = x[i]
        for signed_eps in [eps,-eps]:
          x[i] = xi0 + signed_eps
          self.unpack()
          x[i] = xi0
          t_r = self.target_functor(compute_gradients=False)
          fs.append(t_r.target_work())
        g_fin.append((fs[0]-fs[1])/(2*eps))
      self.unpack()
      assert approx_equal(g_fin, g)
    return f, g

def get_single_atom_selection_string(atom):
  labels = atom.fetch_labels()
  altloc = labels.altloc
  if (altloc == '') : altloc = ' ' # XXX this is gross
  sele = \
    "chain '%s' and resname %s and name '%s' and altloc '%s' and resid %s" % \
      (labels.chain_id, labels.resname, labels.name, altloc, labels.resid())
  return sele

def find_anomalous_scatterer_groups(
    pdb_atoms,
    xray_structure,
    group_same_element=True, # XXX should this be True by default?
    out=None):
  """
  Automatic setup of anomalously scattering atoms, defined here as anything
  with atomic number 15 (P) or greater.  Not yet accessible from phenix.refine.
  """
  from cctbx.eltbx import sasaki
  from cctbx import xray
  if (out is None) : out = sys.stdout
  element_i_seqs = {}
  groups = []
  if (out is None) : out = null_out()
  hd_selection = xray_structure.hd_selection()
  for i_seq, scatterer in enumerate(xray_structure.scatterers()):
    if (hd_selection[i_seq]):
      continue
    element = scatterer.element_symbol().strip()
    try :
      atomic_number = sasaki.table(element).atomic_number()
    except RuntimeError as e :
      print("Error for %s" % pdb_atoms[i_seq].id_str(), file=out)
      print("  " + str(e), file=out)
      continue
    if (atomic_number >= 15):
      if (group_same_element):
        if (not element in element_i_seqs):
          element_i_seqs[element] = flex.size_t()
        element_i_seqs[element].append(i_seq)
      else :
        print("  creating anomalous group for %s" % \
          pdb_atoms[i_seq].id_str(), file=out)
        asg = xray.anomalous_scatterer_group(
          iselection=flex.size_t([i_seq]),
          f_prime=0,
          f_double_prime=0,
          refine=["f_prime","f_double_prime"],
          selection_string=get_single_atom_selection_string(pdb_atoms[i_seq]))
        groups.append(asg)
  if (group_same_element):
    for elem in sorted(element_i_seqs.keys()):
      iselection = element_i_seqs[elem]
      print("  creating anomalous group for element %s with %d atoms" % \
        (elem, len(iselection)), file=out)
      asg = xray.anomalous_scatterer_group(
        iselection=iselection,
        f_prime=0,
        f_double_prime=0,
        refine=["f_prime","f_double_prime"],
        selection_string="element %s" % elem)
      groups.append(asg)
  return groups

def refine_anomalous_substructure(
    fmodel,
    pdb_hierarchy,
    wavelength=None,
    map_type="anom_residual",
    exclude_waters=False,
    exclude_non_water_light_elements=True,
    n_cycles_max=None,
    map_sigma_min=3.0,
    refine=("f_prime","f_double_prime"),
    reset_water_u_iso=True,
    use_all_anomalous=True,
    verbose=True,
    out=sys.stdout):
  """
  Crude mimic of Phaser's substructure completion, with two essential
  differences: only the existing real scatterers in the input model will be
  used (with the assumption that the model is already more or less complete),
  and the anomalous refinement will be performed in Phenix, yielding both
  f-prime and f-double-prime.  The refined f-prime provides us with an
  orthogonal estimate of the number of electrons missing from an incorrectly
  labeled scatterer.

  :param wavelength: X-ray wavelenth in Angstroms
  :param exclude_waters: Don't refine anomalous scattering for water oxygens
  :param exclude_non_water_light_elements: Don't refine anomalous scattering
    for light atoms other than water (CHNO).
  :param n_cycles_max: Maximum number of refinement cycles
  :param map_sigma_min: Sigma cutoff for identify anomalous scatterers
  :param reset_water_u_iso: Reset B-factors for water atoms prior to f'
    refinement
  :param use_all_anomalous: include any scatterers which are already modeled
    as anomalous in the refinement
  """
  from cctbx import xray
  assert (fmodel.f_obs().anomalous_flag())
  assert (map_type in ["llg", "anom_residual"])
  make_sub_header("Iterative anomalous substructure refinement", out=out)
  fmodel.update(target_name="ls")
  pdb_atoms = pdb_hierarchy.atoms()
  non_water_non_hd_selection = pdb_hierarchy.atom_selection_cache().selection(
    "(not element H and not element D and not resname HOH)")
  sites_frac = fmodel.xray_structure.sites_frac()
  scatterers = fmodel.xray_structure.scatterers()
  u_iso_mean = flex.mean(
    fmodel.xray_structure.extract_u_iso_or_u_equiv().select(
      non_water_non_hd_selection))
  anomalous_iselection = flex.size_t()
  anomalous_groups = []
  t_start = time.time()
  n_cycle = 0
  while ((n_cycles_max is None) or (n_cycle < n_cycles_max)):
    n_cycle += 1
    n_new_groups = 0
    t_start_cycle = time.time()
    print("Cycle %d" % n_cycle, file=out)
    anom_map = fmodel.map_coefficients(map_type=map_type).fft_map(
      resolution_factor=0.25).apply_sigma_scaling().real_map_unpadded()
    map_min = abs(flex.min(anom_map.as_1d()))
    map_max = flex.max(anom_map.as_1d())
    print("  map range: -%.2f sigma to %.2f sigma" % (map_min, map_max), file=out)
    reset_u_iso_selection = flex.size_t()
    for i_seq, atom in enumerate(pdb_atoms):
      resname = atom.parent().resname
      elem = atom.element.strip()
      if  ((i_seq in anomalous_iselection) or
           ((exclude_waters) and (resname == "HOH")) or
           ((elem in ["H","D","N","C","O"]) and (resname != "HOH") and
            exclude_non_water_light_elements)):
        continue
      scatterer = scatterers[i_seq]
      site_frac = sites_frac[i_seq]
      anom_map_value = anom_map.tricubic_interpolation(site_frac)
      if ((anom_map_value >= map_sigma_min) or
          ((scatterer.fdp != 0) and use_all_anomalous)):
        if (verbose):
          if (n_new_groups == 0):
            print("", file=out)
            print("  new anomalous scatterers:", file=out)
          print("    %-34s  map height: %6.2f sigma" % (atom.id_str(),
            anom_map_value), file=out)
        anomalous_iselection.append(i_seq)
        selection_string = get_single_atom_selection_string(atom)
        group = xray.anomalous_scatterer_group(
          iselection=flex.size_t([i_seq]),
          f_prime=0,
          f_double_prime=0,
          refine=list(refine),
          selection_string=selection_string)
        anomalous_groups.append(group)
        n_new_groups += 1
        if (resname == "HOH") and (reset_water_u_iso):
          water_u_iso = scatterer.u_iso
          if (water_u_iso < u_iso_mean):
            reset_u_iso_selection.append(i_seq)
    if (n_new_groups == 0):
      print("", file=out)
      print("No new groups - anomalous scatterer search terminated.", file=out)
      break
    elif (not verbose):
      print("  %d new groups" % n_new_groups, file=out)
    for i_seq in anomalous_iselection :
      sc = scatterers[i_seq]
      sc.fp = 0
      sc.fdp = 0
    if (verbose):
      print("", file=out)
      print("Anomalous refinement:", file=out)
      fmodel.info().show_targets(text="before minimization", out=out)
      print("", file=out)
    u_iso = fmodel.xray_structure.extract_u_iso_or_u_equiv()
    u_iso.set_selected(reset_u_iso_selection, u_iso_mean)
    fmodel.xray_structure.set_u_iso(values=u_iso)
    fmodel.update_xray_structure(update_f_calc=True)
    minimizer(fmodel=fmodel, groups=anomalous_groups)
    if (verbose):
      fmodel.info().show_targets(text="after minimization", out=out)
      print("", file=out)
      print("  Refined sites:", file=out)
      for i_seq, group in zip(anomalous_iselection, anomalous_groups):
        print("    %-34s  f' = %6.3f  f'' = %6.3f" % (
          pdb_atoms[i_seq].id_str(), group.f_prime, group.f_double_prime), file=out)
    t_end_cycle = time.time()
    print("", file=out)
    if (verbose):
      print("  time for this cycle: %.1fs" % (t_end_cycle-t_start_cycle), file=out)
  fmodel.update(target_name="ml")
  print("%d anomalous scatterer groups refined" % len(anomalous_groups), file=out)
  t_end = time.time()
  print("overall time: %.1fs" % (t_end - t_start), file=out)
  return anomalous_groups


 *******************************************************************************


 *******************************************************************************
mmtbx/refinement/calculators.py
from __future__ import division
from libtbx import adopt_init_args
from cctbx import adptbx

class individual(object):
  def __init__(self,
               data,
               x,
               data_weight=1,
               restraints=None,
               restraints_weight=None):
    adopt_init_args(self, locals())
    self.t = None
    self.g = None
    self.d = None
    self.use_curvatures=False
    self.n = x.size()
    self.lower_bound = restraints.lower_bound()
    self.upper_bound = restraints.upper_bound()
    self.bound_flags = restraints.bound_flags()
    #
    self.update(x = self.x)
    self.f_start = self.t

  def __call__(self):
    f, g = self.target_and_gradients()
    return self.x, f, g

  def target(self): return self.t

  def gradients(self): return self.g

  def update(self, x):
    self.data.update(x = x)
    if self.restraints is not None:
      self.restraints.update(x = x)
    self.x = x
    self.t, self.g = 0, 0
    for p in [[self.data,       self.data_weight],
              [self.restraints, self.restraints_weight]]:
      source, weight = p
      if not None in [source, weight]:
        t,g = source.target(), source.gradients()
        if t is not None:
          self.t += t*weight
          self.g += g*weight

  def target_and_gradients(self):
    self.update(x = self.x)
    return self.t, self.g

  def compute_functional_and_gradients(self):
    return self.target_and_gradients()

class xyz(object):
  def __init__(self,
               data              = None,
               restraints        = None,
               selection         = None,
               data_weight       = 1.,
               restraints_weight = 1.,
               max_shift         = None):
    adopt_init_args(self, locals())
    assert isinstance(max_shift, float)
    assert [data, restraints].count(None) != 2
    if data is not None:
      data.set_refine_sites(selection = selection)
      x = data.get_x()
    if restraints is not None:
      restraints.set_use_xyz(selection = selection, max_shift = max_shift)
      x = restraints.get_x()
    self._calculator = individual(
      data              = data,
      restraints        = restraints,
      data_weight       = data_weight,
      restraints_weight = restraints_weight,
      x                 = x)

  def calculator(self):
    return self._calculator

class adp(object):
  def __init__(self,
               data              = None,
               restraints        = None,
               selection         = None,
               data_weight       = 1,
               restraints_weight = None,
               u_min             = None,
               u_max             = None):
    adopt_init_args(self, locals())
    assert [data, restraints].count(None) != 2
    if data is not None:
      data.set_refine_u_iso(selection = selection)
      x = data.get_x()
    if restraints is not None:
      restraints.set_use_adp(
        selection = selection,
        b_min     = adptbx.u_as_b(u_min),
        b_max     = adptbx.u_as_b(u_max))
      x = restraints.get_x()
    self._calculator = individual(
      data              = data,
      restraints        = restraints,
      data_weight       = data_weight,
      restraints_weight = restraints_weight,
      x                 = x)

  def calculator(self):
    return self._calculator

class occ(object):
  def __init__(self,
               data              = None,
               restraints        = None,
               selection         = None,
               data_weight       = 1,
               restraints_weight = None,
               q_min             = None,
               q_max             = None):
    adopt_init_args(self, locals())
    assert [data, restraints].count(None) != 2
    if data is not None:
      data.set_refine_occupancy(selection = selection)
      x = data.get_x()
    if restraints is not None:
      restraints.set_use_occ(
        selection = selection,
        q_min     = q_min,
        q_max     = q_max)
      x = restraints.get_x()
    self._calculator = individual(
      data              = data,
      restraints        = restraints,
      data_weight       = data_weight,
      restraints_weight = restraints_weight,
      x                 = x)

  def calculator(self):
    return self._calculator


 *******************************************************************************


 *******************************************************************************
mmtbx/refinement/data.py
from __future__ import absolute_import, division
from cctbx.array_family import flex
from libtbx import adopt_init_args

class fs(object):
  """
  Fourier (reciprocal) space target and gradinets manager.
  'selection' selects parameters to be refined.
  """
  def __init__(self,
               fmodel,
               sites_cart = False,
               u_iso = False,
               occupancy = False):
    adopt_init_args(self, locals())
    self.scatterers = self.fmodel.xray_structure.scatterers()
    self.size = self.scatterers.size()
    self.target_functor_xray = fmodel.target_functor(
      alpha_beta = None # XXX Check what that means
      )
    self.scatterers.flags_set_grads(state=False)
    all_selection = self.fmodel.xray_structure.all_selection()
    self.selection = all_selection
    self.sites_cart, self.u_iso, self.occupancy = None, None, None

  def get_x(self):
    assert [self.sites_cart, self.u_iso, self.occupancy].count(True) == 1
    xrs = self.fmodel.xray_structure
    if   self.sites_cart: return xrs.sites_cart().as_double()
    elif self.u_iso:      return xrs.extract_u_iso_or_u_equiv()
    elif self.occupancy:  return self.scatterers.extract_occupancies()
    else: assert 0

  def _set_flags(self, scf, selection):
    assert [self.sites_cart, self.u_iso, self.occupancy].count(True) == 1

    #self.scatterers = self.fmodel.xray_structure.scatterers()

    self.scatterers.flags_set_grads(state=False)

    if selection is not None:
      assert isinstance(selection, flex.bool)
      self.selection = selection
    scf(iselection = self.selection.iselection())

  def set_refine_occupancy(self, selection = None):
    self.sites_cart, self.u_iso, self.occupancy = False, False, True
    self._set_flags(
      scf       = self.scatterers.flags_set_grad_occupancy,
      selection = selection)

  def set_refine_u_iso(self, selection = None):
    self.sites_cart, self.u_iso, self.occupancy = False, True, False
    self._set_flags(
      scf       = self.scatterers.flags_set_grad_u_iso,
      selection = selection)

  def set_refine_sites(self, selection = None):
    self.sites_cart, self.u_iso, self.occupancy = True, False, False
    self._set_flags(
      scf       = self.scatterers.flags_set_grad_site,
      selection = selection)

  def update(self, x):
    xrs = self.fmodel.xray_structure
    if  (self.sites_cart): xrs.set_sites_cart(sites_cart = flex.vec3_double(x))
    elif(self.u_iso):      xrs.set_u_iso(values=x, selection=self.selection)
    elif(self.occupancy):  xrs.set_occupancies(value=x, selection=self.selection)
    self.fmodel.update_xray_structure(update_f_calc = True)
    self.tg = self.target_functor_xray(compute_gradients = True)

  def target(self):
    return self.tg.target_work()

  def gradients(self):
    if self.selection is None:
      return self.tg.gradients_wrt_atomic_parameters().packed()
      assert 0
    else:
      if not self.fmodel.is_taam(): # XXX discamb
        if not self.sites_cart:
          g = self.tg.gradients_wrt_atomic_parameters().packed()
          result = flex.double(self.size, 0)
          result.set_selected(self.selection, g)
          return result
        else:
          g = self.tg.d_target_d_site_cart()
          result = flex.vec3_double(self.size, [0,0,0])
          result.set_selected(self.selection, g)
          return result.as_double()
      else:                                                                 # XXX discamb
        d_target_d_fcalc = self.tg.d_target_d_f_calc_work()                 # XXX discamb
        self.fmodel.discamb_wrapper.set_indices(d_target_d_fcalc.indices()) # XXX discamb
        gradients = self.fmodel.discamb_wrapper.d_target_d_params(          # XXX discamb
          list(d_target_d_fcalc.data()))                                    # XXX discamb
        if self.sites_cart:                                                            # XXX discamb
          return flex.vec3_double([g.site_derivatives for g in gradients]).as_double() # XXX discamb
        elif self.u_iso:                                                               # XXX discamb
          g = flex.double([g.adp_derivatives[0] for g in gradients])                   # XXX discamb
          result = flex.double(self.size, 0)                                           # XXX discamb
          result.set_selected(self.selection, g)                                       # XXX discamb
          return result.as_double()                                                    # XXX discamb
        elif self.occupancy:                                                           # XXX discamb
          assert 0                                                                     # XXX discamb
          #return flex.double([g.occupancy_derivatives for g in gradients])            # XXX discamb
        else: assert 0                                                                 # XXX discamb


 *******************************************************************************


 *******************************************************************************
mmtbx/refinement/energy_monitor.py
from __future__ import absolute_import, division, print_function
from libtbx import group_args

to_kcal_mol = { 'ev'      : 23.0609,
                'hartree' : 627.503,
  }

rename = {'pocket+energy-bound': 'Binding Energy',
          'energy-strain' : 'Unbound Energy',
          # 'energy' : 'Bound Energy'
          }

def _print_energy_in_kcal(e, units):
  if units.lower() in to_kcal_mol:
    return '%15.1f %s' % (e*to_kcal_mol[units.lower()], 'kcal/mol')
  else:
    return '%15.1f %s' % (e, units)

def print_energy_in_kcal(ga):
  s=[]
  if ga is None: return s
  for d, e, l, b, c in ga.energies:
    units=ga.units.lower()
    if d in ['opt', 'bound']: atoms=b
    elif d in ['energy', 'strain']: atoms=l
    elif d in ['pocket']: atoms=b-l
    else: assert 0
    d=rename.get(d,d)
    s.append('%-22s %s (atoms %4d, charge %2d)  ' % (d,
                                          _print_energy_in_kcal(e, units), atoms, c))
  return s

class energies(list):
  def __init__(self):
    pass

  def as_string(self, verbose=False):
    # from libtbx import easy_pickle
    # easy_pickle.dump('ga.pickle', self)
    plusses = [ ['pocket', 'energy'],
                # ['pocket', 'strain'],
      ]
    pairs = [['bound', 'opt'],
             ['bound-opt', 'strain'],
             ['pocket+energy', 'bound'],
             # ['pocket+energy-strain', 'bound'],
             ['energy', 'strain'],
      ]
    s=''
    tmp = {}
    t_atoms = {}
    for i, gas in enumerate(self):
      tmp.setdefault(i, {})
      t_atoms.setdefault(i, {})
      t=''
      units = None
      for j, ga in enumerate(gas):
        if ga:
          units=ga.units
          for d, e, l, b, c in ga.energies:
            tmp[i][d]=e
            t_atoms[i][d]=b
        rc = print_energy_in_kcal(ga)
        if rc:
          for line in rc:
            t += '%s%s\n' % (' '*6, line)
      if verbose: print('macro_cycle %d %s' % (i+1,t))

      for k1, k2 in plusses:
        if not (t_atoms[i].get(k1, False) and t_atoms[i].get(k2, False)):
          continue
        if t_atoms[i][k1]!=t_atoms[i][k2]: continue
        t_atoms[i]['%s+%s' % (k1,k2)]=b
        tmp[i]['%s+%s' % (k1,k2)]=tmp[i][k1]+tmp[i][k2]

      for k1, k2 in pairs:
        if not (t_atoms[i].get(k1, False) and t_atoms[i].get(k2, False)):
          continue
        if t_atoms[i][k1]!=t_atoms[i][k2]: continue
        if k1 in tmp[i] and k2 in tmp[i]:
          e = tmp[i][k1]-tmp[i][k2]
          k3='%s-%s' % (k1,k2)
          k3=rename.get(k3, k3)
          t+='%s%-22s %s (atoms %4d)\n' % (' '*6,
                             k3,
                             _print_energy_in_kcal(e, units),
                             t_atoms[i][k1],
                             )
          tmp[i][k3]=e
          t_atoms[i][k3]=t_atoms[i][k2]
      if i:
        def _add_dE(e1, e2, units):
          b1=None
          s=''
          if e1 and e2:
            if e1[0]==e2[0]:
              if e1[0] in ['opt', 'bound']:
                b1=e1[3]==e2[3]
              if e1[0] in ['strain', 'energy']:
                b1=e1[2]==e2[2]
              if b1:
                de = e2[1]-e1[1]
                s+='%s%-22s %s\n' % (' '*6,
                                     '%s dE' % e2[0],
                                     _print_energy_in_kcal(e2[1]-e1[1],units))
          return s
        e1=e2=None
        for k in range(2):
          if gas[k] and first[k]:
            e2=gas[k].energies
            e1=first[k].energies
            for f1, f2 in zip(e1,e2):
              t+= _add_dE(f1,f2,gas[k].units)
              if verbose: print(i+1,f1,f2,s)
      else:
        first=gas
      if t:
        s+='%sMacro cycle %d\n' % (' '*4, i+1)
        s+=t
    return s

class all_energies(dict):
  def __init__(self):
    pass

  def as_string(self):
    s='QM energies\n'
    for selection, energies in self.items():
      s+='\n  "%s"\n' % selection
      s+='%s' % energies.as_string()
    return s

def digest_return_energy_object(ga, macro_cycle, energy_only, rc=None):
  if rc is None:
    rc = all_energies()
  if ga is None: return rc
  for selection, es in ga.energies.items():
    rc.setdefault(selection, energies())
    while len(rc[selection])<macro_cycle:
      rc[selection].append([None,None,None])
    if energy_only:
      rc[selection][-1][0]=group_args(energies=es,
                                      units=ga.units,
                                      )
    else:
      rc[selection][-1][1]=group_args(energies=es,
                                      units=ga.units,
                                      )
  return rc

if __name__ == '__main__':
  from libtbx import easy_pickle
  e=easy_pickle.load('ga.pickle')
  rc=e.as_string(verbose=0)
  print(rc)


 *******************************************************************************


 *******************************************************************************
mmtbx/refinement/ensemble_refinement/__init__.py
from __future__ import absolute_import, division, print_function
import mmtbx.solvent.ensemble_ordered_solvent as ensemble_ordered_solvent
from mmtbx.refinement.ensemble_refinement import ensemble_utils
from mmtbx.dynamics import ensemble_cd
from mmtbx import conformation_dependent_library as cdl
from mmtbx.conformation_dependent_library import cdl_setup
import mmtbx.tls.tools as tls_tools
import mmtbx.command_line
import mmtbx.utils
import mmtbx.model
import mmtbx.maps
from mmtbx.den import den_restraints
from iotbx.option_parser import iotbx_option_parser
from iotbx import pdb
import iotbx.phil
import iotbx
from phenix import phenix_info
from cctbx.array_family import flex
from cctbx import adptbx
import scitbx.math
from libtbx.utils import Sorry, user_plus_sys_time, multi_out, show_total_time
from libtbx import adopt_init_args, slots_getstate_setstate
from libtbx.str_utils import format_value, make_header
from libtbx import runtime_utils
from libtbx import easy_mp
import libtbx.load_env
from six.moves import cStringIO as StringIO
from six.moves import cPickle as pickle
import random
import gzip
import math
import os
import sys
from six.moves import range

# these supersede the defaults in included scopes
customization_params = iotbx.phil.parse("""
ensemble_refinement.den.kappa_burn_in_cycles = 0
ensemble_refinement.cartesian_dynamics.number_of_steps = 10
ensemble_refinement.ensemble_ordered_solvent.b_iso_min = 0.0
ensemble_refinement.ensemble_ordered_solvent.b_iso_max = 100.0
ensemble_refinement.ensemble_ordered_solvent.find_peaks.map_next_to_model.max_model_peak_dist = 3.0
ensemble_refinement.ensemble_ordered_solvent.find_peaks.map_next_to_model.use_hydrogens = False
ensemble_refinement.pdb_interpretation.clash_guard.nonbonded_distance_threshold = -1.0
ensemble_refinement.pdb_interpretation.clash_guard.max_number_of_distances_below_threshold = 100000000
ensemble_refinement.pdb_interpretation.clash_guard.max_fraction_of_distances_below_threshold = 1.0
ensemble_refinement.pdb_interpretation.proceed_with_excessive_length_bonds=True
""")

# the extra fetch() at the end with the customized parameters gives us the
# actual master phil object
input_phil = mmtbx.command_line.generate_master_phil_with_inputs(
  phil_string="",
  enable_experimental_phases=True,
  as_phil_string=True)
master_params = iotbx.phil.parse(input_phil + """
ensemble_refinement {
  cartesian_dynamics
    .style = menu_item auto_align box
  {
    include scope mmtbx.dynamics.cartesian_dynamics.master_params
    protein_thermostat = True
      .type = bool
      .help = Use protein atoms thermostat
  }
    den_restraints = True
      .type = bool
      .help = 'Use DEN restraints'
    den
      {
      include scope mmtbx.den.den_params
      }
  update_sigmaa_rfree = 0.001
    .type = float
    .help = test function
  ensemble_reduction = True
    .type = bool
    .help = 'Find miminium number of structures to reproduce simulation R-values'
  ensemble_reduction_rfree_tolerance = 0.0025
    .type = float
  verbose = -1
    .type = int
  output_file_prefix = None
    .type = str
    .help = 'Prefix for all output files'
# TODO
#  write_mmcif_file = False
#    .type = bool
  gzip_final_model = True
    .type = bool
    .style = hidden
  write_centroid_model = False
    .type = bool
    .style = hidden
  write_mean_model = False
    .type = bool
    .style = hidden
  random_seed = 2679941
    .type = int
    .help = 'Random seed'
  nproc = 1
    .type = int
    .short_caption = Number of processors
    .style = renderer:draw_nproc_widget
  tx = None
    .type = float
    .help = 'Relaxation time (ps)'
    .short_caption = Relaxation time (ps)
  equilibrium_n_tx = 10
    .type = int
    .help = 'Length of equilibration period, n times tx'
    .short_caption = Length of equilibration period
  acquisition_block_n_tx = 20
    .type = int
    .help = 'Length of acquisition block, n times tx'
    .short_caption = Length of acquisition block
  number_of_acquisition_periods = 1
    .type = int
    .help = 'Number of acquisition periods'
  pdb_stored_per_block = 200
    .type = int
    .help = 'Number of model coordinates stored per acquisition block'
    .short_caption = Models stored per acquisition block
  wxray_coupled_tbath = True
    .type = bool
    .help = 'Use temperature control wxray'
    .short_caption = Use temperature control X-ray weight
  wxray_coupled_tbath_offset = 5.0
    .type = float
    .help = 'Temperature offset, increasing offset increases wxray'
    .short_caption = Temperature ofset
  wxray = 1.0
    .type = float
    .help = 'Multiplier for xray weighting; used if wxray_coupled_tbath = Flase'
    .short_caption = X-ray weight
  tls_group_selections = None
    .type = atom_selection
    .multiple = True
    .help = 'TLS groups to use for TLS fitting (TLS details in PDB header not used)'
    .style = use_list
  ptls = 0.80
    .type = floats
    .optional = False
    .help = 'The fraction of atoms to include in TLS fitting'
    .short_caption = Fraction of atoms to include in TLS fitting
    .style = bold
  import_tls_pdb = None
    .type = str
    .help = 'PDB path to import TLS from external structure'
    .short_caption = PDB path to import TLS from external structure
    .style = bold
  max_ptls_cycles = 25
    .type = int
    .help = 'Maximum cycles to use in TLS fitting; TLS will stop prior to this if convergence is reached'
    .short_caption = Max. number of cycles of TLS fitting
  isotropic_b_factor_model = False
    .type = bool
    .help = 'Use isotropic B-factor model instead of TLS'
    .short_caption = Use isotropic B-factor model
  pwilson = 0.8
    .type = float
    .help = 'Scale factor for isotropic b-factor model: all atoms = Bwilson * pwilson'
    .short_caption = Scale factor for isotropic b-factor model
  set_occupancies = False
    .type = bool
    .help = 'Set all atoms aoccupancy to 1.0'
    .short_caption = Reset occupancies to 1.0
  target_name = *ml mlhl ls_wunit_k1_fixed ls_wunit_k1
    .type = choice
    .short_caption = Refinement target
    .help = 'Choices for refinement target'
  remove_alt_conf_from_input_pdb = True
    .type = bool
    .help = 'Removes any alternative conformations if present in input PDB model'
  scale_wrt_n_calc_start = False
    .type = bool
    .help = 'Scale <Ncalc> to starting Ncalc'
    .short_caption = Scale <Ncalc> to starting Ncalc
  output_running_kinetic_energy_in_occupancy_column = False
    .type = bool
    .help = 'Output PDB file contains running kinetic energy in place of occupancy'
  ordered_solvent_update = True
    .type = bool
    .help = 'Ordered water molecules automatically updated every nth macro cycle'
  ordered_solvent_update_cycle = 25
    .type = int
    .help = 'Number of macro-cycles per ordered solvent update'
    .short_caption = Solvent update cycles
  harmonic_restraints
    .style = menu_item auto_align box
  {
    selections = None
      .type = atom_selection
      .help = 'Atoms selections to apply harmonic restraints'
    weight = 0.001
      .type = float
      .help = 'Harmonic restraints weight'
    slack = 1.0
      .type = float
      .help = 'Harmonic restraints slack distance'
  }
  electron_density_maps
    .style = menu_item
  {
    apply_default_maps = True
      .type = bool
    include scope mmtbx.maps.map_coeff_params_str
  }
  mask
    .short_caption = Bulk solvent mask
    .style = menu_item auto_align box
  {
    include scope mmtbx.masks.mask_master_params
  }
  ensemble_ordered_solvent
    .style = menu_item auto_align box
  {
    diff_map_cutoff = 2.5
      .type = float
    e_map_cutoff_keep = 0.5
      .type = float
    e_map_cutoff_find = 0.5
      .type = float
    tolerance = 0.9
      .type = float
    ordered_solvent_map_to_model = True
      .type = bool
    include scope mmtbx.solvent.ordered_solvent.output_params_str
    primary_map_type = mFo-DFmodel
      .type=str
    primary_map_cutoff = 3.0
      .type=float
    secondary_map_type = 2mFo-DFmodel
      .type=str
    secondary_map_cutoff_keep = 1.0
      .type=float
    secondary_map_cutoff_find = 1.0
      .type=float
    include scope mmtbx.solvent.ordered_solvent.h_bond_params_str
    include scope mmtbx.solvent.ordered_solvent.adp_occ_params_str
    refine_occupancies = False
      .type = bool
      .help = Refine solvent occupancies.
      .expert_level = 1
    add_hydrogens = False
      .type = bool
      .help = Adds hydrogens to water molecules (except those on special positions)
    refilter = True
      .type = bool
    temperature = 300
      .type = float
      .help = Target temperature for random velocity assignment
    seed = 343534534
      .type = int
      .help = Fixes the random seed for velocity assignment
    preserved_solvent_minimum_distance = 7.0
      .type = float
    find_peaks {
      include scope mmtbx.find_peaks.master_params
    }
  }
  include scope mmtbx.geometry_restraints.external.external_energy_params_str
  include scope mmtbx.monomer_library.pdb_interpretation.grand_master_phil_str
}
gui
  .help = Phenix GUI parameters, not used in command-line program
{
  include scope libtbx.phil.interface.tracking_params
  output_dir = None
    .type = path
    .short_caption = Output directory
    .style = output_dir
}
extra_restraints_file = None
  .type = path
  .short_caption = Custom geometry restraints
  .help = File containing custom geometry restraints, using the same format \
    as phenix.refine.  On the command line this can be specified directly \
    as a command-line argument, but this parameter is used by the Phenix GUI.
  .style = input_file file_type:phil
""", process_includes=True).fetch(source=customization_params)

class er_pickle(object):
  def __init__(self,
               pickle_object,
               pickle_filename):
    pickle.dump(pickle_object, gzip.open(pickle_filename, 'wb'))

class ensemble_refinement_data(object):
  def __init__(self, f_calc_running                      = None,
                     f_calc_data_total                   = None,
                     f_calc_data_current                 = None,
                     f_mask_running                      = None,
                     f_mask_current                      = None,
                     f_mask_total                        = None,
                     total_SF_cntr                       = 0,
                     total_SF_cntr_mask                  = 0,
                     fix_scale_factor                    = None,
                     non_solvent_temp                    = None,
                     solvent_temp                        = None,
                     system_temp                         = None,
                     xray_structures                     = [],
                     pdb_hierarchys                      = [],
                     xray_structures_diff_map            = [],
                     seed                                = None,
                     velocities                          = None,
                     ke_protein_running                  = None,
                     ke_pdb                              = [],
                     geo_grad_rms                        = None,
                     xray_grad_rms                       = None,
                     solvent_sel                         = None,
                     all_sel                             = None,
                     er_harmonic_restraints_info         = None,
                     er_harmonic_restraints_weight       = 0.001,
                     er_harmonic_restraints_slack        = 1.0,
                     macro_cycle                         = None,
                     ):
    adopt_init_args(self, locals())

class er_tls_manager(object):
  def __init__(self, tls_selection_strings_no_sol       = None,
                     tls_selection_strings_no_sol_no_hd = None,
                     tls_selections_with_sol            = None,
                     tls_selections_no_sol              = None,
                     tls_selections_no_sol_no_hd        = None,
                     tls_operators                      = None):
    adopt_init_args(self, locals())

class run_ensemble_refinement(object):
  def __init__(self, fmodel,
                     model,
                     log,
                     raw_data,
                     raw_flags,
                     params,
                     ptls,
                     run_number=None):
    adopt_init_args(self, locals())
    if self.params.target_name in ['ml', 'mlhl'] :
      self.fix_scale = False
    else:
      self.fix_scale = True
    if not self.params.wxray_coupled_tbath:
      self.params.wxray_coupled_tbath_offset = 0.0
    self.wxray = self.params.wxray
    self.params.ensemble_ordered_solvent.temperature = self.params.cartesian_dynamics.temperature
    self.ensemble_utils = ensemble_utils.manager(ensemble_obj = self)
    self.xray_gradient = None
    self.fc_running_ave = None
    self.macro_cycle = 1
    self.sf_model_ave = None
    self.fmodel_total_block_list = []
    self.reset_velocities = True
    self.cmremove = True
    self.cdp = self.params.cartesian_dynamics
    self.bsp = mmtbx.bulk_solvent.bulk_solvent_and_scaling.master_params.extract()
    if (self.params.target_name == 'mlhl'):
      self.bsp.target = 'ml'
    else :
      self.bsp.target = self.params.target_name
    if self.params.tx == None:
      print("\nAutomatically set Tx (parameter not defined)", file=log)
      print("Tx          :  2(1/dmin)**2", file=log)
      self.params.tx = round(2.0 * ((1.0/self.fmodel.f_obs().d_min())**2),1)
      print('Dmin        : ', self.fmodel.f_obs().d_min(), file=log)
      print('Set Tx      : ', self.params.tx, file=log)
    self.n_mc_per_tx = self.params.tx / (self.cdp.time_step * self.cdp.number_of_steps)

    # Set simulation length
    make_header("Simulation length:", out = self.log)
    print("Number of time steps per macro cycle    : ", self.cdp.number_of_steps, file=log)
    print("Tx                                      : ", self.params.tx, file=log)
    print("Number macro cycles per Tx period       : ", self.n_mc_per_tx, file=log)
    self.equilibrium_macro_cycles = int(self.n_mc_per_tx * self.params.equilibrium_n_tx)
    self.acquisition_block_macro_cycles = int(self.n_mc_per_tx * self.params.acquisition_block_n_tx)
    self.total_macro_cycles = int(self.equilibrium_macro_cycles \
                            + (self.acquisition_block_macro_cycles * self.params.number_of_acquisition_periods))
    #
    print("\nEquilibration", file=log)
    print("    Number Tx periods    : ", self.params.equilibrium_n_tx, file=log)
    print("    Number macro cycles  : ", self.equilibrium_macro_cycles, file=log)
    print("    Time (ps)            : ", self.equilibrium_macro_cycles \
                                                  * self.cdp.number_of_steps * self.cdp.time_step, file=log)
    #
    print("\nAcquisition block", file=log)
    print("    Number blocks        : ",  self.params.number_of_acquisition_periods, file=log)

    print("    Number Tx periods    : ",  self.params.acquisition_block_n_tx, file=log)
    print("    Number macro cycles  : ",  self.acquisition_block_macro_cycles, file=log)
    print("    Time (ps)            : ",  self.acquisition_block_macro_cycles \
                                                  * self.cdp.number_of_steps\
                                                  * self.cdp.time_step, file=log)
    #
    print("\nSimulation total", file=log)
    print("    Number Tx periods    : ", self.params.equilibrium_n_tx\
                                                + (self.params.number_of_acquisition_periods\
                                                   * self.params.acquisition_block_n_tx), file=log)
    print("    Number macro cycles  : ", self.total_macro_cycles, file=log)
    self.total_time = self.total_macro_cycles\
                        * self.cdp.number_of_steps\
                        * self.cdp.time_step
    print("    Time (ps)            : ", self.total_time, file=log)
    print("    Total = Equilibration + nAcquisition", file=log)
    # Store block
    self.block_store_cycle_cntr = 0
    self.block_store_cycle = \
        list(range(self.acquisition_block_macro_cycles + self.equilibrium_macro_cycles,
              self.acquisition_block_macro_cycles + self.total_macro_cycles,
              self.acquisition_block_macro_cycles))
    # Store pdb
    self.pdb_store_cycle = max(int(self.acquisition_block_macro_cycles \
                         / self.params.pdb_stored_per_block), 1)

    #Setup ensemble_refinement_data_object
    self.er_data = ensemble_refinement_data()
    #Setup fmodels for running average   = refinement target
    #                  total average     = final model
    #                  current model     = model at time point n
    self.fmodel_running = self.fmodel
    self.fmodel_total = None
    self.fmodel_current = None
    self.tls_manager = None
    self.er_data.seed = self.params.random_seed
    self.run_time_stats_dict = {}

    #Dummy miller array
    self.copy_ma = self.fmodel_running.f_masks()[0].array(data = self.fmodel_running.f_masks()[0].data()*0).deep_copy()
    #
    self.fmodel_running.xray_structure = self.model.get_xray_structure()
    assert self.fmodel_running.xray_structure is self.model.get_xray_structure()
    self.pdb_hierarchy = self.model.get_hierarchy()

    #Atom selections
    self.atom_selections()

    self.model.geometry_statistics()

    self.setup_bulk_solvent_and_scale()

    self.fmodel_running.info(
      free_reflections_per_bin = 100,
      max_number_of_bins       = 999).show_rfactors_targets_in_bins(out = self.log)

    if self.params.target_name in ['ml', 'mlhl'] :
      #Must be called before reseting ADPs
      if self.params.scale_wrt_n_calc_start:
        make_header("Calculate Ncalc and restrain to scale kn", out = self.log)
        self.fmodel_running.n_obs_n_calc(update_nobs_ncalc = True)
        n_obs  = self.fmodel_running.n_obs
        n_calc = self.fmodel_running.n_calc
        self.scale_n1_reference = self.scale_helper(target    = n_calc,
                                                    reference = n_obs
                                                    )
        self.scale_n1_target    = self.scale_n1_reference
        self.scale_n1_current   = self.scale_n1_reference
        self.n_calc_reference = self.fmodel_running.n_calc.deep_copy()
        self.n_mc_per_ncalc_update = max(1, int(self.n_mc_per_tx / 10) )
        print("Number macro cycles per tx     : {0:5.0f}".format(self.n_mc_per_tx), file=self.log)
        print("Number macro cycles per update : {0:5.0f}".format(self.n_mc_per_ncalc_update), file=self.log)
        #
        self.fixed_k1_from_start = self.fmodel_running.scale_k1()
        self.target_k1 = self.fmodel_running.scale_k1()
        self.update_normalisation_factors()
      else:
        make_header("Calculate and fix scale of Ncalc", out=self.log)
        self.fmodel_running.n_obs_n_calc(update_nobs_ncalc=True)
        print("Fix Ncalc scale          : True", file=self.log)
        print("Sum current Ncalc        : {0:5.3f}".format(sum(self.fmodel_running.n_calc)), file=self.log)

    #Set ADP model
    self.tls_manager = er_tls_manager()

    # Fit pTLS to starting atomic model
    if self.params.import_tls_pdb is None:
      self.setup_tls_selections(
        tls_group_selection_strings=self.params.tls_group_selections)
      self.fit_tls(input_model=self.model)
    # Import TLS from reference model
    else:
      fit_tlsos, fit_tls_strings = self.import_tls_selections()
      self.setup_tls_selections(tls_group_selection_strings=fit_tls_strings)
      self.model.tls_groups.tlsos = fit_tlsos
      self.tls_manager.tls_operators = fit_tlsos
    # Assign solvent to TLS group
    self.assign_solvent_tls_groups()

    #Set occupancies to 1.0
    if self.params.set_occupancies:
      make_header("Set occupancies to 1.0", out = self.log)
      self.model.get_xray_structure().set_occupancies(
        value      = 1.0)
      self.model.show_occupancy_statistics(out = self.log)
    #Initiates running average SFs
    self.er_data.f_calc_running = self.fmodel_running.f_calc().data().deep_copy()
    #self.fc_running_ave = self.fmodel_running.f_calc()
    self.fc_running_ave = self.fmodel_running.f_calc().deep_copy()

    #Initial sigmaa array, required for ML target function
    #Set eobs and ecalc normalization factors in Fmodel, required for ML
    if self.params.target_name in ['ml', 'mlhl'] :
      self.sigmaa_array = self.fmodel_running.sigmaa().sigmaa().data()
      self.best_r_free = self.fmodel_running.r_free()
      self.fmodel_running.set_sigmaa = self.sigmaa_array

    #Harmonic restraints
    if self.params.harmonic_restraints.selections is not None:
      self.add_harmonic_restraints()

############################## START Simulation ################################
    make_header("Start simulation", out = self.log)
    while self.macro_cycle <= self.total_macro_cycles:
      self.er_data.macro_cycle = self.macro_cycle
      self.time = self.cdp.time_step * self.cdp.number_of_steps * self.macro_cycle
      #XXX Debug
      if False and self.macro_cycle % 10==0:
        print("Sys temp  : ", self.er_data.system_temp, file=self.log)
        print("Xray grad : ", self.er_data.xray_grad_rms, file=self.log)
        print("Geo grad  : ", self.er_data.geo_grad_rms, file=self.log)
        print("Wx        : ", self.wxray, file=self.log)

      if self.fmodel_running.target_name in ['ml', 'mlhl'] :
        if self.macro_cycle < self.equilibrium_macro_cycles:
          if self.params.scale_wrt_n_calc_start and self.macro_cycle%self.n_mc_per_ncalc_update == 0:
            self.update_normalisation_factors()
          elif self.macro_cycle%int(self.n_mc_per_tx)==0:
            self.update_normalisation_factors()

      # Ordered Solvent Update
      if self.params.ordered_solvent_update \
          and (self.macro_cycle == 1\
          or self.macro_cycle%self.params.ordered_solvent_update_cycle == 0):
        self.ordered_solvent_update()

      xrs_previous = self.model.get_xray_structure().deep_copy_scatterers()
      assert self.fmodel_running.xray_structure is self.model.get_xray_structure()

      if self.cdp.verbose >= 1:
        if self.macro_cycle == 1 or self.macro_cycle%100 == 0:
          cdp_verbose = 1
        else:
          cdp_verbose = -1
      else:
        cdp_verbose = -1

      if is_amber_refinement(self.params):
        assert str(self.model.restraints_manager.geometry)=='Amber manager'

      if self.params.den_restraints:
        if self.macro_cycle == 1:
          make_header("Create DEN restraints", out = self.log)
          # Update den manager due to solvent chain changes from start model
          pdb_hierarchy = self.model.get_hierarchy()
          den_manager = den_restraints(
            pdb_hierarchy     = pdb_hierarchy,
            pdb_hierarchy_ref = None,
            params            = self.params.den,
            log               = self.log)
          self.model.restraints_manager.geometry.den_manager = den_manager
          print(
            "DEN weight  : ",
            self.model.restraints_manager.geometry.den_manager.weight,
            file=self.log)
          print(
            "DEN gamma  : ",
            self.model.restraints_manager.geometry.den_manager.gamma,
            file=self.log)
          #
          den_seed = self.params.random_seed
          flex.set_random_seed(value=den_seed)
          random.seed(den_seed)
          self.model.restraints_manager.geometry.den_manager.build_den_proxies(
          pdb_hierarchy=pdb_hierarchy)
          self.model.restraints_manager.geometry.den_manager.build_den_restraints()
          self.model.restraints_manager.geometry.den_manager.current_cycle = 1
          sites_cart = self.model.get_xray_structure().sites_cart()
          if self.params.verbose > 0:
            print(
              self.model.restraints_manager.geometry.den_manager.show_den_summary(
                sites_cart=sites_cart),
              file=self.log)

        else:
          # Reassign random pairs
          if self.macro_cycle % 500 == 0:
            make_header("Create DEN restraints", out = self.log)
            den_seed += 1
            flex.set_random_seed(value=den_seed)
            pdb_hierarchy = self.model.get_hierarchy()
            den_manager = den_restraints(
              pdb_hierarchy     = pdb_hierarchy,
              pdb_hierarchy_ref = None,
              params            = self.params.den,
              log               = self.log)
            self.model.restraints_manager.geometry.den_manager = den_manager
            self.model.restraints_manager.geometry.den_manager.build_den_proxies(
              pdb_hierarchy=pdb_hierarchy)
            self.model.restraints_manager.geometry.den_manager.build_den_restraints()
            self.model.restraints_manager.geometry.den_manager.current_cycle = 1
            sites_cart = self.model.get_xray_structure().sites_cart()
            if self.params.verbose > 0:
                print(
                  self.model.restraints_manager.geometry.den_manager.show_den_summary(
                    sites_cart=sites_cart),
                  file=self.log)

        # Update eq distances per macro cycle
        self.model.restraints_manager.geometry.den_manager.current_cycle = 1
        self.model.restraints_manager.geometry.den_manager.update_eq_distances(
          sites_cart=self.model.get_xray_structure().sites_cart())

      cd_manager = ensemble_cd.cartesian_dynamics(
        structure                   = self.model.get_xray_structure(),
        restraints_manager          = self.model.restraints_manager,
        temperature                 = self.cdp.temperature - self.params.wxray_coupled_tbath_offset,
        protein_thermostat          = self.cdp.protein_thermostat,
        n_steps                     = self.cdp.number_of_steps,
        n_print                     = self.cdp.n_print,
        time_step                   = self.cdp.time_step,
        initial_velocities_zero_fraction = self.cdp.initial_velocities_zero_fraction,
        fmodel                      = self.fmodel_running,
        xray_target_weight          = self.wxray,
        chem_target_weight          = 1.0,
        xray_structure_last_updated = None,
        shift_update                = 0.0,
        xray_gradient               = self.xray_gradient,
        reset_velocities            = self.reset_velocities,
        stop_cm_motion              = self.cmremove,
        update_f_calc               = False,
        er_data                     = self.er_data,
        verbose                     = cdp_verbose,
        log                         = self.log)

      self.reset_velocities = False
      self.cmremove = False

      # Update CDL restraints
      cdl_proxies = cdl_setup.setup_restraints(
        self.model.restraints_manager.geometry,
        verbose=True)
      cdl.update_restraints(self.model.get_hierarchy(),
        geometry=self.model.restraints_manager.geometry,
        cdl_proxies=cdl_proxies,
        verbose=True)

      #Calc rolling average KE energy
      self.kinetic_energy_running_average()
      #Show KE stats
      if self.params.verbose > 0 and self.macro_cycle % 500 == 0:
        self.ensemble_utils.kinetic_energy_stats()

      #Update Fmodel
      self.fmodel_running.update_xray_structure(
        xray_structure      = self.model.get_xray_structure(),
        update_f_calc       = True,
        update_f_mask       = True,
        force_update_f_mask = True)

      #Save current Fmask
      self.er_data.f_mask_current = self.fmodel_running.f_masks()[0].data().deep_copy()

      #Save current Fcalc
      self.er_data.f_calc_data_current = self.fmodel_running.f_calc().data().deep_copy()

      #Total Fmask calculation
      if self.er_data.f_mask_total is None:
        self.er_data.f_mask_total = self.fmodel_running.f_masks()[0].data().deep_copy()
        self.er_data.total_SF_cntr_mask = 1
      else:
        self.er_data.f_mask_total += self.fmodel_running.f_masks()[0].data().deep_copy()
        self.er_data.total_SF_cntr_mask += 1

      #Total Fcalc calculation
      if self.er_data.f_calc_data_total is None:
        self.er_data.f_calc_data_total = self.fmodel_running.f_calc().data().deep_copy()
        self.er_data.total_SF_cntr = 1
      else:
        self.er_data.f_calc_data_total += self.fmodel_running.f_calc().data().deep_copy()
        self.er_data.total_SF_cntr += 1

      #Running average Fcalc calculation
      if self.params.tx > 0:
        self.a_prime = math.exp(-(self.cdp.time_step * self.cdp.number_of_steps)/self.params.tx)
      else:
        self.a_prime = 0

      self.er_data.f_calc_running \
        = (self.a_prime * self.er_data.f_calc_running) + ((1-self.a_prime) * self.fmodel_running.f_calc().data().deep_copy())
      self.fc_running_ave = self.fc_running_ave.array(data = self.er_data.f_calc_running)

      #Update running average Fmask
      f_mask = self.fmodel_running.f_masks()[0]
      self.copy_ma, f_mask = self.copy_ma.common_sets(f_mask)
      if self.macro_cycle == 1:
        self.er_data.f_mask_running = f_mask.data().deep_copy()
      else:
        self.er_data.f_mask_running \
          = (self.a_prime * self.er_data.f_mask_running) + ((1-self.a_prime) * f_mask.data())
      self.running_f_mask_update = self.copy_ma.array(data = self.er_data.f_mask_running).deep_copy()

      #Update runnning average Fcalc and Fmask
      self.fmodel_running.update(f_calc = self.fc_running_ave,
                                 f_mask = self.running_f_mask_update)

      #Update total average Fcalc
      total_f_mask_update \
          = self.copy_ma.array(data = self.er_data.f_mask_total / self.er_data.total_SF_cntr_mask).deep_copy()


      if self.fmodel_total == None:
        self.fmodel_total = self.fmodel_running.deep_copy()
        self.fmodel_total.update(
          f_calc = self.copy_ma.array(data = self.er_data.f_calc_data_total / self.er_data.total_SF_cntr ),
          f_mask = total_f_mask_update)

        if(self.er_data.fix_scale_factor is not None):
          self.fmodel_total.set_scale_switch = self.er_data.fix_scale_factor
      else:
        self.fmodel_total.update(
          f_calc = self.copy_ma.array(data = self.er_data.f_calc_data_total / self.er_data.total_SF_cntr),
          f_mask = total_f_mask_update)

      #Update current time-step Fcalc
      current_f_mask_update = self.copy_ma.array(data = self.er_data.f_mask_current)

      if self.fmodel_current == None:
        self.fmodel_current = self.fmodel_running.deep_copy()
        self.fmodel_current.update(
          f_calc = self.copy_ma.array(data = self.er_data.f_calc_data_current),
          f_mask = current_f_mask_update)
        if(self.er_data.fix_scale_factor is not None):
          self.fmodel_current.set_scale_switch = self.er_data.fix_scale_factor
      else:
        self.fmodel_current.update(
          f_calc = self.copy_ma.array(data = self.er_data.f_calc_data_current),
          f_mask = current_f_mask_update)

      #ML params update
      if self.params.target_name in ['ml', 'mlhl'] :
        if self.macro_cycle < self.equilibrium_macro_cycles:
          if self.fmodel_running.r_free() < (self.best_r_free - self.params.update_sigmaa_rfree):
            self.update_sigmaa()
      # Wxray coupled to temperature bath
      if self.params.wxray_coupled_tbath:
        if self.macro_cycle < 5:
          self.wxray        = 2.5
        elif self.macro_cycle < self.equilibrium_macro_cycles:
          if self.params.tx == 0:
            a_prime_wx = 0
          else:
            wx_tx = min(self.time, self.params.tx)
            a_prime_wx = math.exp(-(self.cdp.time_step * self.cdp.number_of_steps)/wx_tx)
          wxray_t = self.wxray * max(0.01, self.cdp.temperature / self.er_data.non_solvent_temp)
          self.wxray = (a_prime_wx * self.wxray) + ((1-a_prime_wx) * wxray_t)

      #Store current structure, current KE
      if self.macro_cycle % self.pdb_store_cycle == 0 \
           and self.macro_cycle >= self.equilibrium_macro_cycles:
        self.er_data.xray_structures.append(self.model.get_xray_structure().deep_copy_scatterers())
        self.er_data.pdb_hierarchys.append(self.model.get_hierarchy().deep_copy())
        if self.er_data.ke_protein_running is None:
          self.er_data.ke_pdb.append(flex.double(self.model.get_xray_structure().sites_cart().size(), 0.0) )
        else:
          ke_expanded = flex.double(self.model.get_sites_cart().size(), 0.0)
          ke_expanded.set_selected(~self.model.solvent_selection(),
                                   self.er_data.ke_protein_running)
          self.er_data.ke_pdb.append(ke_expanded)

      #Current structural deviation vs starting structure and previous macro-cycle structure
      if xrs_previous.distances(other = self.model.get_xray_structure()).min_max_mean().mean > 1.0:
        print("\n\nWARNING:", file=self.log)
        print("Macro cycle too long, max atomic deviation w.r.t. previous cycle", file=self.log)
        print("greater than 1.0A", file=self.log)
        print("Reduce params.cartesian_dynamics.number_of_steps", file=self.log)
        print("Max deviation : {0:1.3f}"\
          .format(xrs_previous.distances(other = self.model.get_xray_structure()).min_max_mean().mean), file=self.log)

      if self.fmodel_running.r_work() > 0.75:
        raise Sorry("Simulation aborted, running Rfree > 75%")

      #Print run time stats
      if self.macro_cycle == 1 or self.macro_cycle%50 == 0:
        print("\n________________________________________________________________________________", file=self.log)
        print("    MC        Time     |  Current  |  Rolling  |   Total   | Temp |  Grad Wxray ", file=self.log)
        print("          (ps)     (%) |   Rw   Rf |   Rw   Rf |   Rw   Rf |  (K) |   X/G       ", file=self.log)
      print("~{0:5d} {1:7.2f} {2:7.2f} | {3:4.1f} {4:4.1f} | {5:4.1f} {6:4.1f} | {7:4.1f} {8:4.1f} | {9:4.0f} | {10:5.2f} {11:5.2f}"\
          .format(self.macro_cycle,
                  self.time,
                  100 * self.time / self.total_time,
                  100*self.fmodel_current.r_work(),
                  100*self.fmodel_current.r_free(),
                  100*self.fmodel_running.r_work(),
                  100*self.fmodel_running.r_free(),
                  100*self.fmodel_total.r_work(),
                  100*self.fmodel_total.r_free(),
                  self.er_data.non_solvent_temp,
                  self.er_data.xray_grad_rms / self.er_data.geo_grad_rms,
                  self.wxray), file=self.log)

      if self.params.verbose > 0:
        if self.macro_cycle == 1\
            or self.macro_cycle%100 == 0\
            or self.macro_cycle == self.total_macro_cycles:
          self.print_fmodels_scale_and_solvent_stats()

      if self.params.number_of_acquisition_periods > 1:
        if self.macro_cycle in self.block_store_cycle:
          self.save_multiple_fmodel()

      #End of equilibration period, reset total structure factors, atomic cords, kinetic energies
      if self.macro_cycle == self.equilibrium_macro_cycles:
        self.reset_totals()
      #
      assert self.model.get_xray_structure() is cd_manager.structure
      assert self.fmodel_running.xray_structure is cd_manager.structure
      if self.fix_scale == True:
        assert self.fmodel_running.scale_k1() == self.er_data.fix_scale_factor
      self.macro_cycle +=1

############################## END Simulation ##################################

    self.macro_cycle = self.total_macro_cycles
    #Find optimum section of acquisition period
    if self.params.number_of_acquisition_periods > 1:
      self.optimise_multiple_fmodel()
    else:
      self.fmodel_total.set_scale_switch = 0
      self.fmodel_total.update_all_scales(
        log    = self.log,
        remove_outliers=False,
        params = self.bsp)
    #Minimize number of ensemble models
    if self.params.ensemble_reduction:
      self.ensemble_utils.ensemble_reduction(
          rfree_tolerance=self.params.ensemble_reduction_rfree_tolerance)

    #Optimise fmodel_total k, b_aniso, k_sol, b_sol
    self.fmodel_total.set_scale_switch = 0
    self.print_fmodels_scale_and_solvent_stats()
    self.fmodel_total.update_all_scales(
      log    = self.log,
      remove_outliers=False,
      params = self.bsp)
    self.print_fmodels_scale_and_solvent_stats()
    print("FINAL Rwork = %6.4f Rfree = %6.4f Rf/Rw = %6.4f"\
        %(self.fmodel_total.r_work(),
          self.fmodel_total.r_free(),
          self.fmodel_total.r_free() / self.fmodel_total.r_work()
          ), file=self.log)
    print("Final Twork = %6.4f Tfree = %6.4f Tf/Tw = %6.4f"\
        %(self.fmodel_total.target_w(),
          self.fmodel_total.target_t(),
          self.fmodel_total.target_t() / self.fmodel_total.target_w()
          ), file=self.log)
    info = self.fmodel_total.info(free_reflections_per_bin = 100,
                                  max_number_of_bins       = 999
                                  )
    info.show_remark_3(out = self.log)
    info.show_rfactors_targets_in_bins(out = self.log)

    self.write_output_files(run_number=run_number)

############################## END ER ##########################################

  def write_output_files(self, run_number=None):
    #PDB output
    prefix = self.params.output_file_prefix
    if (run_number is not None):
      prefix += "_%g" % run_number
    pdb_out = prefix + ".pdb"
    cif_out = prefix + ".cif"
    if (self.params.gzip_final_model):
      pdb_out += ".gz"
      with gzip.open(pdb_out, 'wb') as out:
        self.write_ensemble_pdb(out=out, binary=True)
      # TODO
      if False :#(self.params.write_cif_file):
        with gzip.open(cif_out, 'wb') as out:
          self.write_ensemble_mmcif(out=out, binary=True)
    else :
      with open(pdb_out, 'w') as out:
        self.write_ensemble_pdb(out = out)
      # TODO
      if False :#(self.params.write_cif_file):
        with open(cif_out, 'w') as out:
          self.write_ensemble_mmcif(out=out)
    self.pdb_file = pdb_out
    # Map output
    assert (self.fmodel_total is not None)
    self.mtz_file = write_mtz_file(
      fmodel_total=self.fmodel_total,
      raw_data=self.raw_data,
      raw_flags=self.raw_flags,
      prefix=prefix,
      params=self.params)

    if self.params.write_centroid_model or self.params.write_mean_model:
      results_manager = self.ensemble_utils.ensemble_rmsf_stats(
          self.er_data.pdb_hierarchys,
          verbose=True,
          out=self.log,
          )
      crystal_symmetry = self.er_data.xray_structures[0].crystal_symmetry()
      if self.params.write_centroid_model:
        print('\nWriting Centroid Model to \n\t%s' % '%s_centroid.pdb' % prefix, file=self.log)
        results_manager.write_centroid_hierarchy('%s_centroid.pdb' % prefix,
                                                 crystal_symmetry,
                                                 )
      if self.params.write_mean_model:
        print('\nWriting Mean Model to \n\t%s' % '%s_mean.pdb' % prefix, file=self.log)
        results_manager.write_mean_hierarchy('%s_mean.pdb' % prefix,
                                                 crystal_symmetry,
                                                 )
      print('', file=self.log)

  def show_overall(self, message = "", fmodel_running = True):
    if fmodel_running:
      message = "Running: " + message
      self.fmodel_running.info().show_rfactors_targets_scales_overall(header = message, out = self.log)
    else:
      message = "Total: " + message
      self.fmodel_total.info().show_rfactors_targets_scales_overall(header = message, out = self.log)

  def add_harmonic_restraints(self):
    make_header("Add specific harmonic restraints", out = self.log)
    # ensures all solvent atoms are at the end prior to applying harmonic restraints
    self.ordered_solvent_update()
    hr_selections = mmtbx.utils.get_atom_selections(
        model = self.model,
        selection_strings = self.params.harmonic_restraints.selections)
    pdb_atoms = self.pdb_hierarchy.atoms()
    print("\nAdd atomic harmonic restraints:", file=self.log)
    restraint_info = []
    for i_seq in hr_selections[0]:
      atom_info = pdb_atoms[i_seq].fetch_labels()
      print('    {0} {1} {2} {3} {4}     '.format(
                                   atom_info.name,
                                   atom_info.i_seq+1,
                                   atom_info.resseq,
                                   atom_info.resname,
                                   atom_info.chain_id,
                                   ), file=self.log)
      restraint_info.append((i_seq, pdb_atoms[i_seq].xyz))
    self.er_data.er_harmonic_restraints_info = restraint_info
    self.er_data.er_harmonic_restraints_weight = self.params.harmonic_restraints.weight
    self.er_data.er_harmonic_restraints_slack  = self.params.harmonic_restraints.slack
    print("\n|"+"-"*77+"|\n", file=self.log)

  def setup_bulk_solvent_and_scale(self):
    make_header("Setup bulk solvent and scale", out = self.log)
    self.show_overall(message = "pre solvent and scale")
    #
    self.fmodel_running.update_all_scales(
      params = self.bsp,
      remove_outliers=False,
      log    = self.log)
    self.fmodel_running.optimize_mask(params = self.bsp)
    #Fixes scale factor for rolling average #ESSENTIAL for LSQ
    if self.fix_scale == True:
      self.er_data.fix_scale_factor = self.fmodel_running.scale_k1()
      self.fmodel_running.set_scale_switch = self.er_data.fix_scale_factor
    self.show_overall(message = "post solvent and scale")

  def scale_helper(self, reference, target):
    return flex.sum(reference * target) / flex.sum(flex.pow2(target))

  def update_normalisation_factors(self):
    if self.params.scale_wrt_n_calc_start:
      # Adaptive scaling
      # Ncalc_start / Ncalc_current
      make_header("Update Ncalc and restrain to Ncalc ref", out = self.log)
      # Get N_calc current, compare with reference
      n_obs, n_calc =\
        self.fmodel_running.n_obs_n_calc(update_nobs_ncalc = False)
      ref_div_current = self.n_calc_reference / n_calc

      n_calc_coeff    = 1.0-math.exp(-self.n_mc_per_ncalc_update/self.n_mc_per_tx)
      n_calc_scaled   = ref_div_current * n_calc_coeff
      n_calc_update   = (self.fmodel_running.n_calc * (1.0-n_calc_coeff) ) + (self.fmodel_running.n_calc * ref_div_current * n_calc_coeff)

      # Update with scaled array
      self.fmodel_running.n_calc = n_calc_update

    else:
      # Normalise to reference Sum(Ncalc)
      make_header("Update and renormalise Ncalc array", out = self.log)
      eobs_norm_factor, ecalc_norm_factor =\
        self.fmodel_running.n_obs_n_calc(update_nobs_ncalc = False)
      self.scale_n1_current = self.scale_helper(target    = ecalc_norm_factor,
                                                reference = eobs_norm_factor
                                                )
      print("Kn current               : {0:5.3f}".format(self.scale_n1_current), file=self.log)
      ecalc_k = sum(self.fmodel_running.n_calc) / sum(ecalc_norm_factor)
      ecalc_k_alt = flex.sum(self.fmodel_running.n_calc * ecalc_norm_factor) / flex.sum(flex.pow2(ecalc_norm_factor) )
      print("Sum current Ncalc        : {0:5.3f}".format(sum(self.fmodel_running.n_calc) ), file=self.log)
      print("Sum updated Ncalc        : {0:5.3f}".format(sum(ecalc_norm_factor) ), file=self.log)
      print("Rescaling factor         : {0:5.3f}".format(ecalc_k), file=self.log)
      print("Rescaling factor alt     : {0:5.3f}".format(ecalc_k_alt), file=self.log)
      ecalc_norm_factor = ecalc_k * ecalc_norm_factor
      self.fmodel_running.n_calc = ecalc_norm_factor
    print("|"+"-"*77+"|\n", file=self.log)

  def update_sigmaa(self):
    make_header("Update sigmaa", out = self.log)
    if self.params.verbose > 0:
      print("Previous best Rfree      : ", self.best_r_free, file=self.log)
      print("Current       Rfree      : ", self.fmodel_running.r_free(), file=self.log)
      self.print_ml_stats()
      print("  Update sigmaa", file=self.log)
    self.sigmaa_array = self.fmodel_running.sigmaa().sigmaa().data()
    self.fmodel_running.set_sigmaa = self.sigmaa_array
    if self.params.verbose > 0:
      self.print_ml_stats()
    self.best_r_free = self.fmodel_running.r_free()
    print("|"+"-"*77+"|\n", file=self.log)

  def import_tls_selections(self):
    make_header("Import External TLS", out = self.log)
    print('External TLS model: ' + self.params.import_tls_pdb, file=self.log)
    pdb_import_tls = self.params.import_tls_pdb
    pdb_tls_inp = iotbx.pdb.input(file_name=pdb_import_tls)
    tls_params = pdb_tls_inp.extract_tls_params(pdb_tls_inp.construct_hierarchy())
    fit_tlsos = [tls_tools.tlso(t=o.t, l=o.l, s=o.s, origin=o.origin) for o in tls_params.tls_params]
    tls_strings = [o.selection_string for o in tls_params.tls_params]
    return fit_tlsos, tls_strings

  def setup_tls_selections(self, tls_group_selection_strings):
    make_header("Generating TLS selections from input parameters (not including solvent)", out = self.log)
    model_no_solvent = self.model.deep_copy()
    model_no_solvent = model_no_solvent.remove_solvent()

    if len(tls_group_selection_strings) < 1:
      print('\nNo TLS groups supplied - automatic setup', file=self.log)
      # Get chain information
      chains_info = []
      for chain in model_no_solvent.get_hierarchy().chains():
        count_h = 0
        for atom in chain.atoms():
          if atom.element_is_hydrogen(): count_h+=1
        chain_id_non_h = ("'" + chain.id + "'", chain.atoms_size() - count_h)
        # check if this chain is already there, e.g. ligand in the same chain
        # at the end of file
        cur_ch_id_list = [x[0] for x in chains_info]
        if "'" + chain.id + "'" in cur_ch_id_list:
          ind = cur_ch_id_list.index("'" + chain.id + "'")
          old_n_atoms = chains_info[ind][1]
          chains_info[ind] = ("'" + chain.id + "'", old_n_atoms+chain_id_non_h[1])
        else:
          chains_info.append(chain_id_non_h)
      # Check all chains > 63 heavy atoms for TLS fitting
      chains_size = flex.int(list(zip(*chains_info))[1])
      chains_size_ok = flex.bool(chains_size > 63)
      if sum(chains_size) < 63:
        print('\nStructure contains less than 63 atoms (non H/D, non solvent)', file=self.log)
        print('\nUnable to perform TLS fitting, will use isotropic B-factor model', file=self.log)
      elif chains_size_ok.count(False) == 0:
        print('\nTLS selections:', file=self.log)
        print('Chain, number atoms (non H/D)', file=self.log)
        for chain in chains_info:
          tls_group_selection_strings.append('chain ' + chain[0])
          print(chain[0], chain[1], file=self.log)
      else:
        print('\nFollowing chains contain less than 63 atoms (non H/D):', file=self.log)
        tls_group_selection_strings.append('chain ')
        for chain in chains_info:
          tls_group_selection_strings[0] += (chain[0] + ' or chain ')
          if chain[1] < 63:
            print(chain[0], chain[1], file=self.log)
        print('Combining all chains to single TLS group', file=self.log)
        print('WARNING: this may not be the optimum tls groupings to use', file=self.log)
        print('TLS selections:', file=self.log)
        tls_group_selection_strings[0] = tls_group_selection_strings[0][0:-10]
        print(tls_group_selection_strings[0], file=self.log)
    #
    tls_no_sol_selections =  mmtbx.utils.get_atom_selections(
        model = model_no_solvent,
        selection_strings = tls_group_selection_strings)
    #
    tls_no_hd_selection_strings = []
    for selection_string in tls_group_selection_strings:
      no_hd_string = '(' + selection_string + ') and not (element H or element D)'
      tls_no_hd_selection_strings.append(no_hd_string)

    tls_no_sol_no_hd_selections = mmtbx.utils.get_atom_selections(
        model = model_no_solvent,
        selection_strings = tls_no_hd_selection_strings)

    #
    assert self.tls_manager is not None
    self.tls_manager.tls_selection_strings_no_sol       = tls_group_selection_strings
    self.tls_manager.tls_selection_strings_no_sol_no_hd = tls_no_hd_selection_strings
    self.tls_manager.tls_selections_no_sol              = tls_no_sol_selections
    self.tls_manager.tls_selections_no_sol_no_hd        = tls_no_sol_no_hd_selections
    self.tls_manager.tls_operators = mmtbx.tls.tools.generate_tlsos(
        selections     = self.tls_manager.tls_selections_no_sol,
        xray_structure = model_no_solvent.get_xray_structure(),
        value          = 0.0)

    self.model.tls_groups = mmtbx.tls.tools.tls_groups(
        selection_strings = self.tls_manager.tls_selection_strings_no_sol,
        tlsos             = self.tls_manager.tls_operators)

  def fit_tls(self, input_model, verbose=False):
    make_header("Fit TLS from reference model", out = self.log)
    model_copy = input_model.deep_copy()
    model_copy = model_copy.remove_solvent()
    print('Reference model :', file=self.log)
    model_copy.show_adp_statistics(padded = True, out = self.log)
    start_xrs = model_copy.get_xray_structure().deep_copy_scatterers()
    start_xrs.convert_to_isotropic()
    start_biso = start_xrs.scatterers().extract_u_iso()/adptbx.b_as_u(1)
    model_copy.get_xray_structure().convert_to_anisotropic()
    tls_selection_no_sol_hd            = self.tls_manager.tls_selections_no_sol_no_hd
    tls_selection_no_sol_hd_exclusions = self.tls_manager.tls_selections_no_sol_no_hd
    pre_fitted_mean = 999999.99
    #
    use_isotropic = False
    for group in self.tls_manager.tls_selections_no_sol_no_hd:
      if group.size() < 63:
        self.params.isotropic_b_factor_model = True
      elif self.ptls * group.size() < 63:
        self.ptls = 64.0 / group.size()
        print('\nAutomatically increasing pTLS to : {0:5.3f}'.format(self.ptls), file=self.log)
    if self.params.isotropic_b_factor_model:
      print('\nModel contains less than 63 non-solvent, non-H/D atoms', file=self.log)
      print('Insufficient to fit TLS model, using isotropic model', file=self.log)
      iso_b  = self.fmodel_running.wilson_b() * self.params.pwilson
      episq = 8.0*(math.pi**2)
      print('Isotropic translation (B) : {0:5.3f}'.format(iso_b), file=self.log)
      print('  = Wilson b-factor * pwilson', file=self.log)
      iso_u = iso_b / episq
      print('Isotropic translation (U) : {0:5.3f}'.format(iso_u), file=self.log)
      fit_tlsos = []
      for tls_group in self.tls_manager.tls_operators:
        tls_t_new = (iso_u,
                     iso_u,
                     iso_u,
                     0.0,
                     0.0,
                     0.0)
        tls_l_new = (0.0,0.0,0.0,0.0,0.0,0.0)
        tls_s_new = (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0)
        fit_tlsos.append(tls_tools.tlso(t      = tls_t_new,
                                      l      = tls_l_new,
                                      s      = tls_s_new,
                                      origin = tls_group.origin))
      self.tls_manager.tls_operators = fit_tlsos
      for tls_group in self.tls_manager.tls_operators:
        mmtbx.tls.tools.show_tls_one_group(tlso = tls_group,
                                           out  = self.log)

    else:
      for fit_cycle in range(self.params.max_ptls_cycles):
        fit_tlsos = mmtbx.tls.tools.generate_tlsos(
          selections     = tls_selection_no_sol_hd_exclusions,
          xray_structure = model_copy.get_xray_structure(),
          value          = 0.0)
        print('\nFitting cycle : ', fit_cycle+1, file=self.log)
        for rt,rl,rs in [[1,0,1],[1,1,1],[0,1,1],
                         [1,0,0],[0,1,0],[0,0,1],[1,1,1],
                         [0,0,1]]*10:
          fit_tlsos = mmtbx.tls.tools.tls_from_uanisos(
            xray_structure               = model_copy.get_xray_structure(),
            selections                   = tls_selection_no_sol_hd_exclusions,
            tlsos_initial                = fit_tlsos,
            number_of_macro_cycles       = 10,
            max_iterations               = 100,
            refine_T                     = rt,
            refine_L                     = rl,
            refine_S                     = rs,
            enforce_positive_definite_TL = True,
            verbose                      = -1,
            out                          = self.log)
        fitted_tls_xrs = model_copy.get_xray_structure().deep_copy_scatterers()
        us_tls = mmtbx.tls.tools.u_cart_from_tls(
               sites_cart = fitted_tls_xrs.sites_cart(),
               selections = self.tls_manager.tls_selections_no_sol,
               tlsos      = fit_tlsos)
        fitted_tls_xrs.set_u_cart(us_tls)
        fitted_tls_xrs.convert_to_isotropic()
        fitted_biso = fitted_tls_xrs.scatterers().extract_u_iso()/adptbx.b_as_u(1)
        mmtbx.tls.tools.show_tls(tlsos = fit_tlsos, out = self.log)
        #For testing
        if verbose:
          pdb_hierarchy = model_copy.pdb_hierarchy
          pdb_atoms = pdb_hierarchy().atoms()
          not_h_selection = pdb_hierarchy().atom_selection_cache().selection('not element H')
          ca_selection = pdb_hierarchy().atom_selection_cache().selection('name ca')
          print('\nCA atoms (Name/res number/res name/chain/atom number/ref biso/fit biso:', file=self.log)
          for i_seq, ca in enumerate(ca_selection):
            if ca:
              atom_info = pdb_atoms[i_seq].fetch_labels()
              print(atom_info.name, atom_info.resseq, atom_info.resname, atom_info.chain_id, " | ", i_seq, start_biso[i_seq], fitted_biso[i_seq], file=self.log)

        delta_ref_fit = start_biso - fitted_biso
        hd_selection = model_copy.get_hd_selection()
        delta_ref_fit_no_h = delta_ref_fit.select(~hd_selection)
        delta_ref_fit_no_h_basic_stats = scitbx.math.basic_statistics(delta_ref_fit_no_h )
        start_biso_no_hd = start_biso.select(~hd_selection)
        fitted_biso_no_hd = fitted_biso.select(~hd_selection)

        if verbose:
          print('pTLS                                    : ', self.ptls, file=self.log)

        sorted_delta_ref_fit_no_h = sorted(delta_ref_fit_no_h)
        percentile_cutoff = sorted_delta_ref_fit_no_h[int(len(sorted_delta_ref_fit_no_h) * self.ptls)-1]
        if verbose:
          print('Cutoff (<)                              : ', percentile_cutoff, file=self.log)

        print('Number of atoms (non HD)                : ', delta_ref_fit_no_h.size(), file=self.log)
        delta_ref_fit_no_h_include = flex.bool(delta_ref_fit_no_h < percentile_cutoff)
        print('Number of atoms (non HD) used in fit    : ', delta_ref_fit_no_h_include.count(True), file=self.log)
        print('Percentage (non HD) used in fit         : {0:5.3f}'.format(delta_ref_fit_no_h_include.count(True) / delta_ref_fit_no_h.size()), file=self.log)

        # Convergence test
        if fitted_biso_no_hd.min_max_mean().mean == pre_fitted_mean:
          break
        else:
          pre_fitted_mean = fitted_biso_no_hd.min_max_mean().mean

        # N.B. map on to full array including hydrogens for i_seqs
        include_array = flex.bool(delta_ref_fit < percentile_cutoff)
        #
        include_i_seq = []
        assert delta_ref_fit.size() == model_copy.get_number_of_atoms()
        assert include_array.size() == model_copy.get_number_of_atoms()
        for i_seq, include_flag in enumerate(include_array):
          if include_flag and not hd_selection[i_seq]:
            include_i_seq.append(i_seq)
        tls_selection_no_sol_hd_exclusions = []
        for group in range(len(tls_selection_no_sol_hd)):
          new_group = flex.size_t()
          for x in tls_selection_no_sol_hd[group]:
            if x in include_i_seq:
              new_group.append(x)
          if len(new_group) < 63:
            raise Sorry("Number atoms in TLS too small; increase size of group or reduce cut-off")
          if verbose:
            print('TLS group ', group+1, ' number atoms ', len(new_group), file=self.log)
          tls_selection_no_sol_hd_exclusions.append(new_group)

    print('\nFinal non-solvent b-factor model', file=self.log)
    model_copy.get_xray_structure().convert_to_anisotropic()

    us_tls = mmtbx.tls.tools.u_cart_from_tls(
             sites_cart = model_copy.get_sites_cart(),
             selections = self.tls_manager.tls_selections_no_sol_no_hd,
             tlsos      = fit_tlsos)
    model_copy.get_xray_structure().set_u_cart(us_tls)
    model_copy.show_adp_statistics(padded = True, out = self.log)
    del model_copy

    #Update TLS params
    self.model.tls_groups.tlsos = fit_tlsos
    self.tls_manager.tls_operators = fit_tlsos
    self.assign_solvent_tls_groups()

  def tls_parameters_update(self):
    self.model.get_xray_structure().convert_to_anisotropic()
    #Apply TLS w.r.t. to atomic position
    selections = self.tls_manager.tls_selections_with_sol
    us_tls = mmtbx.tls.tools.u_cart_from_tls(
               sites_cart = self.model.get_sites_cart(),
               selections = selections,
               tlsos      = self.tls_manager.tls_operators)
    for selection in selections:
      self.model.get_xray_structure().set_u_cart(us_tls, selection = selection)
    self.fmodel_running.update_xray_structure(
      xray_structure = self.model.get_xray_structure(),
      update_f_calc  = False,
      update_f_mask  = False)

  def assign_solvent_tls_groups(self):
    self.model.get_xray_structure().convert_to_anisotropic(
      selection=self.model.solvent_selection())
    self.fmodel_running.update_xray_structure(
      xray_structure  = self.model.get_xray_structure(),
      update_f_calc   = False,
      update_f_mask   = False)
    #
    self.tls_manager.tls_selections_with_sol = []
    for grp in self.tls_manager.tls_selections_no_sol:
      self.tls_manager.tls_selections_with_sol.append(grp.deep_copy())
    #
    if len(self.tls_manager.tls_selections_with_sol) == 1:
      pdb_atoms     = self.pdb_hierarchy.atoms()
      hoh_selection = self.model.solvent_selection()
      for n, atom in enumerate(pdb_atoms):
        if hoh_selection[n]:
          self.tls_manager.tls_selections_with_sol[0].append(n)
    else:
      model             = self.model.deep_copy()
      solvent_selection = model.solvent_selection()
      solvent_xrs       = model.get_xray_structure().select(solvent_selection)
      model             = model.remove_solvent()
      closest_distances = model.get_xray_structure().closest_distances(
                              sites_frac      = solvent_xrs.sites_frac(),
                              use_selection   = ~model.get_xray_structure().hd_selection(),
                              distance_cutoff = 10.0)
      assert len(solvent_xrs.sites_cart()) == len(closest_distances.i_seqs)
      number_non_solvent_atoms = model.get_number_of_atoms()
      for n, i_seq in enumerate(closest_distances.i_seqs):
        for grp in self.tls_manager.tls_selections_with_sol:
          if i_seq in grp:
            grp.append(n+number_non_solvent_atoms)
            break
    #
    self.tls_parameters_update()

  def kinetic_energy_running_average(self):
    #Kinetic energy
    atomic_weights = self.model.get_xray_structure().atomic_weights()
    ke = 0.5 * atomic_weights * self.er_data.velocities.dot()
    #Select non-solvent atoms
    ke = ke.select(~self.model.solvent_selection() )
    if self.er_data.ke_protein_running == None:
      self.er_data.ke_protein_running = ke
    else:
      self.er_data.ke_protein_running\
        = (self.a_prime * self.er_data.ke_protein_running) + ( (1-self.a_prime) * ke)

  def ordered_solvent_update(self):
    if is_amber_refinement(self.params):
      print('Ensemble refinement with Amber does not support solvent!!!', file=self.log)
      return
    ensemble_ordered_solvent_manager = ensemble_ordered_solvent.manager(
        model             = self.model,
        fmodel            = self.fmodel_running,
        verbose           = self.params.verbose,
        params            = self.params.ensemble_ordered_solvent,
        velocities        = self.er_data.velocities,
        log               = self.log)
    self.model = ensemble_ordered_solvent_manager.model
    self.er_data.velocities = ensemble_ordered_solvent_manager.velocities
    self.fmodel_running.update_xray_structure(
      xray_structure = self.model.get_xray_structure(),
      update_f_calc  = False,
      update_f_mask  = False)
    assert self.fmodel_running.xray_structure is self.model.get_xray_structure()
    self.xray_gradient = None
    #Update atom selections
    self.pdb_hierarchy = self.model.get_hierarchy()
    self.atom_selections()
    #Reset solvent tls groups
    if self.tls_manager is not None:
      self.assign_solvent_tls_groups()

  def reset_totals(self):
    make_header("Reseting structure ensemble and total Fmodel", out = self.log)
    self.er_data.xray_structures = []
    self.er_data.xray_structures_diff_map = []
    self.er_data.pdb_hierarchys = []
    self.er_data.ke_pdb = []
    self.er_data.f_calc_data_total = None
    self.er_data.total_SF_cntr = 0
    self.er_data.f_mask_total = None
    self.er_data.total_SF_cntr_mask = 0

  #Generates list of atom selections (needed to pass to CD)
  def atom_selections(self):
    self.er_data.all_sel     = flex.bool(self.model.get_number_of_atoms(), True)
    self.er_data.solvent_sel = self.model.solvent_selection()

  def save_multiple_fmodel(self):
    make_header("Saving fmodel block", out = self.log)
    #Stores fcalc, fmask, xray structure, pdb hierarchys
    print('{0:<23}: {1:>8} {2:>8} {3:>8} {4:>8}'.format('','MC','Block','Rwork','Rfree'), file=self.log)
    print("{0:<23}: {1:8d} {2:8d} {3:8.3f} {4:8.3f}".format(
        'Fmodel block info',
        self.macro_cycle,
        self.block_store_cycle_cntr+1,
        100 * self.fmodel_total.r_work(),
        100 * self.fmodel_total.r_free() ), file=self.log)
    fcalc_block  = self.er_data.f_calc_data_total / self.er_data.total_SF_cntr
    fmask_block  = self.er_data.f_mask_total / self.er_data.total_SF_cntr_mask
    xrs_block    = self.er_data.xray_structures
    xrs_dm_block = self.er_data.xray_structures_diff_map
    pdb_h_block  = self.er_data.pdb_hierarchys
    ke_pdb_block = self.er_data.ke_pdb

    block_info = (fcalc_block,
                  fmask_block,
                  xrs_block,
                  xrs_dm_block,
                  pdb_h_block,
                  ke_pdb_block)

    self.block_store_cycle_cntr+1
    if self.block_store_cycle_cntr+1 == 1:
      self.block_temp_file_list = []
    prefix = self.params.output_file_prefix
    if (self.run_number is not None):
      prefix += "_%g" % self.run_number
    filename = str(self.block_store_cycle_cntr+1)+'_block_'+prefix+'_TEMP.pZ'
    self.block_temp_file_list.append(filename)
    er_pickle(pickle_object = block_info, pickle_filename = filename)
    self.block_store_cycle_cntr += 1
    if self.macro_cycle != self.total_macro_cycles:
      self.reset_totals()

  def optimise_multiple_fmodel(self):
    make_header("Block selection by Rwork", out = self.log)
    best_r_work = None

    # Load all temp files
    self.fmodel_total_block_list = []
    for filename in self.block_temp_file_list:
      block_info = pickle.load(gzip.open(filename,'rb'))
      self.fmodel_total_block_list.append(block_info)
      os.remove(filename)

    self.fmodel_total.set_scale_switch = 0
    print('  {0:>17} {1:>8} {2:>8}'\
      .format('Block range','Rwork','Rfree','k1'), file=self.log)
    for x in range(len(self.fmodel_total_block_list)):
      x2 = x+1
      y = len(self.fmodel_total_block_list)
      while y > x:
        fcalc_tot = self.fmodel_total_block_list[x][0].deep_copy()
        fmask_tot = self.fmodel_total_block_list[x][1].deep_copy()
        cntr      = 1
        while x2 < y:
          fcalc_tot += self.fmodel_total_block_list[x2][0].deep_copy()
          fmask_tot += self.fmodel_total_block_list[x2][1].deep_copy()
          x2     += 1
          cntr   += 1
        self.fmodel_total.update(
          f_calc = self.copy_ma.array(data = (fcalc_tot / cntr)),
          f_mask = self.copy_ma.array(data = (fmask_tot / cntr)) )
        self.fmodel_total.update_all_scales(
          params = self.bsp,
          remove_outliers=False,
          log = self.log)
        print("  {0:8d} {1:8d} {2:8.3f} {3:8.3f}"\
          .format(x+1,
                  y,
                  self.fmodel_total.r_work(),
                  self.fmodel_total.r_free(),
                  self.fmodel_total.scale_k1()
                  ), file=self.log)
        if best_r_work == None:
          best_r_work = self.fmodel_total.r_work()
          best_r_work_block = [x,y]
          best_r_work_fcalc = (fcalc_tot / cntr)
          best_r_work_fmask = (fmask_tot / cntr)
        elif self.fmodel_total.r_work() < (best_r_work - 0.01):
          best_r_work = self.fmodel_total.r_work()
          best_r_work_block = [x,y]
          best_r_work_fcalc = (fcalc_tot / cntr)
          best_r_work_fmask = (fmask_tot / cntr)
        x2 = x+1
        y -= 1
    self.fmodel_total.update(
      f_calc = self.copy_ma.array(data = best_r_work_fcalc),
      f_mask = self.copy_ma.array(data = best_r_work_fmask) )
    self.fmodel_total.update_all_scales(
          params = self.bsp,
          remove_outliers=False,
          log    = self.log)

    print("\nOptimium block :", file=self.log)
    print("  {0:8d} {1:8d} {2:8.3f} {3:8.3f} {4:8.3f} {5:8.3f}"\
      .format(best_r_work_block[0]+1,
              best_r_work_block[1],
              self.fmodel_total.r_work(),
              self.fmodel_total.r_free(),
              self.fmodel_total.scale_k1(),
              self.fmodel_total.fmodel_kbu().k_sols()[0],
              self.fmodel_total.fmodel_kbu().b_sols()[0]), file=self.log)
    #Update self.er_data.xray_structures and self.er_data.pdb_hierarchys to correspond to optimum fmodel_total
    self.er_data.xray_structures = []
    self.er_data.xray_structures_diff_map =[]
    self.er_data.pdb_hierarchys  = []
    self.er_data.ke_pdb          = []
    for x in range(len(self.fmodel_total_block_list)):
      if x >= best_r_work_block[0] and x < best_r_work_block[1]:
        print("Block | Number of models in block : ", x+1, " | ", len(self.fmodel_total_block_list[x][2]), file=self.log)
        self.er_data.xray_structures.extend(self.fmodel_total_block_list[x][2])
        self.er_data.xray_structures_diff_map.extend(self.fmodel_total_block_list[x][3])
        self.er_data.pdb_hierarchys.extend(self.fmodel_total_block_list[x][4])
        self.er_data.ke_pdb.extend(self.fmodel_total_block_list[x][5])
    assert len(self.er_data.xray_structures) == len(self.er_data.pdb_hierarchys)
    assert len(self.er_data.xray_structures) == len(self.er_data.ke_pdb)
    print("Number of models for PBD          : ", len(self.er_data.xray_structures), file=self.log)
    print("|"+"-"*77+"|\n", file=self.log)

  def print_fmodels_scale_and_solvent_stats(self):
    make_header("Fmodel statistics | macrocycle: "+str(self.macro_cycle),
      out = self.log)
    print('{0:<23}: {1:>8} {2:>8} {3:>8} {4:>8}'.format('','MC',
      'k1','Bsol','ksol'), file=self.log)
    if self.fmodel_current is not None:
      print("{0:<23}: {1:8d} {2:8.3f} {3:8.3f} {4:8.3f}"\
        .format('Fmodel current',
                self.macro_cycle,
                self.fmodel_current.scale_k1(),
                self.fmodel_current.fmodel_kbu().b_sols()[0],
                self.fmodel_current.fmodel_kbu().k_sols()[0],
                ), file=self.log)
    if self.fmodel_running is not None:
      print("{0:<23}: {1:8d} {2:8.3f} {3:8.3f} {4:8.3f}"\
        .format('Fmodel running',
                self.macro_cycle,
                self.fmodel_running.scale_k1(),
                self.fmodel_running.fmodel_kbu().b_sols()[0],
                self.fmodel_running.fmodel_kbu().k_sols()[0] ), file=self.log)
    if self.fmodel_total is not None:
      print("{0:<23}: {1:8d} {2:8.3f} {3:8.3f} {4:8.3f}"\
        .format('Fmodel_Total',
                self.macro_cycle,
                self.fmodel_total.scale_k1(),
                self.fmodel_total.fmodel_kbu().b_sols()[0],
                self.fmodel_total.fmodel_kbu().k_sols()[0] ), file=self.log)
    if self.fmodel_current is not None:
      print("Fmodel current bcart   : {0:14.2f} {1:5.2f} {2:5.2f} {3:5.2f} {4:5.2f} {5:5.2f}".format(*self.fmodel_current.fmodel_kbu().b_cart()), file=self.log)
    if self.fmodel_running is not None:
      print("Fmodel running bcart   : {0:14.2f} {1:5.2f} {2:5.2f} {3:5.2f} {4:5.2f} {5:5.2f}".format(*self.fmodel_running.fmodel_kbu().b_cart()), file=self.log)
    if self.fmodel_total  is not None:
      print("Fmodel total bcart     : {0:14.2f} {1:5.2f} {2:5.2f} {3:5.2f} {4:5.2f} {5:5.2f}".format(*self.fmodel_total.fmodel_kbu().b_cart()), file=self.log)
    print("|"+"-"*77+"|\n", file=self.log)

  def write_diff_map_ensemble(self, out):
    crystal_symmetry = self.er_data.xray_structures_diff_map[0].crystal_symmetry()
    print(pdb.format_cryst1_record(crystal_symmetry = crystal_symmetry), file=out)
    print(pdb.format_scale_records(unit_cell = crystal_symmetry.unit_cell()), file=out)
    for n,xrs in enumerate(self.er_data.xray_structures_diff_map):
      print("MODEL %8d"%(n+1), file=out)
      print(xrs.as_pdb_file(), file=out)
      print("ENDMDL", file=out)
    print("END", file=out)

  def update_single_hierarchy(self, i_model):
    xrs = self.er_data.xray_structures[i_model]
    scatterers = xrs.scatterers()
    sites_cart = xrs.sites_cart()
    u_isos = xrs.extract_u_iso_or_u_equiv()
    occupancies = scatterers.extract_occupancies()
    u_carts = scatterers.extract_u_cart_plus_u_iso(xrs.unit_cell())
    scat_types = scatterers.extract_scattering_types()
    i_model_pdb_hierarchy = self.er_data.pdb_hierarchys[i_model]
    pdb_atoms = i_model_pdb_hierarchy.atoms()
    i_model_ke = self.er_data.ke_pdb[i_model]
    for j_seq, atom in enumerate(pdb_atoms):
      if j_seq < len(sites_cart):
        atom.xyz = sites_cart[j_seq]
        if self.params.output_running_kinetic_energy_in_occupancy_column:
          #XXX * 0.1 to fit in occ col
          atom.occ = 0.1 * i_model_ke[j_seq]
        else:
          atom.occ = 1.0 / len(self.er_data.xray_structures)
        atom.b = adptbx.u_as_b(u_isos[j_seq])
        e = scat_types[j_seq]
        if (len(e) > 1 and "+-0123456789".find(e[1]) >= 0):
          atom.element = "%2s" % e[:1]
          atom.charge = "%-2s" % e[1:]
        elif (len(e) > 2):
          atom.element = "%2s" % e[:2]
          atom.charge = "%-2s" % e[2:]
        else:
          atom.element = "%2s" % e
          atom.charge = "  "
        if (scatterers[j_seq].flags.use_u_aniso()):
          atom.uij = u_carts[j_seq]
        elif(False):
          atom.uij = self.u_cart
        else:
          atom.uij = (-1,-1,-1,-1,-1,-1)
    return i_model_pdb_hierarchy

  def write_ensemble_pdb(self, out, binary=False):
    tmp_out = StringIO()
    crystal_symmetry = self.er_data.xray_structures[0].crystal_symmetry()
    pr = "REMARK   3"
    print(pr, file=tmp_out)
    print("REMARK   3 TIME-AVERAGED ENSEMBLE REFINEMENT.", file=tmp_out)
    ver, tag = phenix_info.version_and_release_tag(f = tmp_out)
    if(ver is None):
      prog = "   PROGRAM     : PHENIX (phenix.ensemble_refinement)"
    else:
      if(tag is not None):
        ver = ver+"_"+tag
      prog = "   PROGRAM     : PHENIX (phenix.ensemble_refinement: %s)"%ver
    print(pr+prog, file=tmp_out)
    authors = phenix_info.phenix_developers_last
    l = pr+"   AUTHORS     :"
    j = 0
    i = j
    n = len(l) + 1
    while (j != len(authors)):
      a = len(authors[j]) + 1
      if (n+a > 79):
        print(l, ",".join(authors[i:j]) + ",", file=tmp_out)
        l = pr+"               :"
        i = j
        n = len(l) + 1
      n += a
      j += 1
    if (i != j):
      print(l, ",".join(authors[i:j]), file=tmp_out)
    fmodel_info = self.fmodel_total.info()
    fmodel_info.show_remark_3(out = tmp_out)
#    model_stats = mmtbx.model_statistics.model(model     = self.model,
#                                               ignore_hd = False)
#    # set mode_stats.geometry to None as refers to final structure NOT ensemble
#    model_stats.geometry = None
#    model_stats.show(out = out, pdb_deposition =True)
    # get mean geometry stats for ensemble
    self.final_geometry_pdb_string = self.ensemble_utils.ensemble_mean_geometry_stats(
        restraints_manager       = self.model.restraints_manager,
        xray_structure           = self.model.get_xray_structure(),
        ensemble_xray_structures = self.er_data.xray_structures,
        ignore_hd                = True,
        verbose                  = False,
        out                      = self.log,
        return_pdb_string        = True)
    print(self.final_geometry_pdb_string, file=tmp_out)
    print(pdb.format_cryst1_record(crystal_symmetry = crystal_symmetry), file=tmp_out)
    print(pdb.format_scale_records(unit_cell = crystal_symmetry.unit_cell()), file=tmp_out)
    atoms_reset_serial = True
    #
    cntr = 0
    assert len(self.er_data.ke_pdb) == len(self.er_data.xray_structures)
    assert len(self.er_data.pdb_hierarchys) == len(self.er_data.xray_structures)
    for i_model, xrs in enumerate(self.er_data.xray_structures):
      cntr += 1
      print("MODEL %8d"%cntr, file=tmp_out)
      i_model_pdb_hierarchy = self.update_single_hierarchy(i_model)
      if (atoms_reset_serial):
        atoms_reset_serial_first_value = 1
      else:
        atoms_reset_serial_first_value = None
      tmp_out.write(i_model_pdb_hierarchy.as_pdb_string(
        append_end=False,
        atoms_reset_serial_first_value=atoms_reset_serial_first_value))
      #
      print("ENDMDL", file=tmp_out)
    print("END", file=tmp_out)
    text = tmp_out.getvalue()
    if binary:
      text = text.encode('utf8')
    out.write(text)

  def print_ml_stats(self):
    if self.fmodel_running.set_sigmaa is not None:
      self.run_time_stats_dict.update({'Sigma_a':self.fmodel_running.set_sigmaa})
    if self.params.target_name in ['ml', 'mlhl'] :
      self.run_time_stats_dict.update({'Alpha':self.fmodel_running.alpha_beta()[0].data()})
      self.run_time_stats_dict.update({'Beta':self.fmodel_running.alpha_beta()[1].data()})
    if self.fmodel_running.n_obs is not None:
      self.run_time_stats_dict.update({'Eobs(fixed)':self.fmodel_running.n_obs})
    if self.fmodel_running.n_calc is not None:
      self.run_time_stats_dict.update({'Ecalc(fixed)':self.fmodel_running.n_calc})

    make_header("ML statistics", out = self.log)
    print('  {0:<23}: {1:>12} {2:>12} {3:>12}'.format('','min','max','mean'), file=self.log)
    for key in sorted(self.run_time_stats_dict.keys()):
      info = self.run_time_stats_dict[key].min_max_mean()
      print('  {0:<23}: {1:12.3f} {2:12.3f} {3:12.3f}'.format(
        key,
        info.min,
        info.max,
        info.mean), file=self.log)
    print("|"+"-"*77+"|\n", file=self.log)

################################################################################

def show_data(fmodel, n_outl, test_flag_value, f_obs_labels, log):
  info = fmodel.info()
  flags_pc = \
   fmodel.r_free_flags().data().count(True)*1./fmodel.r_free_flags().data().size()
  print("Data statistics", file=log)
  try: f_obs_labels = f_obs_labels[:f_obs_labels.index(",")]
  except ValueError: pass
  result = " \n    ".join([
    "data_label                          : %s" % f_obs_labels,
    "high_resolution                     : "+format_value("%-5.2f",info.d_min),
    "low_resolution                      : "+format_value("%-6.2f",info.d_max),
    "completeness_in_range               : " + \
      format_value("%-6.2f",info.completeness_in_range),
    "completeness(d_min-inf)             : " + \
      format_value("%-6.2f",info.completeness_d_min_inf),
    "completeness(6A-inf)                : " + \
      format_value("%-6.2f",info.completeness_6_inf),
    "wilson_b                            : " + \
      format_value("%-6.1f",fmodel.wilson_b()),
    "number_of_reflections               : " + \
      format_value("%-8d",  info.number_of_reflections),
    "test_set_size                       : " + \
      format_value("%-8.4f",flags_pc),
    "test_flag_value                     : " + \
      format_value("%-d",   test_flag_value),
    "number_of_Fobs_outliers             : " + format_value("%-8d",  n_outl),
    "anomalous_flag                      : " + \
      format_value("%-6s",  fmodel.f_obs().anomalous_flag())])
  print("   ", result, file=log)

def show_model_vs_data(fmodel, log):
  d_max, d_min = fmodel.f_obs().d_max_min()
  flags_pc = fmodel.r_free_flags().data().count(True)*100./\
    fmodel.r_free_flags().data().size()
  if(flags_pc == 0): r_free = None
  else: r_free = fmodel.r_free()
  k_sol = format_value("%-5.2f",fmodel.fmodel_kbu().k_sols()[0])
  b_sol = format_value("%-7.2f",fmodel.fmodel_kbu().b_sols()[0])
  b_cart = " ".join([("%8.2f"%v).strip() for v in fmodel.fmodel_kbu().b_cart()])
  print("Model vs data statistics", file=log)
  result = " \n    ".join([
    "r_work(re-computed)                 : " + \
      format_value("%-6.4f",fmodel.r_work()),
    "r_free(re-computed)                 : " + format_value("%-6.4f",r_free),
    "scale_k1                            : " + \
      format_value("%-6.4f",fmodel.scale_k1()),
    "bulk_solvent_(k_sol,b_sol)          : %s%s" % (k_sol, b_sol),
    "overall_anisotropic_scale_(b_cart)  : " + format_value("%-s",b_cart)])
  print("   ", result, file=log)

def write_mtz_file(fmodel_total, raw_data, raw_flags, prefix, params):
  assert (fmodel_total is not None)
  class labels_decorator:
    def __init__(self, amplitudes_label, phases_label):
      self._amplitudes = amplitudes_label
      self._phases = phases_label
    def amplitudes(self):
      return self._amplitudes
    def phases(self, root_label, anomalous_sign=None):
      assert anomalous_sign is None or not anomalous_sign
      return self._phases
  xray_suffix = "_xray"
  f_obs_label = "F-obs"
  i_obs_label = "I-obs"
  flag_label = "R-free-flags"
  if (raw_data.is_xray_intensity_array()):
    column_root_label = i_obs_label
  else:
    column_root_label = f_obs_label
  mtz_dataset_original = raw_data.as_mtz_dataset(
    column_root_label=column_root_label)
  mtz_dataset_original.add_miller_array(
    miller_array = raw_flags,
    column_root_label=flag_label)
  mtz_dataset_original.set_name("Original-experimental-data")
  new_dataset = mtz_dataset_original.mtz_crystal().add_dataset(
    name = "Experimental-data-used-in-refinement", wavelength=1)
  new_dataset.add_miller_array(
    miller_array = fmodel_total.f_obs(),
    column_root_label="F-obs-filtered"+xray_suffix)
  another_dataset = new_dataset.mtz_crystal().add_dataset(
    name = "Model-structure-factors-(bulk-solvent-and-all-scales-included)",
    wavelength=1)
  another_dataset.add_miller_array(
    miller_array = fmodel_total.f_model_scaled_with_k1_composite_work_free(),
    column_root_label="F-model"+xray_suffix)
  yet_another_dataset = another_dataset.mtz_crystal().add_dataset(
    name = "Fourier-map-coefficients", wavelength=1)
  cmo = mmtbx.maps.compute_map_coefficients(
    fmodel = fmodel_total,
    params = params.electron_density_maps.map_coefficients)
  for ma in cmo.mtz_dataset.mtz_object().as_miller_arrays():
    labels=ma.info().labels
    ld = labels_decorator(amplitudes_label=labels[0], phases_label=labels[1])
    yet_another_dataset.add_miller_array(
      miller_array      = ma,
      column_root_label = labels[0],
      label_decorator   = ld)
  yet_another_dataset.mtz_object().write(
    file_name=prefix+".mtz")
  return prefix + ".mtz"

def is_amber_refinement(params):
  if sys.platform=='win32': return False
  if getattr(params, 'amber', False): return params.amber.use_amber
  return params.ensemble_refinement.amber.use_amber

#-----------------------------------------------------------------------
def run(args, command_name = "phenix.ensemble_refinement", out=None,
    validate=False, replace_stderr=True):
  if(len(args) == 0): args = ["--help"]
  command_line = (iotbx_option_parser(
    usage="%s reflection_file pdb_file [options]" % command_name,
    description='Example: %s data.mtz model.pdb' % command_name
  ).enable_dry_run().enable_show_defaults()).process(args=args)
  if (out is None):
    out = sys.stdout
  if(command_line.expert_level is not None):
    master_params.show(
      expert_level=command_line.expert_level,
      attributes_level=command_line.attributes_level,
      out=out)
    return
  inputs = mmtbx.command_line.load_model_and_data(
    args=command_line.args,
    master_phil=master_params,
    out=out,
    create_fmodel=False,
    process_pdb_file=False)
  working_phil = inputs.working_phil
  params = working_phil.extract()
  if (params.extra_restraints_file is not None):
    # XXX this is a revolting hack...
    print("Processing custom geometry restraints in file:", file=out)
    print("  %s" % params.extra_restraints_file, file=out)
    restraints_phil = iotbx.phil.parse(file_name=params.extra_restraints_file)
    cleanup_phil = iotbx.phil.parse("extra_restraints_file=None")
    working_phil = master_params.fetch(
      sources=[working_phil, restraints_phil, cleanup_phil])
    params = working_phil.extract()
  er_params = params.ensemble_refinement

  if er_params.electron_density_maps.apply_default_maps != False\
    or len(er_params.electron_density_maps.map_coefficients) == 0:
    maps_par = libtbx.env.find_in_repositories(
      relative_path=\
        "cctbx_project/mmtbx/refinement/ensemble_refinement/maps.params",
      test=os.path.isfile)
    maps_par_phil = iotbx.phil.parse(file_name=maps_par)
    working_params = mmtbx.refinement.ensemble_refinement.master_params.fetch(
                        sources = [working_phil]+[maps_par_phil])
    er_params = working_params.extract().ensemble_refinement

  if er_params.output_file_prefix == None:
    er_params.output_file_prefix = os.path.splitext(
      os.path.basename(inputs.pdb_file_names[0]))[0] + "_ensemble"
  log = multi_out()
  log.register(label="stdout", file_object=out)
  log.register(
    label="log_buffer",
    file_object=StringIO(),
    atexit_send_to=None)
  if (replace_stderr):
    sys.stderr = log
  log_file = open(er_params.output_file_prefix+'.log', "w")
  log.replace_stringio(
      old_label="log_buffer",
      new_label="log",
      new_file_object=log_file)
  timer = user_plus_sys_time()
  mmtbx.utils.print_programs_start_header(log=log, text=command_name)
  make_header("Ensemble refinement parameters", out = log)
  working_phil.show(out = log)
  make_header("Model and data statistics", out = log)
  print("Data file                               : %s" % \
    format_value("%5s", os.path.basename(params.input.xray_data.file_name)), file=log)
  print("Model file                              : %s \n" % \
    (format_value("%5s",os.path.basename(inputs.pdb_file_names[0]))), file=log)
  print("\nTLS MUST BE IN ATOM RECORDS OF INPUT PDB\n", file=log)
  f_obs = inputs.f_obs
  number_of_reflections = f_obs.indices().size()

  r_free_flags = inputs.r_free_flags
  raw_flags = inputs.raw_flags
  raw_data = inputs.raw_data

  print("\nPDB file name : ", inputs.pdb_file_names[0], file=log)

  # Process PDB file
  cif_objects = inputs.cif_objects
  pdb_file = inputs.pdb_file_names[0]
  # Model
  pdb_inp = iotbx.pdb.input(file_name=pdb_file)
  model = mmtbx.model.manager(
    model_input = pdb_inp,
    restraint_objects = cif_objects,
    log = log)
  if er_params.remove_alt_conf_from_input_pdb:
    n_removed_atoms = model.remove_alternative_conformations(
        always_keep_one_conformer=True)

  if n_removed_atoms > 0:
    pdb_file_removed_alt_confs = os.path.basename(pdb_file)[0:-4]+'_removed_alt_confs.pdb'
    print("\nRemoving alternative conformations", file=log)
    print("All occupancies reset to 1.0", file=log)
    print("New PDB : ", pdb_file_removed_alt_confs, "\n", file=log)
    pdb_str = model.model_as_pdb()
    f = open(pdb_file_removed_alt_confs, 'w')
    f.write(pdb_str)
    f.close()
    pdb_inp = iotbx.pdb.input(file_name=pdb_file_removed_alt_confs)
    model = mmtbx.model.manager(
      model_input = pdb_inp,
      restraint_objects = cif_objects,
      #pdb_interpretation_params = params.ensemble_refinement,
      log = log)

  model.process(pdb_interpretation_params = params.ensemble_refinement,
    make_restraints=True)

  if model.get_number_of_models() > 1:
    raise Sorry("Multiple models not supported.")
  # Remove alternative conformations if present
  n_removed_atoms = 0

  if n_removed_atoms>0 and is_amber_refinement(params):
    raise Sorry('Amber does not support alt. locs. in Ensemble Refinement')


  # Refinement flags
  # Worst hack I've ever seen! No wonder ensemble refinement is semi-broken!
  class rf:
    def __init__(self, size):
      self.individual_sites     = True
      self.individual_adp       = False
      self.sites_individual     = flex.bool(size, True)
      self.sites_torsion_angles = None
      self.torsion_angles       = None
      self.den                  = er_params.den_restraints
      self.adp_individual_iso   = None
      self.adp_individual_aniso = None
    def inflate(self, **keywords): pass
    def select_detached(self, **keywords): pass

  refinement_flags = rf(size = model.get_number_of_atoms())

  model.set_refinement_flags(refinement_flags)
  model.process(make_restraints=True)

  # Geometry file
  xray_structure = model.get_xray_structure()
  sites_cart = xray_structure.sites_cart()
  site_labels = xray_structure.scatterers().extract_labels()
  model.restraints_manager.geometry.show_sorted(
    sites_cart=sites_cart,
    site_labels=site_labels,
    f=open(er_params.output_file_prefix+'.geo','w') )

  print("Unit cell                               :", f_obs.unit_cell(), file=log)
  print("Space group                             :", \
    f_obs.crystal_symmetry().space_group_info().symbol_and_number(), file=log)
  print("Number of symmetry operators            :", \
    f_obs.crystal_symmetry().space_group_info().type().group().order_z(), file=log)
  print("Unit cell volume                        : %-15.4f" % \
    f_obs.unit_cell().volume(), file=log)
  f_obs_labels = f_obs.info().label_string()

  if (command_line.options.dry_run):
    return None

  fmodel = mmtbx.utils.fmodel_simple(
    f_obs                      = f_obs,
    xray_structures            = [model.get_xray_structure()],
    scattering_table           = "n_gaussian",
    r_free_flags               = r_free_flags,
    target_name                = er_params.target_name,
    bulk_solvent_and_scaling   = False,
    bss_params                 = None,
    mask_params                = None,
    twin_laws                  = None,
    skip_twin_detection        = True,
    twin_switch_tolerance      = 2.0,
    outliers_rejection         = True,
    bulk_solvent_correction    = True,
    anisotropic_scaling        = True,
    log                        = log)
  hl_coeffs = inputs.hl_coeffs
  if (hl_coeffs is not None) and (params.input.use_experimental_phases):
    print("Using MLHL target with experimental phases", file=log)
    er_params.target_name = "mlhl"
    hl_coeffs = hl_coeffs.common_set(other=fmodel.f_obs())
  else :
    hl_coeffs = None
  # XXX is this intentional?
  fmodel = mmtbx.f_model.manager(
    mask_params                  = er_params.mask,
    xray_structure               = model.get_xray_structure(),
    f_obs                        = fmodel.f_obs(),
    r_free_flags                 = fmodel.r_free_flags(),
    target_name                  = er_params.target_name,
    abcd                         = hl_coeffs)
  hd_sel = model.get_hd_selection()
  model.get_xray_structure().set_occupancies(
        value     = 1.0,
        selection = hd_sel)
  model.show_occupancy_statistics(out = log)

  fmodel.update_xray_structure(
    xray_structure      = model.get_xray_structure(),
    update_f_calc       = True,
    update_f_mask       = False,
    force_update_f_mask = False)

  n_outl = f_obs.data().size() - fmodel.f_obs().data().size()
  show_data(fmodel          = fmodel,
            n_outl          = n_outl,
            test_flag_value = inputs.test_flag_value,
            f_obs_labels    = f_obs_labels,
            log             = log)
  show_model_vs_data(fmodel = fmodel,
                     log    = log)

  best_trial = None
  if (len(er_params.ptls) == 1):
    best_trial = run_wrapper(
      fmodel               = fmodel,
      model                = model,
      er_params            = er_params,
      raw_data             = raw_data,
      raw_flags            = raw_flags,
      log                  = log).__call__(
        ptls=er_params.ptls[0],
        buffer_output=False,
        append_ptls=False)
  else :
    driver = run_wrapper(
      fmodel               = fmodel,
      model                = model,
      er_params            = er_params,
      raw_data             = raw_data,
      raw_flags            = raw_flags,
      log                  = log)
    trials = []
    if (er_params.nproc in [1, None]) or (sys.platform == "win32"):
      for ptls in er_params.ptls :
        make_header("Running with pTLS = %g" % ptls, out=log)
        trial_result = driver(ptls, buffer_output=False, write_log=False)
        assert (trial_result is not None)
        trials.append(trial_result)
    else :
      trials = easy_mp.pool_map(
        fixed_func=driver,
        args=er_params.ptls,
        processes=er_params.nproc)
    assert (not None in trials)
    best_trial = min(trials, key=lambda t: t.r_free)
    best_trial.save_final(er_params.output_file_prefix)

  show_total_time(out = log)
  return result(
    best_trial=best_trial,
    prefix=er_params.output_file_prefix,
    validate=validate,
    log=log)

class run_wrapper(object):
  def __init__(self, model, fmodel, raw_data, raw_flags, er_params, log):
    adopt_init_args(self, locals())

  def __call__(self, ptls, buffer_output=True, write_log=True,
      append_ptls=True):
    out = self.log
    log_out = None
    if (buffer_output):
      out = StringIO()
    run_number = None
    if (append_ptls):
      run_number = ptls
    ensemble_refinement = run_ensemble_refinement(
      fmodel               = self.fmodel.deep_copy(),
      model                = self.model.deep_copy(),
      params               = self.er_params,
      raw_data             = self.raw_data,
      raw_flags            = self.raw_flags,
      run_number           = run_number,
      ptls                 = ptls,
      log                  = out)
    if (buffer_output):
      log_out = out.getvalue()
      if (write_log):
        log_file_name = self.er_params.output_file_prefix + '_ptls-' + \
          str(ptls) + '.log'
        log_file = open(log_file_name, 'w')
        log_file.write(log_out)
    return trial(
      ptls=ptls,
      r_work=ensemble_refinement.fmodel_total.r_work(),
      r_free=ensemble_refinement.fmodel_total.r_free(),
      pdb_file=ensemble_refinement.pdb_file,
      mtz_file=ensemble_refinement.mtz_file,
      log_out=log_out,
      number_of_models=len(ensemble_refinement.er_data.xray_structures))

class trial(slots_getstate_setstate):
  __slots__ = ["r_work", "r_free", "pdb_file", "mtz_file", "number_of_models",
               "log_out", "ptls"]
  def __init__(self, **kwds):
    kwds = dict(kwds)
    for name in self.__slots__ :
      setattr(self, name, kwds[name])

  def save_final(self, prefix):
    pdb_out = prefix + ".pdb"
    if (self.pdb_file.endswith(".gz")):
      pdb_out += ".gz"
    os.rename(self.pdb_file, pdb_out)
    os.rename(self.mtz_file, prefix + ".mtz")
    self.pdb_file = pdb_out
    self.mtz_file = prefix + ".mtz"

########################################################################
# Phenix GUI hooks
class result(slots_getstate_setstate):
  __slots__ = [
    "directory", "r_work", "r_free",
    "number_of_models", "pdb_file", "mtz_file","validation", "chi_angles"
  ]
  def __init__(self,
      best_trial,
      prefix,
      log,
      validate=False):
    for attr in ["r_work", "r_free", "number_of_models", "pdb_file",
                 "mtz_file"] :
      setattr(self, attr, getattr(best_trial, attr))
    self.directory = os.getcwd()
    self.validation = None
    if (validate):
      from mmtbx.command_line import validation_summary
      self.validation = validation_summary.run(
        args=[self.pdb_file],
        out=log)
      assert (type(self.validation).__name__ == 'ensemble')

      # calculate chi angles for each model
      # each model is assumed to have the same number of protein residues
      self.chi_angles = list()
      hierarchy = pdb.input(self.pdb_file).construct_hierarchy()
      for model in hierarchy.models():
        self.chi_angles.append(calculate_chi_angles(model))

  def get_result_files(self, output_dir=None):
    if (output_dir is None):
      output_dir = self.directory
    return (os.path.join(self.directory, self.pdb_file),
            os.path.join(self.directory, self.mtz_file))

  def finish_job(self):
    pdb_file, mtz_file = self.get_result_files()
    return (
      [(pdb_file, "Final ensemble"),
       (mtz_file, "Map coefficients")],
      [("R-work", "%.4f" % self.r_work),
       ("R-free", "%.4f" % self.r_free),
       ("Models", str(self.number_of_models)),]
    )

class launcher(runtime_utils.target_with_save_result):
  def run(self):
    os.mkdir(self.output_dir)
    os.chdir(self.output_dir)
    return run(args=list(self.args),
      out=sys.stdout,
      validate=True)

def validate_params(params):
  mmtbx.command_line.validate_input_params(params)
  if (params.ensemble_refinement.ptls is None):
    raise Sorry("You must specify a fraction of atoms to use for TLS fitting.")
  elif (len(params.input.xray_data.labels[0].split(",")) > 2):
    raise Sorry("Anomalous data are not allowed in this program.")

  # check files
  mmtbx.command_line.check_files(
    params.input.pdb.file_name, 'pdb',
    'Please provide a valid structure for the Input model.')
  mmtbx.command_line.check_files(
    params.input.monomers.file_name, 'cif',
    'Please provide valid restraints')
  mmtbx.command_line.check_files(
    params.extra_restraints_file, 'phil',
    'Please provide a valid file containing custom restraints.')

  return params

# =============================================================================
from mmtbx.validation import rotalyze
from mmtbx.rotamer import sidechain_angles

def calculate_chi_angles(model=None):
  '''

  =============================================================================
  Function for calculating all dihedral angles for each protein residue in a
  model.

  Parameters:
  -----------
  model - from hierarchy.models()

  Return:
  -------
  dictionary - where 'id_str' is a list containing residue ids
               (atom_group.id_str()) and 'chi_angles' is a list containing
               a list of dihedral angles for each residue (list of lists)

  Notes:
  ------
  The list for an amino acid is of size 0 if it has no defined dihedral angles.
  A dihedral angle in the list for an atom_group can be None if there are
  missing atoms. A protein residue is defined as common_amino_acid.

  =============================================================================

  '''

  id_str = list()
  chi_angles = list()
  xyz = list()

  if (model is not None):
    get_class = pdb.common_residue_names_get_class
    angles = sidechain_angles.SidechainAngles(False)

    # loop over all atom groups in chain
    for chain in model.chains():
      for rg in chain.residue_groups():
        all_dict = rotalyze.construct_complete_sidechain(rg)
        for ag in rg.atom_groups():
          residue_class = get_class(ag.resname)
          if (residue_class == 'common_amino_acid'):
            atom_dict = all_dict.get(ag.altloc)
            id_str.append(ag.id_str())
            xyz.append(ag.atoms().extract_xyz().mean())
            chi_angles.append(angles.measureChiAngles(
              res=ag,atom_dict=atom_dict))

  return { 'id_str': id_str,
           'xyz': xyz,
           'chi_angles': chi_angles }


 *******************************************************************************


 *******************************************************************************
mmtbx/refinement/ensemble_refinement/ensemble_probability.py
from __future__ import absolute_import, division, print_function
import sys
from iotbx.option_parser import iotbx_option_parser
from mmtbx import utils
from iotbx import file_reader
import iotbx.phil
import libtbx.phil
from libtbx.utils import Sorry
from cctbx import maptbx
from cctbx.array_family import flex
from libtbx import group_args
from cctbx import adptbx
from iotbx import pdb
from libtbx.utils import Sorry, multi_out
from six.moves import cStringIO as StringIO
from six.moves import range
from iotbx import extract_xtal_data

master_phil = iotbx.phil.parse("""\
ensemble_probability {
  verbose = True
    .type = bool
    .help = '''Verbose'''
  assign_sigma_from_map = False
    .type = bool
    .help = '''ensemble mtz file containing precalculated map coeffs'''
  ensemble_sigma_map_input = None
    .type = str
    .help = '''ensemble mtz file containing precalculated map coeffs'''
  output_model_and_model_ave_mtz = False
    .type = bool
    .help = '''Output individual and average Fcalc maps'''
  fcalc_high_resolution = 2.0
    .type = float
    .help = '''high resolution limit for Fcalc map'''
  fcalc_low_resolution = None
    .type = float
    .help = '''low resolution limit for Fcalc map'''
  residue_detail = False
    .type = bool
    .help = '''Average probability per residue'''
  ignore_hd = True
    .type = bool
    .help = '''Ignore H/D, only applicable when residue detail is used'''
  sort_ensemble_by_nll_score = False
    .type = bool
    .help = '''ordered each model in ensemble by negative log liklihood score and output'''
  fobs_vs_fcalc_post_nll = False
    .type = bool
    .help = '''Recalc R factors based removing models or atoms based on nll score'''
}
""")

class map_cc_funct(object):
  def __init__(self, map_1,
                     xray_structure,
                     fft_map,
                     atom_radius,
                     hydrogen_atom_radius,
                     model_i,
                     number_previous_scatters,
                     ignore_hd = False,
                     residue_detail = True,
                     selection = None,
                     pdb_hierarchy = None):
    self.xray_structure = xray_structure
    self.selection = selection
    self.pdb_hierarchy = pdb_hierarchy
    self.result = []
    self.map_1_size = map_1.size()
    self.map_1_stat = maptbx.statistics(map_1)
    self.atoms_with_labels = None
    self.residue_detail = residue_detail
    self.model_i = model_i
    if(pdb_hierarchy is not None):
      self.atoms_with_labels = list(pdb_hierarchy.atoms_with_labels())
    scatterers = self.xray_structure.scatterers()
    sigma_occ = flex.double()
    if(self.selection is None):
      self.selection = flex.bool(scatterers.size(), True)
    real_map_unpadded = fft_map.real_map_unpadded()
    sites_cart = self.xray_structure.sites_cart()

    if not self.residue_detail:
      self.gifes = [None,]*scatterers.size()
      self._result = [None,]*scatterers.size()
      #
      atom_radii = flex.double(scatterers.size(), atom_radius)
      for i_seq, sc in enumerate(scatterers):
        if(self.selection[i_seq]):
          if(sc.element_symbol().strip().lower() in ["h","d"]):
            atom_radii[i_seq] = hydrogen_atom_radius
      #
      for i_seq, site_cart in enumerate(sites_cart):
        if(self.selection[i_seq]):
          sel = maptbx.grid_indices_around_sites(
            unit_cell  = self.xray_structure.unit_cell(),
            fft_n_real = real_map_unpadded.focus(),
            fft_m_real = real_map_unpadded.all(),
            sites_cart = flex.vec3_double([site_cart]),
            site_radii = flex.double([atom_radii[i_seq]]))
          self.gifes[i_seq] = sel
          m1 = map_1.select(sel)
          ed1 = map_1.eight_point_interpolation(scatterers[i_seq].site)
          sigma_occ.append(ed1)
          a = None
          if(self.atoms_with_labels is not None):
            a = self.atoms_with_labels[i_seq]
          self._result[i_seq] = group_args(atom = a, m1 = m1, ed1 = ed1,
            xyz=site_cart)
      self.xray_structure.set_occupancies(sigma_occ)

      ### For testing other residue averaging options
      residues = self.extract_residues(model_i = model_i,
                                       number_previous_scatters = number_previous_scatters)
      self.xray_structure.residue_selections = residues

    # Residue detail
    if self.residue_detail:
      assert self.pdb_hierarchy is not None
      residues = self.extract_residues(model_i = model_i,
                                       number_previous_scatters = number_previous_scatters)
      self.gifes = [None,]*len(residues)
      self._result = [None,]*len(residues)
      for i_seq, residue in enumerate(residues):
        residue_sites_cart = sites_cart.select(residue.selection)
        if 0: print(i_seq, list(residue.selection)) # DEBUG
        sel = maptbx.grid_indices_around_sites(
          unit_cell  = self.xray_structure.unit_cell(),
          fft_n_real = real_map_unpadded.focus(),
          fft_m_real = real_map_unpadded.all(),
          sites_cart = residue_sites_cart,
          site_radii = flex.double(residue.selection.size(), atom_radius))
        self.gifes[i_seq] = sel
        m1 = map_1.select(sel)
        ed1 = flex.double()
        for i_seq_r in residue.selection:
          ed1.append(map_1.eight_point_interpolation(scatterers[i_seq_r].site))
        self._result[i_seq] = \
          group_args(residue = residue, m1 = m1, ed1 = flex.mean(ed1),
            xyz=residue_sites_cart.mean(), n_atoms=residue_sites_cart.size())

        residue_scatterers = scatterers.select(residue.selection)
        residue_ed1 = flex.double()
        for n,scatter in enumerate(residue_scatterers):
          if ignore_hd:
            if scatter.element_symbol() not in ['H', 'D']:
              residue_ed1.append(ed1[n])
          else:
            residue_ed1.append(ed1[n])
        for x in range(ed1.size()):
          sigma_occ.append(flex.mean(residue_ed1))

      self.xray_structure.set_occupancies(sigma_occ)
      self.xray_structure.residue_selections = residues

    del map_1

  def extract_residues(self, model_i, number_previous_scatters, combine = True):
    result = []
    model = self.pdb_hierarchy.models()[model_i]
    rm = []
    for chain in model.chains():
      for rg in chain.residue_groups():
        rg_i_seqs = []
        r_name = None
        for ag in rg.atom_groups():
          if(r_name is None): r_name = ag.resname
          for atom in ag.atoms():
            if(self.selection[atom.i_seq - number_previous_scatters]):
              rg_i_seqs.append(atom.i_seq - number_previous_scatters)
        if(len(rg_i_seqs) != 0):
          rm.append(group_args(
            selection = flex.size_t(rg_i_seqs),
            name      = r_name,
            model_id  = model_i,
            resid     = rg.resid(),
            chain_id  = chain.id))
    result.append(rm)

    if(combine):
      r0 = result[0]
      for r in result[1:]:
        for i, ri in enumerate(r):
          r0[i].selection.extend(ri.selection)
          assert r0[i].name == ri.name
    else:
      r0 = result[0]
      for r in result[1:]:
        r0.extend(r)

    return r0


def get_map_sigma(ens_pdb_hierarchy,
                           ens_pdb_xrs,
                           log,
                           model_i,
                           number_previous_scatters,
                           residue_detail = True,
                           ignore_hd = True,
                           map_coeffs_1 = None,
                           fft_map_1 = None):
  assert [map_coeffs_1, fft_map_1].count(None) == 1
  if fft_map_1 == None:
    fft_map_1 = map_coeffs_1.fft_map()
    fft_map_1.apply_sigma_scaling()
  map_1 = fft_map_1.real_map_unpadded()
  atom_radius = 1.5
  hydrogen_atom_radius = 1.0
  map_cc_obj = map_cc_funct(
    map_1                = map_1,
    xray_structure       = ens_pdb_xrs,
    model_i              = model_i,
    number_previous_scatters = number_previous_scatters,
    fft_map              = fft_map_1,
    atom_radius          = atom_radius,
    hydrogen_atom_radius = hydrogen_atom_radius,
    selection            = None,
    residue_detail       = residue_detail,
    ignore_hd            = ignore_hd,
    pdb_hierarchy        = ens_pdb_hierarchy)
  return map_cc_obj.xray_structure

def write_ensemble_pdb(filename,
                       xrs_list,
                       ens_pdb_hierarchy):
    out = open(filename, 'w')
    crystal_symmetry = xrs_list[0].crystal_symmetry()
    print("REMARK   3  TIME-AVERAGED ENSEMBLE REFINEMENT", file=out)
    print("REMARK   3  OCCUPANCY = MAP SIGMA LEVEL", file=out)
    print(pdb.format_cryst1_record(crystal_symmetry = crystal_symmetry), file=out)
    print(pdb.format_scale_records(unit_cell = crystal_symmetry.unit_cell()), file=out)
    atoms_reset_serial = True

    for i_model, xrs in enumerate(xrs_list):
      scatterers = xrs.scatterers()
      sites_cart = xrs.sites_cart()
      u_isos = xrs.extract_u_iso_or_u_equiv()
      occupancies = scatterers.extract_occupancies()
      u_carts = scatterers.extract_u_cart_plus_u_iso(xrs.unit_cell())
      scat_types = scatterers.extract_scattering_types()
      i_model_ens_pdb_hierarchy = ens_pdb_hierarchy.models()[i_model]
      pdb_atoms = i_model_ens_pdb_hierarchy.atoms()
      for j_seq, atom in enumerate(pdb_atoms):
        if j_seq < len(sites_cart):
          atom.xyz = sites_cart[j_seq]
          atom.occ = occupancies[j_seq]
          atom.b = adptbx.u_as_b(u_isos[j_seq])
          e = scat_types[j_seq]
          if (len(e) > 1 and "+-0123456789".find(e[1]) >= 0):
            atom.element = "%2s" % e[:1]
            atom.charge = "%-2s" % e[1:]
          elif (len(e) > 2):
            atom.element = "%2s" % e[:2]
            atom.charge = "%-2s" % e[2:]
          else:
            atom.element = "%2s" % e
            atom.charge = "  "
          if (scatterers[j_seq].flags.use_u_aniso()):
            atom.uij = u_carts[j_seq]
          elif(False):
            atom.uij = self.u_cart
          else:
            atom.uij = (-1,-1,-1,-1,-1,-1)
      if (atoms_reset_serial):
        atoms_reset_serial_first_value = 1
      else:
        atoms_reset_serial_first_value = None
    out.write(ens_pdb_hierarchy.as_pdb_string(
      append_end=False,
      atoms_reset_serial_first_value=atoms_reset_serial_first_value))

def reflection_file_server(crystal_symmetry, reflection_files, log):
  from iotbx import reflection_file_utils
  return reflection_file_utils.reflection_file_server(
    crystal_symmetry=crystal_symmetry,
    force_symmetry=True,
    reflection_files=reflection_files,
    err=log)

class ensemble_probability(object):
  def run(self, args, command_name, out=sys.stdout):
    command_line = (iotbx_option_parser(
      usage="%s [options]" % command_name,
      description='Example: %s data.mtz data.mtz ref_model.pdb'%command_name)
      .option(None, "--show_defaults",
        action="store_true",
        help="Show list of parameters.")
      ).process(args=args)

    cif_file = None
    processed_args = utils.process_command_line_args(
                       args          = args,
                       log           = sys.stdout,
                       master_params = master_phil)
    params = processed_args.params
    if(params is None): params = master_phil
    self.params = params.extract().ensemble_probability
    pdb_file_names = processed_args.pdb_file_names
    if len(pdb_file_names) != 1 :
      raise Sorry("Only one PDB structure may be used")
    pdb_file = file_reader.any_file(pdb_file_names[0])
    self.log = multi_out()
    self.log.register(label="stdout", file_object=sys.stdout)
    self.log.register(
      label="log_buffer",
      file_object=StringIO(),
      atexit_send_to=None)
    sys.stderr = self.log
    log_file = open(pdb_file_names[0].split('/')[-1].replace('.pdb','') + '_pensemble.log', "w")

    self.log.replace_stringio(
        old_label="log_buffer",
        new_label="log",
        new_file_object=log_file)
    utils.print_header(command_name, out = self.log)
    params.show(out = self.log)
    #
    f_obs = None
    r_free_flags = None
    reflection_files = processed_args.reflection_files

    if self.params.fobs_vs_fcalc_post_nll:
      if len(reflection_files) == 0:
        raise Sorry("Fobs from input MTZ required for fobs_vs_fcalc_post_nll")

    if len(reflection_files) > 0:
      crystal_symmetry = processed_args.crystal_symmetry
      print('Reflection file : ', processed_args.reflection_file_names[0], file=self.log)
      utils.print_header("Model and data statistics", out = self.log)
      rfs = reflection_file_server(
        crystal_symmetry = crystal_symmetry,
        reflection_files = processed_args.reflection_files,
        log              = self.log)

      parameters = extract_xtal_data.data_and_flags_master_params().extract()
      determine_data_and_flags_result = extract_xtal_data.run(
        reflection_file_server = rfs,
        parameters             = parameters,
        keep_going             = True)
      f_obs = determine_data_and_flags_result.f_obs
      number_of_reflections = f_obs.indices().size()
      r_free_flags = determine_data_and_flags_result.r_free_flags
      test_flag_value = determine_data_and_flags_result.test_flag_value
      if(r_free_flags is None):
        r_free_flags=f_obs.array(data=flex.bool(f_obs.data().size(), False))

    # process PDB
    pdb_file.assert_file_type("pdb")
    #
    pdb_in = pdb.input(file_name=pdb_file.file_name)
    ens_pdb_hierarchy = pdb_in.construct_hierarchy()
    ens_pdb_hierarchy.atoms().reset_i_seq()
    ens_pdb_xrs_s = pdb_in.xray_structures_simple()
    number_structures = len(ens_pdb_xrs_s)
    print('Number of structure in ensemble : ', number_structures, file=self.log)

    # Calculate sigmas from input map only
    if self.params.assign_sigma_from_map and self.params.ensemble_sigma_map_input is not None:
      # process MTZ
      input_file = file_reader.any_file(self.params.ensemble_sigma_map_input)
      if input_file.file_type == "hkl" :
        if input_file.file_object.file_type() != "ccp4_mtz" :
           raise Sorry("Only MTZ format accepted for map input")
        else:
          mtz_file = input_file
      else:
        raise Sorry("Only MTZ format accepted for map input")
      miller_arrays = mtz_file.file_server.miller_arrays
      map_coeffs_1 = miller_arrays[0]
      #
      xrs_list = []
      for n, ens_pdb_xrs in enumerate(ens_pdb_xrs_s):
        # get sigma levels from ensemble fc for each structure
        xrs = get_map_sigma(ens_pdb_hierarchy = ens_pdb_hierarchy,
                          ens_pdb_xrs       = ens_pdb_xrs,
                          map_coeffs_1      = map_coeffs_1,
                          residue_detail    = self.params.residue_detail,
                          ignore_hd         = self.params.ignore_hd,
                          log               = self.log)
        xrs_list.append(xrs)
      # write ensemble pdb file, occupancies as sigma level
      filename = pdb_file_names[0].split('/')[-1].replace('.pdb','') + '_vs_' + self.params.ensemble_sigma_map_input.replace('.mtz','') + '_pensemble.pdb'
      write_ensemble_pdb(filename = filename,
                         xrs_list = xrs_list,
                         ens_pdb_hierarchy = ens_pdb_hierarchy
                         )

    # Do full analysis vs Fobs
    else:
      model_map_coeffs = []
      fmodel = None
      # Get <fcalc>
      for model, ens_pdb_xrs in enumerate(ens_pdb_xrs_s):
        ens_pdb_xrs.set_occupancies(1.0)
        if model == 0:
          # If mtz not supplied get fobs from xray structure...
          # Use input Fobs for scoring against nll
          if self.params.fobs_vs_fcalc_post_nll:
            dummy_fobs = f_obs
          else:
            if f_obs == None:
              if self.params.fcalc_high_resolution == None:
                raise Sorry("Please supply high resolution limit or input mtz file.")
              dummy_dmin = self.params.fcalc_high_resolution
              dummy_dmax = self.params.fcalc_low_resolution
            else:
              print('Supplied mtz used to determine high and low resolution cuttoffs', file=self.log)
              dummy_dmax, dummy_dmin = f_obs.d_max_min()
            #
            dummy_fobs = abs(ens_pdb_xrs.structure_factors(d_min = dummy_dmin).f_calc())
            dummy_fobs.set_observation_type_xray_amplitude()
            # If mtz supplied, free flags are over written to prevent array size error
            r_free_flags = dummy_fobs.array(data=flex.bool(dummy_fobs.data().size(),False))
          #
          fmodel = utils.fmodel_simple(
                     scattering_table         = "wk1995",
                     xray_structures          = [ens_pdb_xrs],
                     f_obs                    = dummy_fobs,
                     target_name              = 'ls',
                     bulk_solvent_and_scaling = False,
                     r_free_flags             = r_free_flags
                     )
          f_calc_ave = fmodel.f_calc().array(data = fmodel.f_calc().data()*0).deep_copy()
          # XXX Important to ensure scale is identical for each model and <model>
          fmodel.set_scale_switch = 1.0
          f_calc_ave_total = fmodel.f_calc().data().deep_copy()
        else:
          fmodel.update_xray_structure(xray_structure  = ens_pdb_xrs,
                                       update_f_calc   = True,
                                       update_f_mask   = False)
          f_calc_ave_total += fmodel.f_calc().data().deep_copy()
        print('Model :', model+1, file=self.log)
        print("\nStructure vs real Fobs (no bulk solvent or scaling)", file=self.log)
        print('Rwork          : %5.4f '%fmodel.r_work(), file=self.log)
        print('Rfree          : %5.4f '%fmodel.r_free(), file=self.log)
        print('K1             : %5.4f '%fmodel.scale_k1(), file=self.log)
        fcalc_edm        = fmodel.electron_density_map()
        fcalc_map_coeffs = fcalc_edm.map_coefficients(map_type = 'Fc')
        fcalc_mtz_dataset = fcalc_map_coeffs.as_mtz_dataset(column_root_label ='Fc')
        if self.params.output_model_and_model_ave_mtz:
          fcalc_mtz_dataset.mtz_object().write(file_name = str(model+1)+"_Fc.mtz")
        model_map_coeffs.append(fcalc_map_coeffs.deep_copy())

      fmodel.update(f_calc = f_calc_ave.array(f_calc_ave_total / number_structures))
      print("\nEnsemble vs real Fobs (no bulk solvent or scaling)", file=self.log)
      print('Rwork          : %5.4f '%fmodel.r_work(), file=self.log)
      print('Rfree          : %5.4f '%fmodel.r_free(), file=self.log)
      print('K1             : %5.4f '%fmodel.scale_k1(), file=self.log)

      # Get <Fcalc> map
      fcalc_ave_edm        = fmodel.electron_density_map()
      fcalc_ave_map_coeffs = fcalc_ave_edm.map_coefficients(map_type = 'Fc').deep_copy()
      fcalc_ave_mtz_dataset = fcalc_ave_map_coeffs.as_mtz_dataset(column_root_label ='Fc')
      if self.params.output_model_and_model_ave_mtz:
        fcalc_ave_mtz_dataset.mtz_object().write(file_name = "aveFc.mtz")
      fcalc_ave_map_coeffs = fcalc_ave_map_coeffs.fft_map()
      fcalc_ave_map_coeffs.apply_volume_scaling()
      fcalc_ave_map_data   = fcalc_ave_map_coeffs.real_map_unpadded()
      fcalc_ave_map_stats  = maptbx.statistics(fcalc_ave_map_data)

      print("<Fcalc> Map Stats :", file=self.log)
      fcalc_ave_map_stats.show_summary(f = self.log)
      offset = fcalc_ave_map_stats.min()
      model_neg_ll = []

      number_previous_scatters = 0

      # Run through structure list again and get probability
      xrs_list = []
      for model, ens_pdb_xrs in enumerate(ens_pdb_xrs_s):
        if self.params.verbose:
          print('\n\nModel                   : ', model+1, file=self.log)
        # Get model atom sigmas vs Fcalc
        fcalc_map = model_map_coeffs[model].fft_map()
        fcalc_map.apply_volume_scaling()
        fcalc_map_data  = fcalc_map.real_map_unpadded()
        fcalc_map_stats  = maptbx.statistics(fcalc_map_data)
        if self.params.verbose:
          print("Fcalc map stats         :", file=self.log)
        fcalc_map_stats.show_summary(f = self.log)

        xrs = get_map_sigma(ens_pdb_hierarchy = ens_pdb_hierarchy,
                            ens_pdb_xrs       = ens_pdb_xrs,
                            fft_map_1         = fcalc_map,
                            model_i           = model,
                            residue_detail    = self.params.residue_detail,
                            ignore_hd         = self.params.ignore_hd,
                            number_previous_scatters = number_previous_scatters,
                            log               = self.log)
        fcalc_sigmas = xrs.scatterers().extract_occupancies()
        del fcalc_map
        # Get model atom sigmas vs <Fcalc>
        xrs = get_map_sigma(ens_pdb_hierarchy = ens_pdb_hierarchy,
                            ens_pdb_xrs       = ens_pdb_xrs,
                            fft_map_1         = fcalc_ave_map_coeffs,
                            model_i           = model,
                            residue_detail    = self.params.residue_detail,
                            ignore_hd         = self.params.ignore_hd,
                            number_previous_scatters = number_previous_scatters,
                            log               = self.log)

        ### For testing other residue averaging options
        #print xrs.residue_selections

        fcalc_ave_sigmas = xrs.scatterers().extract_occupancies()
        # Probability of model given <model>
        prob = fcalc_ave_sigmas / fcalc_sigmas
        # XXX debug option
        if False:
          for n,p in enumerate(prob):
            print(' {0:5d} {1:5.3f}'.format(n,p), file=self.log)
        # Set probabilty between 0 and 1
        # XXX Make Histogram / more stats
        prob_lss_zero = flex.bool(prob <= 0)
        prob_grt_one = flex.bool(prob > 1)
        prob.set_selected(prob_lss_zero, 0.001)
        prob.set_selected(prob_grt_one, 1.0)
        xrs.set_occupancies(prob)
        xrs_list.append(xrs)
        sum_neg_ll = sum(-flex.log(prob))
        model_neg_ll.append((sum_neg_ll, model))
        if self.params.verbose:
          print('Model probability stats :', file=self.log)
          print(prob.min_max_mean().show(), file=self.log)
          print('  Count < 0.0 : ', prob_lss_zero.count(True), file=self.log)
          print('  Count > 1.0 : ', prob_grt_one.count(True), file=self.log)

        # For averaging by residue
        number_previous_scatters += ens_pdb_xrs.sites_cart().size()

      # write ensemble pdb file, occupancies as sigma level
      write_ensemble_pdb(filename = pdb_file_names[0].split('/')[-1].replace('.pdb','') + '_pensemble.pdb',
                       xrs_list = xrs_list,
                       ens_pdb_hierarchy = ens_pdb_hierarchy
                       )

      # XXX Test ordering models by nll
      # XXX Test removing nth percentile atoms
      if self.params.sort_ensemble_by_nll_score or self.params.fobs_vs_fcalc_post_nll:
        for percentile in [1.0,0.975,0.95,0.9,0.8,0.6,0.2]:
          model_neg_ll = sorted(model_neg_ll)
          f_calc_ave_total_reordered = None
          print_list = []
          for i_neg_ll in model_neg_ll:
            xrs = xrs_list[i_neg_ll[1]]
            nll_occ = xrs.scatterers().extract_occupancies()

            # Set q=0 nth percentile atoms
            sorted_nll_occ = sorted(nll_occ, reverse=True)
            number_atoms = len(sorted_nll_occ)
            percentile_prob_cutoff = sorted_nll_occ[int(number_atoms * percentile)-1]
            cutoff_selections = flex.bool(nll_occ < percentile_prob_cutoff)
            cutoff_nll_occ = flex.double(nll_occ.size(), 1.0).set_selected(cutoff_selections, 0.0)
            #XXX Debug
            if False:
              print('\nDebug')
              for x in range(len(cutoff_selections)):
                print(cutoff_selections[x], nll_occ[x], cutoff_nll_occ[x])
              print(percentile)
              print(percentile_prob_cutoff)
              print(cutoff_selections.count(True))
              print(cutoff_selections.size())
              print(cutoff_nll_occ.count(0.0))
              print('Count q = 1           : ', cutoff_nll_occ.count(1.0))
              print('Count scatterers size : ', cutoff_nll_occ.size())

            xrs.set_occupancies(cutoff_nll_occ)
            fmodel.update_xray_structure(xray_structure  = xrs,
                                         update_f_calc   = True,
                                         update_f_mask   = True)

            if f_calc_ave_total_reordered == None:
              f_calc_ave_total_reordered = fmodel.f_calc().data().deep_copy()
              f_mask_ave_total_reordered = fmodel.f_masks()[0].data().deep_copy()
              cntr = 1
            else:
              f_calc_ave_total_reordered += fmodel.f_calc().data().deep_copy()
              f_mask_ave_total_reordered += fmodel.f_masks()[0].data().deep_copy()
              cntr+=1
            fmodel.update(f_calc = f_calc_ave.array(f_calc_ave_total_reordered / cntr).deep_copy(),
                          f_mask = f_calc_ave.array(f_mask_ave_total_reordered / cntr).deep_copy()
                          )

            # Update solvent and scale
            # XXX Will need to apply_back_trace on latest version
            fmodel.set_scale_switch = 0
            fmodel.update_all_scales()

            # Reset occ for outout
            xrs.set_occupancies(nll_occ)
            # k1 updated vs Fobs
            if self.params.fobs_vs_fcalc_post_nll:
              print_list.append([cntr, i_neg_ll[0], i_neg_ll[1], fmodel.r_work(), fmodel.r_free()])

          # Order models by nll and print summary
          print('\nModels ranked by nll <Fcalc> R-factors recalculated', file=self.log)
          print('Percentile cutoff : {0:5.3f}'.format(percentile), file=self.log)
          xrs_list_sorted_nll = []
          print('      |      NLL     <Rw>     <Rf>    Ens Model', file=self.log)
          for info in print_list:
            print(' {0:4d} | {1:8.1f} {2:8.4f} {3:8.4f} {4:12d}'.format(
              info[0],
              info[1],
              info[3],
              info[4],
              info[2]+1,
              ), file=self.log)
            xrs_list_sorted_nll.append(xrs_list[info[2]])

        # Output nll ordered ensemble

        write_ensemble_pdb(filename = 'nll_ordered_' + pdb_file_names[0].split('/')[-1].replace('.pdb','') + '_pensemble.pdb',
                       xrs_list = xrs_list_sorted_nll,
                       ens_pdb_hierarchy = ens_pdb_hierarchy
                       )


if __name__ == "__main__":
  ep = ensemble_probability()
  ep.run(args         = sys.argv[1:],
         command_name = 'ensemble_probability')


 *******************************************************************************


 *******************************************************************************
mmtbx/refinement/ensemble_refinement/ensemble_utils.py
from __future__ import absolute_import, division, print_function
import sys, math
from cctbx.array_family import flex
from libtbx import adopt_init_args
from mmtbx import utils
import scitbx.math
from cctbx import adptbx
from cctbx import geometry_restraints

import six
from six.moves import range

def selector(hierarchy, ignore_hd=True):
  ensemble_asc = hierarchy.atom_selection_cache()
  ensemble_str = 'not resname HOH'
  if ignore_hd:
    ensemble_str += ' and not element H'
  ensemble_sel = ensemble_asc.selection(ensemble_str)
  ensemble_hierarchy = hierarchy.select(ensemble_sel)
  return ensemble_hierarchy

def get_sites_carts(hierarchys, ignore_hd=True):
  sites_carts = []
  for n, hierarchy in enumerate(hierarchys):
    ensemble_hierarchy = selector(hierarchy, ignore_hd=ignore_hd)
    sites_carts.append(ensemble_hierarchy.atoms().extract_xyz())
  return sites_carts

def get_selected_hierarchys(hierarchys, ignore_hd=True):
  sh = []
  for n, hierarchy in enumerate(hierarchys):
    ensemble_hierarchy = selector(hierarchy, ignore_hd=ignore_hd)
    sh.append(ensemble_hierarchy)
  return sh

def ensemble_mean_hierarchy(hierarchys,
                            ignore_hd=True,
                            verbose=False,
                            out=None,
                            ):
  sites_carts = get_sites_carts(hierarchys, ignore_hd=ignore_hd)
  mean_sites_cart = sites_carts[0].deep_copy()
  for n, sites_cart in enumerate(sites_carts):
    if not n: continue
    mean_sites_cart += sites_cart
  mean_sites_cart = mean_sites_cart * float(1./len(sites_carts))
  if verbose:
    print('Mean Structure Stats', file=out)
    for i, sites_cart in enumerate(sites_carts):
      print('  RMS to mean %3d : %0.2f' % (i+1,
                                           mean_sites_cart.rms_difference(sites_cart)),
            file=out)
  mean_hierarchy = hierarchys[0].deep_copy()
  mean_hierarchy = selector(mean_hierarchy, ignore_hd=ignore_hd)
  mean_hierarchy.atoms().set_xyz(mean_sites_cart)
  return mean_hierarchy

def closest_to_mean(hierarchys, mean_hierarchy, ignore_hd=True, verbose=False):
  sites_carts = get_sites_carts(hierarchys, ignore_hd=ignore_hd)
  mean_sites_cart = mean_hierarchy.atoms().extract_xyz()
  min_sites_cart = None
  min_rms = 1e9
  min_index = None
  for i, sites_cart in enumerate(sites_carts):
    rms = mean_sites_cart.rms_difference(sites_cart)
    if rms<min_rms:
      min_rms=rms
      min_sites_cart = sites_cart
      min_index = i
  assert min_sites_cart
  close_hierarchy = hierarchys[0].deep_copy()
  close_hierarchy = selector(close_hierarchy, ignore_hd=ignore_hd)
  close_hierarchy.atoms().set_xyz(min_sites_cart)
  if verbose:
    print('Closest to Mean\n  %3d : %0.2f (rms)' % (min_index+1, min_rms))
  return close_hierarchy, min_index

def get_centroid_hierarchy(hierarchys, ignore_hd=True, verbose=False):
  sites_carts = get_sites_carts(hierarchys, ignore_hd=ignore_hd)
  rmss = {}
  for i, sites_cart1 in enumerate(sites_carts):
    for j, sites_cart2 in enumerate(sites_carts):
      if i==j: break
      rms = sites_cart2.rms_difference(sites_cart1)
      rmss.setdefault(i, {})
      rmss[i][j]=rms
      rmss.setdefault(j, {})
      rmss[j][i]=rms
  min_sites_cart = None
  min_rms = 1e9
  min_index = None
  max_sites_cart = None
  max_rms = -1e9
  max_index = None
  for key, item in rmss.items():
    s = sum(item.values())
    if s<min_rms:
      min_rms=s
      min_sites_cart = sites_carts[key]
      min_index = key
    if s>max_rms:
      max_rms=s
      max_sites_cart = sites_carts[key]
      max_index = key
  assert min_sites_cart
  centroid_hierarchy = hierarchys[0].deep_copy()
  centroid_hierarchy = selector(centroid_hierarchy, ignore_hd=ignore_hd)
  centroid_hierarchy.atoms().set_xyz(min_sites_cart)
  least_hierarchy = hierarchys[0].deep_copy()
  least_hierarchy = selector(least_hierarchy, ignore_hd=ignore_hd)
  least_hierarchy.atoms().set_xyz(min_sites_cart)
  if verbose:
    print('Centroid Structure\n  %3d : %0.2f (average rms)' % (min_index+1, min_rms/len(rmss)))
    print('Extreme Structure\n  %3d : %0.2f (average rms)' % (max_index+1, max_rms/len(rmss)))
  return centroid_hierarchy, least_hierarchy, min_index, max_index

def dist2(xyz1, xyz2):
  d2=0
  for i in range(3):
    d2+=(xyz2[i]-xyz1[i])**2
  return d2

def get_rmsf_B_factor_per_residue_per_atom(hierarchys,
                                           reference,
                                           ignore_hd=True,
                                           verbose=False):
  sites_carts = get_sites_carts(hierarchys, ignore_hd=ignore_hd)
  sel_hierarchys = get_selected_hierarchys(hierarchys, ignore_hd=ignore_hd)
  ref_sites_cart = reference.atoms().extract_xyz()
  diff = []
  tempFactor = {}
  RMSF = {}
  for j, atom in enumerate(reference.atoms()):
    diff.append(0.)
    for sites_cart in sites_carts:
      # differences between the states and reference_state for each atom.
      d2 = dist2(atom.xyz, sites_cart[j])
      diff[j] += d2
    diff[j] = math.sqrt(diff[j]/len(hierarchys))
    key_atom = sel_hierarchys[0].atoms()[j]
    key = ((key_atom.parent().parent().parent().id,
            key_atom.parent().parent().resseq,
            key_atom.parent().resname,
            key_atom.name
           ))
    tempFactor.setdefault(key, [])

    tempFactor[key].append(diff[j]*8*math.pi**2)

    key = ((key_atom.parent().parent().parent().id,
            key_atom.parent().parent().resseq,
            key_atom.parent().resname,
           ))
    RMSF.setdefault(key, [])
    RMSF[key].append(diff[j])

  for key, item in RMSF.items():
    RMSF[key] =  sum(item)/len(item)
  return tempFactor, RMSF, diff

class manager(object):
  def __init__(self,
               ensemble_obj):
    adopt_init_args(self, locals())

  def print_atom_statisitics(self,
                             selection_string = 'name CA',
                             selection_bool_array = None,
                             model = None):
    if model == None:
      model = self.ensemble_obj.model
    pdb_hierarchy = model.get_hierarchy()
    pdb_atoms = pdb_hierarchy.atoms()
    if selection_bool_array == None:
      selection_bool_array = pdb_hierarchy.atom_selection_cache().selection(selection_string)
    xrs = model.get_xray_structure().deep_copy_scatterers()
    xrs.convert_to_isotropic()
    b_iso_atoms = xrs.scatterers().extract_u_iso()/adptbx.b_as_u(1)
    q_atoms = xrs.scatterers().extract_occupancies()
    for i_seq, x in enumerate(selection_bool_array):
      if x:
        atom_info = pdb_atoms[i_seq].fetch_labels()
        if self.ensemble_obj.er_data.ke_protein_running == None:
          print(' {0:6} {1:6} {2:6} {3:6} {4:6d} {5:6.3f} {6:6.3f} '.format(
                   atom_info.name,
                   atom_info.resseq,
                   atom_info.resname,
                   atom_info.chain_id,
                   i_seq,
                   b_iso_atoms[i_seq],
                   q_atoms[i_seq]), file=self.ensemble_obj.log)
        else:
          print(' {0:6} {1:6} {2:6} {3:6} {4:6d} {5:6.3f} {6:6.3f} {7:6.3f}'.format(
                   atom_info.name,
                   atom_info.resseq,
                   atom_info.resname,
                   atom_info.chain_id,
                   i_seq,
                   b_iso_atoms[i_seq],
                   q_atoms[i_seq],
                   self.ensemble_obj.er_data.ke_protein_running[i_seq]), file=self.ensemble_obj.log)

  #Print KE stats for simulation/model validation
  def kinetic_energy_stats(self):
    if self.ensemble_obj is not None:
      if self.ensemble_obj.er_data.ke_protein_running is not None:
        utils.print_header(
          line ="Non-solvent KE Statistics | MC : "+str(self.ensemble_obj.macro_cycle),
          out  = self.ensemble_obj.log)
        ke_basic_stats = scitbx.math.basic_statistics(self.ensemble_obj.er_data.ke_protein_running)
        print('  {0:<11}  {1:>12} {2:>12} {3:>12} {4:>12} {5:>12} '.format(
                                    '','min','max','mean', 'sdev', 'skew'), file=self.ensemble_obj.log)
        print('  KE MC {0:<5}: {1:12.3f} {2:12.3f} {3:12.3f} {4:12.3f} {5:12.3f}'.format(
                                    self.ensemble_obj.macro_cycle,
                                    ke_basic_stats.min,
                                    ke_basic_stats.max,
                                    ke_basic_stats.mean,
                                    ke_basic_stats.biased_standard_deviation,
                                    ke_basic_stats.skew), file=self.ensemble_obj.log)
        ke_atom_number_tuple_list = []
        ke_list_histo = []
        for n, ke in enumerate(self.ensemble_obj.er_data.ke_protein_running):
          ke_atom_number_tuple_list.append( (n, ke) )
          ke_list_histo.append(ke)
        assert len(ke_atom_number_tuple_list) == len(self.ensemble_obj.er_data.ke_protein_running)
        sorted_by_ke_ke_atom_number_tuple_list = \
          sorted(ke_atom_number_tuple_list, key=lambda ke:ke[-1])
        ke_list_histo = sorted(ke_list_histo)
        #Lowest KE Atoms
        pdb_atoms = self.ensemble_obj.pdb_hierarchy().atoms()
        print("\nNon-solvent atoms lowest KE : ", file=self.ensemble_obj.log)
        print('  {0:3} : {1:>44} {2:>12} {3:>12}'.format(
                'rank',
                'KE',
                'dmean/sdev',
                '%cum freq'), file=self.ensemble_obj.log)
        low_five_percent = (len(ke_atom_number_tuple_list) * 0.05)
        cntr = 0
        lowest_range = min(25, int(0.5 * len(ke_atom_number_tuple_list) ) )
        while cntr < lowest_range:
          atom_info = pdb_atoms[sorted_by_ke_ke_atom_number_tuple_list[cntr][0]].fetch_labels()
          assert atom_info.i_seq == sorted_by_ke_ke_atom_number_tuple_list[cntr][0]
          print(' {0:5} : {1:6} {2:6} {3:6} {4:6} {5:6} {6:9.3f} {7:12.3f} {8:12.1f}'.format(
                cntr+1,
                sorted_by_ke_ke_atom_number_tuple_list[cntr][0],
                atom_info.name,
                atom_info.resname,
                atom_info.chain_id,
                atom_info.resseq,
                sorted_by_ke_ke_atom_number_tuple_list[cntr][1],
                (sorted_by_ke_ke_atom_number_tuple_list[cntr][1]-ke_basic_stats.mean)\
                  / ke_basic_stats.biased_standard_deviation,
                100 * (float(cntr)/float(len(ke_atom_number_tuple_list))) ), file=self.ensemble_obj.log)
          cntr+=1
        #Highest KE Atoms
        print("\nNon-solvent atoms highest KE : ", file=self.ensemble_obj.log)
        print('  {0:3} : {1:>44} {2:>12} {3:>12}'.format(
                'rank',
                'KE',
                'dmean/sdev',
                '%cum freq'), file=self.ensemble_obj.log)
        cntr = len(ke_atom_number_tuple_list) - min(25, int(0.5 * len(ke_atom_number_tuple_list) ) )
        while cntr < len(ke_atom_number_tuple_list):
          atom_info = pdb_atoms[sorted_by_ke_ke_atom_number_tuple_list[cntr][0]].fetch_labels()
          assert atom_info.i_seq == sorted_by_ke_ke_atom_number_tuple_list[cntr][0]
          print(' {0:5} : {1:6} {2:6} {3:6} {4:6} {5:6} {6:9.3f} {7:12.3f} {8:12.1f}'.format(
                cntr+1,
                sorted_by_ke_ke_atom_number_tuple_list[cntr][0],
                atom_info.name,
                atom_info.resname,
                atom_info.chain_id,
                atom_info.resseq,
                sorted_by_ke_ke_atom_number_tuple_list[cntr][1],
                (sorted_by_ke_ke_atom_number_tuple_list[cntr][1]-ke_basic_stats.mean)\
                  / ke_basic_stats.biased_standard_deviation,
                100 * (float(cntr)/float(len(ke_atom_number_tuple_list))) ), file=self.ensemble_obj.log)
          cntr+=1
        #XXX Add print stats by for <ke>/residue

        #Histogram
        bin_list, bin_range_list = self.bin_generator_equal_range(
            array          = ke_list_histo[:-int(0.1 * (len(ke_list_histo) ) )],
            number_of_bins = 50)
        bin_range_list[-1][1] = max(ke_list_histo)
        self.bivariate_histogram(
          bin_array      = ke_list_histo,
          value_array    = ke_list_histo,
          name           = 'KE Histogram',
          bin_list       = bin_list,
          bin_range_list = bin_range_list)
        print("|"+"-"*77+"|\n", file=self.ensemble_obj.log)

  def bin_generator_equal_range(self, array, number_of_bins = 10):
    array_max   = max(array)
    array_min   = min(array)
    array_range = array_max - array_min
    bin_range   = array_range / (number_of_bins)
    bin_list = []
    bin_range_list = []
    val = array_min
    for x in range(number_of_bins):
      bin_list.append(val)
      bin_range_list.append([val, val+bin_range])
      val += bin_range
    return bin_list, bin_range_list

  def bin_generator_equal_size(self, array, number_of_bins = 8, return_bin_ave = False):
    sorted_array = sorted(array)
    bin_size = int(len(sorted_array) / number_of_bins+1)
    bin_list = []
    bin_range_list = []
    bin_ave_calc = []
    bin_average_array = flex.double()
    for n, x in enumerate(sorted_array):
      bin_ave_calc.append(x)
      if n%bin_size==0:
        bin_list.append(x)
        if n > 1:
          bin_average_array.append( sum(bin_ave_calc)/len(bin_ave_calc) )
          bin_ave_calc = []
        if len(bin_list) == number_of_bins:
          bin_range_list.append([sorted_array[n], sorted_array[-1]])
        else:
          bin_range_list.append([sorted_array[n],sorted_array[n+bin_size-1]])
    bin_average_array.append( sum(bin_ave_calc)/len(bin_ave_calc) )
    if return_bin_ave:
      return bin_list, bin_range_list, bin_average_array
    else:
      return bin_list, bin_range_list

  def bivariate_histogram(self, bin_array,
                                value_array,
                                name,
                                bin_list,
                                bin_range_list,
                                verbose = True,
                                return_bin_ave = False):
    if verbose:
      print("\nBivariate histogram "+name+" MC: "+str(self.ensemble_obj.macro_cycle), file=self.ensemble_obj.log)
    assert len(value_array) == len(bin_array)
    binned_array = [[]*1 for i in range(len(bin_list))]
    assert len(bin_list) == len(binned_array)
    for x in range(len(bin_array)):
      for bin_int, bin_value in enumerate(bin_list):
        flag = False
        if bin_array[x] < bin_value:
          binned_array[bin_int-1].append( value_array[x] )
          flag = True
          break
      if not flag:
        binned_array[-1].append( value_array[x] )
    if return_bin_ave:
      return_bin_array = flex.double()
    if verbose:
      print("{0:>20} | {1:>6} {2:>10} {3:>10} {4:>10} {5:>10}"\
        .format('Bin Range',
                'Freq',
                'Mean',
                'Min',
                'Max',
                'StdDev'), file=self.ensemble_obj.log)
    for n,bin_array in enumerate(binned_array):
      if len(bin_array) > 0:
        bin_array = flex.double(bin_array)
        if return_bin_ave:
          return_bin_array.append( sum(bin_array)/len(bin_array) )
        if verbose:
          print("{0:3d} {1:7.2f} -{2:7.2f} | {3:6d} {4:10.3f} {5:10.3f} {6:10.3f} {7:10.3f}".format(n+1,
                     bin_range_list[n][0],
                     bin_range_list[n][1],
                     len(bin_array),
                     sum(bin_array)/len(bin_array),
                     min(bin_array),
                     max(bin_array),
                     scitbx.math.basic_statistics(bin_array).biased_standard_deviation), file=self.ensemble_obj.log)
      else:
        if verbose:
          print("{0:3d} {1:7.2f} -{2:7.2f} | {3:6d} {4:10.3f} {5:10.3f} {6:10.3f} {7:10.3f}".format(n+1,
                     bin_range_list[n][0],
                     bin_range_list[n][1],
                     len(bin_array),
                     0,
                     0,
                     0,
                     0), file=self.ensemble_obj.log)
    if return_bin_ave:
      return return_bin_array

  def ensemble_reduction(self,
                         rfree_tolerance = 0.0025):
    #Reduces number of models to minimum required to reproduce Rfree
    utils.print_header("Ensemble reducer", out = self.ensemble_obj.log)
    self.ensemble_obj.show_overall(message        = "Full simulation fmodel final",
                             fmodel_running = False)
    final_rfree = self.ensemble_obj.fmodel_total.r_free()
    final_rwork = self.ensemble_obj.fmodel_total.r_work()

    # XXX no b_iso - how to apply this???
#    print >> self.ensemble_obj.log, "\nApply B_iso to all model in ensemble"
#    shift_b_iso  = self.ensemble_obj.fmodel_total.b_iso()
#    print >> self.ensemble_obj.log, 'Shift B_iso : {0:8.3f}'.format(shift_b_iso)
#    for x in self.ensemble_obj.er_data.xray_structures:
#      x.shift_us(b_shift = shift_b_iso)

    total_number_xrs = len(self.ensemble_obj.er_data.xray_structures)
    print("\nReduce ensemble with equal distribution though trajectory :", file=self.ensemble_obj.log)
    print("Rfree tolerance (%) : ", rfree_tolerance * 100, file=self.ensemble_obj.log)
    print('\n {0:>12} {1:>8} {2:>8} {3:>8}'\
        .format('Num','Rwork','Rfree','k1'), file=self.ensemble_obj.log)
    target_rfree = final_rfree
    final_div    = None
    for div_int in [1,2,3,4,5,6,7,8,9,10,12,14,16,18,20,25,30,35,40,45,50,60,70,80,90,100,200,300,400,500,600,700,800,900,1000,2000,3000,4000,5000]:
      if div_int <= total_number_xrs:
        self.fmodel_ens = self.ensemble_obj.fmodel_total.deep_copy()
        cntr = 0.0
        fcalc_total = None
        fmask_total = None

  #      self.fmodel_ens.update(k_sols  = self.ensemble_obj.fmodel_total.k_sols(),
  #                             b_sol   = self.ensemble_obj.fmodel_total.b_sol(),
  #                             b_cart  = self.ensemble_obj.fmodel_total.b_cart() )

        for x in range(total_number_xrs):
          if x%int(div_int) == 0:
            #Apply back trace of Biso here...
            self.fmodel_ens.update_xray_structure(
              xray_structure      = self.ensemble_obj.er_data.xray_structures[x],
              update_f_calc       = True,
              update_f_mask       = True,
              force_update_f_mask = True)
            if fcalc_total == None:
              fcalc_total = self.fmodel_ens.f_calc().data().deep_copy()
              fmask_total = self.fmodel_ens.f_masks()[0].data().deep_copy()
              cntr = 1
            else:
              fcalc_total += self.fmodel_ens.f_calc().data().deep_copy()
              fmask_total += self.fmodel_ens.f_masks()[0].data().deep_copy()
              cntr += 1
          if x == total_number_xrs-1:
            self.fmodel_ens.update(
              f_calc = self.ensemble_obj.copy_ma.array(data = (fcalc_total / cntr)),
              f_mask = self.ensemble_obj.copy_ma.array(data = (fmask_total / cntr)) )
            self.fmodel_ens.update_all_scales(
              log    = self.ensemble_obj.log,
              remove_outliers=False,
              params = self.ensemble_obj.bsp)
            if cntr < 4:
              break
            print("Ens: {0:8d} {1:8.3f} {2:8.3f} {3:8.3f}"\
              .format(cntr,
                      self.fmodel_ens.r_work(),
                      self.fmodel_ens.r_free(),
                      self.fmodel_ens.scale_k1()
                      ), file=self.ensemble_obj.log)
            if self.fmodel_ens.r_free() < (target_rfree + rfree_tolerance):
              final_div    = div_int
              final_f_calc = self.ensemble_obj.copy_ma.array(data = (fcalc_total / cntr))
              final_f_mask = self.ensemble_obj.copy_ma.array(data = (fmask_total / cntr))
              if self.fmodel_ens.r_free() < target_rfree:
                target_rfree = self.fmodel_ens.r_free()

    if final_div == None:
      print("Warning pdb ensemble does not contain sufficent models and missrepresents simulation.  Simulation Rfree: {0:2.3f} %".format(100*(final_rfree)), file=self.ensemble_obj.log)
    else:
      #Update fmodel_total
      self.ensemble_obj.fmodel_total.update(f_calc = final_f_calc,
                                      f_mask = final_f_mask)
      self.ensemble_obj.fmodel_total.update_all_scales(
        log    = self.ensemble_obj.log,
        remove_outliers=False,
        params = self.ensemble_obj.bsp)
      #Parse arrays for output PDB
      copy_ed_data_xray_structures = []
      copy_pdb_hierarchys = []
      copy_ed_data_ke_pdb = []
      for x in range(len(self.ensemble_obj.er_data.xray_structures)):
        if x%int(final_div) == 0:
          copy_ed_data_xray_structures.append(self.ensemble_obj.er_data.xray_structures[x])
          copy_pdb_hierarchys.append(self.ensemble_obj.er_data.pdb_hierarchys[x])
          copy_ed_data_ke_pdb.append(self.ensemble_obj.er_data.ke_pdb[x])
      self.ensemble_obj.er_data.xray_structures          = copy_ed_data_xray_structures
      self.ensemble_obj.er_data.pdb_hierarchys           = copy_pdb_hierarchys
      self.ensemble_obj.er_data.ke_pdb                   = copy_ed_data_ke_pdb
      print("Final pdb ensemble contains {0:3d} models".format(len(self.ensemble_obj.er_data.xray_structures)), file=self.ensemble_obj.log)
      assert len(self.ensemble_obj.er_data.xray_structures) == len(self.ensemble_obj.er_data.pdb_hierarchys)
      assert len(self.ensemble_obj.er_data.xray_structures) == len(self.ensemble_obj.er_data.ke_pdb)

      print("|"+"-"*77+"|\n", file=self.ensemble_obj.log)

  def ensemble_rmsf_stats( self,
                           ensemble_hierarchys,
                           transfer_b_factors=True,
                           ignore_hd = True,
                           max_print=10,
                           verbose = False,
                           out = None,
                           ):
    if (out is None): out = sys.stdout
    if verbose:
      utils.print_header("Ensemble mean and centroid geometry statistics", out = out)
    ensemble_size = len(ensemble_hierarchys)
    self.mean_hierarchy = ensemble_mean_hierarchy( ensemble_hierarchys,
                                                   ignore_hd=ignore_hd,
                                                   verbose=verbose,
                                                   )
    close_hierarchy, self.closest_to_mean_index = closest_to_mean(
      ensemble_hierarchys,
      self.mean_hierarchy,
      ignore_hd=ignore_hd,
      verbose=verbose,
      )
    self.centroid_hierarchy, least_hierarchy, self.centroid_index, self.least_index = \
      get_centroid_hierarchy( ensemble_hierarchys,
                              ignore_hd=ignore_hd,
                              verbose=verbose,
                              )
    self.tempFactor, self.per_residue, self.per_atom = \
      get_rmsf_B_factor_per_residue_per_atom(
        ensemble_hierarchys,
        self.centroid_hierarchy,
        # mean_sites_cart,
        ignore_hd=ignore_hd,
        verbose=verbose,
        )

    if verbose:
      print('Per residue rmsf', file=out)
      for i, (key, item) in enumerate(self.per_residue.items()):
        print('  %5d : %s %0.2f' % (i,key,item), file=out)
        if i>=max_print: break

      print('B-factor', file=out)
      for i, (key, item) in enumerate(self.tempFactor.items()):
        print('  %5d : %s %7.2f' % (i,key,item[0]), file=out)
        if i>=max_print: break

      print('Per atom rmsf', file=out)
      for i, atom in enumerate(ensemble_hierarchys[0].atoms()):
        print('  %5d : %s %0.2f' % (i, atom.quote(), self.per_atom[i]), file=out)
        if i>=max_print: break

    if transfer_b_factors:
      atoms = self.centroid_hierarchy.atoms()
      occupancies = atoms.extract_occ()
      occupancies *= len(ensemble_hierarchys)
      atoms.set_occ(occupancies)
      for i, (key, item) in enumerate(self.tempFactor.items()):
        atom = atoms[i]
        atom.b = item[0]

    return self

  def write_mean_hierarchy(self, filename, crystal_symmetry):
    if not hasattr(self, 'mean_hierarchy'):
      assert 0, 'need to run ensemble_rmsf_stats'
    self.mean_hierarchy.write_pdb_file(
      filename,
      crystal_symmetry = crystal_symmetry,
    )

  def write_centroid_hierarchy(self, filename, crystal_symmetry):
    if not hasattr(self, 'centroid_hierarchy'):
      assert 0, 'need to run ensemble_rmsf_stats'
    self.centroid_hierarchy.write_pdb_file(
      filename,
      crystal_symmetry = crystal_symmetry,
    )

  def ensemble_mean_geometry_stats(self,
                                   restraints_manager,
                                   xray_structure,
                                   ensemble_xray_structures,
                                   ignore_hd = True,
                                   verbose = False,
                                   out = None,
                                   return_pdb_string = False):
    if (out is None): out = sys.stdout
    if verbose:
      utils.print_header("Ensemble mean geometry statistics", out = out)
    ensemble_size = len(ensemble_xray_structures)
    print("Ensemble size : ", ensemble_size, file=out)

    # Dictionaries to store deltas
    ensemble_bond_deltas = {}
    ensemble_angle_deltas = {}
    ensemble_chirality_deltas = {}
    ensemble_planarity_deltas = {}
    ensemble_dihedral_deltas = {}

    # List to store rmsd of each model
    structures_bond_rmsd = flex.double()
    structures_angle_rmsd = flex.double()
    structures_chirality_rmsd = flex.double()
    structures_planarity_rmsd = flex.double()
    structures_dihedral_rmsd = flex.double()

    # Remove water and hd atoms from global restraints manager
    selection = flex.bool()
    for sc in xray_structure.scatterers():
      if sc.label.find('HOH') > -1:
        selection.append(True)
      else:
        selection.append(False)
    if ignore_hd:
      hd_selection = xray_structure.hd_selection()
      assert hd_selection.size() == selection.size()
      for n in range(hd_selection.size()):
        if hd_selection[n] or selection[n]:
          selection[n] = True
    restraints_manager = restraints_manager.select(selection = ~selection)

    # Get all deltas
    for n, structure in enumerate(ensemble_xray_structures):
      if verbose:
        print("\nModel : ", n+1, file=out)
      sites_cart = structure.sites_cart()
      # Remove water and hd atoms from individual structures sites cart
      selection = flex.bool()
      for sc in structure.scatterers():
        if sc.label.find('HOH') > -1:
          selection.append(True)
        else:
          selection.append(False)
      if ignore_hd:
        hd_selection = structure.hd_selection()
        assert hd_selection.size() == selection.size()
        for n in range(hd_selection.size()):
          if hd_selection[n] or selection[n]:
            selection[n] = True
      sites_cart = sites_cart.select(~selection)
      assert sites_cart is not None
      site_labels = None
      energies_sites = restraints_manager.energies_sites(
          sites_cart        = sites_cart,
          compute_gradients = False)

      # Rmsd of individual model
      bond_rmsd = energies_sites.geometry.bond_deviations()[2]
      angle_rmsd = energies_sites.geometry.angle_deviations()[2]
      chirality_rmsd = energies_sites.geometry.chirality_deviations()[2]
      planarity_rmsd = energies_sites.geometry.planarity_deviations()[2]
      dihedral_rmsd = energies_sites.geometry.dihedral_deviations()[2]

      structures_bond_rmsd.append(bond_rmsd)
      structures_angle_rmsd.append(angle_rmsd)
      structures_chirality_rmsd.append(chirality_rmsd)
      structures_planarity_rmsd.append(planarity_rmsd)
      structures_dihedral_rmsd.append(dihedral_rmsd)

      if verbose:
        print("  Model RMSD", file=out)
        print("    bond      : %.6g" % bond_rmsd, file=out)
        print("    angle     : %.6g" % angle_rmsd, file=out)
        print("    chirality : %.6g" % chirality_rmsd, file=out)
        print("    planarity : %.6g" % planarity_rmsd, file=out)
        print("    dihedral  : %.6g" % dihedral_rmsd, file=out)

      # Bond
      pair_proxies = restraints_manager.geometry.pair_proxies(flags=None, sites_cart=sites_cart)
      assert pair_proxies is not None
      if verbose:
        pair_proxies.bond_proxies.show_histogram_of_deltas(
          sites_cart  = sites_cart,
          n_slots     = 10,
          f           = out)
      for proxy in pair_proxies.bond_proxies.simple:
        bond_simple_proxy = geometry_restraints.bond(
            sites_cart = sites_cart,
            proxy      = proxy)
        if proxy.i_seqs in ensemble_bond_deltas:
          ensemble_bond_deltas[proxy.i_seqs][0]+=bond_simple_proxy.delta
          ensemble_bond_deltas[proxy.i_seqs][1]+=1
        else:
          ensemble_bond_deltas[proxy.i_seqs] = [bond_simple_proxy.delta, 1]
        if verbose:
          print("bond simple :", proxy.i_seqs, file=out)
          print("  distance_ideal : %.6g" % proxy.distance_ideal, file=out)
          print("  distance_model : %.6g" % bond_simple_proxy.distance_model, file=out)
          print("  detla          : %.6g" % bond_simple_proxy.delta, file=out)
      if (pair_proxies.bond_proxies.asu.size() > 0):
        asu_mappings = pair_proxies.bond_proxies.asu_mappings()
        for proxy in pair_proxies.bond_proxies.asu:
          rt_mx = asu_mappings.get_rt_mx_ji(pair=proxy)
          bond_asu_proxy = geometry_restraints.bond(
              sites_cart   = sites_cart,
              asu_mappings = asu_mappings,
              proxy        = proxy)
          proxy_i_seqs = (proxy.i_seq, proxy.j_seq)
          if proxy_i_seqs in ensemble_bond_deltas:
            ensemble_bond_deltas[proxy_i_seqs][0]+=bond_asu_proxy.delta
            ensemble_bond_deltas[proxy_i_seqs][1]+=1
          else:
            ensemble_bond_deltas[proxy_i_seqs] = [bond_asu_proxy.delta, 1]
          if verbose:
            print("bond asu :", (proxy.i_seq, proxy.j_seq), rt_mx, file=out)
            print("  distance_ideal : %.6g" % proxy.distance_ideal, file=out)
            print("  distance_model : %.6g" % bond_asu_proxy.distance_model, file=out)
            print("  delta          : %.6g" % bond_asu_proxy.delta, file=out)

      # Angle
      if verbose:
        restraints_manager.geometry.angle_proxies.show_histogram_of_deltas(
            sites_cart  = sites_cart,
            n_slots     = 10,
            f           = out)
      for proxy in restraints_manager.geometry.angle_proxies:
        angle_proxy = geometry_restraints.angle(
            sites_cart = sites_cart,
            proxy      = proxy)
        if proxy.i_seqs in ensemble_angle_deltas:
          ensemble_angle_deltas[proxy.i_seqs][0]+=angle_proxy.delta
          ensemble_angle_deltas[proxy.i_seqs][1]+=1
        else:
          ensemble_angle_deltas[proxy.i_seqs] = [angle_proxy.delta, 1]
        if verbose:
          print("angle : ", proxy.i_seqs, file=out)
          print("  angle_ideal   : %.6g" % proxy.angle_ideal, file=out)
          print("  angle_model   : %.6g" % angle_proxy.angle_model, file=out)
          print("  delta         : %.6g" % angle_proxy.delta, file=out)

      # Chirality
      if verbose:
        restraints_manager.geometry.chirality_proxies.show_histogram_of_deltas(
            sites_cart  = sites_cart,
            n_slots     = 10,
            f           = out)
      for proxy in restraints_manager.geometry.chirality_proxies:
        chirality_proxy = geometry_restraints.chirality(
            sites_cart = sites_cart,
            proxy      = proxy)
        if proxy.i_seqs in ensemble_chirality_deltas:
          ensemble_chirality_deltas[proxy.i_seqs][0]+=chirality_proxy.delta
          ensemble_chirality_deltas[proxy.i_seqs][1]+=1
        else:
          ensemble_chirality_deltas[proxy.i_seqs] = [chirality_proxy.delta, 1]
        if verbose:
          print("chirality : ", proxy.i_seqs, file=out)
          print("  chirality_ideal : %.6g" % proxy.volume_ideal, file=out)
          print("  chirality_model : %.6g" % chirality_proxy.volume_model, file=out)
          print("  chirality       : %.6g" % chirality_proxy.delta, file=out)

      # Planarity
      for proxy in restraints_manager.geometry.planarity_proxies:
        planarity_proxy = geometry_restraints.planarity(
            sites_cart = sites_cart,
            proxy      = proxy)
        proxy_i_seqs = []
        for i_seq in proxy.i_seqs:
          proxy_i_seqs.append(i_seq)
        proxy_i_seqs = tuple(proxy_i_seqs)
        if proxy_i_seqs in ensemble_planarity_deltas:
          ensemble_planarity_deltas[proxy_i_seqs][0]+=planarity_proxy.rms_deltas()
          ensemble_planarity_deltas[proxy_i_seqs][1]+=1
        else:
          ensemble_planarity_deltas[proxy_i_seqs] = [planarity_proxy.rms_deltas(), 1]
        if verbose:
          print("planarity : ", proxy_i_seqs, file=out)
          print("  planarity rms_deltas : %.6g" % planarity_proxy.rms_deltas(), file=out)

      # Dihedral
      if verbose:
        restraints_manager.geometry.dihedral_proxies.show_histogram_of_deltas(
            sites_cart  = sites_cart,
            n_slots     = 10,
            f           = out)
      for proxy in restraints_manager.geometry.dihedral_proxies:
        dihedral_proxy = geometry_restraints.dihedral(
            sites_cart = sites_cart,
            proxy      = proxy)
        if proxy.i_seqs in ensemble_dihedral_deltas:
          ensemble_dihedral_deltas[proxy.i_seqs][0]+=dihedral_proxy.delta
          ensemble_dihedral_deltas[proxy.i_seqs][1]+=1
        else:
          ensemble_dihedral_deltas[proxy.i_seqs] = [dihedral_proxy.delta, 1]
        if verbose:
          print("dihedral : ", proxy.i_seqs, file=out)
          print("  dihedral_ideal  : %.6g" % proxy.angle_ideal, file=out)
          print("  periodicity     : %.6g" % proxy.periodicity, file=out)
          print("  dihedral_model  : %.6g" % dihedral_proxy.angle_model, file=out)
          print("  delta           : %.6g" % dihedral_proxy.delta, file=out)

    # Calculate RMSDs for ensemble model
    # Bond
    mean_bond_delta = flex.double()
    for proxy, info in six.iteritems(ensemble_bond_deltas):
      # assert info[1] == ensemble_size
      if info[1]!=ensemble_size:
        print('skipping bond RMSD calns of ensemble %s' % info, file=out)
        continue
      mean_delta = info[0] / info[1]
      mean_bond_delta.append(mean_delta)
    bond_delta_sq = mean_bond_delta * mean_bond_delta
    ensemble_bond_rmsd = math.sqrt(flex.mean_default(bond_delta_sq, 0))

    # Angle
    mean_angle_delta = flex.double()
    for proxy, info in six.iteritems(ensemble_angle_deltas):
      assert info[1] == ensemble_size
      mean_delta = info[0] / info[1]
      mean_angle_delta.append(mean_delta)
    angle_delta_sq = mean_angle_delta * mean_angle_delta
    ensemble_angle_rmsd = math.sqrt(flex.mean_default(angle_delta_sq, 0))

    # Chirality
    mean_chirality_delta = flex.double()
    for proxy, info in six.iteritems(ensemble_chirality_deltas):
      assert info[1] == ensemble_size
      mean_delta = info[0] / info[1]
      mean_chirality_delta.append(mean_delta)
    chirality_delta_sq = mean_chirality_delta * mean_chirality_delta
    ensemble_chirality_rmsd = math.sqrt(flex.mean_default(chirality_delta_sq, 0))

    # Planarity
    mean_planarity_delta = flex.double()
    for proxy, info in six.iteritems(ensemble_planarity_deltas):
      assert info[1] == ensemble_size
      mean_delta = info[0] / info[1]
      mean_planarity_delta.append(mean_delta)
    planarity_delta_sq = mean_planarity_delta * mean_planarity_delta
    ensemble_planarity_rmsd = math.sqrt(flex.mean_default(planarity_delta_sq, 0))

    # Dihedral
    mean_dihedral_delta = flex.double()
    for proxy, info in six.iteritems(ensemble_dihedral_deltas):
      assert info[1] == ensemble_size
      mean_delta = info[0] / info[1]
      mean_dihedral_delta.append(mean_delta)
    dihedral_delta_sq = mean_dihedral_delta * mean_dihedral_delta
    ensemble_dihedral_rmsd = math.sqrt(flex.mean_default(dihedral_delta_sq, 0))

    # Calculate <structure rmsd>
    assert ensemble_size == structures_bond_rmsd
    assert ensemble_size == structures_angle_rmsd
    assert ensemble_size == structures_chirality_rmsd
    assert ensemble_size == structures_planarity_rmsd
    assert ensemble_size == structures_dihedral_rmsd
    structure_bond_rmsd_mean = structures_bond_rmsd.min_max_mean().mean
    structure_angle_rmsd_mean = structures_angle_rmsd.min_max_mean().mean
    structure_chirality_rmsd_mean = structures_chirality_rmsd.min_max_mean().mean
    structure_planarity_rmsd_mean = structures_planarity_rmsd.min_max_mean().mean
    structure_dihedral_rmsd_mean = structures_dihedral_rmsd.min_max_mean().mean

    # Show summary
    utils.print_header("Ensemble RMSD summary", out = out)
    print("  RMSD (mean delta per restraint)", file=out)
    print("    bond      : %.6g" % ensemble_bond_rmsd, file=out)
    print("    angle     : %.6g" % ensemble_angle_rmsd, file=out)
    print("    chirality : %.6g" % ensemble_chirality_rmsd, file=out)
    print("    planarity : %.6g" % ensemble_planarity_rmsd, file=out)
    print("    dihedral  : %.6g" % ensemble_dihedral_rmsd, file=out)
    print("  RMSD (mean RMSD per structure)", file=out)
    print("    bond      : %.6g" % structure_bond_rmsd_mean, file=out)
    print("    angle     : %.6g" % structure_angle_rmsd_mean, file=out)
    print("    chirality : %.6g" % structure_chirality_rmsd_mean, file=out)
    print("    planarity : %.6g" % structure_planarity_rmsd_mean, file=out)
    print("    dihedral  : %.6g" % structure_dihedral_rmsd_mean, file=out)
    if ignore_hd:
      print("\n  Calculated excluding H/D", file=out)
    else:
      print("\n  Calculated including H/D", file=out)

    if return_pdb_string:
      ens_geo_pdb_string  = "REMARK   3"
      ens_geo_pdb_string += "\nREMARK   3  NUMBER STRUCTURES IN ENSEMBLE : {0:5d}".format(ensemble_size)
      if ignore_hd:
        ens_geo_pdb_string += "\nREMARK   3  RMS DEVIATIONS FROM IDEAL VALUES (EXCLUDING H/D)"
      else:
        ens_geo_pdb_string += "\nREMARK   3  RMS DEVIATIONS FROM IDEAL VALUES (INCLUDING H/D)"
      ens_geo_pdb_string += "\nREMARK   3  RMSD (MEAN DELTA PER RESTRAINT)"
      ens_geo_pdb_string += "\nREMARK   3    BOND      : {0:5.3f}".format(ensemble_bond_rmsd)
      ens_geo_pdb_string += "\nREMARK   3    ANGLE     : {0:5.3f}".format(ensemble_angle_rmsd)
      ens_geo_pdb_string += "\nREMARK   3    CHIRALITY : {0:5.3f}".format(ensemble_chirality_rmsd)
      ens_geo_pdb_string += "\nREMARK   3    PLANARITY : {0:5.3f}".format(ensemble_planarity_rmsd)
      ens_geo_pdb_string += "\nREMARK   3    DIHEDRAL  : {0:5.2f}".format(ensemble_dihedral_rmsd)
      ens_geo_pdb_string += "\nREMARK   3  RMSD (MEAN RMSD PER STRUCTURE)"
      ens_geo_pdb_string += "\nREMARK   3    BOND      : {0:5.3f}".format(structure_bond_rmsd_mean)
      ens_geo_pdb_string += "\nREMARK   3    ANGLE     : {0:5.3f}".format(structure_angle_rmsd_mean)
      ens_geo_pdb_string += "\nREMARK   3    CHIRALITY : {0:5.3f}".format(structure_chirality_rmsd_mean)
      ens_geo_pdb_string += "\nREMARK   3    PLANARITY : {0:5.3f}".format(structure_planarity_rmsd_mean)
      ens_geo_pdb_string += "\nREMARK   3    DIHEDRAL  : {0:5.2f}".format(structure_dihedral_rmsd_mean)
      ens_geo_pdb_string += "\nREMARK   3"
      return ens_geo_pdb_string

if __name__ == '__main__':
  from iotbx import pdb

  def get_pdb_hierarchies(pdb_file_names):
    pdb_hierarchys = []
    if len(pdb_file_names)==1:
      ensemble_filename = pdb_file_names[0]
      pdb_inp = pdb.input(ensemble_filename)
      pdb_hierarchy = pdb_inp.construct_hierarchy()
      for i, model in enumerate(pdb_hierarchy.models()):
        pdb_hierarchys.append(pdb_hierarchy.deep_copy())
        for j in range(len(pdb_hierarchy.models())-1,-1,-1):
          if j!=i:
            pdb_hierarchys[-1].remove_model(j)
        # pdb_hierarchys[-1].write_pdb_file('test_%s.pdb' % i)
    else:
      for i, pdb_file_name in enumerate(pdb_file_names):
        print(i,pdb_file_name)
        pdb_inp = pdb.input(pdb_file_name)
        pdb_hierarchy = pdb_inp.construct_hierarchy()
        pdb_hierarchys.append(pdb_hierarchy)
    return pdb_inp, pdb_hierarchys

  erm = manager(None)
  print('Ensemble Refinement Manager')
  ensemble_filenames = sys.argv[1:]
  print('ensemble_filename', ensemble_filenames)
  pdb_input, pdb_hierarchys = get_pdb_hierarchies(ensemble_filenames)
  print('Number of models : %s' % (len(pdb_hierarchys)))
  erm.ensemble_rmsf_stats(pdb_hierarchys)
  erm.write_centroid_hierarchy('centroid.pdb', pdb_input.crystal_symmetry())


 *******************************************************************************


 *******************************************************************************
mmtbx/refinement/ensemble_refinement/scripts/ens_pdb.py
from __future__ import absolute_import, division, print_function
# PDB manipulation tools for ensemble models
# Tom Burnley

import sys, re, pickle
from six.moves import input

def remove_HOH(start_pdb, noHOH_pdb):

  print("Removing HOH...")

  openpdb = open(start_pdb,"r")
  noHOHpdb = open(noHOH_pdb, 'w')
  find_HOH = re.compile('HOH')

  for line in openpdb:
    if not find_HOH.search(line):
      noHOHpdb.write(line)

def find_number_model(open_pdb):
  find_MODEL = re.compile('MODEL')
  max_model_number = 0
  for line in open_pdb:
    if find_MODEL.match(line):
      model_number = int(line.split()[-1])
      if model_number > max_model_number:
        max_model_number = model_number

  print("Number of models in PDB file : ", max_model_number)
  return max_model_number

def find_header(open_pdb, num_pdb):
  print("Getting header info...................")
  find_CRYST1 = re.compile('CRYST')
  find_SCALE = re.compile('SCALE')
  for line in open_pdb:
    if find_CRYST1.search(line):
      num_pdb.write(line)
    if find_SCALE.search(line):
      num_pdb.write(line)

def model_parse(open_pdb, num_pdb, model_num, new_model_num, start_pdb):
  model_num = str(model_num)
  find_MODEL = re.compile('MODEL')
  find_ENDMDL = re.compile('ENDMDL')
  open_pdb = open(start_pdb,"r")
  for line in open_pdb:
    if find_MODEL.search(line):
      items = line.split()
      if items[1] == model_num:
        newN = str(new_model_num)
        if new_model_num < 10:
          new_line = str('MODEL        '+ newN+'\n')
        elif new_model_num < 100:
          new_line = str('MODEL       '+ newN+'\n')
        elif new_model_num < 1000:
          new_line = str('MODEL      '+ newN+'\n')
        num_pdb.write(new_line)
        for line in open_pdb:
          if find_ENDMDL.search(line):
            num_pdb.write(line)
            return
          else:
            num_pdb.write(line)

def remove_specific_HOH(open_pdb,open_water_list,wat_pdb):
  water_list = []
  for wat_num in open_water_list:
    x = wat_num.split()
    water_list.append(x[0])

  find_HOH = re.compile('HOH')

  for line in open_pdb:
    if not find_HOH.search(line):
      wat_pdb.write(line)
    else:
      linesplit = line.split()
      if (len(linesplit) > 5):
        if (linesplit[4] in water_list) or (linesplit[5] in water_list):
          print(linesplit)
          wat_pdb.write(line)

def parse_bfactor_occ_infomation(open_pdb, start_pdb):
  print("Parsing B and Occ information")
  find_MODEL = re.compile('MODEL')
  find_ENDMDL = re.compile('ENDMDL')
  #
  b_total_array = []
  b_model_array = []
  q_total_array = []
  q_model_array = []
  for line in open_pdb:
    if find_MODEL.search(line):
      if len(b_model_array) > 0:
        b_total_array.append(b_model_array)
        b_model_array=[]
        q_total_array.append(q_model_array)
        q_model_array=[]
    items = line.split()
    if items[0] == 'ATOM' or items[0] == 'HETATM':
      b_model_array.append(float(items[-3]))
      q_model_array.append(float(items[-4]))
  #Save last model b's
  b_total_array.append(b_model_array)
  q_total_array.append(q_model_array)
  b_and_q={'b_array' : b_total_array, 'q_array' : q_total_array}
  #Pickle b_and_q information
#  pickle_name = start_pdb+"_b_q_pickle.pkl"
#  pickle_jar = open(pickle_name, 'wb')
#  pickle.dump(b_and_q, pickle_jar)
#  pickle_jar.close()
  print(b_and_q['b_array'])

def parse_specific_pdb(open_pdb, start_pdb, num_pdb, last_model, new_model_num=1):
  model_num = str(last_model)
  find_MODEL = re.compile('MODEL')
  find_ENDMDL = re.compile('ENDMDL')
  open_pdb = open(start_pdb,"r")
  for line in open_pdb:
    if find_MODEL.search(line):
      items = line.split()
      if items[1] == model_num:
        newN = str(new_model_num)
        if new_model_num < 10:
          new_line = str('MODEL        '+ newN+'\n')
        elif new_model_num < 100:
          new_line = str('MODEL       '+ newN+'\n')
        elif new_model_num < 1000:
          new_line = str('MODEL      '+ newN+'\n')

        num_pdb.write(new_line)
        for line in open_pdb:
          if find_ENDMDL.search(line):
            num_pdb.write(line)
            return
          else:
            num_pdb.write(line)

def col_swap(open_pdb, open_second_pdb, get_col, replace_col, new_pdb):
  if get_col == 'q':
    get_range = [54,60]
  else:
    get_range = [60,66]

  if replace_col == 'q':
    replace_range = [54,60]
  else:
    replace_range = [60,66]

  get_col_info=[]
  for line in open_second_pdb:
    if len(line) > 66:
      get_col_info.append(line[get_range[0]:get_range[1]])

  n = 0
  for line in open_pdb:
    if len(line) > 66:
      new_pdb.write((line[0:(replace_range[0])] + get_col_info[n] + line[(replace_range[1]):]))
      n+=1
    else:
      new_pdb.write(line)

  print("Replaced : ",  replace_col, " with : ", get_col)

def remove_anisou(start_pdb, fin_pdb):
  print("Removing ANISOU...")
  openpdb = open(start_pdb,"r")
  finpdb = open(fin_pdb, 'w')
  find_ANISOU = re.compile('ANISOU')

  for line in openpdb:
    if not find_ANISOU.search(line):
      finpdb.write(line)

def phxgro_to_phx(start_pdb, fin_pdb):
  openpdb = open(start_pdb,"r")
  finpdb = open(fin_pdb, 'w')

  #Change res names
  residue_to_change = {'CY2':'CYS', 'HIB':'HIS', 'LYH':'LYS', 'TRY':'TRP'}

  for line in openpdb:
    if len(line) > 66:
      residue_name =  line[17:20]
      if residue_name in residue_to_change:
        line = line[0:17] + residue_to_change[residue_name] + line[20:]
      if residue_name == 'HOH':
        water_atom_name = line[13:16]
        new_w_a_n = water_atom_name.replace('W', "")
        new_w_a_n = new_w_a_n + " "
        line = line[0:13] + new_w_a_n + line[16:]

    finpdb.write(line)

def phx_to_phxgro(start_pdb, fin_pdb):
  openpdb = open(start_pdb,"r")
  finpdb = open(fin_pdb, 'w')

  #Change res names
  residue_to_change = {'CYS':'CY2', 'HIS':'HIB', 'LYS':'LYH', 'TRP':'TRY'}

  for line in openpdb:
    if len(line) > 66:
      residue_name =  line[17:20]
      if residue_name in residue_to_change:
        line = line[0:17] + residue_to_change[residue_name] + line[20:]
      if residue_name == 'HOH':
        water_atom_name = line[13:16]
        if water_atom_name == "O  ":
          new_w_a_n = "OW "
        if water_atom_name == "H1 ":
          new_w_a_n = "HW1"
        if water_atom_name == "H2 ":
          new_w_a_n = "HW2"
        line = line[0:13] + new_w_a_n + line[16:]

    finpdb.write(line)

def extra_conformation(start_pdb, fin_pdb):
  open_pdb = open(start_pdb,"r")
  fin_pdb = open(fin_pdb, 'w')

  find_ATOM = re.compile('ATOM')

  for line in open_pdb:
    if len(line) > 66 and find_ATOM.search(line):
      print(line)
      new_lineA = line[0:16] + 'A' + line[17:56] + '0.50' + line[60:]
      new_lineB = line[0:16] + 'B' + line[17:56] + '0.50' + line[60:]
      print(new_lineA)
      print(new_lineB)
      fin_pdb.write(new_lineA)
      fin_pdb.write(new_lineB)
    else:
      fin_pdb.write(line)
def main():

  print("\n\n\
=========================== Ensemble PDB Display Tools =========================")
  if len(sys.argv) < 2:
    print("\n\nSpecify pdb file at command line:\n")
    print("    python ens_pdb.py my.pdb\n")
    return
  else:
    print("1. Downsample ensemble")
    print("2. Remove explicit solvent atoms")
    print("3. Downsample and remove solvent")
#    print "4. Remove specific water from list"
#    print "5. Parse B-factor and Occupancy info"
#    print "6. Last ensemble model to single pdb"
#    print "7. Swap B and Q cols"
#    print "8. Remove ANISOU lines"
#    print "9. Convert phx_gro to phx"
#    print "10. Convert phx to phx_gro"
#    print "11. Add extra conformation"
    option = str(input("Input option number : "))

    if option == str("1") or option == str("3"):
      start_pdb = sys.argv[1]
      open_pdb = open(start_pdb,"r")

      number_models = find_number_model(open_pdb)

      target_number = input("Number models for output (default 25) : ")
      try:
        target_number = int(target_number)
        div = int(number_models / target_number)
      except ValueError:
        target_number = 25
        div = int(number_models / 25)

      fin_pdb = str(str(target_number) + "_dwnsmp_" + start_pdb)
      num_pdb = open(fin_pdb, 'w')

      open_pdb = open(start_pdb,"r")
      find_header(open_pdb = open_pdb, num_pdb = num_pdb)

      model_num = int(1)
      new_model_num = int(1)

      while new_model_num <= target_number:
        print("Parsing model  :", model_num)
        model_parse(open_pdb, num_pdb, model_num, new_model_num, start_pdb)
        model_num = model_num + div
        new_model_num += 1

      num_pdb.close()
      if option == str("1"):
        return

    if option == str("2"):
      start_pdb = sys.argv[1]
      fin_pdb = str("noHOH_" + start_pdb)
      remove_HOH(start_pdb, fin_pdb)
      return

    if option == str("3"):
      noHOH_pdb = str("noHOH_" + fin_pdb)
      remove_HOH(fin_pdb, noHOH_pdb)
      return

    if option == str("4"):
      print("Remove specific HOH")
      if len(sys.argv) < 3:
        print("\n\nERROR - specify water list at command line after PDB file!\n\n")
        main()
      start_pdb = sys.argv[1]
      open_pdb = open(start_pdb,"r")
      water_list = sys.argv[2]
      open_water_list = open(water_list,"r")
      wat_pdb = open('goodHOH.pdb', 'w')
      remove_specific_HOH(open_pdb,open_water_list,wat_pdb)

    if option == str("5"):
      start_pdb = sys.argv[1]
      open_pdb = open(start_pdb,"r")
      parse_bfactor_occ_infomation(open_pdb, start_pdb)

    if option == str("6"):
      start_pdb = sys.argv[1]
      open_pdb = open(start_pdb,"r")
      last_model = input("Parse x model)...     ")
      fin_pdb = str(last_model + "_" + start_pdb)
      num_pdb = open(fin_pdb, 'w')
      find_header(open_pdb, num_pdb)
      parse_specific_pdb(open_pdb, start_pdb, num_pdb, last_model, new_model_num=1)

    if option == str("7"):
      start_pdb = sys.argv[1]
      open_pdb = open(start_pdb,"r")
      if len(sys.argv) > 2:
        second_pdb = sys.argv[2]
      else:
        second_pdb = input("Second pdb import infomation from..... : ")
      open_second_pdb = open(second_pdb,"r")
      get_col = input("Import 'b' or 'q' column from second pdb  : ")
      if get_col != 'q':
        get_col = 'b'
      print("Importing col : ", get_col)
      replace_col = input("Replace 'b' or 'q' column from first pdb  : ")
      if replace_col != 'q':
        replace_col = 'b'
      new_pdb_name = start_pdb + "_swap_" + replace_col + "_with_" + get_col + ".pdb"
      new_pdb = open(new_pdb_name, 'w')
      col_swap(open_pdb, open_second_pdb, get_col, replace_col, new_pdb)

    if option == str("8"):
      start_pdb = sys.argv[1]
      fin_pdb = str("noANISOU_" + start_pdb)
      remove_anisou(start_pdb, fin_pdb)
      return

    if option == str("9"):
      start_pdb = sys.argv[1]
      fin_pdb = str("phxgro_to_phx_" + start_pdb)
      phxgro_to_phx(start_pdb, fin_pdb)
      return

    if option == str("10"):
      start_pdb = sys.argv[1]
      fin_pdb = str("phx_to_phxgro" + start_pdb)
      phx_to_phxgro(start_pdb, fin_pdb)
      return

    if option == str("11"):
      start_pdb = sys.argv[1]
      fin_pdb = str("extra_con" + start_pdb)
      extra_conformation(start_pdb, fin_pdb)
      return

    else:
      print("\n\nPlease choose from list : ")
      main()

if __name__ == '__main__':
  main()


 *******************************************************************************


 *******************************************************************************
mmtbx/refinement/ensemble_refinement/scripts/ens_tools.py
from __future__ import absolute_import, division, print_function
# Ensemble tools for pymol
# Tom Burnley

import math, sys
from pymol import cmd, stored
from six.moves import zip
from six.moves import range

class LogWriter:
      def __init__(self, stdout, filename):
          self.stdout = stdout
          self.logfile = open(filename, 'a')

      def write(self, text):
          self.stdout.write(text)
          self.logfile.write(text)

      def close(self):
          self.stdout.close()
          self.logfile.close()

def print_array_stats(array = None,
                      log = None):
    if array is not None:
        if len(array) == 0: array = [0.0]
        n         = len(array)
        mean      = sum(array) / len(array)
        maximum   = max(array)
        minimum   = min(array)
        print("n         : %4d" % n, file=log)
        print("mean      : %4.3f" % mean, file=log)
        print("min       : %4.3f" % minimum, file=log)
        print("max       : %4.3f" % maximum, file=log)

# Return distance between two coords
def distance(x,y):
    return math.sqrt((x[0]-y[0])*(x[0]-y[0]) + (x[1]-y[1])*(x[1]-y[1]) + (x[2]-y[2])*(x[2]-y[2]))

def ens_measure(pk1 = None,
                pk2 = None,
                pk3 = None,
                pk4 = None,
                name = None,
                log = None,
                verbose = True):
    '''
DESCRIPTION

    Statistics from ensemble structure measurements.  If:
      2 selections give = distance
      3 selections give = angle
      4 selections give = dihedral angle

USAGE

    ens_measure pk1, pk2, pk3, pk4, name, log, verbose

ARGUMENTS

    log = name of log file
    verbose = prints individual measurements

 EXAMPLE

    ens_measure atom1, atom2, name = 'measure', log 'ens.log'
  '''
    print('\nEnsemble measurement', file=log)

    if [pk1, pk2, pk3, pk4].count(None) > 2:
        print('\nERROR: Please supply at least 2 seletions')
        return
    number_models = cmd.count_states(pk1)
    measurements = []

    # distance
    if [pk1, pk2, pk3, pk4].count(None) == 2:
        print('Distance', file=log)
        if name == None: name = 'ens_distance'
        # display as object
        cmd.distance(name = name,
                     selection1 = pk1,
                     selection2 = pk2)

        # get individual values
        for n in range(number_models):
            measurements.append( cmd.get_distance(pk1, pk2, n+1) )
        assert len(measurements) == number_models

    # angle
    if [pk1, pk2, pk3, pk4].count(None) == 1:
        print('Angle', file=log)
        # display as object
        if name == None: name = 'ens_angle'
        cmd.angle(name = name,
                  selection1 = pk1,
                  selection2 = pk2,
                  selection3 = pk3)

        # get individual values
        for n in range(number_models):
            measurements.append( cmd.get_angle(atom1 = pk1,
                                               atom2 = pk2,
                                               atom3 = pk3,
                                               state = n+1) )
        assert len(measurements) == number_models

    # Dihedral angle
    if [pk1, pk2, pk3, pk4].count(None) == 0:
        print('Dihedral angle', file=log)
        # display as object
        if name == None: name = 'ens_dihedral'
        cmd.dihedral(name = name,
                  selection1 = pk1,
                  selection2 = pk2,
                  selection3 = pk3,
                  selection4 = pk4)

        # get individual values
        for n in range(number_models):
            measurements.append( cmd.get_dihedral(atom1 = pk1,
                                                  atom2 = pk2,
                                                  atom3 = pk3,
                                                  atom4 = pk4,
                                                  state = n+1) )
        assert len(measurements) == number_models

    # print stats
    if verbose:
        print(' State  Value', file=log)
        for n, measurement in enumerate(measurements):
            print('  %4d  %3.3f '%(n+1, measurement), file=log)

    print('\nMeasurement statistics', file=log)
    print_array_stats(array                 = measurements,
                      log                   = log)

def ens_rmsd(ens_selection,
             ref_selection,
             log_name = None):
    '''
DESCRIPTION

    Prints RMSD per structure in ensemble w.r.t. a reference structure

USAGE

    ens_rmsd ensemble_selection, reference_selection, name, log,

ARGUMENTS

    log = name of log file
    verbose = calculates structure by structure RMSD for ensemble w.r.t. a single reference structure

 EXAMPLE

    ens_rmsd ensemble_selection, reference_selection, name = 'rmsd', log 'ens.log'
  '''

    if log_name == None:
      log = LogWriter(sys.stdout, 'log.txt')
    else:
      log = LogWriter(sys.stdout, log_name+'.txt')

    # initialize arrays
    ens_selection = ens_selection + ' and not resn hoh'
    ref_selection = ref_selection + ' and not resn hoh'
    rmsd_states = []
    mean_coords = None
    number_models = cmd.count_states(ens_selection)

    # get models, mean coords
    print('\nRMSD by state', file=log)
    print('\n State | RMSD', file=log)
    for i in range(number_models):
      ens_coords = cmd.get_model(ens_selection,state=i+1).get_coord_list()
      ref_coords = cmd.get_model(ref_selection,state=1).get_coord_list()
      atom_sqr_dev = []
      for atom in range(len(ref_coords)):
        x = ref_coords[atom]
        y = ens_coords[atom]
        atom_sqr_dev.append(distance(x,y)**2)
      rmsd = math.sqrt(sum(atom_sqr_dev) / len(atom_sqr_dev))
      rmsd_states.append(rmsd)
      print(' %5d | %5.3f '%(i+1, rmsd), file=log)


    print_array_stats(array                 = rmsd_states,
                      log                   = log)
    print('\nRMSD all states : %5.3f '%(cmd.rms(ens_selection, ref_selection)))

def ens_rmsf(selection,
             rmsf_spectrum = False,
             mean_structure = False,
             mean_per_resi = True,
             log_name = None):
    '''
DESCRIPTION

    Generates and colors structure by RMSF statistics, calculated from mulistate ensembles

USAGE

    ens_rmsf selection, sigma_rmsf_spectrum, mean_structure, histogram_number_bins, log_name

ARGUMENTS

    sigma_rmsf_spectrum = overwrite q col with sigma RMSF (per atom), color by q
    mean_structure = generate new single state structure with mean coords
    histogram_number_bins = number of bins for output histograms
    log_name = name of log file

EXAMPLE

    ens_rmsf protein, True, True, 25, 'ens_rmsf.log'
  '''
    if log_name == None:
      log = LogWriter(sys.stdout, 'log.txt')
    else:
      log = LogWriter(sys.stdout, log_name+'.txt')
    print("\nEnsemble RMSF", file=log)
    print("N.B. waters excluded", file=log)
    print("N.B. B column information from first model", file=log)
    print("Rmsf (Angstrom) [w.r.t mean structure]", file=log)
    print("B_atom (Angstrom^2) [atomic Bfactor used in simulation]", file=log)
    print("B_rmsf (Angstrom^2) [rmsf converted to Bfactor]", file=log)

    # initialize arrays
    selection = selection + ' and not resn hoh'
    models = []
    mean_coords = None
    number_models = cmd.count_states(selection)
    r_number_models = 1.0 / number_models

    # get models, mean coords
    for i in range(number_models):
      models.append(cmd.get_model(selection,state=i+1))
      coords_for_mean = [[x[0]*r_number_models,x[1]*r_number_models,x[2]*r_number_models] for x in models[i].get_coord_list()]
      if mean_coords == None:
        mean_coords = coords_for_mean
      else:
        n = []
        for mean, for_mean in zip(mean_coords, coords_for_mean):
          mean = [sum(_x) for _x in zip(mean,for_mean)]
          n.append(mean)
        mean_coords = n

    # calculate RMSF w.r.t. mean structure
    rmsf_coord = [0.0]*len(mean_coords)
    for i in range(number_models):
      coord_array = models[i].get_coord_list()
      for i_seq, xyz in enumerate(coord_array):
        rmsf_coord[i_seq] += distance(xyz, mean_coords[i_seq])**2
    rmsf_coord = [(x / number_models)**0.5 for x in rmsf_coord]

    # Generate new model object with average xyz coord
    if mean_structure:
      mean_structure_name = 'mean_xyz_'+selection
      cmd.create(mean_structure_name, selection, 1)
      stored.xyz = [[v[0],v[1],v[2]] for v in mean_coords]
      cmd.alter_state(1, mean_structure_name,'(x,y,z) = stored.xyz.pop(0)')

    # convert to B factor (A^2)
    rmsf_as_b_coord = [x**2 * ((8.0 * math.pi**2) / 3.0) for x in rmsf_coord]

    # get atomic b factor info
    atom_b = [at.b for at in models[0].atom]
    b_atom_plus_b_rsmf = [sum(_x) for _x in zip(atom_b,rmsf_as_b_coord)]

    # Colour by rmsf
    if rmsf_spectrum:
#      cmd.color('grey', 'all')
#      cmd.alter('all','q=0')
      atom = models[0].atom

      for n, atom in enumerate(models[0].atom):
        atom_sel = 'id ' + str(atom.id)
        atom_action = 'q = ' + str(rmsf_sigma[n])
        cmd.alter(atom_sel,atom_action)

      print("\n\nQ infomation updated with RMSF sigma\n")
      cmd.spectrum('q',selection = selection)

      if mean_structure:
        cmd.spectrum('q', selection = mean_structure_name)

    else:
      for n, atom in enumerate(models[0].atom):
        atom_sel = 'id ' + str(atom.id)
        atom_action = 'q = ' + str(rmsf_coord[n])
        cmd.alter(atom_sel,atom_action)

      print("\n\nQ infomation updated with RMSF (Angstrom)\n")
      cmd.spectrum('q',selection = selection)

    # array stats
    print('\nB_atom (A^2): ', file=log)
    print_array_stats(array                 = atom_b,
                      log                   = log)
    print('\nRmsf (A): ', file=log)
    print_array_stats(array                 = rmsf_coord,
                      log                   = log)
    print('\nB_rmsf (A^2): ', file=log)
    print_array_stats(array                 = rmsf_as_b_coord,
                      log                   = log)
    print('\nB_atom + B_rmsf (A^2):', file=log)
    print_array_stats(array                 = b_atom_plus_b_rsmf,
                      log                   = log)

    # individual atom stats
    print('\n\n Resi | Name | Chain   | Rmsf | B_rmsf | B_atom |    B_rmsf+B_atom\n', file=log)
    for i_seq, b_factor in enumerate(atom_b):
      print(' %7s %7s %7s | %8.3f | %8.3f %8.3f | %8.3f'%(
                    models[0].atom[i_seq].resi,
                    models[0].atom[i_seq].name,
                    models[0].atom[i_seq].chain,
                    rmsf_coord[i_seq],
                    rmsf_as_b_coord[i_seq],
                    b_factor,
                    b_atom_plus_b_rsmf[i_seq]), file=log)

    if mean_per_resi:
      print('\nMean per residue', file=log)
      # update atom to include info
      for n, atom in enumerate(models[0].atom):
        atom.rmsf               = rmsf_coord[n]
        atom.b_rmsf             = rmsf_as_b_coord[n]
        atom.b_atom_plus_b_rsmf = atom.b + atom.b_rmsf

      print(' Resi Resn | Atoms | Atom_rmsf | B_atom B_rmsf B_atom+B_rmsf', file=log)
      def print_mean_residue():
          print(' %4s %5s|   %3d |  %8.3f | %8.3f %8.3f %8.3f '%(
                current_resi,
                current_resn,
                len(res_b),
                sum(res_rmsf) / len(res_rmsf),
                sum(res_b) / len(res_b),
                sum(res_b_rmsf) / len(res_b_rmsf),
                sum(res_b_atom_plus_b_rsmf) / len(res_b_atom_plus_b_rsmf) ), file=log)

      current_resi = models[0].atom[0].resi
      current_resn = models[0].atom[0].resn
      res_rmsf = []
      res_b = []
      res_b_rmsf = []
      res_b_atom_plus_b_rsmf = []

      for atom in models[0].atom:
        if atom.resi == current_resi:
          assert atom.resn == current_resn
          res_rmsf.append(atom.rmsf)
          res_b.append(atom.b)
          res_b_rmsf.append(atom.b_rmsf)
          res_b_atom_plus_b_rsmf.append(atom.b_atom_plus_b_rsmf)
        else:
          print_mean_residue()
          current_resi = atom.resi
          current_resn = atom.resn
          res_rmsf = [atom.rmsf]
          res_b = [atom.b]
          res_b_rmsf = [atom.b_rmsf]
          res_b_atom_plus_b_rsmf = [atom.b_atom_plus_b_rsmf]
      print_mean_residue()

def ens_prob():
  print('\n\nEnsemble probability options')
  # get models, mean coords
  models = []
  selection = 'all'
  for i in range(cmd.count_states(selection)):
    models.append(cmd.get_model(selection,state=i+1))

  for n, model in enumerate(models):
    residues = model.get_residues()
    for residue in residues:
      # Get individual atom info
      q_list = []
      q_list_mc  = []
      q_list_sc = []
      for i_seq in range (residue[0], residue[1]):
        # Ignore hydrogens
        if model.atom[i_seq].symbol != 'H':
          q_list.append(float(model.atom[i_seq].q))
          if model.atom[i_seq].name in ['N','CA','C','O']:
            q_list_mc.append(float(model.atom[i_seq].q))
          else:
            q_list_sc.append(float(model.atom[i_seq].q))

      # Set probability per residue
      # Mean p
      if len(q_list) > 0:    p_new = sum(q_list) / len(q_list)
      if len(q_list_mc) > 0: p_new_mc = sum(q_list_mc) / len(q_list_mc)
      if len(q_list_sc) > 0: p_new_sc = sum(q_list_sc) / len(q_list_sc)

#      # Joint
      p_new = q_list[0]
      for p in q_list[1:]:
        p_new *= p

#      # nll
#      p_new = math.log(q_list[0])
#      for p in q_list[1:]:
#        p_new += math.log(max(p,0.001))
#      p_new *= -1

      if i_seq == residue[1]-1:
        for i_seq in range (residue[0], residue[1]):
          if True:
            atom_sel = 'id ' + str(model.atom[i_seq].id) + ' and state ' + str(n+1)
            atom_action = 'b = ' + str(p_new)
            cmd.alter(atom_sel, atom_action)
          else:
            atom_sel = 'id ' + str(model.atom[i_seq].id) + ' and state ' + str(n+1)
            if model.atom[i_seq].name in ['N','CA','C','O']:
              atom_action = 'b = ' + str(p_new_mc)
            else:
              atom_action = 'b = ' + str(p_new_sc)
            cmd.alter(atom_sel, atom_action)

def print_names(selection):
  print('\n\nSelection:\n\n\n\n\n')
  selection_string = 'select sel_name, id '
  for x in cmd.identify(selection,0):
    print(x)
    selection_string += string.strip(str(x) + '+')
  print(selection_string[:-1])

cmd.extend('ens_measure',ens_measure)
cmd.extend('ens_rmsf',ens_rmsf)
cmd.extend('ens_rmsd',ens_rmsd)
cmd.extend('ens_prob',ens_prob)
cmd.extend('print_names',print_names)


 *******************************************************************************


 *******************************************************************************
mmtbx/refinement/flip_peptide_side_chain.py
from __future__ import absolute_import, division, print_function
from scitbx.matrix import rotate_point_around_axis
from scitbx.math import dihedral_angle
from six.moves import cStringIO as StringIO

# flippable_sidechains:
#   key: flippable residue name
#   value: list of 5 atom names: first 4 define one angle,
#          1st, 2nd, 3rd and 5th define another torsion angle
#          6th, 7th - additional atoms to flip
flippable_sidechains = {
    "LEU": [" CA ", " CB ", " CG ", " CD1", " CD2"],
    "GLU": [" CB ", " CG ", " CD ", " OE1", " OE2"],
    "ASP": [" CA ", " CB ", " CG ", " OD1", " OD2"],
    "ASN": [" CA ", " CB ", " CG ", " OD1", " ND2"],
    "GLN": [" CB ", " CG ", " CD ", " OE1", " NE2"],
    "ARG": [" CD ", " NE ", " CZ ", " NH1", " NH2"],
    "VAL": [" C  ", " CA ", " CB ", " CG1", " CG2"],
    "PHE": [" CA ", " CB ", " CG ", " CD1", " CD2", " CE1", " CE2"],
    "HIS": [" CA ", " CB ", " CG ", " ND1", " CD2", " CE1", " NE2"],
    "TYR": [" CA ", " CB ", " CG ", " CD1", " CD2", " CE1", " CE2"],
}

def flip_residue(residue, mon_lib_srv=None):
  if residue.resname in flippable_sidechains:
    from mmtbx.utils import rotatable_bonds
    if mon_lib_srv is None:
      mon_lib_srv = mmtbx.monomer_library.server.server()
    null_log = StringIO()
    tardy_model = rotatable_bonds.tardy_model_one_residue(
        residue=residue,
        mon_lib_srv=mon_lib_srv,
        log=null_log)
    if tardy_model is None:
      return None
    clusters = tardy_model.tardy_tree.cluster_manager.clusters[1:]
    axes = tardy_model.tardy_tree.cluster_manager.hinge_edges[1:]
    assert len(axes) == len(clusters)
    residue_atoms = residue.atoms()
    ctr = 0
    ic = 0
    axis = None
    while ctr < len(axes):
      if residue_atoms[axes[ctr][0]].name.strip().upper() == flippable_sidechains[residue.resname][1].strip().upper() and \
          residue_atoms[axes[ctr][1]].name.strip().upper() == flippable_sidechains[residue.resname][2].strip().upper():
        axis = axes[ctr]
        ic = ctr
      ctr += 1
    atoms_to_rotate = []
    for ci in clusters[ic:]:
      atoms_to_rotate.extend(ci)
    if (axis is None):
      return None

    for atom in atoms_to_rotate:
      new_xyz = rotate_point_around_axis(
        axis_point_1=residue.atoms()[axis[0]].xyz,
        axis_point_2=residue.atoms()[axis[1]].xyz,
        point=residue.atoms()[atom].xyz,
        angle=180.0, deg=True)
      residue.atoms()[atom].xyz = new_xyz

def should_be_flipped(residue_1, residue_2):
  """ are these residues in similar flip orientation?"""
  # assert residue_1.resname == residue_2.resname, "%s %s" % (
  #     residue_1.id_str(), residue_2.id_str())
  if residue_1.resname != residue_2.resname:
    # e.g. 1u54, 3srv: PTR mutation of TYR residues
    return False
  if residue_1.resname in flippable_sidechains:
    tor_1_sites = []
    for aname in flippable_sidechains[residue_1.resname][:4]:
      a = residue_1.find_atom_by(name=aname)
      if a is None:
        return False
      else:
        tor_1_sites.append(a.xyz)
    tor_23_sites = []
    for aname in flippable_sidechains[residue_1.resname][:5]:
      a = residue_2.find_atom_by(name=aname)
      if a is None:
        return False
      else:
        tor_23_sites.append(a.xyz)
    tor1 = dihedral_angle(
      sites=tor_1_sites,
      deg=True)
    tor2 = dihedral_angle(
      sites=tor_23_sites[:4],
      deg=True)
    tor3 = dihedral_angle(
      sites=tor_23_sites[:3]+[tor_23_sites[4]],
      deg=True)
    if tor1 is None or tor2 is None or tor3 is None:
      return False
    return abs(tor1-tor2) > abs(tor1-tor3)+5
  return False


 *******************************************************************************


 *******************************************************************************
mmtbx/refinement/geometry_minimization.py
from __future__ import absolute_import, division, print_function
from cctbx import geometry_restraints
import cctbx.geometry_restraints.lbfgs
import mmtbx.refinement.minimization_ncs_constraints
import scitbx.lbfgs
import sys
import mmtbx.utils
from scitbx.array_family import flex
from mmtbx import monomer_library
from six.moves import range

class lbfgs(geometry_restraints.lbfgs.lbfgs):

  def __init__(self,
        sites_cart,
        geometry_restraints_manager,
        geometry_restraints_flags,
        lbfgs_termination_params,
        correct_special_position_tolerance,
        riding_h_manager=None,
        sites_cart_selection=None,
        lbfgs_exception_handling_params=None,
        rmsd_bonds_termination_cutoff=0,
        rmsd_angles_termination_cutoff=0,
        states_collector=None,
        site_labels=None):
    self.rmsd_bonds_termination_cutoff = rmsd_bonds_termination_cutoff
    self.rmsd_angles_termination_cutoff = rmsd_angles_termination_cutoff
    self.states_collector = states_collector
    geometry_restraints.lbfgs.lbfgs.__init__(self,
      sites_cart=sites_cart,
      riding_h_manager = riding_h_manager,
      correct_special_position_tolerance=correct_special_position_tolerance,
      geometry_restraints_manager=geometry_restraints_manager,
      geometry_restraints_flags=geometry_restraints_flags,
      lbfgs_termination_params=lbfgs_termination_params,
      sites_cart_selection=sites_cart_selection,
      lbfgs_exception_handling_params=lbfgs_exception_handling_params,
      site_labels=site_labels,
      states_collector=states_collector)

  def callback_after_step(self, minimizer):
    self.apply_shifts()
    if([self.rmsd_angles, self.rmsd_bonds].count(None) == 0):
      if(self.rmsd_angles < self.rmsd_angles_termination_cutoff and
         self.rmsd_bonds < self.rmsd_bonds_termination_cutoff):
        return True

def add_rotamer_restraints(
      pdb_hierarchy,
      restraints_manager,
      selection,
      sigma,
      mode=None,
      accept_allowed=True,
      mon_lib_srv=None,
      rotamer_manager=None):
  if(mode is not None):
    pdb_hierarchy_for_proxies = mmtbx.utils.switch_rotamers(
      pdb_hierarchy  = pdb_hierarchy.deep_copy(),
      mode           = mode,
      accept_allowed = accept_allowed,
      selection      = selection,
      mon_lib_srv    = mon_lib_srv,
      rotamer_manager= rotamer_manager)
  else:
    pdb_hierarchy_for_proxies = pdb_hierarchy.deep_copy()
  sc = pdb_hierarchy_for_proxies.atoms().extract_xyz()
  if selection is not None:
    sc = sc.select(selection)
  restraints_manager.geometry.add_chi_torsion_restraints_in_place(
    pdb_hierarchy   = pdb_hierarchy_for_proxies,
    sites_cart      = sc,
    selection       = selection,
    chi_angles_only = True,
    sigma           = sigma)
  return pdb_hierarchy_for_proxies, restraints_manager

class run2(object):
  def __init__(self,
               restraints_manager,
               pdb_hierarchy,
               correct_special_position_tolerance,
               riding_h_manager               = None,
               ncs_restraints_group_list      = [], # These are actually for NCS CONSTRAINTS!
               max_number_of_iterations       = 500,
               number_of_macro_cycles         = 5,
               selection                      = None,
               bond                           = False,
               nonbonded                      = False,
               angle                          = False,
               dihedral                       = False,
               chirality                      = False,
               planarity                      = False,
               parallelity                    = False,
               rmsd_bonds_termination_cutoff  = 0,
               rmsd_angles_termination_cutoff = 0,
               alternate_nonbonded_off_on     = False,
               cdl                            = False,
               rdl                            = False,
               correct_hydrogens              = False,
               fix_rotamer_outliers           = True,
               allow_allowed_rotamers         = True,
               states_collector               = None,
               log                            = None,
               mon_lib_srv                    = None,
               ias_selection                  = None,
               ):
    self.log = log
    if self.log is None:
      self.log = sys.stdout
    self.pdb_hierarchy = pdb_hierarchy
    self.ias_selection = ias_selection
    self.minimized = None
    self.mon_lib_srv = mon_lib_srv
    if self.mon_lib_srv is None:
      self.mon_lib_srv = monomer_library.server.server()
    self.restraints_manager = restraints_manager
    assert max_number_of_iterations+number_of_macro_cycles > 0
    assert [bond,nonbonded,angle,dihedral,chirality,planarity,
            parallelity].count(False) < 7
    self.cdl_proxies = None
    self.rdl_proxies = None
    self.rotamer_manager = None
    if fix_rotamer_outliers:
      from mmtbx.rotamer.rotamer_eval import RotamerEval
      self.rotamer_manager = RotamerEval(mon_lib_srv=self.mon_lib_srv)
    if(cdl):
      from mmtbx.conformation_dependent_library.cdl_setup import setup_restraints
      self.cdl_proxies = setup_restraints(self.restraints_manager.geometry)
    self.correct_hydrogens = correct_hydrogens
    if(alternate_nonbonded_off_on and number_of_macro_cycles % 2 != 0):
      number_of_macro_cycles += 1
    import scitbx.lbfgs
    lbfgs_termination_params = scitbx.lbfgs.termination_parameters(
      max_iterations = max_number_of_iterations)
    exception_handling_params = scitbx.lbfgs.exception_handling_parameters(
      ignore_line_search_failed_step_at_lower_bound = True)
    geometry_restraints_flags = geometry_restraints.flags.flags(
      bond               = bond,
      nonbonded          = nonbonded,
      angle              = angle,
      dihedral           = dihedral,
      chirality          = chirality,
      planarity          = planarity,
      parallelity        = parallelity,
      reference_coordinate = True,
      reference_dihedral = True,
      bond_similarity    = True,
      ramachandran_restraints = True)
    self.update_cdl_restraints()
    self.show()
    for i_macro_cycle in range(number_of_macro_cycles):
      print("  macro-cycle:", i_macro_cycle, file=self.log)
      self.restraints_manager.geometry.update_ramachandran_restraints_phi_psi_targets(
        hierarchy=self.pdb_hierarchy)
      if(alternate_nonbonded_off_on and i_macro_cycle<=number_of_macro_cycles/2):
        geometry_restraints_flags.nonbonded = bool(i_macro_cycle % 2)
      self.update_cdl_restraints(macro_cycle=i_macro_cycle)
      if(fix_rotamer_outliers):
        junk, self.restraints_manager = add_rotamer_restraints(
          pdb_hierarchy      = self.pdb_hierarchy,
          restraints_manager = self.restraints_manager,
          selection          = selection,
          sigma              = 10,
          mode               = "fix_outliers",
          accept_allowed     = allow_allowed_rotamers,
          mon_lib_srv        = self.mon_lib_srv,
          rotamer_manager    = self.rotamer_manager)
      sites_cart = self.pdb_hierarchy.atoms().extract_xyz()
      if rdl:
        self.updaterdl(prefix="Update RDL restraints")
      if (ncs_restraints_group_list is not None
          and len(ncs_restraints_group_list)) > 0:
        # do ncs minimization
        print("Using NCS constraints.", file=self.log)
        xrs = self.pdb_hierarchy.extract_xray_structure().deep_copy_scatterers()
        refine_selection = flex.size_t(range(xrs.scatterers().size()))
        tfg_obj = mmtbx.refinement.minimization_ncs_constraints.\
            target_function_and_grads_geometry_minimization(
                xray_structure=xrs,
                ncs_restraints_group_list=ncs_restraints_group_list, # CONSTRAINTS
                refine_selection=refine_selection,
                restraints_manager=self.restraints_manager.geometry,
                refine_sites=True,
                refine_transformations=False,
                )
        minimized = mmtbx.refinement.minimization_ncs_constraints.lbfgs(
          target_and_grads_object      = tfg_obj,
          xray_structure               = xrs,
          ncs_restraints_group_list    = ncs_restraints_group_list, # CONSTRAINTS
          refine_selection             = refine_selection,
          finite_grad_differences_test = False,
          max_iterations               = max_number_of_iterations,
          refine_sites                 = True,
          refine_transformations       = False)
        self.pdb_hierarchy.adopt_xray_structure(xrs)
      else:
        sites_cart_orig = sites_cart.deep_copy()
        if ias_selection is not None and ias_selection.count(True) > 0:
          sites_cart = sites_cart.select(~ias_selection)
        self.minimized = lbfgs(
          sites_cart                      = sites_cart,
          riding_h_manager                = riding_h_manager,
          correct_special_position_tolerance=correct_special_position_tolerance,
          geometry_restraints_manager     = restraints_manager.geometry,
          geometry_restraints_flags       = geometry_restraints_flags,
          lbfgs_termination_params        = lbfgs_termination_params,
          lbfgs_exception_handling_params = exception_handling_params,
          sites_cart_selection            = selection,
          rmsd_bonds_termination_cutoff   = rmsd_bonds_termination_cutoff,
          rmsd_angles_termination_cutoff  = rmsd_angles_termination_cutoff,
          states_collector                = states_collector,
          site_labels                     = None)
        if(ias_selection is not None):
          for i_seq, ias_s in enumerate(ias_selection): # assumes that IAS appended to the back
            if(not ias_s):
              sites_cart_orig[i_seq] = sites_cart[i_seq]
        else:
          sites_cart_orig = sites_cart
        self.pdb_hierarchy.atoms().set_xyz(sites_cart_orig)
      self.show()
      self.log.flush()
      geometry_restraints_flags.nonbonded = nonbonded
      lbfgs_termination_params = scitbx.lbfgs.termination_parameters(
          max_iterations = max_number_of_iterations)

  def update_cdl_restraints(self, macro_cycle=None):
    if(self.cdl_proxies is not None):
      from mmtbx.conformation_dependent_library import update_restraints
      if(macro_cycle is None):
        rc = update_restraints(
          self.pdb_hierarchy,
          self.restraints_manager.geometry,
          cdl_proxies=self.cdl_proxies,
          log=self.log,
          verbose=False)
      elif(macro_cycle>0):
        rc = update_restraints(
          self.pdb_hierarchy,
          self.restraints_manager.geometry,
          sites_cart=self.pdb_hierarchy.atoms().extract_xyz(),
          cdl_proxies=self.cdl_proxies,
          log=self.log,
          verbose=False)

  def updaterdl(self, prefix):
    if self.restraints_manager is None: return
    from mmtbx.conformation_dependent_library import rotamers
    from mmtbx.refinement import print_statistics
    print_statistics.make_header(prefix, out=self.log)
    self.rdl_proxies = None #rotamers.setup_restraints(result)
    rc = rotamers.update_restraints(
      self.pdb_hierarchy,
      self.restraints_manager.geometry,
      current_geometry=self.pdb_hierarchy.extract_xray_structure(),
      rdl_proxies=self.rdl_proxies,
      log=self.log,
      verbose=False,
      )
    print("="*79, file=self.log)
    return rc

  def show(self):
    es = self.restraints_manager.geometry.energies_sites(
      sites_cart = self.pdb_hierarchy.atoms().extract_xyz(),
      compute_gradients = False)
    es.show(prefix="    ", f=self.log)


def minimize_wrapper_for_ramachandran(
    model,
    original_pdb_h,
    excl_string_selection,
    processed_pdb_file = None,
    log=None,
    reference_rotamers = True,
    number_of_cycles=1,
    run_first_minimization_without_reference=False,
    oldfield_weight_scale=3,
    oldfield_plot_cutoff=0.03,
    nonbonded_weight=500,
    reference_sigma=0.7):
  """ Wrapper around geometry minimization specifically tuned for eliminating
  Ramachandran outliers.
  probably not working anymore... no processed_pdb_file available.
  WARNING: no setting sites_cart at the end...
  """
  grm = model.get_restraints_manager()
  assert grm is not None
  from mmtbx.geometry_restraints import reference
  from mmtbx.geometry_restraints.torsion_restraints.reference_model import \
      reference_model, reference_model_params
  from libtbx.utils import null_out
  from scitbx.array_family import flex
  if log is None:
    log = null_out()
  # assert hierarchy.atoms_size()==xrs.scatterers().size(), "%d %d" % (
  #     hierarchy.atoms_size(), xrs.scatterers().size())

  ncs_restraints_group_list = model.get_ncs_groups()
  if ncs_restraints_group_list is None:
    ncs_restraints_group_list = []

  grm.geometry.pair_proxies(
      sites_cart=model.get_sites_cart())
  grm.geometry.update_ramachandran_restraints_phi_psi_targets(
      hierarchy=model.get_hierarchy())

  if reference_rotamers and original_pdb_h is not None:
    # make selection excluding rotamer outliers
    from mmtbx.rotamer.rotamer_eval import RotamerEval
    # print "Excluding rotamer outliers"
    rotamer_manager = model.get_rotamer_manager()
    non_rot_outliers_selection = flex.bool(model.get_number_of_atoms(), False)
    for m in original_pdb_h.models():
      for chain in m.chains():
        for conf in chain.conformers():
          for res in conf.residues():
            ev = rotamer_manager.evaluate_residue_2(res)
            if ev != "OUTLIER" or ev is None:
              for a in res.atoms():
                non_rot_outliers_selection[a.i_seq] = True
            # else:
            #   print "  ", res.id_str()

    if processed_pdb_file is not None:
      rm_params = reference_model_params.extract()
      rm_params.reference_model.enabled=True
      rm_params.reference_model.strict_rotamer_matching=False
      rm_params.reference_model.main_chain=False
      rm = reference_model(
        processed_pdb_file=processed_pdb_file,
        reference_file_list=None,
        reference_hierarchy_list=[original_pdb_h],
        mon_lib_srv=model.get_mon_lib_srv(),
        ener_lib=model.get_ener_lib(),
        has_hd=None,
        params=rm_params.reference_model,
        selection=non_rot_outliers_selection,
        log=log)
      rm.show_reference_summary(log=log)
      grm.geometry.adopt_reference_dihedral_manager(rm)

  # dealing with SS
  if model.get_ss_annotation() is not None:
    from mmtbx.secondary_structure import manager
    ss_manager = manager(
        pdb_hierarchy=model.get_hierarchy(),
        geometry_restraints_manager=grm.geometry,
        sec_str_from_pdb_file=model.get_ss_annotation(),
        params=None,
        mon_lib_srv=model.get_mon_lib_srv(),
        verbose=-1,
        log=log)
    grm.geometry.set_secondary_structure_restraints(
        ss_manager=ss_manager,
        hierarchy=model.get_hierarchy(),
        log=log)

  if run_first_minimization_without_reference:
    obj = run2(
      restraints_manager=grm,
      pdb_hierarchy=model.get_hierarchy(),
      correct_special_position_tolerance=1.0,
      ncs_restraints_group_list=ncs_restraints_group_list,
      max_number_of_iterations=300,
      number_of_macro_cycles=number_of_cycles,
      bond=True,
      nonbonded=True,
      angle=True,
      dihedral=True,
      chirality=True,
      planarity=True,
      fix_rotamer_outliers=True,
      log=log)

  if original_pdb_h is not None:
    if not excl_string_selection or len(excl_string_selection) == 0:
      excl_string_selection = "all"
    asc = original_pdb_h.atom_selection_cache()
    sel = asc.selection("(%s) and (name CA or name C or name N or name O)" % excl_string_selection)

    grm.geometry.append_reference_coordinate_restraints_in_place(
        reference.add_coordinate_restraints(
            sites_cart = original_pdb_h.atoms().extract_xyz().select(sel),
            selection  = sel,
            sigma      = reference_sigma,
            top_out_potential=True))

  obj = run2(
      restraints_manager       = grm,
      pdb_hierarchy            = model.get_hierarchy(),
      correct_special_position_tolerance = 1.0,
      ncs_restraints_group_list=ncs_restraints_group_list,
      max_number_of_iterations = 300,
      number_of_macro_cycles   = number_of_cycles,
      bond                     = True,
      nonbonded                = True,
      angle                    = True,
      dihedral                 = True,
      chirality                = True,
      planarity                = True,
      fix_rotamer_outliers     = True,
      log                      = log)
  grm.geometry.reference_dihedral_manager=None


 *******************************************************************************


 *******************************************************************************
mmtbx/refinement/group.py
from __future__ import absolute_import, division, print_function
from cctbx.array_family import flex
from libtbx import adopt_init_args
import sys
from libtbx.test_utils import approx_equal
from scitbx import lbfgs
import copy
from cctbx import adptbx
from cctbx import xray
from libtbx.utils import user_plus_sys_time
from cctbx import crystal
import random
from six.moves import zip
from six.moves import range

time_group_py  = 0.0

def show_times(out = None):
  if(out is None): out = sys.stdout
  total = time_group_py
  if(total > 0.01):
     print("Group ADP refinement:", file=out)
     print("  time_group_py                          = %-7.2f" % time_group_py, file=out)
  return total

class sphere_similarity_restraints(object):
  def __init__(
        self,
        xray_structure,
        selection,
        refine_adp,
        refine_occ,
        sphere_radius = 5.0):
    self.selection = selection
    assert str(type(selection).__name__) == "bool"
    self.sphere_radius = sphere_radius
    self.refine_adp = refine_adp
    self.refine_occ = refine_occ
    pair_asu_table = xray_structure.pair_asu_table(
      distance_cutoff = self.sphere_radius)
    self.pair_sym_table = pair_asu_table.extract_pair_sym_table()
    assert [self.refine_adp, self.refine_occ].count(True) == 1
    self.sites_frac = xray_structure.sites_frac()
    self.orthogonalization_matrix = \
      xray_structure.unit_cell().orthogonalization_matrix()

  def target_and_gradients(self, xray_structure, to_compute_weight=False):
    if(to_compute_weight):
      xrs = xray_structure.deep_copy_scatterers()
      # This may be useful to explore:
      #xrs.shake_adp_if_all_equal(b_iso_tolerance = 1.e-3)
      #xrs.shake_adp(spread=10, keep_anisotropic= False)
    else:
      xrs = xray_structure
    if(self.refine_adp): params = xrs.extract_u_iso_or_u_equiv()
    if(self.refine_occ): params = xrs.scatterers().extract_occupancies()
    if(to_compute_weight):
      pmin = flex.min(params)
      pmax = flex.max(params)
      if(abs(pmin+pmax)!=0. and abs(pmin-pmax)/abs(pmin+pmax)*2*100<1.e-3):
        pmean = flex.mean(params)
        n_par = params.size()
        params = flex.double()
        for i in range(n_par):
          params.append(pmean + 0.1 * pmean * random.choice([-1,0,1]))
    return crystal.adp_iso_local_sphere_restraints_energies(
      pair_sym_table           = self.pair_sym_table,
      orthogonalization_matrix = self.orthogonalization_matrix,
      sites_frac               = self.sites_frac,
      u_isos                   = params,
      selection                = self.selection,
      use_u_iso                = self.selection,
      grad_u_iso               = self.selection,
      sphere_radius            = self.sphere_radius,
      distance_power           = 2,
      average_power            = 1,
      min_u_sum                = 1.e-6,
      compute_gradients        = True,
      collect                  = False)

class manager(object):
  def __init__(
        self,
        fmodel,
        selections                  = None,
        max_number_of_iterations    = 50,
        number_of_macro_cycles      = 5,
        use_restraints              = False,
        restraints_weight           = None,
        convergence_test            = True,
        convergence_delta           = 0.00001,
        run_finite_differences_test = False,
        refine_adp                  = False,
        refine_occ                  = False,
        log                         = None,
        occupancy_max               = None,
        occupancy_min               = None):
    global time_group_py
    #
    tmp = flex.size_t()
    for s in selections: tmp.extend(s)
    selections_as_bool = flex.bool(fmodel.xray_structure.scatterers().size(),
      tmp)
    #
    if(log is None): log = sys.stdout
    timer = user_plus_sys_time()
    self.show(
      rw         = fmodel.r_work(),
      rf         = fmodel.r_free(),
      tw         = fmodel.target_w(),
      mc         = 0,
      it         = 0,
      refine_adp = refine_adp,
      refine_occ = refine_occ,
      weight     = restraints_weight,
      out        = log)
    assert [refine_adp, refine_occ].count(True) == 1
    if(selections is None):
      selections = []
      selections.append(
        flex.bool(fmodel.xray_structure.scatterers().size(), True))
    else: assert len(selections) > 0
    par_initial = []
    selections_ = []
    for sel in selections:
      if(refine_adp): par_initial.append(adptbx.b_as_u(0.0))
      if(refine_occ): par_initial.append(0.0)
      if(str(type(sel).__name__) == "bool"):
        selections_.append(sel.iselection())
      else:
        selections_.append(sel)
    selections = selections_
    scatterers = fmodel.xray_structure.scatterers()
    scatterers.flags_set_grads(state=False)
    # XXX very inefficient: same code is in driver.py file. fix asap. Pavel.
    save_use_u_iso = fmodel.xray_structure.use_u_iso()
    save_use_u_aniso = fmodel.xray_structure.use_u_aniso()
    for sel in selections:
      if(refine_adp):
         for s in sel:
           sc = scatterers[s]
           if(not sc.flags.use_u_iso()):
             sc.flags.set_use_u_iso(True)
             if(sc.u_iso == -1): sc.u_iso = 0
         scatterers.flags_set_grad_u_iso(iselection = sel)
      if(refine_occ):
        scatterers.flags_set_grad_occupancy(iselection = sel)
    restraints_manager = None
    if(use_restraints):
      restraints_manager = sphere_similarity_restraints(
        xray_structure = fmodel.xray_structure,
        selection      = selections_as_bool,
        refine_adp     = refine_adp,
        refine_occ     = refine_occ)
    fmodel_copy = fmodel.deep_copy()
    rworks = flex.double()
    sc_start = fmodel.xray_structure.scatterers().deep_copy()
    minimized = None
    self.tested = 0
    for macro_cycle in range(1,number_of_macro_cycles+1,1):
      if(minimized is not None): par_initial = minimized.par_min
      minimized = group_minimizer(
        fmodel                      = fmodel_copy,
        sc_start                    = sc_start,
        selections                  = selections,
        par_initial                 = par_initial,
        refine_adp                  = refine_adp,
        refine_occ                  = refine_occ,
        max_number_of_iterations    = max_number_of_iterations,
        run_finite_differences_test = run_finite_differences_test,
        restraints_manager          = restraints_manager,
        restraints_weight           = restraints_weight)
      if(minimized is not None):
        par_initial = minimized.par_min
        self.tested += minimized.tested
      apply_transformation(
        xray_structure = fmodel.xray_structure,
        par            = par_initial,
        sc_start       = sc_start,
        selections     = selections,
        refine_adp     = refine_adp,
        refine_occ     = refine_occ)
      fmodel_copy.update_xray_structure(
        xray_structure = fmodel.xray_structure,
        update_f_calc  = True)
      rwork = minimized.fmodel.r_work()
      rfree = minimized.fmodel.r_free()
      assert approx_equal(rwork, fmodel_copy.r_work())
      self.show(
        rw         = rwork,
        rf         = rfree,
        tw         = minimized.fmodel.target_w(),
        mc         = macro_cycle,
        it         = minimized.counter,
        refine_adp = refine_adp,
        refine_occ = refine_occ,
        weight     = minimized.weight,
        out        = log)
      if(convergence_test):
        rworks.append(rwork)
        if(rworks.size() > 1):
          size = rworks.size() - 1
          if(abs(rworks[size]-rworks[size-1])<convergence_delta):
             break
    fmodel_copy.xray_structure.tidy_us()
    fmodel.update_xray_structure(
      xray_structure = fmodel_copy.xray_structure, update_f_calc  = True)
    if(refine_occ):
      i_selection = flex.size_t()
      for sel in selections:
        i_selection.extend(sel)
      fmodel.xray_structure.adjust_occupancy(
        occ_max   = occupancy_max,
        occ_min   = occupancy_min,
        selection = i_selection)
    self.fmodel = fmodel
    time_group_py += timer.elapsed()

  def show(
        self,
        rw,
        rf,
        tw,
        mc,
        it,
        refine_adp,
        refine_occ,
        weight,
        out):
    if(out is None): return
    mc = str(mc)
    it = str(it)
    if(refine_adp): part1 = "|-group b-factor refinement (macro cycle = "
    if(refine_occ): part1 = "|-group occupancy refinement (macro cycle = "
    part2 = "; iterations = "
    n = 77 - len(part1 + part2 + mc + it)
    part3 = ")"+"-"*n+"|"
    print(part1 + mc + part2 + it + part3, file=out)
    part1 = "| "
    if(weight is None):
      part4 = " restraints weight = "+str(weight)
    else:
      part4 = " restraints weight = "+str("%10.3f"%weight).strip()
    rw = "| r_work = "+str("%.4f"%rw)
    rf = " r_free = "+str("%.4f"%rf)
    tw = " target = "+str("%.6f"%tw)
    n = 78 - len(rw+rf+tw+part4)
    end = " "*n+"|"
    print(rw+rf+tw+part4+end, file=out)
    print("|" +"-"*77+"|", file=out)

class group_minimizer(object):
  def __init__(
        self,
        fmodel,
        sc_start,
        selections,
        par_initial,
        refine_adp,
        refine_occ,
        max_number_of_iterations,
        run_finite_differences_test = False,
        restraints_weight = None,
        restraints_manager = None):
    adopt_init_args(self, locals())
    self.target_functor = fmodel.target_functor()
    self.target_functor.prepare_for_minimization()
    self.counter=0
    assert len(self.selections) == len(self.par_initial)
    self.par_min = copy.deepcopy(self.par_initial)
    self.x = self.pack(self.par_min)
    self.n = self.x.size()
    self.weight = restraints_weight
    if(self.restraints_manager is not None and self.weight is None):
      gx = self.target_functor(
        compute_gradients=True).gradients_wrt_atomic_parameters(
          u_iso     = refine_adp,
          occupancy = refine_occ)
      rtg = self.restraints_manager.target_and_gradients(
        xray_structure    = self.fmodel.xray_structure,
        to_compute_weight = True)
      gx_norm = gx.norm()
      if(gx_norm != 0):
        self.weight = rtg.gradients.norm()/gx_norm
      else: self.weight = 1.0
    if(self.weight is not None):
      assert self.restraints_manager is not None
    if(run_finite_differences_test):
      self.buffer_ana = flex.double()
      self.buffer_fin = flex.double()
    self.minimizer = lbfgs.run(
      target_evaluator = self,
      termination_params = lbfgs.termination_parameters(
        max_iterations = max_number_of_iterations),
      exception_handling_params = lbfgs.exception_handling_parameters(
        ignore_line_search_failed_step_at_lower_bound = True,
        ignore_line_search_failed_step_at_upper_bound = True))
    self.compute_functional_and_gradients()
    del self.x
    self.tested = 0
    if(run_finite_differences_test):
      # For debugging only
      #for a,f in zip(self.buffer_ana, self.buffer_fin):
      #  print a, f
      diff = flex.abs(self.buffer_ana - self.buffer_fin)
      s = diff < 1.e-3
      if(s.size()>0 and s.count(True)*100./s.size()>50):
        self.tested += 1

  def get_restraints_tg(self):
    result = None
    if(self.restraints_manager is not None):
      result = self.restraints_manager.target_and_gradients(
        xray_structure = self.fmodel.xray_structure)
    return result

  def get_tg(self, compute_gradients):
    return target_and_grads(
      target_functor    = self.target_functor,
      selections        = self.selections,
      refine_adp        = self.refine_adp,
      refine_occ        = self.refine_occ,
      compute_gradients = compute_gradients,
      rtg               = self.get_restraints_tg(),
      weight            = self.weight)

  def apply_shifts(self, par):
    apply_transformation(
      xray_structure = self.fmodel.xray_structure,
      par            = par,
      sc_start       = self.sc_start,
      selections     = self.selections,
      refine_adp     = self.refine_adp,
      refine_occ     = self.refine_occ)

  def pack(self, par):
    return flex.double(tuple(par))

  def unpack_x(self):
    self.par_min = tuple(self.x)

  def finite_difference_test(self):
    if(self.fmodel.r_work()>1.e-3):
      i_g_max = flex.max_index(flex.abs(self.g))
      eps = 1.e-5
      par_eps = list(self.par_min)
      par_eps[i_g_max] = self.par_min[i_g_max] + eps
      self.apply_shifts(par = par_eps)
      self.fmodel.update_xray_structure(update_f_calc=True)
      t1 = self.get_tg(compute_gradients=False).target()
      par_eps[i_g_max] = self.par_min[i_g_max] - eps
      self.apply_shifts(par = par_eps)
      del par_eps
      self.fmodel.update_xray_structure(update_f_calc=True)
      t2 = self.get_tg(compute_gradients=False).target()
      self.apply_shifts(par = self.par_min)
      self.fmodel.update_xray_structure(update_f_calc=True)
      self.buffer_ana.append(self.g[i_g_max])
      self.buffer_fin.append((t1-t2)/(eps*2))

  def compute_functional_and_gradients(self):
    self.unpack_x()
    self.counter += 1
    self.apply_shifts(par = self.par_min)
    self.fmodel.update_xray_structure(update_f_calc=True)
    tg_obj = self.get_tg(compute_gradients=True)
    self.f = tg_obj.target()
    self.g = flex.double(tg_obj.gradients_wrt_par())
    if(self.run_finite_differences_test):
      self.finite_difference_test()
    return self.f, self.g

def apply_transformation(
      xray_structure,
      par,
      selections,
      sc_start,
      refine_adp,
      refine_occ):
  assert len(selections) == len(par)
  assert [refine_adp, refine_occ].count(True) == 1
  new_sc = sc_start.deep_copy()
  for sel, pari in zip(selections, par):
    if(refine_adp):
      xray.shift_us(
        scatterers = new_sc,
        unit_cell  = xray_structure.unit_cell(),
        u_shift    = pari,
        selection  = sel)
    if(refine_occ):
      xray.shift_occupancies(
        scatterers = new_sc,
        q_shift    = pari,
        selection  = sel)
  xray_structure.replace_scatterers(new_sc)

class target_and_grads(object):
  def __init__(
        self,
        target_functor,
        selections,
        refine_adp,
        refine_occ,
        compute_gradients=True,
        rtg=None,
        weight=None):
    assert [refine_adp, refine_occ].count(True) == 1
    t_r = target_functor(compute_gradients=compute_gradients)
    self.f = t_r.target_work()
    if(rtg is not None): self.f = self.f*weight+rtg.residual_sum
    if(compute_gradients):
      target_grads_wrt_par = t_r.gradients_wrt_atomic_parameters(
        u_iso     = refine_adp,
        occupancy = refine_occ)
      if(rtg is not None):
        target_grads_wrt_par = target_grads_wrt_par*weight+rtg.gradients
      self.grads_wrt_par = []
      for sel in selections:
        target_grads_wrt_par_sel = target_grads_wrt_par.select(sel)
        self.grads_wrt_par.append(flex.sum(target_grads_wrt_par_sel))
    else:
      self.grads_wrt_par = None

  def target(self):
    return self.f

  def gradients_wrt_par(self):
    return self.grads_wrt_par


 *******************************************************************************


 *******************************************************************************
mmtbx/refinement/inspect_tardy_trajectory.py
from __future__ import absolute_import, division, print_function
from libtbx import easy_pickle
import sys, os
from six.moves import zip
op = os.path

def run(args):
  assert len(args) == 1, "trajectory_directory"
  traj_dir = args[0]
  labels = easy_pickle.load(file_name=op.join(traj_dir, "labels"))
  fn = op.join(traj_dir, "xray_structure")
  if (op.isfile(fn)):
    xray_structure = easy_pickle.load(file_name=fn)
  sites_file_names_by_index = {}
  for fn in os.listdir(traj_dir):
    if (not fn.startswith("sites_")): continue
    try: i = int(fn[6:])
    except ValueError: continue
    assert i not in sites_file_names_by_index
    sites_file_names_by_index[i] = fn
  assert 0 in sites_file_names_by_index
  sites_0 = easy_pickle.load(
    file_name=op.join(traj_dir, sites_file_names_by_index[0]))
  max_i = max(sites_file_names_by_index.keys())
  sites_final = easy_pickle.load(
    file_name=op.join(traj_dir, sites_file_names_by_index[i]))
  print("rms difference start vs. final: %.3f" % \
    sites_final.rms_difference(sites_0))
  #
  import scitbx.math.superpose
  ls = scitbx.math.superpose.least_squares_fit(
    reference_sites=sites_0, other_sites=sites_final)
  print("ls r:", ls.r)
  fm = scitbx.math.r3_rotation_axis_and_angle_from_matrix(r=ls.r)
  print("ls r axis, angle:", fm.axis, "%.2f" % fm.angle(deg=True))
  print("ls t:", ls.t.transpose())
  sites_fit = ls.other_sites_best_fit()
  print("rms difference start vs. best fit: %.3f" % \
    sites_fit.rms_difference(sites_0))
  #
  from six.moves import cStringIO as StringIO
  out = StringIO()
  fit_dists_sq = (sites_fit - sites_0).dot()
  from scitbx.array_family import flex
  perm = flex.sort_permutation(fit_dists_sq, reverse=True)
  for lbl,fit_d,final_d in zip(
        flex.std_string(labels).select(perm),
        flex.sqrt(fit_dists_sq.select(perm)),
        flex.sqrt((sites_final - sites_0).dot().select(perm))):
    print(lbl, "%.2f %.2f" % (fit_d, final_d), file=out)
  if (0):
    sys.stdout.write(out.getvalue())
  #
  if (xray_structure is not None):
    from cctbx import crystal
    site_symmetry_table = xray_structure.site_symmetry_table()
    for i_seq in site_symmetry_table.special_position_indices():
      print("special position:", labels[i_seq])
      try:
        crystal.correct_special_position(
          crystal_symmetry = xray_structure,
          special_op       = site_symmetry_table.get(i_seq).special_op(),
          site_cart        = sites_final[i_seq])
      except AssertionError as e:
        from libtbx.str_utils import prefix_each_line
        print(prefix_each_line(prefix="  ", lines_as_one_string=str(e)))
        print()

if (__name__ == "__main__"):
  run(args=sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
mmtbx/refinement/minimization.py
from __future__ import absolute_import, division, print_function
from cctbx import xray
from cctbx import crystal
from cctbx.array_family import flex
import scitbx.lbfgs
from libtbx import adopt_init_args
import math
import sys
from libtbx.utils import user_plus_sys_time
from cctbx import adptbx
from libtbx.str_utils import format_value
from libtbx.utils import Sorry
import mmtbx.refinement.minimization_ncs_constraints

class lbfgs(object):

  def __init__(self, fmodels,
                     restraints_manager       = None,
                     model                    = None,
                     is_neutron_scat_table    = None,
                     target_weights           = None,
                     refine_xyz               = False,
                     refine_adp               = False,
                     lbfgs_termination_params = None,
                     use_fortran              = False,
                     verbose                  = 0,
                     correct_special_position_tolerance = 1.0,
                     iso_restraints           = None,
                     h_params                 = None,
                     qblib_params             = None,
                     macro_cycle              = None,
                     u_min                    = adptbx.b_as_u(-5.0),
                     u_max                    = adptbx.b_as_u(999.99),
                     collect_monitor          = True,
                     log                      = None):
    timer = user_plus_sys_time()
    adopt_init_args(self, locals())
    self.f=None
    self.xray_structure = self.fmodels.fmodel_xray().xray_structure
    self.fmodels.create_target_functors()
    self.fmodels.prepare_target_functors_for_minimization()
    if(self.refine_adp and fmodels.fmodel_neutron() is None):
      self.xray_structure.tidy_us()
      self.fmodels.update_xray_structure(
        xray_structure = self.xray_structure,
        update_f_calc  = True)
    self.weights = None
# QBLIB INSERT
    self.qblib_params = qblib_params
    if(self.qblib_params is not None and self.qblib_params.qblib):
        self.macro = macro_cycle
        self.qblib_cycle_count = 0
        self.tmp_XYZ = None
        self.XYZ_diff_curr=None
# QBLIB END
    self.correct_special_position_tolerance = correct_special_position_tolerance
    if(refine_xyz and target_weights is not None):
      self.weights = target_weights.xyz_weights_result
    elif(refine_adp and target_weights is not None):
      self.weights = target_weights.adp_weights_result
    else:
      from mmtbx.refinement import weights
      self.weights = weights.weights(wx = 1, wx_scale = 1, w = 0)
    if(self.collect_monitor):
      self.monitor = monitor(
        weights        = self.weights,
        fmodels        = fmodels,
        model          = model,
        iso_restraints = iso_restraints,
        refine_xyz     = refine_xyz,
        refine_adp     = refine_adp,
        refine_occ     = False)
    if(self.collect_monitor): self.monitor.collect()
    self.neutron_refinement = (self.fmodels.fmodel_n is not None)
    self.x = flex.double(self.xray_structure.n_parameters(), 0)
    self.riding_h_manager = self.model.riding_h_manager
    self._scatterers_start = self.xray_structure.scatterers()
    #lbfgs_core_params = scitbx.lbfgs.core_parameters(
    #  stpmin = 1.e-9,
    #  stpmax = adptbx.b_as_u(10))
    self.minimizer = scitbx.lbfgs.run(
      target_evaluator          = self,
      termination_params        = lbfgs_termination_params,
      use_fortran               = use_fortran,
    #  core_params = lbfgs_core_params,
    #  gradient_only=True,
      exception_handling_params = scitbx.lbfgs.exception_handling_parameters(
                         ignore_line_search_failed_step_at_lower_bound = True))
    self.apply_shifts()
    del self._scatterers_start
    self.compute_target(compute_gradients = False,u_iso_refinable_params = None)
    if(self.refine_adp and self.fmodels.fmodel_neutron() is None):
      self.xray_structure.tidy_us()
      self.fmodels.update_xray_structure(
        xray_structure = self.xray_structure,
        update_f_calc  = True)
    if(self.collect_monitor):
      self.monitor.collect(iter = self.minimizer.iter(),
                           nfun = self.minimizer.nfun())
    self.fmodels.create_target_functors()
# QBLIB INSERT
    if(self.qblib_params is not None and self.qblib_params.qblib):
       print('{:-^80}'.format(""), file=self.qblib_params.qblib_log)
       print(file=self.qblib_params.qblib_log)
# QBLIB END

  def apply_shifts(self):
    if(self.refine_adp):
      xray.ext.truncate_shifts(
        shifts    = self.x,
        min_value = self.u_min,
        max_value = self.u_max)
    if(self.riding_h_manager is not None and self.refine_xyz):
      # temporary array with zero shifts for all atoms
      tmp = flex.vec3_double(self._scatterers_start.size(), [0,0,0])
      # Set shifts according to self.x
      tmp = tmp.set_selected(self.model.refinement_flags.sites_individual,
        flex.vec3_double(self.x))
      # Set shifts of H atoms to zero (they will be idealized later anyway)
      shifts_h = flex.vec3_double(self.riding_h_manager.hd_selection.count(True),
        [0,0,0])
      tmp = tmp.set_selected(self.riding_h_manager.hd_selection, shifts_h)
      # Select entries for shifted atoms only (e.g. if there is xyz selection)
      tmp_x = tmp.select(self.model.refinement_flags.sites_individual)
      # orig
      #tmp = tmp.set_selected(self.riding_h_manager.not_hd_selection,
      #  flex.vec3_double(self.x))
      # Set entries in self.x corresponding to H atoms to 0
      self.x.set_selected(flex.bool(self.x.size()), tmp_x.as_double())
      # orig
      #self.x.set_selected(flex.bool(self.x.size()), tmp.as_double())

    apply_shifts_result = xray.ext.minimization_apply_shifts(
      unit_cell      = self.xray_structure.unit_cell(),
      scatterers     = self._scatterers_start,
      shifts         = self.x)
    scatterers_shifted = apply_shifts_result.shifted_scatterers
    if(self.refine_xyz):
      site_symmetry_table = self.xray_structure.site_symmetry_table()
      for i_seq in site_symmetry_table.special_position_indices():
        try:
          scatterers_shifted[i_seq].site = crystal.correct_special_position(
            crystal_symmetry = self.xray_structure,
            special_op       = site_symmetry_table.get(i_seq).special_op(),
            site_frac        = scatterers_shifted[i_seq].site,
            site_label       = scatterers_shifted[i_seq].label,
            tolerance        = self.correct_special_position_tolerance)
        except Exception as e:
          print(str(e), file=self.log)
    self.xray_structure.replace_scatterers(scatterers = scatterers_shifted)
    #
    if(self.riding_h_manager is not None and self.refine_xyz):
      sites_cart = self.xray_structure.sites_cart()
      self.riding_h_manager.idealize_riding_h_positions(
        sites_cart = sites_cart,
        selection_bool = self.model.refinement_flags.sites_individual)
      self.xray_structure.set_sites_cart(sites_cart=sites_cart)
    #
    if(self.refine_adp):
      return apply_shifts_result.u_iso_refinable_params
    else:
      return None

  def compute_target(self, compute_gradients, u_iso_refinable_params):
    self.stereochemistry_residuals = None
    self.fmodels.update_xray_structure(
      xray_structure = self.xray_structure,
      update_f_calc  = True)
    fmodels_target_and_gradients = self.fmodels.target_and_gradients(
      weights                = self.weights,
      compute_gradients      = compute_gradients,
      u_iso_refinable_params = u_iso_refinable_params)
    self.f = fmodels_target_and_gradients.target()
    self.g = fmodels_target_and_gradients.gradients()
    if(self.refine_xyz and self.restraints_manager is not None and
       self.weights.w > 0.0):
      self.stereochemistry_residuals = \
        self.model.restraints_manager_energies_sites(
          compute_gradients = compute_gradients)
# QBLIB INSERT
      if(self.qblib_params is not None and self.qblib_params.qblib):
        from qbpy import qb_refinement
        self.qblib_cycle_count +=1
        if(self.tmp_XYZ is not None):
          diff,max_diff,max_elem = qb_refinement.array_difference(
            self.tmp_XYZ,
            self.model.get_sites_cart(),
            )
          if(self.XYZ_diff_curr is not None):
            self.XYZ_diff_curr=max_elem
          else:
            self.XYZ_diff_curr=max_elem
          if(max_elem>=self.qblib_params.skip_divcon_threshold):
               self.tmp_XYZ = self.model.get_sites_cart()
        else:
          self.tmp_XYZ = self.model.get_sites_cart()
        if (self.macro_cycle != self.qblib_params.macro_cycle_to_skip):
          qblib_call = qb_refinement.QBblib_call_manager(
            hierarchy = self.model.get_hierarchy(),
            xray_structure=self.model.get_xray_structure(),
            geometry_residuals = self.stereochemistry_residuals,
            qblib_params=self.qblib_params,
            diff_curr=self.XYZ_diff_curr,
            macro=self.macro_cycle,
            micro=self.qblib_cycle_count,
            )
          try:
            qblib_call.run()
          except Exception as e:
            if e.__class__.__name__=="QB_none_fatal_error":
              raise Sorry(str(e))
            else:
              raise e

          if(qblib_call.QBStatus):
            qblib_g=qblib_call.result_QBlib.gradients
            qblib_f=qblib_call.result_QBlib.target
            self.stereochemistry_residuals.gradients=qblib_g
            self.stereochemistry_residuals.target=qblib_f
# QBLIB END
      er = self.stereochemistry_residuals.target
      self.f += er * self.weights.w
      if(compute_gradients):
        sgc = self.stereochemistry_residuals.gradients
        # ias do not participate in geometry restraints
        if self.model is not None and self.model.ias_manager is not None:
          ias_selection = self.model.ias_manager.get_ias_selection()
          if ias_selection is not None and ias_selection.count(True) > 0:
            sgc.extend(flex.vec3_double(ias_selection.count(True),[0,0,0]))
        xray.minimization.add_gradients(
          scatterers     = self.xray_structure.scatterers(),
          xray_gradients = self.g,
          site_gradients = sgc*self.weights.w)
    if(self.refine_adp and self.restraints_manager is not None and
       self.restraints_manager.geometry is not None
       and self.weights.w > 0.0 and self.iso_restraints is not None):
      use_hd = False
      if(self.fmodels.fmodel_n is not None or
         (self.is_neutron_scat_table is not None and
          self.is_neutron_scat_table == "neutron") or
         self.h_params.refine == "individual"):
        use_hd = True
      energies_adp = self.model.energies_adp(
        iso_restraints    = self.iso_restraints,
        use_hd            = use_hd,
        compute_gradients = compute_gradients)
      self.f += energies_adp.target * self.weights.w
      if(compute_gradients):
        if(energies_adp.u_aniso_gradients is None):
          xray.minimization.add_gradients(
            scatterers      = self.xray_structure.scatterers(),
            xray_gradients  = self.g,
            u_iso_gradients = energies_adp.u_iso_gradients * self.weights.w)
        else:
          energies_adp.u_aniso_gradients *= self.weights.w
          if(energies_adp.u_iso_gradients is not None):
            energies_adp.u_iso_gradients *= self.weights.w
          xray.minimization.add_gradients(
            scatterers        = self.xray_structure.scatterers(),
            xray_gradients    = self.g,
            u_aniso_gradients = energies_adp.u_aniso_gradients,
            u_iso_gradients   = energies_adp.u_iso_gradients)
          energies_adp.u_aniso_gradients = None # just for safety
          energies_adp.u_iso_gradients = None

  def callback_after_step(self, minimizer):
    if (self.verbose > 0):
      print("refinement.minimization step: f,iter,nfun:", end=' ')
      print(self.f,minimizer.iter(),minimizer.nfun())

  def compute_functional_and_gradients(self):
    u_iso_refinable_params = self.apply_shifts()
    self.compute_target(compute_gradients     = True,
                        u_iso_refinable_params = u_iso_refinable_params)
    if (self.verbose > 1):
      print("xray.minimization line search: f,rms(g):", end=' ')
      print(self.f, math.sqrt(flex.mean_sq(self.g)))
    return self.f, self.g

class monitor(object):
  def __init__(self, weights,
                     fmodels,
                     model,
                     iso_restraints,
                     refine_xyz = False,
                     refine_adp = False,
                     refine_occ = False):
    adopt_init_args(self, locals())
    self.ex = []
    self.en = []
    self.er = []
    self.et = []
    self.rxw = []
    self.rxf = []
    self.rnw = []
    self.rnf = []
    self.iter = None
    self.nfun = None

  def collect(self, iter = None, nfun = None):
    if(iter is not None): self.iter = format_value("%-4d", iter)
    if(nfun is not None): self.nfun = format_value("%-4d", nfun)
    self.fmodels.fmodel_xray().xray_structure == self.model.get_xray_structure()
    fmodels_tg = self.fmodels.target_and_gradients(
      weights           = self.weights,
      compute_gradients = False)
    self.rxw.append(format_value("%6.4f", self.fmodels.fmodel_xray().r_work()))
    self.rxf.append(format_value("%6.4f", self.fmodels.fmodel_xray().r_free()))
    self.ex.append(format_value("%10.4f", fmodels_tg.target_work_xray))
    if(self.fmodels.fmodel_neutron() is not None):
      self.rnw.append(format_value("%6.4f", self.fmodels.fmodel_neutron().r_work()))
      self.rnf.append(format_value("%6.4f", self.fmodels.fmodel_neutron().r_free()))
      self.en.append(format_value("%10.4f", fmodels_tg.target_work_neutron))
    if(self.refine_xyz and self.weights.w > 0):
      er = self.model.restraints_manager_energies_sites(
        compute_gradients = False).target
    elif(self.refine_adp and self.weights.w > 0):
      use_hd = False
      if(self.fmodels.fmodel_n is not None): use_hd = True
      er = self.model.energies_adp(
        iso_restraints = self.iso_restraints,
        use_hd = use_hd,
        compute_gradients = False).target
    elif(self.refine_occ):
      er = 0
    else: er = 0
    self.er.append(format_value("%10.4f", er))
    self.et.append(format_value(
      "%10.4f", fmodels_tg.target()+er*self.weights.w))

  def show(self, message = "", log = None):
    if(log is None): log = sys.stdout
    print("|-"+message+"-"*(79-len("| "+message+"|"))+"|", file=log)
    if(self.fmodels.fmodel_neutron() is not None):
      print("|"+" "*33+"x-ray data:"+" "*33+"|", file=log)
    print("| start r-factor (work) = %s      final r-factor "\
     "(work) = %s          |"%(self.rxw[0], self.rxw[1]), file=log)
    print("| start r-factor (free) = %s      final r-factor "\
     "(free) = %s          |"%(self.rxf[0], self.rxf[1]), file=log)
    if(self.fmodels.fmodel_neutron() is not None):
      print("|"+" "*32+"neutron data:"+" "*32+"|", file=log)
      print("| start r-factor (work) = %s      final r-factor "\
      "(work) = %s          |"%(self.rnw[0], self.rnw[1]), file=log)
      print("| start r-factor (free) = %s      final r-factor "\
      "(free) = %s          |"%(self.rnf[0], self.rnf[1]), file=log)
    print("|"+"-"*77+"|", file=log)
    #
    if(self.fmodels.fmodel_neutron() is None):
      for i_seq, stage in enumerate(["start", "final"]):
        if(self.refine_xyz):
          print("| T_%s = wxc * wxc_scale * Exray + wc * Echem"%stage+" "*30+"|", file=log)
          print(self.xray_line(i_seq), file=log)
        elif(self.refine_adp):
          print("| T_%s = wxu * wxu_scale * Exray + wu * Eadp"%stage+" "*31+"|", file=log)
          print(self.xray_line(i_seq), file=log)
        elif(self.refine_occ):
          print("| T_%s = 1.0 * 1.0 * Exray"%stage+" "*43+"|", file=log)
          print(self.xray_line(i_seq), file=log)
        if(i_seq == 0): print("|"+" "*77+"|", file=log)
    #
    if(self.fmodels.fmodel_neutron() is not None):
      for i_seq, stage in enumerate(["start", "final"]):
        if(self.refine_xyz):
          print("| T_%s = wnxc * (wxc_scale * Exray + wnc_scale * Eneutron) + wc * Echem"%stage+" "*4+"|", file=log)
          print(self.neutron_line(i_seq), file=log)
        elif(self.refine_adp):
          print("| T_%s = wnxu * (wxu_scale * Exray + wnu_scale * Eneutron) + wu * Eadp"%stage+" "*5+"|", file=log)
          print(self.neutron_line(i_seq), file=log)
        elif(self.refine_occ):
          print("| T_%s = 1.0 * (1.0 * Exray + 1.0 * Eneutron)"%stage+" "*30+"|", file=log)
          print(self.neutron_line(i_seq), file=log)
        if(i_seq == 0): print("|"+" "*77+"|", file=log)
    #
    print("|"+"-"*77+"|", file=log)
    print("| number of iterations = %s    |    number of function "\
                  "evaluations = %s   |"%(self.iter, self.nfun), file=log)
    print("|"+"-"*77+"|", file=log)
    log.flush()

  def neutron_line(self, i_seq):
    line = "| %s = %s * (%s * %s + %s * %s) + %s * %s"%(
      self.et[i_seq].strip(),
      format_value("%6.2f",self.weights.wxn).strip(),
      format_value("%6.2f",self.weights.wx_scale).strip(),
      self.ex[i_seq].strip(),
      format_value("%6.2f",self.weights.wn_scale).strip(),
      self.en[i_seq].strip(),
      format_value("%6.2f",self.weights.w).strip(),
      self.er[i_seq].strip())
    line = line + " "*(78 - len(line))+"|"
    return line

  def xray_line(self, i_seq):
    line = "| %s = %s * %s * %s + %s * %s"%(
      self.et[i_seq].strip(),
      format_value("%6.2f",self.weights.wx).strip(),
      format_value("%6.2f",self.weights.wx_scale).strip(),
      self.ex[i_seq].strip(),
      format_value("%6.2f",self.weights.w).strip(),
      self.er[i_seq].strip())
    line = line + " "*(78 - len(line))+"|"
    return line

class run_constrained(object):
  def __init__(self,
               model,
               fmodel,
               target_weight,
               log,
               params,
               prefix,
               refine_sites           = False,
               refine_u_iso           = False,
               refine_transformations = False):
    #
    # XXX Same needs to be done for refine_u_iso
    #
    refine_selection = None
    if(refine_sites):
      refine_selection = model.refinement_flags.sites_individual.iselection()
    #
    tg_object = mmtbx.refinement.minimization_ncs_constraints.\
      target_function_and_grads_reciprocal_space(
        fmodel                    = fmodel,
        ncs_restraints_group_list = model.get_ncs_groups(),
        refine_selection          = refine_selection, # XXX
        restraints_manager        = model.restraints_manager,
        iso_restraints            = params.adp_restraints.iso,
        data_weight               = target_weight,
        refine_sites              = refine_sites,
        refine_u_iso              = refine_u_iso,
        refine_transformations    = refine_transformations)
    self.minimized = mmtbx.refinement.minimization_ncs_constraints.lbfgs(
       target_and_grads_object      = tg_object,
       xray_structure               = fmodel.xray_structure,
       ncs_restraints_group_list    = model.get_ncs_groups(),
       refine_selection             = refine_selection, # XXX
       finite_grad_differences_test = False,
       max_iterations               = params.main.max_number_of_iterations,
       refine_sites                 = refine_sites,
        refine_u_iso                = refine_u_iso,
        refine_transformations      = refine_transformations)
    fmodel.update_xray_structure(update_f_calc=True)
    model.set_xray_structure(fmodel.xray_structure)


 *******************************************************************************


 *******************************************************************************
mmtbx/refinement/minimization_monitor.py
from __future__ import absolute_import, division, print_function
from libtbx import adopt_init_args, Auto, group_args
from six.moves import range

class minimization_monitor(object):
  def __init__(self,
      number_of_cycles,
      max_number_of_cycles,
      cycles_to_converge=5,
      mode="simple_cycles"):
    adopt_init_args(self, locals())
    assert self.mode in ["simple_cycles", "min_outliers", "no_outliers"]
    self.current_cycle = 0
    self.cycles_params = []
    self.cycles_geometry = [] # mmtbx.model_statistics.geometry_no_grm

  def need_more_cycles(self):
    if self.current_cycle == 0:
      return True
    if self.current_cycle >= self.max_number_of_cycles:
      return False
    if self.mode == "simple_cycles" and self.current_cycle >= number_of_cycles:
      return False
    if self.number_of_cycles is not Auto:
      return self.current_cycle < self.number_of_cycles
    elif self.mode == "min_outliers":
      return self.geometry_improved() or not self.geometry_is_ok()
    elif self.mode == "no_outliers":
      return not (self.geometry_is_perfect() or self.converged())
    return False

  def save_cycle_results(self, geometry=None):
    self.current_cycle += 1
    if geometry is None:
      assert self.mode == "simple_cycles", "Need geometry validation for decision making in other running modes"
    else:
      rama = geometry.ramachandran()
      omega = geometry.omega()
      res = group_args(
          ramachandran_outliers = rama.outliers,
          n_twisted_general = omega.n_twisted_general,
          twisted_proline = omega.twisted_proline,
          twisted_general = omega.twisted_general,
          )
      self.cycles_geometry.append(res)

  def geometry_improved(self):
    if len(self.cycles_geometry) > 1:
      for geometry_param in ["ramachandran_outliers", "n_twisted_general"]:
        if getattr(self.cycles_geometry[-2], geometry_param) > getattr(self.cycles_geometry[-1], geometry_param):
          return True
    else:
      return False

  def get_current_cycle_n(self):
    return self.current_cycle

  def same_geometry(self, g1, g2): # g2 is previous
    for geometry_param in ["ramachandran_outliers", "n_twisted_general"]:
      if getattr(g2, geometry_param) - getattr(g1, geometry_param) > 0.1:
        return False
    return True

  def converged(self):
    if len(self.cycles_geometry) < self.cycles_to_converge:
      return False
    for i in range(1, self.cycles_to_converge):
      if not self.same_geometry(self.cycles_geometry[-i], self.cycles_geometry[-i-1]):
        return False
    return True

  def geometry_is_perfect(self):
    if len(self.cycles_geometry) == 0:
      return False
    else:
      if (self.cycles_geometry[-1].ramachandran_outliers < 0.01 and
          self.cycles_geometry[-1].twisted_general + self.cycles_geometry[-1].twisted_proline < 0.01):
        return True
    return False

  def geometry_is_ok(self):
    if len(self.cycles_geometry) == 0:
      return False
    else:
      if (self.cycles_geometry[-1].ramachandran_outliers < 2 or
          self.cycles_geometry[-1].twisted_general + self.cycles_geometry[-1].twisted_proline < 2):
        return True
    return False

  def need_weight_optimization(self):
    # currently once in 3 macro-cycles
    return True
    result = False
    if self.current_cycle == 0:
      result = True
    elif len(self.cycles_params) < 3:
      result = False
    else:
      result = True
      for i in range(1,4):
        if self.cycles_params[-i]["did_wo"]:
          result = False
    self.cycles_params.append({"did_wo": result})
    return result


 *******************************************************************************


 *******************************************************************************
mmtbx/refinement/minimization_ncs_constraints.py
from __future__ import absolute_import, division, print_function
from cctbx import maptbx
from scitbx.array_family import flex
from libtbx import adopt_init_args
import mmtbx.ncs.ncs_utils as nu
from cctbx import xray
import scitbx.lbfgs
from six.moves import zip

def grads_asu_to_one_ncs(
      ncs_restraints_group_list,
      extended_ncs_selection,
      grad,
      refine_sites):
  """
  Apply NCS constraints to reduce the gradients corresponding to whole ASU, to
  gradients corresponding to one NCS copy.

  Args:
    ncs_restraints_group_list: list of ncs_restraint_group objects
    grad: gradient of the complete asu
    refine_sites: (bool) Flag indicating sites refinement
    extended_ncs_selection (flex.siz_t): selection of all ncs groups master ncs
      selection and non ncs related portions that are being refined (exclude
      NCS copies)
  """
  for nrg in ncs_restraints_group_list:
    ncs_selection = nrg.master_iselection
    for ncs_copy in nrg.copies:
      asu_selection = ncs_copy.iselection
      g = grad.select(asu_selection)
      if(refine_sites):
        # apply inverse transformation
        rt = ncs_copy.r.transpose().elems
        g = (rt*g)
      grad.set_selected(ncs_selection, g + grad.select(ncs_selection))

  g_ncs = grad.select(extended_ncs_selection)
  if(refine_sites): g_ncs = flex.vec3_double(g_ncs)
  assert type(grad)==type(g_ncs)
  return g_ncs

def grads_one_ncs_to_asu(ncs_restraints_group_list,
                         total_asu_length,
                         extended_ncs_selection,
                         master_grad):
  """
  Expand average gradient of a single NCS to all ASU
  (only for u_iso refinement)

  Args:
    ncs_restraints_group_list: list of ncs_restraint_group objects
    total_asu_length (int): length of the complete ASU
    extended_ncs_selection (flex.size_t): selection of all ncs groups master ncs
      selection and non ncs related portions that are being refined (exclude
      NCS copies)
    master_grad: gradient of a single ncs copy (the master copy)
  """
  g_length = total_asu_length
  if isinstance(master_grad,flex.vec3_double):
    g = flex.vec3_double([(0.0,0.0,0.0)]*g_length)
  elif isinstance(master_grad,flex.double):
    g = flex.double([0.0]*g_length)
  else:
    raise TypeError('Non supported grad type')
  # update newly created flex.vec3 with master NCS info
  g.set_selected(extended_ncs_selection ,master_grad)
  # update newly created flex.vec3 with NCS copies
  for nrg in ncs_restraints_group_list:
    master_selection = nrg.master_iselection
    master_grad_portion = g.select(master_selection)
    for ncs_copy in nrg.copies:
      copy_selection = ncs_copy.iselection
      g.set_selected(copy_selection,master_grad_portion)
  return g.as_double()

def restraints_target_and_grads(
      ncs_restraints_group_list,
      restraints_manager,
      xray_structure,
      grad,
      x,
      refine_sites=False,
      refine_u_iso=False,
      refine_transformations=False,
      iso_restraints=None,
      use_hd=None):
  """
  Args:
    ncs_restraints_group_list: NCS operator list
    restraints_manager: (object)
    xray_structure: (object)
    grad: gradient without restraints
    x: refined parameter
    refine_sites: (bool) indicate refinement type
    refine_u_iso: (bool) indicate refinement type
    refine_transformations: (bool) indicate refinement type
    iso_restraints: (object) iso restraints parameters
    use_hd: (bool) Use hydrogen

  Returns:
    ef_target: (float) restraints target function value
    ef_grad: (flex.double or flex.vec3_double) restraints gradient
  """
  assert [refine_sites, refine_u_iso, refine_transformations].count(True)==1
  ef_grad = None
  ef = None
  if(restraints_manager is not None):
    if(refine_sites):
      ef = restraints_manager.energies_sites(
        sites_cart        = xray_structure.sites_cart(),
        compute_gradients = True)
      ef_grad = ef.gradients
    elif(refine_u_iso):
      ef = restraints_manager.energies_adp_iso(
        xray_structure    = xray_structure,
        parameters        = iso_restraints,
        use_u_local_only  = iso_restraints.use_u_local_only,
        use_hd            = use_hd,
        compute_gradients = True)
      ef_grad = ef.gradients
    elif(refine_transformations):
      ef = restraints_manager.energies_sites(
        sites_cart        = xray_structure.sites_cart(),
        compute_gradients = True)
      ef_grad = nu.compute_transform_grad(
        grad_wrt_xyz      = ef.gradients.as_double(),
        ncs_restraints_group_list = ncs_restraints_group_list,
        xyz_asu           = xray_structure.sites_cart(),
        x                 = x)
  if(ef is not None): return ef.target, ef_grad
  elif not grad: return 0, 0
  elif isinstance(grad,flex.vec3_double):
    return 0, flex.vec3_double([(0,0,0),]*len(grad))
  elif isinstance(grad,flex.double):
    return 0, flex.double((0,0,0)*(len(grad)//3))
  else: return None,None


class target_function_and_grads_geometry_minimization(object):
  """
  Target and gradients evaluator for geometry minimization (no data)
  """
  def __init__(
        self,
        xray_structure,
        ncs_restraints_group_list,
        refine_selection=None,
        use_ncs_constraints=True,
        restraints_manager=None,
        refine_sites=False,
        refine_transformations=False):
    adopt_init_args(self, locals())
    self.refine_selection = nu.get_refine_selection(
      refine_selection=self.refine_selection,
      number_of_atoms=self.xray_structure.sites_cart().size())
    self.extended_ncs_selection = ncs_restraints_group_list.get_extended_ncs_selection(
        refine_selection=self.refine_selection)
    self.unit_cell = self.xray_structure.unit_cell()

  def target_and_gradients(self,compute_gradients,xray_structure,x):
    self.xray_structure.set_sites_cart(sites_cart = xray_structure.sites_cart())
    g = flex.vec3_double(xray_structure.scatterers().size(), [0,0,0])
    t = [0]*xray_structure.scatterers().size()
    t_restraints, g_restraints = restraints_target_and_grads(
      restraints_manager     = self.restraints_manager,
      xray_structure         = self.xray_structure,
      ncs_restraints_group_list = self.ncs_restraints_group_list,
      refine_sites           = self.refine_sites,
      refine_transformations = self.refine_transformations,
      x                      = x,
      grad                   = g)
    t = t_restraints
    if(compute_gradients):
      g = g_restraints
      if not self.refine_transformations:
        if self.use_ncs_constraints:
          g = grads_asu_to_one_ncs(
            ncs_restraints_group_list = self.ncs_restraints_group_list,
            extended_ncs_selection    = self.extended_ncs_selection,
            grad                      = g,
            refine_sites              = self.refine_sites).as_double()
        else:
          g = g.as_double()
    return t, g

class target_function_and_grads_real_space(object):
  """
  Real-space target and gradients evaluator
  """
  def __init__(
        self,
        map_data,
        xray_structure,
        ncs_restraints_group_list,
        real_space_gradients_delta,
        refine_selection=None,
        use_ncs_constraints=True,
        restraints_manager=None,
        data_weight=None,
        refine_sites=False):
    adopt_init_args(self, locals())
    self.refine_selection = nu.get_refine_selection(
      refine_selection=self.refine_selection,
      number_of_atoms=self.xray_structure.sites_cart().size())
    self.extended_ncs_selection = ncs_restraints_group_list.get_extended_ncs_selection(
        refine_selection=self.refine_selection)
    self.unit_cell = self.xray_structure.unit_cell()
    # get selection to refine
    asu_size = xray_structure.scatterers().size()
    self.selection = flex.bool(asu_size, refine_selection)

  def data_target_and_grads(self, compute_gradients,x):
    """
    Args:
      compute_gradients: (bool) when True compute gradients
      x: refined parameters
    """
    g = None
    tg = maptbx.target_and_gradients_simple(
      unit_cell   = self.unit_cell,
      map_target  = self.map_data,
      sites_cart  = self.xray_structure.sites_cart(),
      delta       = self.real_space_gradients_delta,
      selection   = self.selection)
    t = tg.target()*(-1)
    if compute_gradients:
      if self.refine_sites:
        g = tg.gradients()*(-1)
    return t, g

  def target_and_gradients(self,compute_gradients,xray_structure,x):
    """
    Args:
      compute_gradients: (bool) when True compute gradients
      xray_structure: xray_structure object
      x: refined parameters
    """
    self.xray_structure.set_sites_cart(sites_cart = xray_structure.sites_cart())
    g = None
    t_data, g_data = self.data_target_and_grads(compute_gradients,x)
    t_restraints, g_restraints = restraints_target_and_grads(
      restraints_manager     = self.restraints_manager,
      xray_structure         = self.xray_structure,
      ncs_restraints_group_list = self.ncs_restraints_group_list,
      refine_sites           = self.refine_sites,
      x                      = x,
      grad                   = g_data)
    if(self.data_weight is None): self.data_weight=1. #XXX BAD!
    t = t_data*self.data_weight + t_restraints
    if(compute_gradients):
      g = g_data*self.data_weight + g_restraints
      if self.use_ncs_constraints:
        g = grads_asu_to_one_ncs(
          ncs_restraints_group_list = self.ncs_restraints_group_list,
          extended_ncs_selection    = self.extended_ncs_selection,
          grad                      = g,
          refine_sites              = self.refine_sites).as_double()
      else:
        g = g.as_double()
    return t, g

class target_function_and_grads_reciprocal_space(object):
  """
  Reciprocal-space target and gradients evaluator

  Args:
    refine_selection (flex.size_t): of all ncs related copies and
      non ncs related parts to be refined
  """
  def __init__(
        self,
        fmodel,
        ncs_restraints_group_list,
        refine_selection=None,
        use_ncs_constraints=True,
        restraints_manager=None,
        data_weight=None,
        refine_sites=False,
        refine_u_iso=False,
        refine_transformations=False,
        iso_restraints = None,
        use_hd         = False):
    adopt_init_args(self, locals())
    asu_size = self.fmodel.xray_structure.sites_cart().size()
    self.refine_selection = nu.get_refine_selection(
      refine_selection=self.refine_selection,
      number_of_atoms=asu_size)
    self.extended_ncs_selection = ncs_restraints_group_list.get_extended_ncs_selection(
        refine_selection=self.refine_selection)
    self.fmodel.xray_structure.scatterers().flags_set_grads(state=False)
    self.x_target_functor = self.fmodel.target_functor()
    self.xray_structure = self.fmodel.xray_structure
    if self.refine_sites:
      xray.set_scatterer_grad_flags(
        scatterers = self.fmodel.xray_structure.scatterers(),
        site       = True)
    elif self.refine_u_iso:
      xray.set_scatterer_grad_flags(
        scatterers = self.fmodel.xray_structure.scatterers(),
        u_iso      = True)
    elif self.refine_transformations:
      xray.set_scatterer_grad_flags(
        scatterers = self.fmodel.xray_structure.scatterers(),
        site       = True)

  def data_target_and_grads(self, compute_gradients, sites_cart, x):
    g = None
    tgx = self.x_target_functor(compute_gradients=compute_gradients)
    t = tgx.target_work()
    if compute_gradients:
      if self.refine_sites:
        g = flex.vec3_double(
          tgx.gradients_wrt_atomic_parameters(site=True).packed())
      elif self.refine_u_iso:
        g = tgx.gradients_wrt_atomic_parameters(u_iso=True)
      elif self.refine_transformations:
        grad_wrt_xyz = tgx.gradients_wrt_atomic_parameters(site=True).packed()
        g = nu.compute_transform_grad(
          grad_wrt_xyz      = grad_wrt_xyz,
          ncs_restraints_group_list = self.ncs_restraints_group_list,
          xyz_asu           = sites_cart,
          x                 = x)
    return t, g

  def target_and_gradients(self, compute_gradients, xray_structure, x):
    sites_cart = xray_structure.sites_cart()
    self.xray_structure.set_sites_cart(sites_cart=sites_cart)
    self.fmodel.update_xray_structure(
      xray_structure = self.xray_structure,
      update_f_calc  = True)
    g = None
    t_data, g_data = self.data_target_and_grads(
      compute_gradients=compute_gradients, x=x, sites_cart=sites_cart)
    t_restraints, g_restraints =  restraints_target_and_grads(
      restraints_manager     = self.restraints_manager,
      xray_structure         = self.xray_structure,
      ncs_restraints_group_list = self.ncs_restraints_group_list,
      refine_sites           = self.refine_sites,
      refine_u_iso           = self.refine_u_iso,
      refine_transformations = self.refine_transformations,
      x                      = x,
      iso_restraints         = self.iso_restraints,
      use_hd                 = self.use_hd,
      grad                   = g_data)
    if(self.data_weight is None): self.data_weight=1.
    if(self.refine_transformations): # no restraints
      t = t_data
    else:
      t = t_data*self.data_weight + t_restraints
    if(compute_gradients):
      if(self.refine_transformations): # no restraints
        g = g_data
      else:
        g = g_data*self.data_weight + g_restraints
      if(not self.refine_transformations):
        if self.use_ncs_constraints:
          g = grads_asu_to_one_ncs(
            ncs_restraints_group_list = self.ncs_restraints_group_list,
            extended_ncs_selection    = self.extended_ncs_selection,
            grad                      = g,
            refine_sites              = self.refine_sites).as_double()
        else:
          g = g.as_double()
    return t, g

  def finalize(self):
    self.fmodel.xray_structure.tidy_us()
    self.fmodel.xray_structure.apply_symmetry_sites()
    self.fmodel.update_xray_structure(
      xray_structure = self.fmodel.xray_structure,
      update_f_calc  = True,
      update_f_mask  = True)

class lbfgs(object):
  def __init__(self,
        ncs_restraints_group_list,
        target_and_grads_object,
        xray_structure,
        refine_selection             = None,
        finite_grad_differences_test = False,
        finite_grad_difference_val   = 0,
        max_iterations               = 35,
        refine_sites                 = False,
        refine_u_iso                 = False,
        refine_transformations       = False):
    """
    NCS constrained ADP and coordinates refinement. Also refines NCS operators.
    """
    adopt_init_args(self, args=locals(),exclude=['ncs_restraints_group_list'])
    self.x_previous = None
    self.refine_selection = nu.get_refine_selection(
      refine_selection=self.refine_selection,
      number_of_atoms=self.xray_structure.sites_cart().size())
    self.use_ncs_constraints = target_and_grads_object.use_ncs_constraints
    self.ncs_restraints_group_list = ncs_restraints_group_list.deep_copy()
    self.ncs_groups_coordinates_centers = []
    self.extended_ncs_selection = self.ncs_restraints_group_list.get_extended_ncs_selection(
        refine_selection=self.refine_selection)
    assert [self.refine_sites,
            self.refine_u_iso, self.refine_transformations].count(True) == 1
    self.total_asu_length = len(xray_structure.sites_cart())
    traditional_convergence_test_eps = 1.0e-6
    if self.use_ncs_constraints:
      xray_structure_one_ncs_copy = xray_structure.select(
        self.extended_ncs_selection)
    else:
      xray_structure_one_ncs_copy = xray_structure.select(self.refine_selection)
    if self.refine_sites:
      self.x = xray_structure_one_ncs_copy.sites_cart().as_double()
    elif self.refine_u_iso:
      assert xray_structure_one_ncs_copy.scatterers().size() == \
        xray_structure_one_ncs_copy.use_u_iso().count(True)
      self.x = xray_structure_one_ncs_copy.extract_u_iso_or_u_equiv()
    elif self.refine_transformations:
      # move refinable parameters to coordinate center
      self.ncs_groups_coordinates_centers = self.ncs_restraints_group_list.get_ncs_groups_centers(
          sites_cart=self.xray_structure.sites_cart())
      self.ncs_restraints_group_list = self.ncs_restraints_group_list.shift_translation_to_center(
          shifts = self.ncs_groups_coordinates_centers)
      self.x = self.ncs_restraints_group_list.concatenate_rot_tran()
    lbfgs_core_params = scitbx.lbfgs.core_parameters(
      stpmax = 25.0)
    minimizer = scitbx.lbfgs.run(
      target_evaluator=self,
      core_params=lbfgs_core_params,
      termination_params=scitbx.lbfgs.termination_parameters(
        max_iterations=max_iterations,
        traditional_convergence_test_eps=traditional_convergence_test_eps),
      exception_handling_params=scitbx.lbfgs.exception_handling_parameters(
        ignore_line_search_failed_rounding_errors=True,
        ignore_line_search_failed_step_at_lower_bound=True,
        ignore_line_search_failed_step_at_upper_bound=True,
        ignore_line_search_failed_maxfev=True))
    # change transforms to the original coordinate system
    if self.refine_transformations:
      self.ncs_restraints_group_list = self.ncs_restraints_group_list.shift_translation_back_to_place(
          shifts = self.ncs_groups_coordinates_centers)
    if(getattr(self.target_and_grads_object, "finalize", None)):
      self.target_and_grads_object.finalize()
    # pass the refined ncs_restraints_group_list to original object
    for g1,g2 in zip(ncs_restraints_group_list,self.ncs_restraints_group_list):
      for tr1,tr2 in zip(g1.copies,g2.copies):
        tr1.r = tr2.r
        tr1.t = tr2.t

  def compute_functional_and_gradients(self, compute_gradients=True):
    x_current = self.x
    if(self.x_previous is None):
      self.x_previous = x_current.deep_copy()
    else:
      xray.ext.damp_shifts(previous=self.x_previous, current=x_current,
        max_value=10.)
      self.x_previous = x_current.deep_copy()

    t,g = self.target_and_grads_object.target_and_gradients(
      compute_gradients=compute_gradients,
      xray_structure = self.update_xray_structure(),
      x=self.x)
    if(self.finite_grad_differences_test and compute_gradients):
      self.finite_difference_test(g)
    return t, g

  def update_xray_structure(self, x=None):
    """
    Update xray_structure with refined parameters, then update
    fmodel object with updated xray_structure.
    """
    if not x: x = self.x
    if self.refine_transformations:
      # update the ncs_restraint_groups transforms
      self.ncs_restraints_group_list.update_rot_tran(x=x)
      # Use the new transformations to create the ASU
      x_ncs = self.xray_structure.sites_cart().\
          select(self.extended_ncs_selection).as_double()
      x_asu = self.refinable_params_one_ncs_to_asu(x_ncs)
      self.xray_structure.set_sites_cart(
        sites_cart = flex.vec3_double(x_asu))
    elif self.refine_sites:
      x_asu = self.refinable_params_one_ncs_to_asu(x)
      self.xray_structure.set_sites_cart(
        sites_cart = flex.vec3_double(x_asu))
    elif self.refine_u_iso:
      x_asu = self.refinable_params_one_ncs_to_asu()
      self.xray_structure.set_u_iso(values = x_asu)
    return self.xray_structure

  def refinable_params_one_ncs_to_asu(self, x=None):
    """
    Expand refinabale parameters corresponding to one NCS copy to parameters
    corresponding to whole ASU.
    """
    x_old = self.xray_structure.sites_cart()
    if not x : x = self.x
    if self.refine_sites or self.refine_transformations:
      if self.use_ncs_constraints or self.refine_transformations:
        new_x = nu.apply_transforms(
          ncs_coordinates = flex.vec3_double(x),
          ncs_restraints_group_list = self.ncs_restraints_group_list,
          total_asu_length = x_old.size(),
          extended_ncs_selection = self.extended_ncs_selection,
          round_coordinates = False,
          center_of_coordinates = self.ncs_groups_coordinates_centers)
        new_x = new_x.select(self.refine_selection)
        new_x = x_old.set_selected(self.refine_selection,new_x)
      else:
        new_x = self.x
      return new_x.as_double()
    elif self.refine_u_iso:
      if self.use_ncs_constraints:
        return grads_one_ncs_to_asu(
          ncs_restraints_group_list = self.ncs_restraints_group_list,
          extended_ncs_selection = self.extended_ncs_selection,
          total_asu_length = x_old.size(),
          master_grad = x)
      else:
        return flex.double(list(x))

  def finite_difference_test(self,g):
    """
    Compare analytical and finite differences gradients.

    finite_grad_difference_val = abs(analytical - finite differences)
    """
    g = g.as_double()
    # find the index of the max gradient value
    i_g_max = flex.max_index(flex.abs(g))
    # Set displacement for finite gradient calculation
    d = max(self.x[i_g_max]*1e-6,1e-6)
    # calc t(x+d)
    self.x[i_g_max] += d
    t1,_ = self.compute_functional_and_gradients(compute_gradients=False)
    # calc t(x-d)
    self.x[i_g_max] -= 2*d
    t2,_ = self.compute_functional_and_gradients(compute_gradients=False)
    # Return fmodel to the correct coordinates values
    self.x[i_g_max] += d
    self.update_xray_structure()
    finite_gard = (t1-t2)/(d*2)
    self.finite_grad_difference_val = abs(g[i_g_max] - finite_gard)


 *******************************************************************************


 *******************************************************************************
mmtbx/refinement/occupancies.py

from __future__ import absolute_import, division, print_function
from mmtbx.utils import get_atom_selections
import mmtbx.utils
from cctbx.array_family import flex
from scitbx import lbfgs
from libtbx.str_utils import format_value, make_sub_header
from libtbx.utils import Sorry, null_out
from libtbx import adopt_init_args
from six.moves import range

class manager(object):
  def __init__(self, fmodels,
                     model,
                     max_number_of_iterations    = 25,
                     number_of_macro_cycles      = 3,
                     occupancy_max               = None,
                     occupancy_min               = None,
                     log                         = None,
                     exclude_hd                  = False):
    self.show(fmodels=fmodels, log= log, message="occupancy refinement: start")
    fmodels.update_xray_structure(xray_structure = model.get_xray_structure(),
                                  update_f_calc  = True)
    selections = model.refinement_flags.s_occupancies
    # exclude H or D from refinement if requested
    if(exclude_hd):
      hd_sel = model.get_hd_selection()
      tmp_sel = []
      for sel in selections:
        tmp_sel_ = []
        for sel_ in sel:
          tmp_sel__ = flex.size_t()
          for sel__ in sel_:
            if(not hd_sel[sel__]):
              tmp_sel__.append(sel__)
          if(tmp_sel__.size()>0):
            tmp_sel_.append(tmp_sel__)
        if(len(tmp_sel_)>0):
          tmp_sel.append(tmp_sel_)
      selections = tmp_sel
    #
    if(len(selections)>0):
      i_selection = flex.size_t()
      for s in selections:
        for ss in s:
          i_selection.extend(ss)
      fmodels.fmodel_xray().xray_structure.scatterers().flags_set_grads(
        state=False)
      fmodels.fmodel_xray().xray_structure.scatterers().flags_set_grad_occupancy(
        iselection = i_selection)
      fmodels.fmodel_xray().xray_structure.adjust_occupancy(
        occ_max   = occupancy_max,
        occ_min   = occupancy_min,
        selection = i_selection)
      xray_structure_dc = fmodels.fmodel_xray().xray_structure.\
        deep_copy_scatterers()
      par_initial = flex.double()
      occupancies = xray_structure_dc.scatterers().extract_occupancies()
      constrained_groups_selections = []
      group_counter = 0
      for sel in selections:
        ss = []
        for sel_ in sel:
          ss.append(group_counter)
          group_counter += 1
          val = flex.mean(occupancies.select(sel_))
          par_initial.append(val)
        constrained_groups_selections.append(ss)
      minimized = None
      for macro_cycle in range(number_of_macro_cycles):
        if(minimized is not None): par_initial = minimized.par_min
        minimized = minimizer(
          fmodels                       = fmodels,
          selections                    = selections,
          constrained_groups_selections = constrained_groups_selections,
          par_initial                   = par_initial,
          max_number_of_iterations      = max_number_of_iterations)
        if(minimized is not None): par_initial = minimized.par_min
        set_refinable_parameters(
          xray_structure     = fmodels.fmodel_xray().xray_structure,
          parameters         = par_initial,
          selections         = selections,
          enforce_positivity = (occupancy_min>=0))
        fmodels.fmodel_xray().xray_structure.adjust_occupancy(
          occ_max   = occupancy_max,
          occ_min   = occupancy_min,
          selection = i_selection)
      xray_structure_final = fmodels.fmodel_xray().xray_structure
      model.set_xray_structure(xray_structure_final)
      fmodels.update_xray_structure(xray_structure = xray_structure_final,
                                    update_f_calc  = True)
      refined_occ = xray_structure_final.scatterers().extract_occupancies().\
        select(i_selection)
      assert flex.min(refined_occ) >= occupancy_min
      assert flex.max(refined_occ) <= occupancy_max
      self.show(fmodels= fmodels, log = log, message="occupancy refinement: end")

  def show(self, fmodels, message, log):
    if(log is not None):
      print("|-"+message+"-"*(79-len("|-"+message+"|"))+"|", file=log)
      fm_x, fm_n = fmodels.fmodel_xray(), fmodels.fmodel_neutron()
      if(fm_n is not None):
        print("|"+" "*36+"X-ray"+" "*36+"|", file=log)
      self.show_helper(fmodel = fm_x, log = log)
      if(fm_n is not None):
        print("|"+" "*35+"neutron"+" "*35+"|", file=log)
        self.show_helper(fmodel = fm_n, log = log)
      occupancies = fm_x.xray_structure.scatterers().extract_occupancies()
      occ_max = format_value("%4.2f", flex.max(occupancies))
      occ_min = format_value("%4.2f", flex.min(occupancies))
      number_small = format_value("%8d", (occupancies < 0.1).count(True))
      print("| occupancies: max = %s  min = %s   number of occupancies < 0.1: %s |"%(
        occ_max, occ_min, number_small), file=log)
      print("|"+"-"*77+"|", file=log)

  def show_helper(self, fmodel, log):
    r_work = format_value("%6.4f", fmodel.r_work())
    r_free = format_value("%6.4f", fmodel.r_free())
    target = format_value("%-13.3f", fmodel.target_w())
    target_name = format_value("%s", fmodel.target_name)
    p1 = "| r_work = %s r_free = %s" % (r_work, r_free)
    p2 = "target_work(%s) = %s |" % (target_name, target)
    print(p1+" "*(79-len(p1+p2))+p2, file=log)

class minimizer(object):
  def __init__(self,
               fmodels,
               constrained_groups_selections,
               selections,
               par_initial,
               max_number_of_iterations):
    adopt_init_args(self, locals())
    self.fmodels.create_target_functors()
    self.fmodels.prepare_target_functors_for_minimization()
    from mmtbx.refinement import weights
    self.weights = weights.weights(wx = 1, wx_scale = 1, w = 0)
    self.par_min = self.par_initial.deep_copy()
    self.x = self.pack(self.par_min)
    self.n = self.x.size()
    self.minimizer = lbfgs.run(
    target_evaluator = self,
    termination_params = lbfgs.termination_parameters(
      max_iterations = max_number_of_iterations),
    exception_handling_params = lbfgs.exception_handling_parameters(
      ignore_line_search_failed_step_at_lower_bound = True,
      ignore_line_search_failed_step_at_upper_bound = True))
    self.compute_functional_and_gradients()
    del self.x

  def pack(self, par):
    return pack_unpack(x = par, table = self.constrained_groups_selections)

  def unpack_x(self):
    self.par_min = pack_unpack(x = self.x,
      table = self.constrained_groups_selections)

  def compute_functional_and_gradients(self):
    self.unpack_x()
    set_refinable_parameters(
      xray_structure = self.fmodels.fmodel_xray().xray_structure,
      parameters     = self.par_min,
      selections     = self.selections)
    self.fmodels.update_xray_structure(update_f_calc = True)
    fmodels_target_and_gradients = self.fmodels.target_and_gradients(
      weights           = self.weights,
      compute_gradients = True,
      occupancy         = True)
    self.f = fmodels_target_and_gradients.target()
    g =  fmodels_target_and_gradients.gradients()
    # do group grads first then pack for constraints
    grads_wrt_par = flex.double()
    for sel in self.selections:
      for sel_ in sel:
        grads_wrt_par.append(flex.sum( g.select(sel_) ))
    # now apply constraints
    self.g = pack_gradients(x = grads_wrt_par,
      table = self.constrained_groups_selections)
    return self.f, self.g

def set_refinable_parameters(xray_structure, parameters, selections,
                             enforce_positivity=False):
  # XXX PVA: Code below is terribly inefficient and MUST be moved into C++
  sz = xray_structure.scatterers().size()
  i = 0
  for sel in selections:
    # pre-check for positivity begin
    # spread negative occupancies across i_seqs having positive ones
    par_all = flex.double()
    par_neg = flex.double()
    i_p = i
    for sel_ in sel:
      p = parameters[i_p]
      par_all.append(p)
      if(p<0): par_neg.append(p)
      i_p += 1
    if(enforce_positivity and par_neg.size()>0):
      par_all = par_all - flex.min(par_all)
      fs = flex.sum(par_all)
      if(fs != 0):
        par_all = par_all / fs
    # pre-check for positivity end
    for j, sel_ in enumerate(sel):
      sel__b = flex.bool(sz, flex.size_t(sel_))
      xray_structure.set_occupancies(par_all[j], sel__b)
      i+=1

def pack_unpack(x, table):
  result = x.deep_copy()
  for indices in table :
    if (len(indices) == 1):
      i0 = indices[0]
      result[i0] = x[i0]
    else :
      xsum = 0
      for i in indices[0:-1] :
        result[i] = x[i]
        xsum += x[i]
      result[indices[-1]] = 1. - xsum
  return result

def pack_gradients(x, table):
  result = flex.double(x.size(), 0)
  for indices in table :
    if(len(indices) == 1):
      i0 = indices[0]
      result[i0] = x[i0]
    elif(len(indices) == 2):
      i0,i1 = indices
      result[i0] = x[i0] - x[i1]
      result[i1] =-x[i1] # ??? zero or this value?
    else :
      for i in indices[0:-1] :
        result[i] = x[i] - x[indices[-1]]
      result[indices[-1]] = 0
  return result

#-----------------------------------------------------------------------
# SELECTION HANDLING

def list_3d_as_bool_selection(list_3d, size):
  result = flex.bool(size, False)
  for i in list_3d:
    for j in i:
      for k in j:
        if (result[k]):
          raise Sorry("Duplicate selection for occupancies.")
        result[k] = True
  return result

def add_occupancy_selection(result, size, selection, hd_special=None):
  result_as_1d_array_b = list_3d_as_bool_selection(list_3d=result, size=size)
  sel_b = selection
  if(isinstance(selection, flex.size_t)):
    sel_b = flex.bool(size, selection)
  if(hd_special is not None):
    not_common = ((sel_b != result_as_1d_array_b) & (sel_b == True))
    not_common_ = ((not_common != hd_special) & (not_common == True)).iselection()
    not_common = not_common_
  else:
    not_common = ((sel_b != result_as_1d_array_b) & (sel_b == True)).iselection()
  sel_checked = []
  for i in not_common:
    sel_checked.append([[i]])
  if(len(sel_checked) > 0):
    result.extend(sel_checked)
  return result

def extract_partial_occupancy_selections(hierarchy):
  result = []
  for model in hierarchy.models():
    for chain in model.chains():
      for residue_group in chain.residue_groups():
        if(not residue_group.have_conformers()):
          assert len(residue_group.atom_groups()) == 1
          occs = flex.double()
          i_seqs = []
          for atom in residue_group.atoms():
            occs.append(atom.occ)
            i_seqs.append(atom.i_seq)
          if(occs[0]<1 and occs[0]!=0 and occs.all_eq(occs[0]) and occs.size()>1):
            result.append([i_seqs])
  return result

def occupancy_selections(
      model,
      add_water                          = False,
      other_individual_selection_strings = None,
      other_constrained_groups           = None,
      remove_selection                   = None,
      as_flex_arrays                     = True,
      constrain_correlated_3d_groups     = False,
      log                                = None):
  # set up defaults
  if(other_individual_selection_strings is not None and
     len(other_individual_selection_strings) == 0):
    other_individual_selection_strings = None
  if(other_constrained_groups is not None and
     len(other_constrained_groups) == 0):
    other_constrained_groups = None
  if(remove_selection is not None and len(remove_selection) == 0):
    remove_selection = None
  result = model.get_hierarchy().occupancy_groups_simple(
    common_residue_name_class_only = None,
    always_group_adjacent          = False,
    ignore_hydrogens               = False)
  exchangable_hd_pairs = model.get_hierarchy().exchangeable_hd_selections()
  if(len(exchangable_hd_pairs)==0 and result is not None):
    occupancy_regroupping(
      pdb_hierarchy = model.get_hierarchy(),
      cgs           = result)
  result = mmtbx.utils.remove_selections(selection = result,
    other = exchangable_hd_pairs,
    size = model.get_number_of_atoms())
  result.extend(exchangable_hd_pairs)
  # extract group-[0,1]-constrained atoms withing a residue
  pogl = extract_partial_occupancy_selections(hierarchy = model.get_hierarchy())
  rm_duplicate_with_pogl = []
  for t_ in pogl:
    for t__ in t_:
      for t___ in t__:
        rm_duplicate_with_pogl.append(t___)
  result = mmtbx.utils.remove_selections(selection = result, other = pogl,
    size = model.get_number_of_atoms())
  result.extend(pogl)
  # add partial occupancies
  occupancies = model.get_xray_structure().scatterers().extract_occupancies()
  sel = (occupancies != 1.) & (occupancies != 0.)
  result = add_occupancy_selection(
    result     = result,
    size       = model.get_number_of_atoms(),
    selection  = sel,
    hd_special = None)
  # check user's input
  all_sel_strgs = []
  if(other_individual_selection_strings is not None):
    all_sel_strgs = all_sel_strgs + other_individual_selection_strings
  if(other_constrained_groups is not None):
    for other_constrained_group in other_constrained_groups:
      for other_constrained_group in other_constrained_groups:
        if(len(other_constrained_group.selection)>0):
          all_sel_strgs = all_sel_strgs + other_constrained_group.selection
  if(len(all_sel_strgs) > 0):
    for sel_str in all_sel_strgs:
      sel_str_sel = get_atom_selections(
        model               = model,
        selection_strings   = [sel_str],
        iselection          = True,
        one_selection_array = True)
      if(sel_str_sel.size() == 0):
        raise Sorry("Empty selection: %s"%sel_str)
  #
  if([other_individual_selection_strings,
      other_constrained_groups].count(None) == 0):
    sel1 = get_atom_selections(
      model               = model,
      selection_strings   = other_individual_selection_strings,
      iselection          = True,
      one_selection_array = True)
    for other_constrained_group in other_constrained_groups:
      for other_constrained_group in other_constrained_groups:
        for cg_sel_strs in other_constrained_group.selection:
          sel2 = get_atom_selections(
            model               = model,
            selection_strings   = cg_sel_strs,
            iselection          = True,
            one_selection_array = True)
          if(sel1.intersection(sel2).size() > 0):
            raise Sorry("Duplicate selection: same atoms selected for individual and group occupancy refinement.")
  # check user's input and apply remove_selection to default selection
  if(remove_selection is not None):
    sel1 = get_atom_selections(
      model               = model,
      selection_strings   = remove_selection,
      iselection          = True,
      one_selection_array = True)
    if(sel1.size() == 0): # XXX check all and not total.
      raise Sorry("Empty selection: remove_selection.")
    if(other_individual_selection_strings is not None):
      sel2 = get_atom_selections(
        model               = model,
        selection_strings   = other_individual_selection_strings,
        iselection          = True,
        one_selection_array = True)
      if(sel1.intersection(sel2).size() > 0):
        raise Sorry("Duplicate selection: occupancies of same atoms selected to be fixed and to be refined.")
    if(other_constrained_groups is not None):
      for other_constrained_group in other_constrained_groups:
        for cg_sel_strs in other_constrained_group.selection:
          sel2 = get_atom_selections(
            model               = model,
            selection_strings   = cg_sel_strs,
            iselection          = True,
            one_selection_array = True)
          if(sel1.intersection(sel2).size() > 0):
            raise Sorry("Duplicate selection: occupancies of same atoms selected to be fixed and to be refined.")
    result = mmtbx.utils.remove_selections(selection = result, other = sel1,
      size = model.get_number_of_atoms())
  #
  if(other_individual_selection_strings is not None):
    sel = get_atom_selections(
      model               = model,
      selection_strings   = other_individual_selection_strings,
      iselection          = True,
      one_selection_array = True)
    result = mmtbx.utils.remove_selections(selection = result, other = sel,
      size = model.get_number_of_atoms())
    result = add_occupancy_selection(
      result     = result,
      size       = model.get_number_of_atoms(),
      selection  = sel,
      hd_special = None)
  if(other_constrained_groups is not None):
    for other_constrained_group in other_constrained_groups:
      cg_sel = []
      for cg_sel_strs in other_constrained_group.selection:
        sel = get_atom_selections(
          model               = model,
          selection_strings   = cg_sel_strs,
          iselection          = True,
          one_selection_array = True)
        result = mmtbx.utils.remove_selections(selection = result, other = sel,
          size = model.get_number_of_atoms())
        if(sel.size() > 0):
          cg_sel.append(list(sel))
      if(len(cg_sel) > 0):
        result.append(cg_sel)

  if (constrain_correlated_3d_groups) and (len(result) > 0):
      result = assemble_constraint_groups_3d(
        xray_structure=model.get_xray_structure(),
        pdb_atoms=model.get_atoms(),
        constraint_groups=result,
        log=log)

  if(add_water):
    water_selection = get_atom_selections(
      model                 = model,
      selection_strings     = ['water'],
      iselection            = True,
      allow_empty_selection = True,
      one_selection_array   = True)
    if(remove_selection is not None):
      sel_rm = get_atom_selections(
        model               = model,
        selection_strings   = remove_selection,
        iselection          = True,
        one_selection_array = True)
      if water_selection is not None:
        water_selection = flex.size_t(
          list(set(water_selection).difference(sel_rm)))
    if water_selection.size()>0:
      def flatten(lst):
        flat_list = []
        for item in lst:
            if isinstance(item, list):
                flat_list.extend(flatten(item))
            else:
                flat_list.append(item)
        return flat_list
      occ_groups_of_more_than_one = []
      for g in result:
        if len(g)>1:
          occ_groups_of_more_than_one.extend( flatten(g) )
      water_selection = list(water_selection)
      wocc = model.get_hierarchy().atoms().extract_occ()
      wsel = model.solvent_selection().iselection()
      wremove = []
      for i in wsel:
         if wocc[i]<1.e-6:
           water_selection.remove(i)
         if i in occ_groups_of_more_than_one:
           water_selection.remove(i)
      result = add_occupancy_selection(
        result     = result,
        size       = model.get_number_of_atoms(),
        selection  = flex.size_t(water_selection),
        hd_special = None)

  list_3d_as_bool_selection(
    list_3d=result, size=model.get_number_of_atoms())

  if(len(result) == 0): result = None
  if(as_flex_arrays and result is not None):
    result_ = []
    for gsel in result:
      result__ = []
      for sel in gsel:
        result__.append(flex.size_t(sel))
      result_.append(result__)
    result = result_

  return result

def occupancy_regroupping(pdb_hierarchy, cgs):
  h = pdb_hierarchy
  awl = list(h.atoms_with_labels())
  elements = h.atoms().extract_element()
  rgs = list(h.residue_groups())
  for cg in cgs: # loop over constraint groups
    for c in cg: # loop over conformers of constrained group
      altloc_h = awl[c[0]].altloc
      ind=None
      for ind_ in c:
        if(elements[ind_].strip().upper() in ["H","D"] and
           awl[ind_].name.strip().upper() in ["H","D"] and
           altloc_h.strip() != ""):
          ind = ind_
          break
      if(ind is not None):
        # find "-1" (previous to given constraint group) residue group
        rg_prev = None
        for i_rg, rg in enumerate(rgs):
          if(i_rg > 0 and ind in rg.atoms().extract_i_seq()):
            rg_prev = rgs[i_rg-1]
            break
        if(rg_prev is None): continue
        rg_prev_i_seqs = rg_prev.atoms().extract_i_seq()
        rg_i_seqs = rg.atoms().extract_i_seq()
        # find i_seq of C of previous residue
        i_seqs_c_prev=[]
        for a_prev in rg_prev.atoms():
          if(a_prev.name.strip().upper()=="C"):
            i_seqs_c_prev.append(a_prev.i_seq)
        if(i_seqs_c_prev == []): continue
        # find constarint group corresponding to rg_prev
        cg_prev=None
        for cg2 in cgs:
          for c2 in cg2:
            i_seqs_inter = set(c2).intersection(set(rg_prev_i_seqs))
            if(len(i_seqs_inter)>1 or
               (len(i_seqs_inter)==1 and not
                awl[list(i_seqs_inter)[0]].name.strip().upper() in ["H","D"])):
              for cg2_ in cg2:
                for i_seq_c_prev in i_seqs_c_prev:
                  if(i_seq_c_prev in cg2_):
                    cg_prev = cg2
                    break
          if(cg_prev is not None): break
        if(cg_prev is None): continue
        # identify to which constraint group H belongs and move it there
        for cg2 in cgs:
          for c2 in cg2:
            if(ind in c2):
              c2.remove(ind)
              break
        found = False
        for conformer_prev in rg_prev.conformers():
          if(conformer_prev.altloc == altloc_h):
            conformer_prev_i_seqs = conformer_prev.atoms().extract_i_seq()
            for cg2 in cgs:
              for c2 in cg2:
                i_seqs_inter = set(c2).intersection(set(conformer_prev_i_seqs))
                if(len(i_seqs_inter)>1 or
                   (len(i_seqs_inter)==1 and not
                    awl[list(i_seqs_inter)[0]].name.strip().upper() in ["H","D"])):
                  for i_seq_c_prev in i_seqs_c_prev:
                    if(i_seq_c_prev in c2):
                      c2.append(ind)
                      found = True
  for cg in cgs:
    while cg.count([None])>0:
      cg.remove([None])
    while cg.count([])>0:
      cg.remove([])
  while cgs.count([])>0:
    cgs.remove([])

def assemble_constraint_groups_3d(
    xray_structure,
    pdb_atoms,
    constraint_groups,
    interaction_distance_cutoff=4.0,
    verbose=False,
    log=None):
  """
  Re-sorts occupancy constraint groups so that conformers whose motion is
  correlated (i.e. they interact in 3D, without necessarily being part of
  the same fragment/molecule/ASU) are grouped together.  As input, it expects
  the constraint groups output by mmtbx.utils.occupancy_selections(), which
  will already have connectivity taken into account.  This function will exit
  with an error if the occupancies for the new groups are not consistent.
  """
  if (log is None):
    log = null_out()
  make_sub_header("Correlated occupancy grouping", out=log)
  print("""
  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  !!                  WARNING - EXPERIMENTAL FEATURE                        !!
  !!                                                                        !!
  !! Grouping of occupancy constraints in 3D is experimental and not fully  !!
  !! tested.  Use at your own risk!  For bug reports, etc. contact us by    !!
  !! email at help@phenix-online.org.                                       !!
  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
""", file=log)
  occupancies = pdb_atoms.extract_occ()
  pair_asu_table = xray_structure.pair_asu_table(
    distance_cutoff=interaction_distance_cutoff)
  pair_sym_table = pair_asu_table.extract_pair_sym_table()
  k = 0
  n_groups_start = len(constraint_groups)
  while (k < len(constraint_groups)):
    groups = constraint_groups[k]
    print("Constraint group %d: %d conformers" % (k+1, len(groups)), file=log)
    merge_constraints = []
    for i_sel, selection in enumerate(groups):
      occ = occupancies.select(selection)
      altloc = pdb_atoms[selection[0]].fetch_labels().altloc
      print("  conformer '%s': %d atoms" % (altloc, len(selection)), file=log)
      if (not occ.all_eq(occ[0])):
        raise Sorry("At least one occupancy constraint group has "+
          "inconsistent occupancies for atoms in a single conformer.  To use "+
          "the automatic 3D constraints, the starting occupancies must be "+
          "uniform within each selection.")
      for i_seq in selection :
        labels = pdb_atoms[i_seq].fetch_labels()
        if (labels.altloc.strip() == ''):
          continue
        pair_sym_dict = pair_sym_table[i_seq]
        if (verbose):
          print("%s (group %d):" % (pdb_atoms[i_seq].id_str(), k+1))
        for j_seq, sym_ops in pair_sym_dict.items():
          kk = k + 1
          while (kk < len(constraint_groups)):
            combine_group = False
            for other_selection in constraint_groups[kk] :
              if (j_seq in other_selection):
                if (verbose):
                  print("  %s (group %d)" % (pdb_atoms[j_seq].id_str(), kk+1))
                merge_constraints.append(constraint_groups[kk])
                del constraint_groups[kk]
                combine_group = True
                break
            if (not combine_group):
              kk += 1
    if (len(merge_constraints) > 0):
      print("Merging %d constraint groups with group %d" % (
        len(merge_constraints), (k+1)), file=log)
      for selection in groups :
        first_atom = pdb_atoms[selection[0]]
        altloc = first_atom.fetch_labels().altloc
        if (altloc.strip() == ''):
          raise RuntimeError(("Atom '%s' in occupancy constraint group has "+
            "blank altloc ID") % first_atom.id_str())
        for merge_groups in merge_constraints :
          kk = 0
          while (kk < len(merge_groups)):
            other_selection = merge_groups[kk]
            altloc2 = pdb_atoms[other_selection[0]].fetch_labels().altloc
            if (altloc2 == altloc):
              print("  combining %d atoms with altloc %s" % \
                (len(other_selection), altloc), file=log)
              occ1 = occupancies.select(selection)
              occ2 = occupancies.select(other_selection)
              if (not occ1.all_eq(occ2[0])) or (not occ2.all_eq(occ1[0])):
                raise Sorry(
                  ("Inconsistent occupancies in spatially related groups "+
                  "(%.2f versus %.2f).  To use automatic 3D occupancy "+
                  "restraints, the correlated conformers must start with "+
                  "the same initial occupancy.") % (occ1[0], occ2[0]))
              selection.extend(other_selection)
              del merge_groups[kk]
            else :
              kk += 1
      for merge_groups in merge_constraints :
        if (len(merge_groups) > 0):
          for other_selection in merge_groups :
            altloc = pdb_atoms[other_selection[0]].fetch_labels().altloc
            print(("  warning: %d atoms with altloc %s do not "+
              "correspond to an existing group") % (len(other_selection),
              altloc), file=log)
            groups.append(other_selection)
    k += 1
  if (len(constraint_groups) != n_groups_start):
    print("New occupancy constraint groups:", file=log)
    for i_group, constraint_group in enumerate(constraint_groups):
      print("  group %d:" % (i_group+1), file=log)
      for selection in constraint_group :
        resids = []
        altlocs = set()
        for i_seq in selection :
          atom_group = pdb_atoms[i_seq].parent()
          ag_id = atom_group.id_str()
          altlocs.add(atom_group.altloc)
          if (not ag_id in resids):
            resids.append(ag_id)
        assert len(altlocs) == 1
        print("    conformer '%s' (%d atoms):" % (list(altlocs)[0],
          len(selection)), file=log)
        for ag_id in resids :
          print("      atom_group %s" % ag_id, file=log)
  else :
    print("Occupancy constraint groups unmodified.", file=log)
  print("", file=log)
  return constraint_groups


 *******************************************************************************
