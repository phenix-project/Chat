

 *******************************************************************************
mmtbx/scaling/__init__.py

"""
Base module for Xtriage and related scaling functionality; this imports the
Boost.Python extensions into the local namespace, and provides core functions
for displaying the results of Xtriage.
"""

from __future__ import absolute_import, division, print_function
import cctbx.array_family.flex # import dependency
from libtbx.str_utils import make_sub_header, make_header, make_big_header
from libtbx import slots_getstate_setstate
from six.moves import cStringIO as StringIO
import sys

import boost_adaptbx.boost.python as bp
from six.moves import range
ext = bp.import_ext("mmtbx_scaling_ext")
from mmtbx_scaling_ext import *


class data_analysis(slots_getstate_setstate):
  def show(self, out=sys.stdout, prefix=""):
    raise NotImplementedError()

class xtriage_output(slots_getstate_setstate):
  """
  Base class for generic output wrappers.
  """
  # this is used to toggle behavior in some output methods
  gui_output = False

  def show_big_header(self, title):
    """
    Print a big header with the specified title.
    """
    raise NotImplementedError()

  def show_header(self, title):
    """
    Start a new section with the specified title.
    """
    raise NotImplementedError()

  def show_sub_header(self, title):
    """
    Start a sub-section with the specified title.
    """
    raise NotImplementedError()

  def show_text(self, text):
    """
    Show unformatted text.
    """
    raise NotImplementedError()

  def show(self, text):
    return self.show_text(text)

  def show_preformatted_text(self, text):
    """
    Show text with spaces and line breaks preserved; in some contexts this
    will be done using a monospaced font.
    """
    raise NotImplementedError()

  def show_lines(self, text):
    """
    Show partially formatted text, preserving paragraph breaks.
    """
    raise NotImplementedError()

  def show_paragraph_header(self, text):
    """
    Show a header/title for a paragraph or small block of text.
    """
    raise NotImplementedError()

  def show_table(self, table, indent=0, plot_button=None,
      equal_widths=True):
    """
    Display a formatted table.
    """
    raise NotImplementedError()

  def show_plot(self, table):
    """
    Display a plot, if supported by the given output class.
    """
    raise NotImplementedError()

  def show_plots_row(self, tables):
    """
    Display a series of plots in a single row.  Only used for the Phenix GUI.
    """
    raise NotImplementedError()

  def show_text_columns(self, rows, indent=0):
    """
    Display a set of left-justified text columns.  The number of columns is
    arbitrary but this will usually be key:value pairs.
    """
    raise NotImplementedError()

  def newline(self):
    """
    Print a newline and nothing else.
    """
    raise NotImplementedError()

  def write(self, text):
    """
    Support for generic filehandle methods.
    """
    self.show(text)

  def flush(self):
    """
    Support for generic filehandle methods.
    """
    pass

  def warn(self, text):
    """
    Display a warning message.
    """
    raise NotImplementedError()

class printed_output(xtriage_output):
  """
  Output class for displaying raw text with minimal formatting.
  """
  __slots__ = ["out"]
  def __init__(self, out):
    assert hasattr(out, "write") and hasattr(out, "flush")
    self.out = out
    self._warnings = []

  def show_big_header(self, text):
    make_big_header(text, out=self.out)

  def show_header(self, text):
    make_header(text, out=self.out)

  def show_sub_header(self, title):
    out_tmp = StringIO()
    make_sub_header(title, out=out_tmp)
    for line in out_tmp.getvalue().splitlines():
      self.out.write("%s\n" % line.rstrip())

  def show_text(self, text):
    print(text, file=self.out)

  def show_paragraph_header(self, text):
    print(text, file=self.out) #+ ":"

  def show_preformatted_text(self, text):
    print(text, file=self.out)

  def show_lines(self, text):
    print(text, file=self.out)

  def show_table(self, table, indent=2, plot_button=None, equal_widths=True):
    print(table.format(indent=indent, equal_widths=equal_widths), file=self.out)

  def show_plot(self, table):
    pass

  def show_plots_row(self, tables):
    pass

  def show_text_columns(self, rows, indent=0):
    prefix = " "*indent
    n_cols = len(rows[0])
    col_sizes = [ max([ len(row[i]) for row in rows ]) for i in range(n_cols) ]
    for row in rows :
      assert len(row) == n_cols
      formats = prefix+" ".join([ "%%%ds" % x for x in col_sizes ])
      print(formats % tuple(row), file=self.out)

  def newline(self):
    print("", file=self.out)

  def write(self, text):
    self.out.write(text)

  def warn(self, text):
    self._warnings.append(text)
    out_tmp = StringIO()
    make_sub_header("WARNING", out=out_tmp, sep='*')
    for line in out_tmp.getvalue().splitlines():
      self.out.write("%s\n" % line.rstrip())
    self.out.write(text)

class loggraph_output(xtriage_output):
  """
  Output class for displaying 'loggraph' format (from ccp4i) as plain text.
  """
  gui_output = True
  def __init__(self, out):
    assert hasattr(out, "write") and hasattr(out, "flush")
    self.out = out

  def show_big_header(self, text) : pass
  def show_header(self, text) : pass
  def show_sub_header(self, title) : pass
  def show_text(self, text) : pass
  def show_paragraph_header(self, text) : pass
  def show_preformatted_text(self, text) : pass
  def show_lines(self, text) : pass
  def show_table(self, *args, **kwds) : pass
  def show_text_columns(self, *args, **kwds) : pass
  def newline(self) : pass
  def write(self, text) : pass
  def warn(self, text) : pass

  def show_plot(self, table):
    print("", file=self.out)
    print(table.format_loggraph(), file=self.out)

  def show_plots_row(self, tables):
    for table in tables :
      self.show_plot(table)

class xtriage_analysis(object):
  """
  Base class for analyses performed by Xtriage.  This does not impose any
  restrictions on content or functionality, but simply provides a show()
  method suitable for either filehandle-like objects or objects derived from
  the xtriage_output class.  Child classes should implement _show_impl.
  """
  def show(self, out=None):
    if out is None:
      out=sys.stdout
    if (not isinstance(out, xtriage_output)):
      out = printed_output(out)
    self._show_impl(out=out)
    return self

  def _show_impl(self, out):
    raise NotImplementedError()

  def summarize_issues(self):
    return []


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/absence_likelihood.py
from __future__ import absolute_import, division, print_function
import math
import scitbx.math
from scitbx.array_family import flex
from libtbx.test_utils import approx_equal
from six.moves import range

def halton_x(n=100):
  halton_object = scitbx.math.halton(1)
  x = flex.double()
  for ii in range(n):
    x.append( halton_object.nth_given_base(3,ii) )
  return x

x_precomputed = halton_x(40)

def log_p(z,sigz,centric_flag,n=40,lim=5.0):
  max_e = None
  max_s = None
  def experimental_array(x,z,sigz):
    dd = (z-x*x)*(z-x*x)/(2.0*sigz*sigz)
    sel = flex.bool(dd > 100)
    dd.set_selected( sel, 100)
    result = (1.0/(math.sqrt(2.0*math.pi)*sigz))*flex.exp( -dd )
    return result
  def experimental_single(x,z,sigz):
    dd = (z-x*x)*(z-x*x)/(2.0*sigz*sigz)
    if dd >= 100.0:
      dd = 100.0
    result = math.exp( -dd )*(1.0/(math.sqrt(2.0*math.pi)*sigz))
    return result

  def centric(x):
    result = math.sqrt(2.0/math.pi)*flex.exp( -x*x/2.0 )
    return result
  def centric_single(x):
    result = math.sqrt(2.0/math.pi)*math.exp( -x*x/2.0 )
    return result

  def acentric(x):
    result = 2.0*x*flex.exp( -x*x )
    return result
  def acentric_single(x):
    result = 2.0*x*math.exp( -x*x )
    return result

  def f_single(x,z,sigz,centric_flag):
    if centric_flag:
      result = -x*x/2.0 - (z-x*x)**2/(2*sigz*sigz)
    if not centric_flag:
      result = -x*x + math.log(x+1e-9)-(z-x*x)**2/(2*sigz*sigz)
    return -result

  def f_rat(x,z,sigz,centric_flag,level=5):
    result = f_single(x,z,sigz,centric_flag)-max_s
    result = (result -level*level)**2.0
    return result


  def golden_section_max(z,sigz,centric_flag,function,a=0,b=6,eps=1E-5,max_count=100):
    c   = (-1+math.sqrt(5))/2.0
    x1  = c*a+(1-c)*b
    fx1 = function(x1,z,sigz,centric_flag)
    x2  = (1-c)*a + c*b
    fx2 = function(x2,z,sigz,centric_flag)
    delta = b-a
    count = 0
    while delta> eps:
      if fx1 < fx2:
        b   = x2
        x2  = x1
        fx2 = fx1
        x1  = c*a+(1-c)*b
        fx1 = function(x1,z,sigz,centric_flag)
        delta = b-a
      else:
        a   = x1
        x1  = x2
        fx1 = fx2
        x2  = (1-c)*a+c*b
        fx2 = function(x2,z,sigz,centric_flag)
        delta = b-a
      count+=1
      if count >= max_count:
        delta=eps
      #print a,b,function(a,z,sigz,centric_flag),function(b,z,sigz,centric_flag)

    return (a+b)/2.0

  result = 0
  x = None
  if n == 40:
    x = x_precomputed
  else:
    x = halton_x(n)
  jac = lim
  if centric_flag == None: #absent
    result = experimental_single(0,z,sigz)
  else:
    max_e = golden_section_max(z,sigz,centric_flag,f_single)
    max_s = f_single(max_e,z,sigz,centric_flag)
    low_e = golden_section_max(z,sigz,centric_flag,f_rat,0,max_e)
    high_e = golden_section_max(z,sigz,centric_flag,f_rat,max_e,6)#math.sqrt(abs(z))+5.0*sigz )
    if centric_flag==True:
      x = x*(high_e-low_e)+low_e
      jac = (high_e-low_e)
      result = experimental_array(x,z,sigz)*centric(x)
      result = flex.sum(result)*jac/n
    if centric_flag==False:
      x = x*(high_e-low_e)+low_e
      jac = (high_e-low_e)
      result = experimental_array(x,z,sigz)*acentric(x)
      result = flex.sum(result)*jac/n
  return -math.log(result+1E-12)



def test():
  sigz = 0.000050
  for ii in range(600):
    z = ii/50.0 +0.3
    pac = log_p(z=z,sigz=sigz,centric_flag=False,n=20)
    pcc = log_p(z=z,sigz=sigz,centric_flag=True,n=20)
    tac = 0
    tcc = 0
    if z >=0:
     tac  =  z
     tmp  =  math.sqrt( 1.0 / (2.0*math.pi*max(z,1E-6)) )
     if z <=0:
       tmp = 0.0
     tcc  =  -math.log(math.exp(-z/2.0)*tmp+1E-12)
    assert approx_equal(pac,tac,eps=1e-1)
    assert approx_equal(pcc,tcc,eps=1e-1)
  print("OK")


if __name__ == "__main__":
  test()


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/absences.py

from __future__ import absolute_import, division, print_function
import mmtbx.scaling
from cctbx.array_family import flex
from cctbx import crystal
from cctbx import miller
from cctbx import sgtbx
from libtbx import table_utils
import math
from six.moves import zip

# name     hkl selector  condition
absence_and_conditions = {
  "2_0 (a)" : [(None,0,0),  (0 ,0 ,0 ) ],
  "2_0 (b)" : [(0,None,0),  (0,0,0)],
  "2_0 (c)" : [(0,0,None),  (0,0,0)],


  "2_1 (a)" : [(None,0,0),  (1.0/2.0,0,0)],
  "2_1 (b)" : [(0,None,0),  (0,1.0/2.0,0)],
  "2_1 (c)" : [(0,0,None),  (0,0,1.0/2.0)],


  "3_0 (c)" : [(0,0,None),  (0,0,0)],
  "3_1 (c)" : [(0,0,None),  (0,0,1.0/3.0)],
  "3_2 (c)" : [(0,0,None),  (0,0,1.0/3.0)],

  "4_0 (c)" : [(0,0,None),  (0,0,0)],
  "4_1 (c)" : [(0,0,None),  (0,0,1.0/4.0)],
  "4_2 (c)" : [(0,0,None),  (0,0,2.0/4.0)],
  "4_3 (c)" : [(0,0,None),  (0,0,3.0/4.0)],


  "4_0 (a)" : [(None,0,0),  (0,0,0)],
  "4_1 (a)" : [(None,0,0),  (1.0/4.0,0,0)],
  "4_2 (a)" : [(None,0,0),  (2.0/4.0,0,0)],
  "4_3 (a)" : [(None,0,0),  (3.0/4.0,0,0)],


  "4_0 (b)" : [(0,None,0),  (0,0,0)],
  "4_1 (b)" : [(0,None,0),  (0,1.0/4.0,0)],
  "4_2 (b)" : [(0,None,0),  (0,2.0/4.0,0)],
  "4_3 (b)" : [(0,None,0),  (0,3.0/4.0,0)],


  "6_0 (c)" : [(0,0,None),  (0,0,0)],
  "6_1 (c)" : [(0,0,None),  (0,0,1.0/6.0)],
  "6_2 (c)" : [(0,0,None),  (0,0,2.0/6.0)],
  "6_3 (c)" : [(0,0,None),  (0,0,3.0/6.0)],
  "6_4 (c)" : [(0,0,None),  (0,0,4.0/6.0)],
  "6_5 (c)" : [(0,0,None),  (0,0,5.0/6.0)],

  "b (a)"   : [(0,None,None),  (0,1.0/2.0,0)],
  "c (a)"   : [(0,None,None),  (0,0,1.0/2.0)],
  "n (a)"   : [(0,None,None),  (0,1.0/2.0,1.0/2.0)],
  "d (a)"   : [(0,None,None),  (0,1.0/4.0,1.0/4.0)],

  "a (b)"   : [(None,0,None),  (1.0/2.0,0,0)],
  "c (b)"   : [(None,0,None),  (0,0,1.0/2.0)],
  "n (b)"   : [(None,0,None),  (1.0/2.0,0,1.0/2.0)],
  "d (b)"   : [(None,0,None),  (1.0/4.0,0,1.0/4.0)],

  "a (c)"   : [(None,None,0),  (1.0/2.0,0,0)],
  "b (c)"   : [(None,None,0),  (0,1.0/2.0,0)],
  "n (c)"   : [(None,None,0),  (1.0/2.0,1.0/2.0,0)],
  "d (c)"   : [(None,None,0),  (1.0/4.0,1.0/4.0,0)]
}


absence_classes = {
  "along a 2" : ["2_0 (a)", "2_1 (a)"],
  "along a"   : ["b (a)", "c (a)", "n (a)", "d (a)"],
  "along b 4" : ["4_0 (b)", "4_1 (b)", "4_2 (b)", "4_3 (b)"],
  "along b 2" : ["2_0 (b)", "2_1 (b)"],
  "along b"   : ["a (b)", "c (b)", "n (b)", "d (b)"],
  "along c"   : ["a (c)", "b (c)", "n (c)", "d (c)"],
  "along c 2" : ["2_0 (c)", "2_1 (c)"],
  "along c 3" : ["3_0 (c)", "3_1 (c)", "3_2 (c)"],
  "along c 4" : ["4_0 (c)", "4_1 (c)", "4_2 (c)", "4_3 (c)"],
  "along c 6" : ["6_0 (c)", "6_1 (c)", "6_2 (c)", "6_3 (c)", "6_4 (c)", "6_5 (c)"] }


along_a   = absence_classes["along a"]
along_a_2 = absence_classes["along a 2"]

along_b_4 = absence_classes["along b 4"]
along_b   = absence_classes["along b"]
along_b_2 = absence_classes["along b 2"]

along_c   = absence_classes["along c"]
along_c_2 = absence_classes["along c 2"]
along_c_3 = absence_classes["along c 3"]
along_c_4 = absence_classes["along c 4"]
along_c_6 = absence_classes["along c 6"]

absences_via_intensity_symmetry = {
"P -1"       : []                      ,# l>0 or (l==0 and (h>0 or (h==0 and k>=0)))

"P 1 2/m 1"  : along_b+along_b_2       ,# k>=0 and (l>0 or (l==0 and h>=0))
"C 1 2/m 1"  : along_b+along_b_2       ,# k>=0 and (l>0 or (l==0 and h>=0))
"A 1 2/m 1"  : along_b+along_b_2       ,# k>=0 and (l>0 or (l==0 and h>=0))
"I 1 2/m 1"  : along_b+along_b_2       ,# k>=0 and (l>0 or (l==0 and h>=0))

"P 1 1 2/m"  : along_c+along_c_2       ,# k>=0 and (l>0 or (l==0 and h>=0))
"A 1 1 2/m"  : along_c+along_c_2       ,# k>=0 and (l>0 or (l==0 and h>=0))
"B 1 1 2/m"  : along_c+along_c_2       ,# k>=0 and (l>0 or (l==0 and h>=0))
"I 1 1 2/m"  : along_c+along_c_2       ,# k>=0 and (l>0 or (l==0 and h>=0))

"P 2/m 1 1"  : along_b+along_b_2       ,# k>=0 and (l>0 or (l==0 and h>=0))
"B 2/m 1 1"  : along_b+along_b_2       ,# k>=0 and (l>0 or (l==0 and h>=0))
"C 2/m 1 1"  : along_b+along_b_2       ,# k>=0 and (l>0 or (l==0 and h>=0))
"I 2/m 1 1"  : along_b+along_b_2       ,# k>=0 and (l>0 or (l==0 and h>=0))

"P m m m"    : along_a+along_b+along_c+along_c_2 ,# h>=0 and k>=0 and l>=0
"C m m m"    : along_a+along_b+along_c+along_c_2 ,# h>=0 and k>=0 and l>=0
"A m m m"    : along_a+along_b+along_c+along_c_2 ,# h>=0 and k>=0 and l>=0
"B m m m"    : along_a+along_b+along_c+along_c_2 ,# h>=0 and k>=0 and l>=0
"F m m m"    : along_a+along_b+along_c+along_c_2 ,# h>=0 and k>=0 and l>=0
"I m m m"    : along_a+along_b+along_c+along_c_2 ,# h>=0 and k>=0 and l>=0

"P 4/m"      : along_c_4+along_c ,# l>=0 and ((h>=0 and k>0) or (h==0 and k==0))
"I 4/m"      : along_c_4+along_c         ,# l>=0 and ((h>=0 and k>0) or (h==0 and k==0))

"P 4/m m m"  : along_a+along_c_4+along_c         ,# h>=k and k>=0 and l>=0
"I 4/m m m"  : along_a+along_c_4+along_c         ,# h>=k and k>=0 and l>=0

"P -3"       : along_b+along_c_3+along_c         ,# (h>=0 and k>0) or (h==0 and k==0 and l>=0)
"R -3 :H"    : along_b+along_c_3+along_c        ,# (h>=0 and k>0) or (h==0 and k==0 and l>=0)
"R -3 :R"    : along_b+along_c_3+along_c         ,# (h>=0 and k>0) or (h==0 and k==0 and l>=0)

"P -3 1 m"   : along_b+along_c_3+along_c+along_b_2        ,# h>=k and k>=0 and (k>0 or l>=0)
"P -3 m 1"   : along_b+along_c_3+along_c+along_b_2        ,# h>=k and k>=0 and (h>k or l>=0)
"R -3 m :H"  : along_b+along_c_3+along_c+along_b_2        ,# h>=k and k>=0 and (h>k or l>=0)
"R -3 m :R"  : along_b+along_c_3+along_c+along_b_2        ,# h>=k and k>=0 and (h>k or l>=0)

"P 6/m"      : along_c+along_b+along_c_6         ,# l>=0 and ((h>=0 and k>0) or (h==0 and k==0))
"P 6/m m m"  : along_c+along_b+along_c_6         ,# h>=k and k>=0 and l>=0

"P m -3"     : along_c+along_c_2+along_c_4+along_b_2                ,# h>=0 and ((l>=h and k>h) or (l==h and k==h))
"F m -3"     : along_c+along_c_2+along_c_4+along_b_2                 ,# h>=0 and ((l>=h and k>h) or (l==h and k==h))
"I m -3"     : along_c+along_c_2+along_c_4+along_b_2                 ,# h>=0 and ((l>=h and k>h) or (l==h and k==h))

"P m -3 m"   : along_b+along_b_4+along_b_2                 ,# k>=l and l>=h and h>=0
"F m -3 m"   : along_b+along_b_4+along_b_2                 ,# k>=l and l>=h and h>=0
"I m -3 m"   : along_b+along_b_4+along_b_2                 ,# k>=l and l>=h and h>=0
}


absences_via_intensity_symmetry_prot = {
"P -1"       : []                      ,# l>0 or (l==0 and (h>0 or (h==0 and k>=0)))

"P 1 2/m 1"  : along_b_2       ,# k>=0 and (l>0 or (l==0 and h>=0))
"C 1 2/m 1"  : along_b_2       ,# k>=0 and (l>0 or (l==0 and h>=0))
"A 1 2/m 1"  : along_b_2       ,# k>=0 and (l>0 or (l==0 and h>=0))
"I 1 2/m 1"  : along_b_2       ,# k>=0 and (l>0 or (l==0 and h>=0))

"P 1 1 2/m"  : along_c_2       ,# k>=0 and (l>0 or (l==0 and h>=0))
"A 1 1 2/m"  : along_c_2       ,# k>=0 and (l>0 or (l==0 and h>=0))
"B 1 1 2/m"  : along_c_2       ,# k>=0 and (l>0 or (l==0 and h>=0))
"I 1 1 2/m"  : along_c_2       ,# k>=0 and (l>0 or (l==0 and h>=0))

"P 2/m 1 1"  : along_b_2       ,# k>=0 and (l>0 or (l==0 and h>=0))
"B 2/m 1 1"  : along_b_2       ,# k>=0 and (l>0 or (l==0 and h>=0))
"C 2/m 1 1"  : along_b_2       ,# k>=0 and (l>0 or (l==0 and h>=0))
"I 2/m 1 1"  : along_b_2       ,# k>=0 and (l>0 or (l==0 and h>=0))

"P m m m"    : along_a_2+along_b_2+along_c_2 ,# h>=0 and k>=0 and l>=0
"C m m m"    : along_c_2 ,# h>=0 and k>=0 and l>=0
"A m m m"    : along_a_2 ,# h>=0 and k>=0 and l>=0
"B m m m"    : along_b_2 ,# h>=0 and k>=0 and l>=0
"F m m m"    : along_a_2+along_b_2+along_c_2 ,# h>=0 and k>=0 and l>=0
"I m m m"    : along_a_2+along_b_2+along_c_2 ,# h>=0 and k>=0 and l>=0

"P 4/m"      : along_c_4 ,# l>=0 and ((h>=0 and k>0) or (h==0 and k==0))
"I 4/m"      : along_c_4         ,# l>=0 and ((h>=0 and k>0) or (h==0 and k==0))

"P 4/m m m"  : along_a_2+along_c_4         ,# h>=k and k>=0 and l>=0
"I 4/m m m"  : along_a_2+along_c_4         ,# h>=k and k>=0 and l>=0

"P -3"       : along_c_3        ,# (h>=0 and k>0) or (h==0 and k==0 and l>=0)
"R -3 :H"    : along_c_3        ,# (h>=0 and k>0) or (h==0 and k==0 and l>=0)
"R -3 :R"    : along_c_3         ,# (h>=0 and k>0) or (h==0 and k==0 and l>=0)

"P -3 1 m"   : along_c_3        ,# h>=k and k>=0 and (k>0 or l>=0)
"P -3 m 1"   : along_c_3        ,# h>=k and k>=0 and (h>k or l>=0)
"R -3 m :H"  : along_c_3        ,# h>=k and k>=0 and (h>k or l>=0)
"R -3 m :R"  : along_c_3        ,# h>=k and k>=0 and (h>k or l>=0)

"P 6/m"      : along_c_6         ,# l>=0 and ((h>=0 and k>0) or (h==0 and k==0))
"P 6/m m m"  : along_c_6         ,# h>=k and k>=0 and l>=0

"P m -3"     : along_c_2+along_c_4+along_b_2                ,# h>=0 and ((l>=h and k>h) or (l==h and k==h))
"F m -3"     : along_c_2+along_c_4+along_b_2                 ,# h>=0 and ((l>=h and k>h) or (l==h and k==h))
"I m -3"     : along_c_2+along_c_4+along_b_2                 ,# h>=0 and ((l>=h and k>h) or (l==h and k==h))

"P m -3 m"   : along_b_4+along_b_2                 ,# k>=l and l>=h and h>=0
"F m -3 m"   : along_b_4+along_b_2                 ,# k>=l and l>=h and h>=0
"I m -3 m"   : along_b_4+along_b_2                 ,# k>=l and l>=h and h>=0
}


lattice_from_string = { "1 (0, 0, 0) 0,1/2,1/2"   : "A",
                        "1 (0, 0, 0) 1/2,0,1/2"   : "B",
                        "1 (0, 0, 0) 1/2,1/2,0"   : "C",
                        "1 (0, 0, 0) 1/2,1/2,1/2" : "I",
                      }


symbol_from_string = { "-2 (1, 0, 0) 0,1/2,0" : "b (a)",
                       "-2 (1, 0, 0) 0,0,1/2" : "c (a)",
                       "-2 (1, 0, 0) 0,1/2,1/2" : "n (a)",
                       "2 (1, 0, 0) 0,0,0" : "2_0 (a)",     # 2a
                       "2 (1, 0, 0) 1/2,0,0" : "2_1 (a)",
                       "-2 (0, 0, 1) 1/2,1/2,0" : "n (c)",
                       "6 (0, 0, 1) 0,0,0" : "6_0 (c)",     # 6c
                       "6 (0, 0, 1) 0,0,2/3" : "6_4 (c)",
                       "-2 (1, 0, 0) 0,1/4,1/4" : "d (a)",
                       "6 (0, 0, 1) 0,0,5/6" : "6_5 (c)",
                       "6 (0, 0, 1) 0,0,1/2" : "6_3 (c)",
                       "-2 (0, 1, 0) 1/2,0,0" : "a (b)",
                       "2 (0, 0, 1) 0,0,0"  : "2_0 (c)",    #2c
                       "2 (0, 0, 1) 0,0,1/2" : "2_1 (c)",
                       "4 (0, 0, 1) 0,0,0" : "4_0 (c)",     #4c
                       "4 (0, 0, 1) 0,0,1/4" : "4_1 (c)",
                       "-2 (0, 0, 1) 1/4,1/4,0" : "d (c)",
                       "4 (1, 0, 0) 1/4,0,0" : "4_1 (a)",
                       "4 (1, 0, 0) 0,0,0" : "4_0 (a)",     #4a
                       "3 (0, 0, 1) 0,0,0" : "3_0 (c)",     #3c
                       "3 (0, 0, 1) 0,0,1/3" : "3_1 (c)",
                       "4 (0, 0, 1) 0,0,3/4" : "4_3 (c)",
                       "-2 (0, 0, 1) 0,1/2,0" : "b (c)",
                       "6 (0, 0, 1) 0,0,1/3" : "6_2 (c)",
                       "-2 (0, 1, 0) 1/2,0,1/2" : "n (b)",
                       "4 (1, 0, 0) 3/4,0,0" : "4_3 (a)",
                       "-2 (0, 1, 0) 0,0,1/2" : "c (b)",
                       "6 (0, 0, 1) 0,0,1/6" : "6_1 (c)",
                       "-2 (0, 0, 1) 1/2,0,0" : "a (c)",
                       "2 (0, 1, 0) 0,0,0" : "2_0 (b)",     #2b
                       "2 (0, 1, 0) 0,1/2,0" : "2_1 (b)",
                       "-2 (0, 1, 0) 1/4,0,1/4" : "d (b)",
                       "4 (0, 0, 1) 0,0,1/2" : "4_2 (c)",
                       "4 (1, 0, 0) 2/4,0,0" : "4_2 (a)",
                       "3 (0, 0, 1) 0,0,2/3" : "3_2 (c)" }

string_from_symbol ={"c (b)"  :  "-2 (0, 1, 0) 0,0,1/2",
                     "n (b)"  :  "-2 (0, 1, 0) 1/2,0,1/2",
                     "a (b)"  :  "-2 (0, 1, 0) 1/2,0,0",
                     "a (c)"  :  "-2 (0, 0, 1) 1/2,0,0",
                     "n (c)"  :  "-2 (0, 0, 1) 1/2,1/2,0",
                     "b (c)"  :  "-2 (0, 0, 1) 0,1/2,0",
                     "b (a)"  :  "-2 (1, 0, 0) 0,1/2,0",
                     "n (a)"  :  "-2 (1, 0, 0) 0,1/2,1/2",
                     "c (a)"  :  "-2 (1, 0, 0) 0,0,1/2",
                     "2_1 (b)":   "2 (0, 1, 0) 0,1/2,0",
                     "2_1 (c)":   "2 (0, 0, 1) 0,0,1/2",
                     "2_1 (a)":   "2 (1, 0, 0) 1/2,0,0",
                     "2_0 (b)":   "2 (0, 1, 0) 0,0,0",
                     "2_0 (c)":   "2 (0, 0, 1) 0,0,0",
                     "2_0 (a)":   "2 (1, 0, 0) 0,0,0",
                     "d (b)"  :  "-2 (0, 1, 0) 1/4,0,1/4",
                     "d (c)"  :  "-2 (0, 0, 1) 1/4,1/4,0",
                     "d (a)"  :  "-2 (1, 0, 0) 0,1/4,1/4",
                     "4_0 (c)":   "4 (0, 0, 1) 0,0,0",
                     "4_1 (c)":   "4 (0, 0, 1) 0,0,1/4",
                     "4_2 (c)":   "4 (0, 0, 1) 0,0,1/2",
                     "4_3 (c)":   "4 (0, 0, 1) 0,0,3/4",
                     "4_0 (a)":   "4 (1, 0, 0) 0,0,0",
                     "4_1 (a)":   "4 (1, 0, 0) 1/4,0,0",
                     "4_2 (a)":   "4 (1, 0, 0) 2/4,0,0",
                     "4_3 (a)":   "4 (1, 0, 0) 3/4,0,0",
                     "3_0 (c)":   "3 (0, 0, 1) 0,0,0",
                     "3_1 (c)":   "3 (0, 0, 1) 0,0,1/3",
                     "3_2 (c)":   "3 (0, 0, 1) 0,0,2/3",
                     "6_0 (c)":   "6 (0, 0, 1) 0,0,0",
                     "6_1 (c)":   "6 (0, 0, 1) 0,0,1/6",
                     "6_5 (c)":   "6 (0, 0, 1) 0,0,5/6",
                     "6_4 (c)":   "6 (0, 0, 1) 0,0,2/3",
                     "6_2 (c)":   "6 (0, 0, 1) 0,0,1/3",
                     "6_3 (c)":   "6 (0, 0, 1) 0,0,1/2" }

equivs = { "6_1 (c)":"6_5 (c)", "6_2 (c)":"6_4 (c)",
           "6_5 (c)":"6_1 (c)", "6_4 (c)":"6_2 (c)",
           "4_1 (c)":"4_3 (c)",
           "4_3 (c)":"4_1 (c)",
           "3_1 (c)":"3_2 (c)",
           "3_2 (c)":"3_1 (c)" }


class conditions_for_operator(object):
  def __init__(self, s ):
    r_info = sgtbx.rot_mx_info( s.r() )
    t_info = sgtbx.translation_part_info( s )

    self.type = r_info.type()
    self.ev   = r_info.ev()
    self.trans= t_info.intrinsic_part()

  def condition(self):
    cond = []
    # get conditions for hkl
    if self.type < 0:
      if ( list(self.trans.as_double()) ).count(0) <3 :
        for ii in ev:
          if ii == 0:
            cond.append( None )
          else:
            cond.append( 0 )
      else:
        cond = (None,None,None)
    if type > 0:
      if ( list(self.trans.as_double()) ).count(0) <3 :
        for ii in ev:
          if ii != 0:
            cond.append(None)
          else:
            cond.append( 0 )

    if len(cond) == 0:
      cond = [ None,None,None ]

    return [ cond, list(t) ]

  def absence_type(self):
    #get conditions for translations
    t = self.trans.as_double()
    id_string = str(self.type)+" "+str(self.ev)+" "+str(self.trans)
    op_name = "Non absent"
    if id_string in symbol_from_string:
      op_name = symbol_from_string[ id_string ]
    return op_name

class absences(object):
  def __init__(self, mult=2.0, threshold=0.95):
    self.lib = absence_and_conditions
    self.absence_classes = absences_via_intensity_symmetry_prot
    self.mult = mult
    self.threshold = threshold

  def check(self, abs_type, hkl, return_bool=False ):
    # check if it is there
    message = "The reflection with index %s is "%(str(hkl))
    if abs_type in self.lib:
      mask, condition = self.lib[ abs_type ]
      mc = self.check_mask( hkl, mask )
      cc = self.check_condition(hkl, condition )
      if mc:
        if cc:
          message += " PRESENT under the %s operator"%(abs_type)
        else:
          message += " ABSENT under the %s operartor"%(abs_type)
      else:
        message += " not of the type that is absent under %s"%(abs_type)
    else:
      message =  "No such type of absence"
    if return_bool:
      return mc,cc
    else:
      return message

  def check_mask(self, hkl, mask):
    mask_none_count =0
    for ii in mask:
      if ii is None:
        mask_none_count += 1
    count = 0
    result = False
    for m,h in zip(mask,hkl):
      if m == h:
        count += 1
    if count == 3-mask_none_count:
      result = True
    return result

  def check_condition(self, hkl, condition):
    """magic"""
    result = False
    x = hkl[0]*condition[0] + hkl[1]*condition[1] + hkl[2]*condition[2]
    y = math.cos( x*2.0*math.pi )
    z = 0.5*(math.tanh(self.mult*y)+1)
    if z > self.threshold:
      result = True
    return result

def likelihood(z,sigz,absent_or_centric_or_acentric,sigma_inflation=1.0):
  from mmtbx.scaling import absence_likelihood
  flag = absent_or_centric_or_acentric
  result = absence_likelihood.log_p( z,sigz*sigma_inflation,flag )
  return result


class analyze_absences(mmtbx.scaling.xtriage_analysis):
  def __init__(self, miller_array, isigi_cut=3, sigma_inflation=1.0):
    self.cut = isigi_cut
    self.sigma_inflation=sigma_inflation
    self.miller_array = miller_array.deep_copy()

    self.n_abs        = []
    self.n_n_abs      = []
    self.n_tot        = []
    self.n_abs_viol   = []
    self.n_n_abs_viol = []
    self.n_tot_viol   = []

    self.isi_abs      = []
    self.isi_n_abs    = []
    self.isi_tot      = []

    self.i_abs        = []
    self.i_n_abs      = []
    self.i_tot        = []

    self.op_name      = []
    self.score        = []
    self.present      = []

    assert self.miller_array.sigmas() is not None
    #we need to have this in the standard setting please
    self.sg = sgtbx.space_group_info( group = self.miller_array.space_group() )
    self.cb_op = self.sg.change_of_basis_op_to_reference_setting()
    self.miller_array = self.miller_array.change_basis( self.cb_op )
    self.abs_check = absences()
    self.check_conditions()

  def _show_impl(self, out):
    out.show_sub_header("Table of systematic absence rules")
    out.show("""\
 The following table gives information about systematic absences allowed for
 the specified intensity point group.

 For each operator, the reflections are split in three classes:
""")
    out.show_preformatted_text("""
  Systematic absence: Reflections that are absent for this operator.
  Non absence       : Reflections of the same type (i.e. (0,0,l)) as above, but they
                      should be present.
  Other reflections : All other reflections.
""")
    out.show("""\
For each class, the <I/sigI> is reported, as well as the number of
violations. A violation is a reflection that is absent when it is expected
to be present for a particular space group, or present when it is
expected to be absent. The criteria are:""")
    out.show_preformatted_text("""
  Systematic absence violation: I/sigI > %(cut)2.1f
  Non absence violation       : I/sigI < %(cut)2.1f
  Other relections violation  : I/sigI < %(cut)2.1f
""" % {"cut":self.cut})
    out.show_text("""\
Operators with low associated violations for *both* systematically absent and
non absent reflections, are likely to be true screw axis or glide planes. Both
the number of violations and their percentages are given.  The number of
violations within the 'other reflections' class, can be used as a comparison
for the number of violations in the non-absent class.
""")
    out.show_table(self.table)

  def score_isigi(self,isig, absent=False, a=30.0):
    tmp = 0.5*(1+math.tanh( a*(isig-self.cut) ) )
    if not absent:
      return abs(math.log(tmp+1e-8))
    else:
      return abs(math.log(1-tmp+1e-8) )

  def propose(self, ops, thres=1):
    in_sg_and_seemingly_correct       = []
    not_in_sg_and_seemingly_incorrect = []
    in_sg_and_seemingly_incorrect     = []
    observed_but_not_in_sg            = []
    in_sg_but_no_observations         = []

    absent_class_violations     = 0
    not_absent_class_violations = 0

    total_score = 0

    for op, sc, n, n_n, n_v, n_n_v  in zip(self.op_name, self.score,
        self.n_abs, self.n_n_abs, self.n_abs_viol, self.n_n_abs_viol):
      if op in ops:
        total_score += sc
        absent_class_violations += n_v
        not_absent_class_violations += n_n_v

      if n > 0: # observed
        if sc <= thres: # correct
          if op in ops: # in proposed sg
            in_sg_and_seemingly_correct.append( op )
          else: # correct but not in proposed sg
            observed_but_not_in_sg.append( op )
        else: # not correct
          if op not in ops: # not in sg, and not correct
            not_in_sg_and_seemingly_incorrect.append( op )
      else: # not observed
        if op in ops: #in proposed sg
          in_sg_but_no_observations.append( op )

    pos = len(in_sg_and_seemingly_correct) + \
          len(not_in_sg_and_seemingly_incorrect)
    neg = len(in_sg_and_seemingly_incorrect) + len(observed_but_not_in_sg)
    abstain = len(in_sg_but_no_observations)

    return total_score,absent_class_violations, not_absent_class_violations

  def check_conditions(self,abs_lower_i_threshold=1e-6):
    table_labels = ('Operator',
      "# expected systematic absences",
      "<I/sigI> (violations)",
      "# expected non absences",
      "<I/sigI> (violations)",
      "# other reflections",
      "<I/sigI> (violations)",
      "Score")
    for  item in [0]: # absence_class in self.abs_check.absence_classes[ self.sg.group().crystal_system() ]:
      table_rows = []
      for condition in self.abs_check.absence_classes[
        str(sgtbx.space_group_info(
          group=self.sg.group().build_derived_reflection_intensity_group(False))\
            .as_reference_setting()
            ) ] : # crystal_system() ]:
        n_abs   = 0
        n_n_abs = 0
        n_tot   = 0
        n_abs_viol   = 0
        n_n_abs_viol = 0
        n_tot_viol   = 0

        isi_abs     = 0
        isi_n_abs   = 0
        isi_tot = 0

        i_abs     = 0
        i_n_abs   = 0
        i_tot  = 0

        score = 0

        for hkl, centric_flag, i, sigi in zip(self.miller_array.indices(), self.miller_array.centric_flags(), self.miller_array.data(), self.miller_array.sigmas() ):
          mc, cc = self.abs_check.check(condition,hkl, return_bool=True)
          if abs(i) < abs_lower_i_threshold:
            sigi=max(sigi,abs_lower_i_threshold)
          if mc: # mask checks out
            if cc: # not absent
              n_n_abs += 1
              isi_n_abs += i/sigi
              i_n_abs   += i
              # should be present. flag if not significant
              if i/sigi < self.cut:
                n_n_abs_viol += 1
              score += likelihood(i,sigi,centric_flag[1],self.sigma_inflation)
            else: #absent
              n_abs += 1
              isi_abs += i/sigi
              i_abs   += i
              # should be absent: flag if significant
              if i/sigi > self.cut:
                n_abs_viol += 1
              score += likelihood( i,sigi,None)
          else:
            n_tot +=1
            isi_tot += i/sigi
            i_tot += i
            if i/sigi <  self.cut:
              n_tot_viol += 1
        if n_abs > 0:
          isi_abs   = isi_abs/n_abs
          i_abs   = i_abs/n_abs
        if n_n_abs > 0:
          isi_n_abs = isi_n_abs/n_n_abs
          i_n_abs = i_n_abs/n_n_abs
        if n_tot > 0:
          isi_tot   = isi_tot/n_tot
          i_tot   = i_tot/n_tot

        self.n_abs.append(n_abs)
        self.n_n_abs.append(n_n_abs)
        self.n_tot.append(n_tot)
        self.n_abs_viol.append(n_abs_viol)
        self.n_n_abs_viol.append(n_n_abs_viol)
        self.n_tot_viol.append(n_tot_viol)

        self.isi_abs.append(isi_abs)
        self.isi_n_abs.append(isi_n_abs)
        self.isi_tot.append(isi_tot)

        self.i_abs.append(i_abs)
        self.i_n_abs.append(i_n_abs)
        self.i_tot.append(i_tot)

        self.op_name.append( condition )
        score = float(score)/max(1,n_abs+n_n_abs)
        self.score.append( score )

        table_rows.append( [condition,
          str("%8.0f"%(n_abs)),
          str("%8.2f  (%i, %4.1f%%)" % (isi_abs, n_abs_viol,
            100.0*float(n_abs_viol)/max(1,n_abs))),
          str("%8.0f"%(n_n_abs)),
          str("%8.2f  (%i, %4.1f%%)" % (isi_n_abs, n_n_abs_viol,
            100.0*float(n_n_abs_viol)/max(1,n_n_abs))),
          str("%8.0f"%(n_tot)),
          str("%8.2f  (%i, %4.1f%%)" % (isi_tot, n_tot_viol,
            100.0*float(n_tot_viol)/max(1,n_tot))),
          str("%8.2e"%(abs(score)))
        ])
      self.table = table_utils.simple_table(
        column_headers=table_labels,
        table_rows=table_rows)

class sgi_iterator(object):
  def __init__(self,chiral=True, crystal_system=None, intensity_symmetry=None):
    self.chiral = chiral
    self.crystal_system = crystal_system
    self.intensity_symmetry = intensity_symmetry
    assert ( [self.crystal_system , self.intensity_symmetry] ).count(None) != 0

  def comparator(self, sgi):
    if self.crystal_system is not None:
      return ((self.crystal_system is None) or
              (self.crystal_system == sgi.group().crystal_system()))
    else:
      return ((self.intensity_symmetry is None) or (self.intensity_symmetry ==
        sgi.group().build_derived_reflection_intensity_group(False)))

  def list(self):
    for symbols in sgtbx.space_group_symbol_iterator():
      sgi = sgtbx.space_group_info(group=sgtbx.space_group(
        space_group_symbols=symbols))
      if self.comparator(sgi):
        if (self.chiral is None) or (self.chiral == sgi.group().is_chiral()):
          yield sgi


class protein_space_group_choices(mmtbx.scaling.xtriage_analysis):
  def __init__(self,
      miller_array,
      threshold = 3,
      protein=True,
      print_all=True,
      sigma_inflation=1.0,
      original_data=None):
    self.threshold = 3.0
    assert miller_array.is_xray_intensity_array()
    self.miller_array = miller_array.deep_copy().f_sq_as_f(
      ).average_bijvoet_mates().f_as_f_sq().map_to_asu()
    space_group = self.miller_array.space_group()

    self.absences_table = analyze_absences(
      miller_array=self.miller_array,
      isigi_cut=threshold,
      sigma_inflation=sigma_inflation)
    if (original_data is not None):
      self.absences_list = absences_list(obs=original_data,
        was_filtered=False)
    else :
      self.absences_list = absences_list(obs=self.miller_array,
        was_filtered=True)

    self.sg_iterator = sgi_iterator(chiral = True,
      intensity_symmetry = \
        space_group.build_derived_reflection_intensity_group(False) )

    self.sg_choices  = []
    self.mean_i      = []
    self.mean_isigi  = []
    self.n           = []
    self.violations  = []
    self.abs_types   = []
    self.tuple_score = []

    score = []

    for sg in self.sg_iterator.list():
      xs = crystal.symmetry(
        unit_cell = self.miller_array.unit_cell(),
        space_group = sg.group())
      tmp_miller = self.miller_array.customized_copy( crystal_symmetry = xs )
      these_absent_millers = tmp_miller.select(
        tmp_miller.sys_absent_flags().data() )

      if these_absent_millers.data().size() > 0:
        tmp_mean_i = flex.mean( these_absent_millers.data() )
        zero_sel = these_absent_millers.sigmas()==0
        these_absent_millers = these_absent_millers.select(~zero_sel)
        #print sg, list(these_absent_millers.indices()), list(these_absent_millers.data())
        tmp_mean_isigi = flex.mean(
          these_absent_millers.data() / these_absent_millers.sigmas() )
        tmp_n = these_absent_millers.data().size()
        tmp_violations = flex.bool( these_absent_millers.data() /
          these_absent_millers.sigmas() > self.threshold ).count( True )
      else:
        tmp_mean_i = 0
        tmp_mean_isigi = 0
        tmp_n = 0
        tmp_violations = 0

      to_be_checked = []
      for s in sg.group():
        #check if this is an operator that causes absences
        tmp =  conditions_for_operator( s )
        if tmp.absence_type() != "None":
          if tmp.absence_type() in self.absences_table.op_name:
            ii = self.absences_table.op_name.index( tmp.absence_type() )
            if tmp.absence_type() not in to_be_checked:
              if tmp.absence_type() in equivs:
                if equivs[ tmp.absence_type() ] not in to_be_checked:
                  to_be_checked.append( tmp.absence_type() )
                  tmp_score = self.absences_table.score[ ii ]
              else:
                  to_be_checked.append( tmp.absence_type() )
                  tmp_score = self.absences_table.score[ ii ]

      self.abs_types.append( to_be_checked )
      tuple_score =  self.absences_table.propose( to_be_checked )
      self.tuple_score.append( tuple_score )

      self.sg_choices.append(  sg )
      self.mean_i.append( tmp_mean_i )
      self.mean_isigi.append( tmp_mean_isigi )
      self.n.append( tmp_n )
      self.violations.append( tmp_violations )
    tmp_rows = self.suggest_likely_candidates()
    self.sorted_table = table_utils.simple_table(
      column_headers=['space group', '#  absent', '<Z>_absent',
                      '<Z/sigZ>_absent', '+++', '---', 'score'],
      table_rows=tmp_rows)

  def _show_impl(self, out):
    out.show_header("Systematic absences")
    self.absences_table.show(out)
    out.show_sub_header("Space group identification")
    out.show_text("""\
Analyses of the absences table indicates a number of likely space group
candidates, which are listed below. For each space group, the number of
systematic absence violations are listed under the '+++' column. The number of
non-absence violations (weak reflections) are listed under '---'. The last
column is a likelihood based score for the particular space group.  Note that
enantiomorphic spacegroups will have equal scores. Also, if absences were
removed while processing the data, they will be regarded as missing
information, rather then as enforcing that absence in the space group choices.
""")
    out.show_table(self.sorted_table)
    if (getattr(self, "absences_list", None) is not None) : # backwards compat.
      if (self.absences_list.n_possible_max > 0):
        self.absences_list.show(out)

  def suggest_likely_candidates( self, acceptable_violations = 1e+90 ):
    used = flex.bool( len(self.sg_choices), False )
    order = []

    all_done = False
    count = -1
    if (len(self.tuple_score) == 0):
      return []
    tmp_scores = []
    for tt in self.tuple_score:
      tmp_scores.append( tt[0] )
    order = flex.sort_permutation( flex.double( tmp_scores ), False  )


    sorted_rows = []
    max_score = flex.min( flex.double( tmp_scores ) )
    for ii in order:
      sg             = self.sg_choices[ii]
      tmp_n          = self.n[ii]
      tmp_violations = self.violations[ii]
      tmp_mean_i     = self.mean_i[ii]
      tmp_mean_isigi = self.mean_isigi[ii]
      tuple_score    = self.tuple_score[ii]

      sorted_rows.append( [str(sg), '%i'%(tmp_n),
                           '%8.2f  '%(tmp_mean_i),
                           '%8.2f  '%(tmp_mean_isigi),
                           ' %i '%(tuple_score[1]),
                           ' %i '%(tuple_score[2]),
                           ' %8.3e '%((tuple_score[0]-max_score))
                          ])

    return sorted_rows

class absences_list(mmtbx.scaling.xtriage_analysis,
                     miller.systematic_absences_info):
  """
  Container for lists of systematic absences.  This subclass simply overrides
  the default output of the base class in cctbx.miller to be consistent with
  the rest of Xtriage.
  """
  def show(self, *args, **kwds):
    mmtbx.scaling.xtriage_analysis.show(self, *args, **kwds)

  def _show_impl(self, out):
    """
    For each possible space group, show a list of possible systematically
    absent reflections and corresponding I/sigmaI.
    """
    out.show_sub_header("List of individual systematic absences")
    out.show_text("""\
 Note: this analysis uses the original input data rather than the filtered data
 used for twinning detection; therefore, the results shown here may include
 more reflections than shown above.
""")
    if (self.input_amplitudes):
      out.show_text("""\
 Also note that the input data were amplitudes, which means that weaker
 reflections may have been modified by French-Wilson treatment or discarded
 altogether, and the original intensities will not be recovered.
""")
    for group_info, absences in self.space_group_symbols_and_selections :
      group_note = ""
      if (str(group_info) == str(self.space_group_info)):
        group_note = " (input space group)"
      if (absences == False):
        out.show_paragraph_header("%s%s: no systematic absences possible" % \
          (group_info, group_note))
      elif (absences is None):
        out.show_paragraph_header("%s%s: no absences found" % \
          (group_info, group_note))
      else :
        out.show_paragraph_header("%s%s" % (group_info, group_note))
        lines = []
        for i_hkl, hkl in enumerate(absences.indices()):
          intensity = absences.data()[i_hkl]
          sigma = absences.sigmas()[i_hkl]
          indices_fmt = "(%4d, %4d, %4d)" % hkl
          if (sigma == 0):
            lines.append("  %s: i/sigi = undefined" % indices_fmt)
          else :
            lines.append("  %s: i/sigi = %6.1f" % (indices_fmt,
              intensity/sigma))
        out.show_preformatted_text("\n".join(lines))

def test():
  tmp = absences()
  assert tmp.check( "2_1 (c)", (0,0,1),True ) == (True, False)
  assert tmp.check( "2_1 (c)", (0,0,4),True ) == (True, True)
  assert tmp.check( "4_1 (a)", (4,0,0),True ) == (True, True)
  assert tmp.check( "3_1 (c)", (0,0,3),True ) == (True, True)

  tmp = sgi_iterator(chiral = True,
    crystal_system = None,
    intensity_symmetry = sgtbx.space_group_info( "P222").group().build_derived_reflection_intensity_group(False)  )
  sg_list = []
  abs_list = []
  for sg in tmp.list():
    sg_list.append( str(sg) )
    if str(sg)== "P 21 21 21":
      for s in sg.group():
        abs_list.append( conditions_for_operator( s ).absence_type() )
  assert "2_1 (a)" in abs_list
  assert "2_1 (b)" in abs_list
  assert "2_1 (c)" in abs_list


  assert "P 2 2 2" in sg_list
  assert "P 21 2 2" in sg_list
  assert "P 2 21 2" in sg_list
  assert "P 2 2 21" in sg_list
  assert "P 21 21 2" in sg_list
  assert "P 21 2 21" in sg_list
  assert "P 2 21 21" in sg_list
  assert "P 21 21 21" in sg_list

  """
  tmp = sgi_iterator(chiral=None)
  pg  = []
  asu = []
  for sgi in tmp.list():
    a = str( sgtbx.space_group_info( group = sgi.group().build_derived_reflection_intensity_group(False) ) )
    b = str( sgi.reciprocal_space_asu().reference_as_string() )
    if a not in pg:
      pg.append( a )
      asu.append( b )
  for i,j in zip(pg,asu):
    print i, "   ", j
  """

  print("OK")

if __name__ == "__main__":
  test()


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/absolute_scaling.py

## Peter Zwart, April 18, 2005

from __future__ import absolute_import, division, print_function
from mmtbx import scaling
from cctbx.eltbx import xray_scattering
from cctbx.array_family import flex
from cctbx import adptbx
from cctbx import uctbx
from scitbx.math import chebyshev_polynome
from scitbx.math import chebyshev_lsq_fit
from scitbx.linalg import eigensystem
import scitbx.lbfgs
from libtbx.utils import Sorry
from libtbx.str_utils import format_value
from libtbx import table_utils
import math
import sys
from six.moves import range

class gamma_protein(object):
  __slots__ = [
    "gamma",
    "sigma_gamma",
  ]
  def __init__(self, d_star_sq):
    ## Coefficient for gamma as a function of resolution
    ## described by a chebyshev polynome.
    coefs_mean = \
            [-0.24994838652402987,    0.15287426147680838,
              0.068108692925184011,   0.15780196907582875,
             -0.07811375753346686,    0.043211175909300889,
             -0.043407219965134192,   0.024613271516995903,
              0.0035146404613345932, -0.064118486637211411,
              0.10521875419321854,   -0.10153928782775833,
              0.0335706778430487,    -0.0066629477818811282,
             -0.0058221659481290031,  0.0136026246654981,
             -0.013385834361135244,   0.022526368996167032,
             -0.019843844247892727,   0.018128145323325774,
             -0.0091740188657759101,  0.0068283902389141915,
             -0.0060880807366142566,  0.0004002124110802677,
             -0.00065686973991185187,-0.0039358839200389316,
              0.0056185833386634149, -0.0075257168326962913,
             -0.0015215201587884459, -0.0036383549957990221,
             -0.0064289154284325831,  0.0059080442658917334,
             -0.0089851215734611887,  0.0036488156067441039,
             -0.0047375008148055706, -0.00090999496111171302,
              0.00096986728652170276,-0.0051006830761911011,
              0.0046838536228956777, -0.0031683076118337885,
              0.0037866523617167236,  0.0015810274077361975,
              0.0011030841357086191,  0.0015715596895281762,
             -0.0041354783162507788]

    coefs_mean = flex.double(coefs_mean)

    ## Coefficients descriubing the standard deviation of the above term
    coefs_sigma =\
            [ 0.040664777671929754,   0.015978463897495004,
              0.013068746157907499,  -0.00022301829884293671,
             -3.6158446516473002e-05,-0.0038078038613494048,
             -0.0016909798393289835, -0.003513220109509628,
             -0.00097404245360874664,-0.0037187631008071421,
              0.00031757305576918596,-0.0045169213215130082,
             -0.00086517495110945088,-0.0028285478264746034,
              0.00079199093295738952, 0.00062164093803723265,
              0.0018573920701968332,  0.0012407439272070957,
              0.0002221210281800356, -0.00026366883273910315,
             -0.0011200111831549365, -0.00083301832564164021,
             -0.0011493805850995207, -0.00083852924805887018,
             -0.00019326928638570406,-0.00039548849332659371,
             -0.00022423676327422079,-0.00093964924566378681,
             -0.00063394326307545398,-0.00033546289190621823,
              0.00040784160433128666, 0.001443822356327713,
              0.0013545273776501506,  0.0016735119787882374,
              0.0014189539521390023,  0.0013332965706491645,
              0.00087782212397582277, 0.00043943299411748545,
              0.00063740734290143163, 0.0007244345027539082,
              0.00099161845846209391, 0.00083124705069858489,
              0.00013275127763292434,-0.00023456844928215886,
             -0.001094278715119489]

    coefs_sigma = flex.double( coefs_sigma )

    low = 0.008
    high = 0.69
    gamma_cheb=chebyshev_polynome(
      45, low, high, coefs_mean)
    gamma_sigma_cheb = chebyshev_polynome(
      45, low, high, coefs_sigma)

    ## Make sure that the d_star_sq array
    ## does not have any elements that fall outside
    ## the allowed range (determined by the range
    ## on which the chebyshev polynome was computed;
    ## i.e: self.low<=d_star_sq<=self.high
    d_star_sq = d_star_sq.deep_copy()

    lower_then_low_limit = d_star_sq <= low
    d_star_sq = d_star_sq.set_selected(
      lower_then_low_limit, low)

    higher_then_high_limit = d_star_sq >= high
    d_star_sq = d_star_sq.set_selected(higher_then_high_limit, high)
    ## If everything is okai, this assertion should be
    ## passed withgout any problem
    assert ( flex.min(d_star_sq)>=low  )
    assert ( flex.max(d_star_sq)<=high )

    self.gamma = gamma_cheb.f( d_star_sq )
    self.sigma_gamma = gamma_sigma_cheb.f( d_star_sq )

class gamma_nucleic(object):
  def __init__(self, d_star_sq):
    ## Coefficient for gamma as a function of resolution
    ## described by a chebyshev polynome.
    coefs_mean = \
            [-0.30244055359995714,    0.14551540259035137,
              0.06404885418364728,    0.14126884888694674,
             -0.076010848339056358,   0.10445808072140797,
             -0.13817185173869803,    0.03162832786042917,
              0.041599262771740309,  -0.088562816354662108,
              0.063708058411421117,  -0.044796037515868393,
              0.0088130627883259444,  0.02692514601906355,
             -0.050931900034622016,   0.019443590649642444,
             -0.0011195556039252301, -0.01644343506476071,
              0.0065957064914017524, -0.018596655500261718,
              0.0096270346410321905,  0.016307576063048754,
             -0.02680646640174009,    0.020734177937708331,
              0.0028123353629064345,  0.0045005299107411609,
              0.0076229925628943053, -0.008362403313550976,
              0.0034163962268388306,  0.001904748909797396,
             -0.013325099196913409,   0.0048138529463141863,
              0.0037576434237086738, -0.011440938719878148,
              0.0070463203562045043, -0.014417892444775739,
              0.00051623208479814814,-0.007030834594537072,
             -0.010592510032603445,   0.0099794223029419579,
             -0.0042803299088959388,  0.0018056147902035455,
              9.1732385471614747e-05, 0.0048087303990040917,
              0.0033924291685209331]

    coefs_mean = flex.double(coefs_mean)

    ## Coefficients descriubing the standard deviation of the above term
    coefs_sigma =\
            [ 0.13066942262051989,    0.02540993472514427,
              0.022640055258519923,   0.010682155584811278,
              0.0055933901688389942,  0.010202224633747257,
             -0.0068126652213876008,  0.00074050873381034524,
              0.0043056775404382332, -0.0068162235999210587,
              0.0043883564931143154, -0.0046223069963272981,
             -0.0021799388224634842,  0.0018700994720378869,
             -0.0051883494911385414, -0.00036639670195559728,
             -0.0018731351222522098, -0.005641953724585742,
             -0.0021296034270177015, -0.0037654091933662288,
             -0.0031915331246228089,  0.0017569392295630887,
             -0.0023581953932665491,  0.0043374380859762538,
              0.003490459547329672,   0.0030620317182512053,
              0.0037626939824912907, -0.0014184248052271247,
             -0.0032475452005936508, -0.0053177954201788511,
             -0.0085157840734136816, -0.0057322608003856712,
             -0.0051182987317167803, -0.0052003177422633084,
             -0.001085721076048506,  -0.00072459199543249329,
              0.0010209328663554243,  0.00076695099249463397,
              0.00034115347063572426, 0.0021264541997130233,
             -0.00031955842674212867,-0.00148958769833968,
              0.0003181991857060145, -0.00069586514533741132,
             -0.00046211335387235546]

    coefs_sigma = flex.double( coefs_sigma )

    low = 0.008
    high = 0.69
    gamma_cheb=chebyshev_polynome(
      45, low, high, coefs_mean)
    gamma_sigma_cheb = chebyshev_polynome(
      45, low, high, coefs_sigma)

    ## Make sure that the d_star_sq array
    ## does not have any elements that fall outside
    ## the allowed range (determined by the range
    ## on which the chebyshev polynome was computed;
    ## i.e: self.low<=d_star_sq<=self.high
    d_star_sq = d_star_sq.deep_copy()
    lower_then_low_limit = d_star_sq <= low
    d_star_sq = d_star_sq.set_selected(lower_then_low_limit, low)

    higher_then_high_limit = d_star_sq >= high
    d_star_sq = d_star_sq.set_selected(higher_then_high_limit, high)
    ## If everything is okai, this assertion should be
    ## passed withgout any problem
    assert ( flex.min(d_star_sq)>=low  )
    assert ( flex.max(d_star_sq)<=high )

    self.gamma = gamma_cheb.f( d_star_sq )
    self.sigma_gamma = gamma_sigma_cheb.f( d_star_sq )


class expected_intensity(object):
  """
  This class computes the expected intensity for a given d_star_sq_array given
  some basic info about ASU contents.
  """
  def __init__(self,
               scattering_info,
               d_star_sq_array,
               p_scale=0.0,
               b_wilson=0.0,
               magic_fudge_factor=2.0):
    ## First recompute some parameters
    scattering_info.scat_data(d_star_sq_array)
    ## mean intensity
    self.mean_intensity = scattering_info.sigma_tot_sq
    self.mean_intensity = self.mean_intensity*(1.0+scattering_info.gamma_tot)
    ## I am missing a factor 2 somewhere
    self.mean_intensity/=magic_fudge_factor
    self.mean_intensity=self.mean_intensity*flex.exp(
      -d_star_sq_array*b_wilson/2.0)
    self.mean_intensity*=math.exp(-p_scale)

    ## the associated standard deviation
    self.sigma_intensity = scattering_info.gamma_tot_sigma
    self.sigma_intensity = scattering_info.sigma_tot_sq*self.sigma_intensity
    self.sigma_intensity = self.sigma_intensity*flex.exp(
      -d_star_sq_array*b_wilson/2.0)
    self.sigma_intensity*= math.exp(-p_scale)


class scattering_information(object):
  def __init__(self,
               n_residues=None,
               n_bases=None,
               asu_contents=None,
               fraction_protein=None,
               fraction_nucleic=None):
    """ Returns scattering info for specified structure"""
    ## Preference is given to a supplied asu_content dictionairy
    ## If this is not available, one will made up given the
    ## number of residues or bases.

    self.sigma_tot_sq = None
    self.gamma_tot = None
    self.gamma_tot_sigma = None

    ## if aus_contents is not none no resiudes should be specified
    if asu_contents is not None:
      assert (n_residues==None)
      assert (n_bases==None)
    ## and vise versa
    if ( (n_residues is not None) or (n_bases is not None) ):
      assert (asu_contents==None)
    ## if aus_contents is not none, fractions need to bve specified
    if asu_contents is not None:
      assert( fraction_protein is not None)
      assert( fraction_nucleic is not None)

    ## determin the fractions first
    if asu_contents is None:
      if n_residues is None:
        n_residues=0.0
      fraction_protein = (8.0*1.0*1.0+
                          5.0*6.0*6.0+
                          1.5*7.0*7.0+
                          1.2*8.0*8.0)*float(n_residues)
      if n_bases is None:
        n_bases=0.0
      fraction_nucleic = (16.0*1.0*1.0+
                          9.7*6.0*6.0+
                          3.8*7.0*7.0+
                          5.9*8.0*8.0+
                          1.0*15.0*15.0)*float(n_bases)
      tot = fraction_protein+fraction_nucleic
      fraction_protein/=tot
      fraction_nucleic/=tot


    if fraction_protein is not None:
      self.fraction_protein = fraction_protein
    else:
      self.fraction_protein = 0.0

    if fraction_nucleic is not None:
      self.fraction_nucleic = fraction_nucleic
    else:
      self.fraction_nucleic = 0.0
    assert ( self.fraction_protein+self.fraction_nucleic<=1.0 )


    if asu_contents is None:
      asu_contents = None
      if n_residues > 0:
        asu_contents = {"H":8.0*float(n_residues),
                        "C":5.0*float(n_residues),
                        "N":1.5*float(n_residues),
                        "O":1.2*float(n_residues)
                       }
      if n_bases > 0:
        ## These values are rather approximate
        asu_contents = {"H":float(n_bases)*16.0,
                        "C":float(n_bases)*9.7,
                        "N":float(n_bases)*3.8,
                        "O":float(n_bases)*5.9,
                        "P":float(n_bases)*1.0
                        }
      if n_bases > 0:
        if n_residues > 0:
          asu_contents = {"H":float(n_bases)*16.0 + 8.0*float(n_residues),
                          "C":float(n_bases)*9.7  + 5.0*float(n_residues),
                          "N":float(n_bases)*3.8  + 1.5*float(n_residues),
                          "O":float(n_bases)*5.9  + 1.2*float(n_residues),
                          "P":float(n_bases)*1.0
                          }
    self.asu_contents =  asu_contents


  def scat_data(self, d_star_sq=None):

    if d_star_sq is None:
      self.sigma_tot_sq=None
      self.gamma_tot_sigma=None
      self.gamma_tot=None

    if d_star_sq is not None:
      self.sigma_tot_sq = flex.double( d_star_sq.size() )
      gaussians = {}
      for chemical_type, n_atoms in self.asu_contents.items():
        gaussians[chemical_type] = xray_scattering.wk1995(
          chemical_type).fetch()
        f0 = gaussians[chemical_type].at_d_star_sq(d_star_sq)
        self.sigma_tot_sq += f0*f0*n_atoms

      if(d_star_sq.size()>0):
        ## Protein part
        gamma_prot = gamma_protein(d_star_sq)
        self.gamma_prot = gamma_prot.gamma*self.fraction_protein
        ## Nucleotide part; needs to be completed
        gamma_nuc = gamma_nucleic(d_star_sq)
        self.gamma_nuc = gamma_nuc.gamma*self.fraction_nucleic ##
        ## Totals
        self.gamma_tot = self.gamma_prot*self.fraction_protein +\
                         self.gamma_nuc*self.fraction_nucleic
        self.gamma_tot_sigma = (gamma_prot.sigma_gamma*self.fraction_protein)*\
                               (gamma_prot.sigma_gamma*self.fraction_protein)+\
                               (gamma_nuc.sigma_gamma*self.fraction_nucleic)*\
                               (gamma_nuc.sigma_gamma*self.fraction_nucleic)
        self.gamma_tot_sigma = flex.sqrt(  self.gamma_tot_sigma )




def anisotropic_correction(cache_0,
                           p_scale,
                           u_star,
                           b_add=None,
                           must_be_greater_than=0.):
  ## Make sure that u_star is not rwgk scaled, i.e. like you get it from
  ## the ml_absolute_scale_aniso routine (!which is !!NOT!! scaled!)
  work_array = None
  try:
    work_array = cache_0.input.select( cache_0.input.data() > must_be_greater_than)
  except KeyboardInterrupt: raise
  except Exception: pass
  if work_array is None:
    work_array = cache_0.select( cache_0.data() > must_be_greater_than)

  change_back_to_intensity=False
  if work_array.is_xray_intensity_array():
    work_array = work_array.f_sq_as_f()
    change_back_to_intensity=True

  assert not work_array.is_xray_intensity_array()

  if b_add is not None:
    u_star_add =  adptbx.b_iso_as_u_star( work_array.unit_cell(),
                                          b_add )
    u_star = u_star+u_star_add



  corrected_amplitudes = scaling.ml_normalise_aniso( work_array.indices(),
                                                     work_array.data(),
                                                     p_scale,
                                                     work_array.unit_cell(),
                                                     u_star )
  if work_array.sigmas() is not None:
    corrected_sigmas = scaling.ml_normalise_aniso( work_array.indices(),
                                                   work_array.sigmas(),
                                                   p_scale,
                                                   work_array.unit_cell(),
                                                   u_star )
  else:
    corrected_sigmas = None


  work_array = work_array.customized_copy(
    data = corrected_amplitudes,
    sigmas = corrected_sigmas ).set_observation_type(work_array)
  if change_back_to_intensity:
    # XXX check for floating-point overflows (which trigger the Boost trap
    # and crash the interpreter).  The only known case is 2q8o:IOBS2,SIGIOBS2
    # which is missing nearly all acentric hkls but it clearly points to a bug
    # in this routine when dealing with pathological data.
    f_max = flex.max(work_array.data())
    if (not f_max < math.sqrt(sys.float_info.max)):
      raise OverflowError("Amplitudes will exceed floating point limit if "+
        "converted to intensities (max F = %e)." % f_max)
    work_array = work_array.f_as_f_sq()
  return work_array

########################################################################
# SCALING ROUTINES
########################################################################

class ml_iso_absolute_scaling(scaling.xtriage_analysis):
  """
  Maximum likelihood isotropic wilson scaling.

  :param miller_array: experimental data (will be converted to amplitudes
                       if necessary
  :param n_residues: number of protein residues in ASU
  :param n_bases: number of nucleic acid bases in ASU
  :param asu_contents: a dictionary specifying scattering types and numbers
                       ( i.e. {'Au':1, 'C':2.5, 'O':1', 'H':3 } )
  :param prot_frac: fraction of scattering from protein
  :param nuc_frac: fraction of scattering from nucleic acids
  """
  def __init__(self,
              miller_array,
              n_residues=None,
              n_bases=None,
              asu_contents=None,
              prot_frac = 1.0,
              nuc_frac= 0.0,
              include_array_info=True):
    self.p_scale, self.b_wilson = None, None
    ## Checking input combinations
    if (n_residues is None):
      if (n_bases is None):
        assert asu_contents is not None
        assert (type(asu_contents) == type({}) )
    if asu_contents is None:
      assert ( (n_residues is not None) or (n_bases is not None) )
    assert (prot_frac+nuc_frac<=1.0)
    if ( miller_array.is_xray_intensity_array() ):
      miller_array = miller_array.f_sq_as_f()
    assert ( miller_array.is_real_array() )
    self.info = None
    if (include_array_info):
      ## Save the information of the file name etc
      self.info = miller_array.info()
    work_array = miller_array.resolution_filter(
      d_max=1.0/math.sqrt(  scaling.get_d_star_sq_low_limit() ),
      d_min=1.0/math.sqrt( scaling.get_d_star_sq_high_limit() ))
    if work_array.data().size() > 0:
      work_array = work_array.select(work_array.data() > 0)
      self.d_star_sq = work_array.d_star_sq().data()
      self.scat_info =  None
      if asu_contents is None:
        self.scat_info = scattering_information(
          n_residues=n_residues,
          n_bases = n_bases,
          fraction_protein = prot_frac,
          fraction_nucleic = nuc_frac)
      else:
        self.scat_info = scattering_information(
          asu_contents = asu_contents,
          fraction_protein = prot_frac,
          fraction_nucleic = nuc_frac)
      if (work_array.size() > 0 ):
        ## Compute the terms
        self.scat_info.scat_data(self.d_star_sq)
        self.f_obs = work_array.data()
        ## Make sure sigma's are used when available
        if (work_array.sigmas() is not None):
          self.sigma_f_obs = work_array.sigmas()
        else:
          self.sigma_f_obs = flex.double(self.f_obs.size(),0.0)
        if (flex.min( self.sigma_f_obs ) < 0):
          self.sigma_f_obs = self.sigma_f_obs*0.0
        ## multiplicities and d_star_sq
        self.epsilon = work_array.epsilons().data().as_double()
        ## centric flags
        self.centric = flex.bool(work_array.centric_flags().data())
        ## Wilson parameters come from scattering_information class
        self.gamma_prot = self.scat_info.gamma_tot
        self.sigma_prot_sq = self.scat_info.sigma_tot_sq
        ## Optimisation stuff
        self.x = flex.double(2,0.0)
        self.x[0]=0.0
        self.x[1]=50.0
        self.f=0
        term_parameters = scitbx.lbfgs.termination_parameters( max_iterations = 1e6 ) # just for safety
        minimizer = scitbx.lbfgs.run(target_evaluator=self,
          termination_params=term_parameters)
        self.p_scale = self.x[0]
        self.b_wilson = self.x[1]
        ## this we do not need anymore
        del self.x
        del self.f_obs
        del self.sigma_f_obs
        del self.epsilon
        del self.gamma_prot
        del self.sigma_prot_sq
        del self.d_star_sq
        del self.centric

  def compute_functional_and_gradients(self):
    f = scaling.wilson_total_nll(
      d_star_sq=self.d_star_sq,
      f_obs=self.f_obs,
      sigma_f_obs=self.sigma_f_obs,
      epsilon=self.epsilon,
      sigma_sq=self.sigma_prot_sq,
      gamma_prot=self.gamma_prot,
      centric=self.centric,
      p_scale=self.x[0],
      p_B_wilson=self.x[1])
    g = flex.double( scaling.wilson_total_nll_gradient(
      d_star_sq=self.d_star_sq,
      f_obs=self.f_obs,
      sigma_f_obs=self.sigma_f_obs,
      epsilon=self.epsilon,
      sigma_sq=self.sigma_prot_sq,
      gamma_prot=self.gamma_prot,
      centric=self.centric,
      p_scale=self.x[0],
      p_B_wilson=self.x[1]) )
    self.f = f
    return f, g

  def _show_impl(self, out):
    label_suffix = ""
    if (self.info is not None):
      label_suffix = " of %s" % str(self.info)
    out.show_sub_header("Maximum likelihood isotropic Wilson scaling")
    out.show(""" ML estimate of overall B value%s:""" % label_suffix)
    out.show_preformatted_text("   %s A**2" %
      format_value("%5.2f", self.b_wilson))
    out.show(""" Estimated -log of scale factor%s:""" % label_suffix)
    out.show_preformatted_text("""   %s""" %
      format_value("%5.2f", self.p_scale))
    out.show("""\
 The overall B value ("Wilson B-factor", derived from the Wilson plot) gives
 an isotropic approximation for the falloff of intensity as a function of
 resolution.  Note that this approximation may be misleading for anisotropic
 data (where the crystal is poorly ordered along an axis).  The Wilson B is
 strongly correlated with refined atomic B-factors but these may differ by
 a significant amount, especially if anisotropy is present.""")
    if (self.b_wilson < 0):
      out.warn("The Wilson B-factor is negative!  This indicates that "+
        "the data are either unusually pathological, or have been "+
        "artifically manipulated (for instance by applying anisotropic "+
        "scaling directly).  Please inspect the raw data and/or use "+
        "unmodified reflections for phasing and refinement, as any model "+
        "generated from the current dataset will be unrealistic.")
    # TODO compare to datasets in PDB at similar resolution

  def summarize_issues(self):
    if (self.b_wilson < 0):
      return [(2, "The Wilson plot isotropic B-factor is negative.",
        "Maximum likelihood isotropic Wilson scaling")]
    elif (self.b_wilson > 200):
      return [(1, "The Wilson plot isotropic B-factor is greater than 200.",
        "Maximum likelihood isotropic Wilson scaling")]
    return []

class ml_aniso_absolute_scaling(scaling.xtriage_analysis):
  """
  Maximum likelihood anisotropic wilson scaling.

  :param miller_array: experimental data (will be converted to amplitudes
                       if necessary
  :param n_residues: number of protein residues in ASU
  :param n_bases: number of nucleic acid bases in ASU
  :param asu_contents: a dictionary specifying scattering types and numbers
                       ( i.e. {'Au':1, 'C':2.5, 'O':1', 'H':3 } )
  :param prot_frac: fraction of scattering from protein
  :param nuc_frac: fraction of scattering from nucleic acids
  """
  def __init__(self,
               miller_array,
               n_residues=None,
               n_bases=None,
               asu_contents=None,
               prot_frac = 1.0,
               nuc_frac= 0.0,
               ignore_errors=False):
    """ Maximum likelihood anisotropic wilson scaling"""
    #Checking input
    if (n_residues is None):
      if (n_bases is None):
        assert asu_contents is not None
        assert (type(asu_contents) == type({}) )
    if asu_contents is None:
      assert ( (n_residues is not None) or (n_bases is not None) )
    assert (prot_frac+nuc_frac<=1.0)
    assert ( miller_array.is_real_array() )

    self.info = miller_array.info()
    if ( miller_array.is_xray_intensity_array() ):
      miller_array = miller_array.f_sq_as_f()

    work_array = miller_array.resolution_filter(
      d_max=1.0/math.sqrt(  scaling.get_d_star_sq_low_limit() ),
      d_min=1.0/math.sqrt( scaling.get_d_star_sq_high_limit() ))
    work_array = work_array.select(work_array.data()>0)
    self.d_star_sq = work_array.d_star_sq().data()
    self.scat_info =  None
    if asu_contents is None:
      self.scat_info= scattering_information(
                                        n_residues=n_residues,
                                        n_bases = n_bases)
    else:
      self.scat_info = scattering_information(
                                         asu_contents = asu_contents,
                                         fraction_protein = prot_frac,
                                         fraction_nucleic = nuc_frac)
    self.scat_info.scat_data(self.d_star_sq)
    self.b_cart = None
    if (work_array.size() > 0 ):
      self.hkl = work_array.indices()
      self.f_obs = work_array.data()
      self.unit_cell =  uctbx.unit_cell(
        miller_array.unit_cell().parameters() )
      ## Make sure sigma's are used when available
      if (work_array.sigmas() is not None):
        self.sigma_f_obs = work_array.sigmas()
      else:
        self.sigma_f_obs = flex.double(self.f_obs.size(),0.0)
      if (flex.min( self.sigma_f_obs ) < 0):
        self.sigma_f_obs = self.sigma_f_obs*0.0

      ## multiplicities
      self.epsilon = work_array.epsilons().data().as_double()
      ## Determine Wilson parameters
      self.gamma_prot = self.scat_info.gamma_tot
      self.sigma_prot_sq = self.scat_info.sigma_tot_sq
      ## centric flags
      self.centric = flex.bool(work_array.centric_flags().data())
      ## Symmetry stuff
      self.sg = work_array.space_group()
      self.adp_constraints = self.sg.adp_constraints()
      self.dim_u = self.adp_constraints.n_independent_params()
      ## Setup number of parameters
      assert self.dim_u <= 6
      ## Optimisation stuff
      self.x = flex.double(self.dim_u+1, 0.0) ## B-values and scale factor!
      exception_handling_params = scitbx.lbfgs.exception_handling_parameters(
        ignore_line_search_failed_step_at_lower_bound = ignore_errors,
        ignore_line_search_failed_step_at_upper_bound = ignore_errors,
        ignore_line_search_failed_maxfev              = ignore_errors)
      term_parameters = scitbx.lbfgs.termination_parameters(
        max_iterations = 50)

      minimizer = scitbx.lbfgs.run(target_evaluator=self,
        termination_params=term_parameters,
        exception_handling_params=exception_handling_params)

      ## Done refining
      Vrwgk = math.pow(self.unit_cell.volume(),2.0/3.0)
      self.p_scale = self.x[0]
      self.u_star = self.unpack()
      self.u_star = list( flex.double(self.u_star) / Vrwgk )
      self.b_cart = adptbx.u_as_b(adptbx.u_star_as_u_cart(self.unit_cell,
                                       self.u_star))
      self.u_cif = adptbx.u_star_as_u_cif(self.unit_cell,
                                          self.u_star)
      #get eigenvalues of B-cart
      eigen = eigensystem.real_symmetric( self.b_cart )
      self.eigen_values = eigen.values()  # TODO verify that Im not a dictionary values method
      self.eigen_vectors = eigen.vectors()

      self.work_array  = work_array # i need this for further analyses
      self.analyze_aniso_correction()
      # FIXME see 3ihm:IOBS4,SIGIOBS4
      if (self.eigen_values[0] != 0):
        self.anirat = (abs(self.eigen_values[0] - self.eigen_values[2]) /
                     self.eigen_values[0])
      else :
        self.anirat = None

      del self.x
      del self.f_obs
      del self.sigma_f_obs
      del self.epsilon
      del self.gamma_prot
      del self.sigma_prot_sq
      del self.centric
      del self.hkl
      del self.d_star_sq
      del self.adp_constraints

  def pack(self,g):
    ## generate a set of reduced parameters for the minimizer
    g_independent = [g[0]]
    g_independent = g_independent + \
      list(
       self.adp_constraints.independent_gradients(list(g[1:]))
      )
    return flex.double(g_independent)

  def unpack(self):
    ## generate all parameters from the reduced set
    ## for target function computing and so forth
    u_star_full = self.adp_constraints.all_params(list(self.x[1:]))
    return u_star_full

  def compute_functional_and_gradients(self):
    u = self.unpack()
    f = scaling.wilson_total_nll_aniso(self.hkl,
                                       self.f_obs,
                                       self.sigma_f_obs,
                                       self.epsilon,
                                       self.sigma_prot_sq,
                                       self.gamma_prot,
                                       self.centric,
                                       self.x[0],
                                       self.unit_cell,
                                       u)
    self.f=f
    g_full_exact = flex.double( scaling.wilson_total_nll_aniso_gradient(
      self.hkl,
      self.f_obs,
      self.sigma_f_obs,
      self.epsilon,
      self.sigma_prot_sq,
      self.gamma_prot,
      self.centric,
      self.x[0],
      self.unit_cell,
      u ))

    g = self.pack(g_full_exact)
    return f, g

  def format_it(self,x,format="%3.2f"):
    xx = format%(x)
    if x > 0:
      xx = " "+xx
    return(xx)

  def aniso_ratio_p_value(self,rat):
    return -3
    coefs = flex.double( [-1.7647171873040273, -3.4427008004789115,
      -1.097150249786379, 0.17303317520973829, 0.35955513268118661,
      0.066276397961476205, -0.064575726062529232, -0.0063025873711609016,
      0.0749945566688624, 0.14803702885155121, 0.154284467861286])
    fit_e = scitbx.math.chebyshev_polynome(11,0,1.0,coefs)
    x = flex.double( range(1000) )/999.0
    start = int(rat*1000)
    norma = flex.sum(flex.exp(fit_e.f(x)))/x[1]
    x = x*(1-rat)+rat
    norma2 = flex.sum(flex.exp(fit_e.f(x)))/(x[1]-x[0])
    return -math.log(norma2/norma )

  def analyze_aniso_correction(self, n_check=2000, p_check=0.25, level=3,
      z_level=9):
    self.min_d = None
    self.max_d = None
    self.level = None
    self.z_level = None
    self.z_low = None
    self.z_high = None
    self.z_tot = None
    self.mean_isigi = None
    self.mean_count = None
    self.mean_isigi_low_correction_factor = None
    self.frac_below_low_correction = None
    self.mean_isigi_high_correction_factor = None
    self.frac_below_high_correction = None
    if self.work_array.sigmas() is None:
       return "No further analysis of anisotropy carried out because of absence of sigmas"

    correction_factors = self.work_array.customized_copy(
                 data=self.work_array.data()*0.0+1.0, sigmas=None )
    correction_factors = anisotropic_correction(
      correction_factors,0.0,self.u_star ).data()
    self.work_array = self.work_array.f_as_f_sq()
    isigi = self.work_array.data() / (
        self.work_array.sigmas()+max(1e-8,flex.min(self.work_array.sigmas())))
    d_spacings = self.work_array.d_spacings().data().as_double()
    if d_spacings.size() <= n_check:
      n_check = d_spacings.size()-2
    d_sort   = flex.sort_permutation( d_spacings )
    d_select = d_sort[0:n_check]
    min_d = d_spacings[ d_select[0] ]
    max_d = d_spacings[ d_select[ n_check-1] ]
    isigi = isigi.select( d_select )
    mean_isigi = flex.mean( isigi )
    observed_count = flex.bool( isigi > level ).as_double()
    mean_count = flex.mean( observed_count )
    correction_factors = correction_factors.select( d_select )
    isigi_rank      = flex.sort_permutation(isigi)
    correction_rank = flex.sort_permutation(correction_factors, reverse=True)
    n_again = int(correction_rank.size()*p_check )
    sel_hc = correction_rank[0:n_again]
    sel_lc = correction_rank[n_again:]
    mean_isigi_low_correction_factor  = flex.mean(isigi.select(sel_lc) )
    mean_isigi_high_correction_factor = flex.mean(isigi.select(sel_hc) )
    frac_below_low_correction        = flex.mean(observed_count.select(sel_lc))
    frac_below_high_correction       = flex.mean(observed_count.select(sel_hc))
    mu = flex.mean( observed_count )
    var = math.sqrt(mu*(1.0-mu)/n_again)
    z_low  = abs(frac_below_low_correction-mean_count)/max(1e-8,var)
    z_high = abs(frac_below_high_correction-mean_count)/max(1e-8,var)
    z_tot  = math.sqrt( (z_low*z_low + z_high*z_high) )
    # save some of these for later
    self.min_d = min_d
    self.max_d = max_d
    self.level = level
    self.z_level = z_level
    self.z_low = z_low
    self.z_high = z_high
    self.z_tot = z_tot
    self.mean_isigi = mean_isigi
    self.mean_count = mean_count
    self.mean_isigi_low_correction_factor = mean_isigi_low_correction_factor
    self.frac_below_low_correction = frac_below_low_correction
    self.mean_isigi_high_correction_factor = mean_isigi_high_correction_factor
    self.frac_below_high_correction = frac_below_high_correction

  def _show_impl(self, out):
    assert (self.b_cart is not None)
    out.show_sub_header("Maximum likelihood anisotropic Wilson scaling")
    out.show("ML estimate of overall B_cart value:")
    out.show_preformatted_text("""\
  %5.2f, %5.2f, %5.2f
  %12.2f, %5.2f
  %19.2f
""" % (self.b_cart[0], self.b_cart[3], self.b_cart[4],
                       self.b_cart[1], self.b_cart[5],
                                       self.b_cart[2]))
    out.show("Equivalent representation as U_cif:")
    out.show_preformatted_text("""\
  %5.2f, %5.2f, %5.2f
  %12.2f, %5.2f
  %19.2f
""" % (self.u_cif[0], self.u_cif[3], self.u_cif[4],
                      self.u_cif[1], self.u_cif[5],
                                     self.u_cif[2]))
    out.show("Eigen analyses of B-cart:")
    def format_it(x,format="%3.2f"):
      xx = format%(x)
      if x > 0:
        xx = " "+xx
      return(xx)
    rows = [
      [ "1", format_it(self.eigen_values[0],"%5.3f"),
       "(%s, %s, %s)" % (format_it(self.eigen_vectors[0]),
       format_it(self.eigen_vectors[1]),
       format_it(self.eigen_vectors[2])) ],
      [ "2", format_it(self.eigen_values[1],"%5.3f"),
       "(%s, %s, %s)" % (format_it(self.eigen_vectors[3]),
       format_it(self.eigen_vectors[4]),
       format_it(self.eigen_vectors[5])) ],
      [ "3", format_it(self.eigen_values[2],"%5.3f"),
       "(%s, %s, %s)" % (format_it(self.eigen_vectors[6]),
       format_it(self.eigen_vectors[7]),
       format_it(self.eigen_vectors[8])) ],
    ]
    table = table_utils.simple_table(
      column_headers=["Eigenvector", "Value", "Vector"],
      table_rows=rows)
    out.show_table(table)
    out.show("ML estimate of  -log of scale factor:")
    out.show_preformatted_text("  %5.2f" %(self.p_scale))
    out.show_sub_header("Anisotropy analyses")
    if (self.eigen_values[0] == 0):
      raise Sorry("Fatal error: eigenvector 1 of the overall anisotropic "+
        "B-factor B_cart is zero.  This "+
        "may indicate severe problems with the input data, for instance "+
        "if only a single plane through reciprocal space is present.")
#    ani_rat_p = self.aniso_ratio_p_value(self.anirat)
#    if ani_rat_p < 0:
#      ani_rat_p = 0.0
#    out.show_preformatted_text("""\
#Anisotropy    ( [MaxAnisoB-MinAnisoB]/[MaxAnisoB] ) :  %7.3e
#                          Anisotropic ratio p-value :  %7.3e
#""" % (self.anirat, ani_rat_p))
#    out.show("""
# The p-value is a measure of the severity of anisotropy as observed in the PDB.
# The p-value of %5.3e indicates that roughly %4.1f %% of datasets available in
# the PDB have an anisotropy equal to or worse than this dataset.""" %
#      (ani_rat_p, 100.0*math.exp(-ani_rat_p)))
    message = """indicates that there probably is no
 significant systematic noise amplification."""
    if (self.z_tot is not None) and (self.z_tot > self.z_level):
      if self.mean_isigi_high_correction_factor < self.level:
        message =  """indicates that there probably is significant
 systematic noise amplification that could possibly lead to artefacts in the
 maps or difficulties in refinement"""
      else:
        message =  """indicates that there probably is some
 systematic dependence between the anisotropy and not-so-well-defined
 intensities. Because the signal to noise for the most affected intensities
 is relatively good, the affect on maps or refinement behavior is most likely
 not very serious."""
    if (self.mean_count is not None):
      out.show("""
 For the resolution shell spanning between %4.2f - %4.2f Angstrom,
 the mean I/sigI is equal to %5.2f. %4.1f %% of these intensities have
 an I/sigI > 3. When sorting these intensities by their anisotropic
 correction factor and analysing the I/sigI behavior for this ordered
 list, we can gauge the presence of 'anisotropy induced noise amplification'
 in reciprocal space.
""" % (self.max_d, self.min_d, self.mean_isigi, 100.0*self.mean_count))
      out.show("""\
 The quarter of Intensities *least* affected by the anisotropy correction show
""")
      out.show_preformatted_text("""\
    <I/sigI>                 :   %5.2e
    Fraction of I/sigI > 3   :   %5.2e     ( Z = %8.2f )""" %
        (self.mean_isigi_low_correction_factor,
        self.frac_below_low_correction,
        self.z_low))
      out.show("""\
  The quarter of Intensities *most* affected by the anisotropy correction show
""")
      out.show_preformatted_text("""\
    <I/sigI>                 :   %5.2e
    Fraction of I/sigI > 3   :   %5.2e     ( Z = %8.2f )""" %
        (self.mean_isigi_high_correction_factor,
         self.frac_below_high_correction,
         self.z_high))
      #out.show(""" The combined Z-score of %8.2f %s""" % (self.z_tot,
      #  message))
      out.show("""\
 Z-scores are computed on the basis of a Bernoulli model assuming independence
 of weak reflections with respect to anisotropy.""")

  def summarize_issues(self):
    b_cart_mean = sum(self.b_cart[0:3]) / 3
    b_cart_range = max(self.b_cart[0:3]) - min(self.b_cart[0:3])
    aniso_ratio = b_cart_range / b_cart_mean
    # Using VTF cutoffs (from Read et al. 2011)
    if (aniso_ratio >= 1) and (b_cart_mean > 20):
      return [(2, "The data show severe anisotropy.",
        "Maximum likelihood anisotropic Wilson scaling")]
    elif (aniso_ratio >= 0.5) and (b_cart_mean > 20):
      return [(1, "The data are moderately anisotropic.",
        "Maximum likelihood anisotropic Wilson scaling")]
    else :
      return [(0, "The data are not significantly anisotropic.",
        "Maximum likelihood anisotropic Wilson scaling")]

class kernel_normalisation(object):
  def __init__(self,
               miller_array,
               kernel_width=None,
               n_bins=23,
               n_term=13,
               d_star_sq_low=None,
               d_star_sq_high=None,
               auto_kernel=False,
               number_of_sorted_reflections_for_auto_kernel=50):
    ## Autokernel is either False, true or a specific integer
    if kernel_width is None:
      assert (auto_kernel is not False)
    if auto_kernel is not False:
      assert (kernel_width==None)
    assert miller_array.size()>0
    ## intensity arrays please
    work_array = None
    if not miller_array.is_real_array():
      raise RuntimeError("Please provide real arrays only")
      ## I might have to change this upper condition
    if miller_array.is_xray_amplitude_array():
      work_array = miller_array.f_as_f_sq()
    if miller_array.is_xray_intensity_array():
      work_array = miller_array.deep_copy()
      work_array = work_array.set_observation_type(miller_array)
    ## If type is not intensity or amplitude
    ## raise an execption please
    if not miller_array.is_xray_intensity_array():
      if not miller_array.is_xray_amplitude_array():
        raise RuntimeError("Observation type unknown")
    ## declare some shorthands
    I_obs = work_array.data()
    epsilons = work_array.epsilons().data().as_double()
    d_star_sq_hkl = work_array.d_spacings().data()
    d_star_sq_hkl = 1.0/(d_star_sq_hkl*d_star_sq_hkl)
    ## Set up some limits
    if d_star_sq_low is None:
      d_star_sq_low = flex.min(d_star_sq_hkl)
    if d_star_sq_high is None:
      d_star_sq_high = flex.max(d_star_sq_hkl)
    ## A feeble attempt to determine an appropriate kernel width
    ## that seems to work reasonable in practice
    self.kernel_width=kernel_width
    if auto_kernel is not False:
      ## Otherwise we can get into an infinite loop below
      assert d_star_sq_hkl.size() > 1
      ## get the d_star_sq_array and sort it
      sort_permut = flex.sort_permutation(d_star_sq_hkl)
      ##
      if auto_kernel==True:
        number=number_of_sorted_reflections_for_auto_kernel
      else:
        number=int(auto_kernel)
      number = min(number, d_star_sq_hkl.size()-1)
      self.kernel_width = d_star_sq_hkl[sort_permut[number]]-d_star_sq_low
      if self.kernel_width ==0:
        original_number=number
        while number < d_star_sq_hkl.size()/2:
          number+=original_number
          self.kernel_width = d_star_sq_hkl[sort_permut[number]]-d_star_sq_low
          if self.kernel_width>0: break

      assert self.kernel_width > 0
    ## Making the d_star_sq_array
    assert (n_bins>1) ## assure that there are more then 1 bins for interpolation
    self.d_star_sq_array = chebyshev_lsq_fit.chebyshev_nodes(
      n=n_bins,
      low=d_star_sq_low,
      high=d_star_sq_high,
      include_limits=True)

    ## Now get the average intensity please
    ##
    ## This step can be reasonably time consuming
    self.mean_I_array = scaling.kernel_normalisation(
      d_star_sq_hkl = d_star_sq_hkl,
      I_hkl = I_obs,
      epsilon = epsilons,
      d_star_sq_array = self.d_star_sq_array,
      kernel_width = self.kernel_width
      )
    self.var_I_array = scaling.kernel_normalisation(
      d_star_sq_hkl = d_star_sq_hkl,
      I_hkl = I_obs*I_obs,
      epsilon = epsilons*epsilons,
      d_star_sq_array = self.d_star_sq_array,
      kernel_width = self.kernel_width
      )
    self.var_I_array = self.var_I_array - self.mean_I_array*self.mean_I_array
    self.weight_sum = self.var_I_array = scaling.kernel_normalisation(
      d_star_sq_hkl = d_star_sq_hkl,
      I_hkl = I_obs*0.0+1.0,
      epsilon = epsilons*0.0+1.0,
      d_star_sq_array = self.d_star_sq_array,
      kernel_width = self.kernel_width
      )
    eps = 1e-16 # XXX Maybe this should be larger?
    self.bin_selection = (self.mean_I_array > eps)
    sel_pos = self.bin_selection.iselection()
    # FIXME rare bug: this crashes when the majority of the data are zero,
    # e.g. because resolution limit was set too high and F/I filled in with 0.
    # it would be good to catch such cases in advance by inspecting the binned
    # values, and raise a different error message.
    assert sel_pos.size() > 0
    if (sel_pos.size() < self.mean_I_array.size() / 2):
      raise Sorry("Analysis could not be continued because more than half "+
        "of the data have values below 1e-16.  This usually indicates either "+
        "an inappropriately high resolution cutoff, or an error in the data "+
        "file which artificially creates a higher resolution limit.")
    self.mean_I_array = self.mean_I_array.select(sel_pos)
    self.d_star_sq_array = self.d_star_sq_array.select(sel_pos)
    self.var_I_array = flex.log( self.var_I_array.select( sel_pos ) )
    self.weight_sum = self.weight_sum.select(sel_pos)
    self.mean_I_array = flex.log( self.mean_I_array )
    ## Fit a chebyshev polynome please
    normalizer_fit_lsq = chebyshev_lsq_fit.chebyshev_lsq_fit(
      n_term,
      self.d_star_sq_array,
      self.mean_I_array )
    self.normalizer = chebyshev_polynome(
      n_term,
      d_star_sq_low,
      d_star_sq_high,
      normalizer_fit_lsq.coefs)
    var_lsq_fit = chebyshev_lsq_fit.chebyshev_lsq_fit(
      n_term,
      self.d_star_sq_array,
      self.var_I_array )
    self.var_norm = chebyshev_polynome(
      n_term,
      d_star_sq_low,
      d_star_sq_high,
      var_lsq_fit.coefs)
    ws_fit = chebyshev_lsq_fit.chebyshev_lsq_fit(
      n_term,
      self.d_star_sq_array,
      self.weight_sum )
    self.weight_sum = chebyshev_polynome(
      n_term,
      d_star_sq_low,
      d_star_sq_high,
      ws_fit.coefs)

    ## The data wil now be normalised using the
    ## chebyshev polynome we have just obtained
    self.mean_I_array = flex.exp( self.mean_I_array)
    self.normalizer_for_miller_array =  flex.exp( self.normalizer.f(d_star_sq_hkl) )
    self.var_I_array = flex.exp( self.var_I_array )
    self.var_norm = flex.exp( self.var_norm.f(d_star_sq_hkl) )
    self.weight_sum = flex.exp( self.weight_sum.f(d_star_sq_hkl))
    self.normalised_miller = None
    self.normalised_miller_dev_eps = None
    if work_array.sigmas() is not None:
      self.normalised_miller = work_array.customized_copy(
        data = work_array.data()/self.normalizer_for_miller_array,
        sigmas = work_array.sigmas()/self.normalizer_for_miller_array
        ).set_observation_type(work_array)
      self.normalised_miller_dev_eps = self.normalised_miller.customized_copy(
        data = self.normalised_miller.data()/epsilons,
        sigmas = self.normalised_miller.sigmas()/epsilons)\
        .set_observation_type(work_array)
    else:
      self.normalised_miller = work_array.customized_copy(
        data = work_array.data()/self.normalizer_for_miller_array
        ).set_observation_type(work_array)
      self.normalised_miller_dev_eps = self.normalised_miller.customized_copy(
        data = self.normalised_miller.data()/epsilons)\
        .set_observation_type(work_array)


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/basic_analyses.py
from __future__ import absolute_import, division, print_function
from cctbx import adptbx
from cctbx.array_family import flex
from libtbx.utils import Sorry, show_exception_info_if_full_testing
import mmtbx.scaling
from mmtbx.scaling import absolute_scaling, relative_wilson
from mmtbx.scaling import matthews
from mmtbx.scaling import data_statistics
import iotbx.bioinformatics
import sys


class basic_analyses(object):  # XXX is this ever used?
  def __init__(self,
               miller_array,
               phil_object,
               out=None,
               out_plot=None, miller_calc=None,
               original_intensities=None,
               completeness_as_non_anomalous=None,
               verbose=0):
    if out is None:
      out=sys.stdout
    if verbose>0:
      print(file=out)
      print(file=out)
      print("Matthews coefficient and Solvent content statistics", file=out)
    n_copies_solc = 1.0
    self.nres_known = False
    if (phil_object.scaling.input.asu_contents.n_residues is not None or
        phil_object.scaling.input.asu_contents.n_bases is not None):
      self.nres_known = True
      if (phil_object.scaling.input.asu_contents.sequence_file is not None):
        print("  warning: ignoring sequence file", file=out)
    elif (phil_object.scaling.input.asu_contents.sequence_file is not None):
      print("  determining composition from sequence file %s" % \
        phil_object.scaling.input.asu_contents.sequence_file, file=out)
      seq_comp = iotbx.bioinformatics.composition_from_sequence_file(
        file_name=phil_object.scaling.input.asu_contents.sequence_file,
        log=out)
      if (seq_comp is not None):
        phil_object.scaling.input.asu_contents.n_residues = seq_comp.n_residues
        phil_object.scaling.input.asu_contents.n_bases = seq_comp.n_bases
        self.nres_known = True
    matthews_results =matthews.matthews_rupp(
      crystal_symmetry = miller_array,
      n_residues = phil_object.scaling.input.asu_contents.n_residues,
      n_bases = phil_object.scaling.input.asu_contents.n_bases,
      out=out,verbose=1)
    phil_object.scaling.input.asu_contents.n_residues = matthews_results[0]
    phil_object.scaling.input.asu_contents.n_bases = matthews_results[1]
    n_copies_solc = matthews_results[2]
    self.matthews_results = matthews_results

    if phil_object.scaling.input.asu_contents.n_copies_per_asu is not None:
      n_copies_solc = phil_object.scaling.input.asu_contents.n_copies_per_asu
      self.defined_copies = n_copies_solc
      if verbose>0:
        print("Number of copies per asymmetric unit provided", file=out)
        print(" Will use user specified value of ", n_copies_solc, file=out)
    else:
      phil_object.scaling.input.asu_contents.n_copies_per_asu = n_copies_solc
      self.guessed_copies = n_copies_solc

    # first report on I over sigma
    miller_array_new = miller_array
    self.data_strength = None
    miller_array_intensities = miller_array
    if (original_intensities is not None):
      assert original_intensities.is_xray_intensity_array()
      miller_array_intensities = original_intensities
    if miller_array_intensities.sigmas() is not None:
      data_strength=data_statistics.i_sigi_completeness_stats(
        miller_array_intensities,
        isigi_cut = phil_object.scaling.input.parameters.misc_twin_parameters.twin_test_cuts.isigi_cut,
        completeness_cut = phil_object.scaling.input.parameters.misc_twin_parameters.twin_test_cuts.completeness_cut,
      completeness_as_non_anomalous=completeness_as_non_anomalous)
      data_strength.show(out)
      self.data_strength = data_strength
      if phil_object.scaling.input.parameters.misc_twin_parameters.twin_test_cuts.high_resolution is None:
        if data_strength.resolution_cut > data_strength.resolution_at_least:
          phil_object.scaling.input.parameters.misc_twin_parameters.twin_test_cuts.high_resolution = data_strength.resolution_at_least
        else:
           phil_object.scaling.input.parameters.misc_twin_parameters.twin_test_cuts.high_resolution = data_strength.resolution_cut

    ## Isotropic wilson scaling
    if verbose>0:
      print(file=out)
      print(file=out)
      print("Maximum likelihood isotropic Wilson scaling ", file=out)

    n_residues =  phil_object.scaling.input.asu_contents.n_residues
    n_bases = phil_object.scaling.input.asu_contents.n_bases
    if n_residues is None:
      n_residues = 0
    if n_bases is None:
      n_bases = 0
    if n_bases+n_residues==0:
      raise Sorry("No scatterers available")
    iso_scale_and_b = absolute_scaling.ml_iso_absolute_scaling(
      miller_array = miller_array_new,
      n_residues = n_residues*
      miller_array.space_group().order_z()*n_copies_solc,
      n_bases=n_bases*
      miller_array.space_group().order_z()*n_copies_solc)
    iso_scale_and_b.show(out=out,verbose=verbose)
    self.iso_scale_and_b = iso_scale_and_b
    ## Store the b and scale values from isotropic ML scaling
    self.iso_p_scale = iso_scale_and_b.p_scale
    self.iso_b_wilson =  iso_scale_and_b.b_wilson


    ## Anisotropic ml wilson scaling
    if verbose>0:
      print(file=out)
      print(file=out)
      print("Maximum likelihood anisotropic Wilson scaling ", file=out)
    aniso_scale_and_b = absolute_scaling.ml_aniso_absolute_scaling(
      miller_array = miller_array_new,
      n_residues = n_residues*miller_array.space_group().order_z()*n_copies_solc,
      n_bases = n_bases*miller_array.space_group().order_z()*n_copies_solc)
    aniso_scale_and_b.show(out=out,verbose=1)

    self.aniso_scale_and_b = aniso_scale_and_b

    try: b_cart = aniso_scale_and_b.b_cart
    except AttributeError as e:
      print("*** ERROR ***", file=out)
      print(str(e), file=out)
      show_exception_info_if_full_testing()
      return

    self.aniso_p_scale = aniso_scale_and_b.p_scale
    self.aniso_u_star  = aniso_scale_and_b.u_star
    self.aniso_b_cart  = aniso_scale_and_b.b_cart
    # XXX: for GUI
    self.overall_b_cart = getattr(aniso_scale_and_b, "overall_b_cart", None)

    ## Correcting for anisotropy
    if verbose>0:
      print("Correcting for anisotropy in the data", file=out)
      print(file=out)

    b_cart_observed = aniso_scale_and_b.b_cart

    b_trace_average = (b_cart_observed[0]+
                       b_cart_observed[1]+
                       b_cart_observed[2])/3.0
    b_trace_min = b_cart_observed[0]
    if  b_cart_observed[1] <b_trace_min: b_trace_min=b_cart_observed[1]
    if  b_cart_observed[2] <b_trace_min: b_trace_min=b_cart_observed[2]

    if phil_object.scaling.input.optional.aniso.final_b == "eigen_min":
       b_use=aniso_scale_and_b.eigen_values[2]
    elif phil_object.scaling.input.optional.aniso.final_b == "eigen_mean" :
       b_use=flex.mean(aniso_scale_and_b.eigen_values)
    elif phil_object.scaling.input.optional.aniso.final_b == "user_b_iso":
       assert phil_object.scaling.input.optional.aniso.b_iso is not None
       b_use=phil_object.scaling.input.optional.aniso.b_iso
    else:
       b_use = 30

    b_cart_aniso_removed = [ -b_use,
                             -b_use,
                             -b_use,
                             0,
                             0,
                             0]
    u_star_aniso_removed = adptbx.u_cart_as_u_star(
      miller_array.unit_cell(),
      adptbx.b_as_u( b_cart_aniso_removed  ) )
    ## I do things in two steps, but can easely be done in 1 step
    ## just for clarity, thats all.
    self.no_aniso_array = absolute_scaling.anisotropic_correction(
      miller_array_new,0.0,aniso_scale_and_b.u_star )
    self.no_aniso_array = absolute_scaling.anisotropic_correction(
      self.no_aniso_array,0.0,u_star_aniso_removed)
    self.no_aniso_array = self.no_aniso_array.set_observation_type(
      miller_array )


    ## Make normalised structure factors please

    sel_big = self.no_aniso_array.data() > 1.e+50
    self.no_aniso_array = self.no_aniso_array.array(
      data = self.no_aniso_array.data().set_selected(sel_big, 0))
    self.no_aniso_array = self.no_aniso_array.set_observation_type(
      miller_array )

    normalistion = absolute_scaling.kernel_normalisation(
      self.no_aniso_array,auto_kernel=True)
    self.normalised_miller = normalistion.normalised_miller.deep_copy()


    self.phil_object=phil_object

    ## Some basic statistics and sanity checks follow
    if verbose>0:
      print("Some basic intensity statistics follow.", file=out)
      print(file=out)

    basic_data_stats = data_statistics.basic_intensity_statistics(
      miller_array,
      aniso_scale_and_b.p_scale,
      aniso_scale_and_b.u_star,
      iso_scale_and_b.scat_info,
      out=out,
      out_plot=out_plot)
    self.basic_data_stats = basic_data_stats
    self.miller_array = basic_data_stats.new_miller

    #relative wilson plot
    self.rel_wilson = None
    if (miller_calc is not None) and (miller_calc.d_min() < 4.0):
      try :
        self.rel_wilson = relative_wilson.relative_wilson(
          miller_obs=miller_array,
          miller_calc=miller_calc)
      except RuntimeError as e :
        print("*** Error calculating relative Wilson plot - skipping.", file=out)
        print("", file=out)

    if verbose>0:
      print("Basic analyses completed", file=out)


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/bayesian_estimator.py
from __future__ import absolute_import, division, print_function
#
# bayesian_estimator.py
# tct 053008, updated 2014-10-29
#
# Creates a Bayesian estimator from a list of records containing y and {x_i}
# Then you can use it to estimate values of y from a new  {x_i}
#
# For typical use just copy and edit exercise_2 below
#
# For use with data that may have missing entries for some predictors, try
#  exercise_group below
#
# Parameters to set:  number of bins, minimum observations in each bin,
#  number of bins to smooth over.   Normally you want 20 or more observations
#  in each bin when you are training the estimator. You also want enough bins
#  so that adjacent bins have similar values compared to the SD of the
#  predictor variables (i.e., the range of values of x_1 in bin 3 should overlap
#  the range of values of x_1 in bin 4)  (no big jumps).
#  If you have too few observations to satisify these criteria, then you can
#  smooth the bins (it rarely hurts in any event).
#
# purpose: take a list of N measurements x_ij of M types x_i, and
#  N perfect values p_j, #   and generate an estimator that will estimate
#  y from one set of measurements of the M types.
#
# Method: assume that for any given value of y, the values of the
# M measurements will have Gaussian distributions around their means u_i(y)
# NOTE: Current version assumes flat priors.
#
# Here is how we can calculate p({x_i},y) any time we want for
# any {x_i} and any y:

#  From our assumption:
#       p({x_i},y)=p({x_i}|y)p(y)   # definition
#  If our distributions are Gaussian then we can represent the joint
#    probability of all the measurements (given y) of p({x_i}|y) as:
#    p({x_i}|y) = exp{-0.5(x-u)S-1(x-u)T } / (2pi det(S)
#  where x is {x_i}, u is {u_i} (the vector of means) and S is the covariance
#  matrix. We can calculate u and S from the members of our list of
#  measurements that led to the value y.
#
# Now given a set of measurements {x_i} we use Bayes' rule to estimate y:
#
# p(y)= p({x_i},y)/ integral_over_y[ p({x_i},y ]
#
# Implementation details:

# Need a set of bins n_bins_i for each variable x_i  and for y as we have
# no model for  how x_i and y are related (we want this to be general).

# If a bin has too few data, just pull data from nearby bins until there are
#   at least min_in_bin data.

# May be necessary to smooth the data in the bins if it is sparse.
# Average the means vector and the covariance matrices over smooth_bins if so


import os,sys,math,random
from scitbx import matrix
from cctbx.array_family import flex
from copy import deepcopy
from libtbx.utils import Sorry,null_out
from six.moves import zip
from six.moves import range

pi=2.*math.atan2(1.,0.)

class bayesian_estimator:
  def __init__(self,out=None,verbose=False,skip_covariance=True,
     minimum_records=20,trim_predictor_values=True):
    if out is None:
      self.out=sys.stdout
    else:
      self.out=out
    self.verbose=verbose
    self.trim_predictor_values=trim_predictor_values # limit to range of training set
    self.skip_covariance=skip_covariance
    self.minimum_records=minimum_records
    self.ok=True

  def create_estimator(self,record_list=None,
    n_bin=None,range_low=None,range_high=None,
    min_in_bin=20,use_flat_y_dist=False,
    smooth_bins=10):

    if not n_bin:
       raise Sorry(
     "Please set values for n_bin,eg., n_bin=100, min_in_bin=5, smooth_bins=5")
    self.record_list=record_list# vectors with measurements. First one is y
    self.n_bin=n_bin # bins of y
    self.smooth_bins=smooth_bins # smoothing the bins
    self.use_flat_y_dist=use_flat_y_dist # allow user to ignore dist of y
    self.min_in_bin=min_in_bin # minimum records in a bin
    self.range_low=range_low  # range_low of y
    self.range_high=range_high  # range_high of y
    self.record_list_range_low=None
    self.record_list_range_high=None
    # get set up:
    ok=self.check_estimator_inputs() # check and print out what we are doing..
    if not ok:
      self.ok=False
      return  # give up

    self.record_list.sort()  # sort on y (first variable in each record)
                             # so it is easy to bin
    # Split by bins of y
    self.get_range()  # decide on range of y if user didn't set it.
    self.split_into_bins() # split y and measurements x_i into self.n_bin bins
    self.merge_bins_if_nec() # if a bin has fewer than min_in_bin...take its
                             # neighbors too..
    # ready to create estimator
    self.create_estimator_lists()
    self.setup_p_y()

    # Now we can calculate p({x_i}|y) based on our means and covariance:
    #  prob=self.get_p_xx(values,bin)

    # and to get  y_bar and SD using p(y|{x_i}) we just sum up
    #  y_bar,sd=self.get_y_bar_sd_given_x({x_i})

  def apply_estimator(self,record_list_or_values,single_value=False):
    if not self.ok: return None,None

    if single_value:
      return self.get_y_bar_sd_given_x(record_list_or_values)

    predicted_data=flex.double()
    predicted_sd_data=flex.double()
    for values in record_list_or_values:
      y_bar,sd=self.get_y_bar_sd_given_x(values)
      predicted_data.append(y_bar)
      predicted_sd_data.append(sd)
    return predicted_data,predicted_sd_data

  def get_y_bar_sd_given_x(self,x_list):
    # return weighted value of y and SD of this
    p_y=flex.double()
    for bin in self.bins:
      p_y.append(self.get_p_xx(x_list,bin)*self.get_p_y(bin))
    sum=flex.sum(p_y)
    if sum<=0.0: sum=1.
    p_y=p_y/sum  # normalized probability
    weighted_bin=flex.sum(p_y*self.bin_mid_double)
    weighted_sq_bin=flex.sum(p_y*self.bin_mid_double*self.bin_mid_double)
    sd=weighted_sq_bin-weighted_bin**2
    if sd<0.: sd=0.
    sd=math.sqrt(sd)
    return weighted_bin,sd

  def get_p_xx(self,x_list,bin):
    if self.trim_predictor_values:
      x_vector=flex.double()
      for lower,higher,value in zip(self.record_list_range_low,
         self.record_list_range_high,x_list):
        x_vector.append(max(lower,min(higher,value)))
    else:
      x_vector=flex.double(x_list)

    means_vector,cov_matrix_inv,determinant=self.get_mean_cov_inv_determinant(bin)
    if means_vector is None or cov_matrix_inv is None or determinant is None:
      return 0.0 #
    delta_vect=x_vector-means_vector
    delta_row_matrix=matrix.rec(delta_vect,[len(x_list),1])
    delta_col_matrix=delta_row_matrix.transpose()
    inner=delta_col_matrix*cov_matrix_inv*delta_row_matrix
    v=inner.as_list_of_lists()[0][0]
    if v < -64.: v=-64.
    elif v> 64.: v=64.
    prob=math.exp(-0.5*v)/(2.*pi*determinant)
    return prob

  def get_p_y(self,bin):
    return self.bin_p_y[bin]

  def setup_p_y(self):
    sum=0.
    for bin in self.bins:
      if self.use_flat_y_dist:
        sum=sum+1.
      else:
        sum=sum+self.bin_orig_number_of_records[bin]
    self.bin_p_y=[]
    if sum==0: sum=1.
    for bin in self.bins:
      if self.use_flat_y_dist:
        self.bin_p_y.append(1./sum)
      else:
        self.bin_p_y.append(self.bin_orig_number_of_records[bin]/sum)

  def create_estimator_lists(self):
    self.bin_means_list=[]
    self.bin_cov_list=[]

    self.bin_cov_inv_list=[]
    self.bin_determinant_list=[]
    for bin in self.bins:
       records=self.get_records_in_bin(bin,
           self.bin_start_list,self.bin_after_end_list)
       if self.verbose:
         print("Working on bin ",bin," with ",len(records)," records", file=self.out)
       # now get vector of means and covariance matrix for these records.
       means_vector,cov_matrix=self.get_mean_cov_from_records(records)
       self.bin_means_list.append(means_vector)
       self.bin_cov_list.append(cov_matrix)

    # now smooth values of cov_matrix and means_list if desired
    self.smooth_cov_and_means()
    for bin in self.bins:
       cov_matrix=self.bin_cov_list[bin]
       try:
         self.bin_cov_inv_list.append(cov_matrix.inverse())
         self.bin_determinant_list.append(cov_matrix.determinant())
       except Exception as e:  # sets result to zero later if we do this...
         self.bin_cov_inv_list.append(None)
         self.bin_determinant_list.append(None)

  def smooth_cov_and_means(self): # smooth with window of smooth_bins
                                  # only smooth symmetrically, so ends not done
    if self.smooth_bins<2: return
    new_bin_cov_list=self.bin_cov_list
    new_bin_means_list=self.bin_means_list
    for bin in self.bins:
      local_window=self.smooth_bins//2
      if bin < local_window: local_window=bin
      elif self.n_bin-1-bin < local_window: local_window=self.n_bin-1-bin
      new_bin_cov_list[bin]=self.smooth_list(self.bin_cov_list,bin,local_window,
        is_matrix=True)
      new_bin_means_list[bin]=\
        self.smooth_list(self.bin_means_list,bin,local_window)
      if self.verbose:
        print("Bin ",bin," smoothed with window of ",local_window, file=self.out)
    self.bin_cov_list=new_bin_cov_list
    self.bin_means_list=new_bin_means_list

  def smooth_list(self,bin_value_list,bin,local_window,is_matrix=False):
    # smooth each entry in value_list[bin] with local window.
    new_entry_list=[]
    for i in range(len(bin_value_list[bin])): # one entry at a time
      sum=0.
      sumn=0.
      for local_bin in range(max(0,bin-local_window),
        min(bin+local_window+1,len(bin_value_list))): # XXX necessary?
        sumn+=1.
        if is_matrix:
          sum+=bin_value_list[local_bin].elems[i]
        else:
          sum+=bin_value_list[local_bin][i]
      if sumn==0.: sumn=1.
      new_entry_list.append(sum/sumn)
    if is_matrix:
      new_entry_list=matrix.rec(flex.double(new_entry_list),[self.n,self.n])
    else:
      new_entry_list=flex.double(new_entry_list)

    if self.verbose:
      print('smoothing bin ',bin,' value=',bin_value_list[bin],\
       'new_value=',new_entry_list, file=self.out)
    return new_entry_list



  def get_mean_cov_from_records(self,records):
    # get the vector of means and covariance matr
    # 070308: If self.skip_covariance=True then set covariance terms to zero.
    # rewrite our data into a list of flex.double() vectors
    if records is None or len(records)<2: return None,None

    list_of_vectors=[]
    for i in range(len(records[0])-1):
     list_of_vectors.append(flex.double())
    for record in records:
      values=record[1:]  # skip the y-value at the start
      for value,vector in zip(values,list_of_vectors):
        vector.append(value)

    list_of_means=[]
    list_of_indices=[]
    i=0
    for x in list_of_vectors:
      i+=1
      list_of_means.append(flex.mean(x))
      list_of_indices.append(i)
    if self.verbose:
      print("Means: ",list_of_means, file=self.out)
    m=[]
    for i,x,mean_x in zip(list_of_indices,list_of_vectors,list_of_means):
      for j,y,mean_y in zip(list_of_indices,list_of_vectors,list_of_means):
        covar=flex.mean((x-mean_x)*(y-mean_y))
        if self.verbose:
          print("covar: ",i,j,covar, file=self.out)
        if self.skip_covariance and i!=j:
          covar=0.0
          if self.verbose:
            print("NOTE: Covariance set to zero as skip_covariance=True", file=self.out)
        m.append(covar)
    mx=matrix.rec(m,2*[len(list_of_vectors)])
    return flex.double(list_of_means),mx

  def get_mean_cov_inv_determinant(self,bin):
    if bin<0 or bin>self.n_bin-1: return None,None
    return self.bin_means_list[bin],self.bin_cov_inv_list[bin], \
      self.bin_determinant_list[bin]

  def check_estimator_inputs(self):
    assert self.record_list is not None
    assert len(self.record_list)>0
    self.n=len(self.record_list[0])-1   # number of indep variables
    print("Creating Bayesian estimator from ",self.n," measurement vectors", file=self.out)
    print("and one vector of perfect values", file=self.out)
    assert self.n>0 # need at least one meas and one obs
    print("Total of ",len(self.record_list)," values of each", file=self.out)
    if len(self.record_list)<self.minimum_records:
      return False
    for v in self.record_list:
       assert len(v)==len(self.record_list[0])
       assert type(v)==type([1,2,3])

    if self.n_bin is None:
      self.n_bin=len(self.record_list)//50
      if self.n_bin < 2: self.n_bin=2
    print("Number of bins: ",self.n_bin, file=self.out)
    assert self.n_bin>0
    return True

  def get_range(self):
    if self.range_low is None or self.range_high is None:
      self.range_low=self.record_list[0][0]
      self.range_high=self.record_list[-1][0]
    print("Range of y: ",self.range_low,self.range_high, file=self.out)
    self.full_range=self.range_high-self.range_low
    assert self.full_range > 0.
    self.delta_range=self.full_range/self.n_bin
    print("Delta range: ",self.delta_range, file=self.out)
    self.bins=range(self.n_bin)

    # 2014-11-13 also get range for x-values
    n=len(self.record_list[0])-1
    self.record_list_range_low=n*[None]
    self.record_list_range_high=n*[None]
    for record in self.record_list:
      for i in range(n):
        ii=i+1
        if self.record_list_range_low[i] is None or \
             record[ii]<self.record_list_range_low[i]:
             self.record_list_range_low[i]=record[ii]
        if self.record_list_range_high[i] is None or \
             record[ii]>self.record_list_range_high[i]:
             self.record_list_range_high[i]=record[ii]
    print("Range for predictor variables:", file=self.out)
    for i in range(n):
      print("%d:  %7.2f - %7.2f " %(
          i,self.record_list_range_low[i],self.record_list_range_high[i]), file=self.out)

  def split_into_bins(self):
    self.bin_start_list=self.n_bin*[None]  # first record in this bin
    self.bin_after_end_list=self.n_bin*[None] # (last record in this bin)+1
    i_record=-1
    for record in self.record_list:
      i_record+=1
      i_bin=self.get_bin(record[0])  # get bin based on y
      if self.bin_start_list[i_bin] is None:
         self.bin_start_list[i_bin]=i_record
      if self.bin_after_end_list[i_bin] is None or \
             self.bin_after_end_list[i_bin] < i_record+1:
         self.bin_after_end_list[i_bin]=i_record+1

    # save original number of records in each bin
    self.bin_orig_number_of_records=[]
    self.bin_mid_list=[]
    for bin in self.bins:
       records=self.get_records_in_bin(bin,
           self.bin_start_list,self.bin_after_end_list)
       self.bin_orig_number_of_records.append(len(records))
       self.bin_mid_list.append(self.get_mid(bin))
    self.bin_mid_double=flex.double(self.bin_mid_list)

    if self.verbose:
      for bin,n in zip(self.bins,self.bin_orig_number_of_records):
        print("Records in bin ",bin,": ",n, file=self.out)

  def merge_bins_if_nec(self):
    # if a bin has fewer than min_in_bin...take neighbors
    self.new_bin_start_list=deepcopy(self.bin_start_list)
    self.new_bin_after_end_list=deepcopy(self.bin_after_end_list)
    for bin in self.bins:
       n=len(self.get_records_in_bin(bin,
           self.bin_start_list,self.bin_after_end_list))
       if n>=self.min_in_bin: continue

       offset=0
       while n < self.min_in_bin and offset < self.n_bin:
         offset+=1
         low=bin-offset
         high=bin+offset
         if low<0: low=0
         if high>self.n_bin-1: high=self.n_bin-1
         new_start=self.bin_start_list[low]
         new_after_end=self.bin_after_end_list[high]
         if new_start is not None:
            self.new_bin_start_list[bin]=new_start
         if new_after_end is not None:
            self.new_bin_after_end_list[bin]=new_after_end

         n=len(self.get_records_in_bin(bin,
           self.new_bin_start_list,self.new_bin_after_end_list))
       if self.verbose:
         print("Reset bin ",bin,\
            " to contain records ",self.new_bin_start_list[bin],\
            ' to ',self.new_bin_after_end_list[bin], file=self.out)
       if self.new_bin_start_list[bin] is None or  \
          self.new_bin_after_end_list[bin] is None:
          raise AssertionError("Sorry, need more data or smaller value of "+\
             "min_in_bin (currently "+str(self.min_in_bin)+")")
       if self.verbose:
         print("New number of records: ", \
         -self.new_bin_start_list[bin] +self.new_bin_after_end_list[bin], file=self.out)
    self.bin_start_list=self.new_bin_start_list
    self.bin_after_end_list=self.new_bin_after_end_list


  def get_records_in_bin(self,i_bin,bin_start_list,bin_after_end_list):
    # return the records in bin i_bin
    if i_bin<0 or i_bin>self.n_bin : return []
    start=bin_start_list[i_bin]
    after_end=bin_after_end_list[i_bin]
    if start is None or after_end is None: return []
    return self.record_list[start:after_end]


  def get_mid(self,i_bin):  # midpoint in y of bin i_bin
    return self.range_low+self.delta_range*float(i_bin)

  def get_bin(self,value):
    a_bin= float(self.n_bin)*(value - self.range_low)/self.full_range
    i_bin=int(a_bin)
    # bin 0 goes from range_low to range_low+(range_high-range_low)/self.n_bin
    if i_bin>self.n_bin-1: i_bin=self.n_bin-1
    if i_bin<0: i_bin=0
    return i_bin

  def generate_data(self,sig_list,n=1000,missing=True,non_linear=False):
    perfect_data=flex.double()
    data_record_list=[]
    record_list=[]
    for i in range(n):
      u=float(i)*1000./float(n)
      record=[]
      pp=u/100.
      if non_linear:
        r=pp/10
        r=r**2+.5*r**3+2.*r**4
      else:
        r=pp
      if missing and u>199. and u<275.: continue
      if missing and u>0 and u<50.:continue
      record.append(pp)
      perfect_data.append(pp)
      s1=random.gauss(0.,sig_list[0])
      for s in sig_list[1:]:
        ss=random.gauss(0.,s)
        record.append(r+s1+ss)
      record_list.append(record)
      data_record_list.append(record[1:])
    print("Set up ",len(record_list),\
         " y values and measurements of a,b ", file=self.out)
    return perfect_data,data_record_list,record_list

  def get_average_data(self,records):
    avg=flex.double()
    for values in records:
      v=flex.double(values)
      avg.append(flex.mean(v))
    return avg

  def exercise(self,iseed=39771):

    print("\nTESTING runs of Bayesian estimator with and without missing ", file=self.out)
    print("data and with and without use of covariance.\n", file=self.out)
    # set up our arrays
    # our model is a and b are related to p, with correlated error s1 and
    #   independent errors s2 and s3
    # a=p+s1+s2
    # b=p+s1+s3
    import random
    random.seed(iseed)
    result_list=[]
    for missing,non_linear in zip([True,False],[False,True]):
     for skip_covar in [False,True]:
      print("Missing data: ",missing,"   Non-linear: ",non_linear," Skip-covariance: ",skip_covar, file=self.out)
      record_list=[]
      sig_list=[.50,0.2,0.8]
      perfect_data,records,record_list=\
         self.generate_data(sig_list,n=1000,missing=missing,
         non_linear=non_linear)
      self.skip_covariance=skip_covar
      self.create_estimator(record_list=record_list,n_bin=100,
         use_flat_y_dist=True,
         min_in_bin=5,
         smooth_bins=0)

      more_perfect_data,more_records,more_record_list=\
         self.generate_data(sig_list,n=200,missing=missing,
         non_linear=non_linear)
      predicted_data,predicted_sd_data=\
         self.apply_estimator(more_records)

      c=flex.linear_correlation(more_perfect_data,predicted_data)
      cc=c.coefficient()

      average_data=self.get_average_data(more_records)
      c=flex.linear_correlation(more_perfect_data,average_data)
      cc_avg=c.coefficient()
      print(" CC: ",cc," just averaging: ",cc_avg, file=self.out)
      print(file=self.out)
      result_list.append(cc_avg)
      if 0:
        f=open('test'+str(non_linear)+'.out','w')
        for perf,pred,sd,values in zip(
          more_perfect_data,predicted_data,predicted_sd_data,more_records):
          print(perf,pred,sd, end=' ', file=f)
        for value in values: print(value, end=' ', file=f)
        print(file=f)
        f.close()
    return result_list


  def exercise_2(self,iseed=712771,out=sys.stdout):

   # You can just cut and paste this into a new python script and run it
   # Then edit it for your purpose.

   print("\nTESTING Bayesian estimator on sample randomized data\n", file=self.out)
   import math,random
   from mmtbx.scaling.bayesian_estimator import bayesian_estimator
   from cctbx.array_family import flex

   # create some training data. First value=target, rest are predictor variables
   record_list=[]
   random.seed(iseed)
   for i in range(1000):
    y=float(i)/100.
    x1=y+random.gauss(1.,.5)
    x2=y+random.gauss(-2.,.8)
    record_list.append([y,x1,x2])

   run=bayesian_estimator(out=out)
   run.create_estimator(
         record_list=record_list,
         n_bin=100,
         min_in_bin=5,
         smooth_bins=5)

   # Now create some observed data.
   # (just predictor variables, save target y for comparison)
   obs_record_list=[]
   target_y_list=[]  # just for comparison
   for i in range(200):
    y=float(i)/20.
    x1=y+random.gauss(1.,.5)
    x2=y+random.gauss(-2.,.8)
    obs_record_list.append([x1,x2]) # note no y here!
    target_y_list.append(y)  # just for comparison

   # get the predicted values:
   predicted_data,predicted_sd_data=\
       run.apply_estimator(obs_record_list)


   if 0:
     f=open("out.dat","w")
     for observed_record,predicted_value,y in zip (
         obs_record_list,predicted_data,target_y_list):
        print(observed_record,predicted_value,y, file=f)
     print("Data, predicted y, actual y written to out.dat", file=out)
     f.close()

   cc=flex.linear_correlation(flex.double(target_y_list),flex.double(predicted_data)).coefficient()
   print("CC: ",cc, file=self.out)
   return cc
#

def get_table_as_list(lines=None,text="",file_name=None,record_list=None,
   info_list=None,info_items=None,
   data_items=None,target_variable=None,
   start_column_header=None,
   select_only_complete=False, minus_one_as_none=True,
   d_min_as_data=False,out=sys.stdout):

  if not record_list:
     record_list=[]  # data records: [target,predictor1,predictor2...]
     info_list=[]    # info records [key, resolution]
  if not data_items:
     data_items=[]
     info_items=[]
  skip_columns=[]
  if not record_list:
    n=None
    first=True
    n_info=0
    if file_name:
      with open(file_name) as f:
        text=f.read()
    if not lines:
      lines=text.splitlines()
    for line in lines:
      if line.startswith("#"): continue # skip comments
      if not line.replace(" ",""): continue # skip empty lines
      if line.lstrip().startswith("Total"): continue # clean up file format
      spl=line.split()
      if first and not spl[0].lstrip().startswith('0'):
        print("Input data file columns:%s" %(line), file=out)
        # figure out if some of the columns are "key" and "d_min"
        all_info_items=[]
        all_items=line.split()
        if start_column_header:
           new_all_items=[]
           started=False
           i=0
           for item in all_items:
             if not started and item==start_column_header:
               started=True
             if started:
                new_all_items.append(item)
             else:
                skip_columns.append(i)
             i+=1
           all_items=new_all_items
           print("Skipped columns: %s" %(str(skip_columns)), file=out)
           print("Columns to read: %s" %(str(all_items)), file=out)
           if not started:
             raise Sorry("Sorry the header %s "% (start_column_header) +
               "was not found in the line %s" %(line))


        if all_items[0].lower()=='key':
          all_info_items.append(all_items[0])
          all_items=all_items[1:]
        if not d_min_as_data and \
          all_items[0].lower() in  ['d_min','dmin','res','resolution']:
          all_info_items.append('d_min') # standardize
          all_items=all_items[1:]
        target=all_items[0]
        all_items=all_items[1:]
        n_info=len(all_info_items)
        if not data_items:
          data_items=all_items
          info_items=all_info_items
        if not target_variable:
          target_variable=target
        first=False
        continue
      if skip_columns:
        spl=spl[len(skip_columns):]
      xx=[]
      info=spl[:n_info]
      for i in range(1,n_info):
        info[i]=float(info[i])
      for x in spl[n_info:]:
        if x.lower() in ["-1","-1.000","None","none"]:
          xx.append(None)
        else:
          xx.append(float(x))
      if n is None: n=len(xx)
      assert len(xx)==n
      if xx[0] is not None:
        record_list.append(xx)
        info_list.append(info)

  if select_only_complete:
    new_list=[]
    new_info_list=[]
    for x,y in zip(record_list,info_list):
      if not None in x:
        new_list.append(x)
        new_info_list.append(y)
    record_list=new_list
    info_list=new_info_list
  return record_list,info_list,target_variable,data_items,info_items

class estimator_group:
  # holder for a group of estimators based on the same data but using different
  # subsets for prediction.  Useful in case the request for estimation is
  # missing some of the predictor variables.

  # Also useful for using only data to a certain resolution cutoff in the
  # prediction. This is turned on if resolution_cutoffs is supplied and a
  #   resolution is supplied for apply_estimator()
  # if a resolution is requested that is outside the range of resolution_cutoffs
  #  then a predictor based on the nearest resolution is used.

  def __init__(self,
        n_bin=100,
        min_in_bin=5,
        smooth_bins=5,
        resolution_cutoffs=None,
        skip_covariance=True,
        minimum_records=20,
        verbose=False,
        out=sys.stdout):

    self.estimator_dict={}
    self.keys=[]
    self.combinations={} # keyed on resolution
    self.variable_names=[]
    self.info_names=[]
    self.n_bin=n_bin
    self.min_in_bin=min_in_bin
    self.smooth_bins=smooth_bins
    self.out=out
    self.verbose=verbose
    if self.verbose:
      self.verbose_out=out
    else:
      self.verbose_out=null_out()
    self.training_records=[]
    self.training_file_name='None'
    self.skip_covariance=skip_covariance
    self.minimum_records=minimum_records

    if resolution_cutoffs:
       resolution_cutoffs.sort()
       resolution_cutoffs.reverse() # highest first
    else:
       resolution_cutoffs=[0]
    self.resolution_cutoffs=resolution_cutoffs

  def show_summary(self):
    # show summary of this estimator
    print("\nSummary of Bayesian estimator:", file=self.out)
    print("\nVariable names:", end=' ', file=self.out)
    for x in self.variable_names: print(x, end=' ', file=self.out)
    print(file=self.out)
    print("Resolution cutoffs:", end=' ', file=self.out)
    for resolution_cutoff in self.resolution_cutoffs:
       print("%5.2f" %(resolution_cutoff), end=' ', file=self.out)
    print(file=self.out)

    if self.combinations:
      print("\nCombination groups:", file=self.verbose_out)
      for resolution_cutoff in self.resolution_cutoffs:
        print("Resolution cutoff: %5.2f A" %(resolution_cutoff), end=' ', file=self.verbose_out)
        for x in self.combinations[resolution_cutoff]:
          print("(", end=' ', file=self.verbose_out)
          for id in x:
            print("%s" %(self.variable_names[id]), end=' ', file=self.verbose_out)
          print(")  ", end=' ', file=self.verbose_out)
        print(file=self.verbose_out)

    print("Bins: %d   Smoothing bins: %d  Minimum in bins: %d " %(
        self.n_bin,self.smooth_bins,self.min_in_bin), file=self.verbose_out)

    print("Data records used in training estimator: %d\n" %(
      len(self.training_records)), file=self.out)
    print("Data file source of training data: %s\n" %(
      self.training_file_name), file=self.out)

  def add_estimator(self,estimator,resolution_cutoff=None,combination=None):
    key=self.get_key(combination=combination,
        resolution_cutoff=resolution_cutoff)
    self.estimator_dict[key]=estimator
    self.keys.append(key)
    if not resolution_cutoff in self.combinations:
      self.combinations[resolution_cutoff]=[]
    self.combinations[resolution_cutoff].append(combination)

  def set_up_estimators(self,
      record_list=None,
      data_items=None,
      file_name=None,
      text=None,
      select_only_complete=False,
      minimum_complete=True):

    if not record_list:
      record_list,info_list,target_variable,data_items,info_items=\
       self.get_record_list(
        file_name=file_name,
        text=text,
        select_only_complete=select_only_complete)

    n=len(record_list[0])
    assert data_items is not None


    self.training_records=record_list

    assert data_items
    self.variable_names=data_items
    self.info_names=info_items
    print("Total of %d records in data file" %(len(record_list)), file=self.out)
    if len(record_list)<1: return
    n=len(record_list[0])


    # now split in all possible ways and get estimators for each combination
    # if self.resolution_cutoffs is set, do it with each resolution cutoff
    # 2014-12-18 if minimum_complete=True, require that there are
    #  self.minimum_records complete ones
    for resolution_cutoff in self.resolution_cutoffs:
      print("\nResolution cutoff of %5.2f A" %(resolution_cutoff), file=self.verbose_out)
      local_record_list,local_info_list=self.select_records(
        record_list=record_list,
        info_list=info_list,resolution_cutoff=resolution_cutoff,
        minimum_complete=minimum_complete)
      if len(local_record_list)<1:
        print("No records selected for this resolution cutoff", file=self.verbose_out)
        self.delete_resolution_cutoff(resolution_cutoff)
        continue

      print("%d records selected for this resolution cutoff" %(
       len(local_record_list)), file=self.verbose_out)
      n_predictors=n-1
      print("\nPredictor variables: %d" %(n_predictors), file=self.verbose_out)
      for pv in data_items:
        print("%s" %(pv), end=' ', file=self.verbose_out)
      print(file=self.verbose_out)
      if len(data_items)!=n_predictors:
        raise Sorry("Number of predictor variables (%d) must be the same as " %(
            len(data_items)) +
          "the number \nof predictor variables in data records(%d)" %(
            n_predictors))

      combinations=self.get_combinations(list(range(n_predictors)))
      for combination in combinations:
       estimator=self.get_estimator(
          record_list=local_record_list,columns=combination)
       if estimator is not None:
         key=self.get_key(combination=combination,
           resolution_cutoff=resolution_cutoff)
         self.add_estimator(estimator,resolution_cutoff=resolution_cutoff,
           combination=combination)
    for resolution_cutoff in self.resolution_cutoffs: # make sure there is an estimator
      if not resolution_cutoff in self.combinations:
        self.delete_resolution_cutoff(resolution_cutoff)

  def get_record_list(self,
     file_name=None,
     text=None,
     select_only_complete=None):
      record_list=[]
      data_items=[]
      if text:
        print("Setting up Bayesian estimator using supplied data", file=self.out)
        lines=text.splitlines()
      else:
        if not file_name:
          import libtbx.load_env
          file_name=libtbx.env.find_in_repositories(
            relative_path=os.path.join("mmtbx","scaling","cc_ano_data.dat"),
            test=os.path.isfile)
        if not file_name:
          raise Sorry("Need file with training data")
        print("\nSetting up Bayesian estimator using data in %s\n" %(file_name), file=self.out)
        with open(file_name) as f:
          lines=f.readlines()
        self.training_file_name=file_name
      return get_table_as_list(
         lines=lines,
         select_only_complete=select_only_complete,out=self.out)

  def delete_resolution_cutoff(self,resolution_cutoff):
    new_cutoffs=[]
    for rc in self.resolution_cutoffs:
      if not rc==resolution_cutoff:
        new_cutoffs.append(rc)
    self.resolution_cutoffs=new_cutoffs

  def select_records(self,
      record_list=None,
      info_list=None,
      next_cutoff=None,
      minimum_complete=None,
      resolution_cutoff=None,
      first=True):
    # select those records that are >= resolution_cutoff and not >= the
    # next-highest cutoff in the list (if any)
    if not self.info_names or not resolution_cutoff or \
         not self.resolution_cutoffs or len(self.resolution_cutoffs)==1:
      return record_list,info_list

    if self.info_names[-1]!='d_min' and len(self.resolution_cutoffs)>1:
      raise Sorry("Cannot select records on resolution unless the database"+
        "(%s) has resolution information" %(str(self.training_file_name)))
    if first and next_cutoff is None:
      for rc in self.resolution_cutoffs:
        if (next_cutoff is None or rc <next_cutoff) and rc > resolution_cutoff:
          next_cutoff=rc

    new_records=[]
    new_info=[]
    count=0
    for record,info in zip(record_list,info_list):
      if info[-1]>=resolution_cutoff and \
        (next_cutoff is None or info[-1]<next_cutoff):
         new_records.append(record)
         new_info.append(info)
         if minimum_complete:
            if not (None in record):
              count+=1
    if next_cutoff is not None and next_cutoff<info_list[-1][-1] and (
       len(new_records)<self.minimum_records or
       minimum_complete and count < self.minimum_records):
      # try again with bigger range
      next_next_cutoff=None
      for rc in self.resolution_cutoffs:
        if (next_next_cutoff is None or rc <next_next_cutoff) \
          and rc > next_cutoff:
          next_next_cutoff=rc
      new_records,new_info=self.select_records(
          record_list=record_list,info_list=info_list,
          next_cutoff=next_next_cutoff,resolution_cutoff=resolution_cutoff,
          minimum_complete=minimum_complete,first=False)
    return new_records,new_info

  def get_key(self,combination=None,resolution_cutoff=None):
    if resolution_cutoff is None: resolution_cutoff=0
    return str(combination)+"_"+str(resolution_cutoff)

  def variable_names(self):
    return self.variable_names

  def get_estimator(self,record_list=[],
      columns=[]):

    from mmtbx.scaling.bayesian_estimator import bayesian_estimator
    from cctbx.array_family import flex

    from copy import deepcopy
    record_list=deepcopy(record_list)

    column_chooser=[]
    n=max(columns)
    for x in range(n+1):
      if x in columns:
        column_chooser.append(True)
      else:
        column_chooser.append(False)

    selected_list=[]
    for xx in record_list:
      new_x=[]
      have_all=True
      for x,keep in zip(xx,[True]+column_chooser):
         if keep:
           new_x.append(x)
           if x is None:
             have_all=False
      if have_all:
        selected_list.append(new_x)

    if not selected_list:
      return None

    estimator=bayesian_estimator(out=null_out(),
         skip_covariance=self.skip_covariance)
    estimator.create_estimator(
          record_list=selected_list,
         n_bin=self.n_bin,
         min_in_bin=self.min_in_bin,
         smooth_bins=self.smooth_bins)
    if not estimator.ok: return None

    return estimator

  def get_combinations(self,all_together):
    if len(all_together)<1: return []
    elif len(all_together)==1: return [all_together]
    else:  # return all sub_combinations
      combinations=[all_together]
      for n in range(len(all_together)):
        for x in self.get_combinations(all_together[:n]+all_together[n+1:]):
          if not x in combinations:
            combinations.append(x)
      return combinations

  def apply_estimators(self,value_list=None,data_items=None,
    resolution=None):
    assert len(value_list)==len(self.variable_names)
    if data_items != self.variable_names:
      print("WARNING: data items do not match working variables:", file=self.out)
      print("Item: variable name", file=self.out)
      for data_item,variable in zip(data_items,self.variable_names):
         print("%s: %s " %(data_item,variable), file=self.out)
      raise Sorry("Data items do not match working variables. Use verbose to see more info.")
    if len(self.resolution_cutoffs)>1 and resolution is None:
      raise Sorry("Must supply resolution if resolution_cutoffs is set")

    # choose which resolution cutoff to use
    rc=self.resolution_cutoffs[-1] # take highest-res  no matching resolution
    for resolution_cutoff in self.resolution_cutoffs:
      if resolution and resolution >= resolution_cutoff:
        rc=resolution_cutoff
        break

    # figure out which data are present and select the appropriate estimator
    columns=[]
    values=[]
    for x,n in zip(value_list,range(len(value_list))):
      if x is not None:
        columns.append(n)
        values.append(x)
    key=self.get_key(combination=columns,resolution_cutoff=rc)
    if not key in self.keys:
      return None,None
    estimator=self.estimator_dict[key]

    return estimator.apply_estimator(values,single_value=True)

prediction_values="""cc_perfect cc      skew    e
0.361   None    0.0124  1.04
0.476   0.382   0.0455  0.536
0.672   0.752   0.1546  0.14
0.47    0.321   0.0743  0.636
0.317   0.145   0.0299  0.795
0.046   0.026   -0.0088 0.978
0.055   None    -0.0121 1.007
0.391   None    0.0252  0.749
0.407   None    0.025   0.711
0.635   0.381   0.1601  0.495
0.684   0.481   0.1846  0.453
0.684   0.481   0.1846  0.453
0.057   0.008   -0.0002 0.938
0.349   0.189   -0.0004 0.781
0.482   0.375   0.0405  0.607
0.031   -0.046  0.0002  1.027
0.195   0.035   0.0073  0.914
0.268   0.12    0.0052  0.856
0.417   None    0.0427  0.657
0.524   None    0.0279  0.576
0.527   None    0.0256  0.751
0.284   0.152   0.0144  0.923
0.425   0.376   0.0803  0.655
0.13    -0.051  -0.0042 0.984
0.152   -0.029  0.0056  0.991
0.217   0.045   0.0117  0.969
0.177   0.029   -0.0084 0.979
0.266   0.088   0.0075  0.905
0.492   0.191   0.0105  0.664
0.309   0.021   -0.0021 0.874
0.62    0.354   0.1432  0.659
0.784   0.608   0.192   0.515
0.67    0.499   0.147   0.588
0.082   -0.059  0.0053  0.952
0.11    0.023   0.0163  0.96
0.174   0.061   0.0028  0.914
0.108   0.023   0.0072  0.989
0.572   None    0.0384  0.351
0.404   0.16    0.0306  0.818
0.625   0.464   0.1383  0.616
0.306   0.099   0.0056  0.791
0.21    0.147   0.0348  0.745
0.213   0.123   0.0195  0.76
0.391   0.303   0.0248  0.812
0.214   0.062   -0.0023 0.961
0.053   0.014   0.0154  1.016
0.066   None    -0.0295 0.919
0.34    None    0.0443  0.943
0.404   None    0.0448  0.856
0.371   0.275   0.0263  0.688
0.06    -0.08   0.005   0.997
0.426   0.22    0.047   0.589
0.178   0.034   0.0031  0.973
0.22    0.071   0.012   0.949
0.045   -0.02   0.0011  0.844
0.318   0.088   0.0155  0.965
0.026   -0.033  -0.0089 0.922
0.25    0.044   -0.0053 0.822
0.161   0.014   0.0194  0.998
0.352   0.168   0.0434  0.801
0.296   0.115   0.0156  0.817
0.231   0.1     0.0079  0.803
0.192   0.03    -0.0028 0.94
0.041   -0.022  -0.0091 1.005
0.236   0.057   0.0161  0.987
0.521   0.315   0.017   0.539
0.633   0.549   0.0266  0.314
0.175   0.089   0.0219  0.843
0.251   0.139   0.0024  0.972
0.175   0.068   0.0132  1.009
0.141   0.038   0.0224  0.93
0.342   0.141   0.0378  0.799
0.288   0.074   -0.0065 0.94
0.049   -0.074  -0.0049 1.013
0.326   0.166   0.0252  0.862
0.28    None    0.0173  0.952
0.045   None    -0.019  0.896
0.26    0.088   0.0022  0.835
0.103   -0.007  0.031   0.926
0.139   0.042   0.0038  0.87
0.302   0.226   0.0464  0.803
0.236   0.067   0.0255  0.987
0.255   0.11    0.0031  0.925
0.071   -0.026  -0.0097 0.996
0.508   0.352   0.0556  0.676
0.472   0.262   0.0668  0.695
0.427   0.172   0.0241  0.744
0.206   0.039   0.0038  0.913
0.194   0.039   -0.0038 0.92
0.006   -0.009  -0.0089 0.981
0.049   0.001   -0.005  0.993
0.257   0.139   0.0064  0.853
0.368   0.336   0.0599  0.727
0.29    0.186   0.0179  0.834
0.175   0.033   0.0212  0.867
0.326   0.107   0.0176  0.844
0.41    0.192   0.0192  0.762
0.341   0.111   0.0106  0.822
0.486   0.229   0.0675  0.783
0.497   0.262   0.08    0.802
0.385   0.134   0.0202  0.768
0.605   0.498   0.064   0.468
0.41    0.202   0.0293  0.757
0.506   0.273   0.0651  0.63
0.276   0.078   0.0391  0.97
0.286   0.098   0.0099  0.888
0.372   0.127   0.0349  0.795
0.311   0.087   0.0406  0.859
0.353   0.189   0.0057  0.709
0.35    0.184   0.0121  0.714
0.278   0.119   0.0122  0.829
0.459   0.151   0.0412  0.693
0.508   0.199   0.0454  0.658
0.593   0.294   0.0599  0.568
0.473   0.185   0.0471  0.789
0.604   0.352   0.0732  0.588
0.658   0.419   0.093   0.507
0.274   0.057   0.0103  0.849
0.282   0.071   0.0125  0.812
0.449   0.319   0.0284  0.647
0.528   0.455   0.0389  0.51
0.434   0.28    0.0264  0.648
0.351   0.19    0.0151  0.76
0.375   0.22    0.0135  0.726
0.298   0.161   0.0146  0.824
0.394   0.278   0.0231  0.702
0.29    0.147   0.0083  0.853
0.324   0.104   0.0141  0.952
0.371   0.15    0.032   1
0.463   0.201   0.028   0.791
0.461   0.292   0.0129  0.644
0.202   0.03    -0.0009 0.947
0.259   0.066   -0.0132 0.903
0.225   0.037   0.0121  0.924
0.323   0.197   0.0218  0.72
0.344   0.287   0.0503  0.619
0.302   0.204   0.0149  0.751
0.266   0.059   0.0206  0.892
0.252   0.059   0.0153  0.907
0.319   0.12    0.0252  0.833
0.273   0.064   0.0197  0.874
0.13    -0.008  0.0236  0.972
0.378   0.129   0.0451  0.804
0.478   0.301   0.0579  0.635
0.285   0.093   0.0091  0.857
0.317   0.133   0.032   0.8
0.263   0.177   0.0086  0.818
0.114   0.037   0.0031  0.931
0.31    0.081   0.0121  0.843
0.432   0.19    0.0482  0.716
0.303   0.075   0.0212  0.861
0.337   0.089   0.0264  0.847
0.322   0.086   0.0074  0.839
0.381   0.136   0.0425  0.773
0.474   0.211   0.0415  0.675
0.317   0.109   0.017   0.807
0.286   0.083   0.0024  0.833
0.39    0.175   0.0534  0.741
0.473   0.251   0.0671  0.587
0.185   0.058   -0.0086 0.951
0.272   0.138   -0.0133 0.843
0.282   0.145   0.0079  0.882
0.269   0.111   0.0149  0.862
0.406   0.171   0.0179  0.709
0.53    0.345   0.0473  0.531
0.217   0.147   0.0109  0.92
0.315   0.272   0.0307  0.819
0.295   0.189   0.0065  0.852
0.331   0.249   0.0061  0.796
0.196   0.017   0.0052  0.968
0.198   0.018   0.0122  0.982
0.502   None    0.0185  0.49
0.286   None    0.0138  0.993
0.247   None    -0.0123 1.033
0.341   0.158   0.0175  0.88
0.253   0.134   0.0064  0.903
0.368   0.209   0.0115  0.78
0.479   0.282   0.0399  0.658
0.618   0.474   0.1005  0.437
0.642   0.506   0.0807  0.427
0.34    0.301   0.0188  0.82
0.331   0.263   0.0254  0.846
0.374   0.354   0.0414  0.767
0.08    None    0.0179  0.801
0.071   None    0.0774  0.579
0.301   None    0.0168  0.721
0.317   None    0.02    0.845
0.045   None    -0.0017 0.972
0.272   None    0.0002  0.587
0.346   None    0.0127  0.307
0.24    None    0.0224  0.619
0.223   None    0.0286  0.611
0.183   None    -0.005  0.568
0.423   None    0.0686  0.441
0.487   None    0.0429  0.452
0.375   None    0.052   0.817
0.424   None    0.0113  0.5
0.368   None    0.0121  0.637
0.319   None    0.0026  0.594
0.317   None    0.0276  0.486
0.39    None    0.0287  0.568
0.526   None    0.0595  0.401
0.274   None    0.0148  0.648
0.481   None    0.014   0.337
0.261   None    0.0028  0.835
0.324   None    0.0513  0.659
0.347   None    0.0107  0.709
0.304   None    0.0254  0.603
0.385   None    0.0326  0.295
0.037   None    0.0397  0.991
0.305   None    0.0259  0.755
0.448   None    0.0457  0.699
0.247   None    -0.0111 0.739
0.105   None    0.0012  0.661
0.491   None    0.0492  0.486
0.338   None    -0.0031 0.648
0.385   None    0.0491  0.809
0.218   None    -0.002  0.679
0.544   None    0.0488  0.477
0.345   None    0.0295  0.643"""


pred_data="""cc_perfect    cc skew      e
0.023   0.0163  0.96
None    0.0028  0.914
0.023   0.0072  None
None    None 0.351
0.16    None  None
0.464   0.1383  0.616
0.099   0.0056  0.791
0.147   None 0.745
0.123   0.0195  0.76
None  None  None
0.303   0.0248  0.812"""

target_values=[0.476,0.672,0.47,0.317,0.046,0.635,0.684,0.684,0.057]

def exercise_group(out=sys.stdout):

  print("\nTESTING exercise_group(): group of predictors with same ", file=out)
  print("data but some missing entries.\n", file=out)

  import libtbx.load_env
  file_name=libtbx.env.find_in_repositories(
      relative_path=os.path.join("mmtbx","scaling","cc_ano_data.dat"),
      test=os.path.isfile)

  estimators=estimator_group(resolution_cutoffs=[0.,1.,2.,3.,4.,5.],
     out=out)
  estimators.set_up_estimators(file_name=file_name)
  estimators.show_summary()

  print("Running estimator now", file=out)
  prediction_values_as_list,info_values_as_list,\
     dummy_target_variable,dummy_data_items,dummy_info_items=\
    get_table_as_list(file_name=file_name, select_only_complete=False,out=out)

  # run through all prediction_values data
  from cctbx.array_family import flex
  target_list=flex.double()
  result_list=flex.double()
  for target_and_value,info in zip(
      prediction_values_as_list,info_values_as_list):
    resolution=info[-1]
    y,sd=estimators.apply_estimators(value_list=target_and_value[1:],
      data_items=estimators.variable_names,resolution=resolution)
    if y is None:
      raise Sorry("Estimator failed")
    else:
      target_list.append(target_and_value[0])
      result_list.append(y)
  cc1=flex.linear_correlation(target_list,result_list).coefficient()
  print("Prediction CC: %7.3f " %(cc1), file=out)

  # and using another dataset included here
  print(file=out)
  prediction_values_as_list,info_values_as_list,target_variable,\
     data_items,info_items=get_table_as_list(
     text=prediction_values,select_only_complete=False,out=out)

  estimators=estimator_group(out=out)
  estimators.set_up_estimators(
    text=prediction_values,select_only_complete=True,
    data_items=data_items)
  estimators.show_summary()

  all_value_list,all_info_list,target_variable,data_items,info_items=\
      get_table_as_list(text=pred_data,out=out)

  target_list=flex.double()
  result_list=flex.double()

  for value_list,target in zip(all_value_list,target_values):
    y,sd=estimators.apply_estimators(
     value_list=value_list,data_items=data_items)
    if y is not None:
      print("Y SD Target diff : %7.3f  %7.3f   %7.3f  %7.3f" %(
        y,sd,target,y-target), file=out)
      target_list.append(target)
      result_list.append(y)
    else:
      print("No data:  %7.3f " %(target), file=out)
  cc2=flex.linear_correlation(target_list,result_list).coefficient()
  print("Correlation: %6.3f" %(cc2), file=out)
  return cc1,cc2


# cross-validation of estimates of cc* from skew, cc_half, esqr
# 2014-10-28 tt
#

def get_suitable_target_and_values(target_and_values_list,test_values):
  suitable_test_values=[]
  for test_value in test_values:
    if test_value is not None:
      suitable_test_values.append(test_value)

  suitable_target_and_values_list=[]
  for target_and_values in target_and_values_list:
    new_target_and_values=[target_and_values[0]]
    for test_value,value in zip(test_values,target_and_values[1:]):
      if test_value is not None:
         new_target_and_values.append(value)
    if not None in new_target_and_values:
      suitable_target_and_values_list.append(new_target_and_values)
  return suitable_test_values,suitable_target_and_values_list

def jacknife(target_and_values_list,i,
    skip_covariance=True,
    skip_jacknife=False,out=sys.stdout):
  # take out entry i and get predictor and then predict value for entry i

  # if test entry does not have values for all the predictor variables then
  #  use only the ones that it has
  values=target_and_values_list[i][1:] # everything except target
  if skip_jacknife: # include everything
    tv_list=deepcopy(target_and_values_list)
  else:  # usual
    tv_list=deepcopy(target_and_values_list[:i])+\
            deepcopy(target_and_values_list[i+1:])
  # select only work entries that have all the necessary values
  suitable_test_values,suitable_target_and_values_list=\
     get_suitable_target_and_values(tv_list,values)
  if not suitable_test_values: return None,None

  target=target_and_values_list[i][0]
  estimator=bayesian_estimator(
    skip_covariance=skip_covariance,
    minimum_records=1,
    out=null_out())
  estimator.create_estimator(
        record_list=suitable_target_and_values_list,
        n_bin=100,
        min_in_bin=5,
        smooth_bins=5,
  )
  est,sd=estimator.apply_estimator(suitable_test_values,single_value=True)
  return target,est

def run_jacknife(args=None,no_jump=True,
     file_name=None,record_list=None,info_list=None,out=sys.stdout):

  if args is None: args=[]
  verbose=('verbose' in args)
  if 'testing' in args:
    print("\nTESTING jacknife run of bayesian estimator.\n", file=out)
  skip_covariance=(not 'include_covariance' in args)

  if file_name is not None or record_list is not None:
    # already have it
    pass
  elif args and os.path.isfile(args[0]):
    file_name=args[0]
    print("Setting up jacknife Bayesian predictor with data from %s" %(
      file_name), file=out)
  else:
    import libtbx.load_env
    file_name=libtbx.env.find_in_repositories(
      relative_path=os.path.join("mmtbx","scaling","cc_ano_data.dat"),
      test=os.path.isfile)
    print("Setting up jacknife Bayesian predictor with data from %s" %(
      file_name), file=out)
  if not record_list and (not file_name or not os.path.isfile(file_name)):
    raise Sorry("Unable to find the file '%s' " %(str(file_name)))

  if record_list is None:
    record_list,info_list,target_variable,data_items,info_items=\
       get_table_as_list(
         file_name=file_name,
         select_only_complete=False,
         out=out)
    print("Size of data list: %d" %(len(record_list)), file=out)
  if info_list is None:
    info_list=len(record_list)*["None"]

  # Set up estimator using all but one entry and predict it from the others

  if no_jump:
    n_jump=1
  elif no_jump is None and 'no_jump' in args:
    n_jump=1
  else:
    n_jump=20
    print("Testing every %d'th entry in jacknife" %(n_jump), file=out)
  from cctbx.array_family import flex
  target_list=flex.double()
  result_list=flex.double()
  for i in range(len(record_list)):
    if n_jump*(i//n_jump)!=i: continue
    target,value=jacknife(record_list,i,skip_covariance=skip_covariance,
        out=out)
    if target is None or value is None:
       raise Sorry("estimator failed to return a result")
    target_list.append(target)
    result_list.append(value)
    info=info_list[i]
    if verbose:
      for x in info:
        print(x, end=' ', file=out)
      print("%7.3f  %7.3f " %(target,value), file=out)
  cc=flex.linear_correlation(
    target_list,result_list).coefficient()
  print("CC: %7.3f " %(cc), file=out)
  return cc,target_list,result_list

if __name__=="__main__":
  args=sys.argv[1:]
  if not 'testing' in args: args.append('testing')
  if 'run_jacknife' in args:
    run_jacknife(args=args,no_jump=None)
  elif 'exercise_group' in args:
    exercise_group()
  else:
    run_jacknife(args=args,no_jump=None)
    exercise_group()
    run=bayesian_estimator()
    run.exercise()
    run.exercise_2()


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/data_statistics.py

"""
Collect multiple analyses of experimental data quality, including
signal-to-noise ratio, completeness, ice rings and other suspicious
outliers, anomalous measurability, and Wilson plot.
"""

from __future__ import absolute_import, division, print_function
from mmtbx.scaling import absolute_scaling
from mmtbx import scaling
from iotbx import data_plots
from cctbx import miller
from cctbx import adptbx
from cctbx.array_family import flex
from scitbx.math import chebyshev_polynome
from scitbx.math import chebyshev_lsq_fit
from scitbx.math import erf
from libtbx.test_utils import approx_equal
from libtbx.utils import Sorry
from libtbx import table_utils
import math
from six.moves import zip
from six.moves import range


class i_sigi_completeness_stats(scaling.xtriage_analysis):
  """
  Collects resolution-dependent statistics on I/sigma expressed as percentage
  of reflections above specified cutoffs.
  """
  def __init__(self,
               miller_array,
               n_bins=15,
               isigi_cut=3.0,
               completeness_cut=0.85,
               resolution_at_least=3.5,
               completeness_as_non_anomalous=None):
    miller_array = miller_array.deep_copy().set_observation_type(miller_array)
    self.amplitudes_flag = False
    #check to see if we have intensities
    if miller_array.is_real_array():
      if miller_array.is_xray_amplitude_array():
        self.amplitudes_flag = True
        miller_array = miller_array.f_as_f_sq()
    assert miller_array.is_xray_intensity_array()
    #make sure we have sigmas
    assert miller_array.sigmas() is not None
    # select things with sigma larger then zero
    #miller_array = miller_array.select( miller_array.sigmas() > 0 )
    miller_array.setup_binner(n_bins=n_bins)
    self.resolution_bins = list(miller_array.binner().limits())
    self.overall = miller_array.completeness(use_binning=False,
       as_non_anomalous_array = completeness_as_non_anomalous)
    self.overall_binned = miller_array.completeness(use_binning=True,
       as_non_anomalous_array = completeness_as_non_anomalous).data[1:-1]
    self.completeness_bins = []
    def cut_completeness(cut_value):
      tmp_miller = miller_array.select(
        miller_array.data() > cut_value*miller_array.sigmas() )
      tmp_miller.use_binning_of(miller_array)
      completeness = tmp_miller.completeness(use_binning=True,
               return_fail=0.0,
               as_non_anomalous_array = completeness_as_non_anomalous)
      return completeness.data
    # create table
    table_data = []
    cut_list=[1,2,3,5,10,15]
    for cut_value in cut_list:
      self.completeness_bins.append( cut_completeness(cut_value) )
    legend = ["Res. Range", "I/sigI>1 ", "I/sigI>2 ", "I/sigI>3 ",
              "I/sigI>5 ", "I/sigI>10", "I/sigI>15" ]
    self.table = data_plots.table_data(
      title="Completeness and data strength",
      column_labels=["Low res.", "High res."] + legend[1:],
      column_formats=["%4.2f", "%4.2f" ] + [ "%5.1f" ] * 6,
      graph_names=["I/sigI by shell"],
      graph_labels=[("High resolution of shell", "% of total")],
      graph_columns=[list(range(1,8))],
      x_is_inverse_d_min=True,
      first_two_columns_are_resolution=True)
    for ii in range(1,len(self.resolution_bins)-1):
      a = self.resolution_bins[ii-1]
      b = self.resolution_bins[ii]
      row = [ a, b ]
      for jj in  self.completeness_bins:
        row.append(100.0*jj[ii])
      self.table.add_row(row)
    self.isigi_cut=isigi_cut
    self.completeness_cut=completeness_cut
    self.resolution_at_least=resolution_at_least
    self.resolution_cut = 4.0
    comp_data = cut_completeness(isigi_cut)
    reso=4.0
    for ii in range(1,len(self.resolution_bins)-1):
      a = self.resolution_bins[ii-1]
      b = self.resolution_bins[ii]
      if b < resolution_at_least:
        if comp_data[ii] > completeness_cut:
          self.resolution_cut = b**-0.5

  def _show_impl(self, out):
    out.show_sub_header("Completeness at I/sigma cutoffs")
    out.show_text("""
 The following table lists the completeness in various resolution ranges,
 after applying a I/sigI cut. Miller indices for which individual I/sigI
 values are larger than the value specified in the top row of the table, are
 retained, while other intensities are discarded. The resulting completeness
 profiles are an indication of the strength of the data.
""")
    if (self.amplitudes_flag):
      out.warn("""\
Please be aware that the input data were given as amplitudes and squared for
the purposes of this analysis, therefore the numbers displayed here are less
reliable than the values calculated from the raw intensities.
""")
    out.show_table(self.table, indent=2)
    out.show_plot(self.table)
    message = """\
  The completeness of data for which I/sig(I)>%.2f, exceeds %.0f %%
  for resolution ranges lower than %.2fA.
""" % (self.isigi_cut, self.completeness_cut*100, self.resolution_cut)

    if self.resolution_cut < self.resolution_at_least:
      message += """\
  The data are cut at this resolution for the potential twin tests and
  intensity statistics.
"""
    else:
      message += """\
  As we do not want to throw away too much data, the resolution for
  analyzing the intensity statistics will be limited to %3.2fA.
""" % self.resolution_at_least
    out.show_text(message)


# XXX where is this used?
class completeness_enforcement(object):
  def __init__(self,
               miller_array,
               minimum_completeness=0.75,
               completeness_as_non_anomalous=None):

    self.miller_array = miller_array.deep_copy()
    self.miller_array.setup_binner_d_star_sq_step(auto_binning=True)
    completeness = self.miller_array.completeness(use_binning=True,
      return_fail=1.0,
      as_non_anomalous_array = completeness_as_non_anomalous)
    selection_array = flex.bool( self.miller_array.indices().size(), False )
    #In this pass, we make sure that we have reasonable completeness
    for bin in completeness.binner.range_all():
      selection = completeness.binner.selection(bin).iselection()
      if completeness.data[bin] >= minimum_completeness:
        # the completeness is okai
        # use these indices please
        selection_array = selection_array.set_selected( selection, True )
    # now select the indices please
    self.new_miller = miller_array.select( selection_array )

class analyze_resolution_limits(scaling.xtriage_analysis):
  """
  Check for elliptical truncation, which may be applied to the data by some
  processing software (or as a post-processing step).  As a general rule this
  is not recommended since phenix.refine and related programs will handle
  anisotropy automatically, and users tend to apply it blindly (and even
  deposit the modified data).
  """
  def __init__(self, miller_set, d_min_max_delta=0.25):
    # XXX very important - for high-symmetry space groups we need to examine
    # all possible positive indices
    tmp_miller = miller_set.expand_to_p1()
    if (miller_set.unit_cell().parameters()[3:] != (90,90,90)):
    #if (miller_set.space_group().n_smx() > 2):
      all_pos_neg_indices = flex.miller_index()
      for h,k,l in tmp_miller.indices():
        if (h >= 0 and k >= 0 and l >= 0) or (h <= 0 and k <= 0 and l <= 0):
          all_pos_neg_indices.append((h,k,l))
      if isinstance(tmp_miller, miller.array):
        tmp_miller = tmp_miller.set(indices=all_pos_neg_indices)
      else :
        tmp_miller = tmp_miller.customized_copy(indices=all_pos_neg_indices)
    else :
      tmp_miller = tmp_miller.expand_to_p1()
    self.d_min_overall = tmp_miller.d_min()
    # FIXME this still fails for some space groups - need to test on entire
    # PDB
    assert approx_equal(self.d_min_overall, miller_set.d_min(), eps=0.1)
    self.d_min_a, self.d_min_b, self.d_min_c = \
      tmp_miller.d_min_along_a_b_c_star()
    self.d_min_max_delta = d_min_max_delta

  def max_d_min_delta(self):
    """
    Return the maximum difference in d_min along any two axes.
    """
    limits = [self.d_min_a, self.d_min_b, self.d_min_c]
    max_delta = 0
    for i in range(2):
      for j in range(i,3):
        max_delta = max(max_delta, abs(limits[i] - limits[j]))
    return max_delta

  def is_elliptically_truncated(self, d_min_max_delta=None):
    if (d_min_max_delta is None):
      d_min_max_delta = self.d_min_max_delta
    return self.max_d_min_delta() > d_min_max_delta

  def _show_impl(self, out):
    out.show_sub_header("Analysis of resolution limits")
    out.show_text("""\
Your data have been examined to determine the resolution limits of the data
along the reciprocal space axes (a*, b*, and c*).  These are expected to vary
slightly depending on unit cell parameters and overall resolution, but should
never be significantly different for complete data.  (This is distinct from the
amount of anisotropy present in the data, which changes the effective
resolution but does not actually exclude reflections.)""")
    max_delta = self.max_d_min_delta()
    out.show_preformatted_text("""
    overall d_min                = %.3f
    d_min along a*               = %.3f
    d_min along b*               = %.3f
    d_min along c*               = %.3f
    max. difference between axes = %.3f
""" % (self.d_min_overall, self.d_min_a,
       self.d_min_b, self.d_min_c, max_delta))
    if (max_delta > self.d_min_max_delta):
      out.show_text("""\
The resolution limit appears to be direction-dependent, which may indicate
issues with the data collection geometry, processing errors, or that elliptical
truncation has been applied.  We do not recommend using elliptically truncated
data, as anisotropy is handled automatically by Phaser, phenix.refine, and
related programs, and discarding large numbers of weak reflections may result
in increased map bias and/or artifacts.  You should always deposit the original,
uncorrected reflections in the PDB, not the truncated data.""")
    else :
      out.show_text("""Resolution limits are within expected tolerances.""")


class log_binned_completeness(scaling.xtriage_analysis):
  """
  Table of completeness using log-scale resolution binning.
  """
  def __init__(self, miller_array,
      n_reflections_in_lowest_resolution_bin=100,
      max_number_of_bins=30,
      min_reflections_in_bin=50,
      completeness_as_non_anomalous=None):
    rows = []
    bins = miller_array.log_binning(
      n_reflections_in_lowest_resolution_bin=\
        n_reflections_in_lowest_resolution_bin,
      max_number_of_bins=max_number_of_bins,
      min_reflections_in_bin=min_reflections_in_bin)
    for selection in bins :
      bin_array = miller_array.select(selection)
      d_max, d_min = bin_array.d_max_min()
      n_refl = bin_array.size()
      n_refl_expect = bin_array.complete_set(d_max=d_max).size()
      completeness = bin_array.completeness(d_max=d_max,
          as_non_anomalous_array = completeness_as_non_anomalous)
      rows.append(("%.4f - %.4f" % (d_max, d_min), "%d/%d" % (n_refl,
        n_refl_expect), "%.1f%%" % (completeness*100)))
    self.table = table_utils.simple_table(
      column_headers=["Resolution", "Reflections", "Completeness"],
      table_rows=rows)

  def _show_impl(self, out):
    out.show_sub_header("Completeness (log-binning)")
    out.show_text("""\
The table below presents an alternative overview of data completeness, using
the entire resolution range but on a logarithmic scale.  This is more sensitive
to missing low-resolution data (and is complementary to the separate table
showing low-resolution completeness only).""")
    out.show_table(self.table)

#-----------------------------------------------------------------------
# OUTLIER FILTERING
class possible_outliers(scaling.xtriage_analysis):
  """
  Flag specific reflections with suspicious intensities.  Inspired by:
  Read, Acta Cryst. (1999). D55, 1759-1764
  """
  def __init__(self,
               miller_array,
               prob_cut_ex=1.0E-1,
               prob_cut_wil=1.0E-6):
    if miller_array.observation_type() is None:
      raise Sorry("Unknown observation type")
    if not miller_array.is_xray_intensity_array():
      miller_array = miller_array.f_as_f_sq()
    else:
      miller_array = miller_array.deep_copy()
    assert miller_array.is_xray_intensity_array()
    work_array = miller_array.deep_copy()
    normalizer = absolute_scaling.kernel_normalisation(
      work_array, auto_kernel=True)
    work_array = work_array.array(
      data=normalizer.normalised_miller.data()
      /work_array.epsilons().data().as_double())
    self.n_refl = work_array.size()
    # array processing
    centric_cut = work_array.select_centric().set_observation_type(work_array)
    acentric_cut = work_array.select_acentric().set_observation_type(
      work_array)
    assert centric_cut.data().all_ge(0)
    p_acentric_single = 1.0 - (1.0 - flex.exp(-acentric_cut.data() ))
    p_centric_single = 1.0 - erf(flex.sqrt(centric_cut.data()/2.0) )
    n_centric = p_centric_single.size()
    n_acentric = p_acentric_single.size()
    extreme_acentric = 1.0 -  \
       flex.pow(1.0 - flex.exp(-acentric_cut.data() ),float(n_acentric))
    extreme_centric = 1.0 - \
       flex.pow(erf(flex.sqrt(centric_cut.data()/2.0) ),float(n_centric))

    ## combine both the wilson and extreme value cut-off values
    acentric_outlier = (extreme_acentric < prob_cut_ex) or (
     p_acentric_single < prob_cut_wil)
    centric_outlier = (extreme_centric < prob_cut_ex) or (
     p_centric_single  < prob_cut_wil)

    ## acentrics
    self.acentric_outlier_miller = acentric_cut.indices().select(
      acentric_outlier)
    self.acentric_outlier_e_vals = acentric_cut.data().select(
      acentric_outlier)
    self.acentric_outlier_e_vals = flex.sqrt(self.acentric_outlier_e_vals)
    self.acentric_outlier_d_spacings = acentric_cut.d_spacings().data()\
                                       .select(acentric_outlier)
    self.acentric_outlier_p_val = p_acentric_single.select(acentric_outlier)
    self.acentric_outlier_extreme_val = extreme_acentric.select(
      acentric_outlier)
    self.acentric_outliers_table = data_plots.table_data(
      title="Acentric reflections",
      column_labels=["d_spacing", "H K L", "|E|", "p(wilson)", "p(extreme)"],
      column_formats=["%8.3f","%5i,%5i,%5i", "%6.2f", "%9.2e", "%10.2e"],
      graph_names=["Possible acentric outliers"],
      graph_columns=[[0,1,2,3,4]])
    for d,hkl,e,p,extr in zip(self.acentric_outlier_d_spacings,
                              self.acentric_outlier_miller,
                              self.acentric_outlier_e_vals,
                              self.acentric_outlier_p_val,
                              self.acentric_outlier_extreme_val):
      self.acentric_outliers_table.add_row([d, hkl, e, p, extr])

    ## centrics
    self.centric_outlier_miller = centric_cut.indices().select(
      centric_outlier)
    self.centric_outlier_e_vals = centric_cut.data().select(
      centric_outlier)
    self.centric_outlier_e_vals = flex.sqrt(self.centric_outlier_e_vals)
    self.centric_outlier_d_spacings = centric_cut.d_spacings().data()\
                                       .select(centric_outlier)
    self.centric_outlier_p_val = p_centric_single.select(centric_outlier)
    self.centric_outlier_extreme_val = extreme_centric.select(centric_outlier)
    self.centric_outliers_table = data_plots.table_data(
      title="Centric reflections",
      column_labels=["d_spacing", "H K L", "|E|", "p(wilson)", "p(extreme)"],
      column_formats=["%8.3f","%5i,%5i,%5i", "%6.2f", "%9.2e", "%10.2e"],
      graph_names=["Possible centric outliers"],
      graph_columns=[[0,1,2,3,4]])
    for d,hkl,e,p,extr in zip(self.centric_outlier_d_spacings,
                              self.centric_outlier_miller,
                              self.centric_outlier_e_vals,
                              self.centric_outlier_p_val,
                              self.centric_outlier_extreme_val):
      self.centric_outliers_table.add_row([d, hkl, e, p, extr])

  def fraction_outliers(self):
    return self.n_outliers() / self.n_refl

  def n_outliers(self):
    n_acentric = self.acentric_outlier_miller.size()
    n_centric = self.centric_outlier_miller.size()
    return n_acentric + n_centric

  def remove_outliers(self, miller_array):
    ## remove the outliers please
    centric_matches = miller.match_indices( miller_array.indices(),
                                     self.centric_outlier_miller )
    miller_array = miller_array.select( centric_matches.single_selection(0) )
    acentric_matches = miller.match_indices( miller_array.indices(),
                                      self.acentric_outlier_miller )
    miller_array = miller_array.select( acentric_matches.single_selection(0) )
    return miller_array

  def _show_impl(self, out):
    out.show_sub_header("Possible outliers")
    out.show("  Inspired by: Read, Acta Cryst. (1999). D55, 1759-1764\n")
    out.show("Acentric reflections:")
    if self.acentric_outlier_miller.size() ==0:
      out.show("            None\n")
    else:
      out.show_table(self.acentric_outliers_table, indent=2)
      out.show_preformatted_text("""\n
 p(wilson)  : 1-(1-exp[-|E|^2])
 p(extreme) : 1-(1-exp[-|E|^2])^(n_acentrics)
""")
      out.show("""
 p(wilson) is the probability that an E-value of the specified value would be
 observed if it were selected at random the given data set. p(extreme) is the
 probability that the largest |E| value is larger or equal than the observed
 largest |E| value.

 Both measures can be used for outlier detection. p(extreme) takes into
 account the size of the dataset.
""")

    out.show("Centric reflections:")
    if self.centric_outlier_miller.size() ==0:
      out.show("            None\n")
    else :
      out.show_table(self.centric_outliers_table, indent=2)
      out.show_preformatted_text("""\n
 p(wilson)  : 1-(erf[|E|/sqrt(2)])
 p(extreme) : 1-(erf[|E|/sqrt(2)])^(n_acentrics)
""")
      out.show("""
 p(wilson) is the probability that an E-value of the specified
 value would be observed when it would selected at random from
 the given data set.
 p(extreme) is the probability that the largest |E| value is
 larger or equal than the observed largest |E| value.

 Both measures can be used for outlier detection. p(extreme)
 takes into account the size of the dataset.
""")

  def summarize_issues(self):
    if (self.fraction_outliers() > 0.001):
      return [(1, "There are a large number of outliers in the data.",
        "Possible outliers")]
    else :
      return [(0, "The fraction of outliers in the data is less than 0.1%.",
        "Possible outliers")]

#-----------------------------------------------------------------------
# ICE RINGS
class ice_ring_checker(scaling.xtriage_analysis):
  """
  Check intensity and completeness statistics in specific resolution ranges
  known to have strong diffraction when crystalline ice is present.
  """
  def __init__(self,
               bin_centers,
               completeness_data,
               z_scores_data,
               completeness_abnormality_level=4.0,
               intensity_level=0.1,
               z_score_limit=10):
    self.completeness_abnormality_level = completeness_abnormality_level
    self.intensity_level = intensity_level
    self.z_score_limit = z_score_limit
    self.ice_d_spacings=flex.double(
      [3.897,3.669,3.441,2.671,2.249,
       2.072,1.948,1.918,1.883,1.721])
    self.ice_rel_intens=flex.double(
      [1.000, 0.750, 0.530, 0.170, 0.390,
       0.300, 0.040, 0.180, 0.030, 0.020])
    self.ice_ring_bin_location=\
      [None, None, None, None, None,
       None, None, None, None, None]
    self.mean_comp=None
    self.mean_z_score=None
    tmp_low_d_star_sq=bin_centers[0]
    tmp_high_d_star_sq = bin_centers[bin_centers.size()-1]
    tmp_step = (bin_centers[1]-bin_centers[0])
    count=0
    ## array of weights masking ice ring sensitive areas
    weights = flex.double( bin_centers.size(), 1)
    for ice_ring in self.ice_d_spacings:
      tmp_ice_ring = 1.0/(ice_ring**2.0)
      tmp_ice_ring_bin = tmp_ice_ring - tmp_low_d_star_sq
      tmp_ice_ring_bin = (tmp_ice_ring_bin-tmp_step/2.0)/tmp_step
      tmp_ice_ring_bin = int(tmp_ice_ring_bin+0.5)+1
      if tmp_ice_ring_bin < 0 or tmp_ice_ring_bin > bin_centers.size() - 1:
        tmp_ice_ring_bin = None
      self.ice_ring_bin_location[count] = tmp_ice_ring_bin
      count+=1
      if tmp_ice_ring_bin is not None:
        weights[tmp_ice_ring_bin]=0.0
        ## also ignore flanking bins for safety
        if (tmp_ice_ring_bin-1) >= 0:
          weights[tmp_ice_ring_bin-1]=0.0
        if (tmp_ice_ring_bin+1) <=bin_centers.size()-1:
          weights[tmp_ice_ring_bin+1]=0.0
    mean_z_score = flex.sum(  weights*z_scores_data )
    mean_z_score /= flex.sum(weights)
    std_z_score = flex.sum(  weights*z_scores_data*z_scores_data )
    std_z_score /= flex.sum(weights)
    std_z_score = math.sqrt(std_z_score-mean_z_score*mean_z_score)
    mean_comp = flex.sum(  weights*completeness_data )
    mean_comp /= flex.sum(weights)
    std_comp = flex.sum(  weights*completeness_data*completeness_data)
    std_comp /= flex.sum(weights)
    std_comp = math.sqrt(std_comp-mean_comp*mean_comp)
    self.mean_comp=mean_comp
    self.mean_z_score= mean_z_score
    self.std_comp=std_comp
    self.std_z_score= std_z_score
    ## This array has z-score like features
    ## to detect ice rings (were are looking for spikes)
    self.abnormality_intensity = flex.double(10,0.0)
    self.value_intensity = flex.double(10,0.0)
    ## This array looks at the completeness
    ## and checks for sudden 'dips'
    self.abnormality_completeness = flex.double(10,0.0)
    self.value_completeness = flex.double(10,0.0)
    for ii in range(10):
      if self.ice_ring_bin_location[ii] is not None:
        ## checking is there is any weird, out of order z-score
        self.value_intensity[ii] = z_scores_data[
          self.ice_ring_bin_location[ii]]
        self.abnormality_intensity[ii]=z_scores_data[
          self.ice_ring_bin_location[ii]]
        self.abnormality_intensity[ii]-=mean_z_score
        self.abnormality_intensity[ii]/=(std_z_score+1.0e-6)
        ## checking for sudden dips in completeness
        self.value_completeness[ii]=completeness_data[
          self.ice_ring_bin_location[ii]]
        self.abnormality_completeness[ii]=completeness_data[
          self.ice_ring_bin_location[ii]]
        self.abnormality_completeness[ii]-=mean_comp
        self.abnormality_completeness[ii]/=(std_comp+1.0e-6)
    self.warnings=0 ## Number of ice ring related warnings
    icy_shells = []
    for ii in range(10):
      if self.ice_ring_bin_location[ii] is not None:
        icy_shells.append([ "%9.3f" % self.ice_d_spacings[ii],
                            "%10.3f" % abs(self.ice_rel_intens[ii]),
                            "%7.2f" % abs(self.value_intensity[ii]),
                            "%7.2f" % abs(self.value_completeness[ii]), ])
    self.table = table_utils.simple_table(
      table_rows=icy_shells,
      column_headers=["d_spacing", "Expected rel. I", "Data Z-score", "Completeness"])
    self.warnings = 0 ## Number of ice ring related warnings
    cutoff = self.completeness_abnormality_level
    for ii in range(10):
      if ((self.ice_ring_bin_location[ii] is not None) and
          (self.ice_rel_intens[ii] > self.intensity_level)):
        abnormality_completeness = abs(self.abnormality_completeness[ii])
        if ((abnormality_completeness >= cutoff) or
            (self.value_intensity[ii] > z_score_limit) or
            (abs(self.abnormality_intensity[ii]) >= cutoff)):
          self.warnings += 1

  def _show_impl(self, out):
    out.show_sub_header("Ice ring related problems")
    out.show("""\
 The following statistics were obtained from ice-ring insensitive resolution
 ranges:
""")
    out.show_preformatted_text("""\
    mean bin z_score      : %4.2f
        ( rms deviation   : %4.2f )
    mean bin completeness : %4.2f
        ( rms deviation   : %4.2f )
""" % (self.mean_z_score, self.std_z_score, self.mean_comp, self.std_comp))
    out.show("""\
 The following table shows the Wilson plot Z-scores and completeness for
 observed data in ice-ring sensitive areas.  The expected relative intensity
 is the theoretical intensity of crystalline ice at the given resolution.
 Large z-scores and high completeness in these resolution ranges might
 be a reason to re-assess your data processsing if ice rings were present.
""")
    out.show_table(self.table, indent=2)
    out.show("""\
 Abnormalities in mean intensity or completeness at resolution ranges with a
 relative ice ring intensity lower than %3.2f will be ignored.""" %
      (self.intensity_level))
    problems_detected = False
    cutoff = self.completeness_abnormality_level
    for ii in range(10):
      if ((self.ice_ring_bin_location[ii] is not None) and
          (self.ice_rel_intens[ii] > self.intensity_level)):
        abnormality_completeness = abs(self.abnormality_completeness[ii])
        if (abnormality_completeness >= cutoff):
          problems_detected = True
          out.show("""\
 At %3.2f A there is a lower completeness than expected from the rest of the
 data set.""" % self.ice_d_spacings[ii])
          if (abs(self.abnormality_intensity[ii]) >= cutoff):
            out.show("""\
 At the same resolution range, the expected mean intensity does not behave as
 it should.""")
          else :
            out.show("""\
 Even though the completeness is lower than expected, the mean intensity is
 still reasonable at this resolution.""")
        if ((abs(self.abnormality_intensity[ii]) >= cutoff) or
            (abs(self.value_intensity[ii])>getattr(self,"z_score_limit",10))):
          problems_detected = True
          if (abs(self.abnormality_completeness[ii])<=cutoff):
            out.show("""\
 At %3.2f A the z-score is more than %3.2f times the standard deviation of
 all z-scores, while at the same time, the completeness does not go down.
""" %(self.ice_d_spacings[ii], cutoff))

    if (not problems_detected):
      out.show("""\
 No ice ring related problems detected.
 If ice rings were present, the data does not look worse at ice ring related
 d_spacings as compared to the rest of the data set.""")
    if self.warnings == 1:
      out.show("""\
 As there was only 1 ice-ring related warning, it is not clear whether or not
 ice ring related features are really present.""")
    elif (self.warnings >= 2):
      out.show("""\
 There were %d ice ring related warnings.  This could indicate the presence of
 ice rings.""" % self.warnings)

  def summarize_issues(self):
    if (self.warnings > 0):
      return [(1, "The data appear to have one or more ice rings.",
        "Ice ring related problems")]
    else :
      return [(0, "Ice rings do not appear to be present.",
        "Ice ring related problems")]

#-----------------------------------------------------------------------
# ANOMALOUS MEASURABILITY
class analyze_measurability(scaling.xtriage_analysis):
  def __init__(self,
               d_star_sq,
               smooth_approx,
               meas_data,
               miller_array=None,
               low_level_cut=0.03,
               high_level_cut=0.06):
    low_level_range = smooth_approx > low_level_cut
    high_level_range = smooth_approx > high_level_cut
    tmp_low = d_star_sq.select(low_level_range)
    tmp_high = d_star_sq.select(high_level_range)
    self.low_d_cut = None
    self.high_d_cut = None
    ref_marks = [[None, None]]
    if tmp_low.size()>0:
      self.low_d_cut = flex.max(tmp_low) ** (-0.5)
      ref_marks[0][0] = flex.max(tmp_low)
    if tmp_high.size()>0:
      self.high_d_cut = flex.max(tmp_high) ** (-0.5)
      ref_marks[0][1] = flex.max(tmp_high)
    self.meas_table=None
    if miller_array is not None:
      work_array = miller_array.deep_copy()
      work_array.setup_binner(n_bins=10)
      self.meas_table = work_array.measurability(
        use_binning=True).as_simple_table(data_label="Measurability")
    self.table = data_plots.table_data(
      title="Measurability of Anomalous signal",
      column_labels=["1/resol**2", "Measurability", "Smooth approximation"],
      graph_names=["Anomalous measurability"],
      graph_labels=[("Resolution", "Measurability")],
      graph_columns=[[0,1,2]],
      data=[list(d_star_sq), list(meas_data), list(smooth_approx)],
      x_is_inverse_d_min=True,
      reference_marks=ref_marks,
      force_exact_x_labels=True)

  def _show_impl(self, out):
    out.show_sub_header("Measurability of anomalous signal")
    message = None
    unknown_cutoffs = ([self.low_d_cut,self.high_d_cut]).count(None)
    if unknown_cutoffs == 0 :
      message = None
      if self.low_d_cut ==  self.high_d_cut :
        message = """\
 The full resolution range seems to contain a useful amount of anomalous
 signal. Depending on your specific substructure, you could use all the data
 available for the location of the heavy atoms, or cut the resolution
 to speed up the search."""
      elif self.low_d_cut < self.high_d_cut:
        message = """\
 The anomalous signal seems to extend to about %3.1f A (or to %3.1f A, from a
 more optimistic point of view).  The quoted resolution limits can be used as a
 guideline to decide where to cut the resolution for phenix.hyss.""" % \
          (self.high_d_cut, self.low_d_cut)
        if self.high_d_cut < 3.0:
          message += """
 Depending however on the size and nature of your substructure you could cut
 the data at an even lower resolution to speed up the search."""
        elif self.high_d_cut > 4.5:
          message += """
 As the anomalous signal is not very strong in this dataset substructure
 solution via SAD might prove to be a challenge.  Especially if only low
 resolution reflections are used, the resulting substructures could contain a
 significant amount of false positives."""
    elif unknown_cutoffs == 2 :
      message = """\
 There seems to be no real significant anomalous differences in this dataset."""
    elif unknown_cutoffs == 1 :
      if self.high_d_cut is None:
        message="""\
 The anomalous signal seems to extend to about %3.1f A, but doesn't seem very
 strong. The quoted resolution limits can be used as a guideline to decide
 where to cut the resolution for phenix.hyss."""%(self.low_d_cut)
      else:
         message = """\
 This should not have happend: please contact software authors at
 help@phenix-online.org."""
    else :
      message = """\
 This should not have happend: please contact software authors at
 help@phenix-online.org."""
    if message is not None :
      out.show(message)
    # more details
    if (self.meas_table is not None):
      out.show("\nTable of measurability as a function of resolution:\n")
      out.show_table(self.meas_table, indent=2)
      out.show("""
 The measurability is defined as the fraction of Bijvoet related intensity
 differences for which the following relationship holds:""")
      out.show_preformatted_text("""
    |delta_I|/sigma_delta_I > 3.0
    min[I(+)/sigma_I(+), I(-)/sigma_I(-)] > 3.0
""")
      out.show("""\
  The measurability provides an intuitive feeling of the quality of the data,
  as it is related to the number of reliable Bijvoet differences.  When the
  data are processed properly and the standard deviations have been estimated
  accurately, values larger than 0.05 are encouraging.  Note that this
  analysis relies on the correctness of the estimated standard deviations of
  the intensities.
""")
      out.show_plot(self.table)

# TODO anomalous completeness

#-----------------------------------------------------------------------
# WRAPPER CLASSES

class data_strength_and_completeness(scaling.xtriage_analysis):
  """
  Collect basic info about overall completeness and signal-to-noise ratios,
  independent of scaling.
  """
  def __init__(self,
      miller_array,
      isigi_cut=3.0,
      completeness_cut=0.85,
      completeness_as_non_anomalous=None):
    # Make a deep copy not to upset or change things in
    # the binner that might be present
    tmp_miller = miller_array.deep_copy()
    tmp_miller.setup_binner_d_star_sq_step(auto_binning=True)
    self.d_star_sq_ori = tmp_miller.binner().bin_centers(2)
    # Signal-to-noise and completeness
    self.i_sig_i = None
    self.i_sig_i_table = None
    self.overall_i_sig_i = None
    if tmp_miller.sigmas() is not None:
      self.overall_i_sig_i = tmp_miller.i_over_sig_i(use_binning=False,
        return_fail=0)

      i_over_sigma = tmp_miller.i_over_sig_i(use_binning=True,
        return_fail=0)
      self.i_sig_i = i_over_sigma.data[1:len(i_over_sigma.data)-1]
      self.i_sig_i =  flex.double( self.i_sig_i )
      self.i_sig_i_table = data_plots.table_data(
        title="Signal to noise (<I/sigma_I>)",
        column_labels=["1/resol**2", "<I/sigma_I>"],
        graph_names=["Signal to noise"],
        graph_labels=[("Resolution", "<I/sigma_I>")],
        graph_columns=[[0,1]],
        data=[self.d_star_sq_ori,self.i_sig_i],
        x_is_inverse_d_min=True)
    #
    self.data_strength = None
    if (miller_array.sigmas() is not None):
      self.data_strength = i_sigi_completeness_stats(
        miller_array,
        isigi_cut=isigi_cut,
        completeness_cut=completeness_cut,
        completeness_as_non_anomalous=completeness_as_non_anomalous)
    self.completeness_overall = miller_array.completeness(
       as_non_anomalous_array = completeness_as_non_anomalous)
    # low-resolution completeness
    tmp_miller_lowres = miller_array.resolution_filter(d_min=5.0)
    self.low_resolution_completeness = None
    self.low_resolution_completeness_overall = None
    self.low_res_table = None
    if (tmp_miller_lowres.indices().size()>0):
      tmp_miller_lowres.setup_binner(n_bins=10)
      self.low_resolution_completeness_overall = \
        tmp_miller_lowres.completeness(use_binning=False,
          as_non_anomalous_array = completeness_as_non_anomalous)
      low_resolution_completeness = tmp_miller_lowres.completeness(
        use_binning=True,
        as_non_anomalous_array = completeness_as_non_anomalous)
      self.low_resolution_completeness = \
        low_resolution_completeness.as_simple_table(data_label="Completeness")
      binner = low_resolution_completeness.binner
      d_star_sq_ori = []
      comp = []
      for i_bin in binner.range_used():
        d_min = binner.bin_d_min(i_bin)
        d_star_sq_ori.append(1/(d_min**2))
        frac = low_resolution_completeness.data[i_bin]
        if (frac is None):
          frac = 0
        comp.append(frac*100)
      self.low_res_table = data_plots.table_data(
        title="Low-resolution completeness",
        column_labels=["Max. resolution", "Completeness"],
        graph_names=["Low-resolution completeness"],
        graph_labels=[("High resolution of shell", "% of total")],
        graph_columns=[[0,1]],
        data=[d_star_sq_ori, comp],
        x_is_inverse_d_min=True,
        force_exact_x_labels=True)
    try :
      self.d_min_directional = analyze_resolution_limits(miller_array)
    except AssertionError : # FIXME
      self.d_min_directional = None
    self.log_binned = log_binned_completeness(miller_array,
      completeness_as_non_anomalous=
        completeness_as_non_anomalous)

  def _show_impl(self, out):
    out.show_header("Data strength and completeness")

    if (hasattr(self, 'overall_i_sig_i') and
        (self.overall_i_sig_i is not None)):
      out.show("Overall <I/sigma> for this dataset is %7.1f" %(
        self.overall_i_sig_i))
    if (self.data_strength is not None):
      self.data_strength.show(out)
    if self.low_resolution_completeness is not None:
      out.show_sub_header("Low resolution completeness analyses")
      out.show("""\
The following table shows the completeness of the data to 5.0 A.  Poor
low-resolution completeness often leads to map distortions and other
difficulties, and is typically caused by problems with the crystal orientation
during data collection, overexposure of frames, interference with the beamstop,
or omission of reflections by data-processing software.""")
      out.show_table(self.low_resolution_completeness, indent=2)
    self.log_binned.show(out)
    if (self.d_min_directional is not None):
      self.d_min_directional.show(out)

  def high_resolution_for_twin_tests(self):
    if (self.data_strength is None):
      return None
    elif (self.data_strength.resolution_cut >
        self.data_strength.resolution_at_least):
      return self.data_strength.resolution_at_least
    else:
      return self.data_strength.resolution_cut

  def i_over_sigma_outer_shell(self):
    if (self.i_sig_i is not None):
      return self.i_sig_i[-1]
    return None

  def summarize_issues(self):
    issues = []
    if (self.d_min_directional is not None):
      if self.d_min_directional.is_elliptically_truncated():
        issues.append((1,"The data appear to have been elliptically truncated.",
          "Analysis of resolution limits"))
      else :
        issues.append((0,
          "The resolution cutoff appears to be similar in all directions.",
          "Analysis of resolution limits"))
    if self.low_resolution_completeness_overall and 0 < self.low_resolution_completeness_overall < 0.75:
      issues.append((2, "The overall completeness in low-resolution shells "+
        "is less than 90%.", "Low resolution completeness analyses"))
    elif self.low_resolution_completeness_overall and 0.75 <= self.low_resolution_completeness_overall < 0.9:
      issues.append((1, "The overall completeness in low-resolution shells "+
        "is less than 90%.", "Low resolution completeness analyses"))
    else :
      issues.append((0, "The overall completeness in low-resolution shells "+
        "is at least 90%.", "Low resolution completeness analyses"))
    return issues

class anomalous(scaling.xtriage_analysis):
  def __init__(self, miller_array, merging_stats=None,
      plan_sad_experiment_stats=None):
    assert miller_array.anomalous_flag()
    tmp_miller = miller_array.deep_copy()
    tmp_miller.setup_binner_d_star_sq_step(auto_binning=True)
    self.d_star_sq_ori = tmp_miller.binner().bin_centers(2)
    self.cc_anom_table = None
    if (merging_stats is not None):
      self.cc_anom_table = merging_stats.cc_anom_table
    self.plan_sad_experiment_stats=None
    if (plan_sad_experiment_stats is not None):
      self.plan_sad_experiment_stats=plan_sad_experiment_stats
    self.measurability = None
    # Get measurability if data is anomalous
    if (tmp_miller.sigmas() is not None):
      measurability = tmp_miller.measurability(use_binning=True,
        return_fail=0)
      meas_data = flex.double( measurability.data[1:len(
        measurability.data)-1])
      # make a smooth approximation to the measurability please
      smooth_meas_approx = chebyshev_lsq_fit.chebyshev_lsq_fit(
        int(self.d_star_sq_ori.size()/10) +3,
        self.d_star_sq_ori,
        meas_data )
      smooth_meas_approx = chebyshev_polynome(
        int(self.d_star_sq_ori.size()/10) +3,
        flex.min(self.d_star_sq_ori),
        flex.max(self.d_star_sq_ori),
        smooth_meas_approx.coefs)
      meas_smooth = smooth_meas_approx.f(self.d_star_sq_ori)
      self.measurability = analyze_measurability(
        d_star_sq=self.d_star_sq_ori,
        smooth_approx=meas_smooth,
        meas_data=meas_data,
        miller_array=tmp_miller)

  def _show_impl(self, out):
    out.show_header("Anomalous signal")
    if (self.cc_anom_table is not None):
      out.show_sub_header("Half-dataset anomalous correlation")
      out.show_text("""\
 This statistic (also called CC_anom) is calculated from unmerged data, which
 have been split into two half-datasets of approximately the same size and
 merged to obtain unique (anomalous) intensities, and their anomalous
 differences compared.  Resolution shells with CC_anom above 0.3 contain
 substantial anomalous signal useful for heavy atom location.""")
      if (out.gui_output):
        out.show_plot(self.cc_anom_table)
      else :
        out.show_table(self.cc_anom_table)
    if (self.measurability is not None):
      self.measurability.show(out)

    if (hasattr(self, 'plan_sad_experiment_stats') and
        (self.plan_sad_experiment_stats is not None)):
      out.show_header("Analysis of probability of finding %d sites with this anomalous data" %(
        self.plan_sad_experiment_stats.sites))
      if (out.gui_output):
        self.plan_sad_experiment_stats.show_in_wxgui(out=out)
      else:
        self.plan_sad_experiment_stats.show_summary(out=out)

  def summarize_issues(self):
    '''
    Traffic light
    '''
    issues = list()
    if (hasattr(self, 'plan_sad_experiment_stats') and
        (self.plan_sad_experiment_stats is not None)):
      p_substr = self.plan_sad_experiment_stats.get_p_substr()
      # round to nearest 5% when below 97%, otherwise round to nearest 1%
      if (p_substr < 97):
        p_substr = int(5.0*round(p_substr/5.0))
      else:
        p_substr = int(round(p_substr))
      sites = self.plan_sad_experiment_stats.sites
      message = 'The probability of finding %i sites with this data is around %i%%.' %\
                (sites, p_substr)
      section_name = 'Probability of finding sites and expected FOM'
      hi_limit = 85.0
      lo_limit = 50.0
      if (p_substr >= hi_limit):
        issues.append((0, message, section_name))
      elif ( (p_substr >= lo_limit) and (p_substr < hi_limit) ):
        issues.append((1, message, section_name))
      elif (p_substr < lo_limit):
        issues.append((2, message, section_name))
    return issues

class wilson_scaling(scaling.xtriage_analysis):
  """
  Calculates isotropic and anisotropic scale factors, Wilson plot, and various
  derived analyses such as ice rings and outliers.
  """
  def __init__(self,
               miller_array,
               n_residues,
               remove_aniso_final_b="eigen_min",
               use_b_iso=None,
               n_copies_solc=1,
               n_bases=0,
               z_score_cut=4.5,
               completeness_as_non_anomalous=None):
    assert (n_bases+n_residues != 0) and (n_copies_solc != 0)
    info = miller_array.info()
    tmp_miller = miller_array.deep_copy()
    tmp_miller.setup_binner_d_star_sq_step(auto_binning=True)
    self.d_star_sq_ori = tmp_miller.binner().bin_centers(2)
    if n_residues is None:
      n_residues = 0
    if n_bases is None:
      n_bases = 0
    if n_bases+n_residues==0:
      raise Sorry("No scatterers available")

    #-------------------------------------------------------------------
    # WILSON SCALING
    #
    # Isotropic
    order_z = miller_array.space_group().order_z()
    iso_scale_and_b = absolute_scaling.ml_iso_absolute_scaling(
      miller_array = miller_array,
      n_residues = n_residues * order_z * n_copies_solc,
      n_bases=n_bases * order_z * n_copies_solc)
    self.iso_scale_and_b = iso_scale_and_b
    ## Store the b and scale values from isotropic ML scaling
    self.iso_p_scale = iso_scale_and_b.p_scale
    self.iso_b_wilson =  iso_scale_and_b.b_wilson
    scat_info = self.iso_scale_and_b.scat_info

    ## Anisotropic ml wilson scaling
    order_z = miller_array.space_group().order_z()
    aniso_scale_and_b = absolute_scaling.ml_aniso_absolute_scaling(
      miller_array = miller_array,
      n_residues = n_residues * order_z * n_copies_solc,
      n_bases = n_bases * order_z * n_copies_solc)
    b_cart = aniso_scale_and_b.b_cart
    self.aniso_scale_and_b = aniso_scale_and_b

    self.aniso_p_scale = aniso_scale_and_b.p_scale
    self.aniso_u_star  = aniso_scale_and_b.u_star
    self.aniso_b_cart  = aniso_scale_and_b.b_cart
    # XXX: for GUI
    self.overall_b_cart = getattr(aniso_scale_and_b, "overall_b_cart", None)

    #-------------------------------------------------------------------
    # ANISOTROPY CORRECTION
    b_cart_observed = aniso_scale_and_b.b_cart
    b_trace_average = (b_cart_observed[0]+
                       b_cart_observed[1]+
                       b_cart_observed[2])/3.0
    b_trace_min = b_cart_observed[0]
    if  b_cart_observed[1] <b_trace_min: b_trace_min=b_cart_observed[1]
    if  b_cart_observed[2] <b_trace_min: b_trace_min=b_cart_observed[2]
    if (remove_aniso_final_b == "eigen_min"):
      b_use=aniso_scale_and_b.eigen_values[2]
    elif (remove_aniso_final_b == "eigen_mean"):
      b_use=flex.mean(aniso_scale_and_b.eigen_values)
    elif (remove_aniso_final_b == "user_b_iso"):
      assert remove_aniso_b_iso is not None
      b_use = use_b_iso
    else:
      b_use = 30

    b_cart_aniso_removed = [ -b_use, -b_use, -b_use, 0, 0, 0 ]
    u_star_aniso_removed = adptbx.u_cart_as_u_star(
      miller_array.unit_cell(),
      adptbx.b_as_u( b_cart_aniso_removed  ) )
    ## I do things in two steps, but can easely be done in 1 step
    ## just for clarity, thats all.
    self.no_aniso_array = absolute_scaling.anisotropic_correction(
      miller_array,0.0,aniso_scale_and_b.u_star )
    self.no_aniso_array = absolute_scaling.anisotropic_correction(
      self.no_aniso_array,0.0,u_star_aniso_removed)
    self.no_aniso_array = self.no_aniso_array.set_observation_type(
      miller_array )

    ## Make normalised structure factors please
    sel_big = self.no_aniso_array.data() > 1.e+50
    self.no_aniso_array = self.no_aniso_array.array(
      data = self.no_aniso_array.data().set_selected(sel_big, 0))
    self.no_aniso_array = self.no_aniso_array.set_observation_type(
      miller_array )
    normalistion = absolute_scaling.kernel_normalisation(
      self.no_aniso_array,auto_kernel=True)
    self.normalised_miller = normalistion.normalised_miller.deep_copy()

    # First we have to apply a resolution cut to make sure the reslution limts
    # match those of the empirical gamma array
    absolute_miller = miller_array.resolution_filter(
      d_max = math.sqrt(1.0/0.008),
      d_min = math.sqrt(1.0/0.69))
    #absolute_miller.as_mtz_dataset(column_root_label="F").mtz_object().write("start.mtz")
    ## anisotropy correction, bring data to absolute scale and B-value zero
    absolute_miller = absolute_scaling.anisotropic_correction(
      cache_0=absolute_miller,
      p_scale=self.aniso_p_scale,
      u_star=self.aniso_u_star)
    #absolute_miller.as_mtz_dataset(column_root_label="F").mtz_object().write("end.mtz")
    absolute_miller.set_observation_type( miller_array )
    # Now do some binning ala Popov&Bourenkov
    absolute_miller.setup_binner_d_star_sq_step(auto_binning=True)
    d_star_sq = absolute_miller.binner().bin_centers(2)
    d_star_sq[d_star_sq.size()-1] = 1.0/(
      flex.min( absolute_miller.d_spacings().data())
      *flex.min( absolute_miller.d_spacings().data()) )
    # Binning
    mean_observed_intensity = absolute_miller\
                              .mean_of_intensity_divided_by_epsilon(
                                 use_binning=True, return_fail=0.0)
    completeness = absolute_miller.completeness(use_binning=True,
        return_fail=1.0,
        as_non_anomalous_array = completeness_as_non_anomalous)
    # Recompute the scattering info summats please
    scat_info.scat_data(d_star_sq)
    # Please compute normalised structure factors
    normalisation = absolute_scaling.kernel_normalisation(
      absolute_miller,auto_kernel=True)
    normalised_miller = normalisation.normalised_miller.deep_copy()
    # set up a binner for this array as well please
    normalised_miller.setup_binner_d_star_sq_step(auto_binning=True)
    # Make a deep copy not to upset or change things in
    # the binner that might be present
    tmp_miller = miller_array.deep_copy()
    tmp_miller.setup_binner_d_star_sq_step(auto_binning=True)
    self.d_star_sq_ori = tmp_miller.binner().bin_centers(2)

    # Set up some arrays for plotting and analyses purposes
    self.d_star_sq = d_star_sq
    self.mean_I_normalisation = flex.exp(normalisation.normalizer.f(
      self.d_star_sq))
    self.mean_I_obs_data = flex.double(
      mean_observed_intensity.data[1:len(mean_observed_intensity.data)-1])
    # expected intensity
    theory = absolute_scaling.expected_intensity(
      scat_info,d_star_sq)
    self.mean_I_obs_theory = theory.mean_intensity
    # add standard deviations of experimental part
    # assuming wilson statistics for simplicity
    counts = flex.double( normalised_miller.binner().counts_given())
    counts = counts[1:counts.size()-1]
    self.mean_I_obs_sigma = self.mean_I_obs_data*self.mean_I_obs_data/(counts+1e-6)
    self.mean_I_obs_sigma+=theory.sigma_intensity*theory.sigma_intensity
    self.mean_I_obs_sigma=flex.sqrt(self.mean_I_obs_sigma)

    # z scores and completeness
    self.z_scores = flex.abs( self.mean_I_obs_data - self.mean_I_obs_theory )/\
                    self.mean_I_obs_sigma
    self.completeness = flex.double(completeness.data[
      1:len(mean_observed_intensity.data)-1])

    self.outliers = possible_outliers(absolute_miller)
    self.miller_array_filtered = self.outliers.remove_outliers(absolute_miller)
    self.ice_rings = None
    if (self.d_star_sq.size() > 1) and (flex.min( self.d_star_sq ) > 0.01):
      self.ice_rings = ice_ring_checker(
        bin_centers=self.d_star_sq,
        completeness_data=self.completeness,
        z_scores_data=self.z_scores)

    self.wilson_table = data_plots.table_data(
      title="Intensity plots",
      column_labels=["1/resol**2", "<I> smooth approximation",
                    "<I> via binning", "<I> expected"],
      graph_names=["Intensity plots"],
      graph_labels=[("Resolution", "<I>")],
      graph_columns=[[0,1,2,3]],
      data=[list(self.d_star_sq), list(self.mean_I_normalisation),
            list(self.mean_I_obs_data), list(self.mean_I_obs_theory)],
      x_is_inverse_d_min=True,
      force_exact_x_labels=True)
    # collect suspicious resolution shells
    worrisome = self.z_scores > z_score_cut
    self.n_worrisome = worrisome.count(True)
    self.outlier_shell_table = data_plots.table_data(
      title="Mean intensity by shell (outliers)",
      column_labels=["d_spacing", "z_score", "completeness", "<Iobs>/<Iexp>"],
      column_formats=["%9.3f", "%7.2f", "%7.2f", "%10.3f"],
      graph_names=["Worrisome resolution shells"],
      graph_columns=[[0,1,2,3]])
    for ii in range(self.d_star_sq.size()):
      if worrisome[ii]:
        d_space = self.d_star_sq[ii]**(-0.5)
        z_score = self.z_scores[ii]
        comp =  self.completeness[ii]
        ratio = self.mean_I_obs_data[ii] / self.mean_I_obs_theory[ii]
        self.outlier_shell_table.add_row([d_space,z_score,comp,ratio])
    ## z scores and completeness
    self.zscore_table = data_plots.table_data(
      title="Z scores and completeness",
      column_labels=["1/resol**2", "Z_score", "Fractional completeness"],
      graph_names=["Data sanity and completeness check"],
      graph_labels=[("Resolution", "Z score or fractional completeness")],
      graph_columns=[[0,1,2]],
      data=[list(self.d_star_sq), list(self.z_scores), list(self.completeness)],
      x_is_inverse_d_min=True)

  def show_worrisome_shells(self, out):
    out.show_sub_header("Mean intensity analyses")
    out.show("""\
 Inspired by: Morris et al. (2004). J. Synch. Rad.11, 56-59.
 The following resolution shells are worrisome:""")
    if (self.n_worrisome > 0):
      out.show_table(self.outlier_shell_table, indent=2)
      out.show("""\
 Possible reasons for the presence of the reported unexpected low or elevated
 mean intensity in a given resolution bin are :
   - missing overloaded or weak reflections
   - suboptimal data processing
   - satellite (ice) crystals
   - NCS
   - translational pseudo symmetry (detected elsewhere)
   - outliers (detected elsewhere)
   - ice rings (detected elsewhere)
   - other problems
 Note that the presence of abnormalities in a certain region of reciprocal
 space might confuse the data validation algorithm throughout a large region
 of reciprocal space, even though the data are acceptable in those areas.

""")
    else :
      out.show(" *** None ***")

  def _show_impl(self, out):
    out.show_header("Absolute scaling and Wilson analysis")
    self.iso_scale_and_b.show(out=out)
    self.aniso_scale_and_b.show(out=out)
    out.show_sub_header("Wilson plot")
    # FIXME get feedback from TPTB
    out.show_text("""\
 The Wilson plot shows the falloff in intensity as a function in resolution;
 this is used to calculate the overall B-factor ("Wilson B-factor") for the
 data shown above.  The expected plot is calculated based on analysis of
 macromolecule structures in the PDB, and the distinctive appearance is due to
 the non-random arrangement of atoms in the crystal.  Some variation is
 natural, but major deviations from the expected plot may indicate pathological
 data (including ice rings, detector problems, or processing errors).""")
    out.show_plot(self.wilson_table)
    # XXX is this really necessary?
    #out.show_plot(self.zscore_table)
    self.show_worrisome_shells(out)
    self.outliers.show(out)
    if self.ice_rings is not None:
      self.ice_rings.show(out)

  def summarize_issues(self):
    issues = []
    if self.ice_rings is not None:
      issues.extend(self.ice_rings.summarize_issues())
    issues.extend(self.outliers.summarize_issues())
    issues.extend(self.iso_scale_and_b.summarize_issues())
    issues.extend(self.aniso_scale_and_b.summarize_issues())
    return issues

  # Objects of this class are relatively bulky due to the storage of multiple
  # derived Miller arrays that may be used elsewhere.  Since we do not need
  # these arrays for simply displaying the results (in the Phenix GUI or
  # elsewhere), they are deleted prior to pickling to reduce the amount of
  # data that needs to be transfered or saved.  It is not necessary to
  # implement __setstate__, since we are still just pickling self.__dict__.
  def __getstate__(self):
    """
    Pickling function with storage efficiency optimizations.
    """
    self.miller_array_filtered = None
    self.no_aniso_array = None
    self.normalised_miller = None
    self.d_star_sq_ori = None
    self.iso_scale_and_b.work_array = None
    self.aniso_scale_and_b.work_array = None
    self.iso_scale_and_b.scat_info = None
    self.aniso_scale_and_b.scat_info = None
    return self.__dict__


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/fa_estimation.py
from __future__ import absolute_import, division, print_function
from cctbx import miller
from cctbx.array_family import flex
from libtbx.utils import Sorry
import iotbx.phil
import mmtbx.scaling
from mmtbx.scaling import absolute_scaling, relative_scaling
from mmtbx.scaling import pair_analyses
import sys
from six.moves import range


class combined_scaling(object):
  def __init__(self,
               miller_array_x1,
               miller_array_x2,
               options=None,
               out=None):
    ## miller_array_x1 : 'reference'
    ## miller_array_x2 : 'derivative' or something similar
    ## options : a phil file with scaling options
    ## convergence : rescale after rejection?
    ##
    ##
    ## These are the tasks ahead
    ## 0) map to asu, common sets
    ## 1) least squares scaling of x1 and x2
    ## 2) local scaling of x1 and x2
    ## 3) outlier rejections
    ## 4) least squares scaling of x1 and x2
    ## 5) local scaling of x1 and x2
    ## 6) generation of delta F's
    ##
    ## we will perform operations on this arrays
    self.x1 = miller_array_x1.deep_copy().map_to_asu()
    self.x2 = miller_array_x2.deep_copy().map_to_asu()
    ## Get the scaling and rejection options
    self.lsq_options = None
    self.loc_options = None
    self.out_options = None
    self.overall_protocol = None
    self.cut_level_rms = None
    if options is not None:
      self.lsq_options = options.least_squares_options
      self.loc_options = options.local_scaling_options
      self.out_options = options.outlier_rejection_options
      self.overall_protocol = options.target
      self.cut_level_rms = None
    ## cycling options
    self.auto_cycle=False
    if options is not None:
      if options.iterations=='auto':
        self.auto_cycle=True

    self.max_iterations = 10
    if options is not None:
      self.max_iterations = options.max_iterations
    if not self.auto_cycle:
      assert self.max_iterations > 0

    ## output options
    self.out=out
    if self.out==None:
      self.out=sys.stdout
    ## get common sets, and make a reference copy please
    self.x1, self.x2 = self.x1.common_sets( self.x2 )
    self.s1 = self.x1.deep_copy().map_to_asu()
    self.s2 = self.x2.deep_copy().map_to_asu()

    scaling_tasks={'lsq':False, 'local':False }

    if self.overall_protocol=='ls':
      scaling_tasks['lsq']=True
      scaling_tasks['local']=False

    if self.overall_protocol=='loc':
      scaling_tasks['lsq']=False
      scaling_tasks['local']=True

    if self.overall_protocol=='ls_and_loc':
      scaling_tasks['lsq']=True
      scaling_tasks['local']=True

    print(scaling_tasks)
    print(self.overall_protocol)
    #assert ( scaling_tasks['lsq'] or scaling_tasks['local'] )

    self.convergence=False
    counter = 0

    print(file=self.out)
    print("==========================================", file=self.out)
    print("=             Relative scaling           =", file=self.out)
    print("==========================================", file=self.out)
    print(file=self.out)


    while not self.convergence:
      print(file=self.out)
      print("--------------------------", file=self.out)
      print("    Scaling cycle %i   "%(counter), file=self.out)
      print("--------------------------", file=self.out)

      if counter == 0:
        self.cut_level_rms = 3
        if options is not None:
          self.cut_level_rms = self.out_options.cut_level_rms_primary
      if counter > 0:
        self.cut_level_rms = 3
        if options is not None:
          self.cut_level_rms = self.out_options.cut_level_rms_secondary

      ## take the common sets
      self.s1 = self.s1.common_set(  self.x1 )
      self.s2 = self.s2.common_set(  self.x2 )
      self.x1, self.x2 = self.s1.common_sets( self.s2 )

      if scaling_tasks['lsq']:
        self.perform_least_squares_scaling()
      if scaling_tasks['local']:
        self.perform_local_scaling()
      num_reject = self.perform_outlier_rejection()
      if num_reject==0:
        self.convergence=True
      if not self.auto_cycle:
        if counter==self.max_iterations:
          self.convergence=True
      counter+=1

    ## Now the datasets have been scaled, we can return bnotyh datasets
    ## They can be used by the usewr to computer either
    ## - isomorphous differences
    ## - be used in an F1-F2 FFT
    del self.s1
    del self.s2




  def perform_least_squares_scaling(self):
    print(file=self.out)
    print("Least squares scaling", file=self.out)
    print("---------------------", file=self.out)
    print(file=self.out)

    ##-----Get options-----
    use_exp_sigmas=self.lsq_options.use_experimental_sigmas

    use_int = True
    if self.lsq_options.scale_data=='amplitudes':
      use_int=False

    use_wt=True
    if self.lsq_options.scale_target == 'basic':
      use_wt=False
    ##-------------------------------------------
    ls_scaling = relative_scaling.ls_rel_scale_driver(
        self.x1,
        self.x2,
        use_intensities=use_int,
        scale_weight=use_wt,
        use_weights=use_exp_sigmas)
    ls_scaling.show(out=self.out)
    ##----- Update the miller arrays please-------
    self.x1 = ls_scaling.native.deep_copy()
    self.x2 = ls_scaling.derivative.deep_copy()

  def perform_local_scaling(self):
    print(file=self.out)
    print("Local scaling", file=self.out)
    print("-------------", file=self.out)
    print(file=self.out)

    ##-----Get options-----
    use_exp_sigmas=self.loc_options.use_experimental_sigmas
    use_int = True

    if self.loc_options.scale_data=='amplitudes':
      use_int=False

    local_scaling_target_dictionary ={
      'local_moment':False,
      'local_lsq':False,
      'local_nikonov':False}
    local_scaling_target_dictionary[self.loc_options.scale_target]=True
    print(local_scaling_target_dictionary)

    ##--------------------
    local_scaling = relative_scaling.local_scaling_driver(
      self.x1,
      self.x2,
      local_scaling_target_dictionary,
      use_intensities=use_int,
      use_weights=use_exp_sigmas,
      max_depth=self.loc_options.max_depth,
      target_neighbours=self.loc_options.target_neighbours,
      sphere=self.loc_options.neighbourhood_sphere,
      out=self.out)

    self.x1 = local_scaling.native.deep_copy()
    self.x2 = local_scaling.derivative.deep_copy()


  def perform_outlier_rejection(self):
    print(file=self.out)
    print("Outlier rejections", file=self.out)
    print("------------------", file=self.out)
    print(file=self.out)
    print(" sigma criterion : %4.1f "%(self.out_options.cut_level_sigma), file=self.out)
    print(" rms criterion   : %4.1f "%(self.cut_level_rms), file=self.out)
    print(" protocol        :", self.out_options.protocol, file=self.out)
    print(file=self.out)

    outlier_protocol ={'solve':False,'rms':False, 'rms_and_sigma':False }
    ## Please set the protocol to what is specified
    outlier_protocol[ self.out_options.protocol ]=True

    outlier_rejection = pair_analyses.outlier_rejection(
      self.x1,
      self.x2,
      cut_level_rms=self.cut_level_rms,
      cut_level_sigma=self.out_options.cut_level_sigma
    )

    number_of_outliers = self.x1.size() - outlier_rejection.nat.size()
    number_of_outliers += self.x2.size() - outlier_rejection.der.size()

    self.x1 = outlier_rejection.nat.deep_copy()
    self.x2 = outlier_rejection.der.deep_copy()

    self.x1, self.x2 = self.x1.common_sets( self.x2 )
    return( number_of_outliers )




class ano_scaling(object):
  def __init__(self,
               miller_array_x1,
               options=None,
               out=None):
    ## These are the tasks ahead
    ##
    ## 1) splitting up x1 in hemsispheres x1p x1n
    ## 2) sumbiut the two halve data sets to the combined scaler
    ##

    assert miller_array_x1.anomalous_flag()
    assert miller_array_x1.indices().size() > 0

    self.options = options

    self.s1p, self.s1n = miller_array_x1.hemispheres_acentrics()

    self.s1p = self.s1p.set_observation_type( miller_array_x1 )

    self.s1n = self.s1n.customized_copy( indices=-self.s1n.indices() )
    self.s1n = self.s1n.set_observation_type( miller_array_x1 )


    assert self.s1p.indices().size() == self.s1n.indices().size()

    self.x1p = self.s1p.deep_copy()
    self.x1n = self.s1n.deep_copy()

    ## Now we have a 'native' and a 'derivative'
    ##
    ## Submit these things to the combined scaler
    if self.options is not None:
      ano_scaler=combined_scaling(
        self.x1p,
        self.x1n,
        options,
        out)
      self.s1p = ano_scaler.x1.deep_copy()
      self.s1n = ano_scaler.x2.deep_copy()
      del ano_scaler



class naive_fa_estimation(object):
  def __init__(self,
               ano,
               iso,
               options,
               out=None):
    if out == None:
      out = sys.stdout

    ## get stuff
    self.options = options
    self.iso = iso.deep_copy().map_to_asu()
    self.ano = ano.deep_copy().map_to_asu()
    ## get common sets
    self.iso, self.ano = self.iso.common_sets( self.ano )

    ## perform normalisation
    normalizer_iso = absolute_scaling.kernel_normalisation(
      self.iso, auto_kernel=True, n_term=options.number_of_terms_in_normalisation_curve)
    normalizer_ano = absolute_scaling.kernel_normalisation(
      self.ano, auto_kernel=True, n_term=options.number_of_terms_in_normalisation_curve)

    self.fa = self.iso.customized_copy(
      data = flex.sqrt( self.iso.data()*self.iso.data()\
               /normalizer_iso.normalizer_for_miller_array
               +
               self.ano.data()*self.ano.data()\
               /normalizer_ano.normalizer_for_miller_array
              ),
      sigmas = flex.sqrt( self.iso.sigmas()*self.iso.sigmas()\
               /(normalizer_iso.normalizer_for_miller_array*
                 normalizer_iso.normalizer_for_miller_array
                 )
               +
               self.ano.sigmas()*self.ano.sigmas()\
               /(normalizer_ano.normalizer_for_miller_array
                 *normalizer_ano.normalizer_for_miller_array)
              ))





class cns_fa_driver(object):
  def __init__(self,
               lambdas):
    ## first generate all anomalous differences
    self.ano_and_iso = []
    self.na_ano=0

    for set in lambdas:
      assert set.is_xray_amplitude_array()
      if set.anomalous_flag():
        self.na_ano+=1
        plus, minus = set.hemispheres_acentrics()
        d_ano = plus.customized_copy(
          data = flex.abs( plus.data() - minus.data() ),
          sigmas = flex.sqrt( plus.sigmas()*plus.sigmas() +
                              minus.sigmas()*minus.sigmas() )
          ).set_observation_type( set )
        self.ano_and_iso.append( d_ano )
    #now generate all isomorphous differences
    self.n_iso=0
    for set1 in range(len(lambdas)):
      for set2 in range(set1+1,len(lambdas)):
        self.n_iso+=1
        t1 = lambdas[set1].average_bijvoet_mates().set_observation_type(
          lambdas[set1] )
        t2 = lambdas[set2].average_bijvoet_mates().set_observation_type(
          lambdas[set2] )
        tmp1,tmp2 = t1.common_sets(t2)
        tmp1 = tmp1.customized_copy(
          data = flex.abs( tmp1.data() - tmp2.data() ),
          sigmas = flex.sqrt( tmp1.sigmas()*tmp1.sigmas() +
                              tmp2.sigmas()*tmp2.sigmas() )
        ).set_observation_type( tmp1 )
        self.ano_and_iso.append( tmp1 )


    self.normalise_all()
    self.average_all()

  def normalise_all(self):
    ## normalise all difference data please
    for set in self.ano_and_iso:
      tmp_norm = absolute_scaling.kernel_normalisation(
        set,
        auto_kernel=True)
      set = tmp_norm.normalised_miller.deep_copy().set_observation_type(
        tmp_norm.normalised_miller)


  def average_all(self):
    ## get started quickly please
    mean_index = self.ano_and_iso[0].indices()
    mean_data = self.ano_and_iso[0].data()
    mean_sigmas = self.ano_and_iso[0].sigmas()

    ## loop over the remaining arrays
    for set_no in range( 1,len(self.ano_and_iso) ):
      mean_index = mean_index.concatenate(
        self.ano_and_iso[ set_no ].indices() )

      mean_data= mean_data.concatenate(
        self.ano_and_iso[ set_no ].data() )

      mean_sigmas = mean_sigmas.concatenate(
        self.ano_and_iso[ set_no ].sigmas() )

    final_miller = self.ano_and_iso[0].customized_copy(
      indices= mean_index,
      data = mean_data,
      sigmas = mean_sigmas).set_observation_type( self.ano_and_iso[0] )

    final_miller = final_miller.f_as_f_sq()

    merged = final_miller.merge_equivalents()
    merged.show_summary()
    self.fa = merged.array().set_observation_type(final_miller).f_sq_as_f()



class mum_dad(object):
  def __init__(self,
               lambda1,
               lambda2,
               k1=1.0):
    ## assumed is of course that the data are scaled.
    ## lambda1 is the 'reference'
    self.w1=lambda1.deep_copy()
    self.w2=lambda2.deep_copy()

    if not self.w1.is_xray_amplitude_array():
      self.w1 = self.w1.f_sq_as_f()
    if not self.w2.is_xray_amplitude_array():
      self.w2 = self.w2.f_sq_as_f()

    self.w1, self.w2 = self.w1.common_sets( self.w2 )

    l1p, l1n = self.w1.hemispheres_acentrics()
    self.mean1 = l1p.data()+l1n.data()
    self.diff1 = l1p.data()-l1n.data()
    self.v1 = ( l1p.sigmas()*l1p.sigmas() +
                l1n.sigmas()*l1n.sigmas() )

    l2p, l2n = self.w2.hemispheres_acentrics()
    self.mean2 = l2p.data()+l2n.data()
    self.diff2 = l2p.data()-l2n.data()
    self.v2 = ( l2p.sigmas()*l2p.sigmas() +
                l2n.sigmas()*l2n.sigmas() )

    self.new_diff = flex.abs( (self.diff1 + k1*self.diff2)/2.0 )
    self.new_sigma_mean = flex.sqrt( (self.v1+k1*k1*self.v2)/2.0 )

    self.dad = l1p.customized_copy(
      data = self.new_diff,
      sigmas = self.new_sigma_mean ).set_observation_type( self.w1 )


class singh_ramasheshan_fa_estimate(object):
  def __init__(self,
               w1,
               w2,
               k1,
               k2):
    self.w1=w1.deep_copy()
    self.w2=w2.deep_copy()

    if self.w1.is_xray_amplitude_array():
      self.w1 = self.w1.f_as_f_sq()
    if self.w2.is_xray_amplitude_array():
      self.w2 = self.w2.f_as_f_sq()

    ## common sets please
    self.w1,self.w2 = self.w1.common_sets( self.w2 )

    ## get differences and sums please
    self.p1, self.n1 = self.w1.hemispheres_acentrics()
    self.p2, self.n2 = self.w2.hemispheres_acentrics()

    self.diff1 = self.p1.data() - self.n1.data()
    self.diff2 = self.p2.data() - self.n2.data()

    self.s1 =   self.p1.sigmas()*self.p1.sigmas()\
              + self.n1.sigmas()*self.n1.sigmas()
    self.s1 =  flex.sqrt( self.s1 )

    self.s2 =   self.p2.sigmas()*self.p2.sigmas()\
              + self.n2.sigmas()*self.n2.sigmas()
    self.s2 =  flex.sqrt( self.s2 )

    self.sum1 = self.p1.data() + self.n1.data()
    self.sum2 = self.p2.data() + self.n2.data()

    self.k1_sq = k1*k1
    self.k2_sq = k2*k2

    self.determinant=None
    self.fa=None
    self.sigfa=None


    self.selector=None
    self.iselector=None

    self.a=None
    self.b=None
    self.c=None

    self.compute_fa_values()

    self.fa = self.p1.customized_copy(
      data = self.fa,
      sigmas = self.sigfa).set_observation_type( self.p1 )

  def set_sigma_ratio(self):
    tmp = self.s2/flex.abs( self.diff2 +1e-6 ) +\
          self.s1/flex.abs( self.diff1 +1e-6 )
    tmp = tmp/2.0
    self.sigfa = tmp

  def compute_coefs(self):
    self.a = ( self.k2_sq*self.k2_sq
               + self.k2_sq*(1 + self.k1_sq)
               + (self.k1_sq-1)*(self.k1_sq-1)
             )
    self.b = -self.k2_sq*(self.sum1 + self.sum2) \
             -(self.k1_sq-1)*(self.sum1 - self.sum2)
    self.c = 0.25*(self.sum1 - self.sum2)*(self.sum1 - self.sum2)\
            +(1.0/8.0)*self.k2_sq*(  self.diff2*self.diff2
                                   + self.diff1*self.diff1/self.k1_sq)

  def compute_determinant(self):
    self.determinant = self.b*self.b - 4.0*self.a*self.c
    self.selector = (self.determinant>0)
    self.iselector = self.selector.iselection()

  def compute_fa_values(self):
    self.compute_coefs()
    self.compute_determinant()
    reset_selector = (~self.selector).iselection()
    self.determinant = self.determinant.set_selected(  reset_selector, 0 )

    choice1 = -self.b + flex.sqrt( self.determinant )
    choice1 /= 2*self.a
    choice2 = -self.b - flex.sqrt( self.determinant )
    choice2 /= 2*self.a

    select1 = choice1 > choice2
    select2 = ~select1

    choice1 = choice1.set_selected( select1.iselection(), 0 )
    choice2 = choice2.set_selected( select2.iselection(), 0 )

    choice1 = choice1+choice2
    select1 = (choice1<0).iselection()
    choice1 = choice1.set_selected( select1 , 0 )
    self.fa =  choice1 + choice2

    self.set_sigma_ratio()

    self.sigfa = self.sigfa*self.fa



class twmad_fa_driver(object):
  def __init__(self,
               lambda1,
               lambda2,
               k1,
               k2,
               options,
               out=None):
    self.out=out
    if self.out==None:
      self.out=sys.stdout

    self.options = options
    print("FA estimation", file=self.out)
    print("=============", file=self.out)

    if k1 is None:
      raise Sorry("f\"(w1)/f\"(w2) ratio is not defined. Please provide f\" values upon input")

    if k2 is None:
      if self.options.protocol=='algebraic':
        raise Sorry("""
delta f' f\" ratio is not defined.
Either provide f' and f\" values upon input,
or chose different Fa estimation protocol.
               """)

    self.options = options

    protocol = {'algebraic': False,
                'cns': False,
                'combine_ano': False}
    protocol[ self.options.protocol ] = True

    self.fa_values = None

    if protocol['algebraic']:
      print(" Using algebraic approach to estimate FA values ", file=self.out)
      print(file=self.out)
      tmp = singh_ramasheshan_fa_estimate(
        lambda1,
        lambda2,
        k1,
        k2)
      self.fa_values = tmp.fa.f_sq_as_f()

    if protocol['cns']:
      print(" Using CNS approach to estimate FA values ", file=self.out)
      print(file=self.out)

      tmp = cns_fa_driver( [lambda1, lambda2] )
      self.fa_values = tmp.fa

    if protocol['combine_ano']:
      print(" Combining anomalous data only", file=self.out)
      print(file=self.out)

      tmp = mum_dad(
        lambda1,
        lambda2,
        k1)
      self.fa_values = tmp.dad

    norma = absolute_scaling.kernel_normalisation(
      self.fa_values,
      auto_kernel=True)

    self.fa_values = norma.normalised_miller.f_sq_as_f()


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/make_param.py
from __future__ import absolute_import, division, print_function
import iotbx.phil
import sys

class phil_lego(object):
  """
This class facilitates the construction of phil parameter files
for the FA estimation program FATSO.
"""
  def __init__(self):

    self.default_expert_level_for_parameters_that_should_be_sensible_defaults='1'

    self.scaling_input = """ scaling.input{
__REPLACE__

expert_level=0
.type=int
.expert_level=__EXPERT_LEVEL__
}
"""
    self.basic_info = """basic{
  n_residues=None
  .type=float
  n_bases=None
  .type=float
  n_copies_per_asu=None
  .type=float
}
"""
    self.xray_data_basic="""xray_data{
  unit_cell=None
  .type=unit_cell

  space_group=None
  .type=space_group

  __REPLACE__
}
"""

    self.data_type="""__REPLACE__{
  file_name=None
  .type=path
  labels=None
  .type=strings
}
"""
    self.scaling_strategy="""scaling_strategy
.expert_level=__EXPERT_LEVEL__
{
  __REPLACE__
}
"""

    self.pre_scaler_protocol="""pre_scaler_protocol
.expert_level=__EXPERT_LEVEL__
{
high_resolution=None
.type=float
low_resolution=None
.type=float
aniso_correction=True
.type=bool
b_add = None
.type = float
outlier_level_wilson=1e-6
.type=float
 outlier_level_extreme=1e-2
.type=float
}"""

    self.scale_protocol="""__REPLACE__
.expert_level=__EXPERT_LEVEL__
{
           target = ls loc *ls_and_loc None
         .type=choice
         iterations = *auto specified_by_max_iterations
         .type=choice
         max_iterations = 2
         .type=int

         least_squares_options{
           use_experimental_sigmas=True
           .type=bool
           scale_data=*intensities amplitudes
           .type=choice
           scale_target=basic *fancy
           .type=choice
         }

         local_scaling_options{
           use_experimental_sigmas=True
           .type=bool
           scale_data=intensities *amplitudes
           .type=choice
           scale_target=local_moment local_lsq *local_nikonov
           .type=choice
           max_depth=10
           .type=int
           target_neighbours=150
           .type=int
           neighbourhood_sphere=1
           .type=int
         }

         outlier_rejection_options{
           cut_level_sigma=3
           .type=float
           cut_level_rms_primary=4
           .type=float
           cut_level_rms_secondary=4
           .type=float
           protocol=solve rms *rms_and_sigma
           .type=choice
         }


}"""

    self.fa_estimation="""fa_estimation
.expert_level=__EXPERT_LEVEL__
{
   protocol = *algebraic cns combine_ano
   .type = choice
   number_of_terms_in_normalisation_curve=23
   .type=int
}
"""


    self.output="""output
{
     log = 'fatso.log'
     .type = path
     hklout = 'fatso.mtz'
     .type = path
     outlabel = '_ATSO'
     .type = str

}
"""

    self.omit="""omit
.expert_level=__EXPERT_LEVEL__
{
   perform_omit=False
   .type=bool
   fraction=0.15
   .type=float
   max_number=1e5
   .type=int
   number_of_sets=10
   .type=int
   root_name='omit_'
   .type=str
}
"""


  def add_wavelength_info(self):
    tmp= """
    use_anomalous=True
    .type=bool
    .expert_level=5
    use_in_dispersive=True
    .type=bool
    .expert_level=5
    wavelength=None
    .type=float
    .expert_level=15
    f_prime=None
    .type=float
    .expert_level=0
    f_double_prime=None
    .type=float
    .expert_level=0
    }
    """
    self.data_type =self.data_type.replace( '}', tmp)


  def default_sad(self):
    outer_level = self.scaling_input
    outer_level = outer_level.replace( '__EXPERT_LEVEL__',
      self.default_expert_level_for_parameters_that_should_be_sensible_defaults)

    basic = self.basic_info
    data = self.data_type.replace( '__REPLACE__',
                                      'reference' )
    data = self.xray_data_basic.replace('__REPLACE__',
                                           data )
    scaler = self.pre_scaler_protocol + \
             self.scale_protocol.replace('__REPLACE__',
                                         'ano_protocol' )
    scaler = scaler.replace('__EXPERT_LEVEL__',
                            '1' )
    scaler = self.scaling_strategy.replace('__REPLACE__',
                                           scaler )
    scaler = scaler.replace('__EXPERT_LEVEL__',
                            '1' )
    scaler = scaler.replace( 'ls loc *ls_and_loc None',
                              '*loc None' )
    output = self.output

    omit = self.omit
    omit = omit.replace('__EXPERT_LEVEL__',
                        '1' )

    result = outer_level.replace('__REPLACE__',
                                 basic+data+scaler+omit+output)
    return result

  def default_sir(self):
    outer_level = self.scaling_input
    outer_level = outer_level.replace( '__EXPERT_LEVEL__',
      self.default_expert_level_for_parameters_that_should_be_sensible_defaults)

    basic = self.basic_info
    data = self.data_type.replace( '__REPLACE__',
                                      'native' ) \
                                      + \
            self.data_type.replace( '__REPLACE__',
                                      'derivative' )


    data = self.xray_data_basic.replace('__REPLACE__',
                                           data )

    scaler = self.scale_protocol.replace('__REPLACE__','iso_protocol' )

    scaler = self.pre_scaler_protocol + scaler

    scaler = scaler.replace('__EXPERT_LEVEL__',
                            '1' )
    scaler = self.scaling_strategy.replace('__REPLACE__',
                                           scaler )
    scaler = scaler.replace('__EXPERT_LEVEL__',
                            '1' )
    output = self.output

    result = outer_level.replace('__REPLACE__',
                                 basic+data+scaler+output)
    return result

  def default_rip(self):
    tmp = self.default_sir()
    tmp = tmp.replace( 'native', 'after_burn' )
    tmp = tmp.replace( 'derivative', 'before_burn' )
    tmp = tmp.replace( 'least_squares_options{',
                       """nsr_bias=1.0
.type=float
.expert_level=0

least_squares_options{""")
    return tmp


  def default_siras(self):
    outer_level = self.scaling_input
    outer_level = outer_level.replace( '__EXPERT_LEVEL__',
      self.default_expert_level_for_parameters_that_should_be_sensible_defaults)

    basic = self.basic_info
    data = self.data_type.replace( '__REPLACE__',
                                      'native' ) \
                                      + \
            self.data_type.replace( '__REPLACE__',
                                      'derivative' )

    data = self.xray_data_basic.replace('__REPLACE__',
                                           data )

    scaler = self.scale_protocol.replace('__REPLACE__',
                                         'ano_protocol' )
    scaler = scaler.replace('ls loc *ls_and_loc None',
                            '*loc None' )

    scaler = self.pre_scaler_protocol + scaler + \
             self.scale_protocol.replace('__REPLACE__','iso_protocol' )

    scaler = scaler.replace('__EXPERT_LEVEL__',
                            '1' )
    scaler = self.scaling_strategy.replace('__REPLACE__',
                                           scaler )
    scaler = scaler.replace('__EXPERT_LEVEL__',
                            '1' )

    fa = self.fa_estimation.replace('__EXPERT_LEVEL__',
                            '10' )
    output = self.output

    result = outer_level.replace('__REPLACE__',
                                 basic+data+scaler+fa+output)
    return result

  def default_2wmad(self):
    self.add_wavelength_info()

    outer_level = self.scaling_input
    outer_level = outer_level.replace( '__EXPERT_LEVEL__',
      self.default_expert_level_for_parameters_that_should_be_sensible_defaults)

    basic = self.basic_info
    data = self.data_type.replace( '__REPLACE__',
                                      'wavelength1' ) \
                                      + \
            self.data_type.replace( '__REPLACE__',
                                      'wavelength2' )

    data = self.xray_data_basic.replace('__REPLACE__',
                                           data )

    scaler = self.scale_protocol.replace('__REPLACE__',
                                         'ano_protocol' )
    scaler = scaler.replace('ls loc *ls_and_loc None',
                            '*loc None' )

    scaler = self.pre_scaler_protocol + scaler + \
             self.scale_protocol.replace('__REPLACE__','iso_protocol' )

    scaler = scaler.replace('__EXPERT_LEVEL__',
                            '1' )
    scaler = self.scaling_strategy.replace('__REPLACE__',
                                           scaler )
    scaler = scaler.replace('__EXPERT_LEVEL__',
                            '1' )
    fa = self.fa_estimation.replace('__EXPERT_LEVEL__',
                            '10' )
    output = self.output

    result = outer_level.replace('__REPLACE__',
                                 basic+data+scaler+fa+output)
    return result


  def default_3wmad(self):
    self.add_wavelength_info()

    outer_level = self.scaling_input
    outer_level = outer_level.replace( '__EXPERT_LEVEL__',
      self.default_expert_level_for_parameters_that_should_be_sensible_defaults)

    basic = self.basic_info
    data = self.data_type.replace( '__REPLACE__',
                                      'wavelength1' ) \
                                      + \
            self.data_type.replace( '__REPLACE__',
                                      'wavelength2' )    \
                                      + \
            self.data_type.replace( '__REPLACE__',
                                      'wavelength3' )

    data = self.xray_data_basic.replace('__REPLACE__',
                                           data )

    scaler = self.scale_protocol.replace('__REPLACE__',
                                         'ano_protocol' )
    scaler = scaler.replace('ls loc *ls_and_loc None',
                            '*loc None' )

    scaler = self.pre_scaler_protocol + scaler + \
             self.scale_protocol.replace('__REPLACE__','iso_protocol' )

    scaler = scaler.replace('__EXPERT_LEVEL__',
                            '1' )

    scaler = self.scaling_strategy.replace('__REPLACE__',
                                           scaler )
    scaler = scaler.replace('__EXPERT_LEVEL__',
                            '1' )

    fa = self.fa_estimation.replace('__EXPERT_LEVEL__',
                            '10' )
    output = self.output

    result = outer_level.replace('__REPLACE__',
                                 basic+data+scaler+fa+output)
    return result



def run(args):
  okai=True
  if len(args)==0:
    print("Example parameter files lego-ed together from several phil blocks")
    print()
    print("specifiy 'expert level' on command line via ")
    print("    python make_param.py <expert_level>      ")
    okai=False

  if okai:
    tester = phil_lego()
    print(" ---------- SAD ----------")
    master_params = iotbx.phil.parse( tester.default_sad() )
    master_params.show(expert_level = int(args[0]) )
    print(" ---------- SIR ----------")
    del master_params
    del tester
    tester = phil_lego()
    master_params = iotbx.phil.parse( tester.default_sir() )
    master_params.show(expert_level=int(args[0]))
    print(" ---------- SIRAS ----------")
    del master_params
    del tester
    tester = phil_lego()
    master_params = iotbx.phil.parse( tester.default_siras() )
    master_params.show(expert_level=int(args[0]))
    print(" ---------- 2WMAD ----------")
    del master_params
    del tester
    tester = phil_lego()
    master_params = iotbx.phil.parse( tester.default_2wmad() )
    master_params.show(expert_level=int(args[0]))
    print(" ---------- 3WMAD ----------")
    del master_params
    del tester
    tester = phil_lego()
    master_params = iotbx.phil.parse( tester.default_3wmad() )
    master_params.show(expert_level=int(args[0]))
    print(" ---------- 3WMAD ----------")
    del master_params
    del tester
    tester = phil_lego()
    master_params = iotbx.phil.parse( tester.default_3wmad() )
    master_params.show(expert_level=int(args[0]))

    print(" ---------- RIP ----------")
    del master_params
    del tester
    tester = phil_lego()
    master_params = iotbx.phil.parse( tester.default_rip() )
    master_params.show(expert_level=int(args[0]))




if (__name__ == "__main__"):
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/massage_twin_detwin_data.py

from __future__ import absolute_import, division, print_function
from mmtbx.scaling import outlier_rejection
from mmtbx.scaling import absolute_scaling
import mmtbx.scaling
import iotbx.phil
from cctbx.array_family import flex
from cctbx import miller
from cctbx import adptbx
from libtbx.utils import Sorry, null_out
from libtbx import Auto
import os.path
import sys

output_params_str = """
  hklout = None
    .type = path
  hklout_type=mtz sca *Auto
    .type = choice
  label_extension="massaged"
    .type = str
"""

master_params = iotbx.phil.parse("""
    aniso
      .help="Parameters dealing with anisotropy correction"
      .short_caption = Anisotropy correction
      .style = box auto_align
    {
      action=*remove_aniso None
        .type=choice
        .caption = Remove_anisotropy None
        .help="Remove anisotropy?"
        .style = bold
      final_b=*eigen_min eigen_mean user_b_iso
        .help="Final b value"
        .type=choice
        .caption = Minimum_eigenvalue Mean_eigenvalue User_specified
        .short_caption = Final B-factor source
      b_iso=None
        .type=float
        .help="User specified B value"
        .short_caption = User-specified B-factor
    }
    outlier
      .help="Outlier analyses"
      .short_caption = Outlier analyses
      .style = box auto_align
    {
      action=*extreme basic beamstop None
        .help="Outlier protocol"
        .type=choice
        .short_caption = Outlier rejection protocol
      parameters
        .help="Parameters for outlier detection"
      {
        basic_wilson{
          level = 1E-6
            .type=float
            .short_caption = Basic Wilson level
        }
        extreme_wilson{
          level = 0.01
            .type=float
            .short_caption = Extreme Wilson level
        }
        beamstop{
          level = 0.001
            .type=float
            .short_caption = Beamstop level
          d_min = 10.0
            .type=float
            .short_caption = Max. resolution
            .style = resolution
        }
      }
    }
    symmetry
      .short_caption = Detwinning
      .style = box auto_align
    {
      action = detwin twin *None
        .type = choice
        .short_caption = Action
        .style = bold
      twinning_parameters{
        twin_law = None
          .type = str
          .short_caption = Twin law
          .input_size = 120
          .style = bold
        fraction = None
          .type = float
          .short_caption = Detwinning fraction
          .style = bold
      }
    }
""")

class massage_data(object):
  def __init__(self,
               miller_array,
               parameters,
               out=None,
               n_residues=100,
               n_bases=0):

    self.params=parameters
    self.miller_array=miller_array.deep_copy().set_observation_type(
      miller_array).merge_equivalents().array()
    self.out = out
    if self.out is None:
      self.out = sys.stdout
    if self.out == "silent":
      self.out = null_out()


    self.no_aniso_array = self.miller_array
    if self.params.aniso.action == "remove_aniso":
      # first perfom aniso scaling
      aniso_scale_and_b = absolute_scaling.ml_aniso_absolute_scaling(
        miller_array = self.miller_array,
        n_residues = n_residues,
        n_bases = n_bases)
      aniso_scale_and_b.p_scale = 0 # set the p_scale back to 0!
      aniso_scale_and_b.show(out=out)
      # now do aniso correction please
      self.aniso_p_scale = aniso_scale_and_b.p_scale
      self.aniso_u_star  = aniso_scale_and_b.u_star
      self.aniso_b_cart  = aniso_scale_and_b.b_cart
      if self.params.aniso.final_b == "eigen_min":
        b_use=aniso_scale_and_b.eigen_values[2]
      elif self.params.aniso.final_b == "eigen_mean" :
        b_use=flex.mean(aniso_scale_and_b.eigen_values)
      elif self.params.aniso.final_b == "user_b_iso":
        assert self.params.aniso.b_iso is not None
        b_use=self.params.aniso.b_iso
      else:
        b_use = 30

      b_cart_aniso_removed = [ -b_use,
                               -b_use,
                               -b_use,
                               0,
                               0,
                               0]
      u_star_aniso_removed = adptbx.u_cart_as_u_star(
        miller_array.unit_cell(),
        adptbx.b_as_u( b_cart_aniso_removed  ) )
      ## I do things in two steps, but can easely be done in 1 step
      ## just for clarity, thats all.
      self.no_aniso_array = absolute_scaling.anisotropic_correction(
        self.miller_array,0.0,aniso_scale_and_b.u_star )
      self.no_aniso_array = absolute_scaling.anisotropic_correction(
        self.no_aniso_array,0.0,u_star_aniso_removed)
      self.no_aniso_array = self.no_aniso_array.set_observation_type(
        miller_array )

    # that is done now, now we can do outlier detection if desired
    outlier_manager = outlier_rejection.outlier_manager(
      self.no_aniso_array,
      None,
      out=self.out)


    self.new_miller_array = self.no_aniso_array
    if self.params.outlier.action == "basic":
      print("Non-outliers found by the basic wilson statistics", file=self.out)
      print("protocol will be written out.", file=self.out)
      basic_array = outlier_manager.basic_wilson_outliers(
        p_basic_wilson = self.params.outlier.parameters.basic_wilson.level,
        return_data = True)
      self.new_miller_array = basic_array

    if self.params.outlier.action == "extreme":
      print("Non-outliers found by the extreme value wilson statistics", file=self.out)
      print("protocol will be written out.", file=self.out)
      extreme_array = outlier_manager.extreme_wilson_outliers(
      p_extreme_wilson = self.params.outlier.parameters.extreme_wilson.level,
      return_data = True)
      self.new_miller_array = extreme_array

    if self.params.outlier.action == "beamstop":
      print("Outliers found for the beamstop shadow", file=self.out)
      print("problems detection protocol will be written out.", file=self.out)
      beamstop_array = outlier_manager.beamstop_shadow_outliers(
        level = self.params.outlier.parameters.beamstop.level,
        d_min = self.params.outlier.parameters.beamstop.d_min,
        return_data=True)
      self.new_miller_array = beamstop_array

    if self.params.outlier.action == "None":
      self.new_miller_array =  self.no_aniso_array



    # now we can twin or detwin the data if needed
    self.final_array = self.new_miller_array
    if self.params.symmetry.action == "twin":
      alpha = self.params.symmetry.twinning_parameters.fraction
      if (alpha is None):
        raise Sorry("Twin fraction not specified, not twinning data")
      elif not (0 <= alpha <= 0.5):
        raise Sorry("Twin fraction must be between 0 and 0.5.")
      print(file=self.out)
      print("Twinning given data", file=self.out)
      print("-------------------", file=self.out)
      print(file=self.out)
      print("Artifically twinning the data with fraction %3.2f" %\
        alpha, file=self.out)

      self.final_array = self.new_miller_array.twin_data(
        twin_law = self.params.symmetry.twinning_parameters.twin_law,
        alpha=alpha).as_intensity_array()

    elif (self.params.symmetry.action == "detwin"):
      twin_law = self.params.symmetry.twinning_parameters.twin_law
      alpha = self.params.symmetry.twinning_parameters.fraction
      if (alpha is None):
        raise Sorry("Twin fraction not specified, not detwinning data")
      elif not (0 <= alpha <= 0.5):
        raise Sorry("Twin fraction must be between 0 and 0.5.")
      print("""

Attempting to detwin data
-------------------------
Detwinning data with:
  - twin law:      %s
  - twin fraciton: %.2f

BE WARNED! DETWINNING OF DATA DOES NOT SOLVE YOUR TWINNING PROBLEM!
PREFERABLY, REFINEMENT SHOULD BE CARRIED OUT AGAINST ORIGINAL DATA
ONLY USING A TWIN SPECIFIC TARGET FUNCTION!

""" % (twin_law, alpha), file=self.out)
      self.final_array = self.new_miller_array.detwin_data(
        twin_law=twin_law,
        alpha=alpha).as_intensity_array()

    assert self.final_array is not None

  def return_data(self):
    return self.final_array

  def write_data(self,
      file_name,
      output_type=Auto,
      label_extension="massaged"):
    ## write out this miller array as sca if directed to do so:
    if (str(output_type) == "Auto"):
      base, ext = os.path.splitext(file_name)
      if ext in [".mtz",".sca"]:
        output_type = ext[1:]
      else:
        raise Sorry("Unknown or unsupported output type")
    assert (output_type in ["mtz", "sca"]), output_type
    if output_type == "sca":
      import iotbx.scalepack.merge
      iotbx.scalepack.merge.write(
        file_name=file_name,
        miller_array=self.final_array,
        scale_intensities_for_scalepack_merge=True) # scales only if necessary
    elif output_type == "mtz":
      base_label=None
      if self.final_array.is_xray_intensity_array():
        base_label = "I"
      if self.final_array.is_xray_amplitude_array():
        base_label = "F"
      mtz_dataset = self.final_array.as_mtz_dataset(
        column_root_label=base_label+label_extension)
      mtz_dataset.mtz_object().write(file_name)


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/matthews.py

## Peter Zwart Mai 10, 2005
## refactored by Gabor & Nat 20111104

from __future__ import absolute_import, division, print_function
from mmtbx import scaling
import math
import sys
from six.moves import range

log_p_solc = None
def get_log_p_solc():
  global log_p_solc
  from cctbx.array_family import flex
  from scitbx.math import chebyshev_polynome
  if (log_p_solc is None):
    coeffs = flex.double([
      -14.105436736742137,    -0.47015366358636385,
      -2.9151681976244639,   -0.49308859741473005,
      0.90132625209729045,   0.033529051311488103,
      0.088901407582105796,  0.10749856607909694,
      0.055000918494099861, -0.052424473641668454,
      -0.045698882840119227,  0.076048484096718036,
      -0.097645159906868589,  0.03904454313991608,
      -0.072186667173865071])
    log_p_solc = chebyshev_polynome( 15, 0, 1, coeffs)
  return log_p_solc

def p_solc_calc(sc):
  """
  Calculate solvent fraction probability
  """
  if sc < 0 or 1.0 < sc:
    raise ValueError("solvent content out of range")
  log_p_solc = get_log_p_solc()
  return math.exp(log_p_solc.f(sc))

class density_calculator(object):
  """
  Calculate Matthews coefficient and solvent fraction
  """
  def __init__(self, crystal):
    self.asu_volume = (
      crystal.unit_cell().volume() / crystal.space_group().order_z())

  def vm(self, weight):
    if weight <= 0:
      raise ValueError("Incorrect weight")
    return self.asu_volume / weight

  def macromolecule_fraction(self, weight, rho_spec):
    return rho_spec / ( 0.602 * self.vm( weight = weight ) )

  def solvent_fraction(self, weight, rho_spec):
    return 1.0 - self.macromolecule_fraction(
      weight = weight,
      rho_spec = rho_spec)

class component(object):
  """
  Macromolecule component
  """
  def __init__(self, mw, rho_spec):
    self.mw = mw
    self.rho_spec = rho_spec

  def protein(cls, nres):
    return cls( mw = nres * 112.5, rho_spec = 0.74 )

  def nucleic(cls, nres):
    # Kantardjieff & Rupp, Prot. Sci. 12, 1865-1871
    return cls( mw = nres * 311.0, rho_spec = 0.50 )

  protein = classmethod( protein )
  nucleic = classmethod( nucleic )

def number_table(components, density_calculator):
  if not components:
    raise ValueError("Empty component list")
  unit_mfrac = sum([
    density_calculator.macromolecule_fraction(
      weight = c.mw,
      rho_spec = c.rho_spec) for c in components ])
  results = []
  count = 1
  while True:
    sc = 1.0 - count * unit_mfrac
    assert sc <= 1.0
    if sc < 0:
      break
    results.append( ( count, p_solc_calc( sc = sc ) ) )
    count += 1
  return results

class p_vm_calculator(object):
  """solvent content/matthews probability calculator"""
  def __init__(self, crystal_symmetry, n_residues, n_bases=None, out=None,
      verbose=0):
    assert ((crystal_symmetry.unit_cell() is not None) and
            (crystal_symmetry.space_group() is not None))
    self.unit_cell_volume = crystal_symmetry.unit_cell().volume()
    self.z = crystal_symmetry.space_group().order_z()
    if  n_residues is not None:
      self.n_residues = n_residues
    else:
      self.n_residues = 0.0
    if n_bases is not None:
      self.n_bases = n_bases
    else:
      self.n_bases=0.0
    assert (self.n_bases+self.n_residues>0)
    self.mw_residue = 112.5
    self.mw_base = 311.0
    self.rho_spec = 0.74 # only for protein, not DNA/needs to be modified
    self.vm_prop = []
    if out is None:
      out=sys.stdout
    self.out = out
    self.verbose = verbose
    self. vm_prop_table()
    self.best_guess = self.guesstimate()

  def vm(self,copies):
    result = None
    den = ((self.n_residues*self.mw_residue+self.n_bases*self.mw_base)*\
      self.z*copies)
    if(den != 0.0):
      result = self.unit_cell_volume/den
    return(result)

  def solc(self,vm):
    return (1.0-self.rho_spec/(0.602*vm))

  def p_solc_calc(self, sc):
    return p_solc_calc( sc = max( min( sc, 1.0 ), 0 ) )

  def vm_prop_table(self):
    solc = 1.0
    n_copies = 0.0
    tot_p = 0.0
    while solc > 0:
      n_copies+=1.0
      vm = self.vm(n_copies)
      solc = self.solc(vm)
      p_vm = self.p_solc_calc(solc)
      entry = [ n_copies, solc, vm, p_vm ]
      tot_p += p_vm
      self.vm_prop.append(entry)
    tmp = self.vm_prop.pop() ## The last item has a negative solvent content
    tot_p -= tmp[3]
    if (int(n_copies)==1):
      print("Too many residues to fit in the ASU", file=self.out)
      print("  resetting number of residues in monomer to %5.0f" \
            %(self.n_residues/10.0), file=self.out)
      self.n_residues/=10.0
      self.n_bases/=10.0
      self.vm_prop_table()
    for ii in range(int(n_copies)-1):
      self.vm_prop[ii][3] = self.vm_prop[ii][3]/tot_p

  def guesstimate(self):
    max = 0.0
    guess = 1;
    for ii in range(len(self.vm_prop)):
      if (self.vm_prop[ii][3]>max):
        max = self.vm_prop[ii][3]
        guess = ii+1
    return guess

class matthews_rupp(scaling.xtriage_analysis):
  """Probabilistic estimation of number of copies in the asu"""
  def __init__(self,
      crystal_symmetry,
      n_residues=None,
      n_bases=None,
      out=None):
    from iotbx import data_plots
    self.n_residues_in = n_residues
    self.n_bases_in = n_bases
    best_guess_is_n_residues = False
    if (n_residues is None) and (n_bases is None):
      n_residues = 1
      n_bases = 0
      best_guess_is_n_residues = True
    vm_estimator = p_vm_calculator(crystal_symmetry, n_residues, n_bases, out=out)
    if (best_guess_is_n_residues):
      self.n_copies = 1
      self.n_residues = int(vm_estimator.best_guess)
      self.n_bases = 0
    else :
      self.n_copies = int(vm_estimator.best_guess)
      self.n_residues = vm_estimator.n_residues
      self.n_bases = vm_estimator.n_bases
    self.solvent_content = vm_estimator.vm_prop[vm_estimator.best_guess-1][1]
    self.table = data_plots.table_data(
      title="Solvent content analysis",
      column_labels=["Copies", "Solvent content", "Matthews coeff.",
                     "P(solvent content)"],
      column_formats=["%d","%.3f", "%.2f", "%.3f"],
      graph_names=["Solvent content", "Matthews coeff."],
      graph_columns=[[0,1], [0,2]])
    self.n_possible_contents = 0
    for ii in range( len(vm_estimator.vm_prop) ):
      self.n_possible_contents += 1
      row = []
      for jj in range(len(vm_estimator.vm_prop[ii])):
        if (jj == 0) : row.append(int(vm_estimator.vm_prop[ii][jj]))
        else : row.append(vm_estimator.vm_prop[ii][jj])
      self.table.add_row(row)

  def _show_impl(self, out):
    def show_warning():
      out.show_text("""
 Caution: this estimate is based on the distribution of solvent content across
 structures in the PDB, but it does not take into account the resolution of
 the data (which is strongly correlated with solvent content) or the physical
 properties of the model (such as oligomerization state, et cetera).  If you
 encounter problems with molecular replacement and/or refinement, you may need
 to consider the possibility that the ASU contents are different than expected.
""")
    out.show_header("Solvent content and Matthews coefficient")
    if (self.n_residues_in is None) and (self.n_bases_in is None):
      out.newline()
      out.show_text(" Number of residues unknown, assuming 50% solvent content")
      out.newline()
    else :
      clauses = []
      if (self.n_residues_in is not None) and (self.n_residues_in != 0):
        clauses.append("%d protein residues" % self.n_residues_in)
      if (self.n_bases_in is not None) and (self.n_bases_in != 0):
        clauses.append("%d nucleic acid bases" % self.n_bases_in)
      out.show_text(" Crystallized molecule(s) defined as %s" %
        " and ".join(clauses))
    if (self.n_residues_in is None) and (self.n_bases_in is None):
      out.show_text("  Best guess : %4d residues in the ASU" %
        self.n_residues)
      show_warning()
    else :
      out.show_table(self.table, equal_widths=False)
      if (self.n_copies == 1):
        out.show_text(" Best guess : 1 copy in the ASU")
      else :
        out.show_text(" Best guess : %4d copies in the ASU" % self.n_copies)
      if (self.n_possible_contents > 1):
        show_warning()

########################################################################
# REGRESSION
def exercise():
  from cctbx import crystal
  from libtbx.test_utils import approx_equal
  from six.moves import cStringIO as StringIO
  symm = crystal.symmetry(
    unit_cell=(77.3,107.6,84.4,90,94.2,90),
    space_group_symbol="P21")
  out = StringIO()
  result = matthews_rupp(
    crystal_symmetry=symm,
    n_residues=153)
  result.show(out)
  assert ("8 copies in the ASU" in out.getvalue())
  assert approx_equal(result.solvent_content, 0.5165, eps=0.0001)
  result = matthews_rupp(crystal_symmetry=symm)
  assert (result.n_residues == 1281)

if (__name__ == "__main__"):
  exercise()
  print("OK")


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/mean_f_rms_f.py
from __future__ import absolute_import, division, print_function
import sys,math

def mean_fh(d_min,b):
  if b <1.0: b=1.0
  pi=3.14159
  return 6.0 * pi**0.5 * d_min**3 * math.erf(b**0.5/(2.*d_min))/b**1.5 \
    - 6.0 * d_min**2 * math.exp(-1.0*b/(4.0*d_min**2))/b
def mean_fh_square(d_min,b):
  if b <1.0: b=1.0
  pi=3.14159
  return 3.0 * (pi/2.)**0.5 * d_min**3 * math.erf((b/2.)**0.5/d_min)/b**1.5 \
    - 3.0 * d_min**2 * math.exp(-1.0*b/(2.0*d_min**2))/b

def ratio_mean_f_to_rms_f(d_min,b):
  fh=mean_fh(d_min,b)
  fh2=mean_fh_square(d_min,b)
  return fh/fh2**0.5

def test(out=sys.stdout):
  b=90
  d_min=3.5
  ratio=ratio_mean_f_to_rms_f(d_min,b)
  text= "B: %7.2f   D_min: %7.2f  <f>/<f**2>**0.5: %7.3f" %( b,d_min,ratio)
  print(text, file=out)
  expected_text="B:   90.00   D_min:    3.50  <f>/<f**2>**0.5:   0.889"
  o=float(text.split()[-1])
  e=float(expected_text.split()[-1])
  if abs(o-e) > 0.02:
    raise AssertionError("test of mean_f_rms_f failed: Expected %7.3f got %7.3f: diff is %7.3f" %(
      o,e,abs(o-e)))

def exercise_mean_fh():
  i=-1
  for b in [0,30,60,90,200]:
    for d_min in [2.5,3,3.5,4,4.5,5,5.5,6,6.5,7,7.5,8]:
       i+=1
       ratio=ratio_mean_f_to_rms_f(d_min,b)
       print("B: %7.2f   D_min: %7.2f  <f>/<f**2>**0.5: %7.3f" %(
         b,d_min,ratio))

if __name__=="__main__":
  if 'exercise' in sys.argv:
    exercise_mean_fh()
    sys.exit(0)

  if 'tst' in sys.argv:
    test()
    sys.exit(0)

  sum_n=0.
  sum_f=0.
  sum_f2=0.
  d_min=2.5
  b=float(sys.argv[1].split("_")[2])
  fh=mean_fh(d_min,b)
  fh2=mean_fh_square(d_min,b)
  ratio=fh/math.sqrt(fh2)

  import math

  for line in open(sys.argv[1]).readlines():
    spl=line.lstrip().rstrip().split()
    if len(spl) != 5: continue
    f2=float(spl[3])
    f=math.sqrt(f2)
    sum_n+=1
    sum_f+=f
    sum_f2+=f2
  print("Mean f  %7.2f  rms f: %7.2f  Mean/rms: %7.2f  N: %d  Target: %7.2f" %(
     sum_f/sum_n, math.sqrt(sum_f2/sum_n),(sum_f/sum_n)/math.sqrt(sum_f2/sum_n),int(sum_n),ratio))


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/outlier_plots.py
from __future__ import absolute_import, division, print_function
from iotbx.option_parser import option_parser
from scitbx.array_family import flex
import sys
from libtbx.utils import Sorry
from mmtbx import scaling
import math
from iotbx import data_plots
from six.moves import range


def plotit(fobs,
           sigma,
           fcalc,
           alpha,
           beta,
           epsilon,
           centric,
           out,
           limit=5.0,
           steps=1000,
           plot_title="Outlier plot"):

  fobs_a    = flex.double( [fobs] )
  fcalc_a   = flex.double( [fcalc] )
  epsilon_a = flex.double( [epsilon] )
  alpha_a   = flex.double( [alpha] )
  beta_a    = flex.double( [beta] )
  centric_a = flex.bool  ( [centric] )

  p_calc = scaling.likelihood_ratio_outlier_test(
    fobs_a,
    None,
    fcalc_a,
    epsilon_a,
    centric_a,
    alpha_a,
    beta_a)
  print(file=out)
  print("#Input parameters: ", file=out)
  print("#Title        : ", plot_title, file=out)
  print("#F-calc       : ", fcalc, file=out)
  print("#F-obs        : ", fobs, file=out)
  print("#epsilon      : ", epsilon, file=out)
  print("#alpha        : ", alpha, file=out)
  print("#beta         : ", beta, file=out)
  print("#centric      : ", centric, file=out)
  mode = p_calc.posterior_mode()[0]

  snd_der = math.sqrt(1.0/ math.fabs( p_calc.posterior_mode_snd_der()[0] )  )
  print("#A Gaussian approximation of the likelihood function", file=out)
  print("#could be constructed as follows with: ", file=out)
  print("# exp[-(fobs-mode)**2/(2*stdev**2)] /(sqrt(2 pi) stdev)", file=out)
  print("#with", file=out)
  print("#mode         = ", mode, file=out)
  print("#stdev        = ", snd_der, file=out)
  print(file=out)
  print("#The log likelihood values for the mode and ", file=out)
  print("#observed values are", file=out)
  print("#Log[P(fobs)] : ",  p_calc.log_likelihood()[0], file=out)
  print("#Log[P(mode)] : ",  p_calc.posterior_mode_log_likelihood()[0], file=out)
  print("#Their difference is:", file=out)
  print("#delta        : ",  p_calc.log_likelihood()[0]-p_calc.posterior_mode_log_likelihood()[0], file=out)
  print("#", file=out)
  mean_fobs = p_calc.mean_fobs()
  print("#mean f_obs   : ", mean_fobs[0], "   (first moment)", file=out)


  low_limit = mode-snd_der*limit
  if low_limit<0:
    low_limit=0
  high_limit = mode+limit*snd_der

  if fobs < low_limit:
    low_limit = fobs-2.0*snd_der
    if low_limit<0:
      low_limit=0
  if fobs > high_limit:
    high_limit = fobs+2.0*snd_der

  fobs_a = flex.double( range(steps) )*(
    high_limit-low_limit)/float(steps)+low_limit

  fcalc_a   = flex.double( [fcalc]*steps )
  epsilon_a = flex.double( [epsilon]*steps )
  alpha_a   = flex.double( [alpha]*steps )
  beta_a    = flex.double( [beta]*steps )
  centric_a = flex.bool  ( [centric]*steps )

  p_calc = scaling.likelihood_ratio_outlier_test(
    fobs_a,
    None,
    fcalc_a,
    epsilon_a,
    centric_a,
    alpha_a,
    beta_a)

  ll = p_calc.log_likelihood()    #-p_calc.posterior_mode_log_likelihood()
  ll = flex.exp( ll )
  if (sigma is None) or (sigma <=0 ):
    sigma=fobs/30.0

  obs_gauss = (fobs_a - fobs)/float(sigma)
  obs_gauss = flex.exp( -obs_gauss*obs_gauss/2.0 ) /(
    math.sqrt(2.0*math.pi*sigma*sigma))

  max_ll = flex.max( ll )*1.10
  truncate_mask = flex.bool( obs_gauss >= max_ll )
  obs_gauss = obs_gauss.set_selected( truncate_mask, max_ll )


  ccp4_loggraph_plot = data_plots.plot_data(
    plot_title=plot_title,
    x_label = 'Fobs',
    y_label = 'P(Fobs)',
    x_data = fobs_a,
    y_data = ll,
    y_legend = 'P(Fobs|Fcalc,alpha,beta)',
    comments = 'Fobs=%5.2f, sigma=%5.2f, Fcalc=%5.2f'%(fobs,sigma,fcalc) )
  ccp4_loggraph_plot.add_data(
    y_data = obs_gauss,
    y_legend = "P(Fobs|<Fobs>,sigma)"
    )
  data_plots.plot_data_loggraph(ccp4_loggraph_plot,out)

def run(args):
  command_line = (option_parser(
    usage="mmtbx.p-plotter [options]",
    description="produces a gnuplot plot")
                  .option(None, "--fobs",
                          action="store",
                          type="float",
                          help="F obs",
                          metavar="FLOAT")
                  .option( None, "--sigma",
                          action="store",
                          type="float",
                          help="sigma Fobs",
                          metavar="FLOAT")
                  .option(None, "--fcalc",
                          action="store",
                          type="float",
                          help="F calc",
                          metavar="FLOAT")
                  .option(None, "--alpha",
                          action="store",
                          type="float",
                          help="alpha",
                          metavar="FLOAT")
                  .option(None, "--beta",
                          action="store",
                          type="float",
                          help="beta")
                  .option(None, "--epsilon",
                          action="store",
                          type="float",
                          help="epsilon")
                  .option(None, "--centric",
                          action="store_true",
                          default=False,
                          help="centricity flag")
                  .option(None, "--limit",
                          action="store",
                          type="float",
                          default=10,
                          help="plotting limit")
                  .option(None, "--steps",
                          action="store",
                          type="int",
                          default=1000,
                          help="number of steps")

                  ).process(args=args)

  if command_line.options.fobs is None:
    raise Sorry("please provide fobs")
  if command_line.options.fcalc is None:
    raise Sorry("please provide fcalc")
  if command_line.options.epsilon is None:
    raise Sorry("please provide epsilon")
  if command_line.options.alpha is None:
    raise Sorry("please provide alpha")
  if command_line.options.beta is None:
    raise Sorry("please provide beta")

  #print dir(command_line.options)
  plottery =  plotit( command_line.options.fobs,
                       command_line.options.sigma,
                       command_line.options.fcalc,
                       command_line.options.alpha,
                       command_line.options.beta,
                       command_line.options.epsilon,
                       command_line.options.centric,
                       sys.stdout,
                       command_line.options.limit,
                       command_line.options.steps)



if (__name__=="__main__"):
  run(sys.argv[0:])


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/outlier_rejection.py
from __future__ import absolute_import, division, print_function
from cctbx import miller
from mmtbx import scaling
from mmtbx.scaling import sigmaa_estimation
from cctbx.array_family import flex
from libtbx.utils import Sorry
import mmtbx.scaling
from mmtbx.scaling import absolute_scaling, outlier_plots
from scitbx.math import erf
from libtbx import table_utils
from libtbx.utils import null_out
import sys
import math
from libtbx.str_utils import StringIO
from six.moves import zip

class outlier_manager(object):
  def __init__(self,
               miller_obs,
               r_free_flags,
               out=None):
    self.out=out
    if self.out is None:
      self.out=sys.stdout
    if out == "silent":
      self.out = null_out()

    # the original miller array
    self.miller_obs = miller_obs

    if self.miller_obs.observation_type() is None:
      raise Sorry("Unknown observation type")

    # we make a working copy of the above miller array
    self.work_obs = self.miller_obs.deep_copy().set_observation_type(
      self.miller_obs )

    if not self.work_obs.is_xray_intensity_array():
      self.work_obs = self.work_obs.f_as_f_sq()

    if not self.miller_obs.is_xray_amplitude_array():
      self.miller_obs = self.miller_obs.f_sq_as_f()

    self.r_free_flags = r_free_flags

    #-----------------------
    # These calculations are needed for wilson based outlier rejection
    #
    # Normalize the data
    normalizer = absolute_scaling.kernel_normalisation(
      self.work_obs, auto_kernel=True )
    self.norma_work = self.work_obs.customized_copy(
      data=normalizer.normalised_miller.data()/
           normalizer.normalised_miller.epsilons().data().as_double() )
    assert ( flex.min(self.norma_work.data()) >= 0 )
    # split things into centric and acentric sets please
    self.centric_work = self.norma_work.select_centric().set_observation_type(
      self.norma_work)
    self.acentric_work = self.norma_work.select_acentric().set_observation_type(
      self.norma_work)

  def basic_wilson_outliers(self,
                            p_basic_wilson=1E-6,
                            return_data=False):
    p_acentric_single = 1.0 - (1.0 - flex.exp(-self.acentric_work.data() ))
    p_centric_single = 1.0 - erf(flex.sqrt(self.centric_work.data()/2.0) )

    acentric_selection = flex.bool( p_acentric_single > p_basic_wilson )
    centric_selection = flex.bool( p_centric_single > p_basic_wilson )

    # combine all in a single miller array
    all_flags = self.work_obs.customized_copy(
      indices = self.acentric_work.indices().concatenate(
                self.centric_work.indices() ),
      data = acentric_selection.concatenate( centric_selection )
      )
    all_p_values = self.work_obs.customized_copy(
      indices = self.acentric_work.indices().concatenate(
                self.centric_work.indices() ),
      data = p_acentric_single.concatenate( p_centric_single )
      )

    # get the order right
    all_flags = all_flags.common_set(self.miller_obs)
    all_p_values = all_p_values.common_set(self.miller_obs)

    # prepare a table with results please
    log_string = """
Outlier rejection based on basic Wilson statistics.
--------------------------------------------------

See Read, Acta Cryst. (1999). D55, 1759-1764. for details.
Reflections whose normalized intensity have an associated p-value
lower than %s are flagged as possible outliers.
    """%(p_basic_wilson)


    log_string = self.make_log_wilson(log_string, all_flags ,all_p_values )
    print(file=self.out)
    print(log_string, file=self.out)
    print(file=self.out)

    if not return_data:
      return all_flags
    else:
      return self.miller_obs.select( all_flags.data() )


  def extreme_wilson_outliers(self,
                              p_extreme_wilson=1e-1,
                              return_data=False):

    n_acentric = self.acentric_work.data().size()
    n_centric = self.centric_work.data().size()

    extreme_acentric = 1.0 -  \
       flex.pow(1.0 - flex.exp(-self.acentric_work.data() ),float(n_acentric))
    extreme_centric = 1.0 - \
       flex.pow(erf(flex.sqrt(self.centric_work.data()/2.0) ),float(n_centric))

    acentric_selection = flex.bool(extreme_acentric > p_extreme_wilson)
    centric_selection  = flex.bool(extreme_centric > p_extreme_wilson)
    all_flags = self.work_obs.customized_copy(
      indices = self.acentric_work.indices().concatenate(
                self.centric_work.indices() ),
      data    = acentric_selection.concatenate( centric_selection )
    )
    all_p_values = self.work_obs.customized_copy(
      indices = self.acentric_work.indices().concatenate(
                self.centric_work.indices() ),
      data = extreme_acentric.concatenate( extreme_centric )
      )
    all_flags = all_flags.common_set(self.miller_obs)
    all_p_values = all_p_values.common_set(self.miller_obs)


    log_string = """
Outlier rejection based on extreme value Wilson statistics.
-----------------------------------------------------------

Reflections whose normalized intensity have an associated p-value
lower than %s are flagged as possible outliers.
The p-value is obtained using extreme value distributions of the
Wilson distribution.
    """%(p_extreme_wilson)

    log_string = self.make_log_wilson(log_string, all_flags ,all_p_values )

    print(file=self.out)
    print(log_string, file=self.out)
    print(file=self.out)

    if not return_data:
      return all_flags
    else:
      return self.miller_obs.select( all_flags.data() )

  def beamstop_shadow_outliers(self,
                               level=0.01,
                               d_min=10.0,
                               return_data=False):

    # just make sure that things don't get to weerd
    assert level <= 0.3
    z_lim_ac = -math.log(1.0-level)
    z_select_ac = flex.bool( self.norma_work.data() <
                             z_lim_ac )
    # a first order approximation of the NZ of centric is
    # sqrt(2/pi)sqrt(z)
    z_lim_c = level*level*math.pi/2.0
    z_select_c = flex.bool( self.norma_work.data() <
                             z_lim_c )
    centric = self.norma_work.centric_flags().data()
    d_select = flex.bool( self.norma_work.d_spacings().data() >
                          d_min )

    # final selection: all conditions must be full filled
    # acentrics: centric:FALSE
    #            z_select_ac:TRUE
    #            d_select:TRUE
    #  centrics: centric:TRUE
    #            z_select_c:TRUE
    #            d_select:TRUE
    #
    #  final = acentric is True OR centric is True
    #  THIS SHOULD BE DONE WITH OPERATIONS ON FLEX ARRAYS
    #  AM GETTING VERY CONFUSED HOWEVER!
    #  THIS DIDN'T WORK :
    #  a_part = ~(~z_select_ac or ~d_select)
    #  a_part = ~( ~a_part or centric)
    #  c_part = ~(~z_select_c  or ~d_select)
    #  c_part = ~( ~c_part or ~centric)
    #
    #  final_selection = ~(a_part or c_part)
    tmp_final = []
    for zac, zc, ds, cf in zip(z_select_ac,
                               z_select_c,
                               d_select,
                               centric):
      if ds:
        if cf:
          if zc:
            tmp_final.append( False )
          else:
            tmp_final.append( True )
        if not cf:
          if zac:
            tmp_final.append( False )
          else:
            tmp_final.append( True )
      else:
        tmp_final.append( True )
    tmp_final = flex.bool( tmp_final )
    final_selection = self.norma_work.customized_copy(
      data = tmp_final
    )
    log_message = """
Possible outliers due to beamstop shadow
----------------------------------------

Reflection with normalized intensities lower than %4.3e (acentric)
or %4.3e (centric) and an associated d-spacing lower than %3.1f
are considered potential outliers.
The rationale is that these reflection could potentially be in
the shadow of the beamstop.
     """%(z_lim_ac, z_lim_c, d_min)

    final_selection = final_selection.map_to_asu()
    self.miller_obs = self.miller_obs.map_to_asu()
    final_selection = final_selection.common_set( self.miller_obs )
    assert final_selection.indices().all_eq( self.miller_obs.indices() )
    data = self.miller_obs.select( final_selection.data()
                                   ).set_observation_type( self.miller_obs )

    log_message = self.make_log_beam_stop( log_message,final_selection )
    print(log_message, file=self.out)


    if not return_data:
      return final_selection
    else:
      return( data )


  def model_based_outliers(self,
                           f_model,
                           level=.01,
                           return_data=False,
                           plot_out=None):

    assert  self.r_free_flags is not None
    if(self.r_free_flags.data().count(True)==0):
      self.r_free_flags = self.r_free_flags.array(
        data = ~self.r_free_flags.data())
    sigmaa_estimator = sigmaa_estimation.sigmaa_estimator(
      miller_obs   = self.miller_obs,
      miller_calc  = f_model,
      r_free_flags = self.r_free_flags,
      kernel_width_free_reflections = 200,
      n_sampling_points = 20,
      n_chebyshev_terms = 13 )

    sigmaa_estimator.show(out=self.out)
    sigmaa = sigmaa_estimator.sigmaa()
    obs_norm = abs(sigmaa_estimator.normalized_obs)
    calc_norm = sigmaa_estimator.normalized_calc

    f_model_outlier_object = scaling.likelihood_ratio_outlier_test(
      f_obs=obs_norm.data(),
      sigma_obs=None,
      f_calc=calc_norm.data(),
      # the data is prenormalized, all epsies are unity
      epsilon=flex.double(calc_norm.data().size(), 1.0),
      centric=obs_norm.centric_flags().data(),
      alpha=sigmaa.data(),
      beta=1.0-sigmaa.data()*sigmaa.data()
      )
    modes = f_model_outlier_object.posterior_mode()
    lik = f_model_outlier_object.log_likelihood()
    p_lik = f_model_outlier_object.posterior_mode_log_likelihood()
    s_der = f_model_outlier_object.posterior_mode_snd_der()

    ll_gain = f_model_outlier_object.standardized_likelihood()

    # The smallest vallue should be 0.
    # sometimes, due to numerical issues, it comes out
    # a wee bit negative. please repair that
    eps=1.0e-10
    zeros = flex.bool( ll_gain < eps )
    p_values = ll_gain
    p_values = p_values.set_selected( zeros, eps )
    p_values = erf( flex.sqrt(p_values/2.0) )
    p_values = 1.0 - flex.pow( p_values, float(p_values.size()) )

    # select on p-values
    flags    = flex.bool(p_values > level )
    flags    = self.miller_obs.customized_copy( data = flags )
    ll_gain  = self.miller_obs.customized_copy( data = ll_gain )
    p_values = self.miller_obs.customized_copy( data = p_values )

    log_message = """

Model based outlier rejection.
------------------------------

Calculated amplitudes and estimated values of alpha and beta
are used to compute the log-likelihood of the observed amplitude.
The method is inspired by Read, Acta Cryst. (1999). D55, 1759-1764.
Outliers are rejected on the basis of the assumption that a scaled
log likelihood differnce 2(log[P(Fobs)]-log[P(Fmode)])/Q\" is distributed
according to a Chi-square distribution (Q\" is equal to the second
derivative of the log likelihood function of the mode of the
distribution).
The outlier threshold of the p-value relates to the p-value of the
extreme value distribution of the chi-square distribution.

"""

    flags.map_to_asu()
    ll_gain.map_to_asu()
    p_values.map_to_asu()

    assert flags.indices().all_eq( self.miller_obs.indices() )
    assert ll_gain.indices().all_eq( self.miller_obs.indices() )
    assert p_values.indices().all_eq( self.miller_obs.indices() )

    log_message = self.make_log_model( log_message,
                                       flags,
                                       ll_gain,
                                       p_values,
                                       obs_norm,
                                       calc_norm,
                                       sigmaa,
                                       plot_out)
    tmp_log=StringIO()
    print(log_message, file=tmp_log)
    # histogram of log likelihood gain values
    print(file=tmp_log)
    print("The histoghram of scaled (LL-gain) values is shown below.", file=tmp_log)
    print("  Note: scaled (LL-gain) is approximately Chi-square distributed.", file=tmp_log)
    print(file=tmp_log)
    print("  scaled(LL-gain)  Frequency", file=tmp_log)
    histo = flex.histogram( ll_gain.data(), 15 )
    histo.show(f=tmp_log,format_cutoffs='%7.3f')

    print(tmp_log.getvalue(), file=self.out)

    if not return_data:
      return flags
    else:
      assert flags.indices().all_eq(  self.miller_obs.indices() )
      return self.miller_obs.select( flags.data() )

  def apply_scale_to_original_data(self,scale_factor,d_min=None):
    self.miller_obs = self.miller_obs.customized_copy(
      data = self.miller_obs.data()*scale_factor
      ).set_observation_type( self.miller_obs )
    if d_min is not None:
      self.miller_obs = self.miller_obs.resolution_filter(d_min = d_min)

  def make_log_wilson(self, log_message, flags, p_values):
    """ produces a 'nice' table of outliers and their reason for
    being an outlier using basic or extreme wilson statistics """

    header = ("Index", "E_obs", "Centric", "p-value" )
    flags = flags.common_set( self.norma_work)
    p_vals = p_values.common_set( self.norma_work )

    rogues = self.norma_work.select( ~flags.data() )
    p_vals = p_vals.select( ~flags.data() )

    rows = []
    table = "No outliers were found."

    for hkl,e,c,p in zip(rogues.indices(),
                         rogues.data(),
                         rogues.centric_flags().data(),
                         p_vals.data() ):
      if e > 0:
        this_row = [str(hkl), "%5.3f"%(math.sqrt(e)), str(c), "%5.3e"%(p) ]
      else:
        this_row = [str(hkl), "%5.3f"%(0), str(c), " inf" ]
      rows.append( this_row)
    if len(rows)>0:
      table = table_utils.format( [header]+rows,
                                  comments=None,
                                  has_header=True,
                                  separate_rows=False,
                                  prefix='| ',
                                  postfix=' |')
    final = log_message +"\n" + table
    return final

  def make_log_beam_stop(self,
                         log_message, flags):
    self.norma_work = self.norma_work.map_to_asu()
    self.miller_obs = self.miller_obs.map_to_asu()
    flags = flags.map_to_asu()




    data = self.miller_obs.select( ~flags.data() )
    evals = self.norma_work.select( ~flags.data() )



    header = ("Index", "d-spacing", "F_obs","E-value", "centric")
    table = "No outliers were found"
    rows = []
    if data.data().size() > 0:
      if data.data().size() < 500 :
        for hkl, d, fobs, e, c in zip(data.indices(),
                                      data.d_spacings().data(),
                                      data.data(),
                                      evals.data(),
                                      data.centric_flags().data() ):
          this_row = [str(hkl),
                      "%4.2f"%(d),
                      "%6.1f"%(fobs),
                      "%4.2f"%( math.sqrt(e) ),
                      str(c) ]
          rows.append( this_row )

        table = table_utils.format( [header]+rows,
                                    comments=None,
                                    has_header=True,
                                    separate_rows=False,
                                    prefix='| ',
                                    postfix=' |')
      else:
        table = """Over 500 outliers have been found."""


    final = log_message + "\n" + table +"\n \n"
    return final


  def make_log_model(self,
                     log_message,
                     flags,
                     ll_gain,
                     p_values,
                     e_obs,
                     e_calc,
                     sigmaa,
                     plot_out=None):
    header = ("Index", "d-spacing", "E_obs", "E_model", "Score", "p-value", "sigmaa", "centric")
    table="No outliers were found"
    rows = []
    rogues   = e_obs.select( ~flags.data() )
    p_array  = p_values.select( ~flags.data() )
    ll_array = ll_gain.select( ~flags.data() )
    ec_array = e_calc.select( ~flags.data() )
    sa_array = sigmaa.select( ~flags.data() )




    centric_flags = self.miller_obs.centric_flags().select( ~flags.data() )
    if rogues.indices().size() > 0:
      if rogues.indices().size() < 500 :
        sigmas = rogues.sigmas()
        if rogues.sigmas()==None:
          sigmas = rogues.d_spacings().data()*0+10.0

        for hkl, d, eo, ec, llg, p, sa, c, s, e in zip(
          rogues.indices(),
          rogues.d_spacings().data(),
          rogues.data(),
          ec_array.data(),
          ll_array.data(),
          p_array.data(),
          sa_array.data(),
          centric_flags.data(),
          sigmas,
          rogues.epsilons().data().as_double() ):

          this_row = [str(hkl),
                      "%4.2f"%(d),
                      "%6.3f"%(eo),
                      "%6.3f"%(ec),
                      "%5.2f"%(llg),
                      "%5.3e"%(p),
                      "%4.3f"%(sa),
                      str(c) ]
          rows.append( this_row )

          if plot_out is not None:
            outlier_plots.plotit(
              fobs=eo,
              sigma=s,
              fcalc=ec,
              alpha=sa,
              beta=1.0-sa*sa,
              epsilon=e,
              centric=c,
              out=plot_out,
              plot_title=str(hkl) )



        table = table_utils.format( [header]+rows,
                                    comments=None,
                                    has_header=True,
                                    separate_rows=False,
                                    prefix='| ',
                                    postfix=' |')

      else:
        table = "More then 500 outliers were found. This is very suspicious. Check data or limits."

    final = log_message + "\n" + table
    return final


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/pair_analyses.py
from __future__ import absolute_import, division, print_function
from cctbx import miller
from cctbx import crystal
from libtbx import table_utils
import cctbx.sgtbx.cosets
from cctbx.crystal import reindex
from cctbx.array_family import flex
from libtbx.utils import Sorry
import mmtbx.scaling
from scitbx.python_utils import robust_statistics
import sys, math
import scitbx.lbfgs
from six.moves import zip
from six.moves import range



class reindexing(object):
  __doc__=""" Reindexing matrices """
  def __init__(self,
               set_a,
               set_b,
               out=None,
               relative_length_tolerance=0.05,
               absolute_angle_tolerance=3.0,
               lattice_symmetry_max_delta=3.0,
               file_name=None):

    self.relative_length_tolerance=relative_length_tolerance
    self.absolute_angle_tolerance=absolute_angle_tolerance
    self.lattice_symmetry_max_delta=lattice_symmetry_max_delta
    self.out = out
    if self.out is None:
      self.out = sys.stdout
    self.file_name=file_name

    ## make deep copy for safety, and make it non-anomalous
    self.set_b_ori = set_b.deep_copy().set_observation_type( set_b )
    self.set_a = set_a.deep_copy().set_observation_type( set_a ).average_bijvoet_mates()
    self.set_b = set_b.deep_copy().set_observation_type( set_b ).average_bijvoet_mates()

    self.set_a_xs = crystal.symmetry( unit_cell=self.set_a.unit_cell(),
                                      space_group=self.set_a.space_group() )
    self.set_b_xs = crystal.symmetry( unit_cell=self.set_b.unit_cell(),
                                      space_group=self.set_b.space_group() )

    self.cosets = reindex.reindexing_operators(
      xs1=self.set_a_xs,
      xs2=self.set_b_xs,
      relative_length_tolerance=self.relative_length_tolerance,
      absolute_angle_tolerance=self.absolute_angle_tolerance,
      max_delta=self.lattice_symmetry_max_delta,
      anomalous_flag=self.set_a.anomalous_flag(),
      out=self.out
      )

    self.set_a_to_niggli = self.cosets.cb_op_to_n_1
    self.set_b_to_niggli = self.cosets.cb_op_to_n_2

    self.set_a_nig = self.set_a.change_basis( self.set_a_to_niggli  ).map_to_asu()
    self.set_b_nig = self.set_b.change_basis( self.set_b_to_niggli  ).map_to_asu()

    self.ops_in_niggli_setting = self.cosets.cb_ops_in_niggli_setting()
    self.nice_cb_ops = self.cosets.combined_cb_ops()

    self.cc_values = []
    self.matches = []
    if len(self.nice_cb_ops)>0:
      self.analyse()
      self.select_and_transform()
    else:
      print("No simple reindexing relation found between unit cells", file=self.out)

  def analyse(self):
    table_data=[]
    for (cb_op, comb_cb_op) in zip(self.ops_in_niggli_setting,
                                   self.nice_cb_ops):
      tmp_set_b_nig = self.set_b_nig.change_basis( cb_op ).map_to_asu()
      tmp_set_a_nig, tmp_set_b_nig = self.set_a_nig.common_sets(
        tmp_set_b_nig, assert_is_similar_symmetry=False )
      tmp_cc = tmp_set_a_nig.correlation(tmp_set_b_nig,
                                         assert_is_similar_symmetry=False )
      self.cc_values.append(  tmp_cc.coefficient()  )
      self.matches.append( 100.0*float(   tmp_set_a_nig.indices().size()  )/
                           float(  self.set_a_nig.indices().size()  )   )



  def select_and_transform(self,
                           matches_cut_off=0.75
                           ):
    ## hopsa
    max_cc=-1.0
    location = 0
    table_data=[]
    for ii in range(len(self.nice_cb_ops)):
      table_data.append(
        [self.nice_cb_ops[ii].as_hkl(),
         "%4.3f"%(self.cc_values[ii]),
         "%4.1f"%(self.matches[ii]),
         '   ']
        )

      if self.matches[ii]>=matches_cut_off:
        if max_cc<self.cc_values[ii]:
          max_cc = self.cc_values[ii]
          location = ii

    legend = ('Operator', 'Correlation', 'matches (%)', 'choice')
    table_data[location][3]=' <--- '
    self.table = table_utils.format([legend]+table_data,
                                       comments=None,
                                       has_header=True,
                                       separate_rows=False,
                                       prefix='| ',
                                       postfix=' |')
    print(file=self.out)
    print("Reference analyses", file=self.out)
    print("   The following reindexing operators have been found:", file=self.out)
    print(file=self.out)
    print(self.table, file=self.out)
    print(file=self.out)
    if str(self.nice_cb_ops[location].as_hkl()) == "h,k,l":
      print("The data doesn't need to be reindexed! Indexing is consistent between these datasets.", file=self.out)
    else:
      print("If the data is reindexed with operator (%s), the correlation of"%( self.nice_cb_ops[location].as_hkl() ), file=self.out)
      print("the intensities to the reference data is maximized. ", file=self.out)
      if self.file_name is not None:
        print(self.file_name)
        print("This can be done for instance with:", file=self.out)
        print("  phenix.reflection_file_converter %s --change_of_basis=\"%s\" <output_options> "%(self.file_name, self.nice_cb_ops[location].as_hkl() ), file=self.out)
    print("-------------------------------------------------------------------------------", file=self.out)
    ##  change things in primitive setting

    transform_b = self.set_b_ori.change_basis( self.set_b_to_niggli ).change_basis(
      self.ops_in_niggli_setting[location]  ).change_basis(
      self.set_b_to_niggli.inverse() ).map_to_asu().set_observation_type(
      self.set_b_ori)
    return ( transform_b )



class delta_generator(object):
  def __init__(self,
               nat,
               der,
               nsr_bias=1.0):
    self.nat=nat.deep_copy()
    self.der=der.deep_copy()
    self.nsr_bias=1.0/nsr_bias

    assert self.nat.is_real_array()
    assert self.nat.is_real_array()

    if self.nat.is_xray_intensity_array():
      self.nat.f_sq_as_f()
    if self.der.is_xray_intensity_array():
      self.der.f_sq_as_f()

    self.nat,self.der = self.nat.common_sets(self.der)

    self.der = self.der.customized_copy(
      data = self.der.data()*self.nsr_bias,
      sigmas = self.der.sigmas()*self.nsr_bias).set_observation_type(
        self.der)

    self.delta_f=self.nat.customized_copy(
      data = ( self.der.data() - self.nat.data() ),
      sigmas = flex.sqrt( self.der.sigmas()*self.der.sigmas()+
                          self.nat.sigmas()*self.nat.sigmas() )
      ).set_observation_type( self.nat )

    self.abs_delta_f=self.nat.customized_copy(
      data = flex.abs( self.der.data() - self.nat.data() ),
      sigmas = flex.sqrt( self.der.sigmas()*self.der.sigmas()+
                          self.nat.sigmas()*self.nat.sigmas() )
      ).set_observation_type( self.der )

    if not self.nat.is_xray_intensity_array():
      self.nat.f_as_f_sq()
    if not self.der.is_xray_intensity_array():
      self.der.f_as_f_sq()

    self.delta_i=self.nat.customized_copy(
      data = ( self.der.data() - self.nat.data() ),
      sigmas = flex.sqrt( self.der.sigmas()*self.der.sigmas()+
                          self.nat.sigmas()*self.nat.sigmas() )
      ).set_observation_type( self.nat )

    self.abs_delta_i=self.nat.customized_copy(
      data = flex.abs( self.der.data() - self.nat.data() ),
      sigmas = flex.sqrt( self.der.sigmas()*self.der.sigmas()+
                          self.nat.sigmas()*self.nat.sigmas() )
      ).set_observation_type( self.der )



class outlier_rejection(object):
  def __init__(self,
               nat,
               der,
               cut_level_rms=3,
               cut_level_sigma=0,
               method={'solve':False,'rms':False, 'rms_and_sigma':True},
               out=None
               ):
    self.out=out
    self.method = method
    if self.out == None:
      self.out = sys.stdout

    self.cut_level_rms=cut_level_rms
    self.cut_level_sigma=cut_level_sigma
    ## Just make sure that we have the common sets
    self.nat = nat.deep_copy()
    self.der = der.deep_copy()

    self.nat, self.der = self.nat.common_sets(self.der)
    #Make sure we have amplitudes
    assert self.nat.is_real_array()
    assert self.nat.is_real_array()

    if self.nat.is_xray_intensity_array():
      self.nat=self.nat.f_sq_as_f()
    if self.der.is_xray_intensity_array():
      self.der=self.der.f_sq_as_f()

    ## Construct delta f's
    delta_gen = delta_generator( self.nat, self.der )
    self.delta_f = delta_gen.abs_delta_f
    ## Set up a binner please
    self.delta_f.setup_binner_d_star_sq_step(auto_binning=True)
    ## for each bin, I would like to compute
    ## mean dF**2
    ## mean sigma**2
    self.mean_df2 = self.delta_f.mean_of_intensity_divided_by_epsilon(
      use_binning=True,
      return_fail=1e12)
    self.mean_sdf2 = self.delta_f.mean_of_squared_sigma_divided_by_epsilon(
      use_binning=True,
      return_fail=1e12)

    self.result = flex.bool(  self.delta_f.indices().size(), True )

    self.detect_outliers()
    self.remove_outliers()

  def detect_outliers(self):
    count_true = 0
    if (self.method['solve']):
      count_true+=1
    if (self.method['rms']):
      count_true+=1
    if (self.method['rms_and_sigma']):
      count_true+=1


    if not count_true==1:
      raise Sorry("Outlier removal protocol not specified properly")

    if (self.method['solve']):
      self.detect_outliers_solve()
    if (self.method['rms']):
      self.detect_outliers_rms()
    if (self.method['rms_and_sigma']):
      self.detect_outliers_sigma()




  def detect_outliers_solve(self):
    """
    TT says:
    I toss everything > 3 sigma in the scaling,
    where sigma comes from the rms of everything being scaled:

    sigma**2 = <delta**2>- <experimental-sigmas**2>

    Then if a particular
    delta**2 > 3 sigma**2 + experimental-sigmas**2
    then I toss it.
    """
    terwilliger_sigma_array = flex.double(self.mean_df2.data) -\
                              flex.double(self.mean_sdf2.data)

    for bin_number in self.delta_f.binner().range_all():
      ## The selection tells us wether or not somthing is in the correct bin
      selection =  self.delta_f.binner().selection( bin_number ).iselection()
      ## Now just make a global check to test for outlierness:
      tmp_sigma_array =  terwilliger_sigma_array[bin_number] -\
                         self.delta_f.sigmas()*self.delta_f.sigmas()
      tmp_sigma_array = flex.sqrt(tmp_sigma_array)*self.cut_level_rms

      potential_outliers = ( self.delta_f.data()  >  tmp_sigma_array )
      potential_outliers =  potential_outliers.select( selection )

      self.result = self.result.set_selected( selection, potential_outliers )

    print(file=self.out)
    print(" %8i potential outliers detected" %(
      self.result.count(True) ), file=self.out)
    print(" They will be removed from the data set", file=self.out)
    print(file=self.out)


  def detect_outliers_rms(self):
    for bin_number in self.delta_f.binner().range_all():
      selection =  self.delta_f.binner().selection( bin_number ).iselection()
      potential_outliers = (
        self.delta_f.data()  >  self.cut_level_rms*math.sqrt(
        self.mean_df2.data[bin_number])  )
      potential_outliers =  potential_outliers.select( selection )
      self.result = self.result.set_selected( selection, potential_outliers )

    print(file=self.out)
    print(" %8i potential outliers detected" %(
      self.result.count(True) ), file=self.out)
    print(" They will be removed from the data set", file=self.out)
    print(file=self.out)


  def detect_outliers_sigma(self):
    ## Locate outliers in native
    potential_outlier_nat = (self.nat.data()/self.nat.sigmas()
                               < self.cut_level_sigma)
    nat_select = potential_outlier_nat.iselection()

    ## Locate outliers in derivative
    potential_outlier_der = (self.der.data()/self.der.sigmas()
                               <self.cut_level_sigma)
    der_select = potential_outlier_der.iselection()

    for bin_number in self.delta_f.binner().range_all():
      ## RMS outlier removal
      selection =  self.delta_f.binner().selection( bin_number ).iselection()
      potential_outliers = (
        self.delta_f.data()  >  self.cut_level_rms*math.sqrt(
        self.mean_df2.data[bin_number])  )
      potential_outliers =  potential_outliers.select( selection )
      self.result = self.result.set_selected( selection, potential_outliers )

    self.result = self.result.set_selected( nat_select, True )
    self.result = self.result.set_selected( der_select, True )

    print(file=self.out)
    print(" %8i potential outliers detected" %(
      self.result.count(True) ), file=self.out)
    print(" They will be removed from the data set", file=self.out)
    print(file=self.out)


  def remove_outliers(self):
    potential_outliers = self.nat.select( self.result )

    matches = miller.match_indices( self.nat.indices(),
                                    potential_outliers.indices()  )

    self.nat = self.nat.select( matches.single_selection(0) )

    self.nat, self.der = self.nat.common_sets(self.der)


class f_double_prime_ratio(object):
  def __init__(self,
               lambda1,
               lambda2):
    ## make sure we have anomalous data
    assert ( lambda1.anomalous_flag() )
    assert ( lambda2.anomalous_flag() )
    ## make temporary copies
    tmp_l1 = lambda1.deep_copy()
    tmp_l2 = lambda2.deep_copy()
    ##make sure we have intensities
    assert tmp_l1.is_real_array()
    if tmp_l1.is_xray_amplitude_array():
      tmp_l1=tmp_l1.f_as_f_sq()

    assert tmp_l2.is_real_array()
    if tmp_l2.is_xray_amplitude_array():
      tmp_l2=tmp_l2.f_as_f_sq()

    ## we only need the anomalous diffs
    l1p, l1n = tmp_l1.hemispheres_acentrics()
    self.diff1 = l1p.data()-l1n.data()
    self.v1 = ( l1p.sigmas()*l1p.sigmas() +
                l1n.sigmas()*l1n.sigmas() )

    l2p, l2n = tmp_l2.hemispheres_acentrics()
    self.diff2 = l2p.data()-l2n.data()
    self.v2 = ( l2p.sigmas()*l2p.sigmas() +
                l2n.sigmas()*l2n.sigmas() )

    self.x=flex.double([1.0])
    scitbx.lbfgs.run(target_evaluator=self)
    self.ratio=self.x[0]

  def compute_functional_and_gradients(self):
    f = self.compute_functional()
    g = self.compute_gradient()
    ##g2 =  self.compute_gradient_fd()
    return f,g

  def compute_functional(self):
    top = self.diff1 - self.diff2*self.x[0]
    top= top*top
    bottom = self.v1+self.v2*self.x[0]*self.x[0]
    result = top/bottom
    result=flex.sum(result)
    return result

  def compute_gradient(self):
    tmp_bottom = self.v1+self.v2*self.x[0]*self.x[0]
    tmp_top = self.diff1 - self.diff2*self.x[0]
    part1 = -2.0*self.x[0]*tmp_top*tmp_top*self.v2/( tmp_bottom*tmp_bottom )
    part2 = -2.0*self.diff2*tmp_top/tmp_bottom
    result=flex.sum( part1+part2)
    return(flex.double([result]))

  def compute_gradient_fd(self):
    h=0.0000001
    current = self.compute_functional()
    self.x[0]+=h
    new = self.compute_functional()
    self.x[0]-=h
    result = current - new
    result/=-h
    return flex.double([result])

  def show(self,out=None):
    if out is None:
      out = sys.stdout
    print(file=out)
    print("Estimated ratio of fdp(w1)/fwp(w2): %3.2f"%(self.x[0]), file=out)
    print(file=out)



class delta_f_prime_f_double_prime_ratio(object):
  def __init__(self,
               lambda1,
               lambda2,
               level=1.0):
    self.tmp_l1 = lambda1.deep_copy()
    self.tmp_l2 = lambda2.deep_copy()

    ## make sure we have amplitudes please
    if self.tmp_l1.is_xray_intensity_array():
      self.tmp_l1 = self.tmp_l1.f_sq_as_f()
    if self.tmp_l2.is_xray_intensity_array():
      self.tmp_l2 = self.tmp_l2.f_sq_as_f()

    ## Now this is done, swe have to set up a binner
    self.tmp_l1, self.tmp_l2 = self.tmp_l1.common_sets( self.tmp_l2 )
    self.tmp_l1.setup_binner_d_star_sq_step( auto_binning=True )
    self.tmp_l2.use_binner_of( self.tmp_l1 )

    self.tmp_l1_no_ano = self.tmp_l1.average_bijvoet_mates()
    self.tmp_l2_no_ano = self.tmp_l2.average_bijvoet_mates()
    self.tmp_l1_no_ano.setup_binner_d_star_sq_step( auto_binning=True )
    self.tmp_l2_no_ano.setup_binner_d_star_sq_step( auto_binning=True )

    self.plus2, self.minus2 = self.tmp_l2.hemispheres_acentrics()
    self.plus2.use_binning_of( self.tmp_l1_no_ano )
    self.minus2.use_binning_of( self.tmp_l1_no_ano )

    ## we assume that the data is properly scaled of course
    ## Loop over all bins, and in each bin,
    ## compute <diso>, <df> and their ratio
    estimates = flex.double()
    count = 0
    for bin in self.tmp_l1.binner().range_all():
      selection =  self.tmp_l1_no_ano.binner().selection( bin ).iselection()
      tmp1 = self.tmp_l1_no_ano.select( selection ).data()
      tmp2 = self.tmp_l2_no_ano.select( selection ).data()
      stmp1 = self.tmp_l1_no_ano.select( selection ).sigmas()
      stmp2 = self.tmp_l2_no_ano.select( selection ).sigmas()

      selection = self.plus2.binner().selection( bin ).iselection()
      tmp3 = self.plus2.select( selection ).data()
      tmp4 = self.minus2.select( selection ).data()
      stmp3 = self.plus2.select( selection ).sigmas()
      stmp4 = self.minus2.select( selection ).sigmas()

      tmpiso=None
      tmpsiso=None
      tmpano=None
      tmpsano=None

      if tmp1.size() > 0:
        tmpiso = flex.mean( (tmp1 - tmp2)*(tmp1 - tmp2) )
        tmpsiso = flex.mean( stmp1*stmp1 +  stmp2*stmp2  )
      if tmp3.size() > 0:
        tmpano = flex.mean( (tmp3 - tmp4)*(tmp3 - tmp4) )
        tmpsano = flex.mean( stmp3*stmp3 +  stmp4*stmp4  )

      if tmp1.size() > 0: ## make sure something is there
        if tmp3.size() > 0: ## make sure something is there
          if math.sqrt(tmpiso/tmpsiso)>=level:  ## s/n is okai
            if math.sqrt(tmpano/tmpsano)>=level: ## s/n is okai
              delta_iso_sq = math.sqrt( tmpiso  )
              delta_ano_sq = math.sqrt( tmpano  )
              tmp=2.0*delta_iso_sq/delta_ano_sq
              print(bin, tmp)
              estimates.append( tmp )
              count+=1.0

    ## compute the trimean please
    self.ratio = None
    self.number = count
    if (self.number>0):
      self.ratio=robust_statistics.trimean( estimates )



class singh_ramasheshan_fa_estimate(object):
  def __init__(self,
               w1,
               w2,
               k1,
               k2):
    self.w1=w1.deep_copy()
    self.w2=w2.deep_copy()

    if self.w1.is_xray_amplitude_array():
      self.w1 = self.w1.f_as_f_sq()
    if self.w2.is_xray_amplitude_array():
      self.w2 = self.w2.f_as_f_sq()

    ## common sets please
    self.w1,self.w2 = self.w1.common_sets( self.w2 )

    ## get differences and sums please
    self.p1, self.n1 = self.w1.hemispheres_acentrics()
    self.p2, self.n2 = self.w2.hemispheres_acentrics()

    self.diff1 = self.p1.data() - self.n1.data()
    self.diff2 = self.p2.data() - self.n2.data()

    self.s1 =   self.p1.sigmas()*self.p1.sigmas()\
              + self.n1.sigmas()*self.n1.sigmas()
    self.s1 =  flex.sqrt( self.s1 )

    self.s2 =   self.p2.sigmas()*self.p2.sigmas()\
              + self.n2.sigmas()*self.n2.sigmas()
    self.s2 =  flex.sqrt( self.s2 )

    self.sum1 = self.p1.data() + self.n1.data()
    self.sum2 = self.p2.data() + self.n2.data()

    self.k1_sq = k1*k1
    self.k2_sq = k2*k2

    self.determinant=None
    self.fa=None
    self.sigfa=None


    self.selector=None
    self.iselector=None

    self.a=None
    self.b=None
    self.c=None

    self.compute_fa_values()

    self.fa = self.p1.customized_copy(
      data = self.fa,
      sigmas = self.sigfa).set_observation_type( self.p1 )

  def set_sigma_ratio(self):
    tmp = self.s2/flex.abs( self.diff2 +1e-6 ) +\
          self.s1/flex.abs( self.diff1 +1e-6 )
    tmp = tmp/2.0
    self.sigfa = tmp

  def compute_coefs(self):
    self.a = ( self.k2_sq*self.k2_sq
               + self.k2_sq*(1 + self.k1_sq)
               + (self.k1_sq-1)*(self.k1_sq-1)
             )
    self.b = -self.k2_sq*(self.sum1 + self.sum2) \
             -(self.k1_sq-1)*(self.sum1 - self.sum2)
    self.c = 0.25*(self.sum1 - self.sum2)*(self.sum1 - self.sum2)\
            +(1.0/8.0)*self.k2_sq*(  self.diff2*self.diff2
                                   + self.diff1*self.diff1/self.k1_sq)

  def compute_determinant(self):
    self.determinant = self.b*self.b - 4.0*self.a*self.c
    self.selector = (self.determinant>0)
    self.iselector = self.selector.iselection()

  def compute_fa_values(self):
    self.compute_coefs()
    self.compute_determinant()
    reset_selector = (~self.selector).iselection()
    self.determinant = self.determinant.set_selected(  reset_selector, 0 )

    choice1 = -self.b + flex.sqrt( self.determinant )
    choice1 /= 2*self.a
    choice2 = -self.b - flex.sqrt( self.determinant )
    choice2 /= 2*self.a

    select1 = choice1 > choice2
    select2 = ~select1

    choice1 = choice1.set_selected( select1.iselection(), 0 )
    choice2 = choice2.set_selected( select2.iselection(), 0 )

    choice1 = choice1+choice2
    select1 = (choice1<0).iselection()
    choice1 = choice1.set_selected( select1 , 0 )
    self.fa =  choice1 + choice2

    self.set_sigma_ratio()

    self.sigfa = self.sigfa*self.fa


class mum_dad(object):
  def __init__(self,
               lambda1,
               lambda2,
               k1=1.0):
    ## assumed is of course that the data are scaled.
    ## lambda1 is the 'reference'
    self.w1=lambda1.deep_copy()
    self.w2=lambda2.deep_copy()

    if not self.w1.is_xray_amplitude_array():
      self.w1 = self.w1.f_sq_as_f()
    if not self.w2.is_xray_amplitude_array():
      self.w2 = self.w2.f_sq_as_f()

    self.w1, self.w2 = self.w1.common_sets( self.w2 )

    l1p, l1n = self.w1.hemispheres_acentrics()
    self.mean1 = l1p.data()+l1n.data()
    self.diff1 = l1p.data()-l1n.data()
    self.v1 = ( l1p.sigmas()*l1p.sigmas() +
                l1n.sigmas()*l1n.sigmas() )

    l2p, l2n = self.w2.hemispheres_acentrics()
    self.mean2 = l2p.data()+l2n.data()
    self.diff2 = l2p.data()-l2n.data()
    self.v2 = ( l2p.sigmas()*l2p.sigmas() +
                l2n.sigmas()*l2n.sigmas() )

    self.new_diff = flex.abs( (self.diff1 + k1*self.diff2)/2.0 )
    self.new_sigma_mean = flex.sqrt( (self.v1+k1*k1*self.v2)/2.0 )

    self.dad = l1p.customized_copy(
      data = self.new_diff,
      sigmas = self.new_sigma_mean ).set_observation_type( self.w1 )


 *******************************************************************************
