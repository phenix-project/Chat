

 *******************************************************************************
xfel/command_line/cxi_mpi_submit.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.mpi_submit
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.mpi_submit
#
# Submit a cctbx.xfel processing job to the cluster using bsub, after first creating
# an appropiate trial directory in the requested location and copying and modifying
# the given psana config files and cctbx.xfel phil files
#
import os, sys, shutil
from libtbx.utils import Sorry
from libtbx.phil import parse
from libtbx import easy_run
from xfel.util.mp import mp_phil_str, get_submit_command_chooser

help_str = """
Use cxi.mpi_submit to submit a cctbx.xfel job to a cluster for analyzing
diffraction data.  cxi.mpi_submit will create a directory for
you in the location specified by output.output_dir using the run and trial
numbers specified.  If no trial number is specified, one will be chosen auto-
matically by examining the output directory.  The output directory will be
created if it doesn't exist.

Examples (LCLS, for non-LCLS see below):

cxi.mpi_submit input.cfg=cxi49812/thermo.cfg input.experiment=cxi49812 \\
  input.run_num=25 output.output_dir=\\
  /reg/d/psdm/cxi/cxi49812/results/username/results mp.nproc=100 mp.queue=psanaq

This will submit run 25 of experiment cxi49812 to the psana queue for
processing using the modules specified by cxi49812/thermo.cfg.

cxi.mpi_submit submit.phil

Use the phil parameters in submit.phil to control job submission.

Before the job is submitted, the following occurs:

Directory /reg/d/psdm/cxi/cxi49812/results/username/results is created if it
doesn't exist.

Directory /reg/d/psdm/cxi/cxi49812/results/username/results/r0025 is created if it
doesn't exist.

The run directory is searched for the next available trial ID, say 003.  That
directory is created.

Under /reg/d/psdm/cxi/cxi49812/results/username/results/r0025/003, the following is
created:

Directory stdout.  The log for the experiment will go here.
File psana_orig.cfg.  This is a verbatim copy of the input config file.
File psana.cfg.  This is a copy of the input config file, re-written to respect
the new paths for this trial.  Any parameters with _dirname are re-written
to include this trial directory and the associated directories are created. Any
phil files found are copied to the trial directory, renamed, and updated.  This
includes phil files included by copied phil files.

The final bsub command is saved in submit.sh for future use.

For processing non-LCLS data, a different dispatcher and any non-default queueing
and multiprocessing parameters should be explicitly requested. For example,
mp.env_script should be specified on any system where the interactive environment
is not propagated to the queued jobs.

Example for processing XFEL stills at SACLA:

cxi.mpi_submit /path/to/h5/runs/directory/266708-0/run*.h5 \\
  input.dispatcher=dials.stills_process output.output_dir=/path/to/results \\
  input.target=/path/to//stills_process.phil input.trial=0 \\
  mp.method=pbs mp.queue=psmall mp.nnodes=10 mp.nproc_per_node=28 \\
  mp.env_script=/path/to/setpaths.sh
"""


phil_str = '''
  dry_run = False
    .type = bool
    .help = If True, the program will create the trial directory but not submit the job, \
            and will show the command that would have been executed.
  input {
    experiment = None
      .type = str
      .help = Experiment identifier, e.g. cxi84914 (mandatory for xtc stream processing)
    run_num = None
      .type = str
      .help = Run number to process. Can be a string instead of a number.
    trial = None
      .type = int
      .help = Trial number for this job.  Leave blank to auto determine.
    rungroup = None
      .type = int
      .help = Optional. If used, will add _rgXXX to the trial path. Useful for organizing \
              runs with similar parameters into logical groupings.
    dispatcher = cctbx.xfel.xtc_process
      .type = str
      .help = Which program to run. cxi.xtc_process is for module only based processing, \
              such as mod_hitfind and LABELIT. cctbx.xfel.xtc_process uses the DIALS back \
              end but converts the raw data to CBF before processing it. Use \
              cctbx.xfel.process to use the XTC streams natively without CBF.
    target = None
      .type = path
      .help = Optional path to phil file with additional parameters to be run by the \
              processing program.
    locator = None
      .type = str
      .help = Locator file needed for cctbx.xfel.process and dials.stills_process to find \
              the XTC streams
    task = None
      .type = int
      .help = Optional task number, meaning the dispatcher is working on previously \
              processed data from this run. In this case, the trial folder is not \
              created and task is used as a folder within the trial folder to save \
              the output.
  }
  output {
    output_dir = "."
      .type = str
      .help = Directory for output files
    add_output_dir_option = True
      .type = bool
      .help = If True, include output.output_dir on the command line.
    split_logs = True
      .type = bool
      .help = Option to split error and log files into separate per process
  }
'''

phil_scope = parse(phil_str + mp_phil_str, process_includes=True)
mp_phil_scope = parse(mp_phil_str, process_includes=True)

def copy_config(config, dest_dir, root_name, params, target_num):
  """ Copy a config file to a directory, and all of its referenced phil files. Recursively
  copies the files and changes the includes statements to match the new file names.
  @param config config file to copy
  @param dest_dir Full path to directory to copy file to
  @param root_name Name to give to copied file. Included phil files will be given
  this name + _N where N is incremented for each file included in the phil file
  @param params phil parameters
  @param target_num for target phil files found in config, latest number found
  """
   # make a copy of the original cfg file
  shutil.copy(config, os.path.join(dest_dir, "%s_orig.cfg"%root_name))

  config_path = os.path.join(dest_dir, "%s.cfg"%root_name)

  # Re-write the config file, changing paths to be relative to the trial directory.
  # Also copy and re-write included phil files, while updating the config file to
  # the new paths
  # Note, some legacy rewrites done by cxi.lsf are not included here.
  f = open(config_path, 'w')
  for line in open(config).readlines():
    if "[pyana]" in line:
      raise Sorry("Pyana not supported. Check your config file.")
    if "RUN_NO" in line:
      line = line.replace("RUN_NO", str(params.input.run_num))
    if "RUN_STR" in line:
      line = line.replace("RUN_STR", "r%04d"%(params.input.run_num))
    if "trial_id" in line:
      key, val = line.split("=")
      line = "%s= %d\n"%(key,params.input.trial)
    if "rungroup_id" in line:
      key, val = line.split("=")
      line = "%s= %s\n"%(key,params.input.rungroup) # None ok
    elif "_dirname" in line:
      key, val = line.split("=")
      val = os.path.join(dest_dir, os.path.basename(val.strip()))
      if not os.path.exists(val):
        os.mkdir(val)
      line = "%s= %s\n"%(key,val)
    elif "xtal_target" in line:
      key, val = line.split("=")
      val = val.strip()
      if not os.path.exists(val):
        raise Sorry("One of the xtal_target files in the cfg file doesn't exist: %s"%val)
      new_target = "params_%d"%target_num
      copy_target(val, dest_dir, new_target)
      target_num += 1
      line = "%s= %s.phil\n"%(key,os.path.join(dest_dir, new_target))
    f.write(line)
  f.close()
  return target_num

def copy_target(target, dest_dir, root_name):
  """ Copy a phil file to a directory, and all of its included phil files. Recursively
  copies the files and changes the includes statements to match the new file names.
  @param target Phil file to copy
  @param dest_dir Full path to directory to copy file to
  @param root_name Name to give to copied file. Included phil files will be given
  this name + _N where N is incremented for each file included in the phil file
  """
  # Each included phil file will be named root_name_N.phil where N is this number,
  # incremented for each included phil file
  num_sub_targets = 1
  f = open(os.path.join(dest_dir, root_name + ".phil"), 'w')
  for line in open(target).readlines():
    if "include" in line:
      inc_str, include_type, sub_target = line.strip().split() # Example: include file cxi-8.2.phil
      if include_type != 'file':
        raise Sorry("Include isn't a file") # FIXME look up what other values are possible here
      sub_target_root_name = "%s_%d"%(root_name, num_sub_targets)
      line = " ".join([inc_str, include_type, sub_target_root_name  + ".phil\n"])
      # recursive call to check for other included files
      copy_target(os.path.join(os.path.dirname(target), sub_target), dest_dir, sub_target_root_name)
      num_sub_targets += 1
    f.write(line)

def get_trialdir(output_dir, run_num, trial = None, rungroup = None, task = None):
    try:
      rundir = os.path.join(output_dir, "r%04d"%int(run_num))
    except ValueError:
      rundir = os.path.join(output_dir, run_num)

    if not os.path.exists(output_dir):
       os.makedirs(output_dir)

    if not os.path.exists(rundir):
      os.mkdir(rundir)

    # If a trial number wasn't included, find the next available, up to 999 trials
    if trial is None:
      found_one = False
      for i in range(1000):
        trialdir = os.path.join(rundir, "%03d"%i)
        if rungroup is not None:
          trialdir += "_rg%03d"%rungroup
        if not os.path.exists(trialdir):
          found_one = True
          break
      if found_one:
        trial = i
      else:
        raise Sorry("All trial numbers in use")
    else:
      trialdir = os.path.join(rundir, "%03d"%trial)
      if rungroup is not None:
        trialdir += "_rg%03d"%rungroup
      if os.path.exists(trialdir) and task is None:
        raise Sorry("Trial %d already in use"%trial)

    if not os.path.exists(trialdir):
      os.mkdir(trialdir)

    if task is not None:
      trialdir = os.path.join(trialdir, "task%03d"%task)
      if os.path.exists(trialdir):
        raise Sorry("Task %d already exists"%task)
      os.mkdir(trialdir)
    return trial, trialdir

def get_submission_id(result, method):
  if method == "mpi" or method == "lsf":
    submission_id = None
    for line in result.stdout_lines:
      # example for lsf: 'Job <XXXXXX> is submitted to queue <YYYYYYY>.'
      if len(line.split()) < 2: continue
      s = line.split()[1].lstrip('<').rstrip('>')
      try:
        s = int(s)
      except ValueError:
        pass
      else:
        submission_id = str(s)
    print(submission_id)
    return submission_id
  elif method == 'pbs':
    submission_id = "".join(result.stdout_lines).strip()
    print(submission_id)
    return submission_id
  elif method == 'slurm' or method == "shifter":
    # Assuming that all shifter instances are running on NERSC (slurm) systems
    return result.stdout_lines[0].split()[-1].strip()
  elif method == 'htcondor':
    return result.stdout_lines[-1].split()[-1].rstrip('.')
  elif method == 'sge':
    #  example for sge at Diamond: 'Your job JOB_ID ("JOB_NAME") has been submitted'
    line = "".join(result.stdout_lines).strip()
    s = line.split()
    submission_id = ''.join(i for i in s if i.isdigit())   # or just simply use s[2], the command used will extract the number from the "line"
    print (line)
    print('Submission', 'ID', 'is', submission_id)
    return submission_id

def do_submit(command, submit_path, stdoutdir, mp_params, log_name="log.out", err_name="log.err", job_name=None, dry_run=False):
  submit_command = get_submit_command_chooser(command,
                                              submit_path,
                                              stdoutdir,
                                              mp_params,
                                              log_name=log_name,
                                              err_name=err_name,
                                              job_name=job_name,
                                              )
  if mp_params.method in ['lsf', 'sge', 'pbs']:
    parts = submit_command.split(" ")
    script = open(parts.pop(-1), "rb")
    run_command = script.read().split(b"\n")[-2]
    command = " ".join(parts + [run_command.decode()])
  else:
    command = submit_command
  submit_command = str(submit_command) # unicode workaround

  if dry_run:
    print("Dry run: job not submitted. Trial directory created here:", os.path.dirname(submit_path))
    print("Execute this command to submit the job:")
    print(submit_command)
  elif mp_params.method == 'local':
    submission_id = os.fork()
    if submission_id > 0:
      return submission_id
    else:
      stdout = os.open(os.path.join(stdoutdir, 'submit.log'), os.O_WRONLY|os.O_CREAT|os.O_TRUNC); os.dup2(stdout, 1)
      stderr = os.open(os.path.join(stdoutdir, 'submit.err'), os.O_WRONLY|os.O_CREAT|os.O_TRUNC); os.dup2(stderr, 2)
      os.execv(command.split()[0], command.split())
  else:
    try:
      result = easy_run.fully_buffered(command=submit_command)
      result.raise_if_errors()
    except Exception as e:
      if not "Warning: job being submitted without an AFS token." in str(e):
        raise e

    return get_submission_id(result, mp_params.method)

class Script(object):
  """ Script to submit XFEL data for processing"""
  def __init__(self):
    pass

  def run(self, argv = None):
    """ Set up run folder and submit the job. """
    if argv is None:
      argv = sys.argv[1:]

    if len(argv) == 0 or "-h" in argv or "--help" in argv or "-c" in argv:
      print(help_str)
      print("Showing phil parameters:")
      print(phil_scope.as_str(attributes_level = 2))
      return

    user_phil = []
    dispatcher_args = []
    for arg in argv:
      if arg.endswith(".h5"):
        dispatcher_args.append(arg)
      elif (os.path.isfile(arg)):
        try:
          user_phil.append(parse(file_name=arg))
        except Exception as e:
          if os.path.splitext(arg)[1] == ".phil": raise e
          dispatcher_args.append(arg)
      else:
        try:
          user_phil.append(parse(arg))
        except RuntimeError as e:
          dispatcher_args.append(arg)
    scope, unused = phil_scope.fetch(sources=user_phil, track_unused_definitions=True)
    params = scope.extract()
    dispatcher_args.extend(["%s=%s"%(u.path,u.object.words[0].value) for u in unused])

    assert params.input.run_num is not None
    if params.input.dispatcher in ["cxi.xtc_process", "cctbx.xfel.xtc_process"]:
      # processing XTC streams at LCLS -- dispatcher will locate raw data
      assert params.input.experiment is not None or params.input.locator is not None
      print("Submitting run %d of experiment %s"%(int(params.input.run_num), params.input.experiment))
    else:
      print("Submitting run %s"%(params.input.run_num))
    trial, trialdir = get_trialdir(params.output.output_dir, params.input.run_num, params.input.trial, params.input.rungroup, params.input.task)
    params.input.trial = trial
    print("Using trial", params.input.trial)

    # log file will live here
    stdoutdir = os.path.join(trialdir, "stdout")
    os.mkdir(stdoutdir)
    logging_str = ""
    if params.output.split_logs:# test parameter for split_log then open and close log file and loop over nprocs
      if params.mp.method=='shifter' and params.mp.shifter.staging=='DataWarp':
        bbdirstr = "${DW_JOB_STRIPED}/stdout"
        logging_str = "output.logging_dir=%s"%bbdirstr
      else:
        for i in range(params.mp.nproc):
          error_files = os.path.join(stdoutdir,"error_rank%04d.out"%i)
          log_files = os.path.join(stdoutdir,"log_rank%04d.out"%i)
          open(log_files,'a').close()
          open(error_files,'a').close()
        logging_str = "output.logging_dir=%s"%stdoutdir
    else:
      logging_str = ""

    # Copy any config or phil files specified
    target_num = 1
    has_config = False
    redone_args = []
    for arg in dispatcher_args:
      if not len(arg.split('=')) == 2:
        redone_args.append(arg)
        continue
      name, value = arg.split('=')

      if "cfg" in name and os.path.splitext(value)[1].lower() == ".cfg":
        cfg = value
        if not os.path.exists(cfg):
          raise Sorry("Config file doesn't exist: %s"%cfg)
        if has_config:
          raise Sorry("Multiple config files found")
        has_config = True
        target_num = copy_config(cfg, trialdir, "psana", params, target_num)
        redone_args.append("%s=%s"%(name, os.path.join(trialdir, "psana.cfg")))
      elif "target" in name or os.path.splitext(value)[1].lower() == ".phil":
        phil = value
        if not os.path.exists(phil):
          raise Sorry("Phil file doesn't exist: %s"%phil)
        copy_target(phil, trialdir, "params_%d"%target_num)
        redone_args.append("%s=%s"%(name, os.path.join(trialdir, "params_%d.phil"%target_num)))
        target_num += 1
      else:
        redone_args.append(arg)
    dispatcher_args = redone_args

    # If additional phil params are provided, copy them over too
    if params.input.target is not None:
      if not os.path.exists(params.input.target):
        raise Sorry("Target file doesn't exist: %s"%params.input.target)
      copy_target(params.input.target, trialdir, "params_%d"%target_num)
      params.input.target = os.path.join(trialdir, "params_%d.phil"%target_num)
      target_num += 1

    # Some configs files will specify out_dirname. If not, we want to explicitly create one
    # so the dispatcher will have an output directory.
    output_dir = os.path.join(trialdir, "out")
    if not os.path.exists(output_dir):
      os.makedirs(output_dir)

    # Write out a script for submitting this job and submit it
    submit_path = os.path.join(trialdir, "submit.sh")

    extra_str = ""
    data_str = ""

    assert [params.input.locator, params.input.experiment].count(None) != 0
    if params.input.locator is not None:
      locator_file = os.path.join(trialdir, "data.loc")
      shutil.copyfile(params.input.locator, locator_file)
      data_str += locator_file

    from xfel.ui import known_dials_dispatchers
    if params.input.dispatcher in known_dials_dispatchers:
      import importlib
      dispatcher_params = importlib.import_module(known_dials_dispatchers[params.input.dispatcher]).phil_scope.extract()
    else:
      dispatcher_params = None

    if params.input.experiment is None:
      if hasattr(dispatcher_params, 'input') and hasattr(dispatcher_params.input, 'trial'):
        assert hasattr(dispatcher_params.input, 'run_num')
        data_str += " input.trial=%s input.run_num=%s" % ( # pass along for logging
          params.input.trial, params.input.run_num)
    else:
      data_str += " input.trial=%s input.experiment=%s input.run_num=%s" % (
        params.input.trial, params.input.experiment, params.input.run_num)

    for arg in dispatcher_args:
      extra_str += " %s" % arg
    if params.input.target is not None:
      extra_str += " %s" % params.input.target

    if hasattr(dispatcher_params, 'input') and hasattr(dispatcher_params.input, 'rungroup') and params.input.rungroup is not None:
      data_str += " input.rungroup=%d" % params.input.rungroup

    command = f"{params.input.dispatcher} {data_str} {logging_str} {extra_str}"
    if params.output.add_output_dir_option:
      command += f" output.output_dir={output_dir}"

    job_name = "r%s"%params.input.run_num

    submission_id = do_submit(command, submit_path, stdoutdir, params.mp, log_name="log.out", err_name="err.out", job_name=job_name, dry_run=params.dry_run)
    print("Job submitted.  Output in", trialdir)
    return submission_id

if __name__ == "__main__":
  script = Script()
  script.run()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_optical2cbfheader.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.optical2cbfheader
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1
# $Id
#

import sys, os
import libtbx.phil
from libtbx.utils import Usage, Sorry
from xfel.cftbx.detector.cspad_cbf_tbx import read_optical_metrology_from_flat_file
from xfel.cftbx.detector.cspad_cbf_tbx import asic_dimension, asic_gap, write_cspad_cbf

master_phil = libtbx.phil.parse("""
metrology_file = None
  .type = str
  .help = File with optical metrology information posistioning quadrants and sensors
  .optional = False
detector = *CxiDs1 XppDs1
  .type = choice
  .optional = False
  .help = Specifiy CxiDs1 for the CXI Ds1 or DsD detectors which have relative coordinates for each quadrant,
  .help = or XppDs1 for XPP Ds1 detector which specifies absolute positions for each quadrant
plot = False
  .type = bool
  .help = show plots during processing
old_style_diff_path = None
  .type = str
  .help = path to old style metrology directory to compare with the given metrology file
out = None
  .type = str
  .help = Output file name
  .optional = False
""")

if (__name__ == "__main__") :
  user_phil = []
  for arg in sys.argv[1:]:
    if (os.path.isfile(arg)) :
      user_phil.append(libtbx.phil.parse("""metrology_file=\"%s\"""" % arg))
    else :
      try :
        user_phil.append(libtbx.phil.parse(arg))
      except RuntimeError as e :
        raise Sorry("Unrecognized argument '%s' (error: %s)" % (arg, str(e)))

  params = master_phil.fetch(sources=user_phil).extract()
  if (params.metrology_file is None) :
    master_phil.show()
    raise Usage("metrology_file must be defined (either metrology_file=XXX, or the file path alone).")
  assert params.detector is not None
  assert params.plot is not None
  assert params.out is not None

  print(params.metrology_file, params.detector)

  from xfel.cftbx.detector.cspad_cbf_tbx import pixel_size

  metro = read_optical_metrology_from_flat_file(params.metrology_file, params.detector,
                                                pixel_size,asic_dimension,asic_gap,
                                                plot=params.plot,old_style_diff_path=params.old_style_diff_path)

  write_cspad_cbf(None, metro, 'cbf', None, params.out, None, 0, header_only=True)



 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_parameters.py
from __future__ import absolute_import, division, print_function
#-*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.parameters
#
# Utility for printing out default parameters for cctbx.xfel
#

if __name__=='__main__':

  from spotfinder.applications.xfel import cxi_phil
  horizons_phil = cxi_phil.cxi_versioned_extract()
  horizons_phil.persist.show()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_pbs.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.pbs
#
# $Id$

from __future__ import absolute_import, division, print_function

import sys


def run(argv=None):
  import libtbx.load_env

  from os import path
  from libtbx import easy_run

  if argv is None:
    argv = sys.argv

  # Absolute path to the executable Bourne-shell script.
  pbs_sh = libtbx.env.under_dist('xfel', path.join('cxi', 'pbs.sh'))

  return easy_run.call(' '.join([pbs_sh] + argv[1:]))


if __name__ == '__main__':
  sys.exit(run())


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_pickle2cbf.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# LIBTBX_SET_DISPATCHER_NAME cxi.pickle2cbf
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1
# $Id
#

import sys, os
import libtbx.phil
from libtbx.utils import Usage
from xfel.cftbx.detector.cspad_cbf_tbx import write_cspad_cbf
from libtbx import easy_pickle
from xfel.cxi.cspad_ana.parse_calib import calib2sections
from scitbx.array_family import flex

master_phil = libtbx.phil.parse("""
pickle_file = None
  .type = str
  .help = Path to file(s) to convert
  .multiple = True
old_metrology = None
  .type = str
  .help = Path to original metrology this file was created with.  If none, use run 4.
new_metrology = None
  .type = str
  .help = File with optical metrology information posistioning quadrants and sensors or directory with calibration information
  .help = If none, use run 4.
detector = *CxiDs1 XppDs1
  .type = choice
  .optional = False
  .help = Specifiy CxiDs1 for the CXI Ds1 or DsD detectors which have relative coordinates for each quadrant,
  .help = or XppDs1 for XPP Ds1 detector which specifies absolute positions for each quadrant
plot = False
  .type = bool
  .help = show plots during processing
""")

if (__name__ == "__main__") :
  user_phil = []
  for arg in sys.argv[1:]:
    if (os.path.isfile(arg)) :
      user_phil.append(libtbx.phil.parse("""pickle_file=\"%s\"""" % arg))
    else :
      try :
        user_phil.append(libtbx.phil.parse(arg))
      except RuntimeError as e :
        raise Sorry("Unrecognized argument '%s' (error: %s)" % (arg, str(e)))

  params = master_phil.fetch(sources=user_phil).extract()
  assert params.pickle_file is not None
  if len(params.pickle_file) == 0 :
    master_phil.show()
    raise Usage("pickle_file must be defined (either pickle_file=XXX, or the file path(s) alone).")
  assert params.detector is not None
  assert params.plot is not None

  if params.old_metrology is None:
    params.old_metrology=libtbx.env.find_in_repositories(
      "xfel/metrology/CSPad/run4/CxiDs1.0_Cspad.0")
  if os.path.isdir(params.old_metrology):
    sections = calib2sections(params.old_metrology)
  else:
    from xfel.cxi.cspad_ana.cspad_tbx import xpp_active_areas
    assert params.old_metrology in xpp_active_areas

  if params.new_metrology is None:
    params.new_metrology=libtbx.env.find_in_repositories(
      "xfel/metrology/CSPad/run4/CxiDs1.0_Cspad.0")
  assert os.path.isdir(params.new_metrology) or os.path.isfile(params.new_metrology)

  # Read the new metrology, either from a calibration directory or an optical metrology
  # flat file
  if os.path.isdir(params.new_metrology):
    metro_style = "calibdir"

    from xfel.cftbx.detector.metrology2phil import metrology2phil
    metro = metrology2phil(params.new_metrology,False)

    args = []

    import iotbx.phil
    for arg in args:
      metro = metro.fetch(sources=[iotbx.phil.parse(arg)])
  else:
    assert os.path.isfile(params.new_metrology)
    ext = os.path.splitext(params.new_metrology)[1].lower()
    if ext in ['.def','.cbf']:
      metro_style = "cbf"

      from xfel.cftbx.detector.cspad_cbf_tbx import cbf_file_to_basis_dict
      metro = cbf_file_to_basis_dict(params.new_metrology)

    else:
      metro_style = "flatfile"

      from xfel.cftbx.detector.cspad_cbf_tbx import read_optical_metrology_from_flat_file, asic_dimension, asic_gap

      metro = read_optical_metrology_from_flat_file(params.new_metrology, params.detector, img['PIXEL_SIZE'],
                                                    asic_dimension, asic_gap, plot=params.plot)

  for filename in params.pickle_file:
    # Read the pickle file and pull the tiles out of it
    img = easy_pickle.load(filename)

    tiles = {}
    asics = {}
    data = img['DATA']

    if os.path.isdir(params.old_metrology):
      num_sections = len(sections)

      for p in range(num_sections):
        for s in range(len(sections[p])):

          # Pull the sensor block from the image, and rotate it back to
          # the "lying down" convention.
          c = sections[p][s].corners_asic()
          k = (int(round(-sections[p][s].angle / 90.0)) + 1) % 4
          for a in range(2):
            asic = data.matrix_copy_block(
              i_row=c[a][0],
              i_column=c[a][1],
              n_rows=c[a][2] - c[a][0],
              n_columns=c[a][3] - c[a][1])
            asics[(0, p, s, a)] = asic.matrix_rot90(k)

      # validate the quadrants all have the same number of sections, with matching asics
      for p in range(num_sections):
        if not 'section_len' in locals():
          section_len = len(sections[p])
        else:
          assert section_len == len(sections[p])

        for s in range(len(sections[p])):
          for a in range(2):
            if 'asic_focus' not in locals():
              asic_focus = asics[(0,p,s,a)].focus()
            else:
              assert asic_focus == asics[(0,p,s,a)].focus()
    else:
      active_areas = xpp_active_areas[params.old_metrology]['active_areas']
      rotations    = xpp_active_areas[params.old_metrology]['rotations']
      assert len(active_areas) // 4 == len(rotations) == 64

      active_areas = [(active_areas[(i*4)+0],
                       active_areas[(i*4)+1],
                       active_areas[(i*4)+2],
                       active_areas[(i*4)+3]) for i in range(64)]

      tile_id = 0

      num_sections = 4
      section_len = 8

      for p in range(num_sections):
        for s in range(section_len):
          for a in range(2):
            x1,y1,x2,y2 = active_areas[tile_id]
            block = data[x1:x2,y1:y2]
            asics[(0, p, s, a)] = block.matrix_rot90(-rotations[tile_id])
            tile_id += 1

            if not 'asic_focus' in locals():
              asic_focus = asics[(0, p, s, a)].focus()
            else:
              assert asic_focus == asics[(0, p, s, a)].focus()

    # make the tiles dictionary
    for p in range(num_sections):
      tiles[(0,p)] = type(data)(flex.grid(asic_focus[0]*section_len,asic_focus[1]*2))
      for s in range(section_len):
        tiles[(0,p)].matrix_paste_block_in_place(asics[(0,p,s,0)],
                                                 i_row = s*asic_focus[0],
                                                 i_column = 0)
        tiles[(0,p)].matrix_paste_block_in_place(asics[(0,p,s,1)],
                                                 i_row = s*asic_focus[0],
                                                 i_column = asic_focus[1])
      tiles[(0,p)].reshape(flex.grid((section_len,asic_focus[0],asic_focus[1]*2)))

    # Write the cbf file
    destpath = os.path.splitext(filename)[0] + ".cbf"

    write_cspad_cbf(tiles, metro, metro_style, img['TIMESTAMP'], destpath,
                    img['WAVELENGTH'], img['DISTANCE'])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_plotcv.py
#
# LIBTBX_SET_DISPATCHER_NAME cxi.plotcv
#
"""Main idea:  go through the integration log files and grep out the difference
   vectors between observed and predicted spot positions.  Plot these "correction
   vectors", but do so separately for each ASIC tile, giving an independent
   check on whether a tile is positioned properly by the tile_translation
   parameters."""
from __future__ import absolute_import, division, print_function

import iotbx.phil
import sys

from xfel.merging.database.merging_database import mysql_master_phil
master_phil="""
run_numbers = None
  .type = ints
  .help = List of run numbers to be aggregated together to make the plots.
  .help = Set run_numbers to None IF the outdir_template field gives a single file containing all log results.
outdir_template = None
  .type = str
  .help = Full path directory containing the stdout logs, with %%04d tag for run number
bravais_setting_id = None
  .type = int
  .help = ID number for the Bravais setting of interest (Labelit format).  eg, 1=triclinic, 12=hexagonal
show_plots = False
  .type = bool
  .help = Show graphical plots using matplotlib
colormap = False
  .type = bool
  .help = Show colormap correction vector plots
""" + mysql_master_phil

#-----------------------------------------------------------------------
def run(args):
  phil = iotbx.phil.process_command_line(args=args, master_string=master_phil).show()
  work_params = phil.work.extract()
  if ("--help" in args) :
    libtbx.phil.parse(master_phil.show())
    return

  if ((work_params.run_numbers is None and work_params.outdir_template is None) or
      (work_params.outdir_template is None) or
      (work_params.bravais_setting_id is None)) :
    from libtbx.utils import Usage
    raise Usage("cxi.plotcv "
                "run_numbers=16,17,18,19,20,21,22,23,24,25,26,27,71,72,73 "
                "outdir_template=/reg/data/ana11/cxi/cxi49812/scratch/april_2012/r%%04d/042/stdout "
                "bravais_setting_id=5")
  if work_params.show_plots is True:
    from matplotlib import pyplot as plt # special import

  from xfel.cxi.correction_vector_plot import run_correction_vector_plot
  run_correction_vector_plot(work_params)

  return None

if (__name__ == "__main__"):

  result = run(args=sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_psana.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.psana
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT

import os
import sys
from libtbx import easy_run

def run(args):
  sit_reldir = os.environ.get("SIT_RELDIR")
  sit_release = os.environ.get("SIT_RELEASE")
  sit_arch = os.environ.get("SIT_ARCH")
  assert [sit_reldir, sit_release, sit_arch].count(None) == 0
  pyana_path = os.path.join(sit_reldir,sit_release,"arch",sit_arch,"bin","psana")
  cmd = " ".join([pyana_path] + args)
  print(cmd)
  easy_run.call(cmd)


if __name__ == '__main__':
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_pyana.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.pyana
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT

import os
import sys
from libtbx import easy_run

def run(args):
  sit_reldir = os.environ.get("SIT_RELDIR")
  sit_release = os.environ.get("SIT_RELEASE")
  sit_arch = os.environ.get("SIT_ARCH")
  assert [sit_reldir, sit_release, sit_arch].count(None) == 0
  pyana_path = os.path.join(
    sit_reldir, sit_release, "arch", sit_arch, "bin", "pyana")
  cmd = " ".join(["libtbx.python %s" %pyana_path] + args)
  print(cmd)
  easy_run.call(cmd)


if __name__ == '__main__':
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_slaccalib2cbfheader.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.slaccalib2cbfheader
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1
# $Id
#

import sys, os
import libtbx.phil
from libtbx.utils import Usage, Sorry
from serialtbx.detector.cspad import read_slac_metrology
from xfel.cftbx.detector.cspad_cbf_tbx import write_cspad_cbf

master_phil = libtbx.phil.parse("""
metrology_file = None
  .type = str
  .help = File with optical metrology information posistioning quadrants and sensors.
  .help = Usually in the calib/geometry folder of the experiment, in the form of N-end.data
  .optional = False
plot = False
  .type = bool
  .help = show plots during processing
out = None
  .type = str
  .help = Output file name
  .optional = False
""")

def run(args):
  if ("--help" in args or "-h" in args) :
    print("Write a CBF header from a SLAC metrology file. Parameters:")
    master_phil.show(attributes_level=2)
    return

  user_phil = []
  for arg in args:
    if (os.path.isfile(arg)) :
      user_phil.append(libtbx.phil.parse("""metrology_file=\"%s\"""" % arg))
    else :
      try :
        user_phil.append(libtbx.phil.parse(arg))
      except RuntimeError as e :
        raise Sorry("Unrecognized argument '%s' (error: %s)" % (arg, str(e)))

  params = master_phil.fetch(sources=user_phil).extract()
  if (params.metrology_file is None) :
    master_phil.show()
    raise Usage("metrology_file must be defined (either metrology_file=XXX, or the file path alone).")
  assert params.plot is not None
  assert params.out is not None

  print(params.metrology_file)

  metro = read_slac_metrology(params.metrology_file, plot=params.plot)

  write_cspad_cbf(None, metro, 'cbf', None, params.out, None, 0, header_only=True)

if (__name__ == "__main__") :
  args = sys.argv[1:]
  run(args)


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_spots.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.spots
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT

from xfel.cxi.display_spots import run_one
import sys,os

if (__name__ == "__main__"):
  files = [arg for arg in sys.argv[1:] if os.path.isfile(arg)]
  arguments = [arg for arg in sys.argv[1:] if not os.path.isfile(arg)]
  for file in files:
    run_one(file, *arguments, **({'display':True}))


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_stream_to_pickle.py
# LIBTBX_SET_DISPATCHER_NAME cxi.stream_to_pickle
""" Utility for converting stream files from CrystFEL version 0.5.3 to
cctbx.xfel pickle files.
"""

from __future__ import absolute_import, division, print_function
import re
try:
  from six.moves import cPickle as pickle
except ImportError:
  import pickle
from cctbx.array_family import flex
from cctbx.uctbx import unit_cell
from cctbx.crystal import symmetry
from cctbx import miller
import logging

FORMAT = '%(levelname)s %(module)s.%(funcName)s: %(message)s'
logging.basicConfig(level=logging.DEBUG, format=FORMAT)

EV_PER_A = 12398.4187
# Regular expressions, set up so one can use groups to extract the data
re_energy = re.compile(r"photon_energy_eV\s=\s([0-9]+\.[0-9]+)")
re_uc = re.compile(r"""Cell\sparameters\s
                   ([0-9]+\.[0-9]+)\s # a
                   ([0-9]+\.[0-9]+)\s # b
                   ([0-9]+\.[0-9]+)\snm,\s # c
                   ([0-9]+\.[0-9]+)\s # alpha
                   ([0-9]+\.[0-9]+)\s # beta
                   ([0-9]+\.[0-9]+) # gamma""", re.X)

# groups 1-7: h,k,l,I,sig(i),peak, background,fs,ss
re_miller = re.compile(r"""\s*(-?[0-9]{1,3})
                          \s*(-?[0-9]{1,3})
                          \s*(-?[0-9]{1,3}) #h,k,l
                          \s*(-?[0-9]+\.[0-9]+)
                          \s*(-?[0-9]+\.[0-9]+) #I, SigI
                          ([\s*-?[0-9]+\.[0-9]+){2}  #Peak, background
                          \s*([0-9]+\.[0-9]+)
                          \s*([0-9]+\.[0-9]+) #fs, ssi""", re.X)
re_lattice_type = re.compile(r"lattice_type\s=\s([a-zA-Z]+)")
re_centering = re.compile(r"centering\s=\s([A-Z])")

# note the setting lattice is in nm^-1
re_Astar = re.compile(r"""astar\s=\s*(-?\+?[0-9]+\.[0-9]+)
                         \s*(-?\+?[0-9]+\.[0-9]+)
                         \s*(-?\+?[0-9]+\.[0-9]+)""", re.X)
re_Bstar = re.compile(r"""bstar\s=\s*(-?\+?[0-9]+\.[0-9]+)
                         \s*(-?\+?[0-9]+\.[0-9]+)
                         \s*(-?\+?[0-9]+\.[0-9]+)""", re.X)
re_Cstar = re.compile(r"""cstar\s=\s*(-?\+?[0-9]+\.[0-9]+)
                         \s*(-?\+?[0-9]+\.[0-9]+)
                         \s*(-?\+?[0-9]+\.[0-9]+)""", re.X)


def image_template():
    return {'Millers': [], 'Is': [], 'sigIs': [], 'location': []}


def check_image(image):
  if things_in_image == set(image):
    if len(image['Millers']) > 2:
      return True
  else:
    return False


def crystfel_to_cctbx_coord_system(abasis, bbasis, cbasis):
  """CrystFEL is RHS with z down the beam, and y to the ceiling.
  CCTBX.Postrefine is RHS with z to the source, and y to the ceiling.
  """
  from scitbx.matrix import sqr
  from cctbx import crystal_orientation

  a_mat = sqr((abasis[0], bbasis[0], cbasis[0],
               abasis[1], bbasis[1], cbasis[1],
               abasis[2], bbasis[2], cbasis[2]))

  coord_transformation = sqr((  -1,  0,   0,
                                 0,  1,   0,
                                 0,  0,  -1))
  new_coords = a_mat.__mul__(coord_transformation)

  ori = crystal_orientation.crystal_orientation(new_coords, crystal_orientation.basis_type.reciprocal)
  logging.debug("\naStar: {}\nbStar: {}\ncStar: {}".format(abasis, bbasis, cbasis))
  logging.debug(str(ori))
  return ori


def unit_cell_to_symetry_object(img_dict):
  xsym = symmetry(unit_cell=img_dict['current_orientation'][0].unit_cell().niggli_cell(),
                  space_group_symbol=point_group)
  miller_set = miller.set(crystal_symmetry=xsym,
                          indices=flex.miller_index(img_dict['Millers']),
                          anomalous_flag=True)
  miller_array = miller.array(miller_set,
                              flex.double(img_dict['Is']),
                              flex.double(img_dict['sigIs']))
  miller_array.set_observation_type_xray_intensity()
  miller_array.set_info("Raw data obtained by integration using CrystFEL")
  return miller_array


def make_int_pickle(img_dict, filename):
  img_dict['current_orientation'] = [crystfel_to_cctbx_coord_system(
                    img_dict['aStar'],
                    img_dict['bStar'],
                    img_dict['cStar'],)]
  try:
    final_dict = {'observations': [unit_cell_to_symetry_object(img_dict)],
                  'mapped_predictions': [flex.vec2_double(img_dict['location'])],
                  'xbeam': 96.99,
                  'ybeam': 96.97,
                  'distance': 150.9,
                  "sa_parameters": ['None'],
                  "pointgroup": point_group,
                  "unit_cell": img_dict['unit cell'],
                  'current_orientation': img_dict['current_orientation'],
                  'wavelength': img_dict['wavelength'],
                  'centering': img_dict['centering'],
                  'lattice_type': img_dict['lattice_type']}
    pickle.dump(final_dict, open(filename, 'wb'))
    logging.info("dumped image {}".format(filename))
  except AssertionError as a:
    logging.warning("Failed an assertion on image {}! {}".format(filename, a.message))
  except Exception:
    logging.warning("Failed to make a dictionairy for image {}".format(filename))

if __name__ == "__main__":

  logging.critical("NOT READY FOR PRIME-TIME. CONTACT ZELDIN@STANFORD.EDU IF YOU WANT TO USE THIS.")

  import argparse
  parser = argparse.ArgumentParser(description=
                                 ('Create indexing pickles from a'
                                  'crystfel stream file.'))
  parser.add_argument('filename', type=str, nargs=1,
                    help='The filename of the stream file to be converted.')
  parser.add_argument('point_group', type=str, nargs=1,
                    help='The space group to be assigned.')
  parser.add_argument('--tag', type=str, nargs=1,
                    help="Prefix to be used for the indexing pickles.")
  args = parser.parse_args()
  stream_file = args.filename[0]
  point_group = args.point_group[0]
  if args.tag:
    tag = args.tag[0]
  else:
    tag = stream_file[0].split('.')[0]


  things_in_image = {'Millers', 'Is', 'sigIs', 'unit cell', 'aStar', 'bStar',
                     'cStar', 'wavelength', 'centering', 'lattice_type',
                     'location'}
  with open(stream_file, "r+") as stream:
    count = 1
    this_image = image_template()
    for line in stream:
      millers = re_miller.match(line)
      if millers:
        this_image["Millers"].append((int(millers.group(1)),
                                      int(millers.group(2)),
                                      int(millers.group(3))))
        this_image["Is"].append(float(millers.group(4)))
        this_image["sigIs"].append(float(millers.group(5)))
        this_image["location"].append((float(millers.group(7)),
                                       float(millers.group(8))))
        continue

      uc = re_uc.match(line)
      if uc:  # Start of a new crystal dictionary
        if check_image(this_image):  # i.e. it's a complete image dictionairy
          make_int_pickle(this_image, "{}_{:04d}.pickle".format(tag, count))
          count += 1
          this_image = image_template()
          this_image["unit cell"] = unit_cell((float(uc.group(1)) * 10,  # nm to A
                                               float(uc.group(2)) * 10,
                                               float(uc.group(3)) * 10,
                                               float(uc.group(4)),
                                               float(uc.group(5)),
                                               float(uc.group(6))))

        else:
          this_image = image_template()
          this_image["unit cell"] = unit_cell((float(uc.group(1)) * 10,  # nm to A
                                               float(uc.group(2)) * 10,
                                               float(uc.group(3)) * 10,
                                               float(uc.group(4)),
                                               float(uc.group(5)),
                                               float(uc.group(6))))
        continue

      energy = re_energy.match(line)
      if energy:
        this_image['wavelength'] = float(energy.group(1)) / EV_PER_A
        continue

      lattice = re_lattice_type.match(line)
      if lattice:
        this_image['lattice_type'] = lattice.group(1)
        continue

      centering = re_centering.match(line)
      if centering:
        this_image['centering'] = centering.group(1)
        continue

      astar = re_Astar.match(line)
      if astar:
        this_image['aStar'] = [float(astar.group(1)) / 10,
                               float(astar.group(2)) / 10,
                               float(astar.group(3)) / 10]
        continue

      bstar = re_Bstar.match(line)
      if bstar:
        this_image['bStar'] = [float(bstar.group(1)) / 10,
                               float(bstar.group(2)) / 10,
                               float(bstar.group(3)) / 10]
        continue

      cstar = re_Cstar.match(line)
      if cstar:
        this_image['cStar'] = [float(cstar.group(1)) / 10,
                               float(cstar.group(2)) / 10,
                               float(cstar.group(3)) / 10]

        continue
# After this for loop, we should have an array of dictionairies,
# each containing the info needed.
# Just need to pickle the array of dictionairies! et voila!


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_view.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.view
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1

from xfel.cxi.display_spots import view_raw_image
import sys,os

if (__name__ == "__main__"):
  files = [arg for arg in sys.argv[1:] if os.path.isfile(arg)]
  arguments = [arg for arg in sys.argv[1:] if not os.path.isfile(arg)]
  for file in files:
    view_raw_image(file, *arguments, **({'display':True}))


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_xmerge.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.xmerge
#
# $Id$

from __future__ import absolute_import, division, print_function
from six.moves import range

import iotbx.phil
from dials.array_family import flex
from cctbx import uctbx
from iotbx import mtz
from libtbx.utils import Usage, multi_out
from libtbx import easy_pickle
import os
import time
import sys

from xfel.command_line.cxi_merge import master_phil,scaling_manager
from xfel.command_line.cxi_merge import unit_cell_distribution,show_overall_observations
from xfel.command_line.cxi_merge import scaling_result
from xfel.command_line.cxi_merge import consistent_set_and_model
from xfel import column_parser
from cctbx import miller
from six.moves import zip

#-----------------------------------------------------------------------
class xscaling_manager (scaling_manager) :
  def __init__ (self, miller_set, i_model, params, log=None) :
    scaling_manager.__init__(self,miller_set,i_model,params,log)

  def scale_all (self) :
    t1 = time.time()

    self.read_all_mysql()
    self.millers = self.millers_mysql
    self.frames = self.frames_mysql
    self._frames = self._frames_mysql
    self.observations = self.observations_mysql
    self._observations = self._observations_mysql
    if self.params.model is None:
      self.n_accepted = len(self.frames["cc"])
      self.n_low_corr = 0
      self.those_accepted = flex.bool(self.n_accepted, True)
    else:
      self.n_accepted = (self.frames["cc"]>self.params.min_corr).count(True)
      self.n_low_corr = (self.frames["cc"]>self.params.min_corr).count(False)
      self.those_accepted = (self.frames["cc"]>self.params.min_corr)
      statsy = flex.mean_and_variance(self.frames["cc"])
      print("%5d images, individual image correlation coefficients are %6.3f +/- %5.3f"%(
               len(self.frames["cc"]),
               statsy.mean(),  statsy.unweighted_sample_standard_deviation(),
               ), file=self.log)
    if self.params.scaling.report_ML and "half_mosaicity_deg" in self.frames:
      mosaic = self.frames["half_mosaicity_deg"].select(self.those_accepted)
      Mstat = flex.mean_and_variance(mosaic)
      print("%5d images, half mosaicity is %6.3f +/- %5.3f degrees"%(
               len(mosaic), Mstat.mean(), Mstat.unweighted_sample_standard_deviation()), file=self.log)
      domain = self.frames["domain_size_ang"].select(self.those_accepted)
      Dstat = flex.mean_and_variance(domain)
      print("%5d images, domain size is %6.0f +/- %5.0f Angstroms"%(
               len(domain), Dstat.mean(), Dstat.unweighted_sample_standard_deviation()), file=self.log)

      invdomain = 1./domain
      Dstat = flex.mean_and_variance(invdomain)
      print("%5d images, inverse domain size is %f +/- %f Angstroms"%(
               len(domain), Dstat.mean(), Dstat.unweighted_sample_standard_deviation()), file=self.log)
      print("%5d images, domain size is %6.0f +/- %5.0f Angstroms"%(
               len(domain), 1./Dstat.mean(), 1./Dstat.unweighted_sample_standard_deviation()), file=self.log)

    t2 = time.time()
    print("", file=self.log)
    print("#" * 80, file=self.log)
    print("FINISHED MERGING", file=self.log)
    print("  Elapsed time: %.1fs" % (t2 - t1), file=self.log)
    print("  %d integration files were accepted" % (
      self.n_accepted), file=self.log)
    print("  %d rejected due to poor correlation" % \
      self.n_low_corr, file=self.log)

  def read_all_mysql(self):
    print("reading observations from %s database"%(self.params.backend))

    if self.params.backend == 'MySQL':
      from xfel.merging.database.merging_database import manager
    elif self.params.backend == 'SQLite':
      from xfel.merging.database.merging_database_sqlite3 import manager
    else:
      from xfel.merging.database.merging_database_fs import manager

    CART = manager(self.params)
    self.millers_mysql = CART.read_indices()
    self.millers = self.millers_mysql

    self.observations_mysql = CART.read_observations()
    parser = column_parser()
    parser.set_int("hkl_id",self.observations_mysql["hkl_id"])
    parser.set_double("i",self.observations_mysql["i"])
    parser.set_double("sigi",self.observations_mysql["sigi"])
    parser.set_int("frame_id",self.observations_mysql["frame_id"])
    parser.set_int("H",self.observations_mysql["original_h"])
    parser.set_int("K",self.observations_mysql["original_k"])
    parser.set_int("L",self.observations_mysql["original_l"])
    self._observations_mysql = parser
    self.observations = dict(hkl_id=parser.get_int("hkl_id"),
                             i=parser.get_double("i"),
                             sigi=parser.get_double("sigi"),
                             frame_id=parser.get_int("frame_id"),
                             H=parser.get_int("H"),
                             K=parser.get_int("K"),
                             L=parser.get_int("L"),
                             )

    self.frames_mysql = CART.read_frames()
    parser = column_parser()
    parser.set_int("frame_id",self.frames_mysql["frame_id"])
    parser.set_double("wavelength",self.frames_mysql["wavelength"])
    parser.set_double("cc",self.frames_mysql["cc"])
    try:
      parser.set_double("slope",self.frames_mysql["slope"])
      parser.set_double("offset",self.frames_mysql["offset"])
      if self.params.scaling.report_ML:
        parser.set_double("domain_size_ang",self.frames_mysql["domain_size_ang"])
        parser.set_double("half_mosaicity_deg",self.frames_mysql["half_mosaicity_deg"])
    except KeyError: pass
    self._frames_mysql = parser

    CART.join()

#-----------------------------------------------------------------------
def run(args):
  phil = iotbx.phil.process_command_line(args=args, master_string=master_phil).show()
  work_params = phil.work.extract()
  from xfel.merging.phil_validation import application
  application(work_params)
  if ("--help" in args) :
    libtbx.phil.parse(master_phil.show())
    return

  if ((work_params.d_min is None) or
      (work_params.data is None) or
      ( (work_params.model is None) and work_params.scaling.algorithm != "mark1") ) :
    raise Usage("cxi.merge "
                "d_min=4.0 "
                "data=~/scratch/r0220/006/strong/ "
                "model=3bz1_3bz2_core.pdb")
  if ((work_params.rescale_with_average_cell) and
      (not work_params.set_average_unit_cell)) :
    raise Usage("If rescale_with_average_cell=True, you must also specify "+
      "set_average_unit_cell=True.")
  if work_params.raw_data.sdfac_auto and work_params.raw_data.sdfac_refine:
    raise Usage("Cannot specify both sdfac_auto and sdfac_refine")
  if not work_params.include_negatives_fix_27May2018:
    work_params.include_negatives = False # use old behavior

  log = open("%s_%s.log" % (work_params.output.prefix,work_params.scaling.algorithm), "w")
  out = multi_out()
  out.register("log", log, atexit_send_to=None)
  out.register("stdout", sys.stdout)

  # Verify that the externally supplied isomorphous reference, if
  # present, defines a suitable column of intensities, and exit with
  # error if it does not.  Then warn if it is necessary to generate
  # Bijvoet mates.  Failure to catch these issues here would lead to
  # possibly obscure problems in cxi/cxi_cc.py later on.
  try:
    data_SR = mtz.object(work_params.scaling.mtz_file)
  except RuntimeError:
    pass
  else:
    array_SR = None
    obs_labels = []
    for array in data_SR.as_miller_arrays():
      this_label = array.info().label_string().lower()
      if array.observation_type() is not None:
        obs_labels.append(this_label.split(',')[0])
      if this_label.find('fobs')>=0:
        array_SR = array.as_intensity_array()
        break
      if this_label.find('imean')>=0:
        array_SR = array.as_intensity_array()
        break
      if this_label.find(work_params.scaling.mtz_column_F)==0:
        array_SR = array.as_intensity_array()
        break

    if array_SR is None:
      known_labels = ['fobs', 'imean', work_params.scaling.mtz_column_F]
      raise Usage(work_params.scaling.mtz_file +
                  " does not contain any observations labelled [" +
                  ", ".join(known_labels) +
                  "].  Please set scaling.mtz_column_F to one of [" +
                  ",".join(obs_labels) + "].")
    elif not work_params.merge_anomalous and not array_SR.anomalous_flag():
      print("Warning: Preserving anomalous contributors, but %s " \
        "has anomalous contributors merged.  Generating identical Bijvoet " \
        "mates." % work_params.scaling.mtz_file, file=out)

  # Read Nat's reference model from an MTZ file.  XXX The observation
  # type is given as F, not I--should they be squared?  Check with Nat!
  print("I model", file=out)
  if work_params.model is not None:
    from xfel.merging.general_fcalc import run
    i_model = run(work_params)
    work_params.target_unit_cell = i_model.unit_cell()
    work_params.target_space_group = i_model.space_group_info()
    i_model.show_summary()
  else:
    i_model = None

  print("Target unit cell and space group:", file=out)
  print("  ", work_params.target_unit_cell, file=out)
  print("  ", work_params.target_space_group, file=out)

  miller_set, i_model = consistent_set_and_model(work_params,i_model)

# ---- Augment this code with any special procedures for x scaling
  scaler = xscaling_manager(
    miller_set=miller_set,
    i_model=i_model,
    params=work_params,
    log=out)
  scaler.scale_all()
  if scaler.n_accepted == 0:
    return None
# --- End of x scaling
  scaler.uc_values = unit_cell_distribution()
  for icell in range(len(scaler.frames["unit_cell"])):
    if scaler.params.model is None:
      scaler.uc_values.add_cell(
      unit_cell=scaler.frames["unit_cell"][icell])
    else:
      scaler.uc_values.add_cell(
      unit_cell=scaler.frames["unit_cell"][icell],
      rejected=(scaler.frames["cc"][icell] < scaler.params.min_corr))

  scaler.show_unit_cell_histograms()
  if (work_params.rescale_with_average_cell) :
    average_cell_abc = scaler.uc_values.get_average_cell_dimensions()
    average_cell = uctbx.unit_cell(list(average_cell_abc) +
      list(work_params.target_unit_cell.parameters()[3:]))
    work_params.target_unit_cell = average_cell
    print("", file=out)
    print("#" * 80, file=out)
    print("RESCALING WITH NEW TARGET CELL", file=out)
    print("  average cell: %g %g %g %g %g %g" % \
      work_params.target_unit_cell.parameters(), file=out)
    print("", file=out)
    scaler.reset()
    scaler = xscaling_manager(
      miller_set=miller_set,
      i_model=i_model,
      params=work_params,
      log=out)
    scaler.scale_all()
    scaler.uc_values = unit_cell_distribution()
    for icell in range(len(scaler.frames["unit_cell"])):
      if scaler.params.model is None:
        scaler.uc_values.add_cell(
        unit_cell=scaler.frames["unit_cell"][icell])
      else:
        scaler.uc_values.add_cell(
        unit_cell=scaler.frames["unit_cell"][icell],
        rejected=(scaler.frames["cc"][icell] < scaler.params.min_corr))
    scaler.show_unit_cell_histograms()
  if False : #(work_params.output.show_plots) :
    try :
      plot_overall_completeness(completeness)
    except Exception as e :
      print("ERROR: can't show plots")
      print("  %s" % str(e))
  print("\n", file=out)

  reserve_prefix = work_params.output.prefix
  for data_subset in [1,2,0]:
    work_params.data_subset = data_subset
    work_params.output.prefix = "%s_s%1d_%s"%(reserve_prefix,data_subset,work_params.scaling.algorithm)

    if work_params.data_subset == 0:
      scaler.frames["data_subset"] = flex.bool(scaler.frames["frame_id"].size(),True)
    elif work_params.data_subset == 1:
      scaler.frames["data_subset"] = scaler.frames["odd_numbered"]
    elif work_params.data_subset == 2:
      scaler.frames["data_subset"] = scaler.frames["odd_numbered"]==False

  # --------- New code ------------------
    #sanity check
    for mod,obs in zip(miller_set.indices(), scaler.millers["merged_asu_hkl"]):
      if mod!=obs: raise Exception("miller index lists inconsistent--check d_min are equal for merge and xmerge scripts")
      assert mod==obs

    """Sum the observations of I and I/sig(I) for each reflection.
    sum_I = flex.double(i_model.size(), 0.)
    sum_I_SIGI = flex.double(i_model.size(), 0.)
    scaler.completeness = flex.int(i_model.size(), 0)
    scaler.summed_N = flex.int(i_model.size(), 0)
    scaler.summed_wt_I = flex.double(i_model.size(), 0.)
    scaler.summed_weight = flex.double(i_model.size(), 0.)
    scaler.n_rejected = flex.double(scaler.frames["frame_id"].size(), 0.)
    scaler.n_obs = flex.double(scaler.frames["frame_id"].size(), 0.)
    scaler.d_min_values = flex.double(scaler.frames["frame_id"].size(), 0.)
    scaler.ISIGI = {}"""

    from xfel import scaling_results, get_scaling_results, get_isigi_dict
    results = scaling_results(scaler._observations, scaler._frames,
              scaler.millers["merged_asu_hkl"],scaler.frames["data_subset"],
              work_params.include_negatives)
    results.__getattribute__(
      work_params.scaling.algorithm)(
      scaler.params.min_corr, scaler.params.target_unit_cell)

    sum_I, sum_I_SIGI, \
    scaler.completeness, scaler.summed_N, \
    scaler.summed_wt_I, scaler.summed_weight, scaler.n_rejected, scaler.n_obs, \
    scaler.d_min_values, hkl_ids, i_sigi_list = get_scaling_results(results)

    scaler.ISIGI = get_isigi_dict(results)

    if work_params.merging.refine_G_Imodel:
      from xfel.cxi.merging.refine import find_scale

      my_find_scale = find_scale(scaler, work_params)

      sum_I, sum_I_SIGI, \
        scaler.completeness, scaler.summed_N, \
        scaler.summed_wt_I, scaler.summed_weight, scaler.n_rejected, \
        scaler.n_obs, scaler.d_min_values, hkl_ids, i_sigi_list \
        = my_find_scale.get_scaling_results(results, scaler)
      scaler.ISIGI = get_isigi_dict(results)


    scaler.wavelength = scaler.frames["wavelength"]
    scaler.corr_values = scaler.frames["cc"]

    scaler.rejected_fractions = flex.double(scaler.frames["frame_id"].size(), 0.)
    for irej in range(len(scaler.rejected_fractions)):
      if scaler.n_obs[irej] > 0:
        scaler.rejected_fractions = scaler.n_rejected[irej]/scaler.n_obs[irej]
  # ---------- End of new code ----------------

    if work_params.raw_data.sdfac_refine or work_params.raw_data.errors_from_sample_residuals:
      if work_params.raw_data.sdfac_refine:
        if work_params.raw_data.error_models.sdfac_refine.minimizer == 'simplex':
          from xfel.merging.algorithms.error_model.sdfac_refine import sdfac_refine as error_modeler
        elif work_params.raw_data.error_models.sdfac_refine.minimizer == 'lbfgs':
          from xfel.merging.algorithms.error_model.sdfac_refine_lbfgs import sdfac_refine_refltable_lbfgs as error_modeler
        elif self.params.raw_data.error_models.sdfac_refine.minimizer == 'LevMar':
          from xfel.merging.algorithms.error_model.sdfac_refine_levmar import sdfac_refine_refltable_levmar as error_modeler

      if work_params.raw_data.errors_from_sample_residuals:
        from xfel.merging.algorithms.error_model.errors_from_residuals import errors_from_residuals as error_modeler

      error_modeler(scaler).adjust_errors()

    if work_params.raw_data.reduced_chi_squared_correction:
      from xfel.merging.algorithms.error_model.reduced_chi_squared import reduced_chi_squared
      reduced_chi_squared(scaler).compute()

    miller_set_avg = miller_set.customized_copy(
      unit_cell=work_params.target_unit_cell)

    table1 = show_overall_observations(
      obs=miller_set_avg,
      redundancy=scaler.completeness,
      redundancy_to_edge=None,
      summed_wt_I=scaler.summed_wt_I,
      summed_weight=scaler.summed_weight,
      ISIGI=scaler.ISIGI,
      n_bins=work_params.output.n_bins,
      title="Statistics for all reflections",
      out=out,
      work_params=work_params)
    if table1 is None:
      raise Exception("table could not be constructed")
    print("", file=out)
    if work_params.scaling.algorithm == 'mark0':
      n_refl, corr = scaler.get_overall_correlation(sum_I)
    else:
      n_refl, corr = ((scaler.completeness > 0).count(True), 0)
    print("\n", file=out)
    table2 = show_overall_observations(
      obs=miller_set_avg,
      redundancy=scaler.summed_N,
      redundancy_to_edge=None,
      summed_wt_I=scaler.summed_wt_I,
      summed_weight=scaler.summed_weight,
      ISIGI=scaler.ISIGI,
      n_bins=work_params.output.n_bins,
      title="Statistics for reflections where I > 0",
      out=out,
      work_params=work_params)
    if table2 is None:
      raise Exception("table could not be constructed")

    print("", file=out)
    mtz_file, miller_array = scaler.finalize_and_save_data()

    loggraph_file = os.path.abspath("%s_graphs.log" % work_params.output.prefix)
    f = open(loggraph_file, "w")
    f.write(table1.format_loggraph())
    f.write("\n")
    f.write(table2.format_loggraph())
    f.close()
    result = scaling_result(
      miller_array=miller_array,
      plots=scaler.get_plot_statistics(),
      mtz_file=mtz_file,
      loggraph_file=loggraph_file,
      obs_table=table1,
      all_obs_table=table2,
      n_reflections=n_refl,
      overall_correlation=corr)
    easy_pickle.dump("%s.pkl" % work_params.output.prefix, result)
  work_params.output.prefix = reserve_prefix

  # Output table with number of images contribution reflections per
  # resolution bin.
  from libtbx import table_utils

  miller_set_avg.setup_binner(
    d_max=100000, d_min=work_params.d_min, n_bins=work_params.output.n_bins)
  table_data = [["Bin", "Resolution Range", "# images", "%accept"]]
  if work_params.model is None:
    appropriate_min_corr = -1.1 # lowest possible c.c.
  else:
    appropriate_min_corr = work_params.min_corr
  n_frames = (scaler.frames['cc'] > appropriate_min_corr).count(True)
  iselect = 1
  while iselect<work_params.output.n_bins:
    col_count1 = results.count_frames(appropriate_min_corr, miller_set_avg.binner().selection(iselect))
    print("colcount1",col_count1)
    if col_count1>0: break
    iselect +=1
  if col_count1==0: raise Exception("no reflections in any bins")
  for i_bin in miller_set_avg.binner().range_used():
    col_count = '%8d' % results.count_frames(
      appropriate_min_corr, miller_set_avg.binner().selection(i_bin))
    col_legend = '%-13s' % miller_set_avg.binner().bin_legend(
      i_bin=i_bin, show_bin_number=False, show_bin_range=False,
      show_d_range=True, show_counts=False)
    xpercent = results.count_frames(appropriate_min_corr, miller_set_avg.binner().selection(i_bin))/float(n_frames)
    percent = '%5.2f'% (100.*xpercent)
    table_data.append(['%3d' % i_bin, col_legend, col_count,percent])

  table_data.append([""] * len(table_data[0]))
  table_data.append(["All", "", '%8d' % n_frames])
  print(file=out)
  print(table_utils.format(
    table_data, has_header=1, justify='center', delim=' '), file=out)

  reindexing_ops = {"h,k,l":0} # get a list of all reindexing ops for this dataset
  if work_params.merging.reverse_lookup is not None:
    for key in scaler.reverse_lookup:
      if reindexing_ops.get(scaler.reverse_lookup[key], None) is None:
        reindexing_ops[scaler.reverse_lookup[key]]=0
      reindexing_ops[scaler.reverse_lookup[key]]+=1

  from xfel.cxi.cxi_cc import run_cc
  for key in reindexing_ops.keys():
    run_cc(work_params,reindexing_op=key,output=out)

  if isinstance(scaler.ISIGI, dict):
    from xfel.merging import isigi_dict_to_reflection_table
    refls = isigi_dict_to_reflection_table(scaler.miller_set.indices(), scaler.ISIGI)
  else:
    refls = scaler.ISIGI
  easy_pickle.dump("%s.refl"%work_params.output.prefix, refls)

  return result

if (__name__ == "__main__"):
  show_plots = False
  if ("--plots" in sys.argv) :
    sys.argv.remove("--plots")
    show_plots = True
  result = run(args=sys.argv[1:])
  if result is None:
    sys.exit(1)
  if (show_plots) :
    try :
      result.plots.show_all_pyplot()
      from wxtbx.command_line import loggraph
      loggraph.run([result.loggraph_file])
    except Exception as e :
      print("Can't display plots")
      print("You should be able to view them by running this command:")
      print("  wxtbx.loggraph %s" % result.loggraph_file)
      raise e


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_xtc_process.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.xtc_process
#
from psana import *
import os, sys, socket
from libtbx.utils import Sorry
from libtbx.phil import parse
from xfel.cxi.cspad_ana import cspad_tbx

help_str = """
Use cxi.xtc_process to analyze XTC streams from LCLS using psana modules
specified in a config file.

You will likely find cxi.mpi_submit more useful generally, but use this prog-
ram to test your config files for a few events.

Should be ran from your myrelease directory. Be sure to run sit_setup first.

Example usages:

cxi.xtc_process input.cfg=cxi49812/thermo.cfg input.experiment=cxi49812 \\
  input.run_num=25

This will use one process on the current node to analyze every event from run
25 of experiment cxi49812 using the modules specfied in cxi49812/thermo.cfg.

mpirun -n 16 cxi.xtc_process input.cfg=cxi49812/thermo.cfg \\
  input.experiment=cxi49812 input.run_num=25 dispatch.max_events=1000

As above, but use 16 processes on the current node and only process 1000 events.

bsub -a mympi -n 100 -o out.log -q psanaq cxi.xtc_process \\
  input.cfg=cxi49812/thermo.cfg input.experiment=cxi49812 input.run_num=25

Submit a processing job to the psana queue using 100 cores and put the
resultant log in out.log.

"""

phil_scope = parse('''
  dispatch {
    max_events = None
      .type = int
      .help = "If not specified, process all events. Otherwise, only process this many"
  }
  input {
    cfg = None
      .type = str
      .help = "Path to psana config file"
    experiment = None
      .type = str
      .help = "Experiment identifier, e.g. cxi84914"
    run_num = None
      .type = int
      .help = "Run number or run range to process"
    stream = None
      .type = int
      .expert_level = 2
      .help = Stream number to read from. Usually not necessary as psana will read the data \
              from all streams by default
    use_ffb = False
      .type = bool
      .help = "Run on the ffb if possible. Only for active users!"
    xtc_dir = None
      .type = str
      .help = "Optional path to data directory if it's non-standard. Only needed if xtc"
      .help = "streams are not in the standard location for your PSDM installation."
  }
  output {
    output_dir = "."
      .type = str
      .help = "Directory for output files"
  }
  mp {
    method = *mpi sge
      .type = choice
      .help = "Muliprocessing method"
    mpi {
      method = *client_server striping
        .type = choice
        .help = Method of serving data to child processes in MPI. client_server:    \
                use one process as a server that sends timestamps to each process.  \
                All processes will stay busy at all times at the cost of MPI send/  \
                recieve overhead. striping: each process uses its rank to determine \
                which events to process. Some processes will finish early and go    \
                idle, but no MPI overhead is incurred.
    }
  }
  debug {
    write_debug_files = False
      .type = bool
      .help = "If true, will write out a tiny diagnostic file for each event before"
      .help = "and after the event is processed"
    use_debug_files = False
      .type = bool
      .help = "If true, will look for debug diagnostic files in the output_dir and"
      .help = "only process events that crashed previously"
    event_timestamp = None
      .type = str
      .help = "If set to be a timestamp, will only process the event that matches it"
  }
''', process_includes=True)

class Script(object):
  """ Script to process XFEL data at LCLS """
  def __init__(self):
    pass

  def run(self):
    """ Process all images assigned to this thread """
    if len(sys.argv) == 1 or "-h" in sys.argv or "--help" in sys.argv or "-c" in sys.argv:
      print(help_str)
      print("Showing phil parameters:")
      print(phil_scope.as_str(attributes_level = 2))
      return

    user_phil = []
    for arg in sys.argv[1:]:
      if (os.path.isfile(arg)):
        user_phil.append(parse(file_name=arg))
      else:
        try:
          user_phil.append(parse(arg))
        except RuntimeError as e:
          raise Sorry("Unrecognized argument '%s' (error: %s)" % (arg, str(e)))

    params = phil_scope.fetch(sources=user_phil).extract()
    self.params = params

    assert params.input.cfg is not None
    assert params.input.experiment is not None
    assert params.input.run_num is not None

    print("Processing run %d of experiment %s using config file %s"%(params.input.run_num, params.input.experiment, params.input.cfg))

    if params.mp.method == "mpi":
      from libtbx.mpi4py import MPI
      comm = MPI.COMM_WORLD
      rank = comm.Get_rank() # each process in MPI has a unique id, 0-indexed
      size = comm.Get_size() # size: number of processes running in this job
    elif params.mp.method == "sge" and \
        'SGE_TASK_ID'    in os.environ and \
        'SGE_TASK_FIRST' in os.environ and \
        'SGE_TASK_LAST'  in os.environ:
      if 'SGE_STEP_SIZE' in os.environ:
        assert int(os.environ['SGE_STEP_SIZE']) == 1
      if os.environ['SGE_TASK_ID'] == 'undefined' or os.environ['SGE_TASK_ID'] == 'undefined' or os.environ['SGE_TASK_ID'] == 'undefined':
        rank = 0
        size = 1
      else:
        rank = int(os.environ['SGE_TASK_ID']) - int(os.environ['SGE_TASK_FIRST'])
        size = int(os.environ['SGE_TASK_LAST']) - int(os.environ['SGE_TASK_FIRST']) + 1
    else:
      rank = 0
      size = 1

    # set up psana
    setConfigFile(params.input.cfg)
    dataset_name = "exp=%s:run=%s:idx"%(params.input.experiment,params.input.run_num)
    if params.input.xtc_dir is not None:
      if params.input.use_ffb:
        raise Sorry("Cannot specify the xtc_dir and use SLAC's ffb system")
      dataset_name += ":dir=%s"%params.input.xtc_dir
    elif params.input.use_ffb:
      # as ffb is only at SLAC, ok to hardcode /reg/d here
      dataset_name += ":dir=/reg/d/ffb/%s/%s/xtc"%(params.input.experiment[0:3],params.input.experiment)
    if params.input.stream is not None:
      dataset_name += ":stream=%d"%params.input.stream
    ds = DataSource(dataset_name)

    # set this to sys.maxint to analyze all events
    if params.dispatch.max_events is None:
      max_events = sys.maxsize
    else:
      max_events = params.dispatch.max_events

    for run in ds.runs():
      # list of all events
      times = run.times()
      nevents = min(len(times),max_events)

      if params.mp.method == "mpi" and size > 2 and params.mp.mpi.method == 'client_server':
        # use a client/server approach to be sure every process is busy as much as possible
        # only do this if there are more than 2 processes, as one process will be a server
        print("Using MPI client server")
        if rank == 0:
          # server process
          for t in times[:nevents]:
            # a client process will indicate it's ready by sending its rank
            rankreq = comm.recv(source=MPI.ANY_SOURCE)
            comm.send(t,dest=rankreq)
          # send a stop command to each process
          for rankreq in range(size-1):
            rankreq = comm.recv(source=MPI.ANY_SOURCE)
            comm.send('endrun',dest=rankreq)
        else:
          # client process
          while True:
            # inform the server this process is ready for an event
            comm.send(rank,dest=0)
            evttime = comm.recv(source=0)
            if evttime == 'endrun': break
            self.process_event(run, evttime)
      else:
        # chop the list into pieces, depending on rank.  This assigns each process
        # events such that the get every Nth event where N is the number of processes
        print("Striping events")
        mytimes = [times[i] for i in range(nevents) if (i+rank)%size == 0]

        for i in range(len(mytimes)):
          self.process_event(run, mytimes[i])
      run.end()
    ds.end()

  def process_event(self, run, timestamp):
    """
    Process a single event from a run
    @param run psana run object
    @param timestamp psana timestamp object
    """

    ts = cspad_tbx.evt_timestamp((timestamp.seconds(),timestamp.nanoseconds()/1e6))
    if self.params.debug.event_timestamp is not None and self.params.debug.event_timestamp != ts:
      return

    ts_path = os.path.join(self.params.output.output_dir, "debug-" + ts + ".txt")

    if self.params.debug.use_debug_files:
      if not os.path.exists(ts_path):
        print("Skipping event %s: no debug file found"%ts)
        return

      f = open(ts_path, "r")
      if len(f.readlines()) > 1:
        print("Skipping event %s: processed successfully previously"%ts)
        return
      f.close()
      print("Accepted", ts)

    if self.params.debug.write_debug_files:
      f = open(ts_path, "w")
      f.write("%s about to process %s\n"%(socket.gethostname(), ts))
      f.close()

    # load the event.  This will cause any modules to be run.
    evt = run.event(timestamp)

    if self.params.debug.write_debug_files:
      f = open(ts_path, "a")
      f.write("done\n")
      f.close()

    id = evt.get(EventId)

    # nop since the module does all the work.

if __name__ == "__main__":
  script = Script()
  script.run()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/detector_residuals.py
# -*- coding: utf-8 -*-
#!/usr/bin/env python
#
# detector_congruence.py
#
#  Copyright (C) 2016 Lawrence Berkeley National Laboratory (LBNL)
#
#  Author: Aaron Brewster
#
#  This code is distributed under the X license, a copy of which is
#  included in the root directory of this package.
#
# LIBTBX_SET_DISPATCHER_NAME dev.cctbx.xfel.detector_residuals
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.detector_residuals
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1
#
from __future__ import absolute_import, division, print_function
from six.moves import range
from dials.array_family import flex
from dials.util import show_mail_on_error
from scitbx.matrix import col
from matplotlib import pyplot as plt
from matplotlib.patches import Polygon
from matplotlib.colors import Normalize
from matplotlib import cm
from matplotlib.backends.backend_pdf import PdfPages
from matplotlib.colors import LogNorm
import numpy as np
from libtbx.phil import parse
import math
from six.moves import zip

help_message = '''

This program is used to calculate statisical measurements of consistency
between observed and predicted reflections

Example:

  dev.cctbx.xfel.detector_residuals experiment.expt reflections.refl
'''

# Create the phil parameters
phil_scope = parse('''
dot_size = 10
  .type = int
  .help = Size of dots in detector plots
panel_numbers = True
  .type = bool
  .help = Whether to show panel numbers on each panel
verbose = True
  .type = bool
  .help = Whether to print statistics
repredict_input_reflections = True
  .type = bool
  .help = Whether to use the input models to repredict reflection positions \
          prior to making plots
residuals {
  plot_max=0.3
    .type = float
    .help = Maximum residual value to be shown in the detector plot
  histogram_max=None
    .type = float
    .help = Maximum x value to be used computing the histogram
  histogram_xmax=None
    .type = float
    .help = Maximum x value to be shown in the histogram
  histogram_ymax=None
    .type = float
    .help = Maximum y value to be shown in the histogram
  exclude_outliers=None
    .type = bool
    .deprecated = True
    .help = Deprecated. Use residuals.exclude_outliers_from_refinement instead
  exclude_outliers_from_refinement=True
    .type = bool
    .help = Whether to exclude outliers while computing statistics
  i_sigi_cutoff=None
    .type = float
    .help = Minimum I/sigI filter for RMSD plots
  recompute_outliers = False
    .type = bool
    .help = If True, use sauter_poon to recompute outliers and remove them
  delta_scalar = 50
    .type = float
    .help = For deltaXY and similar plots that show for each reflection the offset \
            between the observed and predicted spot in mm, scale that offset by this \
            value
  mcd_filter {
    enable = False
      .type = bool
      .help = on the DeltaPsi plot, apply a 3-feature MCD filter
    mahalanobis_distance = 7
      .type = float(value_min=0.)
      .help = cutoff level expressed as a multivariate Gaussian std dev
    keep = *inliers outliers
      .type = choice
      .help = this should be obvious.  Either keep the good ones or the bad ones.
  }
  print_correlations = True
    .type = bool
    .help = For each panel group, print the correlation between radial offset\
            and delta_psi, and transverse offset and delta_psi.
}

repredict
  .expert_level = 2 {
  enable = False
    .type = bool
    .help = If True, repredict reflection positions using bandpass and mosaic  \
            approximations
  mode = tophat_mosaicity_and_bandpass *gaussian_mosaicity_and_bandpass
    .type = choice
    .help = Use tophat functions or gaussians to approximate the mosaic        \
            parameters of the crystal and spectral properties of the beam
  refine_mode = *per_experiment all None
    .type = choice
    .help = Refine mosaicity for each experiment individually (per_experiment) \
            or refine a single mosaicity of all experiments (all).  If None,   \
            do not refine mosaicity.
  refine_bandpass = True
    .type = bool
    .help = If True and if also refining mosaicity, then refine the bandpass.
  initial_mosaic_parameters = None
    .type = floats(size=2)
    .help = If set, provide two numbers: domain size (angstroms) and half      \
            mosaic angle (degrees). If unset, use the crystal model's          \
            mosaic parameters.
}

plots {
  all_plots = False
    .type = bool
    .help = Override the rest of the flags in the plots phil scope to show    \
            all plots available
  reflection_energies = False
    .type = bool
    .help = Plot the energy each reflection would need to be exactly on the   \
            Ewald sphere
  pos_vs_neg_delta_psi = False
    .type = bool
    .help = For each image, determine how many reflections have a delta psi   \
            > 0 and how many reflections have a delta psi < 0. It is expected \
            that for most images these numbers will be about the same. Then   \
            create a 2D histogram, where each pixel is how many images have   \
            Y reflections with delta psi < 0  vs. X reflections with delta    \
            psi > 0. It is expected most of the data lies on an x=y line.
  deltaXY_histogram = False
    .type = bool
    .help = Histogram of the distance between the observed and predicted      \
            reflections. Mean and mode are plotted, but it is expected that   \
            this forms a Rayleigh distribution, so the Rayleigh mean is also  \
            shown. Further, RMSD and Rayleigh RMSD obs - pred are also shown. \
            This plot is also made for each panel.
  per_image_RMSDs_histogram = True
    .type = bool
    .help = For each image, compute the RMSD of the observered - predicted    \
            reflection locations. Then histogram these RMSDs over all the     \
            images. This plot is also made for each panel.
  per_image_RMSDs_boxplot = False
    .type = bool
    .help = Box plot of per-image RMSDs
  positional_displacements = True
    .type = bool
    .help = Each reflection is plotted as observed on the detector. The color \
            of the reflection is the difference between the reflection's      \
            observed and predicted locations.
  include_radial_and_transverse = True
    .type = bool
    .help = Also plot radial and transverse displacements: the displacement   \
            vectors between observed and predicted spot locations are split   \
            into radial and transverse components, where radial is the        \
            component of the vector along the line from the refleciton to the \
            beam center, and the transvrse component is the component of the  \
            diplacement vector along the line orthogonal to the radial        \
            component.
  deltaXY_by_deltapsi = True
    .type = bool
    .help = For each reflection, compute the displacement vector deltaXY      \
            between the observed and predicted spot locations. Using the      \
            center of the panel as an origin, plot the reflection displaced   \
            from the center of its panel along its deltaXY vector. The        \
            reflections are colored by their delta psi values.
  deltaXY_by_reflection_energy = False
    .type = bool
    .help = As deltaXY_by_deltapsi, but reflections are colored mean pixel   \
            energy. Every pixel in a reflection could be exactly on the Ewald \
            sphere given a specific wavelength. The mean pixel energy for a   \
            reflection is the mean energy of the pixels in the reflection if  \
            each pixel was individually on the Ewald sphere.
  repredict_from_reflection_energies = False
    .type = bool
    .help = As deltaXY_by_deltapsi, but repredict each reflection using the  \
            wavelength needed to place that reflection on the Ewald sphere.
  deltaXY_by_deltaXY = False
    .type = bool
    .help = As deltaXY_by_deltapsi, but color reflections by the magnitude   \
            of the deltaXY vector.
  manual_cdf = False
    .type = bool
  radial_vs_deltaPsi_vs_deltaXY = False
    .type = bool
    .help = For each panel, each reflection is plotted given an origin in the \
            center of the panel. Y: radial displacement (obs-pred along the   \
            radial direction), X: delta psi. Colored by the magnitude of      \
            deltaXY
  radial_difference_histograms = False
    .type = bool
    .help = For each panel, the radial displacements of each reflection are   \
            histogrammed.  The center is marked with a red line. Asymmetry    \
            indicates a panel not properly aligned in the radial direciton.
  delta_vs_azimuthal_angle = False
    .type = bool
    .help = For each reflection, plot either overall, radial, or transverse   \
            displacements vs. azimuthal angle, where the angle is between     \
            (0,1,0), i.e. the vector pointing up in real space, and the XY    \
            component of the vector from the sample to the observed spot      \
            centroid. Plot is a 2D histogram on a log scale.
  intensity_vs_radials_2dhist = False
    .type = bool
    .help = 2D histogram of reflection intensities vs radial displacements.   \
            Each pixel is the number of reflections with a give intensity and \
            a given radial displacement.
  delta2theta_vs_deltapsi_2dhist = False
    .type = bool
    .help = For each reflection, compute the difference in measured vs pred-   \
            icted two theta and the delta psi. 2D histogram is plotted where  \
            each pixel is the number of reflections with a given delta two    \
            theta and a given delta psi.  10 plots are shown, one for each of \
            10 resolution bins.
  delta2theta_vs_2theta_2dhist = False
    .type = bool
    .help = For each reflection, compute the two theta and the difference in  \
            measured vs predicted two theta. 2D histogram is plotted where    \
            each pixel is the number of reflections with a given delta two    \
            theta and a given two theta.
  deltaPsi_vs_2theta_2dhist = False
    .type = bool
    .help = For each reflection, compute the delta psi and two theta angles.  \
            2D histogram is plotted where each pixel is the number of         \
            reflections with a given delta psi and two theta. Result is       \
            similar to a trumpet plot but for the whole dataset.
  grouped_stats = False
    .type = bool
    .help = 5 plots are shown with different stats. For each panel group, the \
            stat is shown and the panel group is colored by that stat. Stats  \
            are: number of reflections, overall, radial and transverse RMSDs, \
            and the CC between delta 2 theta and delta psi among reflections  \
            in that panel group.
  unit_cell_histograms = True
    .type = bool
    .help = Unit cell histograms are shown for each of the a, b, and c axes.
  stats_by_2theta = False
    .type = bool
    .help = Reflections are binned by 2 theta, then various stats are computed\
            per bin. RMSD, R RMSD, T RMSD, RMSD delta 2theta: observed -      \
            predicted RMSDs, including overall, radial and transverse as well \
            as RMSD of delta 2theta. R/T RMSD: ratio of radial and transverse \
            RMSDs.
  stats_by_panelgroup = False
    .type = bool
    .help = As stats_by_2theta, but reflections are binned by panelgroup.
  trumpet_plot = False
    .type = bool
    .help = Show trumpet plot from Sauter 2014 for the first experiment
  ewald_offset_plot = False
    .type = bool
    .help = Show trumpet plot equivalent but using the Ewald offset for Y \
            (I.E. the reflection distance from the Ewald sphere)
  include_offset_dots = False
    .type = bool
    .help = Plot mean offset for spot predictions from observations in \
            deltaXY_by_deltapsi plot
  include_scale_bar_in_pixels = 0
    .type = float (value_min=0)
    .help = For each panel in the deltaXY_by_deltapsi plot, show xy scale \
            bars. Length of the scale bar is specified here in pixels.
}

save_pdf = False
  .type = bool
  .help = Whether to show the plots or save as a multi-page pdf
save_png = False
  .type = bool
  .help = Whether to show the plots or save as a series of pngs
include scope xfel.command_line.cspad_detector_congruence.phil_scope
''', process_includes=True)

def setup_stats(experiments, reflections, two_theta_only = False):
  # Compute a set of radial and transverse displacements for each reflection
  print("Setting up stats...")
  tmp = flex.reflection_table()
  # Need to construct a variety of vectors
  for expt_id, expt in enumerate(experiments):
    expt_refls = reflections.select(reflections['id'] == expt_id)
    if len(expt_refls) == 0: continue
    for panel_id, panel in enumerate(expt.detector):
      refls = expt_refls.select(expt_refls['panel'] == panel_id)
      if len(refls) == 0: continue
      # Compute the beam center in lab space (a vector pointing from the origin to where the beam would intersect
      # the panel, if it did intersect the panel)
      beam = expt.beam
      s0 = beam.get_s0()
      beam_centre = panel.get_beam_centre_lab(s0)
      bcl = flex.vec3_double(len(refls), beam_centre)
      cal_x, cal_y, _ = refls['xyzcal.px'].parts()
      ttc = flex.double([panel.get_two_theta_at_pixel(s0, (cal_x[i], cal_y[i])) for i in range(len(refls))])
      if 'xyzobs.px.value' in refls:
        obs_x, obs_y, _ = refls['xyzobs.px.value'].parts()
        tto = flex.double([panel.get_two_theta_at_pixel(s0, (obs_x[i], obs_y[i])) for i in range(len(refls))])
      refls['beam_centre_lab'] = bcl
      refls['two_theta_cal'] = ttc * (180/math.pi) #+ (0.5*panel_refls['delpsical.rad']*panel_refls['two_theta_obs'])
      if 'xyzobs.px.value' in refls:
        refls['two_theta_obs'] = tto * (180/math.pi)
      if not two_theta_only:
        # Compute obs in lab space
        x, y, _ = refls['xyzobs.mm.value'].parts()
        c = flex.vec2_double(x, y)
        refls['obs_lab_coords'] = panel.get_lab_coord(c)
        # Compute deltaXY in panel space. This vector is relative to the panel origin
        x, y, _ = (refls['xyzcal.mm'] - refls['xyzobs.mm.value']).parts()
        # Convert deltaXY to lab space, subtracting off of the panel origin
        refls['delta_lab_coords'] = panel.get_lab_coord(flex.vec2_double(x,y)) - panel.get_origin()
      tmp.extend(refls)
  reflections = tmp
  return reflections

def get_unweighted_rmsd(reflections, verbose=True):
  n = len(reflections)
  if n == 0:
    return 0
  #weights = 1/reflections['intensity.sum.variance']
  reflections = reflections.select(reflections['xyzobs.mm.variance'].norms() > 0)
  weights = 1/reflections['xyzobs.mm.variance'].norms()

  un_rmsd = math.sqrt( flex.sum(reflections['difference_vector_norms']**2)/n)
  w_rmsd = math.sqrt( flex.sum( weights*(reflections['difference_vector_norms']**2) )/flex.sum(weights))

  if verbose:
    print("%20s%7.3f"%("Unweighted RMSD (μm)", un_rmsd*1000))
    print("%20s%7.3f"%("Weighted RMSD (μm)", w_rmsd*1000))

  return un_rmsd


def reflection_wavelength_from_pixels(experiments, reflections):
  if 'shoebox' not in reflections:
    return reflections

  print("Computing per-reflection wavelengths from shoeboxes")

  from dials.algorithms.shoebox import MaskCode
  valid_code = MaskCode.Valid | MaskCode.Foreground

  table = flex.reflection_table()
  wavelengths = flex.double()
  for expt_id, expt in enumerate(experiments):
    refls = reflections.select(reflections['id'] == expt_id)
    table.extend(refls)

    beam = expt.beam
    unit_cell = expt.crystal.get_unit_cell()
    d = unit_cell.d(refls['miller_index'])

    for i in range(len(refls)):
      sb = refls['shoebox'][i]
      # find the coordinates with signal
      mask = flex.bool([(m & valid_code) != 0 for m in sb.mask])
      coords = sb.coords().select(mask)
      panel = expt.detector[refls['panel'][i]]

      # compute two theta angle for each pixel
      s1 = panel.get_lab_coord(panel.pixel_to_millimeter(flex.vec2_double(coords.parts()[0], coords.parts()[1])))
      two_theta = s1.angle(beam.get_s0())

      # nLambda = 2dST
      wavelengths.append(flex.mean(2*d[i]*flex.sin(two_theta/2)))

  table['reflection_wavelength_from_pixels'] = wavelengths
  return table

def trumpet_plot(experiment, reflections, axis = None):
  half_mosaicity_deg = experiment.crystal.get_half_mosaicity_deg()
  domain_size_ang = experiment.crystal.get_domain_size_ang()
  if not axis:
    fig = plt.figure()
    axis = plt.gca()
  two_thetas = reflections['two_theta_cal']
  delpsi = reflections['delpsical.rad']*180/math.pi
  axis.scatter(two_thetas, delpsi)

  LR = flex.linear_regression(two_thetas, delpsi)
  model_y = LR.slope()*two_thetas + LR.y_intercept()
  axis.plot(two_thetas, model_y, "k-")

  tan_phi_deg = (experiment.crystal.get_unit_cell().d(reflections['miller_index']) / domain_size_ang)*180/math.pi
  tan_outer_deg = tan_phi_deg + (half_mosaicity_deg/2)

  axis.set_title("Mosaicity FW=%4.2f deg, Dsize=%5.0fA on %d spots"%(2*half_mosaicity_deg, domain_size_ang, len(two_thetas)))
  axis.plot(two_thetas, tan_phi_deg, "r.")
  axis.plot(two_thetas, -tan_phi_deg, "r.")
  axis.plot(two_thetas, tan_outer_deg, "g.")
  axis.plot(two_thetas, -tan_outer_deg, "g.")

from xfel.command_line.cspad_detector_congruence import detector_plot_dict
from serialtbx.detector import iterate_detector_at_level, iterate_panels, id_from_name, get_center
from xfel.command_line.cspad_detector_congruence import Script as DCScript
class Script(DCScript):
  ''' Class to parse the command line options. '''

  def __init__(self):
    ''' Set the expected options. '''
    from dials.util.options import ArgumentParser
    import libtbx.load_env

    # Create the option parser
    usage = "usage: %s [options] /path/to/refined/json/file" % libtbx.env.dispatcher_name
    self.parser = ArgumentParser(
      usage=usage,
      sort_options=True,
      phil=phil_scope,
      read_experiments=True,
      read_reflections=True,
      check_format=False,
      epilog=help_message)

  def run(self):
    ''' Parse the options. '''
    from dials.util.options import flatten_experiments, flatten_reflections
    # Parse the command line arguments
    params, options = self.parser.parse_args(show_diff_phil=True)
    self.params = params
    if params.plots.all_plots:
      for attr in dir(params.plots):
        if attr.startswith('__'): continue
        setattr(params.plots, attr, True)

    experiments = flatten_experiments(params.input.experiments)

    # Find all detector objects
    detectors = experiments.detectors()

    # Verify inputs
    if len(params.input.reflections) == len(detectors) and len(detectors) > 1:
      # case for passing in multiple images on the command line
      assert len(params.input.reflections) == len(detectors)
      reflections = flex.reflection_table()
      for expt_id in range(len(detectors)):
        subset = params.input.reflections[expt_id].data
        subset['id'] = flex.int(len(subset), expt_id)
        reflections.extend(subset)
    else:
      # case for passing in combined experiments and reflections
      reflections = flatten_reflections(params.input.reflections)[0]

    ResidualsPlotter(params, experiments, reflections).plot_all()

class ResidualsPlotter(object):
  def __init__(self, params, experiments, reflections):
    self.params = params
    self.experiments = experiments
    self.reflections = reflections

  def get_normalized_colors(self, data, vmin=None, vmax=None):
    if vmax is None:
      vmax = self.params.residuals.plot_max
    if vmax is None:
      vmax = flex.max(data)
    if vmin is None:
      vmin = min(flex.min(data), 0)
    if len(data) > 1:
      assert vmax > vmin, "vmax: %f, vmin: %f"%(vmax, vmin)

    # initialize the color map
    norm = Normalize(vmin=vmin, vmax=vmax)
    cmap = plt.get_cmap(self.params.colormap)
    sm = cm.ScalarMappable(norm=norm, cmap=cmap)
    color_vals = np.linspace(vmin, vmax, 11)
    sm.set_array(color_vals) # needed for colorbar

    return norm, cmap, color_vals, sm

  def plot_deltas(self, reflections, panel = None, ax = None, bounds = None):
    assert panel is not None and ax is not None and bounds is not None

    data = (reflections['xyzcal.mm']-reflections['xyzobs.mm.value']).norms()
    norm, cmap, color_vals, sm = self.get_normalized_colors(data)
    deltas = (reflections['xyzcal.mm']-reflections['xyzobs.mm.value'])*self.params.residuals.delta_scalar

    x, y = panel.get_image_size_mm()
    offset = col((x, y, 0))/2
    deltas += offset
    mm_panel_coords = flex.vec2_double(deltas.parts()[0], deltas.parts()[1])

    lab_coords = panel.get_lab_coord(mm_panel_coords)

    ax.scatter(lab_coords.parts()[0], lab_coords.parts()[1], c = data, norm=norm, cmap = cmap, linewidths=0, s=self.params.dot_size)

    return sm, color_vals

  def plot_obs_colored_by_radial_deltas(self, reflections, panel = None, ax = None, bounds = None):
    return self.plot_obs_colored_by_data(flex.abs(reflections['radial_displacements']), reflections, panel, ax, bounds)

  def plot_obs_colored_by_transverse_deltas(self, reflections, panel = None, ax = None, bounds = None):
    return self.plot_obs_colored_by_data(flex.abs(reflections['transverse_displacements']), reflections, panel, ax, bounds)

  def plot_obs_colored_by_deltas(self, reflections, panel = None, ax = None, bounds = None):
    data = (reflections['xyzcal.mm']-reflections['xyzobs.mm.value']).norms()
    return self.plot_obs_colored_by_data(data, reflections, panel, ax, bounds)

  def plot_obs_colored_by_data(self, data, reflections, panel = None, ax = None, bounds = None):
    assert panel is not None and ax is not None and bounds is not None
    norm, cmap, color_vals, sm = self.get_normalized_colors(data)
    mm_panel_coords = flex.vec2_double(reflections['xyzobs.mm.value'].parts()[0], reflections['xyzobs.mm.value'].parts()[1])
    lab_coords = panel.get_lab_coord(mm_panel_coords)

    ax.scatter(lab_coords.parts()[0], lab_coords.parts()[1], c = data, norm=norm, cmap = cmap, linewidths=0, s=self.params.dot_size)

    return sm, color_vals

  def plot_obs_colored_by_deltapsi(self, reflections, panel = None, ax = None, bounds = None):
    if 'delpsical.rad' not in reflections:
      return
    assert panel is not None and ax is not None and bounds is not None
    data = reflections['delpsical.rad'] * (180/math.pi)
    norm, cmap, color_vals, sm = self.get_normalized_colors(data, vmin=-0.1, vmax=0.1)
    deltas = (reflections['xyzcal.mm']-reflections['xyzobs.mm.value'])*self.params.residuals.delta_scalar

    x, y = panel.get_image_size_mm()
    offset = col((x, y, 0))/2
    deltas += offset
    mm_panel_coords = flex.vec2_double(deltas.parts()[0], deltas.parts()[1])

    lab_coords = panel.get_lab_coord(mm_panel_coords)

    lab_coords_x, lab_coords_y, _ = lab_coords.parts()
    if self.params.residuals.mcd_filter.enable and len(reflections)>5:
      from xfel.metrology.panel_fitting import Panel_MCD_Filter
      MCD = Panel_MCD_Filter(lab_coords_x, lab_coords_y, data, i_panel = reflections["panel"][0],
                      delta_scalar = self.params.residuals.delta_scalar, params = self.params)
      sX,sY,sPsi = MCD.scatter_coords()
      MCD.plot_contours(ax,show=False) # run this to pre-compute the center position
      ax.scatter(sX, sY, c = sPsi, norm=norm, cmap = cmap, linewidths=0, s=self.params.dot_size)
    else:
      ax.scatter(lab_coords_x, lab_coords_y, c = data, norm=norm, cmap = cmap, linewidths=0, s=self.params.dot_size)

    if self.params.plots.include_offset_dots:
      panel_center = panel.get_lab_coord(offset[0:2])
      ax.scatter(panel_center[0], panel_center[1], c='k', s=self.params.dot_size)
      if self.params.plots.include_scale_bar_in_pixels > 0:
        pxlsz0,pxlsz1 = panel.get_pixel_size()
        ax.plot([panel_center[0],panel_center[0]],
                [panel_center[1],panel_center[1]+self.params.plots.include_scale_bar_in_pixels*pxlsz1*self.params.residuals.delta_scalar], 'k-')
        ax.plot([panel_center[0],panel_center[0]+self.params.plots.include_scale_bar_in_pixels*pxlsz0*self.params.residuals.delta_scalar],
                [panel_center[1],panel_center[1]], 'k-')
      ax.scatter(flex.mean(lab_coords_x), flex.mean(lab_coords_y), c='b', s=self.params.dot_size)
      if self.params.residuals.mcd_filter.enable and len(reflections)>5:
        ax.scatter(MCD.robust_model_XY.location_[0],
                   MCD.robust_model_XY.location_[1], c='r', s=self.params.dot_size)
      print(panel.get_name(), (flex.mean(lab_coords_x) - panel_center[0])/self.params.residuals.delta_scalar, (flex.mean(lab_coords_y) - panel_center[0])/self.params.residuals.delta_scalar)

    return sm, color_vals

  def plot_obs_colored_by_deltapsi_pxlambda(self, reflections, panel = None, ax = None, bounds = None):
    if 'delpsical.rad' not in reflections \
      or 'delpsical.rad.pxlambda' not in reflections \
      or 'xyzcal.mm.pxlambda' not in reflections:
      return
    assert panel is not None and ax is not None and bounds is not None
    data = reflections['delpsical.rad.pxlambda'] * (180/math.pi)
    norm, cmap, color_vals, sm = self.get_normalized_colors(data, vmin=-0.1, vmax=0.1)
    deltas = (reflections['xyzcal.mm.pxlambda']-reflections['xyzobs.mm.value'])*self.params.residuals.delta_scalar

    x, y = panel.get_image_size_mm()
    offset = col((x, y, 0))/2
    deltas += offset
    mm_panel_coords = flex.vec2_double(deltas.parts()[0], deltas.parts()[1])

    lab_coords = panel.get_lab_coord(mm_panel_coords)

    ax.scatter(lab_coords.parts()[0], lab_coords.parts()[1], c = data, norm=norm, cmap = cmap, linewidths=0, s=self.params.dot_size)

    return sm, color_vals

  def plot_obs_colored_by_mean_pixel_wavelength(self, reflections, panel = None, ax = None, bounds = None):
    if 'reflection_wavelength_from_pixels' not in reflections:
      return
    assert panel is not None and ax is not None and bounds is not None
    data = 12398.4/reflections['reflection_wavelength_from_pixels']
    norm, cmap, color_vals, sm = self.get_normalized_colors(data, vmin=self.min_energy, vmax=self.max_energy)
    deltas = (reflections['xyzcal.mm']-reflections['xyzobs.mm.value'])*self.params.residuals.delta_scalar

    x, y = panel.get_image_size_mm()
    offset = col((x, y, 0))/2
    deltas += offset
    mm_panel_coords = flex.vec2_double(deltas.parts()[0], deltas.parts()[1])

    lab_coords = panel.get_lab_coord(mm_panel_coords)

    ax.scatter(lab_coords.parts()[0], lab_coords.parts()[1], c = data, norm=norm, cmap = cmap, linewidths=0, s=self.params.dot_size)

    return sm, color_vals

  def plot_radial_displacements_vs_deltapsi(self, reflections, panel = None, ax = None, bounds = None):
    if 'delpsical.rad' not in reflections:
      return
    assert panel is not None and ax is not None and bounds is not None
    data = reflections['difference_vector_norms']
    norm, cmap, color_vals, sm = self.get_normalized_colors(data)

    a = reflections['delpsical.rad']*180/math.pi
    b = reflections['radial_displacements']

    fake_coords = flex.vec2_double(a, b) * self.params.residuals.delta_scalar

    x, y = panel.get_image_size_mm()
    offset = col((x, y))/2

    lab_coords = fake_coords + panel.get_lab_coord(offset)[0:2]

    ax.scatter(lab_coords.parts()[0], lab_coords.parts()[1], c = data, norm=norm, cmap = cmap, linewidths=0, s=self.params.dot_size)

    return sm, color_vals

  def plot_unitcells(self, experiments):
    if len(experiments) == 1:
      return
    all_a = flex.double()
    all_b = flex.double()
    all_c = flex.double()
    for crystal in experiments.crystals():
      a, b, c = crystal.get_unit_cell().parameters()[0:3]
      all_a.append(a); all_b.append(b); all_c.append(c)

    fig, axes = plt.subplots(nrows=3, ncols=1)
    for ax, axis, data in zip(axes, ['A', 'B', 'C'], [all_a, all_b, all_c]):
      stats = flex.mean_and_variance(data)
      cutoff = 4*stats.unweighted_sample_standard_deviation()
      if cutoff < 0.5:
        cutoff = 0.5
      limits = stats.mean()-cutoff, stats.mean()+cutoff
      sel = (data >= limits[0]) & (data <= limits[1])
      subset = data.select(sel)
      h = flex.histogram(subset,n_slots=50)
      ax.plot(h.slot_centers().as_numpy_array(),h.slots().as_numpy_array(),'-')
      ax.set_title("%s axis histogram (showing %d of %d xtals). Mean: %7.2f Stddev: %7.2f"%(
        axis, len(subset), len(data), stats.mean(),
        stats.unweighted_sample_standard_deviation()))
      ax.set_ylabel("N lattices")
      ax.set_xlabel(r"$\AA$")
      ax.set_xlim(limits)
    plt.tight_layout()

  def plot_histograms(self, reflections, panel = None, ax = None, bounds = None):
    data = reflections['difference_vector_norms']
    colors = ['b-', 'g-', 'g--', 'r-', 'b-', 'b--']
    n_slots = 20
    if self.params.residuals.histogram_max is None:
      h = flex.histogram(data, n_slots=n_slots)
    else:
      h = flex.histogram(data.select(data <= self.params.residuals.histogram_max), n_slots=n_slots)

    n = len(reflections)
    rmsd_obs = math.sqrt((reflections['xyzcal.mm']-reflections['xyzobs.mm.value']).sum_sq()/n)
    sigma = mode = h.slot_centers()[list(h.slots()).index(flex.max(h.slots()))]
    mean_obs = flex.mean(data)
    median = flex.median(data)
    mean_rayleigh = math.sqrt(math.pi/2)*sigma
    rmsd_rayleigh = math.sqrt(2)*sigma

    data = flex.vec2_double([(i,j) for i, j in zip(h.slot_centers(), h.slots())])
    n = len(data)
    for i in [mean_obs, mean_rayleigh, mode, rmsd_obs, rmsd_rayleigh]:
      data.extend(flex.vec2_double([(i, 0), (i, flex.max(h.slots()))]))
    data = self.get_bounded_data(data, bounds)
    tmp = [data[:n]]
    for i in range(len(colors)):
      tmp.append(data[n+(i*2):n+((i+1)*2)])
    data = tmp

    for d, c in zip(data, colors):
      ax.plot(d.parts()[0], d.parts()[1], c)

    if ax.get_legend() is None:
      ax.legend([r"$\Delta$XY", "MeanObs", "MeanRayl", "Mode", "RMSDObs", "RMSDRayl"])

  def plot_cdf_manually(self, reflections, panel = None, ax = None, bounds = None):
    colors = ['blue', 'green']
    r = (reflections['xyzcal.mm']-reflections['xyzobs.mm.value']).norms()
    h = flex.histogram(r)
    sigma = h.slot_centers()[list(h.slots()).index(flex.max(h.slots()))] # mode

    x_extent = max(r)
    y_extent = len(r)
    xobs = [i/x_extent for i in sorted(r)]
    yobs = [i/y_extent for i in range(y_extent)]
    obs = [(x, y) for x, y in zip(xobs, yobs)]

    ncalc = 100
    xcalc = [i/ncalc for i in range(ncalc)]
    ycalc = [1-math.exp((-i**2)/(2*(sigma**2))) for i in xcalc]
    calc = [(x, y) for x, y in zip(xcalc, ycalc)]

    data = [flex.vec2_double(obs),
            flex.vec2_double(calc)]
    if bounds is None:
      ax.set_xlim((-1,1))
      ax.set_ylim((-1,1))
      ax.set_title("%s Outlier SP Manually"%self.params.tag)
    if bounds is not None:
      data = [self.get_bounded_data(d, bounds) for d in data]

    if ax is None:
      fig = plt.figure()
      ax = fig.add_subplot(111)

    for subset,c in zip(data, colors):
        ax.plot(subset.parts()[0], subset.parts()[1], '-', c=c)

  def plot_radial_difference_histograms(self, reflections, panel = None, ax = None, bounds = None):
    r = reflections['radial_displacements']*1000
    h = flex.histogram(r, n_slots=10, data_min=-300, data_max=300)

    x = h.slot_centers()
    y = h.slots().as_double()
    x.append(0); y.append(0)
    x.append(0); y.append(flex.max(y))

    if bounds is None:
      ax.set_title("%s Radial differences"%self.params.tag)
    else:
      d = flex.vec2_double(x, y)
      data = self.get_bounded_data(d, bounds)
      x, y = data.parts()

    linex = [x[-2],x[-1]]; liney = [y[-2],y[-1]]
    x = x[:-2]; y = y[:-2]

    if ax is None:
      fig = plt.figure()
      ax = fig.add_subplot(111)
    ax.plot(x.as_numpy_array(), y.as_numpy_array(), '-', c='blue')
    ax.plot(linex, liney, '-', c='red')

  def plot_difference_vector_norms_histograms(self, reflections, panel = None, ax = None, bounds = None):
    r = reflections['difference_vector_norms']*1000
    h = flex.histogram(r, n_slots=10, data_min=0, data_max=100)

    x = h.slot_centers()
    y = h.slots().as_double()

    if bounds is None:
      ax.set_title("%s Residual norms histogram"%self.params.tag)
    else:
      d = flex.vec2_double(x, y)
      data = self.get_bounded_data(d, bounds)
      x, y = data.parts()

    if ax is None:
      fig = plt.figure()
      ax = fig.add_subplot(111)
    ax.plot(x.as_numpy_array(), y.as_numpy_array(), '-', c='blue')

  def get_bounded_data(self, data, bounds):
    assert len(bounds) == 4
    x = [b[0] for b in bounds]
    y = [b[1] for b in bounds]
    left = sorted(x)[1]
    right = sorted(x)[2]
    top = sorted(y)[2]
    bottom = sorted(y)[1]
    origin = col((left, bottom))
    scale_x = right-left
    scale_y = top-bottom
    scale = min(scale_x, scale_y)

    data_max_x = flex.max(data.parts()[0])
    data_min_x = flex.min(data.parts()[0])
    data_max_y = flex.max(data.parts()[1])
    data_min_y = flex.min(data.parts()[1])
    data_scale_x = data_max_x - data_min_x
    data_scale_y = data_max_y - data_min_y

    if data_scale_x == 0 or data_scale_y == 0:
      print("WARNING bad scale")
      return data

    xscale = scale/abs(data_scale_x)
    yscale = scale/abs(data_scale_y)

    return flex.vec2_double(((data.parts()[0] * xscale) - (data_min_x * xscale)),
                            ((data.parts()[1] * yscale) - (data_min_y * yscale))) + origin

  def plot_all(self):
    params = self.params
    experiments = self.experiments
    reflections = self.reflections

    detector = experiments.detectors()[0]

    if params.repredict_input_reflections:
      from dials.algorithms.refinement.prediction.managed_predictors import ExperimentsPredictorFactory
      ref_predictor = ExperimentsPredictorFactory.from_experiments(experiments, force_stills=experiments.all_stills())
      reflections = ref_predictor(reflections)

    if params.verbose: print("N reflections total:", len(reflections))
    if params.residuals.exclude_outliers_from_refinement:
      reflections = reflections.select(reflections.get_flags(reflections.flags.used_in_refinement))
      if params.verbose: print("N reflections used in refinement:", len(reflections))
      if params.verbose: print("Reporting only on those reflections used in refinement")

    if params.residuals.recompute_outliers:
      print("Performing outlier rejection on %d reflections"%len(reflections))
      from dials.algorithms.refinement.outlier_detection.sauter_poon import SauterPoon
      outlier = SauterPoon(px_sz=experiments[0].detector[0].get_pixel_size(), separate_panels=False)
      rejection_occured = outlier(reflections)
      if rejection_occured:
        reflections = reflections.select(~reflections.get_flags(reflections.flags.centroid_outlier))
        if params.verbose: print("N reflections after outlier rejection:", len(reflections))
      else:
        if params.verbose: print("No rejections found")

    if self.params.residuals.i_sigi_cutoff is not None:
      sel = (reflections['intensity.sum.value']/flex.sqrt(reflections['intensity.sum.variance'])) >= self.params.residuals.i_sigi_cutoff
      reflections = reflections.select(sel)
      if params.verbose: print("After filtering by I/sigi cutoff of %f, there are %d reflections left"%(self.params.residuals.i_sigi_cutoff,len(reflections)))

    if 'shoebox' in reflections and (params.repredict.enable or (params.show_plots and params.plots.reflection_energies)):
      reflections = reflection_wavelength_from_pixels(experiments, reflections)
      stats = flex.mean_and_variance(12398.4/reflections['reflection_wavelength_from_pixels'])
      if params.verbose: print("Mean energy: %.1f +/- %.1f"%(stats.mean(), stats.unweighted_sample_standard_deviation()))
      self.min_energy = stats.mean() - stats.unweighted_sample_standard_deviation()
      self.max_energy = stats.mean() + stats.unweighted_sample_standard_deviation()

      try:
        from dials_scratch.asb.predictions_from_reflection_wavelengths import predictions_from_per_reflection_energies, tophat_vector_wavelengths, refine_wavelengths, wavelengths_from_gaussians
      except ImportError:
        if params.repredict.enable:
          from libtbx.utils import Sorry
          raise Sorry("dials_scratch not configured so cannot do reprediction")
      else:
        reflections = predictions_from_per_reflection_energies(experiments, reflections, 'reflection_wavelength_from_pixels', 'pxlambda')

        if params.show_plots and params.plots.reflection_energies:
          fig = plt.figure()
          stats = flex.mean_and_variance(12398.4/reflections['reflection_wavelength_from_pixels'])
          plt.title("Energies derived from indexed pixels, mean: %.1f +/- %.1f"%(stats.mean(), stats.unweighted_sample_standard_deviation()))
          plt.hist(12398.4/reflections['reflection_wavelength_from_pixels'], bins=100)
          plt.xlabel("Energy (eV)")
          plt.ylabel("Count")

      if params.repredict.enable:
        init_mp = params.repredict.initial_mosaic_parameters

        if params.repredict.mode == 'tophat_mosaicity_and_bandpass':
          tag = 'reflection_wavelength_from_mosaicity_and_bandpass'
          dest = 'mosbandp'
          func = tophat_vector_wavelengths
          gaussians = False
        elif params.repredict.mode == 'gaussian_mosaicity_and_bandpass':
          tag = 'reflection_wavelength_from_gaussian_mosaicity_and_bandpass'
          dest = 'gmosbandp'
          func = wavelengths_from_gaussians
          gaussians = True

        if params.repredict.refine_mode == 'per_experiment':
          refined_reflections = flex.reflection_table()
          for expt_id in range(len(experiments)):
            if params.verbose: print("*"*80, "EXPERIMENT", expt_id)
            refls = reflections.select(reflections['id']==expt_id)
            refls['id'] = flex.int(len(refls), 0)
            refls = refine_wavelengths(experiments[expt_id:expt_id+1], refls, init_mp, tag, dest,
              refine_bandpass=params.repredict.refine_bandpass, gaussians=gaussians)
            refls['id'] = flex.int(len(refls), expt_id)
            refined_reflections.extend(refls)
          reflections = refined_reflections
        elif params.repredict.refine_mode == 'all':
          reflections = refine_wavelengths(experiments, reflections, init_mp, tag, dest,
            refine_bandpass=params.repredict.refine_bandpass, gaussians=gaussians)
        elif params.repredict.refine_mode is None or params.repredict.refine_mode == 'None':
          reflections = func(experiments, reflections, init_mp)
        reflections = predictions_from_per_reflection_energies(experiments, reflections, tag, dest)
        stats = flex.mean_and_variance(12398.4/reflections[tag])
        if params.verbose: print("Mean energy: %.1f +/- %.1f"%(stats.mean(), stats.unweighted_sample_standard_deviation()))
        reflections['delpsical.rad'] = reflections['delpsical.rad.%s'%dest]
        reflections['xyzcal.mm'] = reflections['xyzcal.mm.%s'%dest]
        reflections['xyzcal.px'] = reflections['xyzcal.px.%s'%dest]

    if 'xyzobs.mm.value' not in reflections:
      reflections.centroid_px_to_mm(experiments)
    reflections['difference_vector_norms'] = (reflections['xyzcal.mm']-reflections['xyzobs.mm.value']).norms()

    n = len(reflections)
    rmsd = get_unweighted_rmsd(reflections, params.verbose)
    print("%20s%7.3f"%("Dataset RMSD (μm)", rmsd * 1000))

    if params.tag is None:
      tag = ''
    else:
      tag = '%s '%params.tag

    if 'delpsical.rad' in reflections and params.show_plots and params.plots.pos_vs_neg_delta_psi:
      # set up delta-psi ratio heatmap
      fig = plt.figure()
      p = flex.int() # positive
      n = flex.int() # negative
      for i in set(reflections['id']):
        exprefls = reflections.select(reflections['id']==i)
        p.append(len(exprefls.select(exprefls['delpsical.rad']>0)))
        n.append(len(exprefls.select(exprefls['delpsical.rad']<0)))
      plt.hist2d(p, n, bins=30)
      cb = plt.colorbar()
      cb.set_label("N images")
      plt.title(r"%s2D histogram of pos vs. neg $\Delta\Psi$ per image"%tag)
      plt.xlabel(r"N reflections with $\Delta\Psi$ > 0")
      plt.ylabel(r"N reflections with $\Delta\Psi$ < 0")

    # Iterate through the detectors, computing detector statistics at the per-panel level (IE one statistic per panel)
    # Per panel dictionaries
    rmsds = {}
    refl_counts = {}
    transverse_rmsds = {}
    radial_rmsds = {}
    ttdpcorr = {}
    pg_bc_dists = {}
    mean_delta_two_theta = {}
    # per panelgroup flex arrays
    pg_rmsds = flex.double()
    pg_r_rmsds = flex.double()
    pg_t_rmsds = flex.double()
    pg_refls_count = flex.int()
    pg_refls_count_d = {}
    table_header = ["PG id", "RMSD","Radial", "Transverse", "N_refls"]
    table_header2 = ["","(μm)","RMSD(μm)","RMSD(μm)",""]
    if params.residuals.print_correlations:
      table_header += ["Correl", "Correl"]
      table_header2 += ["ΔR,ΔΨ","ΔT,ΔΨ"]
    table_data = []
    table_data.append(table_header)
    table_data.append(table_header2)

    reflections = setup_stats(experiments, reflections)

    # The radial vector points from the center of the reflection to the beam center
    radial_vectors = (reflections['obs_lab_coords'] - reflections['beam_centre_lab']).each_normalize()
    # The transverse vector is orthogonal to the radial vector and the beam vector
    transverse_vectors = radial_vectors.cross(reflections['beam_centre_lab']).each_normalize()
    # Compute the raidal and transverse components of each deltaXY
    reflections['radial_displacements']     = reflections['delta_lab_coords'].dot(radial_vectors)
    reflections['transverse_displacements'] = reflections['delta_lab_coords'].dot(transverse_vectors)

    # Iterate through the detector at the specified hierarchy level
    if hasattr(detector, 'hierarchy'):
      iterable = enumerate(iterate_detector_at_level(detector.hierarchy(), 0, params.hierarchy_level))
    else:
      iterable = enumerate(detector)

    if params.residuals.print_correlations:
      from xfel.metrology.panel_fitting import three_feature_fit
      pg_r_all = flex.double(); pg_t_all = flex.double(); pg_delpsi_all = flex.double()

    s0 = experiments[0].beam.get_s0()

    for pg_id, pg in iterable:
      pg_msd_sum = 0
      pg_r_msd_sum = 0
      pg_t_msd_sum = 0
      pg_refls = 0
      if params.residuals.print_correlations:
        pg_r = flex.double(); pg_t = flex.double()
      pg_delpsi = flex.double()
      pg_deltwotheta = flex.double()
      for p in iterate_panels(pg):
        panel_id = id_from_name(detector, p.get_name())
        panel_refls = reflections.select(reflections['panel'] == panel_id)
        n = len(panel_refls)
        pg_refls += n

        delta_x = panel_refls['xyzcal.mm'].parts()[0] - panel_refls['xyzobs.mm.value'].parts()[0]
        delta_y = panel_refls['xyzcal.mm'].parts()[1] - panel_refls['xyzobs.mm.value'].parts()[1]

        tmp = flex.sum((delta_x**2)+(delta_y**2))
        pg_msd_sum += tmp

        r = panel_refls['radial_displacements']
        t = panel_refls['transverse_displacements']
        if params.residuals.print_correlations:
          pg_r.extend(r); pg_t.extend(t)
        pg_r_msd_sum += flex.sum_sq(r)
        pg_t_msd_sum += flex.sum_sq(t)

        pg_delpsi.extend(panel_refls['delpsical.rad']*180/math.pi)
        pg_deltwotheta.extend(panel_refls['two_theta_obs'] - panel_refls['two_theta_cal'])

      bc = col(pg.get_beam_centre_lab(s0))
      ori = get_center(pg)
      pg_bc_dists[pg.get_name()] = (ori-bc).length()
      if len(pg_deltwotheta) > 0:
        mean_delta_two_theta[pg.get_name()] = flex.mean(pg_deltwotheta)
      else:
        mean_delta_two_theta[pg.get_name()] = 0

      if pg_refls == 0:
        pg_rmsd = pg_r_rmsd = pg_t_rmsd = 0
      else:
        pg_rmsd = math.sqrt(pg_msd_sum/pg_refls) * 1000
        pg_r_rmsd = math.sqrt(pg_r_msd_sum/pg_refls) * 1000
        pg_t_rmsd = math.sqrt(pg_t_msd_sum/pg_refls) * 1000
      pg_rmsds.append(pg_rmsd)
      pg_r_rmsds.append(pg_r_rmsd)
      pg_t_rmsds.append(pg_t_rmsd)
      pg_refls_count.append(pg_refls)
      pg_refls_count_d[pg.get_name()] = pg_refls
      table_data.append(["%d"%pg_id, "%.1f"%pg_rmsd, "%.1f"%pg_r_rmsd, "%.1f"%pg_t_rmsd, "%6d"%pg_refls])
      if params.residuals.print_correlations:
        pg_r_all.extend(pg_r); pg_t_all.extend(pg_t); pg_delpsi_all.extend(pg_delpsi)
        if len(pg_r)>2:
          TF = three_feature_fit(delta_radial = pg_r, delta_transverse = pg_t, delta_psi = pg_delpsi, i_panel=pg_id, verbose=False)
          pg_cc_Rpsi = 100.*TF.cross_correl[2]
          pg_cc_Tpsi = 100.*TF.cross_correl[1]
        else:
          pg_cc_Rpsi = 0.; pg_cc_Tpsi = 0.
        table_data[-1].extend(["%3.0f%%"%pg_cc_Rpsi, "%3.0f%%"%pg_cc_Tpsi])

      refl_counts[pg.get_name()] = pg_refls
      if pg_refls == 0:
        rmsds[p.get_name()] = -1
        radial_rmsds[p.get_name()] = -1
        transverse_rmsds[p.get_name()] = -1
        ttdpcorr[pg.get_name()] = -1
      else:
        rmsds[pg.get_name()] = pg_rmsd
        radial_rmsds[pg.get_name()]     = pg_r_rmsd
        transverse_rmsds[pg.get_name()] = pg_t_rmsd

        lc = flex.linear_correlation(pg_delpsi, pg_deltwotheta)
        ttdpcorr[pg.get_name()] = lc.coefficient()


    r1 = ["Weighted PG mean"]
    r2 = ["Weighted PG stddev"]
    if len(pg_rmsds) > 1:
      stats = flex.mean_and_variance(pg_rmsds, pg_refls_count.as_double())
      r1.append("%.1f"%stats.mean())
      r2.append("%.1f"%stats.gsl_stats_wsd())
      stats = flex.mean_and_variance(pg_r_rmsds, pg_refls_count.as_double())
      r1.append("%.1f"%stats.mean())
      r2.append("%.1f"%stats.gsl_stats_wsd())
      stats = flex.mean_and_variance(pg_t_rmsds, pg_refls_count.as_double())
      r1.append("%.1f"%stats.mean())
      r2.append("%.1f"%stats.gsl_stats_wsd())
    else:
      r1.extend([""]*3)
      r2.extend([""]*3)
    r1.append("")
    r2.append("")
    table_data.append(r1)
    table_data.append(r2)
    table_data.append(["PG Mean", "", "", "", "%8.1f"%flex.mean(pg_refls_count.as_double())])

    if params.residuals.print_correlations:
      TFA = three_feature_fit(delta_radial = pg_r_all, delta_transverse = pg_t_all, delta_psi = pg_delpsi_all,
         i_panel=pg_id, verbose=False)
      table_data.append(["Refls Mean", "", "", "", "",
                       "%3.0f%%"%(100.*TFA.cross_correl[2]), "%3.0f%%"%(100.*TFA.cross_correl[1]) ])

    from libtbx import table_utils
    if params.verbose: print("Detector statistics by panel group (PG)")
    if params.verbose: print(table_utils.format(table_data,has_header=2,justify='center',delim=" "))

    self.histogram(reflections, r"%s$\Delta$XY histogram (mm)"%tag, plots = params.show_plots and params.plots.deltaXY_histogram, verbose = params.verbose)

    if params.show_plots:
      if self.params.tag is None:
        t = ""
      else:
        t = "%s "%self.params.tag
      if params.plots.per_image_RMSDs_histogram: self.image_rmsd_histogram(reflections, tag, boxplot = params.plots.per_image_RMSDs_boxplot)

      # Plots! these are plots with callbacks to draw on individual panels
      if params.plots.positional_displacements:
        self.detector_plot_refls(detector, reflections, '%sOverall positional displacements (mm)'%tag,
                                 show=False, plot_callback=self.plot_obs_colored_by_deltas)
        if params.plots.include_radial_and_transverse:
          self.detector_plot_refls(detector, reflections, '%sRadial positional displacements (mm)'%tag,
                                   show=False, plot_callback=self.plot_obs_colored_by_radial_deltas)
          self.detector_plot_refls(detector, reflections, '%sTransverse positional displacements (mm)'%tag,
                                   show=False, plot_callback=self.plot_obs_colored_by_transverse_deltas)

      if params.plots.deltaXY_by_deltapsi:                self.detector_plot_refls(detector, reflections, r'%s$\Delta\Psi$'%tag,
                                                                                   show=False, plot_callback=self.plot_obs_colored_by_deltapsi, colorbar_units=r"$\circ$")
      if params.plots.deltaXY_by_reflection_energy:       self.detector_plot_refls(detector, reflections, '%sMean pixel energy'%tag,
                                                                                   show=False, plot_callback=self.plot_obs_colored_by_mean_pixel_wavelength, colorbar_units="eV")
      if params.plots.repredict_from_reflection_energies: self.detector_plot_refls(detector, reflections, r'%s$\Delta\Psi$ from mean pixel energies'%tag,
                                                                                   show=False, plot_callback=self.plot_obs_colored_by_deltapsi_pxlambda, colorbar_units=r"$\circ$")
      if params.plots.deltaXY_by_deltaXY:                 self.detector_plot_refls(detector, reflections, r'%s$\Delta$XY*%s'%(tag,
                                                                                   self.params.residuals.delta_scalar), show=False, plot_callback=self.plot_deltas)
      if params.plots.manual_cdf:                         self.detector_plot_refls(detector, reflections, '%sSP Manual CDF'%tag,
                                                                                   show=False, plot_callback=self.plot_cdf_manually)
      if params.plots.deltaXY_histogram:                  self.detector_plot_refls(detector, reflections, r'%s$\Delta$XY Histograms'%tag,
                                                                                   show=False, plot_callback=self.plot_histograms)
      if params.plots.radial_vs_deltaPsi_vs_deltaXY:      self.detector_plot_refls(detector, reflections, r'%sRadial displacements vs. $\Delta\Psi$, colored by $\Delta$XY'%tag,
                                                                                   show=False, plot_callback=self.plot_radial_displacements_vs_deltapsi)
      if params.plots.per_image_RMSDs_histogram:          self.detector_plot_refls(detector, reflections, r'%sPer image RMSD histograms'%tag,
                                                                                   show=False, plot_callback=self.plot_difference_vector_norms_histograms)
      if params.plots.radial_difference_histograms:       self.detector_plot_refls(detector, reflections, r'%sRadial differences'%tag,
                                                                                   show=False, plot_callback=self.plot_radial_difference_histograms)

      if params.plots.delta_vs_azimuthal_angle:
        # Plot deltas (XY, radial, transverse) vs. azimuthal angle
        xy = flex.vec3_double(reflections["s1"].parts()[0],
                              reflections["s1"].parts()[1],
                              flex.double(len(reflections), 0))
        angle = xy.angle((0, 1, 0), deg=True)
        sel = xy.parts()[0] >= 0
        subset = angle.select(sel)
        angle.set_selected(sel, 180 + (180 - subset))

        for column_name, caption in zip(["difference_vector_norms", "radial_displacements", "transverse_displacements"],
                                        ["XY", "radial", "transverse"]):
          fig = plt.figure()
          plt.hist2d(angle.as_numpy_array(), reflections[column_name].as_numpy_array() * 1000, bins=180, norm=LogNorm())
          plt.title(r"2d histogram of $\Delta$ %s vs azimuthal angle"%caption)
          plt.xlabel("Azimuthal angle (deg)")
          plt.ylabel(r"$\Delta$ %s ($\mu$m)"%caption)
          plt.colorbar()

      if params.plots.intensity_vs_radials_2dhist and 'intensity.sum.value' in reflections:
        # Plot intensity vs. radial_displacement
        fig = plt.figure()
        panel_id = 15
        panel_refls = reflections.select(reflections['panel'] == panel_id)
        a = panel_refls['radial_displacements']
        b = panel_refls['intensity.sum.value']
        sel = (a > -0.2) & (a < 0.2) & (b < 50000)
        plt.hist2d(a.select(sel), b.select(sel), bins=100)
        plt.title("%s2D histogram of intensity vs. radial displacement for panel %d"%(tag, panel_id))
        plt.xlabel("Radial displacement (mm)")
        plt.ylabel("Intensity")
        ax = plt.colorbar()
        ax.set_label("Counts")

      if params.plots.delta2theta_vs_deltapsi_2dhist:
        # Plot delta 2theta vs. deltapsi
        n_bins = 10
        bin_size = len(reflections)//n_bins
        bin_low = []
        bin_high = []
        data = flex.sorted(reflections['two_theta_obs'])
        for i in range(n_bins):
          bin_low = data[i*bin_size]
          if (i+1)*bin_size >= len(reflections):
            bin_high = data[-1]
          else:
            bin_high = data[(i+1)*bin_size]
          refls = reflections.select((reflections['two_theta_obs'] >= bin_low) &
                                     (reflections['two_theta_obs'] <= bin_high))
          a = refls['delpsical.rad']*180/math.pi
          b = refls['two_theta_obs'] - refls['two_theta_cal']
          fig = plt.figure()
          sel = (a > -0.2) & (a < 0.2) & (b > -0.05) & (b < 0.05)
          plt.hist2d(a.select(sel), b.select(sel), bins=50, range = [[-0.2, 0.2], [-0.05, 0.05]])
          cb = plt.colorbar()
          cb.set_label("N reflections")
          plt.title(r'%sBin %d (%.02f, %.02f 2$\Theta$) $\Delta2\Theta$ vs. $\Delta\Psi$. Showing %d of %d refls'%(tag,i,bin_low,bin_high,len(a.select(sel)),len(a)))
          plt.xlabel(r'$\Delta\Psi \circ$')
          plt.ylabel(r'$\Delta2\Theta \circ$')

      if params.plots.delta2theta_vs_2theta_2dhist:
        # Plot delta 2theta vs. 2theta
        a = reflections['two_theta_obs']#[:71610]
        b = reflections['two_theta_obs'] - reflections['two_theta_cal']
        fig = plt.figure()
        limits = -0.10, 0.10
        sel = (b > limits[0]) & (b < limits[1])
        plt.hist2d(a.select(sel), b.select(sel), bins=100, range=((0,45), limits))
        plt.clim((0,400))
        cb = plt.colorbar()
        cb.set_label("N reflections")
        plt.title(r'%s$\Delta2\Theta$ vs. 2$\Theta$. Showing %d of %d refls'%(tag,len(a.select(sel)),len(a)))
        plt.xlabel(r'2$\Theta \circ$')
        plt.ylabel(r'$\Delta2\Theta \circ$')

        # calc the trendline
        z = np.polyfit(a.select(sel), b.select(sel), 1)
        if params.verbose: print('y=%.7fx+(%.7f)'%(z[0],z[1]))

      if params.plots.deltaPsi_vs_2theta_2dhist:
        # Plot delta psi vs. 2theta
        x = reflections['two_theta_obs'].as_numpy_array()
        y = (reflections['delpsical.rad']*180/math.pi).as_numpy_array()
        fig = plt.figure()
        plt.hist2d(x, y, bins=100, range=((0,45), (-1,1)), norm=LogNorm())
        cb = plt.colorbar()
        cb.set_label("N reflections")
        plt.title(r'%s$\Delta\Psi$ vs. 2$\Theta$. %d refls'%(tag,len(x)))
        plt.xlabel(r'2$\Theta \circ$')
        plt.ylabel(r'$\Delta\Psi \circ$')

      if params.plots.grouped_stats:
        # Plots with single values per panel
        detector_plot_dict(self.params, detector, refl_counts, u"%s N reflections"%t, u"%6d", show=False)
        detector_plot_dict(self.params, detector, rmsds, "%s Positional RMSDs (microns)"%t, u"%4.1f", show=False)
        detector_plot_dict(self.params, detector, radial_rmsds, "%s Radial RMSDs (microns)"%t, u"%4.1f", show=False)
        detector_plot_dict(self.params, detector, transverse_rmsds, "%s Transverse RMSDs (microns)"%t, u"%4.1f", show=False)
        detector_plot_dict(self.params, detector, ttdpcorr, r"%s $\Delta2\Theta$ vs. $\Delta\Psi$ CC"%t, u"%5.3f", show=False)

      if params.plots.unit_cell_histograms: self.plot_unitcells(experiments)
      if params.plots.stats_by_2theta:      self.plot_data_by_two_theta(reflections, tag)

      if params.plots.stats_by_panelgroup:
        # Plot data by panel group
        sorted_values = sorted(pg_bc_dists.values())
        vdict = {}
        for k in pg_bc_dists:
          vdict[pg_bc_dists[k]] = k
        sorted_keys = [vdict[v] for v in sorted_values if vdict[v] in rmsds]
        pg_bc_dists_keylist = list(pg_bc_dists.keys())
        x = [sorted_values[i] for i in range(len(sorted_values)) if pg_bc_dists_keylist[i] in rmsds]

        self.plot_multi_data(x,
                             [[pg_refls_count_d[k] for k in sorted_keys],
                              ([rmsds[k] for k in sorted_keys],
                               [radial_rmsds[k] for k in sorted_keys],
                               [transverse_rmsds[k] for k in sorted_keys]),
                              [radial_rmsds[k]/transverse_rmsds[k] for k in sorted_keys],
                              [mean_delta_two_theta[k] for k in sorted_keys]],
                             "Panel group distance from beam center (mm)",
                             ["N reflections",
                              ("Overall RMSD",
                               "Radial RMSD",
                               "Transverse RMSD"),
                              "R/T RMSD ratio",
                              "Delta two theta"],
                             ["N reflections",
                              "RMSD (microns)",
                              "R/T RMSD ratio",
                              "Delta two theta (degrees)"],
                             "%sData by panelgroup"%tag)

      # Trumpet plot
      if params.plots.trumpet_plot:
        expt_id = min(set(reflections['id']))
        refls = reflections.select(reflections['id'] == expt_id)
        trumpet_plot(experiments[expt_id], refls)

      if params.plots.ewald_offset_plot:
        n_bins = 10
        all_offsets = flex.double()
        all_twothetas = flex.double()

        binned_offsets = [flex.double() for _ in range(n_bins)]
        binned_isigi = [flex.double() for _ in range(n_bins)]
        for expt_id in range(len(experiments)):
          refls = reflections.select(reflections['id'] == expt_id)
          s1 = refls['s1']
          s0 = flex.vec3_double(len(refls), experiments[expt_id].beam.get_s0())
          q = experiments[expt_id].crystal.get_A() * refls['miller_index'].as_vec3_double()
          wavelength = experiments[expt_id].beam.get_wavelength()
          offset = (q+s0).norms() - (1/wavelength)
          two_thetas = refls['two_theta_cal']
          if expt_id == 0:
            fig = plt.figure()
            plt.scatter(two_thetas, offset)
            plt.title(r"%d: Ewald offset ($\AA^{-1}$) vs $2\theta$ on %d spots"%(expt_id, len(two_thetas)))
            plt.xlabel(r"$2\theta (\circ)$")
            plt.ylabel(r"Ewald offset ($\AA^{-1}$)")

          array = refls.as_miller_array(experiments[expt_id])
          binner = array.setup_binner(d_min=2.0, n_bins=n_bins)
          isigi = refls['intensity.sum.value']/flex.sqrt(refls['intensity.sum.variance'])
          for bin_id, bin_number in enumerate(binner.range_used()):
            sel = binner.selection(bin_number)
            offsetsel = offset.select(sel)
            all_offsets.extend(offsetsel)
            all_twothetas.extend(two_thetas.select(sel))
            binned_offsets[bin_id].extend(offsetsel)
            binned_isigi[bin_id].extend(isigi.select(sel))

        fig = plt.figure()
        legend = []
        h = flex.histogram(all_offsets, n_slots=20)
        for bin_id, bin_number in enumerate(binner.range_used()):
          legend.append("%5.2f-%5.2f"%binner.bin_d_range(bin_number))
          x, y = [], []
          for slot_id, slot in enumerate(h.slot_infos()):
            sel = (binned_offsets[bin_id] >= slot.low_cutoff) & (binned_offsets[bin_id] <= slot.high_cutoff)
            x.append(slot.center())
            y.append(flex.median(binned_isigi[bin_id].select(sel)) if sel.count(True) else 0)
            print (bin_id, binner.bin_d_range(bin_number), sel.count(True), x[-1], y[-1])
          plt.plot(x, y, '-')
        plt.legend(legend)
        plt.title(r"Binned $I/\sigma_I$ vs. Ewald offset")
        plt.xlabel(r"Ewald offset ($\AA^{-1}$)")
        plt.ylabel(r"Median $I/\sigma_I$")

        plt.figure()
        plt.title('Ewald offsets vs. two theta')
        plt.hist2d(all_twothetas.as_numpy_array(), all_offsets.as_numpy_array(), bins=100)

      if self.params.save_pdf:
        pp = PdfPages('residuals_%s.pdf'%(tag.strip()))
        for i in plt.get_fignums():
          pp.savefig(plt.figure(i))
        pp.close()
      elif self.params.save_png:
        if len(tag) == 0:
          prefix = ""
        else:
          prefix = "%s_"%tag.strip()
        for i in plt.get_fignums():
          print("Saving figure", i)
          plt.figure(i).savefig("%sfig%02d.png"%(prefix, i))
      else:
        plt.show()

  def plot_multi_data(self, x, ygroups, xlabel, legends, ylabels, title):
    fig = plt.figure()
    from mpl_toolkits.axes_grid1 import host_subplot
    import mpl_toolkits.axisartist as AA

    host = host_subplot(111, axes_class=AA.Axes)
    plt.subplots_adjust(right=0.75)

    y1 = ygroups[0]
    ygroups = ygroups[1:]
    current_offset = 0
    offset = 60

    host.set_title(title)
    host.set_xlabel(xlabel)
    host.set_ylabel(ylabels[0])
    p1, = host.plot(x, y1, label=legends[0])
    host.axis["left"].label.set_color(p1.get_color())

    for y, legend, ylabel in zip(ygroups, legends[1:], ylabels[1:]):
      par = host.twinx()
      new_fixed_axis = par.get_grid_helper().new_fixed_axis
      par.axis["right"] = new_fixed_axis(loc="right",
                                         axes=par,
                                         offset=(current_offset, 0))
      par.axis["right"].toggle(all=True)
      current_offset += offset

      if isinstance(y, tuple):
        for data, l in zip(y, legend):
          p, = par.plot(x, data, label=l)
      else:
        p, = par.plot(x, y, label=legend)
        par.axis["right"].label.set_color(p.get_color())
      par.set_ylabel(ylabel)

    host.legend(loc='upper center', ncol=2, fancybox=True, shadow=True, fontsize=8)
    plt.draw()

  def plot_data_by_two_theta(self, reflections, tag):
    n_bins = 30
    arbitrary_padding = 1
    sorted_two_theta = flex.sorted(reflections['two_theta_obs'])
    bin_low = [sorted_two_theta[int((len(sorted_two_theta)/n_bins) * i)] for i in range(n_bins)]
    bin_high = [bin_low[i+1] for i in range(n_bins-1)]
    bin_high.append(sorted_two_theta[-1]+arbitrary_padding)

    title = "%sBinned data by two theta (n reflections per bin: %.1f)"%(tag, len(sorted_two_theta)/n_bins)

    x = flex.double()
    x_centers = flex.double()
    n_refls = flex.double()
    rmsds = flex.double()
    radial_rmsds = flex.double()
    transverse_rmsds = flex.double()
    rt_ratio = flex.double()
    #delta_two_theta = flex.double()
    rmsd_delta_two_theta = flex.double()

    for i in range(n_bins):
      x_centers.append(((bin_high[i]-bin_low[i])/2) + bin_low[i])
      refls = reflections.select((reflections['two_theta_obs'] >= bin_low[i]) & (reflections['two_theta_obs'] < bin_high[i]))
      n = len(refls)
      n_refls.append(n)
      if n == 0:
        rmsds.append(0)
        radial_rmsds.append(0)
        transverse_rmsds.append(0)
        rt_ratio.append(0)
        rmsd_delta_two_theta.append(0)
      else:
        rmsds.append(1000*math.sqrt(flex.sum_sq(refls['difference_vector_norms'])/n))
        radial_rmsds.append(1000*math.sqrt(flex.sum_sq(refls['radial_displacements'])/n))
        transverse_rmsds.append(1000*math.sqrt(flex.sum_sq(refls['transverse_displacements'])/n))
        rt_ratio.append(radial_rmsds[-1]/transverse_rmsds[-1])
        rmsd_delta_two_theta.append(math.sqrt(flex.sum_sq(refls['two_theta_obs']-refls['two_theta_cal'])/n))
      #delta_two_theta.append(flex.mean(refls['two_theta_obs']-refls['two_theta_cal']))
    assert len(reflections) == flex.sum(n_refls)

    self.plot_multi_data(x_centers,
                         [rt_ratio, (rmsds, radial_rmsds, transverse_rmsds), rmsd_delta_two_theta],
                         "Two theta (degrees)",
                         ["R/T RMSD",
                          ("RMSD","R RMSD","T RMSD"),
                          r"RMSD $\Delta2\Theta$"],
                         ["R/T RMSD ratio",
                          "Overall, radial, transverse RMSD (microns)",
                          r'$\Delta2\Theta RMSD (\circ)$'],
                         title)

  def histogram(self, reflections, title, plots = True, verbose = True):
    data = reflections['difference_vector_norms']
    n_slots = 100
    if self.params.residuals.histogram_max is None:
      h = flex.histogram(data, n_slots=n_slots)
    else:
      h = flex.histogram(data.select(data <= self.params.residuals.histogram_max), n_slots=n_slots)

    n = len(reflections)
    rmsd = math.sqrt((reflections['xyzcal.mm']-reflections['xyzobs.mm.value']).sum_sq()/n)
    sigma = mode = h.slot_centers()[list(h.slots()).index(flex.max(h.slots()))]
    mean = flex.mean(data)
    median = flex.median(data)
    if verbose: print("RMSD (microns)", rmsd * 1000)
    if verbose: print("Histogram mode (microns):", mode * 1000)
    if verbose: print("Overall mean (microns):", mean * 1000)
    if verbose: print("Overall median (microns):", median * 1000)
    mean2 = math.sqrt(math.pi/2)*sigma
    rmsd2 = math.sqrt(2)*sigma
    if verbose: print("Rayleigh Mean (microns)", mean2 * 1000)
    if verbose: print("Rayleigh RMSD (microns)", rmsd2 * 1000)

    r = reflections['radial_displacements']
    t = reflections['transverse_displacements']
    if verbose: print("Overall radial RMSD (microns)", math.sqrt(flex.sum_sq(r)/len(r)) * 1000)
    if verbose: print("Overall transverse RMSD (microns)", math.sqrt(flex.sum_sq(t)/len(t)) * 1000)

    if not plots: return

    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.plot(h.slot_centers().as_numpy_array(), h.slots().as_numpy_array(), '-')

    vmax = self.params.residuals.plot_max
    if self.params.residuals.histogram_xmax is not None:
      ax.set_xlim((0,self.params.residuals.histogram_xmax))
    if self.params.residuals.histogram_ymax is not None:
      ax.set_ylim((0,self.params.residuals.histogram_ymax))
    plt.title(title)


    ax.plot((mean, mean), (0, flex.max(h.slots())), 'g-')
    ax.plot((mean2, mean2), (0, flex.max(h.slots())), 'g--')
    ax.plot((mode, mode), (0, flex.max(h.slots())), 'r-')
    ax.plot((rmsd, rmsd), (0, flex.max(h.slots())), 'b-')
    ax.plot((rmsd2, rmsd2), (0, flex.max(h.slots())), 'b--')

    ax.legend([r"$\Delta$XY", "MeanObs", "MeanRayl", "Mode", "RMSDObs", "RMSDRayl"])
    ax.set_xlabel("(mm)")
    ax.set_ylabel("Count")

  def image_rmsd_histogram(self, reflections, tag, boxplot = True):
    data = flex.double()
    for i in set(reflections['id']):
      refls = reflections.select(reflections['id']==i)
      if len(refls) == 0:
        continue
      rmsd = math.sqrt(flex.sum_sq(refls['difference_vector_norms'])/len(refls))
      data.append(rmsd)
    data *= 1000
    h = flex.histogram(data, n_slots=40)
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.plot(h.slot_centers().as_numpy_array(), h.slots().as_numpy_array(), '-')
    plt.title("%sHistogram of image RMSDs"%tag)
    ax.set_xlabel("RMSD (microns)")
    ax.set_ylabel("Count")

    if not boxplot: return

    fig = plt.figure()
    ax = fig.add_subplot(111)
    plt.boxplot(data, vert=False)
    plt.title("%sBoxplot of image RMSDs"%tag)
    ax.set_xlabel("RMSD (microns)")

  def detector_plot_refls(self, detector, reflections, title, show=True, plot_callback=None, colorbar_units=None, new_fig = True):
    """
    Use matplotlib to plot a detector, color coding panels according to callback
    @param detector detector reference detector object
    @param title title string for plot
    @param units_str string with a formatting statment for units on each panel
    """
    if new_fig:
      fig = plt.figure()
      ax = fig.add_subplot(111, aspect='equal')
    else:
      fig = plt.gcf()
      ax = plt.gca()
    max_dim = 0
    for panel_id, panel in enumerate(detector):
      # get panel coordinates
      size = panel.get_image_size()
      p0 = col(panel.get_pixel_lab_coord((0,0)))
      p1 = col(panel.get_pixel_lab_coord((size[0]-1,0)))
      p2 = col(panel.get_pixel_lab_coord((size[0]-1,size[1]-1)))
      p3 = col(panel.get_pixel_lab_coord((0,size[1]-1)))
      bounds = (p0[0:2],p1[0:2],p2[0:2],p3[0:2])

      v1 = p1-p0
      v2 = p3-p0
      vcen = ((v2/2) + (v1/2)) + p0

     # add the panel to the plot
      ax.add_patch(Polygon(bounds, closed=True, fill=False))
      if self.params.panel_numbers:
        ax.annotate("%d"%(panel_id), vcen[0:2], ha='center')

      # find the plot maximum dimensions
      for p in [p0, p1, p2, p3]:
        for c in p[0:2]:
          if abs(c) > max_dim:
            max_dim = abs(c)

      panel_refls = reflections.select(reflections['panel'] == panel_id)
      if len(panel_refls) == 0:
        sm = color_vals = None
        continue

      if plot_callback is None:
        sm = color_vals = None
      else:
        result = plot_callback(panel_refls, panel, ax, bounds)
        if result is not None:
          sm, color_vals = result
        else:
          sm = color_vals = None

    # plot the results
    ax.set_xlim((-max_dim,max_dim))
    ax.set_ylim((-max_dim,max_dim))
    ax.set_xlabel("mm")
    ax.set_ylabel("mm")
    if sm is not None and color_vals is not None:
      if colorbar_units is None:
        colorbar_units = "mm"
      cb = ax.figure.colorbar(sm, ticks=color_vals, ax=ax)
      cb.ax.set_yticklabels(["%3.2f %s"%(i,colorbar_units) for i in color_vals])

    plt.title(title)

    if show:
      plt.show()

if __name__ == '__main__':
  with show_mail_on_error():
    script = Script()
    script.run()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/drift.py
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.drift

from __future__ import division
import sys
from xfel.util.drift import phil_scope, params_from_phil, run, message

if __name__ == '__main__':
  if '--help' in sys.argv[1:] or '-h' in sys.argv[1:]:
    print(message)
    exit()
  params = params_from_phil(phil_scope, sys.argv[1:])
  run(params)


 *******************************************************************************


 *******************************************************************************
xfel/command_line/ensemble_refine_pipeline.py
from __future__ import absolute_import, division, print_function
#
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.ensemble_refinement_pipeline
#

help_message = """
This is an MPI enabled pipeline to run several dials and cctbx.xfel commands in row.
Specifically dials.combine_experiments, dials.refine, cctbx.xfel.recompute_mosaicity,
and optionally cctbx.xfel.mpi_integrate
"""

from libtbx.phil import parse
from dials.util import show_mail_on_error
from libtbx.mpi4py import MPI, mpi_abort_on_exception

ensemble_refinement_pipline_str = """
combine_experiments_phil = None
  .type = path
  .help = Path to the phil file for dials.combine_experiments
refine_phil = None
  .type = path
  .help = Path to the phil file for dials.refine
recompute_mosaicity_phil = None
  .type = path
  .help = Path to the phil file for cctbx.xfel.recompute_mosaicity
integration_phil = None
  .type = path
  .help = Path to the phil file for cctbx.xfel.mpi_integrate
"""

phil_scope = parse(ensemble_refinement_pipline_str)

class Script(object):
  ''' Class to parse the command line options. '''

  def __init__(self):
    ''' Set the expected options. '''
    from dials.util.options import ArgumentParser
    import libtbx.load_env

    # Create the option parser
    usage = "usage: %s combine_experiments_phil=combine.phil refine_phil=refine.phil recompute_mosaicity_phil=recompute_mosaicity.phil integration_phil=integration.phil" % libtbx.env.dispatcher_name
    self.parser = ArgumentParser(
      usage=usage,
      phil=phil_scope,
      epilog=help_message)

  @mpi_abort_on_exception
  def run(self):
    ''' Parse the options. '''
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank() # each process in MPI has a unique id, 0-indexed
    size = comm.Get_size() # size: number of processes running in this job

    # Parse the command line arguments
    params, options = self.parser.parse_args(show_diff_phil=True if rank == 0 else False)

    if rank == 0:
      try:
        if params.combine_experiments_phil:
          from dials.command_line.combine_experiments import run
          run(args=[params.combine_experiments_phil])
        if params.refine_phil:
          from dials.command_line.refine import run
          run(args=[params.refine_phil])
        if params.recompute_mosaicity_phil:
          from xfel.command_line.recompute_mosaicity import Script as RecomputeScript
          script = RecomputeScript()
          script.run_with_preparsed(*script.parser.parse_args([params.recompute_mosaicity_phil], show_diff_phil=True))
      except Exception as e:
        if hasattr(e, 'message'):
          print(e.message)
        else:
          print(str(e))
        status = False
      else:
        status = True
    else:
      status = None
    status = comm.bcast(status, root=0)
    if not status:
      print("Rank %d shutting down due to job failure"%rank)
      return

    if params.integration_phil:
      import xfel.merging.command_line.mpi_integrate # reconfigure phils for integration
      from xfel.merging.command_line.merge import Script as IntegrateScript
      IntegrateScript().run(args=[params.integration_phil])

if __name__ == '__main__':
  with show_mail_on_error():
    script = Script()
    script.run()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/experiment_json_to_cbf_def.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.experiment_json_to_cbf_def

# Script to convert the output from a joint refinement using dials.refine to a CSPAD
# cbf header file. Note hardcoded distance of 100 isn't relevant for just a cbf header

from dials.util import show_mail_on_error
from dials.util.options import ArgumentParser
from dials.util.options import flatten_experiments
from xfel.cftbx.detector.cspad_cbf_tbx import write_cspad_cbf, map_detector_to_basis_dict
from libtbx import phil

phil_scope = phil.parse("""
  output_def_file = refined_detector.def
    .type = str
    .help = Name of output .def file
""")


class Script(object):
  def __init__(self):
    # Create the parser
    self.parser = ArgumentParser(
      phil = phil_scope,
      read_experiments = True,
      check_format = False)

  def run(self):
    params, options = self.parser.parse_args(show_diff_phil=True)
    experiments = flatten_experiments(params.input.experiments)

    detector = experiments[0].detector

    metro = map_detector_to_basis_dict(detector)
    write_cspad_cbf(None, metro, 'cbf', None, params.output_def_file, None, detector.hierarchy().get_distance(), header_only=True)

    print("Done")

if __name__ == '__main__':
  with show_mail_on_error():
    script = Script()
    script.run()



 *******************************************************************************


 *******************************************************************************
xfel/command_line/experiment_manager.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME xpp.experiment_manager

import iotbx.phil
from libtbx.utils import Usage, Sorry
import sys, os
import libtbx.load_env
from six.moves import input

master_phil = """
  experiment = None
    .type = str
  experiment_tag = None
    .type = str
  db {
    host = psdb.slac.stanford.edu
      .type = str
    name = None
      .type = str
    user = None
      .type = str
    password = None
      .type = str
  }
"""

def get_bool_from_user(prompt, default=True):
  yes = ['yes','y', 'ye']
  no = ['no','n']
  if default:
    yes.append('')
    prompt += " y/n [y] "
  else:
    no.append('')
    prompt += " y/n [n] "

  while True:
    response = input(prompt).lower()
    if response in yes:
      return True
    if response in no:
      return False

def get_optional_input(prompt):
  response = input(prompt)
  if response == "":
    return "NULL"
  else:
    return response

class initialize(object):
  expected_tables = ["runs", "jobs", "rungroups", "trials", "trial_rungroups", "isoforms", "frames", "observations", "hkls"]

  def __init__(self, params, dbobj, interactive = True, drop_tables = None):
    self.dbobj = dbobj
    self.params = params
    self.interactive = interactive
    self.do_drop_tables = drop_tables

  def __call__(self):
    if self.params.experiment is None:
      self.params.experiment = input("Administrate which experiment? ")

    if self.params.experiment_tag is None:
      if get_bool_from_user("Use experiment name as experiment tag?"):
        self.params.experiment_tag = self.params.experiment
      else:
        self.params.experiment_tag = input("Input an experiment tag: ")

    assert self.params.experiment is not None and self.params.experiment_tag is not None and len(self.params.experiment_tag) > 0

    print("Administering experiment", self.params.experiment, "using tag", self.params.experiment_tag)
    if self.interactive and self.do_drop_tables is None:
      if get_bool_from_user("Drop existing tables for %s?"%self.params.experiment_tag, default=False):
        self.drop_tables()
    elif self.do_drop_tables == True:
      self.drop_tables()

    if not self.verify_tables():
      self.create_tables()
      if not self.verify_tables():
        raise Sorry("Couldn't create experiment tables")

  def verify_tables(self):
    print("Checking tables...", end=' ')

    bools = []
    for table in self.expected_tables:
      cursor = self.dbobj.cursor()
      cmd = "SHOW TABLES LIKE '%s'"%(self.params.experiment_tag + "_" + table)
      cursor.execute(cmd)
      bools.append(cursor.rowcount > 0)

    if bools.count(True) == len(self.expected_tables):
      print("good to go")
      return True
    elif bools.count(False) == len(self.expected_tables):
      print("experiment tag not found")
      return False
    else:
      print("some tables are missing")
      return False

  def drop_tables(self):
    if self.interactive and input("Are you sure? Type drop: ").lower() != "drop":
      return

    print("Dropping tables...")
    for table in self.expected_tables:
      cmd = "SHOW TABLES LIKE '%s'"%(self.params.experiment_tag + "_" + table)
      cursor = self.dbobj.cursor()
      cursor.execute(cmd)
      if cursor.rowcount > 0:
        cmd = "SET FOREIGN_KEY_CHECKS=0; "
        cmd += "DROP TABLE IF EXISTS %s; "%(self.params.experiment_tag + "_" + table);
        cmd += "SET FOREIGN_KEY_CHECKS=1;"
        cursor = self.dbobj.cursor()
        cursor.execute(cmd);

  def create_tables(self, sql_path = None):
    print("Creating tables...")
    if sql_path is None:
      sql_path = os.path.join(libtbx.env.find_in_repositories("xfel/xpp"), "experiment_schema.sql")
    assert os.path.exists(sql_path)

    reading_create = False
    cmd = []
    f = open(sql_path)
    for line in f:
      line = line.strip()
      if not reading_create and "CREATE TABLE" in line:
        line = line.replace("`mydb`.`","`%s`.`%s_")%(self.params.db.name, self.params.experiment_tag)
        reading_create = True

      if "REFERENCES" in line:
        line = line.replace("`mydb`.`","`%s`.`%s_")%(self.params.db.name, self.params.experiment_tag)

      if "CONSTRAINT" in line:
        line = line.replace("`fk_","`fk_%s_")%self.params.experiment_tag

      if reading_create:
        cmd.append(line)
        if ";" in line:
          cmd = " ".join(cmd)
          cursor = self.dbobj.cursor()
          try:
            cursor.execute(cmd)
          except Exception as e:
            print("Failed to create table. SQL command:")
            print(cmd)
            print(e)

          cmd = []
          reading_create = False

    f.close()

class option_chooser(object):
  menu_string = ""
  options = None

  def __init__(self, params, dbobj):
    self.params = params
    self.dbobj = dbobj

  def __call__(self):
    while True:
      print("############################")
      print(self.menu_string)
      for o in self.options:
        print(o, ":", self.options[o].selection_string)
      print("h : get help on a selection")
      print("q : exit this menu")

      i = input("Selection: ").lower()
      print("############################")
      if i == 'h':
        i = input("Help on which item? ")
        if i in self.options:
          print(self.options[i].help_string)
        else:
          print("Option not found:", i)
      elif i == 'q':
        return
      elif i not in self.options:
        print("Option not found:", i)
      else:
        self.options[i](self.params, self.dbobj)()

class db_action(object):
  def __init__(self, params, dbobj):
    self.params = params
    self.dbobj = dbobj

class isoforms_menu(option_chooser):
  selection_string = "Manage isoforms"
  help_string = "Use this menu to manage known crystal isoforms refined during indexing and integration"

  def __init__(self, params, dbobj):
    option_chooser.__init__(self, params, dbobj)
    self.menu_string = "Managing isoforms"
    self.options = {}

class runs_menu(option_chooser):
  selection_string = "Manage runs"
  help_string = "Use this menu to manage runs for this experiment, including adding and removing tags"

  class list_runs(db_action):
    selection_string = "List all runs"
    help_string = "List all runs being logged so far, including any tags on them"

    def __call__(self):
      cmd = "SELECT * from %s_runs"%self.params.experiment_tag
      cursor = self.dbobj.cursor()
      cursor.execute(cmd)
      if cursor.rowcount == 0:
        print("No runs logged yet")
      else:
        for entry in cursor.fetchall():
          id, run, tags = entry
          print("Run %s. Tags: %s"%(run, tags))

  class update_tags(db_action):
    selection_string = "Update run tags"
    help_string = "Add or remove tags from runs"

    def get_runs_from_user(self, prompt):
      if get_bool_from_user("%s All runs? "%prompt):
        return ""
      else:
        start = input("%s Starting with run: "%prompt)
        end = input("%s Ending with run: "%prompt)
        return "WHERE %s_runs.run >= %s and %s_runs.run <= %s"%(
          self.params.experiment_tag, start, self.params.experiment_tag, end)

    def __call__(self):
      choice = input("Add or remove tag? [a/r]: ").lower()
      if choice == 'a':
        prompt = "Add tag."
      elif choice == 'r':
        prompt = "Remove tag."
      else:
        print("Choice not recognized")
        return
      tag = input("Enter tag: ")
      cursor = self.dbobj.cursor()

      cmd = "SELECT * from %s_runs %s"%(self.params.experiment_tag, self.get_runs_from_user(prompt))
      cursor.execute(cmd)
      for entry in cursor.fetchall():
        run_id, run, tags = entry
        if choice == 'a':
          if tags is None or tags == "":
            tags = ""
            comma = ""
          else:
            comma = ","
          if tag not in tags.split(','):
            tags += comma + tag
        else:
          if tags is None or tag not in tags:
            continue
          tags = tags.split(',')
          while tag in tags:
            tags.remove(tag)
          tags = ','.join(tags)

        cmd = "UPDATE %s_runs SET tags='%s' WHERE run_id=%s"%(self.params.experiment_tag, tags, run_id)
        cursor.execute(cmd)

      self.dbobj.commit()
      print("Run tags updated")

  def __init__(self, params, dbobj):
    option_chooser.__init__(self, params, dbobj)
    self.menu_string = "Managing runs"
    self.options = {
      'l': runs_menu.list_runs,
      'u': runs_menu.update_tags
    }

class trials_menu(option_chooser):
  selection_string = "Manage trials"
  help_string = "Use this menu to manage trials for this experiment, including add new trials or inactivating old ones"

  class list_trials(db_action):
    selection_string = "List all trials"
    help_string = "List all trials being used with this experiment tag"

    def __call__(self):
      cmd = "SELECT * from %s_trials"%self.params.experiment_tag
      cursor = self.dbobj.cursor()
      cursor.execute(cmd)
      if cursor.rowcount == 0:
        print("No trials set up yet")
      else:
        for entry in cursor.fetchall():
          id, trial, active, target, comment = entry
          active = bool(active)
          print("Trial %s. Active: %s, target: %s, comment: %s"%(trial, active, target, comment))

  class add_trial(db_action):
    selection_string = "Add trial"
    help_string = "Add a trial for use with this experiment tag"

    def __call__(self):
      trial = input("Trial number: ")
      active = get_bool_from_user("Make trial active? ")
      target = input("Path to target phil file: ")
      comment = input("Add a comment: ")

      cmd = "INSERT INTO %s_trials (trial,active,target_phil_path,comment) VALUES (%s,%s,'%s','%s')"%(self.params.experiment_tag, trial, active, target, comment)
      cursor = self.dbobj.cursor()
      cursor.execute(cmd)
      self.dbobj.commit()
      print("Trial added")

  class update_trial(db_action):
    selection_string = "Update trial"
    help_string = "Activate/inactivate a trial or change its comment"

    def __call__(self):
      trial = input("Trial number: ")
      active = get_bool_from_user("Make trial active? ")
      comment = input("New comment: ")

      cmd = "UPDATE %s_trials SET active=%s, comment='%s' WHERE trial=%s"%(self.params.experiment_tag, active, comment, trial)
      cursor = self.dbobj.cursor()
      cursor.execute(cmd)
      self.dbobj.commit()
      print("Trial updated")

  def __init__(self, params, dbobj):
    option_chooser.__init__(self, params, dbobj)
    self.menu_string = "Managing trials"
    self.options = {
      'l': trials_menu.list_trials,
      'a': trials_menu.add_trial,
      'u': trials_menu.update_trial
    }

class link_trials_to_rungroup_menu(option_chooser):
  selection_string = "Link trials to run groups"
  help_string = "Use this menu to link trials to specific run groups for processing"

  class list_links(db_action):
    selection_string = "List all trial/rungroup links"
    help_string = "List all links between trials and rungroups"

    def __call__(self):
      cmd = "SELECT %s_trial_rungroups.trial_rungroup_id, %s_trials.trial_id, %s_rungroups.rungroup_id, %s_trials.trial, %s_rungroups.startrun, %s_rungroups.endrun, %s_trial_rungroups.active FROM %s_trials JOIN %s_trial_rungroups ON %s_trials.trial_id = %s_trial_rungroups.trials_id JOIN %s_rungroups ON %s_rungroups.rungroup_id = %s_trial_rungroups.rungroups_id"%tuple([self.params.experiment_tag]*14)
      cursor = self.dbobj.cursor()
      cursor.execute(cmd)
      if cursor.rowcount == 0:
        print("No links set up yet")
      else:
        print("Link Trial  RG Start run End run   Active")
        for entry in cursor.fetchall():
          link_id, trial_id, rungroup_id, trial, startrun, endrun, active = entry
          active = bool(active)
          if endrun is None:
            print("% 4d % 5d % 3d % 9d       +   %s"%(link_id, trial, rungroup_id, startrun, active))
          else:
            print("% 4d % 5d % 3d % 9d % 7d   %s"%(link_id, trial, rungroup_id, startrun, endrun, active))

  class add_link(db_action):
    selection_string = "Add trial/rungroup link"
    help_string = "Link a trial to a rungroup for processing"

    def __call__(self):
      trial = input("Trial: ")
      rungroup_id = input("Run group id: ")
      active = get_bool_from_user("Make link active? ")

      cmd = "SELECT trial_id from %s_trials where %s_trials.trial = %s"%(self.params.experiment_tag, self.params.experiment_tag, trial)
      cursor = self.dbobj.cursor()
      cursor.execute(cmd)
      assert cursor.rowcount <= 1
      if cursor.rowcount == 0:
        print("Trial %s not found."%trial)
        return
      trial_id = cursor.fetchall()[0][0]

      cmd = "SELECT rungroup_id from %s_rungroups where %s_rungroups.rungroup_id = %s"%(self.params.experiment_tag, self.params.experiment_tag, rungroup_id)
      cursor = self.dbobj.cursor()
      cursor.execute(cmd)
      assert cursor.rowcount <= 1
      if cursor.rowcount == 0:
        print("Rungroup %s not found."%rungroup_id)
        return

      cmd = "INSERT INTO %s_trial_rungroups (trials_id, rungroups_id, active) VALUES (%s,%s,%s)"%(self.params.experiment_tag, trial_id, rungroup_id, active)

      cursor = self.dbobj.cursor()
      cursor.execute(cmd)
      self.dbobj.commit()
      print("Link added")

  class update_link(db_action):
    selection_string = "Update trial/rungroup links"
    help_string = "Activate or inactivate trial/rungroup links"

    def __call__(self):
      link_id = input("Trial/rungroup link to change: ")
      active = get_bool_from_user("Make link active? ")

      cmd = "UPDATE %s_trial_rungroups SET active=%s WHERE trial_rungroup_id=%s"%(self.params.experiment_tag, active, link_id)
      cursor = self.dbobj.cursor()
      cursor.execute(cmd)
      self.dbobj.commit()
      print("Link updated")

  def __init__(self, params, dbobj):
    option_chooser.__init__(self, params, dbobj)
    self.menu_string = "Managing trial/rungroup links"
    self.options = {
      'l': link_trials_to_rungroup_menu.list_links,
      'a': link_trials_to_rungroup_menu.add_link,
      'u': link_trials_to_rungroup_menu.update_link
    }

class rungroups_menu(option_chooser):
  selection_string = "Manage run groups"
  help_string = "Use this menu to manage settings on groups of runs"

  class list_rungroups(db_action):
    selection_string = "List all run groups"
    help_string = "List all run groups being used with this experiment tag"

    def __call__(self):
      cmd = "SELECT rungroup_id,startrun,endrun,comment from %s_rungroups"%self.params.experiment_tag
      cursor = self.dbobj.cursor()
      cursor.execute(cmd)
      if cursor.rowcount == 0:
        print("No run groups set up yet")
      else:
        print("RG  Start run End run   Comment")
        for entry in cursor.fetchall():
          id, startrun, endrun, comment = entry
          if endrun is None:
            print("% 3d % 9d       +   %s"%(id, startrun, comment))
          else:
            print("% 3d % 9d % 7d   %s"%(id, startrun, endrun, comment))

  class add_rungroup(db_action):
    selection_string = "Add run group"
    help_string = "Add a run group defining a set of runs with the same parameters"

    def __call__(self):
      startrun = input("Start run: ")
      endrun = get_optional_input("End run (leave blank if the last run in this group hasn't been collected yet): ")
      detz_parameter = input("Detz parameter (CXI: detz_offset, XPP: distance): ")
      beamx = get_optional_input("Beam center x (leave blank to not override): ")
      beamy = get_optional_input("Beam center y (leave blank to not override): ")
      pixelmask = get_optional_input("Path to untrusted pixel mask (if available): ")
      darkavg = get_optional_input("Path to dark average image (if available): ")
      darkstddev = get_optional_input("Path to dark standard deviation image (if available): ")
      gainmap = get_optional_input("Path to gain map image (if available): ")
      binning = get_optional_input("Binning (if applicable): ")
      #usecase = get_optional_input("Use case (list available cases here):")
      comment = input("Add a comment: ")

      cmd = "INSERT INTO %s_rungroups (startrun,endrun,detz_parameter,beamx,beamy,untrusted_pixel_mask_path,dark_avg_path,dark_stddev_path,gain_map_path,binning,comment) VALUES (%s,%s,%s,%s,%s,'%s','%s','%s','%s',%s,'%s')"%(self.params.experiment_tag, startrun, endrun, detz_parameter, beamx, beamy, pixelmask, darkavg, darkstddev, gainmap, binning, comment)
      cursor = self.dbobj.cursor()
      cursor.execute(cmd)
      self.dbobj.commit()
      print("Run group added")

  class update_rungroup(db_action):
    selection_string = "Update run group"
    help_string = "Change run group start and end values and comments"

    def __call__(self):
      rungroup = input("Run group number: ")
      startrun = input("New start run: ")
      endrun = get_optional_input("New end run (leave blank if the last run in this group hasn't been collected yet): ")
      comment = input("New comment: ")

      cmd = "UPDATE %s_rungroups SET startrun=%s, endrun=%s, comment='%s' WHERE rungroup_id=%s"%(self.params.experiment_tag, startrun, endrun, comment, rungroup)
      cursor = self.dbobj.cursor()
      cursor.execute(cmd)
      self.dbobj.commit()
      print("Run group updated")

  def __init__(self, params, dbobj):
    option_chooser.__init__(self, params, dbobj)
    self.menu_string = "Managing run groups"
    self.options = {
      'l': rungroups_menu.list_rungroups,
      'a': rungroups_menu.add_rungroup,
      'u': rungroups_menu.update_rungroup
    }

class top_menu(option_chooser):
  def __init__(self, params, dbobj):
    option_chooser.__init__(self, params, dbobj)

    self.menu_string = "Top level menu, administering %s using tag %s"%(self.params.experiment, self.params.experiment_tag)

    self.options = {
      'r': runs_menu,
      'i': isoforms_menu,
      't': trials_menu,
      'g': rungroups_menu,
      'l': link_trials_to_rungroup_menu
    }

def run(args):
  try:
    from cxi_xdr_xes.cftbx.cspad_ana import db as db
  except ImportError:
    raise Sorry("Trial logging not supported for this installation. Contact the developers for access.")

  phil = iotbx.phil.process_command_line(args=args, master_string=master_phil)
  params = phil.work.extract()

  if params.db.host is None:
    raise Usage("Please provide a host name")
  if params.db.name is None:
    raise Usage("Please provide a database name")
  if params.db.user is None:
    raise Usage("Please provide a user name")
  if params.db.password is None:
    import getpass
    password = getpass.getpass()
  else:
    password = params.db.password

  try:
    dbobj = db.dbconnect(host=params.db.host, db=params.db.name, username=params.db.user, password=password)
  except Exception as e:
    raise Sorry(e)

  initialize(params, dbobj)()
  top_menu(params, dbobj)()

  print("Done")

if __name__ == "__main__":
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/experiment_residuals.py
from __future__ import absolute_import, division, print_function

# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.experiment_residuals

from libtbx.phil import parse
import numpy as np
import pylab as plt
import sys
import os
from dials.util import show_mail_on_error
import h5py

help_message = '''
Visualize prediction offsets for a single shot experiment

Example:

  cctbx.xfel.experiment_residuals refined.expt indexed.refl
'''

phil_scope = parse('''
lscale = 25
  .type = float
  .help = scale the offset vector by this amount
lcolor = #777777
  .type = str
  .help = display the offset vector with this color
scatt_cmap = bwr
  .type = str
  .help = display the scatter points with this pylab colormap
clim = None
  .type = floats(size=2)
  .help = colormap limits e.g. clim=[-0.01, 0.01]
axcol = w
  .type = str
  .help = pylab axis face color
mark_scale = 15
  .type = int
  .help = scale of scatter plot marker
edge_color = #777777
  .type = str
  .help = color of marker edge
edge_width = 0.5
  .type = float
  .help = width of marker edge
headlen = 0.5
  .type = float
  .help = length of pylab arrow head
headwid = 0.5
  .type = float
  .help = width of pylab arrow head
noarrow = False
  .type = bool
  .help = do not add arrows to plot
cbarshrink = 0.5
  .type = float
  .help = factor by which to shrink the displayed colorbar
exper_id = 0
  .type = int
  .help = experiment id (if experiment file is multi-shot)
title_tag = None
  .type = str
  .help = add this tag to the title of the plots
plot_rad_trans = False
  .type = bool
  .help = if True, will display a histogram of radial and transverse offsets
output {
  file = None
    .type = str
    .help = path to an optional hdf5 file storing useful output
  overwrite = False
    .type = bool
    .help = force an overwrite
  only = False
    .type = bool
    .help = if True, only write an output file, and don't display plots
}
''', process_includes=True)


class Script:

  def __init__(self):
    from dials.util.options import ArgumentParser

    self.parser = ArgumentParser(
      usage="",
      sort_options=True,
      phil=phil_scope,
      read_experiments=True,
      read_reflections=True,
      check_format=False,
      epilog=help_message)

  def run(self):
    from dials.util.options import flatten_experiments, flatten_reflections
    params, options = self.parser.parse_args(show_diff_phil=True)

    if len(params.input.experiments) > 1:
      print("Please only pass a single experiment file. Exiting...")
      sys.exit()
    if len(params.input.reflections) > 1:
      print("Please only pass a single reflection file. Exiting...")
      sys.exit()

    # do stuff
    ax = plt.gca()

    El = flatten_experiments(params.input.experiments)
    R = flatten_reflections(params.input.reflections)[0]

    nexper = len(El)
    nexper_in_refl = len(set(R["id"]))
    if not nexper == nexper_in_refl:
      print("There are %d experiments and %d possible reflection sets, experiment and reflection table out of sync"
            % (nexper, nexper_in_refl))
      sys.exit()
    if params.exper_id < 0:
      print("Exper Id must be greater than 0")
      sys.exit()
    if params.exper_id > nexper:
      print("exper_id must be less than maximum number of experiments (=%d)" % nexper)
      sys.exit()

    DET = El[params.exper_id].detector
    pixsize = tuple([x*1000 for x in DET[0].get_pixel_size()])
    R = R.select(R["id"] == params.exper_id)

    columns = 'delpsical.rad', 'xyzobs.mm.value', 'panel', 'xyzcal.mm'
    misses = 0
    for col in columns:
      if col not in R[0].keys():
        print("reflection file missing column %s" % col)
        misses += 1
    if misses > 0:
      print("Please reformat refl file to include above columns")
      sys.exit()

    nref = len(R)

    xyz = np.zeros((nref, 3))
    all_rad = []
    all_trans = []
    all_diff = []
    for i_ref in range(nref):
      x, y, _ = R[i_ref]["xyzobs.mm.value"]
      xcal, ycal, _ = R[i_ref]["xyzcal.mm"]
      pid = R[i_ref]['panel']
      panel = DET[pid]
      xyz_lab = panel.get_lab_coord((x,y))
      xyz_cal_lab = panel.get_lab_coord((xcal, ycal))
      xyz[i_ref] = xyz_lab

      diff = np.array(xyz_lab) - np.array(xyz_cal_lab)

      # rad is the unit vector pointing to the observation
      xy_lab = np.array((xyz_lab[0], xyz_lab[1]))
      rad = xy_lab / np.linalg.norm(xy_lab)
      trans = np.array([-rad[1], rad[0]])

      rad_component = np.dot(diff[:2], rad)
      trans_component = np.dot(diff[:2], trans)

      diff_scale = diff*params.lscale
      x, y, _ = xyz_lab
      ax.arrow(x, y, diff_scale[0], diff_scale[1], head_width=params.headwid, head_length=params.headlen, color=params.lcolor,
               length_includes_head=not params.noarrow)
      all_rad.append(rad_component*1000)
      all_trans.append(trans_component*1000)
      all_diff.append(diff*1000)

    if params.output.file is not None:
      if os.path.exists(params.output.file) and not params.output.overwrite:
        print("File %s exists, to overwrite use output.overwrite=True" % params.output.file)
        sys.exit()
      with h5py.File(params.output.file, 'w') as h5:
        h5.create_dataset("radial_offset", data=all_rad)
        h5.create_dataset("transverse_offset", data=all_trans)
        h5.create_dataset("vectors_from_obs_to_cal", data=all_diff)
        exper_filename = params.input.experiments[0].filename
        refl_filename = params.input.reflections[0].filename
        h5.create_dataset("exper_filename", data=np.array([exper_filename], dtype=np.string_))
        h5.create_dataset("refl_filename", data=np.array([refl_filename], dtype=np.string_))
      print("Output saved to file %s" % params.output.file)
      if params.output.only:
        print("Done.")
        sys.exit()

    delpsi = R['delpsical.rad']
    xvals, yvals, zvals = xyz.T

    vmax = max(abs(delpsi))
    vmin = -vmax
    if params.clim is not None:
      vmin, vmax = params.clim

    scatt_arg = xvals, yvals
    scat = ax.scatter(*scatt_arg, s=params.mark_scale, c=delpsi, cmap=params.scatt_cmap, vmin=vmin, vmax=vmax, zorder=2,
                      edgecolors=params.edge_color, linewidths=params.edge_width)

    cbar = plt.colorbar(scat, shrink=params.cbarshrink)

    cbar.ax.set_title(r"$\Delta \psi$")
    ax.set_aspect("equal")
    ax.set_facecolor(params.axcol)
    title = "prediction offsets (arrow points to prediction)"
    title_radtrans = "radial/transverse components"
    if params.title_tag is not None:
      title += ": %s" % params.title_tag
      title_radtrans += ": %s" % params.title_tag
    ax.set_title(title)

    if params.plot_rad_trans:
      plt.figure()
      plt.hist(all_rad, bins='auto', histtype='step')
      plt.hist(all_trans, bins='auto', histtype='step')
      plt.gca().set_title(title_radtrans)
      rad_mn, rad_sig = np.mean(all_rad), np.std(all_rad)
      trans_mn, trans_sig = np.mean(all_trans), np.std(all_trans)
      rad_str = r"radial: %.2f $\pm$ %.2f $\mu m$" % (rad_mn, rad_sig)
      trans_str = r"transverse: %.2f $\pm$ %.2f $\mu m$" % (trans_mn, trans_sig)
      plt.legend((rad_str, trans_str))
      plt.xlabel(r"microns (1 pixel = %.1f $\mu m$ x %.1f $\mu m$)" % pixsize)
      plt.ylabel("number of spots")

    plt.show()


if __name__ == '__main__':
  with show_mail_on_error():
    script = Script()
    script.run()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/fee_calibration.py
from __future__ import division

from libtbx.phil import parse
from libtbx.utils import Sorry
import psana
from matplotlib import pyplot as plt
from serialtbx.util.energy_scan_notch_finder import notch_phil_string, find_notch, plot_notches, calibrate_energy

"""When an energy scan is conducted at LCLS, we acquire FEE spectra of the incident beam with a varying, known, narrow energy band removed -- the "notch". Energy calibration is the process of identifying the notch in each scan and using the known pixel-energy pairs to generate a function (linear fit) returning the energy for any given pixel position on the spectrometer (reported FEE energy in xtc streams). This file automates this process. Helper functions are located in serialtbx in case energy calibration can be useful outside the LCLS use case."""

fee_phil_string = """
experiment = None
  .type = str
  .help = experiment identifier at LCLS, e.g. mfxl1013621
verbose = False
  .type = bool
  .help = print all possible output
output_phil = None
  .type = path
  .help = path where calibrated values should be written as a phil file
max_events = 1000
  .type = int
  .help = use at most max_events per run
"""

phil_scope = parse(fee_phil_string + notch_phil_string)

def tally_fee_data(experiment, runs, plot=True, verbose=True, max_events=None):
  """Check each event of each requested run in the specified experiment for a FEE spectrometer event. Report how many events are missing. Return spectrometer data if present."""
  good = 0
  bad = 0
  events = []
  rundata = []

  for r in runs:
    print(f"Processing run {r}...".format())
    ds = psana.DataSource(f'exp={experiment}:run={r}:idx'.format())
    d = psana.Detector('FEE-SPEC0')
    pr = list(ds.runs())[0]
    times = pr.times()
    data = None
    total = 0
    if max_events is None:
      iterable = range(len(times))
    else:
      iterable = range(min(len(times), max_events))
    for i in iterable:
      e = pr.event(times[i])
      f = d.get(e)
      if f:
        if verbose:
          print(r, i)
        good += 1
        events.append(1)
      else:
        if verbose:
          print(r, i, 'no fee')
        bad += 1
        events.append(0)
        continue
      if data is None:
        data = f.hproj().astype(float)
      else:
        data += f.hproj().astype(float)
      total += 1
    if verbose:
      print(total)
    data /= total
    rundata.append(data)
    print(f"Found {good} events with FEE, {bad} events without ({good+bad} total)".format())
  if plot:
    plt.plot(range(len(events)), events, '-')
    plt.title("FEE presence over time")
    plt.xlabel("Event number")
    plt.ylabel("FEE present (1 yes, 0 no)")
    plt.figure()
  return rundata

def run(args):
  user_phil = []
  runs = []
  energies = []
  for arg in args:
    if ':' in arg: # interpret as tuple of run number and known notch energy
      try:
        srun, senergy = arg.split(':')
        runs.append(int(srun))
        energy = energies.append(int(senergy))
      except Exception:
        raise Sorry("Run numbers and known energies must be supplied as colon-separated pairs (without spaces), e.g. \"5:9415 6:9405 7:9395\"")
    else:
      try:
        user_phil.append(parse(arg))
      except Exception:
        raise Sorry("Unrecognized argument %s"%arg)
  if not runs:
    raise Sorry("Run numbers and known energies must be supplied as colon-separated pairs (without spaces), e.g. \"5:9415 6:9405 7:9395\"")

  params = phil_scope.fetch(sources=user_phil).extract()
  rundata = tally_fee_data(params.experiment, runs, verbose=params.verbose, max_events=params.max_events)
  notches = [find_notch(range(len(data)),
                        data,
                        params.kernel_size,
                        params.fit_half_range,
                        params.baseline_cutoff,
                        ref_spectrum=params.reference_spectrum)
             for data in rundata]
  plot_notches(runs, rundata, notches, params.per_run_plots)
  try:
    eV_offset, eV_per_pixel = calibrate_energy(notches, energies)
    args_str = ' '.join(args)
    with open('fee_calib.out', 'a') as outfile:
      outfile.write(f'using {args_str}, eV_offset={eV_offset} eV_per_pixel={eV_per_pixel}\n')
    print('wrote calibrated values to fee_calib.out')
    if params.output_phil:
      with open(params.output_phil, 'w') as outfile:
        outfile.write(f'spectrum_eV_offset={eV_offset}\n')
        outfile.write(f'spectrum_eV_per_pixel={eV_per_pixel}\n')
      print(f'wrote calibrated values to {params.output_phil}')
  except SystemError as e:
    print(e)
  if params.per_run_plots:
    plt.show()

if __name__ == "__main__":
  import sys
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/fee_view.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.fee_view
#
import sys, os
import psana
from psmon import publish
from psmon.plots import Image, XYPlot
from libtbx.phil import parse
from xfel.cxi.spectra_filter import spectra_filter
from xfel.cxi.cspad_ana import cspad_tbx

"""
Example usage:
cd ~/myrelease
sit_setup
psplot SPECTRUM FEE &
cxi.fee_view params.phil selected_filter=best

For full output, use:
psplot SPECTRUM FEE DC_OFFSET ALL_FEE ALL_FEE_RAW &
"""
phil_scope = parse("""
  selected_filter = None
    .type = str
    .help = Which filter to apply
  runs = None
    .type = str
    .help = Set of runs to display, eg 96 or 95-114 or 95,97,99-101, etc.
  experiment = None
    .type = str
    .help = Experiment name, e.g. cxid9114
  skip_events = None
    .type = int
    .help = Optionally skip N events
  include scope xfel.cxi.spectra_filter.phil_scope
""", process_includes = True)

def run(args):
  user_phil = []
  for arg in args:
    if os.path.isfile(arg):
      try:
        user_phil.append(parse(file_name=arg))
      except Exception as e:
        print(str(e))
        raise Sorry("Couldn't parse phil file %s"%arg)
    else:
      try:
        user_phil.append(parse(arg))
      except Exception as e:
        print(str(e))
        raise Sorry("Couldn't parse argument %s"%arg)
  params = phil_scope.fetch(sources=user_phil).extract()

  # cxid9114, source fee: FeeHxSpectrometer.0:Opal1000.1, downstream: CxiDg3.0:Opal1000.0
  # cxig3614, source fee: FeeHxSpectrometer.0:OrcaFl40.0

  src = psana.Source(params.spectra_filter.detector_address)
  dataset_name = "exp=%s:run=%s:idx"%(params.experiment, params.runs)
  print("Dataset string:", dataset_name)
  ds = psana.DataSource(dataset_name)
  spf = spectra_filter(params)

  if params.selected_filter == None:
    filter_name = params.spectra_filter.filter[0].name
  else:
    filter_name = params.selected_filter

  rank = 0
  size = 1
  max_events = sys.maxsize

  for run in ds.runs():
    print("starting run", run.run())
    # list of all events
    times = run.times()

    if params.skip_events is not None:
      times = times[params.skip_events:]

    nevents = min(len(times),max_events)

    # chop the list into pieces, depending on rank.  This assigns each process
    # events such that the get every Nth event where N is the number of processes
    mytimes = [times[i] for i in range(nevents) if (i+rank)%size == 0]

    for i, t in enumerate(mytimes):
      evt = run.event(t)
      accepted, data, spectrum, dc_offset, all_data, all_data_raw = spf.filter_event(evt, filter_name)

      if not accepted:
        continue

      print(cspad_tbx.evt_timestamp(cspad_tbx.evt_time(evt)), "Publishing data for event", i)

      #header = "Event %d, m/f: %7.7f, f: %d"%(i, peak_max/flux, flux)
      header = "Event %d"%(i)

      if rank == 0:
        fee = Image(header, "FEE", data) # make a 2D plot
        publish.send("FEE", fee) # send to the display

        spectrumplot = XYPlot(header, 'summed 1D trace', list(range(data.shape[1])), spectrum) # make a 1D plot
        publish.send("SPECTRUM", spectrumplot) # send to the display

        fee = Image(header, "DC_OFFSET", dc_offset) # make a 2D plot
        publish.send("DC_OFFSET", fee) # send to the display
        fee = Image(header, "ALL_FEE", all_data) # make a 2D plot
        publish.send("ALL_FEE", fee) # send to the display
        fee = Image(header, "ALL_FEE_RAW", all_data_raw) # make a 2D plot
        publish.send("ALL_FEE_RAW", fee) # send to the display

if __name__ == "__main__":
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/filter_experiments_by_rmsd.py
#!/usr/bin/env python
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# filter_experiments_by_rmsd.py
#
#  Copyright (C) 2016 Lawrence Berkeley National Laboratory (LBNL)
#
#  Author: Aaron Brewster and David Waterman
#
#  This code is distributed under the X license, a copy of which is
#  included in the root directory of this package.
#
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.filter_experiments_by_rmsd
#
from __future__ import absolute_import, division, print_function
from six.moves import range
from dials.array_family import flex
from dials.util import show_mail_on_error
from scitbx.matrix import col
from libtbx.phil import parse
import libtbx.load_env
import math

help_message = '''
Filter a set of experiments and reflections from a multi-experiment job by overall RMSD
using Tukey's rule of thumb.  I.E, for each experiment, determine the RMSD of the
differences between preditions - observations. Then compute the five number summary of
this set of per-image RMSDs. Then, filter outliers more than iqr_multiplier times the
interquartile range from the third quartile. When x=1.5, this is Tukey's rule.

Example:

  %s combined.expt combined.refl
''' % libtbx.env.dispatcher_name

# Create the phil parameters
phil_scope = parse('''
iqr_multiplier = 1.5
  .type = float
  .help = Interquartile multiplier
show_plots = False
  .type = bool
  .help = Show some plots
max_delta = None
  .type = float
  .help = Before filtering, throw out all reflections with obs-pred greater \
          that max_delta mm.
detector = None
  .type = int
  .help = If not None, only filter experiments matching this detector number
output {
  filtered_experiments = filtered.expt
    .type = str
    .help = Name of output filtered experiments file
  filtered_reflections = filtered.refl
    .type = str
    .help = Name of output filtered reflections file
}
delta_psi_filter = None
  .type = float(value_min=0)
  .help = After RMSD filter, filter remaining reflections by delta psi \
          angle (degrees).
''')

def run_with_preparsed(experiments, reflections, params):
  from dxtbx.model import ExperimentList
  from scitbx.math import five_number_summary

  print("Found", len(reflections), "reflections", "and", len(experiments), "experiments")

  filtered_reflections = flex.reflection_table()
  filtered_experiments = ExperimentList()

  skipped_reflections = flex.reflection_table()
  skipped_experiments = ExperimentList()

  if params.detector is not None:
    culled_reflections = flex.reflection_table()
    culled_experiments = ExperimentList()
    detector = experiments.detectors()[params.detector]
    for expt_id, experiment in enumerate(experiments):
      refls = reflections.select(reflections['id']==expt_id)
      if experiment.detector is detector:
        culled_experiments.append(experiment)
        refls['id'] = flex.int(len(refls), len(culled_experiments)-1)
        culled_reflections.extend(refls)
      else:
        skipped_experiments.append(experiment)
        refls['id'] = flex.int(len(refls), len(skipped_experiments)-1)
        skipped_reflections.extend(refls)

    print("RMSD filtering %d experiments using detector %d, out of %d"%(len(culled_experiments), params.detector, len(experiments)))
    reflections = culled_reflections
    experiments = culled_experiments

  difference_vector_norms = (reflections['xyzcal.mm']-reflections['xyzobs.mm.value']).norms()

  if params.max_delta is not None:
    sel = difference_vector_norms <= params.max_delta
    reflections = reflections.select(sel)
    difference_vector_norms = difference_vector_norms.select(sel)

  data = flex.double()
  counts = flex.double()
  for i in range(len(experiments)):
    dvns = difference_vector_norms.select(reflections['id']==i)
    counts.append(len(dvns))
    if len(dvns) == 0:
      data.append(0)
      continue
    rmsd = math.sqrt(flex.sum_sq(dvns)/len(dvns))
    data.append(rmsd)
  data *= 1000
  subset = data.select(counts > 0)
  print(len(subset), "experiments with > 0 reflections")

  if params.show_plots:
    from matplotlib import pyplot as plt
    h = flex.histogram(subset, n_slots=40)
    fig = plt.figure()
    ax = fig.add_subplot('111')
    ax.plot(h.slot_centers().as_numpy_array(), h.slots().as_numpy_array(), '-')
    plt.title("Histogram of %d image RMSDs"%len(subset))

    fig = plt.figure()
    plt.boxplot(subset, vert=False)
    plt.title("Boxplot of %d image RMSDs"%len(subset))
    plt.show()

  outliers = counts == 0
  min_x, q1_x, med_x, q3_x, max_x = five_number_summary(subset)
  print("Five number summary of RMSDs (microns): min %.1f, q1 %.1f, med %.1f, q3 %.1f, max %.1f"%(min_x, q1_x, med_x, q3_x, max_x))
  iqr_x = q3_x - q1_x
  cut_x = params.iqr_multiplier * iqr_x
  outliers.set_selected(data > q3_x + cut_x, True)
  #outliers.set_selected(col < q1_x - cut_x, True) # Don't throw away the images that are outliers in the 'good' direction!

  for i in range(len(experiments)):
    if outliers[i]:
      continue
    refls = reflections.select(reflections['id']==i)
    refls['id'] = flex.int(len(refls), len(filtered_experiments))
    filtered_reflections.extend(refls)
    filtered_experiments.append(experiments[i])

  #import IPython;IPython.embed()
  zeroes = counts == 0
  n_zero = len(counts.select(zeroes))
  print("Removed %d bad experiments and %d experiments with zero reflections, out of %d (%%%.1f)"%(
    len(experiments)-len(filtered_experiments)-n_zero,
    n_zero,
    len(experiments),
    100*((len(experiments)-len(filtered_experiments))/len(experiments))))

  if params.detector is not None:
    crystals = filtered_experiments.crystals()
    for expt_id, experiment in enumerate(skipped_experiments):
      if experiment.crystal in crystals:
        filtered_experiments.append(experiment)
        refls = skipped_reflections.select(skipped_reflections['id'] == expt_id)
        refls['id'] = flex.int(len(refls), len(filtered_experiments)-1)
        filtered_reflections.extend(refls)

  if params.delta_psi_filter is not None:
    delta_psi = filtered_reflections['delpsical.rad']*180/math.pi
    sel = (delta_psi <= params.delta_psi_filter) & (delta_psi >= -params.delta_psi_filter)
    l = len(filtered_reflections)
    filtered_reflections = filtered_reflections.select(sel)
    print("Filtering by delta psi, removing %d out of %d reflections"%(l - len(filtered_reflections), l))

  print("Final experiment count", len(filtered_experiments))
  return filtered_experiments, filtered_reflections

class Script(object):
  ''' Class to parse the command line options. '''

  def __init__(self):
    ''' Set the expected options. '''
    from dials.util.options import ArgumentParser
    import libtbx.load_env

    # Create the option parser
    usage = "usage: %s combined.expt combined.refl" % libtbx.env.dispatcher_name
    self.parser = ArgumentParser(
      usage=usage,
      sort_options=True,
      phil=phil_scope,
      read_experiments=True,
      read_reflections=True,
      check_format=False,
      epilog=help_message)


  def run(self):
    ''' Parse the options. '''
    from dials.util.options import flatten_experiments, flatten_reflections
    # Parse the command line arguments
    params, options = self.parser.parse_args(show_diff_phil=True)
    experiments = flatten_experiments(params.input.experiments)
    reflections = flatten_reflections(params.input.reflections)[0]

    filtered_experiments, filtered_reflections = run_with_preparsed(experiments, reflections, params)
    filtered_experiments.as_file(params.output.filtered_experiments)
    filtered_reflections.as_pickle(params.output.filtered_reflections)

if __name__ == '__main__':
  with show_mail_on_error():
    script = Script()
    script.run()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/find_highres_shots.py
from __future__ import absolute_import, division, print_function
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.find_highres_shots
#
import os, glob
import libtbx.load_env
from dials.util import show_mail_on_error
from dials.util.options import ArgumentParser
from libtbx.phil import parse
from libtbx import easy_pickle
from scitbx.array_family import flex
import six

help_message = """
Script to find high resolution shots from XFEL data
Example usage:
cxi.find_highres_shots data=r000[6-9]/001/integration data=r001[1-4]/001/integration \
  min_good_reflections=100 min_IsigI=10

This looks at each integration pickle and selects the strong reflections (I/sigI >= 10).  Then,
we throw out all images with less than 100 strong reflections.  For the images that are left,
we find the 10 images whose strong reflections diffract to the highest resolution.  The
command prints out a cctbx.image_viewer command you can directly run to see these images.
"""

phil_str = '''
  data = None
    .type = str
    .multiple = True
    .help = Paths to integration dirs
  max_images = 10
    .type = int
    .help = Find at most this many images
  min_resolution = None
    .type = float
    .help = Mininmum resolution to accept
  min_IsigI = 1.0
    .type = float
    .help = Minimum accepted value for I/sigI for determining number of good reflections
  min_good_reflections = None
    .type = int
    .help = Only accept images with at least this many measurements with I/sigI >= min_IsigI
  max_to_examine = None
    .type = int
    .help = Only examine this many images
'''

phil_scope = parse(phil_str)

class Script(object):
  """ Script to find good images """
  def __init__(self):
    """ Set up the option parser. Arguments come from the command line or a phil file """
    self.usage = "%s data=path"%(libtbx.env.dispatcher_name)
    self.parser = ArgumentParser(
      usage = self.usage,
      phil = phil_scope,
      epilog = help_message)

  def run(self):
    """ Find the high res images """

    params, options = self.parser.parse_args(
      show_diff_phil=True)

    assert params.max_images is not None
    assert params.min_IsigI is not None

    final_data = {}
    worst_resolution = None
    worst_image = None
    n_tested = 0

    all_paths = []
    for item in params.data:
      all_paths.extend(glob.glob(item))

    # loop through the integration pickles
    for path in all_paths:
      for filename in os.listdir(path):
        if not os.path.splitext(filename)[1] == ".pickle":
          continue

        filepath = os.path.join(path, filename)
        try:
          data = easy_pickle.load(filepath)
        except Exception as e:
          print("Couldn't read", filepath)
          continue

        if not 'observations' in data:
          print("Not an integration pickle", filepath)
          continue

        n_tested += 1

        # Find the critical parameters for this image
        obs = data['observations'][0]
        uc = obs.unit_cell()
        obs_strong = obs.select((obs.data()/obs.sigmas() >= params.min_IsigI))
        strong_measurements = len(obs_strong.indices())
        d = uc.d(obs_strong.indices())
        resolution = flex.min(d)

        if params.min_resolution is not None and resolution > min_resolution:
          print("Rejecting", filepath, "because resolution", resolution, "is too low")
          continue

        if params.min_good_reflections is not None and strong_measurements < params.min_good_reflections:
          print("Rejecting", filepath, "because number of measurements where I/sigmaI >=", params.min_IsigI, strong_measurements, "is too low")
          continue

        # Save the image if it's good enough
        if len(final_data) < params.max_images:
          print("Accepting", filepath, "not yet reached maximum number good images")
          final_data[filepath] = data, resolution
          if worst_resolution is None or resolution > worst_resolution:
            worst_resolution = resolution
            worst_image = filepath
        elif resolution < worst_resolution:
          print("Accepting", filepath, "better than the worst image")
          final_data.pop(worst_image)
          final_data[filepath] = data, resolution

          worst_resolution = 0
          for key, entry in six.iteritems(final_data):
            data, resolution = entry
            if resolution > worst_resolution:
              worst_resolution = resolution
              worst_image = key
        else:
          print("Rejecting", filepath)

        if params.max_to_examine is not None and n_tested > params.max_to_examine: break
      if params.max_to_examine is not None and n_tested > params.max_to_examine: break

    # Results
    print("Final best images and resolutions:")
    for key in final_data:
      print(key, final_data[key][1])

    # Show a viewing command
    cmd = "cctbx.image_viewer "

    for key in final_data:
      dirname = os.path.dirname(key).replace('integration', 'out')
      basename = os.path.basename(key)

      # tsl: time stamp long, tss: time stamp short
      if basename.endswith("_00000.pickle"):
        tsl = basename.lstrip('int-').split('_00000.pickle')[0]
      elif basename.endswith(".pickle"):
        tsl = os.path.splitext(basename.lstrip('int-'))[0]

      tss = ''.join([c for c in tsl if c in '1234567890'])

      cmd += (os.path.join(dirname, "idx-%s.pickle "%tss))

    print("Use this command to display these images:")
    print(cmd.rstrip())

if __name__ == "__main__":
  with show_mail_on_error():
    script = Script()
    script.run()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/frame_extractor.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.frame_extractor
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.frame_extractor
#
# $Id: frame_extractor.py idyoung $

from dials.util.options import Importer, flatten_reflections, flatten_experiments, ArgumentParser
import iotbx.phil
import cctbx, os, glob
from libtbx import easy_pickle
from six.moves import zip
from serialtbx.util.construct_frame import ConstructFrame

phil_scope = iotbx.phil.parse("""
  input {
    experiments = None
      .type = path
      .help = path to an experiments.expt file
    reflections = None
      .type = path
      .help = path to a reflection table (integrated.refl) file
  }
  output {
    filename = None
      .type = str
      .help = if set, name of final pickle file
    dirname = None
      .type = path
      .help = if set, path to directory to save the new pickle file
    }
    """)

class ConstructFrameFromFiles(ConstructFrame):
  def __init__(self, refl_name, json_name, outname=None):
    # load the integration.refl file (reflection table) into memory and
    # load the experiments.expt file (json) into memory, piecewise.
    # check_format=False because we don't want to load any imagesets in the
    # experiment list
    importer = Importer([refl_name, json_name], read_experiments=True, read_reflections=True, check_format=False)
    if importer.unhandled:
      print("unable to process:", importer.unhandled)
    reflections_l = flatten_reflections(importer.reflections)
    experiments_l = flatten_experiments(importer.experiments)
    assert len(experiments_l) == 1, "cannot construct a single frame from multiple experiments"
    frame = ConstructFrame.__init__(self, reflections_l[0], experiments_l[0])
    if frame is not None:
      self.frame.make_frame()

def construct_frames_from_files(refl_name, json_name, outname=None, outdir=None):
  importer = Importer([refl_name, json_name], read_experiments=True, read_reflections=True, check_format=False)
  if importer.unhandled:
    print("unable to process:", importer.unhandled)
  reflections_l = flatten_reflections(importer.reflections)[0]
  experiments_l = flatten_experiments(importer.experiments)
  frames = []
  if outdir is None:
    outdir = '.'
  if outname is None:
    outname = 'int-%d' + refl_name.split('.pickle')[0] + '_extracted.pickle'
  elif '%' not in outname:
    outname = outname.split(".pickle")[0] + ("_%d.pickle")
  for i in range(len(experiments_l)):
    refl = reflections_l.select(reflections_l['id'] == i)
    if len(refl) == 0: continue
    expt = experiments_l[i]
    frame = ConstructFrame(refl, expt).make_frame()
    name = outname % i
    easy_pickle.dump(os.path.join(outdir, name), frame)

if __name__ == "__main__":
  parser = ArgumentParser(phil=phil_scope)
  params, options = parser.parse_args(show_diff_phil=True)
  if params.output.dirname is not None:
    assert os.path.isdir(params.output.dirname)
  for refl_name, json_name in zip(sorted(glob.glob(params.input.reflections)), sorted(glob.glob(params.input.experiments))):
    if params.output.filename is None:
      basename = os.path.basename(refl_name)
      name = os.path.splitext(basename)[0] + "_extracted.pickle"
    else:
      name = params.output.filename
    construct_frames_from_files(refl_name, json_name, outname=name, outdir=params.output.dirname)


 *******************************************************************************


 *******************************************************************************
xfel/command_line/frame_unpickler.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# LIBTBX_SET_DISPATCHER_NAME frame.unpickler
#
# $Id: frame_unpickler.py idyoung $

from dials.array_family import flex
from scitbx.array_family import flex as sciflex
from libtbx import easy_pickle
from libtbx.utils import Sorry
from dials.util.options import ArgumentParser
from dxtbx.model import BeamFactory, DetectorFactory
from dxtbx.format import FormatMultiImage
from dxtbx.model import Experiment, ExperimentList
from dxtbx import imageset
import dxtbx
import iotbx.phil
import os

def int_pickle_to_filename(int_name, prefix, suffix):
  parts = int_name.split("-")
  parts = parts[0:3] + parts[3].split(":")
  parts = parts[0:4] + parts[4].split(".")
  parts = parts[0:5] + parts[5].split("_")
  outname = prefix + parts[1] + parts[2] \
    + parts[3][0:2] + parts[3][3:5] + parts[4][0:2] + parts[4][3:5] + parts[5] + suffix
  return outname

def find_matching_img(pickle, img_location=None):
  # find the image associated with the pickle file to be processed, or use the pickle file itself as a placeholder
  if pickle is None:
    raise Sorry("Cannot find matching image for NoneType in place of pickle")
  if img_location is None:
    if os.path.exists(pickle.split(".pickle")[0] + ".cbf"):
      return pickle.split(".pickle")[0] + ".cbf"
    elif os.path.exists(os.path.join(os.path.dirname(pickle), "..", "out")):
      loc = os.path.join(os.path.dirname(pickle), "..", "out")
      name = os.path.basename(pickle).split(".pickle")[0]
      imgname = int_pickle_to_filename(name, "idx-", ".pickle")
      if os.path.exists(os.path.join(loc, imgname)):
        return os.path.join(loc, imgname)
      else:
        return None
    else:
      return None
  else:
    loc = img_location
    name = os.path.basename(pickle).split(".pickle")[0]
    imgname = int_pickle_to_filename(name, "idx-", ".pickle")
    if os.path.exists(os.path.join(loc, imgname)):
      return os.path.join(loc, imgname)
    else:
      return None

class construct_reflection_table_and_experiment_list(object):
  def __init__(self, pickle, img_location, pixel_size, proceed_without_image=False):
    # unpickle pickle file and keep track of image location
    if img_location is None:
      if proceed_without_image:
        self.img_location = []
      else:
        raise Sorry("No image found at specified location. Override by setting proceed_without_image to False"
        + "to produce experiment lists that may only be read when check_format is False.")
    else:
      self.img_location = [img_location]

    # pickle can be already unpickled, if so, loading it will fail with an AttributeError. A load
    # error will fail with EOFError
    try:
      self.data = easy_pickle.load(pickle)
    except EOFError:
      self.data = None
      self.pickle = None
    except AttributeError:
      self.data = pickle
      self.pickle = None
    else:
      self.pickle = pickle
    if self.data is not None:
      self.length = len(self.data['observations'][0].data())
      self.pixel_size = pixel_size

  # extract things from pickle file
  def unpack_pickle(self):
    """Extract all relevant information from an integration pickle file."""

    # crystal-dependent
    self.ori = self.data['current_orientation'][0]
    self.ucell = self.data['current_orientation'][0].unit_cell()

    # experiment-dependent
    self.wavelength = self.data['wavelength']
    self.det_dist = self.data['distance']

    # observation-dependent
    self.observations = self.data['observations'][0]
    self.predictions = self.data['mapped_predictions'][0]

    if 'fuller_kapton_absorption_correction' in self.data:
      self.fuller_correction = self.data['fuller_kapton_absorption_correction'][0]
      if 'fuller_kapton_absorption_correction_sigmas' in self.data:
        self.fuller_correction_sigmas = self.data['fuller_kapton_absorption_correction_sigmas'][0]

  # construct the experiments and experiment_list objects
  def expt_beam_maker(self):
    """Construct the beam object for the experiments file."""
    self.beam = BeamFactory.simple(self.wavelength)

  def expt_crystal_maker(self):
    """Construct the crystal object for the experiments file."""
    a, b, c = self.ucell.parameters()[0:3]
    direct_matrix = self.ori.direct_matrix()
    real_a = direct_matrix[0:3]
    real_b = direct_matrix[3:6]
    real_c = direct_matrix[6:9]
    lattice = self.ucell.lattice_symmetry_group()
    found_it = False
    if 'ML_half_mosaicity_deg' in self.data:
      assert 'ML_domain_size_ang' in self.data
      if d['ML_half_mosaicity_deg'][0] is None or d['ML_domain_size_ang'][0] is None:
        assert d['ML_half_mosaicity_deg'][0] is None and d['ML_domain_size_ang'][0] is None
      else:
        found_it = True
        if 'mosaicity' in self.data and self.data['mosaicity'] > 0:
          print("Warning, two kinds of mosaicity found. Using Sauter2014 model")
        from dxtbx.model import MosaicCrystalSauter2014
        self.crystal = MosaicCrystalSauter2014(real_a, real_b, real_c, space_group=lattice)
        self.crystal.set_half_mosaicity_deg(self.data['ML_half_mosaicity_deg'][0])
        self.crystal.set_domain_size_ang(self.data['ML_domain_size_ang'][0])
    if not found_it:
      if 'mosaicity' in self.data:
        from dxtbx.model import MosaicCrystalKabsch2010
        self.crystal = MosaicCrystalKabsch2010(real_a, real_b, real_c, space_group=lattice)
        self.crystal.set_mosaicity(self.data['mosaicity'])
      else:
        from dxtbx.model import Crystal
        self.crystal = Crystal(real_a, real_b, real_c, space_group=lattice)
    if 'identified_isoform' in self.data and self.data['identified_isoform'] is not None:
      self.crystal.identified_isoform = self.data['identified_isoform']

  def expt_detector_maker(self):
    """Construct the detector object for the experiments file. This function generates a monolithic flattening of the
    CSPAD detector if not supplied with an image file."""
    self.distance = self.data['distance']
    self.xbeam, self.ybeam = self.data['xbeam'], self.data['ybeam']
    if len(self.img_location) > 0 and not dxtbx.load(self.img_location[0])._image_file.endswith("_00000.pickle"):
      self.detector = dxtbx.load(self.img_location[0])._detector()
    else:
      self.detector = DetectorFactory.simple('SENSOR_UNKNOWN',self.distance,(self.xbeam, self.ybeam),'+x','-y',
      (self.pixel_size, self.pixel_size),(1765,1765))

  def expt_gonio_maker(self):
    """XFEL data consisting of stills is expected to have been generated by an experiment without a goniometer -- use placeholder None."""
    self.goniometer = None

  def expt_imageset_maker(self):
    """Construct the imageset object for the experiments file."""
    if len(self.img_location) == 0:
      self.imageset = None
      return
    self.filename = self.img_location
    self.format = FormatMultiImage.FormatMultiImage()
    self.reader = imageset.MultiFileReader(self.format, self.filename)
    self.imageset = imageset.ImageSet(self.reader)

  def expt_scan_maker(self):
    """XFEL data consisting of stills is expected not to contain scans -- use placeholder None."""
    self.scan = None

  def assemble_experiments(self):
    self.unpack_pickle()
    self.expt_beam_maker()
    self.expt_crystal_maker()
    self.expt_detector_maker()
    self.expt_gonio_maker()
    self.expt_imageset_maker()
    self.expt_scan_maker()
    self.experiment = Experiment(beam = self.beam,
                                                 crystal=self.crystal,
                                                 detector=self.detector,
                                                 goniometer=self.goniometer,
                                                 imageset=self.imageset,
                                                 scan=self.scan)
    self.experiment_list = ExperimentList([self.experiment])

  def experiments_to_json(self, path_name=None):
    if path_name is None:
      loc = os.path.dirname(self.pickle)
    else:
      loc = path_name
    name = os.path.basename(self.pickle).split(".pickle")[0]
    expt_name = int_pickle_to_filename(name, "idx-", ".expt")
    experiments = os.path.join(loc, expt_name)
    self.experiment_list.as_file(experiments)

  # construct the reflection table
  def refl_table_maker(self):
    self.reflections = flex.reflection_table()

  def refl_bkgd_maker(self):
    self.reflections['background.mean'] = sciflex.double(self.length)
    self.reflections['background.mse'] = sciflex.double(self.length)

  def refl_bbox_maker(self):
    self.reflections['bbox'] = flex.int6(self.length)

  def refl_correlation_maker(self):
    self.reflections['correlation.ideal.profile'] = sciflex.double(self.length)

  def refl_entering_maker(self):
    self.reflections['entering'] = flex.bool(self.length)

  def refl_flags_maker(self):
    self.reflections['flags'] = flex.size_t(self.length, 1)

  def refl_id_maker(self):
    self.reflections['id'] = sciflex.size_t(self.length)

  def refl_intensities_maker(self):
    self.reflections['intensity.sum.value'] = self.observations.data()
    self.reflections['intensity.sum.variance'] = self.observations.sigmas()**2

  def refl_lp_maker(self):
    self.reflections['lp'] = sciflex.double(self.length)

  def refl_millers_maker(self):
    self.reflections['miller_index'] = self.observations._indices

  def refl_nbackgroundforeground_maker(self):
    self.reflections['n_background'] = sciflex.size_t(self.length)
    self.reflections['n_foreground'] = sciflex.size_t(self.length)

  def refl_panel_maker(self):
    self.reflections['panel'] = sciflex.size_t(self.length, 0)

  def refl_profile_maker(self):
    self.reflections['profile.correlation'] = sciflex.double(self.length)

  def refl_s1_maker(self):
    from scitbx.matrix import col
    self.reflections['s1'] = sciflex.vec3_double(self.length)
    for idx in range(self.length):
      coords = col(self.detector[0].get_pixel_lab_coord(self.reflections['xyzobs.px.value'][idx][0:2])).normalize()
      self.reflections['s1'][idx] = tuple(coords)

  def refl_xyzcal_maker(self):
    self.reflections['xyzcal.px'] = sciflex.vec3_double(self.predictions.parts()[1], self.predictions.parts()[0], sciflex.double(self.length, 0.0))
    self.reflections['xyzcal.mm'] = self.pixel_size * self.reflections['xyzcal.px']

  def refl_xyzobs_maker(self):
    self.reflections['xyzobs.px.value'] = sciflex.vec3_double(self.predictions.parts()[1], self.predictions.parts()[0], sciflex.double(self.length, 0.5))
    self.reflections['xyzobs.px.variance'] = sciflex.vec3_double(self.length, (0.0,0.0,0.0))

  def refl_zeta_maker(self):
    self.reflections['zeta'] = sciflex.double(self.length)

  def refl_kapton_correction_maker(self):
    if hasattr(self, 'fuller_correction'):
      self.reflections['kapton_absorption_correction'] = self.fuller_correction
      if hasattr(self, 'fuller_correction_sigmas'):
        self.reflections['kapton_absorption_correction_sigmas'] = self.fuller_correction_sigmas

  def assemble_reflections(self):
    self.refl_table_maker()
    self.refl_bkgd_maker()
    self.refl_bbox_maker()
    self.refl_correlation_maker()
    self.refl_entering_maker()
    self.refl_flags_maker()
    self.refl_id_maker()
    self.refl_intensities_maker()
    self.refl_millers_maker()
    self.refl_nbackgroundforeground_maker()
    self.refl_panel_maker()
    self.refl_xyzcal_maker()
    self.refl_xyzobs_maker()
    self.refl_zeta_maker()
    self.refl_kapton_correction_maker()
    self.refl_s1_maker() # depends on successful completion of refl_xyz_obs_maker

  def reflections_to_pickle(self, path_name=None):
    if path_name is None:
      loc = os.path.dirname(self.pickle)
    else:
      loc = path_name
    name = os.path.basename(self.pickle).split(".refl")[0]
    refl_name = int_pickle_to_filename(name, "idx-", "_integrated.refl")
    reflections = os.path.join(loc, refl_name)
    self.reflections.as_pickle(reflections)

if __name__ == "__main__":
  master_phil_scope = iotbx.phil.parse("""
    pickle_location = None
      .type = path
      .help = supply complete path to integration pickle to be unpacked.
    img_location = None
      .type = path
      .help = supply complete path to image as either cbf or image pickle. If None, will attempt to locate cbfs
      .help = in the same directory or the sibling out directory
    json_location = None
      .type = path
      .help = supply complete path to new json directory. If None, will place all generated files in the
      .help = same directory as the integration pickles.
    refl_location = None
      .type = path
      .help = supply complete path to new reflection table directory. If None, will place all generated
      .help = files in the same directory as the integration pickles.
    pixel_size = 0.11
      .type = float
      .help = detector-specific parameter for pixel size in mm
    proceed_without_image = False
      .type = bool
      .help = override raising exceptions if no image can be found. Write a json with a dummy image (pickle name)
    """)
  parser = ArgumentParser(phil=master_phil_scope)
  params, options = parser.parse_args(show_diff_phil=False)
  for pickle_file in os.listdir(params.pickle_location):
    pickle_path = os.path.join(params.pickle_location, pickle_file)
    result = construct_reflection_table_and_experiment_list(pickle_path, find_matching_img(pickle_path, params.img_location), params.pixel_size, proceed_without_image=params.proceed_without_image)
    if result.data is not None:
      result.assemble_experiments()
      result.assemble_reflections()
      result.experiments_to_json(params.json_location)
      result.reflections_to_pickle(params.refl_location)
    else:
      print("Skipping unreadable pickle file at", pickle_path)
  print('Generated experiments.expt and integrated.refl files.')


 *******************************************************************************


 *******************************************************************************
xfel/command_line/get_next_trial_id.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.get_next_trial_id
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1

from cxi_xdr_xes.cftbx.cspad_ana import db

if (__name__ == "__main__") :
  print(db.get_next_trial_id())


 *******************************************************************************


 *******************************************************************************
xfel/command_line/h5_average.py
from __future__ import absolute_import, division, print_function

# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.h5_average

import numpy as np
import h5py
import iotbx.phil
import libtbx.load_env
from dials.util.options import ArgumentParser
import sys, os
from six.moves import range

phil_scope = iotbx.phil.parse("""
  average = True
    .type = bool
    .help = Generate average image
  max = True
    .type = bool
    .help = Generate maximum projection
  stddev = True
    .type = bool
    .help = Generate standard deviation image
""")

class Processh5(object):
  """
  Compute the average ("avg"), maximum projection ("max") or standard deviation ("stddev") of a
  series of images provided as a multi-image hdf5 file.
  """
  def __init__(self, h5file):
    # read everything from a multi-image h5 file
    self.h5file = h5file
    self.readfile = h5py.File(h5file, "r")
    self.shape_xy = self.readfile.values()[1].values()[0].value.shape
    self.length = len(self.readfile.values())
    #from IPython import embed; embed()

  def prepare_writefile_and_array(self, func):
    writefile_parts = (os.path.basename(os.path.splitext(self.h5file)[0]),
                       os.path.splitext(self.h5file)[1])
    writefile_name = ("_"+func).join(writefile_parts)
    writefile = h5py.File(writefile_name, "w")
    arr = np.zeros((self.length, self.shape_xy[0], self.shape_xy[1]))
    self.readfile.copy(str(self.readfile.values()[1].name), writefile['/'])
    return (writefile, writefile_name, arr)

  def process(self, func):
    # process and write everything to a new avg (or other) file
    writefile, writefile_name, arr = self.prepare_writefile_and_array(func)
    for ii in range(self.length-1):
      arr[ii][:][:] = np.asarray(self.readfile.values()[ii+1].values()[0].value)
    func_lookup = {
      "avg":np.mean,
      "max":np.max,
      "stddev":np.std
    }
    f = func_lookup[func]
    res = f(arr, axis=0)

    # write results to new file
    val = writefile.values()[0].values()[0]
    val.write_direct(res)
    writefile.close()
    print("Wrote", writefile_name)

  def cleanup(self):
    self.readfile.close()

def run(args):
  if ("--help" in args) or ("-h" in args) or (len(args) == 0):
    print("Usage: %s r25792.h5" % libtbx.env.dispatcher_name)
    return
  elif ("--config" in args) or ("-c" in args):
    iotbx.phil.parse(phil_scope).show(attributes_level=2)
    return
  h5s = []
  for arg in args:
    if arg.endswith(".h5") or arg.endswith(".hdf5"):
      h5s.append(args.pop(args.index(arg)))
  sys.argv = [sys.argv[0]] + args
  parser = ArgumentParser(phil=phil_scope)
  params, options = parser.parse_args(show_diff_phil=True)
  for h5 in h5s:
    print("Processing image %s..." % h5)
    processor = Processh5(h5)
    if params.average:
      processor.process("avg")
    if params.max:
      processor.process("max")
    if params.stddev:
      processor.process("stddev")
    processor.cleanup()

if __name__ == "__main__":
  run(sys.argv[1:])



 *******************************************************************************


 *******************************************************************************
xfel/command_line/histogram_finalise.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.hist_finalise
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT

import sys

from libtbx.option_parser import option_parser
from xfel.cxi.cspad_ana.histogram_finalise import histogram_finalise

if __name__ == '__main__':
  args = sys.argv[1:]
  assert len(args) > 0
  command_line = (option_parser()
                  .option("--output_dirname", "-o",
                          type="string",
                          help="Directory for output files.")
                  .option("--pickle_pattern",
                          type="string",
                          help="regex for matching pickle files.")
                  ).process(args=args)
  output_dirname = command_line.options.output_dirname
  pickle_pattern = command_line.options.pickle_pattern
  runs = command_line.args
  if output_dirname is None:
    output_dirname = runs[0]
  print("Output directory: %s" %output_dirname)
  histogram_finalise(output_dirname, runs, pickle_pattern=pickle_pattern)


 *******************************************************************************


 *******************************************************************************
xfel/command_line/jobs_sentinel.py
# LIBTBX_SET_DISPATCHER_NAME xpp.jobs_sentinel

"""
Program to monitor for new runs and submit them for processing. Default is to check for
and submit new runs at 0.1 Hz.

Example usage:
xpp.jobs_sentinel db.name=xppi6115 db.user=xppi6115 experiment=xppi6115 experiment_tag=debug
"""
from __future__ import absolute_import, division, print_function

import iotbx.phil
import libtbx.load_env
from libtbx.utils import Usage, Sorry
import sys, os, time
from six.moves import zip

master_phil = """
  experiment = None
    .type = str
  experiment_tag = None
    .type = str
  config_dir = "."
    .type = str
  output_dir = "."
    .type = str
  dry_run = False
    .type = bool
  mp {
    nproc = 1
      .type = int
      .help = Number of processes
    queue = "psanacsq"
      .type = str
      .help = Queue to submit MPI job to
  }
  db {
    host = psdb.slac.stanford.edu
      .type = str
    name = None
      .type = str
    user = None
      .type = str
    password = None
      .type = str
  }
"""

def parse_entry(entry):
  result = []
  for item in entry:
    if item == "NULL":
      result.append(None)
    else:
      result.append(item)
  return result

def write_hitfind_cfg(params, dbobj, trial_id, trial, rungroup_id):
  assert os.path.exists(params.config_dir)

  config_path = os.path.join(params.config_dir, "%s_%s_t%03d_rg%03d.cfg"%(params.experiment, params.experiment_tag, trial, rungroup_id))

  cmd = "SELECT target_phil_path FROM %s_trials where %s_trials.trial_id = %d"%(params.experiment_tag, params.experiment_tag, trial_id)
  cursor = dbobj.cursor()
  cursor.execute(cmd)
  assert cursor.rowcount == 1
  target_phil_path = parse_entry(cursor.fetchall()[0])[0]

  cmd = "SELECT detz_parameter, beamx, beamy, untrusted_pixel_mask_path, dark_avg_path, dark_stddev_path, gain_map_path, binning FROM %s_rungroups where %s_rungroups.rungroup_id = %d"%(params.experiment_tag, params.experiment_tag, rungroup_id)
  cursor = dbobj.cursor()
  cursor.execute(cmd)
  assert cursor.rowcount == 1
  detz_parameter, beamx, beamy, untrusted_pixel_mask_path, dark_avg_path, dark_stddev_path, gain_map_path, binning = parse_entry(cursor.fetchall()[0])

  template = open(os.path.join(libtbx.env.find_in_repositories("xfel/xpp/cfgs"),"index_all.cfg"))
  cfg = open(config_path, 'w')

  d = dict(
    default_calib_dir         = libtbx.env.find_in_repositories("xfel/metrology/CSPad/run4/CxiDs1.0_Cspad.0"),
    trial_id                  = trial_id,
    rungroup_id               = rungroup_id,
    dark_avg_path             = dark_avg_path,
    dark_stddev_path          = dark_stddev_path,
    untrusted_pixel_mask_path = untrusted_pixel_mask_path,
    detz_parameter            = detz_parameter,
    gain_map_path             = gain_map_path,
    beamx                     = beamx,
    beamy                     = beamy,
    binning                   = binning,
    db_name                   = params.db.name,
    db_experiment_tag         = params.experiment_tag,
    db_user                   = params.db.user,
    db_password               = params.db.password,
    target_phil_path          = target_phil_path)

  for line in template.readlines():
    cfg.write(line.format(**d))

  template.close()
  cfg.close()
  return config_path

def submit_job(params, dbobj, trial_id, trial, rungroup_id, run, config_path):
  assert os.path.exists(config_path)
  submit_phil_path = os.path.join(params.config_dir, "%s_%s_r%04d_t%03d_rg%03d_submit.phil"%(params.experiment, params.experiment_tag, run, trial, rungroup_id))


  template = open(os.path.join(libtbx.env.find_in_repositories("xfel/xpp/cfgs"), "submit.phil"))
  phil = open(submit_phil_path, "w")

  d = dict(dry_run = params.dry_run,
    cfg = config_path,
    experiment = params.experiment,
    run_num = run,
    trial = trial,
    rungroup = rungroup_id,
    output_dir = params.output_dir,
    nproc = params.mp.nproc,
    queue = params.mp.queue
  )

  for line in template.readlines():
    phil.write(line.format(**d))

  template.close()
  phil.close()

  from xfel.command_line.cxi_mpi_submit import Script as submit_script
  submit_script().run([submit_phil_path])

def run(args):
  try:
    from cxi_xdr_xes.cftbx.cspad_ana import db as db
  except ImportError:
    raise Sorry("Trial logging not supported for this installation. Contact the developers for access.")

  phil = iotbx.phil.process_command_line(args=args, master_string=master_phil)
  params = phil.work.extract()

  if params.db.host is None:
    raise Usage("Please provide a host name")
  if params.db.name is None:
    raise Usage("Please provide a database name")
  if params.db.user is None:
    raise Usage("Please provide a user name")
  if params.db.password is None:
    import getpass
    password = getpass.getpass()
  else:
    password = params.db.password

  while True:
    print("Checking for new jobs to submit")
    # need to get a new connection each iteration to prevent caching
    try:
      dbobj = db.dbconnect(host=params.db.host, db=params.db.name, username=params.db.user, password=password)
    except Exception as e:
      raise Sorry(e)

    # Get the set of known runs in the experiment database
    cmd = "SELECT run from %s_runs"%params.experiment_tag
    cursor = dbobj.cursor()
    cursor.execute(cmd)
    known_runs = [int(parse_entry(entry)[0]) for entry in cursor.fetchall()]

    # Get the list of active trials
    cmd = "SELECT trial_id, trial from %s_trials WHERE active = True"%params.experiment_tag
    cursor = dbobj.cursor()
    cursor.execute(cmd)
    entries = [parse_entry(entry) for entry in cursor.fetchall()]
    active_trial_ids = [int(entry[0]) for entry in entries]
    active_trials = [int(entry[1]) for entry in entries]

    for trial_id, trial in zip(active_trial_ids, active_trials):
      # Get the active rungroups for this trial
      cmd = "SELECT rungroups_id from %s_trial_rungroups WHERE trials_id = %d AND active = True"%(params.experiment_tag, trial_id)
      cursor = dbobj.cursor()
      cursor.execute(cmd)
      active_rungroup_ids = [int(parse_entry(entry)[0]) for entry in cursor.fetchall()]

      for rungroup_id in active_rungroup_ids:
        # Get the list of runs for this rungroup
        cmd = "SELECT startrun, endrun from %s_rungroups WHERE %s_rungroups.rungroup_id = %d"%(params.experiment_tag, params.experiment_tag, rungroup_id)
        cursor = dbobj.cursor()
        cursor.execute(cmd)

        assert cursor.rowcount == 1
        startrun, endrun = parse_entry(cursor.fetchall()[0])

        cmd = "SELECT run_id, run from %s_runs WHERE run >= %d"%(params.experiment_tag, startrun)
        if endrun is not None:
          cmd += " AND run <= %d"%endrun
        cursor = dbobj.cursor()
        cursor.execute(cmd)

        entries = [parse_entry(entry) for entry in cursor.fetchall()]
        run_ids = [int(entry[0]) for entry in entries]
        runs = [int(entry[1]) for entry in entries]

        # Find the submitted runs for this trial/rungroup combination
        cmd = "SELECT runs_id from %s_jobs WHERE trials_id = %d and rungroups_id = %d"%(params.experiment_tag, trial_id, rungroup_id)
        cursor = dbobj.cursor()
        cursor.execute(cmd)

        submitted_run_ids = [int(parse_entry(entry)[0]) for entry in cursor.fetchall()]

        # Submit new runs
        for run_id, run in zip(run_ids, runs):
          if run_id in submitted_run_ids:
            continue

          print("Submitting run %d into trial %d using rungroup %d"%(run_id, trial, rungroup_id))

          config_path = write_hitfind_cfg(params, dbobj, trial_id, trial, rungroup_id)
          if submit_job(params, dbobj, trial_id, trial, rungroup_id, run, config_path):
            pass

          cmd = "INSERT INTO %s_jobs (runs_id, trials_id, rungroups_id, status) VALUES (%d, %d, %d, '%s')"%(params.experiment_tag, run_id, trial_id, rungroup_id, "submitted")
          cursor = dbobj.cursor()
          cursor.execute(cmd)
          dbobj.commit()

    time.sleep(10)

if __name__ == "__main__":
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/jungfrau_metrics.py
# -*- coding: utf-8 -*-
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.jungfrau_metrics
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1
#
from __future__ import absolute_import, division, print_function

import math
from dials.array_family import flex
from libtbx.phil import parse
import glob
import numpy as np
from skimage.transform import SimilarityTransform
from skimage.measure import ransac

help_message = '''

This program is used to analyze the agreement between observed and model
Bragg spots on the Jungfrau 16M detector. The purpose is to assess the
quality of the metrology and the spot prediction model using proximal metrics:
1) Does the overall detector position seem to change over time (by run) ?
2) Does the sensor metrology make sense (<MODEL-OBS> = 0) ?
3) Are the spots predicted correctly, in terms of RMSD(MODEL_OBS) ?
3a) Especially radial vs. transverse RMSD ?
The program should challenge dials.refine (or whatever method is used to
determine the metrology) to produce superior proximal metrics.

Example:

  cctbx.xfel.jungfrau_metrics experiment_glob.expt reflections_glob.refl

Assumptions:  Everything is in the reflections files.  Predicted and observed are
taken as given in refls["xyzcal.mm"] and refls["xyzobs.mm.value"].
No reprediction is done.
Only a single *.expt file is read, to get the detector model.
It is assumed that all the detector models are identical (not verified).
Also assume that the files adhere to a specific naming convention so as to
make the regular expressions work.  This could be generalized.
Only supports the Jungfrau 16M with 8 panels x 32 sensors

Root assumptions:  All the reflections are strong spots that have been indexed.
There are no outliers (although the RANSAC fit will remove some outliers).

Requirements:  Python 3, scikit-image
'''

# Create the phil parameters
phil_scope = parse('''
panel_numbers = True
  .type = bool
  .help = Whether to show panel numbers on each panel
verbose = False
  .type = bool
  .help = Whether to print extra statistics
input {
  refl_glob = None
    .type = str
    .optional = False
  expt_glob = None
    .type = str
    .optional = False
  predictions_glob = None
    .type = str
    .help = instead of refl_glob, give separate globs for predictions refls["xyzcalc.mm"] and observations refls["xyzobs.mm.value"]
  observations_glob = None
    .type = str
    .help = instead of refl_glob, give separate globs for predictions refls["xyzcalc.mm"] and observations refls["xyzobs.mm.value"]
}
dispatch {
  by_run = True
    .type = bool
    .help = if True, print out a per-run metric analysis. \
            Assumes the run number is encoded in the file name as done in the regex RUN below.
  by_panel_mcd_filter = True
    .type = bool
    .help = finally, repeat the sensor analysis but with panel-level 3 feature MCD filter
}
residuals {
  mcd_filter {
    enable = False
      .type = bool
      .help = on the DeltaPsi plot, apply a 3-feature MCD filter \
              FIXME refactor this so it is not repeated from detector_residuals
    mahalanobis_distance = 7
      .type = float(value_min=0.)
      .help = cutoff level expressed as a multivariate Gaussian std dev
    keep = *inliers outliers
      .type = choice
      .help = this should be obvious.  Either keep the good ones or the bad ones.
  }
}
output {
  observations_filtered_replaces = None
    .type = str
    .help = After MCD, write out the filtered observations, to a filename patterned \
            on the observations file, but replacing this string with 'filtered'.

}
include scope xfel.command_line.cspad_detector_congruence.phil_scope
''', process_includes=True)

import re
RUN = re.compile(".*run_000([0-9][0-9][0-9])")
EVENT = re.compile(".*master_([0-9][0-9][0-9][0-9][0-9])")

# Next steps

# demonstrate that re-prediction based on expt model does not change the result.
#    that way I can validate on the fly any new code to write out a modified expt file
#    that way also I can include group B

#    have an option for sorting the output table by sensor radius from beam position

# remove the need to create a "groupA" file ahead of time
#   add the ability to get this analysis with group A+B
# add radial and transverse RMSD
# based on RANSAC, output a modified expt file [then plot the new metrology XY and Z positions]

class Script(object):
  ''' Class to parse the command line options. '''

  def __init__(self):
    ''' Set the expected options. '''
    from dials.util.options import ArgumentParser
    import libtbx.load_env

    # Create the option parser
    usage = "usage: %s [options] /path/to/refined/json/file" % libtbx.env.dispatcher_name
    self.parser = ArgumentParser(
      usage=usage,
      sort_options=True,
      phil=phil_scope,
      read_experiments=True,
      read_reflections=True,
      check_format=False,
      epilog=help_message)

  def build_statistics(self,refl_gen):

    n_total = 0
    n_file = 0
    sum_delta_x = 0
    sum_delta_y = 0
    deltax = {}
    deltay = {}
    delRoverR = {}
    panel_deltax = {}
    panel_deltay = {}
    panel_deltapsi = {}
    cumPANNO = {}
    cumOBS = {}
    cumCALC = {}
    run_nos = {}

    print ("Reading %d refl files, printing the first 10"%(len(refl_gen)))
    for item in refl_gen:
      strong_refls = item["strong_refls"]
      nrefls = len(strong_refls)
      if self.params.dispatch.by_run==True:
        run_match = RUN.match(item["strfile"])
        run_token = int(run_match.group(1)); run_nos[run_token]=run_nos.get(run_token,0); run_nos[run_token]+=1
        deltax[run_token] = deltax.get(run_token, flex.double())
        deltay[run_token] = deltay.get(run_token, flex.double())
        delRoverR[run_token] = delRoverR.get(run_token, flex.double())
      OBS = flex.vec2_double()
      CALC = flex.vec2_double()

      if n_file < 10: print (item["strfile"], nrefls)
      n_total += nrefls
      n_file += 1
      fcalc = strong_refls["xyzcal.mm"]
      fobs = strong_refls["xyzobs.mm.value"]
      delpsi = strong_refls["delpsical.rad"]
      panno = strong_refls["panel"]
      if n_file==1:
        for ipanel in range(256):
          panel_deltax[ipanel] = panel_deltax.get(ipanel, flex.double())
          panel_deltay[ipanel] = panel_deltay.get(ipanel, flex.double())
          panel_deltapsi[ipanel] = panel_deltapsi.get(ipanel, flex.double())
        for isensor in range(32):
          cumOBS[isensor] = cumOBS.get(isensor, flex.vec3_double())
          cumCALC[isensor] = cumCALC.get(isensor, flex.vec3_double())
          cumPANNO[isensor] = cumPANNO.get(isensor, flex.int())
        expt_files = glob.glob(self.params.input.expt_glob)
        expt_file = expt_files[0]
        from dxtbx.model.experiment_list import ExperimentList
        int_expt = ExperimentList.from_file(expt_file, check_format=False)[0]
        D = int_expt.detector
        B = int_expt.beam
        Beam = D.hierarchy().get_beam_centre_lab((0,0,-1)) # hard code the sample to source vector for now
        distance = D.hierarchy().get_distance()
      for irefl in range(len(strong_refls)):
        sum_delta_x += (fcalc[irefl][0] - fobs[irefl][0])
        sum_delta_y += (fcalc[irefl][1] - fobs[irefl][1])
        panel = D[panno[irefl]]
        panel_deltax[panno[irefl]].append( (fcalc[irefl][0] - fobs[irefl][0]) )
        panel_deltay[panno[irefl]].append( (fcalc[irefl][1] - fobs[irefl][1]) )
        panel_deltapsi[panno[irefl]].append( delpsi[irefl] )
        CALC.append(panel.get_lab_coord(fcalc[irefl][0:2])[0:2])  # take only the xy coords; assume z along beam
        OBS.append(panel.get_lab_coord(fobs[irefl][0:2])[0:2])
        cumCALC[panno[irefl]//8].append(panel.get_lab_coord(fcalc[irefl][0:2]))
        cumOBS[panno[irefl]//8].append(panel.get_lab_coord(fobs[irefl][0:2]))
        cumPANNO[panno[irefl]//8].append(panno[irefl])
      if self.params.dispatch.by_run==True:
        for irefl in range(len(strong_refls)):
          deltax[run_token].append( (fcalc[irefl][0] - fobs[irefl][0]) )
          deltay[run_token].append( (fcalc[irefl][1] - fobs[irefl][1]) )

      R_sq = CALC.dot(CALC)
      DIFF = CALC-OBS
      Dr_over_r = (DIFF.dot(CALC))/R_sq
      if self.params.dispatch.by_run==True:
        for irefl in range(len(strong_refls)):
          delRoverR[run_token].append( Dr_over_r[irefl] )

    from libtbx import adopt_init_args
    obj = dict(n_total=n_total, n_file=n_file, sum_delta_x=sum_delta_x,
            sum_delta_y=sum_delta_y, deltax=deltax, deltay=deltay, distance=distance,
            delRoverR=delRoverR, panel_deltax=panel_deltax, panel_deltay=panel_deltay, panel_deltapsi=panel_deltapsi,
            cumOBS=cumOBS, cumCALC=cumCALC, cumPANNO=cumPANNO, O="ok")
    adopt_init_args(self, obj)
    self.ordered_run_nos = sorted(list(run_nos.keys()))
    self.run_nos = run_nos

  def build_statistics_rejecting_outliers(self,refl_gen):
    # XXX duplicates behavior of above function; should refactor to cover both cases with same code.
    n_total = 0
    n_file = 0
    sum_delta_x = 0
    sum_delta_y = 0
    deltax = {}
    deltay = {}
    delRoverR = {}
    panel_deltax = {}
    panel_deltay = {}
    panel_deltapsi = {}
    cumPANNO = {}
    cumOBS = {}
    cumCALC = {}
    run_nos = {}

    print ("Reading %d refl files, WITH REJECTION printing the first 10"%(len(refl_gen)))
    for item in refl_gen:
      strong_refls = item["strong_refls"]
      nrefls = len(strong_refls)
      keep_refls = flex.bool(nrefls, True)
      if self.params.dispatch.by_run==True:
        run_match = RUN.match(item["strfile"])
        run_token = int(run_match.group(1)); run_nos[run_token]=run_nos.get(run_token,0); run_nos[run_token]+=1
        deltax[run_token] = deltax.get(run_token, flex.double())
        deltay[run_token] = deltay.get(run_token, flex.double())
        delRoverR[run_token] = delRoverR.get(run_token, flex.double())
      OBS = flex.vec2_double()
      CALC = flex.vec2_double()

      if n_file < 1000: print (item["strfile"], nrefls)
      n_total += nrefls
      n_file += 1
      fcalc = strong_refls["xyzcal.mm"]
      fobs = strong_refls["xyzobs.mm.value"]
      delpsi = strong_refls["delpsical.rad"]
      panno = strong_refls["panel"]
      if n_file==1:
        for ipanel in range(256):
          panel_deltax[ipanel] = panel_deltax.get(ipanel, flex.double())
          panel_deltay[ipanel] = panel_deltay.get(ipanel, flex.double())
          panel_deltapsi[ipanel] = panel_deltapsi.get(ipanel, flex.double())
        for isensor in range(32):
          cumOBS[isensor] = cumOBS.get(isensor, flex.vec3_double())
          cumCALC[isensor] = cumCALC.get(isensor, flex.vec3_double())
          cumPANNO[isensor] = cumPANNO.get(isensor, flex.int())
        expt_files = glob.glob(self.params.input.expt_glob)
        expt_file = expt_files[0]
        from dxtbx.model.experiment_list import ExperimentList
        int_expt = ExperimentList.from_file(expt_file, check_format=False)[0]
        D = int_expt.detector
        B = int_expt.beam
        Beam = D.hierarchy().get_beam_centre_lab((0,0,-1)) # hard code the sample to source vector for now
        distance = D.hierarchy().get_distance()
      for irefl in range(len(strong_refls)):
        trial_delta_x = (fcalc[irefl][0] - fobs[irefl][0])
        trial_delta_y = (fcalc[irefl][1] - fobs[irefl][1])
        #if irefl==26 and nrefls==400: import ipdb;ipdb.set_trace()
        try:
          maha_distance = self.record_MCD_models[panno[irefl]].rob_cov.mahalanobis(X=[ (trial_delta_x,trial_delta_y,delpsi[irefl]) ])[0]
        except KeyError:
          maha_distance = None
        if (
            maha_distance is None or
            maha_distance >= (self.params.residuals.mcd_filter.mahalanobis_distance)**2
        ):
          keep_refls[irefl]=False
          continue
        sum_delta_x += trial_delta_x
        sum_delta_y += trial_delta_y
        panel = D[panno[irefl]]
        panel_deltax[panno[irefl]].append( trial_delta_x )
        panel_deltay[panno[irefl]].append( trial_delta_y )
        panel_deltapsi[panno[irefl]].append( delpsi[irefl] )
        CALC.append(panel.get_lab_coord(fcalc[irefl][0:2])[0:2])  # take only the xy coords; assume z along beam
        OBS.append(panel.get_lab_coord(fobs[irefl][0:2])[0:2])
        cumCALC[panno[irefl]//8].append(panel.get_lab_coord(fcalc[irefl][0:2]))
        cumOBS[panno[irefl]//8].append(panel.get_lab_coord(fobs[irefl][0:2]))
        cumPANNO[panno[irefl]//8].append(panno[irefl])
      if self.params.dispatch.by_run==True:
        for irefl in range(len(strong_refls)):
          deltax[run_token].append( (fcalc[irefl][0] - fobs[irefl][0]) )
          deltay[run_token].append( (fcalc[irefl][1] - fobs[irefl][1]) )

      R_sq = CALC.dot(CALC)
      DIFF = CALC-OBS
      Dr_over_r = (DIFF.dot(CALC))/R_sq
      if self.params.dispatch.by_run==True:
        for irefl in range(len(Dr_over_r)):
          delRoverR[run_token].append( Dr_over_r[irefl] )

      if self.params.output.observations_filtered_replaces is not None:
        filtered_file = item["obsfile"].replace(self.params.output.observations_filtered_replaces,"filtered")
        item["strong_refls"].select(keep_refls).as_file(filtered_file)

    from libtbx import adopt_init_args
    obj = dict(n_total=n_total, n_file=n_file, sum_delta_x=sum_delta_x,
            sum_delta_y=sum_delta_y, deltax=deltax, deltay=deltay, distance=distance,
            delRoverR=delRoverR, panel_deltax=panel_deltax, panel_deltay=panel_deltay, panel_deltapsi=panel_deltapsi,
            cumOBS=cumOBS, cumCALC=cumCALC, cumPANNO=cumPANNO, O="ok")
    adopt_init_args(self, obj)
    self.ordered_run_nos = sorted(list(run_nos.keys()))
    self.run_nos = run_nos

  def per_run_analysis(self):
    print ()
    print ("Total number of files and reflections for each run")
    print ("Average Delta-x,Deltay in microns, along with standard error of the mean, and RMSD(model-obs)")
    print ()
    print ("Run  n_file  n_refl  <Δx>(μm)     <Δy>(μm)  RMSD Δx(μm)  RMSD Δy(μm)  <distance*Δr/r>(μm)")

    for irun in self.ordered_run_nos:
      Sx = flex.mean_and_variance(1000.*self.deltax[irun])
      Sy = flex.mean_and_variance(1000.*self.deltay[irun])
      print(irun,
        "%6d %8d"%(self.run_nos[irun], len(self.deltax[irun])),
        "%6.2f±%.2f %6.2f±%.2f"%(1000.*flex.mean(self.deltax[irun]),Sx.unweighted_standard_error_of_mean(),
                                 1000.*flex.mean(self.deltay[irun]),Sy.unweighted_standard_error_of_mean()),
        "    %5.1f    %5.1f"%(Sx.unweighted_sample_standard_deviation(),Sy.unweighted_sample_standard_deviation()),
        "       %5.1f"%(1000. * self.distance * flex.mean(self.delRoverR[irun]))
        )
    print("All",
        "%6d %8d"%(self.n_file, self.n_total),
        "%6.2f      %6.2f     "%(1000*self.sum_delta_x/self.n_total,
                                 1000*self.sum_delta_y/self.n_total,)
        )
    print ()

  def per_sensor_analysis(self): # hardcoded Jungfrau 16M geometry
    for isensor in range(32):
      print ("Panel Sensor  <Δx>(μm)     <Δy>(μm)      Nrefl  RMS Δx(μm)  RMS Δy(μm) ")

      if len(self.cumCALC[isensor]) <= 3: continue

      for ipanel in range(8*isensor, 8*(1+isensor)):
        if len(self.panel_deltax[ipanel])<2: continue
        Sx = flex.mean_and_variance(1000.*self.panel_deltax[ipanel])
        Sy = flex.mean_and_variance(1000.*self.panel_deltay[ipanel])
        RMSDx = 1000.*math.sqrt(flex.mean(self.panel_deltax[ipanel]*self.panel_deltax[ipanel]))
        RMSDy = 1000.*math.sqrt(flex.mean(self.panel_deltay[ipanel]*self.panel_deltay[ipanel]))
        print("%3d  %3d"%(ipanel,ipanel//8),"%7.2f±%6.2f %7.2f±%6.2f %6d"%(Sx.mean(),Sx.unweighted_standard_error_of_mean(),
                                                 Sy.mean(),Sy.unweighted_standard_error_of_mean(), len(self.panel_deltax[ipanel])),
            "    %5.1f   %5.1f"%(RMSDx,RMSDy),
        )
      print("")
      cumD = (self.cumCALC[isensor]-self.cumOBS[isensor]).parts()
      print ( "All  %3d %7.2f        %7.2f        %6d"%(isensor,1000.*flex.mean(cumD[0]), 1000.*flex.mean(cumD[1]), len(cumD[0])))
      print("")

  # Now we'll do a linear least squares refinement over sensors:
  #Method 1. Simple rectilinear translation.
      if self.params.verbose:
        veclength = len(self.cumCALC[isensor])
        correction = flex.vec3_double( veclength, (flex.mean(cumD[0]), flex.mean(cumD[1]), flex.mean(cumD[2])) )

        new_delta = (self.cumCALC[isensor]-correction ) -self.cumOBS[isensor]
        for ipanel in range(8*isensor, 8*(1+isensor)):
          panel_delta = new_delta.select(self.cumPANNO[isensor]==ipanel)
          if len(panel_delta)<2: continue
          deltax_part, deltay_part = panel_delta.parts()[0:2]
          RMSDx = 1000.*math.sqrt( flex.mean(deltax_part * deltax_part) )
          RMSDy = 1000.*math.sqrt( flex.mean(deltay_part * deltay_part) )
          Sx = flex.mean_and_variance(1000.*deltax_part)
          Sy = flex.mean_and_variance(1000.*deltay_part)
          print("%3d  %3d"%(ipanel,ipanel//8),"%7.2f±%6.2f %7.2f±%6.2f %6d"%(Sx.mean(),Sx.unweighted_standard_error_of_mean(),
                                                 Sy.mean(),Sy.unweighted_standard_error_of_mean(), len(deltax_part)),
          "    %5.1f   %5.1f"%(RMSDx,RMSDy),
          )
        print()
  # Method 2. Translation + rotation.
      src = []
      dst = []
      for icoord in range(len(self.cumCALC[isensor])):
        src.append(self.cumCALC[isensor][icoord][0:2])
        dst.append(self.cumOBS[isensor][icoord][0:2])
      src = np.array(src)
      dst = np.array(dst)

      # estimate affine transform model using all coordinates
      model = SimilarityTransform()
      model.estimate(src, dst)

      # robustly estimate affine transform model with RANSAC
      model_robust, inliers = ransac((src, dst), SimilarityTransform, min_samples=3,
                               residual_threshold=2, max_trials=10)
      outliers = flex.bool(inliers == False)

      # compare "true" and estimated transform parameters
      if self.params.verbose:
        print("Similarity transform:")
        print("%2d"%isensor, "Scale: %.5f,"%(model.scale),
        "Translation(μm): (%7.2f,"%(1000.*model.translation[0]),
        "%7.2f),"%(1000.*model.translation[1]),
        "Rotation (°): %7.4f"%((180./math.pi)*model.rotation))
      print("RANSAC:")
      print("%2d"%isensor, "Scale: %.5f,"%(model_robust.scale),
      "Translation(μm): (%7.2f,"%(1000.*model_robust.translation[0]),
      "%7.2f),"%(1000.*model_robust.translation[1]),
      "Rotation (°): %7.4f,"%((180./math.pi)*model_robust.rotation),
      "Outliers:",outliers.count(True)
      )
      """from documentation:
      X = a0 * x - b0 * y + a1 = s * x * cos(rotation) - s * y * sin(rotation) + a1
      Y = b0 * x + a0 * y + b1 = s * x * sin(rotation) + s * y * cos(rotation) + b1"""

      oldCALC = self.cumCALC[isensor].parts()

      ransacCALC = flex.vec3_double(
               (float(model_robust.scale) * oldCALC[0] * math.cos(model_robust.rotation) -
               float(model_robust.scale) * oldCALC[1] * math.sin(model_robust.rotation) +
               float(model_robust.translation[0])),
               (float(model_robust.scale) * oldCALC[0] * math.sin(model_robust.rotation) +
               float(model_robust.scale) * oldCALC[1] * math.cos(model_robust.rotation) +
               float(model_robust.translation[1])),
               oldCALC[2]
               )
      new_delta = ransacCALC - self.cumOBS[isensor]
      inlier_delta = new_delta.select(~outliers)
      inlier_panno = self.cumPANNO[isensor].select(~outliers)

      for ipanel in range(8*isensor, 8*(1+isensor)):
        panel_delta = inlier_delta.select(inlier_panno==ipanel)
        if len(panel_delta)<2: continue
        deltax_part, deltay_part = panel_delta.parts()[0:2]
        RMSDx = 1000.*math.sqrt( flex.mean(deltax_part * deltax_part) )
        RMSDy = 1000.*math.sqrt( flex.mean(deltay_part * deltay_part) )
        Sx = flex.mean_and_variance(1000.*deltax_part)
        Sy = flex.mean_and_variance(1000.*deltay_part)
        print("%3d  %3d"%(ipanel,ipanel//8),"%7.2f±%6.2f %7.2f±%6.2f %6d"%(Sx.mean(),Sx.unweighted_standard_error_of_mean(),
                                                 Sy.mean(),Sy.unweighted_standard_error_of_mean(), len(deltax_part)),
        "    %5.1f   %5.1f"%(RMSDx,RMSDy),
        )

      if self.params.verbose:
        print("")
        cumD = (inlier_delta).parts()
        print ( "     %3d %7.2f        %7.2f        %6d\n"%(isensor,1000.*flex.mean(cumD[0]), 1000.*flex.mean(cumD[1]), len(cumD[0])))
      print("----\n")

  def per_panel_mcd_filter(self):
    from xfel.metrology.panel_fitting import Panel_MCD_Filter
    self.record_MCD_models = {}
    for isensor in range(32):
      if len(self.cumCALC[isensor]) <= 3: continue
      for ipanel in range(8*isensor, 8*(1+isensor)):
        if len(self.panel_deltax[ipanel])<2: continue
        MCD = Panel_MCD_Filter(lab_coords_x = self.panel_deltax[ipanel],
                               lab_coords_y = self.panel_deltay[ipanel],
                               data = self.panel_deltapsi[ipanel],
                               i_panel=ipanel, delta_scalar=1., params=self.params, verbose=True)
        sX,sY,sPsi = MCD.scatter_coords()
        self.record_MCD_models[ipanel] = MCD

  def build_reflections_generator(self):
    help = """the application accepts two mutually exclusive input formats
    1) indexed strong spots and corresponding predictions from input.refl_glob
    2) strong from input.observations_glob, predictions from input.predictions_glob"""
    assert (self.params.input.refl_glob is not None).__xor__(
           (self.params.input.observations_glob is not None) and
            self.params.input.predictions_glob is not None
           ), help
    if self.params.input.refl_glob is not None:
      class all_refl_list:
        def __init__(O): O._all_file_list = glob.glob(self.params.input.refl_glob)
        def __len__(O): return len(O._all_file_list)
        def __iter__(O):
          for item in O._all_file_list:
            yield dict(strfile = item, obsfile = item, strong_refls=flex.reflection_table.from_file(item))
      return all_refl_list()

    else:
      class all_refl_list:
        def __init__(O):
          O.pred_list = glob.glob(self.params.input.predictions_glob)
          O.obs_list = glob.glob(self.params.input.observations_glob)
          O.obs_reverse_lookup = {}
          for obs in O.obs_list:
              run_match = RUN.match(obs)
              run_token = int(run_match.group(1))
              event_match = EVENT.match(obs)
              event_token = int(event_match.group(1))
              O.obs_reverse_lookup[(run_token,event_token)] = obs
        def __len__(O): return len(O.pred_list)
        def __iter__(O):
          from cctbx.miller import match_indices
          n_total = 0
          n_file = 0
          n_reindex = 0
          n_matches = 0
          n_strong_no_integration = 0
          n_integrated = 0
          n_common = 0
          n_weak = 0

          for item in O.pred_list:
            run_match = RUN.match(item)
            run_token = int(run_match.group(1))
            event_match = EVENT.match(item)
            event_token = int(event_match.group(1))
            obs = O.obs_reverse_lookup[(run_token,event_token)]
            strong_refls = flex.reflection_table.from_file(obs)
            nrefls = len(strong_refls)
            ri = reindex_miller = strong_refls["miller_index"]
            select_indexed = reindex_miller != (0,0,0)
            reindex = (select_indexed).count(True)
            strong_and_indexed = strong_refls.select(select_indexed)
            #print (obs, nrefls, reindex)
            n_total += nrefls
            n_file += 1
            n_reindex += reindex

            int_refls = flex.reflection_table.from_file(item)
            ii = integration_miller = int_refls["miller_index"]
            MM = match_indices(strong_and_indexed["miller_index"], int_refls["miller_index"])
            #print("  Strong+indexed",reindex, "integrated",len(ii), "in common",len(MM.pairs()),
            #"indexed but no integration", len(MM.singles(0)), "integrated weak", len(MM.singles(1)))
            n_integrated += len(ii)
            n_common += len(MM.pairs())
            n_strong_no_integration += len(MM.singles(0))
            n_weak += len(MM.singles(1))

            P = MM.pairs()
            A = P.column(0)
            B = P.column(1)
            strong_and_indexed = strong_and_indexed.select(A)
            int_refls = int_refls.select(B)
            # transfer over the calculated positions from integrate2 to strong refls
            strong_and_indexed["xyzcal.mm"] = int_refls["xyzcal.mm"]
            strong_and_indexed["xyzcal.px"] = int_refls["xyzcal.px"]
            strong_and_indexed["delpsical.rad"] = int_refls["delpsical.rad"]

            yield dict(strfile = item, obsfile=obs, strong_refls=strong_and_indexed)
          print ("Grand total is %d from %d files of which %d reindexed"%(n_total,n_file, n_reindex))
          print ("TOT Strong+indexed",n_reindex, "integrated",n_integrated, "in common",
          n_common, "indexed but no integration", n_strong_no_integration, "integrated weak", n_weak)
      return all_refl_list()

  def run(self):
    ''' Parse the options. '''
    # Parse the command line arguments
    params, options = self.parser.parse_args(show_diff_phil=True)
    assert params.input.expt_glob is not None, "Must give a input.expt_glob= string"
    self.params = params

    generate_refls = self.build_reflections_generator()
    assert generate_refls is not None

    self.build_statistics(refl_gen = generate_refls)
    print("Finished reading total refl files",self.n_file)

    if self.params.dispatch.by_run==True:
      self.per_run_analysis()
    self.per_sensor_analysis()

    if self.params.dispatch.by_panel_mcd_filter:
      self.per_panel_mcd_filter()
      self.build_statistics_rejecting_outliers(refl_gen = self.build_reflections_generator())
      if self.params.dispatch.by_run==True:
        self.per_run_analysis()
      self.per_sensor_analysis()

if __name__ == '__main__':
  script = Script()
  script.run()



 *******************************************************************************


 *******************************************************************************
xfel/command_line/launch_fxs.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME mpi_fxs_launch
from xfel.amo.pnccd_ana.mpi_fxs_launch import launch
import sys
launch(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/list_db_metadata.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.list_db_metadata
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1

import libtbx.phil
from libtbx.utils import Sorry
import sys

def run(args):
  master_phil = libtbx.phil.parse("""
    db {
      host = None
        .type = str
      name = None
        .type = str
      table_name = None
        .type = str
      user = None
        .type = str
      password = None
        .type = str
    }
  """)

  if (__name__ == "__main__") :
    user_phil = []
    for arg in args :
      try :
        user_phil.append(libtbx.phil.parse(arg))
      except RuntimeError as e :
        raise Sorry("Unrecognized argument '%s' (error: %s)" % (arg, str(e)))
    params = master_phil.fetch(sources=user_phil).extract()

    try:
      from cxi_xdr_xes.cftbx.cspad_ana import db as db
    except ImportError:
      raise Sorry("Trial logging not supported for this installation. Conact the developers for access.")

    dbobj = db.dbconnect(host=params.db.host, db=params.db.name, username=params.db.user, password=params.db.password)
    db.list_db_metadata(dbobj, params.db.table_name)

if (__name__ == "__main__") :
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/make_dials_mask.py
from __future__ import absolute_import, division, print_function
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.make_dials_mask
#
# This code reads three images and builds a mask from
# them.  The first image should be an average from a dark run, the second the
# standard deviation from that run.  The third image should be a maximum projection
# from a run with the beam on.
#
# The result is a dials-style pickle file containing a list of flex.bool objects.
#

import dxtbx.format.Registry
from scitbx.array_family import flex
import sys
from libtbx import easy_pickle
from six.moves import zip

def run(argv=None):
  import libtbx.option_parser

  if (argv is None):
    argv = sys.argv

  command_line = (libtbx.option_parser.option_parser(
    usage="%s [-v] [-a avg_max] [-s stddev_max] [-m maxproj_min] [-o output] [-b border] avg_path stddev_path max_path" % libtbx.env.dispatcher_name)
                  .option(None, "--verbose", "-v",
                          action="store_true",
                          default=False,
                          dest="verbose",
                          help="Print more information about progress")
                  .option(None, "--avg_max", "-a",
                          type="float",
                          default=2000.0,
                          dest="avg_max",
                          help="Maximum ADU that pixels in the average image are allowed to have before masked out")
                  .option(None, "--stddev_max", "-s",
                          type="float",
                          default=10.0,
                          dest="stddev_max",
                          help="Maximum ADU that pixels in the standard deviation image are allowed to have before masked out")
                  .option(None, "--maxproj_min", "-m",
                          type="float",
                          default=300.0,
                          dest="maxproj_min",
                          help="Minimum ADU that pixels in the maximum projection image are allowed to have before masked out")
                  .option(None, "--output", "-o",
                          type="string",
                          default="mask.pickle",
                          dest="destpath",
                          help="output file path, should be *.pickle")
                  .option(None, "--border", "-b",
                          type="int",
                          default=0,
                          dest="border",
                          help="border width in pixels to mask out of each tile")
                  ).process(args=argv[1:])

  # Must have exactly three remaining arguments.
  paths = command_line.args
  if (len(paths) != 3):
    command_line.parser.print_usage(file=sys.stderr)
    return

  avg_path    = paths[0]
  stddev_path = paths[1]
  max_path    = paths[2]

  # load the three images
  format_class = dxtbx.format.Registry.get_format_class_for_file(avg_path)
  avg_f = format_class(avg_path)
  avg_d = avg_f.get_raw_data()
  if not isinstance(avg_d, tuple):
    avg_d = (avg_d,)

  stddev_f = format_class(stddev_path)
  stddev_d = stddev_f.get_raw_data()
  if not isinstance(stddev_d, tuple):
    stddev_d = (stddev_d,)

  max_f = format_class(max_path)
  max_d = max_f.get_raw_data()
  if not isinstance(max_d, tuple):
    max_d = (max_d,)

  mask = [flex.bool(flex.grid(p.focus()), True) for p in avg_d]

  count_a = count_s = count_m = 0

  for mask_p, avg_p, stddev_p, max_p in zip(mask, avg_d, stddev_d, max_d):
    # first find all the pixels in the average that are less than zero or greater
    # than a cutoff and set them to the masking value
    sel = avg_p > 0
    count_a += sel.count(False)
    mask_p &= sel
    sel = avg_p <= command_line.options.avg_max
    count_a += sel.count(False)
    mask_p &= sel

    # mask out the overly noisy or flat pixels
    sel = stddev_p > 0
    count_s += sel.count(False)
    mask_p &= sel
    sel = stddev_p <= command_line.options.stddev_max # cxi.make_mask uses <
    count_s += sel.count(False)
    mask_p &= sel

    # these are the non-bonded pixels
    sel = max_p >= command_line.options.maxproj_min
    count_m += sel.count(False)
    mask_p &= sel

    # Add a border around the image
    if command_line.options.border > 0:
      border = command_line.options.border
      height, width = mask_p.all()
      borderx = flex.bool(flex.grid(border, width), False)
      bordery = flex.bool(flex.grid(height, border), False)
      mask_p[0:border,:] = borderx
      mask_p[-border:,:] = borderx
      mask_p[:,0:border] = bordery
      mask_p[:,-border:] = bordery

  easy_pickle.dump(command_line.options.destpath, tuple(mask))

  masked_out = sum([len(mask_p.as_1d().select((~mask_p).as_1d())) for mask_p in mask])
  total = sum([len(mask_p) for mask_p in mask])

  print("From average, masked out %d pixels"%count_a)
  print("From stddev, masked out %d pixels"%count_s)
  print("From max, masked out %d pixels"%count_m)

  print("Masked out %d pixels out of %d (%.2f%%)"% \
    (masked_out,total,masked_out*100/total))

if (__name__ == "__main__"):
  sys.exit(run())


 *******************************************************************************


 *******************************************************************************
xfel/command_line/make_mar_mask.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.make_mar_mask
#
# $Id:
#
# This code dumps a flex array containing masked out pixels given certain specifications
# The result is an image with all pixels which are valid for use set to 0, and
# those that are invalid set to -2 by default, or the value of the option passed in
# to mask_pix_val.
#

from scitbx.array_family import flex
import sys
from libtbx import easy_pickle
from xfel.cxi.cspad_ana.cspad_tbx import dpack, dwritef2

def point_in_polygon(point, poly):
  """ Determine if a point is inside a given polygon or not.  Polygon is a list of (x,y) pairs.
  Code adapted from a dials polygon clipping test algorithm"""
  if len(poly) < 3: return False

  inside = False
  for i in range(len(poly)):
    j = (i+1) % len(poly)
    if (((poly[i][1] > point[1]) != (poly[j][1] > point[1])) and
      (point[0] < (poly[j][0] - poly[i][0]) * (point[1] - poly[i][1]) /
                  (poly[j][1] - poly[i][1]) + poly[i][0])):
      inside = not inside
  return inside

def point_inside_circle(x,y,center_x,center_y,radius):
  """Determine if a given point (x,y) is inside a circle whose center is at
  (center_x,center_y) with radius x."""
  return (x-center_x)**2 + (y - center_y)**2 < radius**2

def run(argv=None):
  import libtbx.option_parser

  if (argv is None):
    argv = sys.argv

  command_line = (libtbx.option_parser.option_parser(
    usage="%s [-v] [-p poly_mask] [-c circle_mask]  [-x mask_pix_val] [-o output] -W mask_width -H mask_height" % libtbx.env.dispatcher_name)
                  .option(None, "--verbose", "-v",
                          action="store_true",
                          default=False,
                          dest="verbose",
                          help="Print more information about progress")
                  .option(None, "--poly_mask", "-p",
                          type="string",
                          default=None,
                          dest="poly_mask",
                          help="Polygon to mask out.  Comma-seperated string of xy pairs.")
                  .option(None, "--circle_mask", "-c",
                          type="string",
                          default=None,
                          dest="circle_mask",
                          help="Circle to mask out.  Comma-seperated string of x, y, and radius.")
                  .option(None, "--mask_pix_val", "-x",
                          type="int",
                          default=-2,
                          dest="mask_pix_val",
                          help="Value for masked out pixels")
                  .option(None, "--mask_width", "-W",
                          type="int",
                          default=None,
                          dest="mask_width",
                          help="Width of output mask")
                   .option(None, "--mask_height", "-H",
                          type="int",
                          default=None,
                          dest="mask_height",
                          help="Height of output mask")
                   .option(None, "--output", "-o",
                          type="string",
                          default="mask.pickle",
                          dest="destpath",
                          help="Output file path, should be *.pickle")
                   .option(None, "--pixel_size", "-s",
                          type="float",
                          default=None,
                          dest="pixel_size",
                          help="Pixel size for detector")
                  ).process(args=argv[1:])

  # Must have width and height set
  if command_line.options.mask_height is None or command_line.options.mask_width is None:
    command_line.parser.print_usage(file=sys.stderr)
    return

  poly_mask = None
  if not command_line.options.poly_mask == None:
    poly_mask = []
    poly_mask_tmp = command_line.options.poly_mask.split(",")
    if len(poly_mask_tmp) % 2 != 0:
      command_line.parser.print_usage(file=sys.stderr)
      return
    odd = True
    for item in poly_mask_tmp:
      try:
        if odd:
          poly_mask.append(int(item))
        else:
          poly_mask[-1] = (poly_mask[-1],int(item))
      except ValueError:
        command_line.parser.print_usage(file=sys.stderr)
        return
      odd = not odd

  circle_mask = None
  if command_line.options.circle_mask is not None:
    circle_mask_tmp = command_line.options.circle_mask.split(",")
    if len(circle_mask_tmp) != 3:
      command_line.parser.print_usage(file=sys.stderr)
      return
    try:
      circle_mask = (int(circle_mask_tmp[0]),int(circle_mask_tmp[1]),int(circle_mask_tmp[2]))
    except ValueError:
      command_line.parser.print_usage(file=sys.stderr)
      return

  mask = flex.int(flex.grid(command_line.options.mask_width,
                             command_line.options.mask_height))

  if poly_mask is not None or circle_mask is not None:
    minx = miny = 0
    maxx = mask.focus()[0]
    maxy = mask.focus()[1]
    if poly_mask is not None:
      minx = min([x[0] for x in poly_mask])
      miny = min([y[1] for y in poly_mask])
      maxx = max([x[0] for x in poly_mask])
      maxy = max([y[1] for y in poly_mask])
    if circle_mask is not None:
      circle_x, circle_y, radius = circle_mask

      if circle_x - radius < minx: minx = circle_x - radius
      if circle_y - radius < miny: miny = circle_y - radius
      if circle_x + radius > maxx: maxx = circle_x + radius
      if circle_y + radius > maxy: maxy = circle_y + radius

    sel = mask == command_line.options.mask_pix_val
    for j in range(miny, maxy):
      for i in range(minx, maxx):
        idx = j * mask.focus()[0] + i
        if not sel[idx]:
          if poly_mask is not None and point_in_polygon((i,j),poly_mask):
            sel[idx] = True
          elif circle_mask is not None and point_inside_circle(i,j,circle_x,circle_y,radius):
            sel[idx] = True
    mask.set_selected(sel,command_line.options.mask_pix_val)

  masked_out = len(mask.as_1d().select((mask == command_line.options.mask_pix_val).as_1d()))

  print("Masked out %d pixels out of %d (%.2f%%)"% \
    (masked_out,len(mask),(masked_out)*100/(len(mask))))

  easy_pickle.dump(command_line.options.destpath, mask)

  d = dpack(
    active_areas=[0,0,command_line.options.mask_width,command_line.options.mask_height],
    address=None,
    beam_center_x=None,
    beam_center_y=None,
    data=mask,
    distance=None,
    timestamp=None,
    wavelength=1,
    xtal_target=None,
    pixel_size=command_line.options.pixel_size,
    saturated_value=None)

  dwritef2(d, command_line.options.destpath)

if (__name__ == "__main__"):
  sys.exit(run())


 *******************************************************************************
