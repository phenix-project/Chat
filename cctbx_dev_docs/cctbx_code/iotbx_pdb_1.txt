

 *******************************************************************************
iotbx/pdb/hybrid_36.py
"""
               Prototype/reference implementation for
                      encoding and decoding
                     atom serial numbers and
                     residue sequence numbers
                          in PDB files.

PDB ATOM and HETATM records reserve columns 7-11 for the atom serial
number. This 5-column number is used as a reference in the CONECT
records, which also reserve exactly five columns for each serial
number.

With the decimal counting system only up to 99999 atoms can be stored
and uniquely referenced in a PDB file. A simple extension to enable
processing of more atoms is to adopt a counting system with more than
ten digits. To maximize backward compatibility, the counting system is
only applied for numbers greater than 99999. The "hybrid-36" counting
system implemented in this file is:

  ATOM      1
  ...
  ATOM  99999
  ATOM  A0000
  ATOM  A0001
  ...
  ATOM  A0009
  ATOM  A000A
  ...
  ATOM  A000Z
  ATOM  ZZZZZ
  ATOM  a0000
  ...
  ATOM  zzzzz

I.e. the first 99999 serial numbers are represented as usual. The
following atoms use a base-36 system (10 digits + 26 letters) with
upper-case letters. 43670016 (26*36**4) additional atoms can be
numbered this way. If there are more than 43770015 (99999+43670016)
atoms, a base-36 system with lower-case letters is used, allowing for
43670016 additional atoms. I.e. in total 87440031 (99999+2*43670016)
atoms can be stored and uniquely referenced via CONECT records.

The counting system is designed to avoid lower-case letters until the
range of numbers addressable by upper-case letters is exhausted.
Importantly, with this counting system the distinction between
"traditional" and "extended" PDB files becomes evident only if there
are more than 99999 atoms to be stored. Programs that are
updated to support the hybrid-36 counting system will continue to
interoperate with programs that do not as long as there are less than
100000 atoms.

PDB ATOM and HETATM records also reserve columns 23-26 for the residue
sequence number. This 4-column number is used as a reference in other
record types (SSBOND, LINK, HYDBND, SLTBRG, CISPEP), which also reserve
exactly four columns for each sequence number.

With the decimal counting system only up to 9999 residues per chain can
be stored and uniquely referenced in a PDB file. If the hybrid-36
system is adopted, 1213056 (26*36**3) additional residues can be
numbered using upper-case letters, and the same number again using
lower-case letters. I.e. in total each chain may contain up to 2436111
(9999+2*1213056) residues that can be uniquely referenced from the
other record types given above.

The implementation in this file should run with Python 2.6 or higher.
There are no other requirements. Run this script without arguments to
obtain usage examples.

Note that there are only about 60 lines of "real" code. The rest is
documentation and unit tests.

To update an existing program to support the hybrid-36 counting system,
simply replace the existing read/write source code for integer values
with equivalents of the hy36decode() and hy36encode() functions below.

This file is unrestricted Open Source (cctbx.sf.net).
Please send corrections and enhancements to cctbx@cci.lbl.gov .

See also:
  http://cci.lbl.gov/hybrid_36/
  http://www.pdb.org/ "Dictionary & File Formats"

Ralf W. Grosse-Kunstleve, Feb 2007.
"""
from __future__ import absolute_import, division, print_function
try:
  from six.moves import range
  from six.moves import zip
except ImportError:
  pass

digits_upper = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ"
digits_lower = digits_upper.lower()
digits_upper_values = dict([pair for pair in zip(digits_upper, range(36))])
digits_lower_values = dict([pair for pair in zip(digits_lower, range(36))])

def encode_pure(digits, value):
  "encodes value using the given digits"
  assert value >= 0
  if (value == 0): return digits[0]
  n = len(digits)
  result = []
  while (value != 0):
    rest = value // n
    result.append(digits[value - rest * n])
    value = rest
  result.reverse()
  return "".join(result)

def decode_pure(digits_values, s):
  "decodes the string s using the digit, value associations for each character"
  result = 0
  n = len(digits_values)
  for c in s:
    result *= n
    result += digits_values[c]
  return result

def hy36encode(width, value):
  "encodes value as base-10/upper-case base-36/lower-case base-36 hybrid"
  i = value
  if (i >= 1-10**(width-1)):
    if (i < 10**width):
      return ("%%%dd" % width) % i
    i -= 10**width
    if (i < 26*36**(width-1)):
      i += 10*36**(width-1)
      return encode_pure(digits_upper, i)
    i -= 26*36**(width-1)
    if (i < 26*36**(width-1)):
      i += 10*36**(width-1)
      return encode_pure(digits_lower, i)
  raise ValueError("value out of range.")

def hy36decode(width, s):
  "decodes base-10/upper-case base-36/lower-case base-36 hybrid"
  if (len(s) == width):
    f = s[0]
    if (f == "-" or f == " " or f.isdigit()):
      try: return int(s)
      except ValueError: pass
      if (s == " "*width): return 0
    elif (f in digits_upper_values):
      try: return decode_pure(
        digits_values=digits_upper_values, s=s) - 10*36**(width-1) + 10**width
      except KeyError: pass
    elif (f in digits_lower_values):
      try: return decode_pure(
        digits_values=digits_lower_values, s=s) + 16*36**(width-1) + 10**width
      except KeyError: pass
  raise ValueError("invalid number literal.")

def exercise(hy36enc=hy36encode, hy36dec=hy36decode):
  for digits,digits_values in [(digits_upper, digits_upper_values),
                               (digits_lower, digits_lower_values)]:
    for value in range(1000):
      s = encode_pure(digits=digits_upper, value=value)
      d = decode_pure(digits_values=digits_upper_values, s=s)
      assert d == value
  #
  def recycle4(value, encoded):
    s = hy36enc(width=4, value=value)
    assert s == encoded
    d = hy36dec(width=4, s=s)
    assert d == value
  #
  assert hy36dec(width=4, s="    ") == 0
  assert hy36dec(width=4, s="  -0") == 0
  recycle4(-999, "-999")
  recycle4(-78, " -78")
  recycle4(-6, "  -6")
  recycle4(0, "   0")
  recycle4(9999, "9999")
  recycle4(10000, "A000")
  recycle4(10001, "A001")
  recycle4(10002, "A002")
  recycle4(10003, "A003")
  recycle4(10004, "A004")
  recycle4(10005, "A005")
  recycle4(10006, "A006")
  recycle4(10007, "A007")
  recycle4(10008, "A008")
  recycle4(10009, "A009")
  recycle4(10010, "A00A")
  recycle4(10011, "A00B")
  recycle4(10012, "A00C")
  recycle4(10013, "A00D")
  recycle4(10014, "A00E")
  recycle4(10015, "A00F")
  recycle4(10016, "A00G")
  recycle4(10017, "A00H")
  recycle4(10018, "A00I")
  recycle4(10019, "A00J")
  recycle4(10020, "A00K")
  recycle4(10021, "A00L")
  recycle4(10022, "A00M")
  recycle4(10023, "A00N")
  recycle4(10024, "A00O")
  recycle4(10025, "A00P")
  recycle4(10026, "A00Q")
  recycle4(10027, "A00R")
  recycle4(10028, "A00S")
  recycle4(10029, "A00T")
  recycle4(10030, "A00U")
  recycle4(10031, "A00V")
  recycle4(10032, "A00W")
  recycle4(10033, "A00X")
  recycle4(10034, "A00Y")
  recycle4(10035, "A00Z")
  recycle4(10036, "A010")
  recycle4(10046, "A01A")
  recycle4(10071, "A01Z")
  recycle4(10072, "A020")
  recycle4(10000+36**2-1, "A0ZZ")
  recycle4(10000+36**2, "A100")
  recycle4(10000+36**3-1, "AZZZ")
  recycle4(10000+36**3, "B000")
  recycle4(10000+26*36**3-1, "ZZZZ")
  recycle4(10000+26*36**3, "a000")
  recycle4(10000+26*36**3+35, "a00z")
  recycle4(10000+26*36**3+36, "a010")
  recycle4(10000+26*36**3+36**2-1, "a0zz")
  recycle4(10000+26*36**3+36**2, "a100")
  recycle4(10000+26*36**3+36**3-1, "azzz")
  recycle4(10000+26*36**3+36**3, "b000")
  recycle4(10000+2*26*36**3-1, "zzzz")
  #
  def recycle5(value, encoded):
    s = hy36enc(width=5, value=value)
    assert s == encoded
    d = hy36dec(width=5, s=s)
    assert d == value
  #
  assert hy36dec(width=5, s="     ") == 0
  assert hy36dec(width=5, s="   -0") == 0
  recycle5(-9999, "-9999")
  recycle5(-123, " -123")
  recycle5(-45, "  -45")
  recycle5(-6, "   -6")
  recycle5(0, "    0")
  recycle5(12, "   12")
  recycle5(345, "  345")
  recycle5(6789, " 6789")
  recycle5(99999, "99999")
  recycle5(100000, "A0000")
  recycle5(100010, "A000A")
  recycle5(100035, "A000Z")
  recycle5(100036, "A0010")
  recycle5(100046, "A001A")
  recycle5(100071, "A001Z")
  recycle5(100072, "A0020")
  recycle5(100000+36**2-1, "A00ZZ")
  recycle5(100000+36**2, "A0100")
  recycle5(100000+36**3-1, "A0ZZZ")
  recycle5(100000+36**3, "A1000")
  recycle5(100000+36**4-1, "AZZZZ")
  recycle5(100000+36**4, "B0000")
  recycle5(100000+2*36**4, "C0000")
  recycle5(100000+26*36**4-1, "ZZZZZ")
  recycle5(100000+26*36**4, "a0000")
  recycle5(100000+26*36**4+36-1, "a000z")
  recycle5(100000+26*36**4+36, "a0010")
  recycle5(100000+26*36**4+36**2-1, "a00zz")
  recycle5(100000+26*36**4+36**2, "a0100")
  recycle5(100000+26*36**4+36**3-1, "a0zzz")
  recycle5(100000+26*36**4+36**3, "a1000")
  recycle5(100000+26*36**4+36**4-1, "azzzz")
  recycle5(100000+26*36**4+36**4, "b0000")
  recycle5(100000+2*26*36**4-1, "zzzzz")
  #
  for width in [4,5]:
    for value in [-(10**(width-1)), 10**width+2*26*36**(width-1)]:
      try: hy36enc(width=width, value=value)
      except (ValueError, RuntimeError) as e:
        assert str(e) == "value out of range."
      else: raise RuntimeError("Exception expected.")
  #
  for width,ss in [
      (4, ["", "    0", " abc", "abc-", "A=BC", "40a0", "40A0"]),
      (5, ["", "     0", " abcd", "ABCD-", "a=bcd", "410b0", "410B0"])]:
    for s in ss:
      try: hy36dec(width, s=s)
      except (ValueError, RuntimeError) as e:
        assert str(e) == "invalid number literal."
      else: raise RuntimeError("Exception expected.")
  #
  import random
  value = -9999
  while value < 100000+2*26*36**4:
    try:
      s = hy36enc(width=5, value=value)
      d = hy36dec(width=5, s=s)
    except Exception:
      print("value:", value)
      raise
    assert d == value
    value += random.randint(0, 10000)

def run():
  import sys
  def usage():
    c = sys.argv[0]
    print("usage examples:")
    print("  python %s info" % c)
    print("  python %s exercise" % c)
    print("  python %s hy36encode 4 10000" % c)
    print("  python %s hy36decode 4 zzzz" % c)
    print("  python %s hy36encode 5 100000" % c)
    print("  python %s hy36decode 5 zzzzz" % c)
    sys.exit(1)
  if (len(sys.argv) < 2):
    usage()
  task = sys.argv[1]
  if (task == "info"):
    sys.stdout.write(__doc__)
    return
  if (task == "exercise"):
    exercise()
    print("OK")
  else:
    if (   len(sys.argv) != 4
        or task not in ["hy36encode", "hy36decode"]):
      usage()
    f = globals()[task]
    width = int(sys.argv[2])
    assert width > 0
    s = sys.argv[3]
    if (task == "hy36encode"):
      print(f(value=int(s), width=width))
    else:
      s = " "*max(0, width-len(s)) + s
      print(f(s=s, width=width))

if (__name__ == "__main__"):
  run()


 *******************************************************************************


 *******************************************************************************
iotbx/pdb/misc_records_output.py
"""Obsolete method to store and output custom links"""
# MARKED_FOR_DELETION_OLEG
# Reason: sub-optimal approach to store and output custom links.
# Proposal: use origin_id in bond proxies. Write a class capable of
# taking GRM and hierarchy/labels to produce links in various formats.
#
from __future__ import absolute_import, division, print_function
def link_record_output(all_chain_proxies):
  outl = ""
  for key, item in all_chain_proxies.pdb_link_records.items():
    if key=="SSBOND":
      for ssi, (atom1, atom2, sym_op) in enumerate(item):
        #
        def _format_ssbond_atom(atom):
            return "%3s %s %4s%s" % (
              atom.parent().resname,
              atom.parent().parent().parent().id,
              atom.parent().parent().resseq,
              atom.parent().parent().icode,
              )
        #
        if str(sym_op)!="x,y,z":
          continue
        outl += "SSBOND %3s %s   %s" % (
          ssi+1,
          _format_ssbond_atom(atom1),
          _format_ssbond_atom(atom2),
          )
        assert str(sym_op)=="x,y,z"
        outl += "\n"
    elif key=="LINK":
      for ssi, (atom1, atom2, sym_op) in enumerate(item):
        def _format_link_atom(atom):
          altloc = atom.parent().altloc
          if not altloc: altloc=" "
          return "%4s%s%3s%2s%4s%s" % (
            atom.name,
            altloc,
            atom.parent().resname,
            atom.parent().parent().parent().id,
            atom.parent().parent().resseq,
            atom.parent().parent().icode,
            )
        if str(sym_op)!="x,y,z":
          continue
        outl += "LINK%s%s%s%s" % (
          ' '*8,
          _format_link_atom(atom1),
          ' '*15,
          _format_link_atom(atom2),
          )
        outl += "\n"
#          print """         1         2         3         4         5         6#         7
#123456789012345678901234567890123456789012345678901234567890123456789012
#LINK         O1  DDA     1                 C3  DDL     2
#LINK        MN    MN   391                 OE2 GLU   217            2565"""
#          print outl
    else:
      raise Sorry("PDB record type %s unknown" % key)
  return outl[:-1]
# END_MARKED_FOR_DELETION_OLEG


 *******************************************************************************


 *******************************************************************************
iotbx/pdb/mmcif.py
"""Interpretation of mmCIF formatted model files"""
from __future__ import absolute_import, division, print_function
import sys
from cctbx.array_family import flex
from libtbx.containers import OrderedDict
from libtbx import group_args
from libtbx.utils import Sorry
from libtbx.str_utils import format_value
import iotbx.pdb
import cctbx.crystal
from iotbx.pdb import hierarchy
from iotbx.pdb import hy36encode
from iotbx.pdb import cryst1_interpretation
from iotbx.pdb.experiment_type import experiment_type
from iotbx.pdb.remark_3_interpretation import \
     refmac_range_to_phenix_string_selection, tls
import iotbx.cif
from iotbx.cif.builders import crystal_symmetry_builder
import iotbx.mtrix_biomt
from six import string_types
from six.moves import range, zip

class pdb_hierarchy_builder(crystal_symmetry_builder):

  # The recommended translation for ATOM records can be found at:
  #   http://mmcif.rcsb.org/dictionaries/pdb-correspondence/pdb2mmcif-2010.html#ATOM
  #   https://www.ebi.ac.uk/pdbe/docs/exchange/pdb-correspondence/pdb2mmcif.html#ATOM

  #  Note: pdb_hierarchy allows the same chain ID to be used in multiple chains.
  #    This is indicated in PDB format with a TER record indicating the end of a chain
  #    The corresponding information in mmCIF is the label_asym_id changes for each
  #    new chain (the auth_asym_id matches the chain ID).
  #    Catch cases where one chain id is split into multiple chains in the following way:
  #    If label_asym_id changes and previous residue was protein or rna/dna or modified
  #    (or auth_asym_id or model_id changes), create a chain break.

  def __init__(self, cif_block):
    crystal_symmetry_builder.__init__(self, cif_block)

    self.hierarchy = hierarchy.root()
    # These items are mandatory for the _atom_site loop, all others are optional
    type_symbol = self._wrap_loop_if_needed(cif_block, "_atom_site.type_symbol")
    atom_labels = self._wrap_loop_if_needed(cif_block, "_atom_site.auth_atom_id")
    if atom_labels is None:
      atom_labels = self._wrap_loop_if_needed(cif_block, "_atom_site.label_atom_id") # corresponds to chem comp atom name
    alt_id = self._wrap_loop_if_needed(cif_block,"_atom_site.label_alt_id") # alternate conformer id
    label_asym_id = self._wrap_loop_if_needed(cif_block, "_atom_site.label_asym_id") # chain id
    auth_asym_id = self._wrap_loop_if_needed(cif_block, "_atom_site.auth_asym_id")
    auth_segid = self._wrap_loop_if_needed(cif_block, "_atom_site.auth_segid")
    auth_break = self._wrap_loop_if_needed(cif_block, "_atom_site.auth_break")
    if label_asym_id is None: label_asym_id = auth_asym_id
    if auth_asym_id is None: auth_asym_id = label_asym_id
    comp_id = self._wrap_loop_if_needed(cif_block, "_atom_site.auth_comp_id")
    if comp_id is None:
      comp_id = self._wrap_loop_if_needed(cif_block, "_atom_site.label_comp_id") # residue name
    entity_id = self._wrap_loop_if_needed(cif_block, "_atom_site.label_entity_id")
    seq_id = self._wrap_loop_if_needed(cif_block, "_atom_site.auth_seq_id")
    if seq_id is None:
      seq_id = self._wrap_loop_if_needed(cif_block, "_atom_site.label_seq_id") # residue number
    error_message = """something is not present - not enough information
    to make a hierarcy out of this CIF file.
    It could be because this is restraints or component cif or a corrupted model cif."""
    assert [atom_labels, alt_id, auth_asym_id, comp_id, entity_id, seq_id].count(None) == 0, error_message
    assert type_symbol is not None

    atom_site_fp = cif_block.get('_atom_site.phenix_scat_dispersion_real')
    atom_site_fdp = cif_block.get('_atom_site.phenix_scat_dispersion_imag')

    pdb_ins_code = cif_block.get("_atom_site.pdbx_PDB_ins_code") # insertion code
    model_ids = cif_block.get("_atom_site.pdbx_PDB_model_num")
    atom_site_id = cif_block.get("_atom_site.id")
    # only permitted values are ATOM or HETATM
    group_PDB = cif_block.get("_atom_site.group_PDB")
    # TODO: read esds
    B_iso_or_equiv = flex.double(self._wrap_loop_if_needed(cif_block, "_atom_site.B_iso_or_equiv"))
    cart_x = flex.double(self._wrap_loop_if_needed(cif_block, "_atom_site.Cartn_x"))
    cart_y = flex.double(self._wrap_loop_if_needed(cif_block, "_atom_site.Cartn_y"))
    cart_z = flex.double(self._wrap_loop_if_needed(cif_block, "_atom_site.Cartn_z"))
    occu =   flex.double(self._wrap_loop_if_needed(cif_block, "_atom_site.occupancy"))
    formal_charge = self._wrap_loop_if_needed(cif_block, "_atom_site.pdbx_formal_charge")
    # anisotropic b-factors
    # TODO: read esds
    anisotrop_id = self._wrap_loop_if_needed(cif_block, "_atom_site_anisotrop.id")
    adps = None
    if anisotrop_id is not None:
      u_ij = [self._wrap_loop_if_needed(cif_block, "_atom_site_anisotrop.U[%s][%s]" %(ij[0], ij[1]))
              for ij in ("11", "22", "33", "12", "13", "23")]
      assert u_ij.count(None) in (0, 6)
      if u_ij.count(None) == 0:
        adps = u_ij
      else:
        assert u_ij.count(None) == 6
        b_ij = [self._wrap_loop_if_needed(cif_block, "_atom_site_anisotrop.B[%s][%s]" %(ij[0], ij[1]))
                for ij in ("11", "22", "33", "12", "13", "23")]
        assert b_ij.count(None) in (0, 6)
        if b_ij.count(None) == 0:
          adps = adptbx.b_as_u(b_ij)
        assert not (u_ij.count(None) and b_ij.count(None)) # illegal for both to be present
      if adps is not None:
        try:
          adps = [flex.double(adp) for adp in adps]
        except ValueError as e:
          raise CifBuilderError("Error interpreting ADPs: " + str(e))
        adps = flex.sym_mat3_double(*adps)
    py_adps = {}
    if anisotrop_id is not None and adps is not None:
      for an_id, adp in zip(list(anisotrop_id), list(adps)):
        py_adps[an_id] = adp
    current_model_id = None
    current_label_asym_id = None
    current_auth_asym_id = None
    current_residue_id = None
    current_ins_code = None

    for i_atom in range(atom_labels.size()):
      # model(s)
      last_model_id = current_model_id
      current_model_id = model_ids[i_atom]
      assert current_model_id is not None
      if current_model_id != last_model_id:
        model = hierarchy.model(id=current_model_id)
        self.hierarchy.append_model(model)

      # chain(s)
      last_label_asym_id = current_label_asym_id
      current_label_asym_id = label_asym_id[i_atom]
      assert current_label_asym_id is not None
      last_auth_asym_id = current_auth_asym_id
      current_auth_asym_id = auth_asym_id[i_atom]
      assert current_auth_asym_id not in [".", "?", " "], "mmCIF file contains " + \
        "record with empty auth_asym_id, which is wrong."
      assert current_label_asym_id is not None
      if (current_auth_asym_id != last_auth_asym_id
          or current_model_id != last_model_id
          or (current_label_asym_id != last_label_asym_id and
             i_atom > 0 and is_aa_or_rna_dna(comp_id[i_atom-1]))
         ): # insert chain breaks
        chain = hierarchy.chain(id=current_auth_asym_id)
        is_first_in_chain = None
        model.append_chain(chain)
      else:
        assert current_auth_asym_id == last_auth_asym_id

      # residue_group(s)
      # defined by residue id and insertion code
      last_residue_id = current_residue_id
      current_residue_id = seq_id[i_atom]
      assert current_residue_id is not None
      last_ins_code = current_ins_code
      if pdb_ins_code is not None:
        current_ins_code = pdb_ins_code[i_atom]
        if current_ins_code in ("?", ".", None): current_ins_code = " "
      if (current_residue_id != last_residue_id
          or current_ins_code != last_ins_code
          or current_auth_asym_id != last_auth_asym_id
          or current_model_id != last_model_id):
        try:
          resseq = hy36encode(width=4, value=int(current_residue_id))
        except ValueError as e:
          resseq = current_residue_id
          assert len(resseq)==4
        residue_group = hierarchy.residue_group(
          resseq=resseq,
          icode=current_ins_code)
        chain.append_residue_group(residue_group)
        if is_first_in_chain is None:
          is_first_in_chain = True
        else:
          is_first_in_chain = False
        atom_groups = OrderedDict() # reset atom_groups cache
      # atom_group(s)
      # defined by resname and altloc id
      current_altloc = alt_id[i_atom]
      if current_altloc == "." or current_altloc == "?":
        current_altloc = "" # Main chain atoms
      current_resname = comp_id[i_atom]
      if (current_altloc, current_resname) not in atom_groups:
        atom_group = hierarchy.atom_group(
          altloc=current_altloc, resname="%3s" % current_resname)
        atom_groups[(current_altloc, current_resname)] = atom_group
        if current_altloc == "":
          residue_group.insert_atom_group(0, atom_group)
        else:
          residue_group.append_atom_group(atom_group)
      else:
        atom_group = atom_groups[(current_altloc, current_resname)]

      # atom(s)
      atom = hierarchy.atom()
      atom_group.append_atom(atom)
      atom.set_element(type_symbol[i_atom])
      atom.set_name(
        format_pdb_atom_name(atom_labels[i_atom], type_symbol[i_atom]))
      atom.set_xyz(
        new_xyz=(cart_x[i_atom], cart_y[i_atom], cart_z[i_atom]))
      atom.set_b(B_iso_or_equiv[i_atom])
      atom.set_occ(occu[i_atom])
      # hy36encode should go once the pdb.hierarchy has been
      # modified to no longer store fixed-width strings
      atom.set_serial(
        hy36encode(width=5, value=int(atom_site_id[i_atom])))
      # some code relies on an empty segid being 4 spaces
      if auth_segid:
        atom.set_segid(auth_segid[i_atom][:4]+(4-len(auth_segid[i_atom]))*" ")
      else:
        atom.set_segid("    ")
      if auth_break and (not is_first_in_chain) and auth_break[i_atom] == "1":
        # insert break before this residue
        residue_group.link_to_previous = False
      if group_PDB is not None and group_PDB[i_atom] == "HETATM":
        atom.hetero = True
      if formal_charge is not None:
        charge = formal_charge[i_atom]
        if charge not in ("?", "."):
          if charge.endswith("-") or charge.startswith("-"):
            sign = "-"
          else:
            sign = "+"
          charge = charge.strip(" -+")
          charge = int(charge)
          if charge == 0: sign = ""
          atom.set_charge("%i%s" %(charge, sign))
      if atom_site_fp is not None:
        fp = atom_site_fp[i_atom]
        if fp not in ("?", "."):
          atom.set_fp(new_fp=float(fp))
      if atom_site_fdp is not None:
        fdp = atom_site_fdp[i_atom]
        if fdp not in ("?", "."):
          atom.set_fdp(new_fdp=float(fdp))
      if anisotrop_id is not None and adps is not None:
        py_u_ij = py_adps.get(atom.serial.strip(), None)
        if py_u_ij is not None:
          atom.set_uij(py_u_ij)
    if len(self.hierarchy.models()) == 1:
      # for compatibility with single-model PDB files
      self.hierarchy.models()[0].id = ""

  def _wrap_loop_if_needed(self, cif_block, name):
    data = cif_block.get(name)
    if data is None:
      return data
    if isinstance(data, string_types):
      data = flex.std_string([data])
    return data


def is_aa_or_rna_dna(resname):
   from iotbx.pdb import common_residue_names_get_class
   resname = resname.strip()
   residue_class = common_residue_names_get_class(resname)
   if residue_class in ['common_amino_acid', 'modified_amino_acid',
                'common_rna_dna', 'modified_rna_dna']:
     return True
   else:
     return False

def format_pdb_atom_name(atom_name, atom_type):
  # The PDB-format atom name is 4 characters long (columns 13 - 16):
  #   "Alignment of one-letter atom name such as C starts at column 14,
  #    while two-letter atom name such as FE starts at column 13."
  # http://www.wwpdb.org/documentation/format33/sect9.html#ATOM
  # Here we assume that "atom name" refers to the atom/element TYPE.

  if len(atom_name) > 4:
    # XXX will bad things happen if the atom name is more than 4 characters?
    return atom_name
  elif len(atom_name) == 4:
    return atom_name
  atom_name = atom_name.strip()
  if len(atom_type) == 1:
    if atom_name.upper()[0] == atom_type.upper():
      atom_name = " %s" %atom_name
  elif len(atom_type) == 2:
    if atom_name.upper()[0] == atom_type.upper():
      atom_name = atom_name
  while len(atom_name) < 4:
    atom_name = "%s " %atom_name
  return atom_name


class extract_tls_from_cif_block(object):
  def __init__(self, cif_block, pdb_hierarchy):
    self.tls_params = []
    self.tls_present = False
    self.error_string = None
    tls_ids = cif_block.get('_pdbx_refine_tls.id')

    T_ijs = [cif_block.get('_pdbx_refine_tls.T[%s][%s]' %(i, j))
             for i, j in ('11', '22', '33', '12', '13', '23')]
    L_ijs = [cif_block.get('_pdbx_refine_tls.L[%s][%s]' %(i, j))
             for i, j in ('11', '22', '33', '12', '13', '23')]
    S_ijs = [cif_block.get('_pdbx_refine_tls.S[%s][%s]' %(i, j))
             for i, j in ('11', '12', '13', '21', '22', '23', '31', '32', '33')]
    origin_xyzs = [cif_block.get('_pdbx_refine_tls.origin_%s' %x) for x in 'xyz']

    if isinstance(tls_ids, string_types):
      # in case the TLS items are not in a loop
      tls_ids = [tls_ids]
      T_ijs = [[T_ij] for T_ij in T_ijs]
      L_ijs = [[L_ij] for L_ij in L_ijs]
      S_ijs = [[S_ij] for S_ij in S_ijs]
      origin_xyzs = [[origin_xyz] for origin_xyz in origin_xyzs]

    assert T_ijs.count(None) in (0, 6)
    assert L_ijs.count(None) in (0, 6)
    assert S_ijs.count(None) in (0, 9)
    assert origin_xyzs.count(None) in (0, 3)

    if T_ijs.count(None) == 6:
      return

    tls_group_ids = cif_block.get('_pdbx_refine_tls_group.id')
    refine_tls_ids = cif_block.get('_pdbx_refine_tls_group.refine_tls_id')
    beg_chain_ids = cif_block.get('_pdbx_refine_tls_group.beg_auth_asym_id')
    beg_seq_ids = cif_block.get('_pdbx_refine_tls_group.beg_auth_seq_id')
    end_chain_ids = cif_block.get('_pdbx_refine_tls_group.end_auth_asym_id')
    end_seq_ids = cif_block.get('_pdbx_refine_tls_group.end_auth_seq_id')
    selection_details = cif_block.get('_pdbx_refine_tls_group.selection_details')

    assert tls_group_ids is not None

    if isinstance(tls_group_ids, string_types):
      # in case the TLS group items are not in a loop
      refine_tls_ids = flex.std_string([refine_tls_ids])
      beg_chain_ids = [beg_chain_ids]
      beg_seq_ids = [beg_seq_ids]
      end_chain_ids = [end_chain_ids]
      end_seq_ids = [end_seq_ids]
      if selection_details is not None:
        selection_details = flex.std_string([selection_details])

    selection_cache = pdb_hierarchy.atom_selection_cache()

    for i, tls_id in enumerate(tls_ids):
      T = [float(T_ij[i]) for T_ij in T_ijs]
      L = [float(L_ij[i]) for L_ij in L_ijs]
      S = [float(S_ij[i]) for S_ij in S_ijs]
      origin = [float(origin_x[i]) for origin_x in origin_xyzs]
      i_groups = (refine_tls_ids == tls_id).iselection()
      sel_strings = []
      for i_group in i_groups:
        if selection_details is not None and not selection_details.all_eq('?'):
          # phenix selection
          sel_strings.append(selection_details[i_group].strip())
          # check it is a valid selection string
          selection_cache.selection(sel_strings[-1])
        else:
          sel_strings.append(refmac_range_to_phenix_string_selection(
            pdb_hierarchy=pdb_hierarchy,
            chain_start=beg_chain_ids[i_group],
            resseq_start=beg_seq_ids[i_group],
            chain_end=end_chain_ids[i_group],
            resseq_end=end_seq_ids[i_group]))
      sel_str = " or ".join(sel_strings)
      self.tls_params.append(tls(
        t=T, l=L, s=S, origin=origin, selection_string=sel_str))
    self.tls_present = True

  def extract(self):
    return self.tls_params

class cif_input(iotbx.pdb.pdb_input_mixin):

  def __init__(self,
               file_name=None,
               cif_object=None,
               source_info=iotbx.pdb.Please_pass_string_or_None,
               lines=None,
               raise_sorry_if_format_error=False):
    if file_name is not None:
      reader = iotbx.cif.reader(file_path=file_name)
      self.cif_model = reader.model()
    elif lines is not None:
      if not isinstance( lines, str ):
        lines = "\n".join(lines)
      reader = iotbx.cif.reader(input_string=lines)
      self.cif_model = reader.model()
    elif cif_object is not None:
      self.cif_model = cif_object
    if len(self.cif_model) == 0:
      raise Sorry("mmCIF file must contain at least one data block")
    self.cif_block = list(self.cif_model.values())[0]
    self._source_info = "file %s" %file_name
    self.hierarchy = None
    self.builder = None

  def file_type(self):
    return "mmcif"

  def construct_hierarchy(self, set_atom_i_seq=True, sort_atoms=True):
    if self.hierarchy is not None:
      return self.hierarchy
    self.builder = pdb_hierarchy_builder(self.cif_block)
    self.hierarchy = self.builder.hierarchy
    if sort_atoms:
      self.hierarchy.sort_atoms_in_place()
      if (set_atom_i_seq):
        self.hierarchy.reset_atom_i_seqs()
      self.hierarchy.atoms_reset_serial()
    return self.hierarchy

  def label_to_auth_asym_id_dictionary(self):
    auth_asym = self.cif_block.get('_atom_site.auth_asym_id')
    label_asym = self.cif_block.get('_atom_site.label_asym_id')
    assert len(label_asym) == len(auth_asym)
    return dict(zip(label_asym, auth_asym))

  def source_info(self):
    return self._source_info

  def atoms(self):
    if self.hierarchy is None:
      self.construct_hierarchy()
    return self.hierarchy.atoms()

  def atoms_with_labels(self):
    if self.hierarchy is None:
      self.construct_hierarchy()
    return self.hierarchy.atoms_with_labels()

  def model_indices(self):
    if self.hierarchy is None:
      self.construct_hierarchy()
    mi = flex.size_t([m.atoms_size() for m in self.hierarchy.models()])
    for i in range(1, len(mi)):
      mi[i] += mi[i-1]
    return mi

  def ter_indices(self):
    # for compatibility with pdb_input
    return flex.size_t()

  def crystal_symmetry(self,
                       crystal_symmetry=None,
                       weak_symmetry=False):
    self_symmetry = self.crystal_symmetry_from_cryst1()
    if (crystal_symmetry is None):
      return self_symmetry
    if (self_symmetry is None):
      return crystal_symmetry
    return self_symmetry.join_symmetry(
      other_symmetry=crystal_symmetry,
      force=not weak_symmetry)

  def crystal_symmetry_from_cryst1(self):
    if self.hierarchy is None:
      self.construct_hierarchy()
    builder_cs = self.builder.crystal_symmetry
    # check for dummy one
    if (builder_cs.unit_cell() is not None and
        cryst1_interpretation.dummy_unit_cell(
            abc = builder_cs.unit_cell().parameters()[:3],
            abg = builder_cs.unit_cell().parameters()[3:],
            sg_symbol=str(builder_cs.space_group_info()))):
      return cctbx.crystal.symmetry(
        unit_cell=None,
        space_group_info=None)
    return builder_cs


  def extract_cryst1_z_columns(self):
    return self.cif_model.values()[0].get("_cell.Z_PDB")

  def connectivity_section(self):
    # XXX Should we extract something from the CIF and return a PDB-like
    # CONECT record, or rework this whole thing?
    return ""

  def connectivity_annotation_section(self):
    return ""

  def extract_secondary_structure(self, log=None):
    from iotbx.pdb import secondary_structure
    return secondary_structure.annotation.from_cif_block(self.cif_block, log=log)

  def remark_section(self):
    return ""

  def heterogen_section(self):
    return ""

  def crystallographic_section(self):
    return ""

  def title_section(self):
    return ""

  def extract_header_year(self):
    yyyymmdd = self.deposition_date()
    if yyyymmdd is not None:
      try:
        return int(yyyymmdd[:4])
      except ValueError:
        pass
    return None


  def deposition_date(self, us_style=True):
    # date format: yyyy-mm-dd
    cif_block = list(self.cif_model.values())[0]
    date_orig = cif_block.get("_pdbx_database_status.recvd_initial_deposition_date")
    result = date_orig
    if not us_style:
      raise NotImplementedError
    return result
    #rev_num = cif_block.get('_database_PDB_rev.num')
    #if rev_num is not None:
    #  date_original = cif_block.get('_database_PDB_rev.date_original')
    #  if isinstance(rev_num, string_types):
    #    return date_original
    #  else:
    #    i = flex.first_index(rev_num, '1')
    #    if date_original is not None:
    #      return date_original[i]

  def get_r_rfree_sigma(self, file_name=None):
    return _cif_get_r_rfree_sigma_object(self.cif_block, file_name)

  def get_solvent_content(self):
    return _float_or_None(self.cif_block.get('_exptl_crystal.density_percent_sol'))

  def get_matthews_coeff(self):
    return _float_or_None(self.cif_block.get('_exptl_crystal.density_Matthews'))

  def get_program_name(self):
    software_name = self.cif_block.get('_software.name')
    software_classification = self.cif_block.get('_software.classification')
    if isinstance(software_classification, string_types):
      if software_classification == 'refinement':
        return software_name
    elif software_classification is not None:
      i = flex.first_index(software_classification, 'refinement')
      if (i is not None) and (i >= 0) and (software_name is not None) and (
           i < len(software_name)):
        return software_name[i]

  def resolution(self):
    result = []
    r1 = _float_or_None(self.cif_block.get('_reflns.d_resolution_high'))
    r2 = _float_or_None(self.cif_block.get('_reflns.resolution'))
    r3 = _float_or_None(self.cif_block.get('_em_3d_reconstruction.resolution'))
    r4 = _float_or_None(self.cif_block.get('_refine.ls_d_res_high'))
    for r in [r1,r2,r3,r4]:
      if(r is not None): result.append(r)
    result = list(set(result))
    if(len(result)  ==0): result = None
    elif(len(result)==1): result = result[0]
    elif(len(result)>1):
      if(r4 is None):
        result = r1
      else:
        result = min(result)
    return result

  def extract_tls_params(self, hierarchy):
    return extract_tls_from_cif_block(self.cif_block, hierarchy)

  def scale_matrix(self):
    if (not hasattr(self, "_scale_matrix")):
      fractionalization_matrix = [
        self.cif_block.get('_atom_sites.fract_transf_matrix[%s][%s]' %(i, j))
        for i,j in ('11', '12', '13', '21', '22', '23', '31','32', '33')]
      if fractionalization_matrix.count(None) == 9:
        return None
      assert fractionalization_matrix.count(None) == 0
      fractionalization_vector = [
        self.cif_block.get('_atom_sites.fract_transf_vector[%s]' %i) for i in '123']
      assert fractionalization_matrix.count(None) == 0
      self._scale_matrix = [[float(i) for i in fractionalization_matrix],
                            [float(i) for i in fractionalization_vector]]
    return self._scale_matrix

  def model_ids(self):
    return flex.std_string([model.id for model in self.hierarchy.models()])

  def extract_f_model_core_constants(self):
    return extract_f_model_core_constants(self.cif_block)

  def extract_wavelength(self, first_only=True):
    wavelengths = self.cif_block.get('_diffrn_source.pdbx_wavelength_list')
    wavelength = _float_or_None(self.cif_block.get(
        '_diffrn_source.pdbx_wavelength'))
    if (first_only):
      if (wavelength is not None):
        return wavelength
      elif (wavelengths is not None):
        wavelengths = [ float(f.strip()) for f in wavelengths.split(",") ]
        return wavelengths[0]
    elif (wavelengths is not None):
        return [ float(f) for f in wavelengths.split(",") ]
    return None

  def get_experiment_type(self):
    exptl_method = self.cif_block.get('_exptl.method')
    lines = []
    if exptl_method is not None:
      if(isinstance(exptl_method,flex.std_string)):
        lines = list(exptl_method)
      else:
        lines = [exptl_method]
    return experiment_type(lines)

  def process_BIOMT_records(self):
    import iotbx.mtrix_biomt
    return iotbx.mtrix_biomt.process_BIOMT_records_cif(
      cif_block = self.cif_block)

  def process_MTRIX_records(self):
    """
    Read MTRIX records from a pdb file

    Arguments:
    ----------

    Returns:
    --------
    result : object containing information on all NCS operations
             iotbx.mtrix_biomt.container
    """
    import iotbx.mtrix_biomt
    return iotbx.mtrix_biomt.process_MTRIX_records_cif(
      cif_block = self.cif_block)

  def get_restraints_used(self):
    return {'CDL' : self.used_cdl_restraints(),
            'omega' : self.used_omega_restraints(),
            'Amber' : self.used_amber_restraints(),
           }

  def _used_what_restraints(self, what):
    rc = False
    for cif_key, cif_block in self.cif_model.items():
      target = cif_block.get("_refine.pdbx_stereochemistry_target_values")
      if (target is not None) and (what in target):
        rc = True
        break
    return rc

  def used_cdl_restraints(self):
    return self._used_what_restraints('CDL')

  def used_omega_cdl_restraints(self):
    return self._used_what_restraints('omega-cdl')

  def used_amber_restraints(self):
    return self._used_what_restraints('Amber')

def _float_or_None(value):
  if value is not None:
    if value == '?' or value == '.':
      return None
    return float(value)

class _cif_get_r_rfree_sigma_object(object):
  def __init__(self, cif_block, file_name):
    self.file_name = file_name
    self.r_work = _float_or_None(cif_block.get('_refine.ls_R_factor_R_work'))
    self.r_free = _float_or_None(cif_block.get('_refine.ls_R_factor_R_free'))
    self.sigma = _float_or_None(cif_block.get('_refine.pdbx_ls_sigma_F'))
    self.high = _float_or_None(cif_block.get('_refine.ls_d_res_high'))
    self.low = _float_or_None(cif_block.get('_refine.ls_d_res_low'))
    self.resolution = _float_or_None(cif_block.get('_reflns.d_resolution_high'))

  def formatted_string(self):
    result = "%s %s %s %s %s %s %s" % (
      format_value("%6s",self.file_name),
      format_value("%6s",str(self.r_work)),
      format_value("%6s",str(self.r_free)),
      format_value("%6s",str(self.sigma)),
      format_value("%6s",str(self.high)),
      format_value("%6s",str(self.low)),
      format_value("%6s",str(self.resolution)))
    return result

  def show(self, log = None):
    if(log is None): log = sys.stdout
    print(self.formatted_string(), file=log)


def extract_f_model_core_constants(cif_block):
  k_sol = _float_or_None(cif_block.get('_refine.solvent_model_param_ksol'))
  b_sol = _float_or_None(cif_block.get('_refine.solvent_model_param_bsol'))
  b_cart = [_float_or_None(cif_block.get('_refine.aniso_B[%s][%s]' %(i, j)))
            for i, j in ('11', '22', '33', '12', '13', '23')]
  assert b_cart.count(None) in (0, 6)
  r_solv = _float_or_None(cif_block.get('_refine.pdbx_solvent_vdw_probe_radii'))
  r_shrink = _float_or_None(cif_block.get('_refine.pdbx_solvent_shrinkage_radii'))
  r_work = _float_or_None(cif_block.get('_refine.ls_R_factor_R_work'))
  r_free = _float_or_None(cif_block.get('_refine.ls_R_factor_R_free'))

  twin_fraction = _float_or_None(cif_block.get('_pdbx_reflns_twin.fraction'))
  twin_law = cif_block.get('_pdbx_reflns_twin.operator')
  # TODO: extract these from the CIF?
  grid_step_factor = None
  return group_args(
    k_sol            = k_sol,
    b_sol            = b_sol,
    b_cart           = b_cart,
    twin_fraction    = twin_fraction,
    twin_law         = twin_law,
    r_solv           = r_solv,
    r_shrink         = r_shrink,
    grid_step_factor = grid_step_factor,
    r_work           = r_work,
    r_free           = r_free)


 *******************************************************************************


 *******************************************************************************
iotbx/pdb/modified_aa_names.py
"""
This file contains all residue names for modified amino acids linked to their
one letter standard parent amino acid.  This file is generated by the following
procedure:

  phenix.python elbow/elbow/scripts/process_amino_acid_parentage_from_chemical_componts.py

This file is intended to be generated monthly.
The date of file generation: Mon Apr 28 12:18:45 2025
"""
from __future__ import absolute_import, division, print_function

lookup = {
  "004" : "?",
  "005" : "?",
  "006" : "?",
  "00B" : "?",
  "00C" : "?",
  "00E" : "?",
  "00F" : "?",
  "00H" : "?",
  "00I" : "?",
  "00K" : "?",
  "00L" : "?",
  "00M" : "?",
  "00N" : "?",
  "00O" : "?",
  "00P" : "?",
  "00Q" : "?",
  "00R" : "?",
  "00U" : "?",
  "00V" : "?",
  "00W" : "?",
  "00Y" : "?",
  "011" : "?",
  "015" : "?",
  "016" : "?",
  "019" : "?",
  "01B" : "?",
  "01C" : "?",
  "01E" : "?",
  "01J" : "?",
  "01L" : "?",
  "01M" : "?",
  "01N" : "?",
  "01S" : "?",
  "01W" : "?",
  "01X" : "?",
  "021" : "?",
  "029" : "?",
  "02A" : "?",
  "02J" : "?",
  "02P" : "?",
  "02V" : "?",
  "038" : "?",
  "03E" : "?",
  "04C" : "?",
  "04Q" : "?",
  "04R" : "?",
  "04X" : "?",
  "05O" : "?",
  "060" : "?",
  "06P" : "?",
  "074" : "?",
  "075" : "?",
  "076" : "?",
  "079" : "?",
  "08L" : "?",
  "08M" : "?",
  "08X" : "?",
  "0AY" : "?",
  "0AZ" : "?",
  "0D3" : "?",
  "0D5" : "?",
  "0D6" : "?",
  "0DB" : "?",
  "0DO" : "?",
  "0DS" : "?",
  "0DY" : "?",
  "0E2" : "?",
  "0E3" : "?",
  "0E4" : "?",
  "0E6" : "?",
  "0E7" : "?",
  "0E8" : "?",
  "0E9" : "?",
  "0ED" : "?",
  "0EF" : "?",
  "0EG" : "?",
  "0EH" : "?",
  "0EK" : "?",
  "0EL" : "?",
  "0EM" : "?",
  "0EO" : "?",
  "0EZ" : "?",
  "0F7" : "?",
  "0FA" : "?",
  "0FG" : "?",
  "0FP" : "?",
  "0G5" : "?",
  "0G6" : "?",
  "0G7" : "?",
  "0GE" : "?",
  "0GG" : "?",
  "0GJ" : "?",
  "0GM" : "?",
  "0GQ" : "?",
  "0GR" : "?",
  "0H8" : "?",
  "0HG" : "?",
  "0HH" : "?",
  "0HT" : "?",
  "0HW" : "?",
  "0HZ" : "?",
  "0I5" : "?",
  "0IT" : "?",
  "0IV" : "?",
  "0IW" : "?",
  "0JT" : "?",
  "0KV" : "?",
  "0MG" : "?",
  "0ML" : "?",
  "0MU" : "?",
  "0P0" : "?",
  "0P1" : "?",
  "0P2" : "?",
  "0PC" : "?",
  "0PI" : "?",
  "0PJ" : "?",
  "0PK" : "?",
  "0PN" : "?",
  "0PO" : "?",
  "0PP" : "?",
  "0PQ" : "?",
  "0PX" : "?",
  "0Q4" : "?",
  "0QF" : "?",
  "0QG" : "?",
  "0QH" : "?",
  "0QI" : "?",
  "0QN" : "?",
  "0QS" : "?",
  "0QZ" : "?",
  "0RJ" : "?",
  "0TH" : "?",
  "0UZ" : "?",
  "0W6" : "?",
  "0XL" : "?",
  "0XO" : "?",
  "0XQ" : "?",
  "0Y9" : "?",
  "0Z0" : "?",
  "0Z1" : "?",
  "0Z2" : "?",
  "0Z3" : "?",
  "0Z4" : "?",
  "0Z6" : "?",
  "0Z9" : "?",
  "0ZB" : "?",
  "0ZC" : "?",
  "0ZE" : "?",
  "0ZG" : "?",
  "0ZI" : "?",
  "0ZJ" : "?",
  "0ZL" : "?",
  "0ZM" : "?",
  "0ZN" : "?",
  "0ZO" : "?",
  "0ZP" : "?",
  "0ZQ" : "?",
  "0ZR" : "?",
  "0ZS" : "?",
  "0ZT" : "?",
  "0ZW" : "?",
  "0ZX" : "?",
  "0ZY" : "?",
  "0ZZ" : "?",
  "10P" : "?",
  "10U" : "?",
  "11U" : "?",
  "13E" : "?",
  "15U" : "?",
  "162" : "?",
  "163" : "?",
  "165" : "?",
  "16U" : "?",
  "176" : "?",
  "177" : "?",
  "192" : "?",
  "193" : "?",
  "19J" : "?",
  "19L" : "?",
  "19M" : "?",
  "19U" : "?",
  "19W" : "?",
  "1C3" : "?",
  "1E3" : "?",
  "1G1" : "?",
  "1G2" : "?",
  "1G3" : "?",
  "1G5" : "?",
  "1G6" : "?",
  "1G8" : "?",
  "1HB" : "?",
  "1HD" : "?",
  "1I7" : "?",
  "1IC" : "?",
  "1IP" : "?",
  "1JM" : "?",
  "1ME" : "?",
  "1MH" : "?",
  "1OL" : "?",
  "1QI" : "?",
  "1TS" : "?",
  "1TX" : "?",
  "1U8" : "?",
  "1VP" : "?",
  "1VR" : "?",
  "1XW" : "?",
  "1Y0" : "?",
  "1YH" : "?",
  "1Z0" : "?",
  "1Z1" : "?",
  "1Z7" : "?",
  "1ZB" : "?",
  "1ZD" : "?",
  "1ZG" : "?",
  "1ZK" : "?",
  "1ZN" : "?",
  "1ZT" : "?",
  "1ZV" : "?",
  "1ZX" : "?",
  "22G" : "?",
  "22U" : "?",
  "23S" : "?",
  "24M" : "?",
  "24O" : "?",
  "28J" : "?",
  "2A0" : "?",
  "2AD" : "?",
  "2AO" : "?",
  "2AS" : "?",
  "2DO" : "?",
  "2G6" : "?",
  "2G8" : "?",
  "2G9" : "?",
  "2GC" : "?",
  "2GZ" : "?",
  "2H0" : "?",
  "2HM" : "?",
  "2JC" : "?",
  "2JF" : "?",
  "2JH" : "?",
  "2JJ" : "?",
  "2JN" : "?",
  "2KY" : "?",
  "2KZ" : "?",
  "2L0" : "?",
  "2L5" : "?",
  "2L6" : "?",
  "2L9" : "?",
  "2LR" : "?",
  "2LV" : "?",
  "2M1" : "?",
  "2MK" : "?",
  "2NC" : "?",
  "2OY" : "?",
  "2PI" : "?",
  "2PS" : "?",
  "2QD" : "?",
  "2QY" : "?",
  "2R1" : "?",
  "2TL" : "?",
  "2TS" : "?",
  "2UA" : "?",
  "2Y2" : "?",
  "2Y3" : "?",
  "2Y4" : "?",
  "2YC" : "?",
  "2YF" : "?",
  "2YG" : "?",
  "2YH" : "?",
  "2YJ" : "?",
  "2YS" : "?",
  "2YT" : "?",
  "2Z0" : "?",
  "2Z3" : "?",
  "2Z4" : "?",
  "2Z5" : "?",
  "2Z9" : "?",
  "2ZF" : "?",
  "2ZS" : "?",
  "32L" : "?",
  "32S" : "?",
  "32T" : "?",
  "32U" : "?",
  "33U" : "?",
  "33X" : "?",
  "34P" : "?",
  "35Y" : "?",
  "37U" : "?",
  "37Y" : "?",
  "38N" : "?",
  "38X" : "?",
  "39Q" : "?",
  "39S" : "?",
  "39V" : "?",
  "3A1" : "?",
  "3A2" : "?",
  "3A5" : "?",
  "3AI" : "?",
  "3AR" : "?",
  "3AZ" : "?",
  "3BV" : "?",
  "3E5" : "?",
  "3EG" : "?",
  "3FG" : "?",
  "3FO" : "?",
  "3FS" : "?",
  "3FU" : "?",
  "3K4" : "?",
  "3NS" : "?",
  "3OE" : "?",
  "3PM" : "?",
  "3QP" : "?",
  "3SD" : "?",
  "3SP" : "?",
  "3TL" : "?",
  "3TY" : "?",
  "3VH" : "?",
  "3WD" : "?",
  "3WT" : "?",
  "3WU" : "?",
  "3WV" : "?",
  "3WW" : "?",
  "3WY" : "?",
  "3WZ" : "?",
  "3XI" : "?",
  "3ZL" : "?",
  "40A" : "?",
  "40C" : "?",
  "40G" : "?",
  "40I" : "?",
  "40T" : "?",
  "45U" : "?",
  "45W" : "?",
  "46U" : "?",
  "48V" : "?",
  "4B5" : "?",
  "4CG" : "?",
  "4DB" : "?",
  "4FO" : "?",
  "4H0" : "?",
  "4IK" : "?",
  "4J2" : "?",
  "4L8" : "?",
  "4M8" : "?",
  "4N3" : "?",
  "4OP" : "?",
  "4P2" : "?",
  "4PK" : "?",
  "4Q5" : "?",
  "4QK" : "?",
  "4SF" : "?",
  "4SH" : "?",
  "4SL" : "?",
  "4UD" : "?",
  "50U" : "?",
  "51U" : "?",
  "53U" : "?",
  "55N" : "?",
  "55Q" : "?",
  "562" : "?",
  "56C" : "?",
  "59S" : "?",
  "5A6" : "?",
  "5DM" : "?",
  "5DW" : "?",
  "5F0" : "?",
  "5FE" : "?",
  "5FP" : "?",
  "5IW" : "?",
  "5IZ" : "?",
  "5LB" : "?",
  "5LE" : "?",
  "5LF" : "?",
  "5LG" : "?",
  "5LH" : "?",
  "5LJ" : "?",
  "5OL" : "?",
  "5OM" : "?",
  "5S4" : "?",
  "5TP" : "?",
  "5X8" : "?",
  "62H" : "?",
  "64U" : "?",
  "66C" : "?",
  "66E" : "?",
  "6DU" : "?",
  "6E4" : "?",
  "6FL" : "?",
  "6G5" : "?",
  "6KM" : "?",
  "6KY" : "?",
  "6RK" : "?",
  "6V9" : "?",
  "6YJ" : "?",
  "6YK" : "?",
  "6ZS" : "?",
  "75A" : "?",
  "78E" : "?",
  "78R" : "?",
  "78X" : "?",
  "7BB" : "?",
  "7C7" : "?",
  "7C9" : "?",
  "7CC" : "?",
  "7DI" : "?",
  "7HA" : "?",
  "7HG" : "?",
  "7J3" : "?",
  "7J4" : "?",
  "7MN" : "?",
  "7RX" : "?",
  "7YF" : "?",
  "7YL" : "?",
  "7YR" : "?",
  "81R" : "?",
  "81S" : "?",
  "85G" : "?",
  "85J" : "?",
  "8B1" : "?",
  "8BB" : "?",
  "8QA" : "?",
  "8VG" : "?",
  "8YR" : "?",
  "8Z0" : "?",
  "91U" : "?",
  "95A" : "?",
  "98P" : "?",
  "99P" : "?",
  "99Y" : "?",
  "9AL" : "?",
  "9AT" : "?",
  "9BA" : "?",
  "9BB" : "?",
  "9BP" : "?",
  "9EV" : "?",
  "9JC" : "?",
  "9JV" : "?",
  "9KK" : "?",
  "9MN" : "?",
  "9OW" : "?",
  "9PR" : "?",
  "9R1" : "?",
  "9R4" : "?",
  "9R7" : "?",
  "9U6" : "?",
  "9U9" : "?",
  "9UC" : "?",
  "9UF" : "?",
  "9V0" : "?",
  "9V6" : "?",
  "9VC" : "?",
  "9VF" : "?",
  "9VL" : "?",
  "9VR" : "?",
  "A02" : "?",
  "A03" : "?",
  "A0G" : "?",
  "A1A2I" : "?",
  "A1A2J" : "?",
  "A1A2K" : "?",
  "A1ADW" : "?",
  "A1ADY" : "?",
  "A1ADZ" : "?",
  "A1AMM" : "?",
  "A1AP1" : "?",
  "A1AQ4" : "?",
  "A1D9H" : "?",
  "A1ECN" : "?",
  "A1H6Z" : "?",
  "A1I67" : "?",
  "A1IJ4" : "?",
  "A1IM3" : "?",
  "A1IM7" : "?",
  "A1IM8" : "?",
  "A1IMX" : "?",
  "A1IU4" : "?",
  "A1IU9" : "?",
  "A1L6N" : "?",
  "A5R" : "?",
  "A66" : "?",
  "A67" : "?",
  "A6I" : "?",
  "A89" : "?",
  "A9A" : "?",
  "AA6" : "?",
  "AB7" : "?",
  "AC5" : "?",
  "ACA" : "?",
  "ADD" : "?",
  "AE5" : "?",
  "AEB" : "?",
  "AFC" : "?",
  "AGD" : "?",
  "AHH" : "?",
  "AHS" : "?",
  "AHT" : "?",
  "AIP" : "?",
  "AJE" : "?",
  "ALG" : "?",
  "ALZ" : "?",
  "APE" : "?",
  "APH" : "?",
  "APM" : "?",
  "APN" : "?",
  "APO" : "?",
  "APP" : "?",
  "AR0" : "?",
  "ARV" : "?",
  "ARX" : "?",
  "AS2" : "?",
  "AS9" : "?",
  "ASJ" : "?",
  "ASM" : "?",
  "ASX" : "?",
  "ATN" : "?",
  "AVN" : "?",
  "AY0" : "?",
  "AYI" : "?",
  "AYQ" : "?",
  "AZ0" : "?",
  "AZL" : "?",
  "B01" : "?",
  "B03" : "?",
  "B04" : "?",
  "B0F" : "?",
  "B1S" : "?",
  "B26" : "?",
  "B2N" : "?",
  "B3L" : "?",
  "B3M" : "?",
  "B3Q" : "?",
  "B3S" : "?",
  "B3T" : "?",
  "B5I" : "?",
  "B8R" : "?",
  "BAL" : "?",
  "BCC" : "?",
  "BE2" : "?",
  "BF7" : "?",
  "BIL" : "?",
  "BJH" : "?",
  "BM2" : "?",
  "BM9" : "?",
  "BNO" : "?",
  "BW5" : "?",
  "BZK" : "?",
  "C0O" : "?",
  "C2N" : "?",
  "C66" : "?",
  "CAB" : "?",
  "CAL" : "?",
  "CAV" : "?",
  "CCJ" : "?",
  "CCL" : "?",
  "CCR" : "?",
  "CDE" : "?",
  "CDV" : "?",
  "CGH" : "?",
  "CHF" : "?",
  "CHG" : "?",
  "CHS" : "?",
  "CIX" : "?",
  "CLB" : "?",
  "CLD" : "?",
  "CNG" : "?",
  "CPC" : "?",
  "CPI" : "?",
  "CPN" : "?",
  "CSK" : "?",
  "CTW" : "?",
  "CUC" : "?",
  "CUD" : "?",
  "D0C" : "?",
  "D0Q" : "?",
  "D0R" : "?",
  "D11" : "?",
  "D3P" : "?",
  "D4P" : "?",
  "DAL" : "?",
  "DAM" : "?",
  "DAR" : "?",
  "DAS" : "?",
  "DBB" : "?",
  "DC0" : "?",
  "DCL" : "?",
  "DCY" : "?",
  "DFF" : "?",
  "DFI" : "?",
  "DFO" : "?",
  "DGH" : "?",
  "DGL" : "?",
  "DGN" : "?",
  "DHI" : "?",
  "DHL" : "?",
  "DHP" : "?",
  "DHV" : "?",
  "DI8" : "?",
  "DIL" : "?",
  "DIV" : "?",
  "DLE" : "?",
  "DLY" : "?",
  "DMT" : "?",
  "DNE" : "?",
  "DNG" : "?",
  "DNM" : "?",
  "DO2" : "?",
  "DOA" : "?",
  "DPN" : "?",
  "DPR" : "?",
  "DSE" : "?",
  "DSG" : "?",
  "DSN" : "?",
  "DSP" : "?",
  "DTH" : "?",
  "DTR" : "?",
  "DTY" : "?",
  "DV7" : "?",
  "DVA" : "?",
  "DYL" : "?",
  "E03" : "?",
  "E13" : "?",
  "E14" : "?",
  "E15" : "?",
  "E16" : "?",
  "E17" : "?",
  "E19" : "?",
  "E2G" : "?",
  "E69" : "?",
  "E95" : "?",
  "EC6" : "?",
  "EEF" : "?",
  "EEP" : "?",
  "EJM" : "?",
  "EO2" : "?",
  "EOE" : "?",
  "EOV" : "?",
  "EP2" : "?",
  "EP9" : "?",
  "ERL" : "?",
  "EYS" : "?",
  "F0G" : "?",
  "F3G" : "?",
  "F3M" : "?",
  "F3T" : "?",
  "F7P" : "?",
  "F7S" : "?",
  "F7V" : "?",
  "F93" : "?",
  "F9D" : "?",
  "FBE" : "?",
  "FGK" : "?",
  "FHR" : "?",
  "FJC" : "?",
  "FOD" : "?",
  "FOG" : "?",
  "FPR" : "?",
  "FRD" : "?",
  "FTO" : "?",
  "FXC" : "?",
  "FXF" : "?",
  "FXL" : "?",
  "FYI" : "?",
  "FYQ" : "?",
  "G85" : "?",
  "G89" : "?",
  "GAH" : "?",
  "GCM" : "?",
  "GGB" : "?",
  "GHP" : "?",
  "GIC" : "?",
  "GKL" : "?",
  "GLM" : "?",
  "GLX" : "?",
  "GPN" : "?",
  "GVE" : "?",
  "GVX" : "?",
  "GWO" : "?",
  "GZJ" : "?",
  "H37" : "?",
  "HAO" : "?",
  "HAQ" : "?",
  "HCL" : "?",
  "HCS" : "?",
  "HD0" : "?",
  "HF2" : "?",
  "HFA" : "?",
  "HG7" : "?",
  "HGL" : "?",
  "HGM" : "?",
  "HH0" : "?",
  "HJH" : "?",
  "HJV" : "?",
  "HJY" : "?",
  "HLX" : "?",
  "HM8" : "?",
  "HM9" : "?",
  "HMF" : "?",
  "HP0" : "?",
  "HTN" : "?",
  "HTY" : "?",
  "HUD" : "?",
  "HV8" : "?",
  "I0X" : "?",
  "I0Y" : "?",
  "IHN" : "?",
  "II7" : "?",
  "IL0" : "?",
  "INN" : "?",
  "IOY" : "?",
  "ITZ" : "?",
  "IVF" : "?",
  "IVV" : "?",
  "IWQ" : "?",
  "IYT" : "?",
  "J7H" : "?",
  "J9A" : "?",
  "JBY" : "?",
  "JE2" : "?",
  "JG3" : "?",
  "JKC" : "?",
  "JKU" : "?",
  "JMO" : "?",
  "JMX" : "?",
  "JZP" : "?",
  "JZQ" : "?",
  "K0L" : "?",
  "K36" : "?",
  "K57" : "?",
  "K95" : "?",
  "KAC" : "?",
  "KBS" : "?",
  "KBV" : "?",
  "KCJ" : "?",
  "KCY" : "?",
  "KJW" : "?",
  "KNI" : "?",
  "KNJ" : "?",
  "KPN" : "?",
  "KVI" : "?",
  "KXV" : "?",
  "KY4" : "?",
  "KY7" : "?",
  "L2A" : "?",
  "L2O" : "?",
  "L3A" : "?",
  "L4R" : "?",
  "LAV" : "?",
  "LCZ" : "?",
  "LDE" : "?",
  "LEE" : "?",
  "LEI" : "?",
  "LEP" : "?",
  "LHC" : "?",
  "LHE" : "?",
  "LHV" : "?",
  "LK0" : "?",
  "LKE" : "?",
  "LNT" : "?",
  "LOL" : "?",
  "LOU" : "?",
  "LOV" : "?",
  "LPH" : "?",
  "LPL" : "?",
  "LTA" : "?",
  "LV0" : "?",
  "LV8" : "?",
  "LVX" : "?",
  "LYH" : "?",
  "LYV" : "?",
  "M31" : "?",
  "M32" : "?",
  "M41" : "?",
  "M60" : "?",
  "M64" : "?",
  "M6J" : "?",
  "M6M" : "?",
  "M6Y" : "?",
  "M90" : "?",
  "M91" : "?",
  "M9G" : "?",
  "MCG" : "?",
  "MD0" : "?",
  "MDH" : "?",
  "MDL" : "?",
  "MED" : "?",
  "MF3" : "?",
  "MFH" : "?",
  "MH8" : "?",
  "MHV" : "?",
  "MHW" : "?",
  "MI0" : "?",
  "MID" : "?",
  "MIN" : "?",
  "MIT" : "?",
  "MIU" : "?",
  "MJS" : "?",
  "MKD" : "?",
  "MLU" : "?",
  "MND" : "?",
  "MOD" : "?",
  "MOQ" : "?",
  "MOZ" : "?",
  "MP0" : "?",
  "MP4" : "?",
  "MPH" : "?",
  "MPJ" : "?",
  "MRQ" : "?",
  "MSP" : "?",
  "MV9" : "?",
  "MY0" : "?",
  "N2C" : "?",
  "N31" : "?",
  "N8P" : "?",
  "NB8" : "?",
  "NCY" : "?",
  "NDF" : "?",
  "NHL" : "?",
  "NKS" : "?",
  "NOR" : "?",
  "NR0" : "?",
  "NRG" : "?",
  "NRL" : "?",
  "NSF" : "?",
  "NSK" : "?",
  "NSU" : "?",
  "O12" : "?",
  "O27" : "?",
  "O33" : "?",
  "O61" : "?",
  "OBF" : "?",
  "ODA" : "?",
  "ODB" : "?",
  "ODS" : "?",
  "OEM" : "?",
  "OEU" : "?",
  "OIC" : "?",
  "OJY" : "?",
  "OK4" : "?",
  "OLE" : "?",
  "OMZ" : "?",
  "ONK" : "?",
  "ONL" : "?",
  "OO1" : "?",
  "OPR" : "?",
  "ORD" : "?",
  "OTB" : "?",
  "OTY" : "?",
  "OV1" : "?",
  "OV2" : "?",
  "OV7" : "?",
  "OWF" : "?",
  "P05" : "?",
  "P0A" : "?",
  "P0H" : "?",
  "P0L" : "?",
  "P0Y" : "?",
  "P3N" : "?",
  "P4D" : "?",
  "P4E" : "?",
  "P4F" : "?",
  "P5D" : "?",
  "P97" : "?",
  "PAU" : "?",
  "PCE" : "?",
  "PDD" : "?",
  "PDL" : "?",
  "PDW" : "?",
  "PFX" : "?",
  "PG9" : "?",
  "PGL" : "?",
  "PH8" : "?",
  "PHV" : "?",
  "PHW" : "?",
  "PI6" : "?",
  "PIV" : "?",
  "PJE" : "?",
  "PJJ" : "?",
  "PKC" : "?",
  "PLE" : "?",
  "PLF" : "?",
  "PLW" : "?",
  "PMJ" : "?",
  "PN4" : "?",
  "PPH" : "?",
  "PPX" : "?",
  "PQ4" : "?",
  "PQG" : "?",
  "PR0" : "?",
  "PRQ" : "?",
  "PRR" : "?",
  "PS0" : "?",
  "PSI" : "?",
  "PSM" : "?",
  "PSS" : "?",
  "PT0" : "?",
  "PTA" : "?",
  "PUK" : "?",
  "PVL" : "?",
  "PVO" : "?",
  "Q00" : "?",
  "Q3S" : "?",
  "Q78" : "?",
  "QAC" : "?",
  "QDD" : "?",
  "QQB" : "?",
  "QRG" : "?",
  "QUJ" : "?",
  "QUK" : "?",
  "QVE" : "?",
  "QVS" : "?",
  "QWE" : "?",
  "QXV" : "?",
  "R00" : "?",
  "R0E" : "?",
  "R2P" : "?",
  "R2T" : "?",
  "R45" : "?",
  "R47" : "?",
  "R4B" : "?",
  "R4C" : "?",
  "R6E" : "?",
  "R99" : "?",
  "RA4" : "?",
  "RA8" : "?",
  "RDF" : "?",
  "RF9" : "?",
  "RGI" : "?",
  "RH0" : "?",
  "RIG" : "?",
  "RIT" : "?",
  "RL2" : "?",
  "ROC" : "?",
  "RON" : "?",
  "RS8" : "?",
  "RSY" : "?",
  "RSZ" : "?",
  "RT1" : "?",
  "RX4" : "?",
  "S00" : "?",
  "S04" : "?",
  "S28" : "?",
  "S29" : "?",
  "S2D" : "?",
  "S49" : "?",
  "S54" : "?",
  "S89" : "?",
  "SBD" : "?",
  "SC2" : "?",
  "SD0" : "?",
  "SD2" : "?",
  "SDR" : "?",
  "SFE" : "?",
  "SKG" : "?",
  "SKJ" : "?",
  "SLR" : "?",
  "SOW" : "?",
  "SQ6" : "?",
  "STA" : "?",
  "SUB" : "?",
  "SV6" : "?",
  "SZ0" : "?",
  "SZF" : "?",
  "T00" : "?",
  "T09" : "?",
  "T19" : "?",
  "T42" : "?",
  "T66" : "?",
  "T79" : "?",
  "T7Q" : "?",
  "TA4" : "?",
  "TCK" : "?",
  "TDD" : "?",
  "TDF" : "?",
  "TDV" : "?",
  "TFN" : "?",
  "TFR" : "?",
  "THO" : "?",
  "TIF" : "?",
  "TIG" : "?",
  "TJI" : "?",
  "TKL" : "?",
  "TPH" : "?",
  "TPN" : "?",
  "TST" : "?",
  "TVA" : "?",
  "TY0" : "?",
  "TYX" : "?",
  "TZB" : "?",
  "TZO" : "?",
  "U2M" : "?",
  "UAL" : "?",
  "UB2" : "?",
  "UB4" : "?",
  "UBS" : "?",
  "UBT" : "?",
  "UBU" : "?",
  "UBV" : "?",
  "UBW" : "?",
  "UBX" : "?",
  "UBY" : "?",
  "UBZ" : "?",
  "UIA" : "?",
  "UKD" : "?",
  "UKY" : "?",
  "UN1" : "?",
  "UN2" : "?",
  "URV" : "?",
  "UU4" : "?",
  "UU5" : "?",
  "UXY" : "?",
  "UY7" : "?",
  "UYA" : "?",
  "UZ4" : "?",
  "UZA" : "?",
  "UZN" : "?",
  "V1V" : "?",
  "V5F" : "?",
  "V8N" : "?",
  "VAS" : "?",
  "VDE" : "?",
  "VDK" : "?",
  "VDL" : "?",
  "VEF" : "?",
  "VLL" : "?",
  "VLM" : "?",
  "VLT" : "?",
  "VME" : "?",
  "VMS" : "?",
  "VNW" : "?",
  "VOL" : "?",
  "VPF" : "?",
  "VSC" : "?",
  "VWW" : "?",
  "WCM" : "?",
  "WRG" : "?",
  "WZJ" : "?",
  "X1V" : "?",
  "X32" : "?",
  "X5H" : "?",
  "X6E" : "?",
  "X9A" : "?",
  "XC0" : "?",
  "XCP" : "?",
  "XDD" : "?",
  "XDJ" : "?",
  "XDY" : "?",
  "XFW" : "?",
  "XLI" : "?",
  "XPC" : "?",
  "XPH" : "?",
  "XXA" : "?",
  "XZA" : "?",
  "Y28" : "?",
  "YBR" : "?",
  "YCP" : "?",
  "YD5" : "?",
  "YEN" : "?",
  "YFZ" : "?",
  "YNM" : "?",
  "YYA" : "?",
  "Z50" : "?",
  "Z9J" : "?",
  "ZAE" : "?",
  "ZAL" : "?",
  "ZFB" : "?",
  "ZGL" : "?",
  "ZNY" : "?",
  "ZPH" : "?",
  "ZPR" : "?",
  "ZRA" : "?",
  "ZRG" : "?",
  "ZRJ" : "?",
  "ZSN" : "?",
  "ZT9" : "?",
  "ZTC" : "?",
  "ZTG" : "?",
  "ZU3" : "?",
  "ZU5" : "?",
  "ZUK" : "?",
  "ZXX" : "?",
  "ZY9" : "?",
  "ZYA" : "?",
  "ZZU" : "?",
  "02K" : "A",
  "02O" : "A",
  "02Y" : "A",
  "0CS" : "A",
  "0FL" : "A",
  "0NC" : "A",
  "1AC" : "A",
  "1L1" : "A",
  "1PI" : "A",
  "23P" : "A",
  "2AG" : "A",
  "2RA" : "A",
  "33W" : "A",
  "3GA" : "A",
  "3WS" : "A",
  "4U7" : "A",
  "4WQ" : "A",
  "5AB" : "A",
  "5FQ" : "A",
  "5OH" : "A",
  "5XU" : "A",
  "6CV" : "A",
  "6GL" : "A",
  "7O5" : "A",
  "7OZ" : "A",
  "7VN" : "A",
  "9WV" : "A",
  "A1ALE" : "A",
  "A1AT6" : "A",
  "A1AT7" : "A",
  "A1H5W" : "A",
  "A1IID" : "A",
  "AA3" : "A",
  "AA4" : "A",
  "ABA" : "A",
  "AHO" : "A",
  "AHP" : "A",
  "AIB" : "A",
  "ALC" : "A",
  "ALM" : "A",
  "ALN" : "A",
  "ALS" : "A",
  "ALT" : "A",
  "ALV" : "A",
  "AN8" : "A",
  "AYA" : "A",
  "AZH" : "A",
  "B2A" : "A",
  "B3A" : "A",
  "BP5" : "A",
  "C22" : "A",
  "CWD" : "A",
  "CZS" : "A",
  "DAB" : "A",
  "DBZ" : "A",
  "DDZ" : "A",
  "DNP" : "A",
  "DNW" : "A",
  "DPP" : "A",
  "FB5" : "A",
  "FB6" : "A",
  "FLA" : "A",
  "H7V" : "A",
  "HAC" : "A",
  "HIX" : "A",
  "HQA" : "A",
  "HV5" : "A",
  "IAM" : "A",
  "KNB" : "A",
  "LAL" : "A",
  "MA" : "A",
  "MAA" : "A",
  "N9P" : "A",
  "NA8" : "A",
  "NAL" : "A",
  "NAM" : "A",
  "NCB" : "A",
  "NPI" : "A",
  "NWD" : "A",
  "ONH" : "A",
  "ORN" : "A",
  "PYA" : "A",
  "Q75" : "A",
  "QM8" : "A",
  "QMB" : "A",
  "QX7" : "A",
  "RVJ" : "A",
  "S2P" : "A",
  "SEG" : "A",
  "TIH" : "A",
  "UM1" : "A",
  "UM2" : "A",
  "UMA" : "A",
  "VVK" : "A",
  "X5P" : "A",
  "XW1" : "A",
  "XYC" : "A",
  "YFQ" : "A",
  "Z01" : "A",
  "ZTK" : "A",
  "ZZJ" : "A",
  "0AR" : "R",
  "0X9" : "R",
  "2MR" : "R",
  "2OR" : "R",
  "4AR" : "R",
  "4D4" : "R",
  "4J5" : "R",
  "73N" : "R",
  "9NR" : "R",
  "A1LTQ" : "R",
  "AAR" : "R",
  "ACL" : "R",
  "AGM" : "R",
  "AR2" : "R",
  "AR7" : "R",
  "ARM" : "R",
  "ARO" : "R",
  "BOR" : "R",
  "BWV" : "R",
  "CIR" : "R",
  "DA2" : "R",
  "DIR" : "R",
  "EI4" : "R",
  "FIO" : "R",
  "G3M" : "R",
  "HAR" : "R",
  "HMR" : "R",
  "HR7" : "R",
  "HRG" : "R",
  "IAR" : "R",
  "IOR" : "R",
  "J9Y" : "R",
  "MAI" : "R",
  "MGG" : "R",
  "MMO" : "R",
  "MYN" : "R",
  "NMM" : "R",
  "NNH" : "R",
  "OAR" : "R",
  "ORQ" : "R",
  "POK" : "R",
  "RGL" : "R",
  "RPI" : "R",
  "THZ" : "R",
  "VR0" : "R",
  "WYK" : "R",
  "02L" : "N",
  "41Q" : "N",
  "5VV" : "N",
  "823" : "N",
  "9DN" : "N",
  "A5N" : "N",
  "AFA" : "N",
  "AHB" : "N",
  "AS7" : "N",
  "B3X" : "N",
  "CE7" : "N",
  "DMH" : "N",
  "MEN" : "N",
  "SD4" : "N",
  "SNN" : "N",
  "XSN" : "N",
  "0A0" : "D",
  "0AK" : "D",
  "0TD" : "D",
  "3MD" : "D",
  "7ID" : "D",
  "7TK" : "D",
  "999" : "D",
  "AKL" : "D",
  "AKZ" : "D",
  "ASA" : "D",
  "ASB" : "D",
  "ASI" : "D",
  "ASK" : "D",
  "ASL" : "D",
  "ASQ" : "D",
  "B3D" : "D",
  "BFD" : "D",
  "BH2" : "D",
  "BHD" : "D",
  "D2T" : "D",
  "DMK" : "D",
  "DOH" : "D",
  "DYA" : "D",
  "FL6" : "D",
  "KKD" : "D",
  "LAA" : "D",
  "OHS" : "D",
  "OXX" : "D",
  "PAS" : "D",
  "PHD" : "D",
  "TAV" : "D",
  "ZJU" : "D",
  "03Y" : "C",
  "07O" : "C",
  "08P" : "C",
  "0A8" : "C",
  "0QL" : "C",
  "143" : "C",
  "2CO" : "C",
  "2XA" : "C",
  "30V" : "C",
  "31Q" : "C",
  "3X9" : "C",
  "4GJ" : "C",
  "4J4" : "C",
  "5CS" : "C",
  "60F" : "C",
  "6M6" : "C",
  "6V1" : "C",
  "6WK" : "C",
  "85F" : "C",
  "85L" : "C",
  "8JB" : "C",
  "A1A7V" : "C",
  "A1BC0" : "C",
  "A1D64" : "C",
  "AEA" : "C",
  "AGT" : "C",
  "BB6" : "C",
  "BB7" : "C",
  "BB9" : "C",
  "BBC" : "C",
  "BCS" : "C",
  "BCX" : "C",
  "BPE" : "C",
  "BTC" : "C",
  "BUC" : "C",
  "C1T" : "C",
  "C3Y" : "C",
  "C4R" : "C",
  "C5C" : "C",
  "C6C" : "C",
  "CAF" : "C",
  "CAS" : "C",
  "CAY" : "C",
  "CCS" : "C",
  "CEA" : "C",
  "CG6" : "C",
  "CGL" : "C",
  "CGV" : "C",
  "CME" : "C",
  "CMH" : "C",
  "CML" : "C",
  "CMT" : "C",
  "CS0" : "C",
  "CS1" : "C",
  "CS3" : "C",
  "CS4" : "C",
  "CSA" : "C",
  "CSB" : "C",
  "CSD" : "C",
  "CSE" : "C",
  "CSJ" : "C",
  "CSO" : "C",
  "CSP" : "C",
  "CSR" : "C",
  "CSS" : "C",
  "CSU" : "C",
  "CSW" : "C",
  "CSX" : "C",
  "CSZ" : "C",
  "CY0" : "C",
  "CY1" : "C",
  "CY3" : "C",
  "CY4" : "C",
  "CYA" : "C",
  "CYD" : "C",
  "CYF" : "C",
  "CYG" : "C",
  "CYM" : "C",
  "CYQ" : "C",
  "CYR" : "C",
  "CYW" : "C",
  "CZ2" : "C",
  "CZZ" : "C",
  "DC2" : "C",
  "DYS" : "C",
  "ECX" : "C",
  "EFC" : "C",
  "EJA" : "C",
  "FFM" : "C",
  "FOE" : "C",
  "GQI" : "C",
  "GT9" : "C",
  "HCM" : "C",
  "HNC" : "C",
  "HTI" : "C",
  "I3L" : "C",
  "ICY" : "C",
  "J3D" : "C",
  "JJJ" : "C",
  "JJK" : "C",
  "JJL" : "C",
  "K1R" : "C",
  "K5H" : "C",
  "M0H" : "C",
  "MCS" : "C",
  "MD3" : "C",
  "MD5" : "C",
  "NPH" : "C",
  "NYB" : "C",
  "NYS" : "C",
  "OCS" : "C",
  "OCY" : "C",
  "P1L" : "C",
  "P9S" : "C",
  "PBB" : "C",
  "PEC" : "C",
  "PR3" : "C",
  "PYX" : "C",
  "Q8X" : "C",
  "QCS" : "C",
  "QNQ" : "C",
  "QNT" : "C",
  "QNW" : "C",
  "QO2" : "C",
  "QO5" : "C",
  "QO8" : "C",
  "QPA" : "C",
  "QVA" : "C",
  "R1A" : "C",
  "S2C" : "C",
  "SCH" : "C",
  "SCS" : "C",
  "SCY" : "C",
  "SHC" : "C",
  "SIB" : "C",
  "SMC" : "C",
  "SNC" : "C",
  "TNB" : "C",
  "TQZ" : "C",
  "TSY" : "C",
  "UJR" : "C",
  "V44" : "C",
  "VI3" : "C",
  "XCN" : "C",
  "YCM" : "C",
  "YRV" : "C",
  "ZBZ" : "C",
  "ZLF" : "C",
  "ZZD" : "C",
  "A1L4B" : "Q",
  "ECC" : "Q",
  "GHG" : "Q",
  "GLH" : "Q",
  "GNC" : "Q",
  "LMQ" : "Q",
  "MEQ" : "Q",
  "MGN" : "Q",
  "NLQ" : "Q",
  "PCA" : "Q",
  "QCI" : "Q",
  "QMM" : "Q",
  "QQ8" : "Q",
  "YTF" : "Q",
  "ZKO" : "Q",
  "11W" : "E",
  "3GL" : "E",
  "3O3" : "E",
  "5HP" : "E",
  "86N" : "E",
  "9NE" : "E",
  "AR4" : "E",
  "B3E" : "E",
  "CGA" : "E",
  "CGU" : "E",
  "DV9" : "E",
  "EME" : "E",
  "G01" : "E",
  "G8M" : "E",
  "GAU" : "E",
  "GHC" : "E",
  "GHW" : "E",
  "GLJ" : "E",
  "GLK" : "E",
  "GLQ" : "E",
  "GMA" : "E",
  "GME" : "E",
  "GSU" : "E",
  "ILG" : "E",
  "LME" : "E",
  "MEG" : "E",
  "R0K" : "E",
  "RGP" : "E",
  "VHF" : "E",
  "X2W" : "E",
  "0AC" : "G",
  "3XH" : "G",
  "5PG" : "G",
  "9DS" : "G",
  "A1AZQ" : "G",
  "CHP" : "G",
  "CR5" : "G",
  "EJJ" : "G",
  "FGL" : "G",
  "GEE" : "G",
  "GL3" : "G",
  "GLZ" : "G",
  "GSC" : "G",
  "HGY" : "G",
  "I4G" : "G",
  "IC0" : "G",
  "IGL" : "G",
  "IPG" : "G",
  "LPG" : "G",
  "LVG" : "G",
  "M30" : "G",
  "MD6" : "G",
  "MEU" : "G",
  "MGY" : "G",
  "MPQ" : "G",
  "MSA" : "G",
  "NLY" : "G",
  "NMC" : "G",
  "OZ3" : "G",
  "PGY" : "G",
  "PRV" : "G",
  "SAR" : "G",
  "SHP" : "G",
  "UGY" : "G",
  "ZSX" : "G",
  "2HF" : "H",
  "2SO" : "H",
  "3AH" : "H",
  "3ZH" : "H",
  "56A" : "H",
  "A1A7R" : "H",
  "A1AS0" : "H",
  "A1ASZ" : "H",
  "A1AZ2" : "H",
  "A1IQX" : "H",
  "B3U" : "H",
  "DDE" : "H",
  "E9V" : "H",
  "HBN" : "H",
  "HHI" : "H",
  "HIA" : "H",
  "HIC" : "H",
  "HIP" : "H",
  "HIQ" : "H",
  "HOO" : "H",
  "HS8" : "H",
  "HS9" : "H",
  "HSK" : "H",
  "HSO" : "H",
  "HSV" : "H",
  "I4O" : "H",
  "MH1" : "H",
  "MHS" : "H",
  "NEM" : "H",
  "NEP" : "H",
  "NZH" : "H",
  "OGC" : "H",
  "OHI" : "H",
  "OLD" : "H",
  "OYL" : "H",
  "PJ3" : "H",
  "PSH" : "H",
  "PVH" : "H",
  "SNK" : "H",
  "V5N" : "H",
  "Z70" : "H",
  "5GM" : "I",
  "66D" : "I",
  "7JA" : "I",
  "B2I" : "I",
  "BIU" : "I",
  "I2M" : "I",
  "IIL" : "I",
  "ILM" : "I",
  "ILX" : "I",
  "IML" : "I",
  "QIL" : "I",
  "RX9" : "I",
  "TS9" : "I",
  "0AG" : "L",
  "2LU" : "L",
  "2ML" : "L",
  "8WY" : "L",
  "A1A5T" : "L",
  "A1ADO" : "L",
  "AN6" : "L",
  "BL2" : "L",
  "BLE" : "L",
  "BTA" : "L",
  "CLE" : "L",
  "DON" : "L",
  "EXY" : "L",
  "FLE" : "L",
  "G5G" : "L",
  "HL2" : "L",
  "HL5" : "L",
  "HLU" : "L",
  "L3O" : "L",
  "LAY" : "L",
  "LED" : "L",
  "LEF" : "L",
  "LEH" : "L",
  "LEM" : "L",
  "LEN" : "L",
  "LEX" : "L",
  "LNE" : "L",
  "LNM" : "L",
  "MHL" : "L",
  "MK8" : "L",
  "MLE" : "L",
  "MLL" : "L",
  "MNL" : "L",
  "NLB" : "L",
  "NLE" : "L",
  "NLN" : "L",
  "NLO" : "L",
  "NLP" : "L",
  "NLW" : "L",
  "NOT" : "L",
  "S0R" : "L",
  "T3R" : "L",
  "WLU" : "L",
  "Y1V" : "L",
  "0A2" : "K",
  "2KK" : "K",
  "2KP" : "K",
  "3QN" : "K",
  "4AK" : "K",
  "5CT" : "K",
  "5GG" : "K",
  "5MW" : "K",
  "5OW" : "K",
  "5T3" : "K",
  "6CL" : "K",
  "6G4" : "K",
  "6HN" : "K",
  "73P" : "K",
  "74P" : "K",
  "7QK" : "K",
  "8RE" : "K",
  "9E7" : "K",
  "9KP" : "K",
  "9TR" : "K",
  "9TU" : "K",
  "9TX" : "K",
  "9U0" : "K",
  "A1BEB" : "K",
  "A1D5B" : "K",
  "A1H5N" : "K",
  "A1LUV" : "K",
  "A1LWV" : "K",
  "ALY" : "K",
  "API" : "K",
  "APK" : "K",
  "AZK" : "K",
  "B3K" : "K",
  "BLY" : "K",
  "BTK" : "K",
  "C1X" : "K",
  "CLG" : "K",
  "CLH" : "K",
  "CYJ" : "K",
  "DLS" : "K",
  "DM0" : "K",
  "DNL" : "K",
  "DNS" : "K",
  "ELY" : "K",
  "EXA" : "K",
  "FAK" : "K",
  "FDL" : "K",
  "FF9" : "K",
  "FH7" : "K",
  "FHL" : "K",
  "FHO" : "K",
  "FQA" : "K",
  "FZN" : "K",
  "GND" : "K",
  "GPL" : "K",
  "HLY" : "K",
  "I2F" : "K",
  "I58" : "K",
  "IEL" : "K",
  "IT1" : "K",
  "JGO" : "K",
  "JLP" : "K",
  "KBE" : "K",
  "KCR" : "K",
  "KCX" : "K",
  "KEO" : "K",
  "KFP" : "K",
  "KGC" : "K",
  "KHB" : "K",
  "KPF" : "K",
  "KPI" : "K",
  "KPY" : "K",
  "KR3" : "K",
  "KST" : "K",
  "KYQ" : "K",
  "L5P" : "K",
  "LA2" : "K",
  "LBY" : "K",
  "LBZ" : "K",
  "LCK" : "K",
  "LCX" : "K",
  "LDH" : "K",
  "LET" : "K",
  "LGY" : "K",
  "LLO" : "K",
  "LLP" : "K",
  "LLY" : "K",
  "LLZ" : "K",
  "LMF" : "K",
  "LP6" : "K",
  "LRK" : "K",
  "LSO" : "K",
  "LYF" : "K",
  "LYK" : "K",
  "LYM" : "K",
  "LYN" : "K",
  "LYO" : "K",
  "LYR" : "K",
  "LYU" : "K",
  "LYX" : "K",
  "LYZ" : "K",
  "M2L" : "K",
  "M3L" : "K",
  "M3R" : "K",
  "MCL" : "K",
  "ML3" : "K",
  "MLY" : "K",
  "MLZ" : "K",
  "MYK" : "K",
  "N65" : "K",
  "OBS" : "K",
  "PE1" : "K",
  "PRK" : "K",
  "PYH" : "K",
  "Q3P" : "K",
  "SHR" : "K",
  "SKH" : "K",
  "SLL" : "K",
  "SLZ" : "K",
  "TLY" : "K",
  "TRG" : "K",
  "V7T" : "K",
  "VB1" : "K",
  "VPV" : "K",
  "XOK" : "K",
  "XRW" : "K",
  "XX1" : "K",
  "Y57" : "K",
  "YHA" : "K",
  "ZAI" : "K",
  "2FM" : "M",
  "4CY" : "M",
  "4MM" : "M",
  "AME" : "M",
  "CXM" : "M",
  "ESC" : "M",
  "FME" : "M",
  "IZO" : "M",
  "KOR" : "M",
  "M2S" : "M",
  "ME0" : "M",
  "MHO" : "M",
  "MME" : "M",
  "MSE" : "M",
  "MSL" : "M",
  "MSO" : "M",
  "MT2" : "M",
  "OMT" : "M",
  "SME" : "M",
  "0A9" : "F",
  "0BN" : "F",
  "1PA" : "F",
  "200" : "F",
  "23F" : "F",
  "2GX" : "F",
  "33S" : "F",
  "3CF" : "F",
  "41H" : "F",
  "4AF" : "F",
  "4CF" : "F",
  "4II" : "F",
  "4OU" : "F",
  "4PH" : "F",
  "4SJ" : "F",
  "55I" : "F",
  "5CR" : "F",
  "7N8" : "F",
  "7T2" : "F",
  "7VU" : "F",
  "7W2" : "F",
  "7XC" : "F",
  "9NF" : "F",
  "A1ECK" : "F",
  "A1ECL" : "F",
  "A1H2H" : "F",
  "A1IPL" : "F",
  "A1L4D" : "F",
  "A3U" : "F",
  "B1F" : "F",
  "B2F" : "F",
  "BB8" : "F",
  "BIF" : "F",
  "BNN" : "F",
  "DAH" : "F",
  "DJD" : "F",
  "EHP" : "F",
  "F2F" : "F",
  "FC0" : "F",
  "FCL" : "F",
  "FX9" : "F",
  "H14" : "F",
  "HOX" : "F",
  "HP9" : "F",
  "HPC" : "F",
  "HPE" : "F",
  "HPH" : "F",
  "HPQ" : "F",
  "IAE" : "F",
  "LWI" : "F",
  "MEA" : "F",
  "MHU" : "F",
  "N0A" : "F",
  "NFA" : "F",
  "OZW" : "F",
  "PBF" : "F",
  "PCS" : "F",
  "PF5" : "F",
  "PFF" : "F",
  "PHA" : "F",
  "PHI" : "F",
  "PHL" : "F",
  "PHM" : "F",
  "PM3" : "F",
  "PPN" : "F",
  "PSA" : "F",
  "QPH" : "F",
  "SMF" : "F",
  "T11" : "F",
  "TEF" : "F",
  "TFQ" : "F",
  "TSQ" : "F",
  "U3X" : "F",
  "UXQ" : "F",
  "V3E" : "F",
  "V61" : "F",
  "WFP" : "F",
  "WPA" : "F",
  "X5U" : "F",
  "XA6" : "F",
  "YC3" : "F",
  "ZCL" : "F",
  "ZV4" : "F",
  "037" : "P",
  "04U" : "P",
  "04V" : "P",
  "05N" : "P",
  "0LF" : "P",
  "0Y8" : "P",
  "11Q" : "P",
  "12L" : "P",
  "12X" : "P",
  "12Y" : "P",
  "2MT" : "P",
  "2P0" : "P",
  "3BY" : "P",
  "3PX" : "P",
  "3WX" : "P",
  "4FB" : "P",
  "4KY" : "P",
  "4L0" : "P",
  "4N7" : "P",
  "4N8" : "P",
  "4N9" : "P",
  "6Y9" : "P",
  "7YO" : "P",
  "8LJ" : "P",
  "A1A3M" : "P",
  "A1D5E" : "P",
  "A1D9O" : "P",
  "DPL" : "P",
  "DYJ" : "P",
  "E0Y" : "P",
  "FP9" : "P",
  "FPK" : "P",
  "G8X" : "P",
  "H5M" : "P",
  "HY3" : "P",
  "HYP" : "P",
  "HZP" : "P",
  "JKH" : "P",
  "LPD" : "P",
  "LWY" : "P",
  "MP8" : "P",
  "N7P" : "P",
  "N80" : "P",
  "P2Y" : "P",
  "PCC" : "P",
  "PH6" : "P",
  "PKR" : "P",
  "PLJ" : "P",
  "POM" : "P",
  "PR4" : "P",
  "PR7" : "P",
  "PR9" : "P",
  "PRJ" : "P",
  "PRS" : "P",
  "PXU" : "P",
  "RT0" : "P",
  "TPJ" : "P",
  "TPK" : "P",
  "V3C" : "P",
  "VH0" : "P",
  "XPR" : "P",
  "XRE" : "P",
  "YPR" : "P",
  "ZI0" : "P",
  "ZPO" : "P",
  "ZYJ" : "P",
  "ZYK" : "P",
  "XPL" : "O",
  "30F" : "U",
  "PSW" : "U",
  "SE7" : "U",
  "SOC" : "U",
  "SYS" : "U",
  "UOX" : "U",
  "0AH" : "S",
  "175" : "S",
  "1X6" : "S",
  "2JG" : "S",
  "2RX" : "S",
  "2ZC" : "S",
  "432" : "S",
  "4HH" : "S",
  "4HJ" : "S",
  "4OV" : "S",
  "4OZ" : "S",
  "5R5" : "S",
  "73C" : "S",
  "8SP" : "S",
  "A1BZO" : "S",
  "A9D" : "S",
  "AZS" : "S",
  "BG1" : "S",
  "BSE" : "S",
  "BWB" : "S",
  "BXT" : "S",
  "CRW" : "S",
  "CRX" : "S",
  "CWR" : "S",
  "DBS" : "S",
  "DHA" : "S",
  "EW6" : "S",
  "FGP" : "S",
  "GFT" : "S",
  "GVL" : "S",
  "HSE" : "S",
  "HSL" : "S",
  "I7F" : "S",
  "J8W" : "S",
  "K5L" : "S",
  "K7K" : "S",
  "LPS" : "S",
  "MC1" : "S",
  "MDO" : "S",
  "MH6" : "S",
  "MIR" : "S",
  "MIS" : "S",
  "MKF" : "S",
  "N10" : "S",
  "NC1" : "S",
  "O2E" : "S",
  "OAS" : "S",
  "OLZ" : "S",
  "OMH" : "S",
  "OSE" : "S",
  "PG1" : "S",
  "QDS" : "S",
  "RVX" : "S",
  "RZ4" : "S",
  "S12" : "S",
  "S1H" : "S",
  "SAC" : "S",
  "SBL" : "S",
  "SDP" : "S",
  "SEB" : "S",
  "SEE" : "S",
  "SEL" : "S",
  "SEM" : "S",
  "SEN" : "S",
  "SEP" : "S",
  "SET" : "S",
  "SGB" : "S",
  "SNM" : "S",
  "SOY" : "S",
  "SRZ" : "S",
  "SUN" : "S",
  "SVA" : "S",
  "SVV" : "S",
  "SVW" : "S",
  "SVX" : "S",
  "SVY" : "S",
  "SVZ" : "S",
  "SWW" : "S",
  "SXE" : "S",
  "TIS" : "S",
  "TNR" : "S",
  "UDS" : "S",
  "UF0" : "S",
  "ZRF" : "S",
  "0E5" : "T",
  "26B" : "T",
  "28X" : "T",
  "2QZ" : "T",
  "6BR" : "T",
  "AEI" : "T",
  "ALO" : "T",
  "B27" : "T",
  "BMT" : "T",
  "CTH" : "T",
  "DBU" : "T",
  "EUP" : "T",
  "NZC" : "T",
  "O7A" : "T",
  "OLT" : "T",
  "OTH" : "T",
  "QNY" : "T",
  "T8L" : "T",
  "T9E" : "T",
  "TBM" : "T",
  "TH5" : "T",
  "TH6" : "T",
  "THC" : "T",
  "TMB" : "T",
  "TMD" : "T",
  "TNY" : "T",
  "TPO" : "T",
  "X5V" : "T",
  "XDT" : "T",
  "YTH" : "T",
  "Z3E" : "T",
  "ZU0" : "T",
  "0AF" : "W",
  "0UO" : "W",
  "1TQ" : "W",
  "4AW" : "W",
  "4DP" : "W",
  "4FW" : "W",
  "4HT" : "W",
  "4IN" : "W",
  "4OG" : "W",
  "4PQ" : "W",
  "54C" : "W",
  "5CW" : "W",
  "6CW" : "W",
  "A1A5S" : "W",
  "A1AWS" : "W",
  "A1D7R" : "W",
  "A1H2I" : "W",
  "A1H45" : "W",
  "BTR" : "W",
  "CTE" : "W",
  "E9M" : "W",
  "EXL" : "W",
  "F7W" : "W",
  "FT6" : "W",
  "FTR" : "W",
  "HRP" : "W",
  "HT7" : "W",
  "HTR" : "W",
  "KYN" : "W",
  "LTR" : "W",
  "LTU" : "W",
  "O6H" : "W",
  "O7D" : "W",
  "PAT" : "W",
  "Q2E" : "W",
  "R4K" : "W",
  "RE0" : "W",
  "RE3" : "W",
  "TCR" : "W",
  "TGH" : "W",
  "TNQ" : "W",
  "TOQ" : "W",
  "TOX" : "W",
  "TPL" : "W",
  "TQI" : "W",
  "TQQ" : "W",
  "TRF" : "W",
  "TRN" : "W",
  "TRO" : "W",
  "TRQ" : "W",
  "TRW" : "W",
  "TRX" : "W",
  "TRY" : "W",
  "TTQ" : "W",
  "UX8" : "W",
  "V6W" : "W",
  "WRP" : "W",
  "WWB" : "W",
  "ZIQ" : "W",
  "0A1" : "Y",
  "0EA" : "Y",
  "0PR" : "Y",
  "0WZ" : "Y",
  "1OP" : "Y",
  "1TY" : "Y",
  "2LT" : "Y",
  "2R3" : "Y",
  "2TY" : "Y",
  "3CT" : "Y",
  "3MY" : "Y",
  "3NF" : "Y",
  "3YM" : "Y",
  "4BF" : "Y",
  "4HL" : "Y",
  "4LZ" : "Y",
  "51T" : "Y",
  "73O" : "Y",
  "A1D5P" : "Y",
  "A30" : "Y",
  "AGQ" : "Y",
  "AZY" : "Y",
  "B3Y" : "Y",
  "BYR" : "Y",
  "DBY" : "Y",
  "DI7" : "Y",
  "DPQ" : "Y",
  "E9C" : "Y",
  "ESB" : "Y",
  "F2Y" : "Y",
  "F7Q" : "Y",
  "FLT" : "Y",
  "FTY" : "Y",
  "FY2" : "Y",
  "FY3" : "Y",
  "G1X" : "Y",
  "IB9" : "Y",
  "IYR" : "Y",
  "J2F" : "Y",
  "MBQ" : "Y",
  "MDF" : "Y",
  "MTY" : "Y",
  "NBQ" : "Y",
  "NIY" : "Y",
  "NTR" : "Y",
  "NTY" : "Y",
  "OMX" : "Y",
  "OMY" : "Y",
  "P2Q" : "Y",
  "P3Q" : "Y",
  "PAQ" : "Y",
  "PTH" : "Y",
  "PTM" : "Y",
  "PTR" : "Y",
  "STY" : "Y",
  "T0I" : "Y",
  "TCQ" : "Y",
  "TPQ" : "Y",
  "TTS" : "Y",
  "TXY" : "Y",
  "TY1" : "Y",
  "TY2" : "Y",
  "TY3" : "Y",
  "TY5" : "Y",
  "TY8" : "Y",
  "TY9" : "Y",
  "TYB" : "Y",
  "TYC" : "Y",
  "TYE" : "Y",
  "TYI" : "Y",
  "TYJ" : "Y",
  "TYN" : "Y",
  "TYO" : "Y",
  "TYQ" : "Y",
  "TYS" : "Y",
  "TYT" : "Y",
  "TYW" : "Y",
  "TYY" : "Y",
  "U2X" : "Y",
  "YOF" : "Y",
  "YPZ" : "Y",
  "YTL" : "Y",
  "ZDJ" : "Y",
  "ZT6" : "Y",
  "0AA" : "V",
  "0AB" : "V",
  "2VA" : "V",
  "34E" : "V",
  "9NV" : "V",
  "A1AVU" : "V",
  "A1ECM" : "V",
  "A8E" : "V",
  "B2V" : "V",
  "BUG" : "V",
  "DHN" : "V",
  "EU0" : "V",
  "FVA" : "V",
  "HVA" : "V",
  "LE1" : "V",
  "LVN" : "V",
  "MNV" : "V",
  "MVA" : "V",
  "NVA" : "V",
  "O7G" : "V",
  "RXL" : "V",
  "TBG" : "V",
  "VAD" : "V",
  "VAF" : "V",
  "VAH" : "V",
  "VAI" : "V",
  "WVL" : "V",
  "X60" : "V",
  "ZQN" : "V",
}
if __name__ == '__main__':
  print(len(lookup.keys()))


 *******************************************************************************


 *******************************************************************************
iotbx/pdb/modified_rna_dna_names.py
"""
This file contains all residue names for modified RNA/DNAs linked to their
one letter standard parent RNA/DNA.  This file is generated by the following
procedure:

  phenix.python elbow/elbow/scripts/process_amino_acid_parentage_from_chemical_componts.py

This file is intended to be generated monthly.
The date of file generation: Mon Apr 28 12:18:50 2025
"""
from __future__ import absolute_import, division, print_function

lookup = {
  "0A" : "?",
  "0C" : "?",
  "0G" : "?",
  "0U" : "?",
  "0U1" : "?",
  "128" : "?",
  "1DP" : "?",
  "1PR" : "?",
  "2SA" : "?",
  "4AC" : "?",
  "4TA" : "?",
  "50L" : "?",
  "50N" : "?",
  "56B" : "?",
  "574" : "?",
  "5CF" : "?",
  "5JO" : "?",
  "6F7" : "?",
  "6FC" : "?",
  "6FU" : "?",
  "6OP" : "?",
  "8AZ" : "?",
  "8RJ" : "?",
  "9V9" : "?",
  "A1ELZ" : "?",
  "A1IY6" : "?",
  "A7C" : "?",
  "ADX" : "?",
  "B8N" : "?",
  "BMP" : "?",
  "BMQ" : "?",
  "BT5" : "?",
  "C4J" : "?",
  "CM0" : "?",
  "CS8" : "?",
  "CVC" : "?",
  "DBM" : "?",
  "DJF" : "?",
  "ENA" : "?",
  "ENP" : "?",
  "ENQ" : "?",
  "F3O" : "?",
  "FA5" : "?",
  "FAI" : "?",
  "FMU" : "?",
  "G4P" : "?",
  "GMX" : "?",
  "HHU" : "?",
  "HHX" : "?",
  "I" : "?",
  "I2T" : "?",
  "IKS" : "?",
  "ILK" : "?",
  "IPN" : "?",
  "IRN" : "?",
  "JLN" : "?",
  "JW5" : "?",
  "K1F" : "?",
  "KGV" : "?",
  "LCC" : "?",
  "LKC" : "?",
  "LMS" : "?",
  "M1Y" : "?",
  "M3O" : "?",
  "MUM" : "?",
  "N" : "?",
  "NF2" : "?",
  "O2C" : "?",
  "OAD" : "?",
  "ODP" : "?",
  "OOB" : "?",
  "OYW" : "?",
  "P1P" : "?",
  "PQ1" : "?",
  "PUY" : "?",
  "PYY" : "?",
  "QSK" : "?",
  "RTP" : "?",
  "RY" : "?",
  "S8M" : "?",
  "T2T" : "?",
  "TAL" : "?",
  "TLB" : "?",
  "TYU" : "?",
  "U4M" : "?",
  "U5M" : "?",
  "U6F" : "?",
  "UDP" : "?",
  "UFB" : "?",
  "UOA" : "?",
  "UOB" : "?",
  "URU" : "?",
  "UY1" : "?",
  "UY4" : "?",
  "UZL" : "?",
  "X0F" : "?",
  "X0O" : "?",
  "XE6" : "?",
  "XEC" : "?",
  "XNY" : "?",
  "Y5P" : "?",
  "YA4" : "?",
  "YWQ" : "?",
  "ZF9" : "?",
  "ZHP" : "?",
  "02I" : "?",
  "0DA" : "?",
  "0DC" : "?",
  "0DG" : "?",
  "0DT" : "?",
  "0KZ" : "?",
  "1TL" : "?",
  "1TW" : "?",
  "1W5" : "?",
  "1WA" : "?",
  "2DF" : "?",
  "2DM" : "?",
  "2FE" : "?",
  "2FI" : "?",
  "2GF" : "?",
  "2IA" : "?",
  "2JU" : "?",
  "2L8" : "?",
  "2LA" : "?",
  "2LF" : "?",
  "3DR" : "?",
  "3ZO" : "?",
  "4DG" : "?",
  "4DU" : "?",
  "4E9" : "?",
  "4EN" : "?",
  "4MF" : "?",
  "5DB" : "?",
  "5EJ" : "?",
  "5MD" : "?",
  "5OC" : "?",
  "5UA" : "?",
  "64P" : "?",
  "6FM" : "?",
  "6MI" : "?",
  "8NI" : "?",
  "8PI" : "?",
  "8Y9" : "?",
  "92F" : "?",
  "93D" : "?",
  "9O4" : "?",
  "A1A0L" : "?",
  "A1BBA" : "?",
  "A1P" : "?",
  "AAB" : "?",
  "ABT" : "?",
  "AFF" : "?",
  "ASU" : "?",
  "AWC" : "?",
  "B1P" : "?",
  "BMN" : "?",
  "BZG" : "?",
  "CGY" : "?",
  "CJ1" : "?",
  "CSM" : "?",
  "D1P" : "?",
  "D3" : "?",
  "D33" : "?",
  "D3N" : "?",
  "DA" : "?",
  "DC" : "?",
  "DCZ" : "?",
  "DDX" : "?",
  "DFT" : "?",
  "DG" : "?",
  "DI" : "?",
  "DN" : "?",
  "DP" : "?",
  "DPY" : "?",
  "DRP" : "?",
  "DRZ" : "?",
  "DT" : "?",
  "DU" : "?",
  "DX" : "?",
  "DXD" : "?",
  "DXN" : "?",
  "DZ" : "?",
  "EDC" : "?",
  "EDI" : "?",
  "EW3" : "?",
  "EWC" : "?",
  "F5H" : "?",
  "F6H" : "?",
  "F6U" : "?",
  "F6X" : "?",
  "F73" : "?",
  "F7O" : "?",
  "F7R" : "?",
  "F7X" : "?",
  "FAG" : "?",
  "FAX" : "?",
  "FFD" : "?",
  "G35" : "?",
  "GNE" : "?",
  "HOL" : "?",
  "J4T" : "?",
  "JSP" : "?",
  "KBC" : "?",
  "LCH" : "?",
  "LHO" : "?",
  "LR6" : "?",
  "LTP" : "?",
  "LWM" : "?",
  "MBZ" : "?",
  "MDJ" : "?",
  "MDK" : "?",
  "MDQ" : "?",
  "MDR" : "?",
  "MDU" : "?",
  "MDV" : "?",
  "MF7" : "?",
  "MFT" : "?",
  "MM7" : "?",
  "N4S" : "?",
  "N5I" : "?",
  "NCX" : "?",
  "NP3" : "?",
  "NR1" : "?",
  "NRI" : "?",
  "NYM" : "?",
  "OFC" : "?",
  "OIQ" : "?",
  "OWR" : "?",
  "P9G" : "?",
  "PBT" : "?",
  "PDU" : "?",
  "QBT" : "?",
  "RCE" : "?",
  "S6M" : "?",
  "S8U" : "?",
  "SAY" : "?",
  "SOS" : "?",
  "T0P" : "?",
  "T0T" : "?",
  "THM" : "?",
  "THP" : "?",
  "THX" : "?",
  "TS" : "?",
  "TT" : "?",
  "TX2" : "?",
  "UEL" : "?",
  "UFP" : "?",
  "URT" : "?",
  "US4" : "?",
  "VET" : "?",
  "WC7" : "?",
  "WVQ" : "?",
  "X4A" : "?",
  "XAE" : "?",
  "XAR" : "?",
  "XCS" : "?",
  "XFC" : "?",
  "XGA" : "?",
  "XTY" : "?",
  "YRR" : "?",
  "12A" : "A",
  "1MA" : "A",
  "2MA" : "A",
  "31H" : "A",
  "31M" : "A",
  "45A" : "A",
  "5FA" : "A",
  "6IA" : "A",
  "6MA" : "A",
  "6MC" : "A",
  "6MT" : "A",
  "6MZ" : "A",
  "6NW" : "A",
  "7AT" : "A",
  "7RZ" : "A",
  "8AH" : "A",
  "8AN" : "A",
  "9SI" : "A",
  "9SY" : "A",
  "A1ID5" : "A",
  "A1IEA" : "A",
  "A23" : "A",
  "A2L" : "A",
  "A2M" : "A",
  "A39" : "A",
  "A3P" : "A",
  "A44" : "A",
  "A5O" : "A",
  "A6A" : "A",
  "A7E" : "A",
  "A9Z" : "A",
  "AET" : "A",
  "AP7" : "A",
  "AVC" : "A",
  "F3N" : "A",
  "LCA" : "A",
  "M7A" : "A",
  "MA6" : "A",
  "MAD" : "A",
  "MGQ" : "A",
  "MIA" : "A",
  "MTU" : "A",
  "O2Z" : "A",
  "P5P" : "A",
  "PPU" : "A",
  "PR5" : "A",
  "PU" : "A",
  "QSQ" : "A",
  "RIA" : "A",
  "SRA" : "A",
  "T6A" : "A",
  "V3L" : "A",
  "ZAD" : "A",
  "10C" : "C",
  "16B" : "C",
  "1SC" : "C",
  "4OC" : "C",
  "5HM" : "C",
  "5IC" : "C",
  "5MC" : "C",
  "6OO" : "C",
  "73W" : "C",
  "7OK" : "C",
  "A5M" : "C",
  "A6C" : "C",
  "B8Q" : "C",
  "B8T" : "C",
  "B9H" : "C",
  "C25" : "C",
  "C2L" : "C",
  "C31" : "C",
  "C43" : "C",
  "C5L" : "C",
  "CBV" : "C",
  "CCC" : "C",
  "CH" : "C",
  "CSF" : "C",
  "E3C" : "C",
  "IC" : "C",
  "JMH" : "C",
  "LC" : "C",
  "LHH" : "C",
  "LV2" : "C",
  "M3X" : "C",
  "M4C" : "C",
  "M5M" : "C",
  "MMX" : "C",
  "N5M" : "C",
  "N7X" : "C",
  "OMC" : "C",
  "PMT" : "C",
  "RPC" : "C",
  "RSP" : "C",
  "RSQ" : "C",
  "S4C" : "C",
  "TC" : "C",
  "ZBC" : "C",
  "ZCY" : "C",
  "3DA" : "DA",
  "5AA" : "DA",
  "FA2" : "DA",
  "0AM" : "DA",
  "0AV" : "DA",
  "0SP" : "DA",
  "1AP" : "DA",
  "2AR" : "DA",
  "2BU" : "DA",
  "2DA" : "DA",
  "6HA" : "DA",
  "6HB" : "DA",
  "7DA" : "DA",
  "8BA" : "DA",
  "A1H3G" : "DA",
  "A34" : "DA",
  "A35" : "DA",
  "A38" : "DA",
  "A3A" : "DA",
  "A40" : "DA",
  "A43" : "DA",
  "A47" : "DA",
  "A5L" : "DA",
  "ABR" : "DA",
  "ABS" : "DA",
  "AD2" : "DA",
  "AF2" : "DA",
  "AS" : "DA",
  "DZM" : "DA",
  "E" : "DA",
  "E1X" : "DA",
  "EDA" : "DA",
  "K2F" : "DA",
  "L3X" : "DA",
  "MA7" : "DA",
  "PRN" : "DA",
  "R" : "DA",
  "RBD" : "DA",
  "RMP" : "DA",
  "S4A" : "DA",
  "SDE" : "DA",
  "SMP" : "DA",
  "TCY" : "DA",
  "TFO" : "DA",
  "XAD" : "DA",
  "XAL" : "DA",
  "XUA" : "DA",
  "Y" : "DA",
  "CAR" : "DC",
  "CB2" : "DC",
  "0AP" : "DC",
  "0R8" : "DC",
  "1CC" : "DC",
  "1FC" : "DC",
  "47C" : "DC",
  "4PC" : "DC",
  "4PD" : "DC",
  "4PE" : "DC",
  "4SC" : "DC",
  "4U3" : "DC",
  "5CM" : "DC",
  "5FC" : "DC",
  "5HC" : "DC",
  "5NC" : "DC",
  "5PC" : "DC",
  "6HC" : "DC",
  "8RO" : "DC",
  "8YN" : "DC",
  "B7C" : "DC",
  "C2S" : "DC",
  "C32" : "DC",
  "C34" : "DC",
  "C36" : "DC",
  "C37" : "DC",
  "C38" : "DC",
  "C42" : "DC",
  "C45" : "DC",
  "C46" : "DC",
  "C49" : "DC",
  "C4S" : "DC",
  "C7R" : "DC",
  "C7S" : "DC",
  "CBR" : "DC",
  "CDW" : "DC",
  "CFL" : "DC",
  "CFZ" : "DC",
  "CMR" : "DC",
  "CP1" : "DC",
  "CSL" : "DC",
  "CX2" : "DC",
  "D00" : "DC",
  "D4B" : "DC",
  "DCT" : "DC",
  "DFC" : "DC",
  "DNR" : "DC",
  "DOC" : "DC",
  "EIX" : "DC",
  "EXC" : "DC",
  "F7H" : "DC",
  "GCK" : "DC",
  "I5C" : "DC",
  "IMC" : "DC",
  "J0X" : "DC",
  "MCY" : "DC",
  "ME6" : "DC",
  "NCU" : "DC",
  "OKN" : "DC",
  "OKQ" : "DC",
  "PVX" : "DC",
  "SC" : "DC",
  "TC1" : "DC",
  "TCJ" : "DC",
  "TPC" : "DC",
  "U48" : "DC",
  "U7B" : "DC",
  "XCL" : "DC",
  "XCR" : "DC",
  "XCT" : "DC",
  "XCY" : "DC",
  "YCO" : "DC",
  "Z" : "DC",
  "5CG" : "DG",
  "GDR" : "DG",
  "LCG" : "DG",
  "0AD" : "DG",
  "0UH" : "DG",
  "2JV" : "DG",
  "2PR" : "DG",
  "63G" : "DG",
  "63H" : "DG",
  "68Z" : "DG",
  "6FK" : "DG",
  "6HG" : "DG",
  "6OG" : "DG",
  "6PO" : "DG",
  "7BG" : "DG",
  "7GU" : "DG",
  "7S8" : "DG",
  "8AG" : "DG",
  "8EB" : "DG",
  "8FG" : "DG",
  "8H2" : "DG",
  "8MG" : "DG",
  "8OG" : "DG",
  "8PY" : "DG",
  "AFG" : "DG",
  "BGM" : "DG",
  "C6G" : "DG",
  "DCG" : "DG",
  "DDG" : "DG",
  "DFG" : "DG",
  "DG8" : "DG",
  "DGI" : "DG",
  "EHG" : "DG",
  "F4Q" : "DG",
  "F74" : "DG",
  "F7K" : "DG",
  "FDG" : "DG",
  "FMG" : "DG",
  "FOX" : "DG",
  "G2S" : "DG",
  "G31" : "DG",
  "G32" : "DG",
  "G33" : "DG",
  "G36" : "DG",
  "G38" : "DG",
  "G42" : "DG",
  "G47" : "DG",
  "G49" : "DG",
  "GF2" : "DG",
  "GFL" : "DG",
  "GMS" : "DG",
  "GN7" : "DG",
  "GS" : "DG",
  "GSR" : "DG",
  "GSS" : "DG",
  "GX1" : "DG",
  "HN0" : "DG",
  "HN1" : "DG",
  "IGU" : "DG",
  "K39" : "DG",
  "LGP" : "DG",
  "M1G" : "DG",
  "MFO" : "DG",
  "MG1" : "DG",
  "MRG" : "DG",
  "OGX" : "DG",
  "P" : "DG",
  "PG7" : "DG",
  "PGN" : "DG",
  "PPW" : "DG",
  "PZG" : "DG",
  "RDG" : "DG",
  "S4G" : "DG",
  "S6G" : "DG",
  "SDG" : "DG",
  "SDH" : "DG",
  "SJO" : "DG",
  "TGP" : "DG",
  "X" : "DG",
  "XGL" : "DG",
  "XGR" : "DG",
  "XGU" : "DG",
  "XPB" : "DG",
  "XUG" : "DG",
  "2BD" : "DI",
  "OIP" : "DI",
  "2AT" : "DT",
  "2BT" : "DT",
  "2GT" : "DT",
  "2NT" : "DT",
  "2OT" : "DT",
  "ATL" : "DT",
  "BOE" : "DT",
  "EIT" : "DT",
  "P2T" : "DT",
  "S2M" : "DT",
  "SMT" : "DT",
  "T38" : "DT",
  "T39" : "DT",
  "T41" : "DT",
  "TFE" : "DT",
  "2DT" : "DT",
  "2ST" : "DT",
  "5AT" : "DT",
  "5HT" : "DT",
  "5IT" : "DT",
  "5PY" : "DT",
  "64T" : "DT",
  "6CT" : "DT",
  "6HT" : "DT",
  "84E" : "DT",
  "94O" : "DT",
  "A1AAZ" : "DT",
  "ATD" : "DT",
  "ATM" : "DT",
  "CTG" : "DT",
  "D3T" : "DT",
  "D4M" : "DT",
  "DPB" : "DT",
  "DRT" : "DT",
  "EAN" : "DT",
  "F3H" : "DT",
  "F4H" : "DT",
  "JDT" : "DT",
  "L5R" : "DT",
  "LSH" : "DT",
  "LST" : "DT",
  "MMT" : "DT",
  "MTR" : "DT",
  "NMS" : "DT",
  "NMT" : "DT",
  "NTT" : "DT",
  "PST" : "DT",
  "QCK" : "DT",
  "SPT" : "DT",
  "T0N" : "DT",
  "T0Q" : "DT",
  "T2S" : "DT",
  "T32" : "DT",
  "T36" : "DT",
  "T37" : "DT",
  "T3P" : "DT",
  "T48" : "DT",
  "T49" : "DT",
  "T4S" : "DT",
  "T5S" : "DT",
  "T64" : "DT",
  "TA3" : "DT",
  "TAF" : "DT",
  "TCP" : "DT",
  "TDY" : "DT",
  "TED" : "DT",
  "TFF" : "DT",
  "TFT" : "DT",
  "TLC" : "DT",
  "TP1" : "DT",
  "TSP" : "DT",
  "TTD" : "DT",
  "TTM" : "DT",
  "US3" : "DT",
  "XTF" : "DT",
  "XTH" : "DT",
  "XTL" : "DT",
  "XTR" : "DT",
  "GMU" : "DU",
  "TLN" : "DU",
  "0AU" : "DU",
  "18Q" : "DU",
  "5HU" : "DU",
  "5IU" : "DU",
  "5SE" : "DU",
  "77Y" : "DU",
  "8DT" : "DU",
  "BRU" : "DU",
  "BVP" : "DU",
  "DDN" : "DU",
  "DRM" : "DU",
  "DUZ" : "DU",
  "HDP" : "DU",
  "HEU" : "DU",
  "NDN" : "DU",
  "NDU" : "DU",
  "OHU" : "DU",
  "OKT" : "DU",
  "P2U" : "DU",
  "T5O" : "DU",
  "TTI" : "DU",
  "U2N" : "DU",
  "U33" : "DU",
  "UBI" : "DU",
  "UBR" : "DU",
  "UCL" : "DU",
  "UF2" : "DU",
  "UFR" : "DU",
  "UFT" : "DU",
  "UMS" : "DU",
  "UMX" : "DU",
  "UPE" : "DU",
  "UPS" : "DU",
  "URX" : "DU",
  "US1" : "DU",
  "US2" : "DU",
  "USM" : "DU",
  "UVX" : "DU",
  "ZDU" : "DU",
  "18M" : "G",
  "1MG" : "G",
  "23G" : "G",
  "2EG" : "G",
  "2MG" : "G",
  "2SG" : "G",
  "7MG" : "G",
  "7S3" : "G",
  "7SN" : "G",
  "8AA" : "G",
  "8OS" : "G",
  "A6G" : "G",
  "B8K" : "G",
  "B8W" : "G",
  "B9B" : "G",
  "BGH" : "G",
  "CG1" : "G",
  "E6G" : "G",
  "E7G" : "G",
  "EQ0" : "G",
  "EQ4" : "G",
  "G25" : "G",
  "G2L" : "G",
  "G46" : "G",
  "G48" : "G",
  "G7M" : "G",
  "GAO" : "G",
  "GDO" : "G",
  "GDP" : "G",
  "GH3" : "G",
  "GOM" : "G",
  "GRB" : "G",
  "HYJ" : "G",
  "IG" : "G",
  "IOO" : "G",
  "KAG" : "G",
  "KAK" : "G",
  "LG" : "G",
  "M2G" : "G",
  "MGV" : "G",
  "MHG" : "G",
  "N6G" : "G",
  "O2G" : "G",
  "OMG" : "G",
  "P7G" : "G",
  "PGP" : "G",
  "QUO" : "G",
  "RFJ" : "G",
  "TG" : "G",
  "TPG" : "G",
  "VC7" : "G",
  "VSN" : "G",
  "XTS" : "G",
  "YG" : "G",
  "YYG" : "G",
  "ZGU" : "G",
  "RT" : "T",
  "WUH" : "TTD",
  "125" : "U",
  "126" : "U",
  "127" : "U",
  "1RN" : "U",
  "2AU" : "U",
  "2MU" : "U",
  "2OM" : "U",
  "3AU" : "U",
  "3ME" : "U",
  "3MU" : "U",
  "3TD" : "U",
  "4SU" : "U",
  "5BU" : "U",
  "5FU" : "U",
  "5MU" : "U",
  "70U" : "U",
  "75B" : "U",
  "85Y" : "U",
  "9QV" : "U",
  "A1IC1" : "U",
  "A1LZ3" : "U",
  "A6U" : "U",
  "B8H" : "U",
  "CNU" : "U",
  "DHU" : "U",
  "F2T" : "U",
  "FHU" : "U",
  "FNU" : "U",
  "H2U" : "U",
  "I4U" : "U",
  "IU" : "U",
  "LHU" : "U",
  "MEP" : "U",
  "MNU" : "U",
  "OMU" : "U",
  "ONE" : "U",
  "P4U" : "U",
  "PSU" : "U",
  "PYO" : "U",
  "RUS" : "U",
  "S4U" : "U",
  "SSU" : "U",
  "SUR" : "U",
  "T31" : "U",
  "U23" : "U",
  "U25" : "U",
  "U2L" : "U",
  "U2P" : "U",
  "U31" : "U",
  "U34" : "U",
  "U36" : "U",
  "U37" : "U",
  "U8U" : "U",
  "UAR" : "U",
  "UBD" : "U",
  "UD5" : "U",
  "UPV" : "U",
  "UR3" : "U",
  "URD" : "U",
  "US5" : "U",
  "XSX" : "U",
  "ZBU" : "U",
}
if __name__ == '__main__':
  print(len(lookup.keys()))


 *******************************************************************************


 *******************************************************************************
iotbx/pdb/nucleic_acid_codes.py
"""One and three-letter RNA and DNA codes"""
from __future__ import absolute_import, division, print_function

rna_one_letter_code_dict = {
 'ADE':'A',
 'CYT':'C',
 'URI':'U',
 'GUA':'G',
 'A':'A',
 'C':'C',
 'U':'U',
 'G':'G',}

dna_one_letter_code_dict = {
 'ADE':'A',
 'CYT':'C',
 'THY':'T',
 'GUA':'G',
 'A':'A',
 'C':'C',
 'T':'T',
 'G':'G',
 'DA':'A',
 'DC':'C',
 'DT':'T',
 'DG':'G',
  }


 *******************************************************************************


 *******************************************************************************
iotbx/pdb/pdb_v3_cif_conversion.py
"""Methods to convert between a hierarchy object and a forward_compatible_pdb compatible string.
"""
from __future__ import absolute_import, division, print_function

'''
Rationale: Hierarchy object and mmcif representations can contain
  chain ID values with n-characters and residue names with 3 or 5
  characters.  PDB format only allows 2 chars for chain ID and 3 for
  residue names.

Approach: Convert all non-forward_compatible_pdb-compliant chain ID and residue names
  to suitable number of characters and save the conversion information
  as a conversion_info object and as RESNAM records (for residue names)
  and REMARK records (for chain ID) in PDB string representations of
  the hierarchy.

Examples of typical uses:

A. Write a forward_compatible_pdb compatible string with conversion information in REMARK
   and RESNAM records from any hierarchy (ph):
   NOTE: any kw and args for as_pdb_string() can be supplied

  from iotbx.pdb.forward_compatible_pdb_cif_conversion import hierarchy_as_forward_compatible_pdb_string
  forward_compatible_pdb_string =  hierarchy_as_forward_compatible_pdb_string(ph)

B. Read a forward_compatible_pdb compatible string (forward_compatible_pdb_string) with conversion
   information in REMARK/RESNAM records and convert to a hierarchy (
   inverse of A).
   NOTE: same function will read any mmcif string as well.

  from iotbx.pdb.forward_compatible_pdb_cif_conversion import pdb_or_mmcif_string_as_hierarchy
  ph = pdb_or_mmcif_string_as_hierarchy(forward_compatible_pdb_string).hierarchy

C. Get conversion info from any hierarchy (ph):

  from iotbx.pdb.forward_compatible_pdb_cif_conversion import forward_compatible_pdb_cif_conversion
  conversion_info = forward_compatible_pdb_cif_conversion(hierarchy = ph)

D. Get conversion info from unique chain_ids and residue names (
    unique_values_dict):

  from iotbx.pdb.forward_compatible_pdb_cif_conversion import forward_compatible_pdb_cif_conversion
  conversion_info = forward_compatible_pdb_cif_conversion(
    unique_values_dict = unique_values_dict)

D. Get conversion info as REMARK and RESNAM string

  from iotbx.pdb.forward_compatible_pdb_cif_conversion import forward_compatible_pdb_cif_conversion
  remark_hetnam_string = forward_compatible_pdb_cif_conversion(ph).conversion_as_remark_hetnam_string()

E. Convert a forward_compatible_pdb compatible hierarchy to a full hierarchy with
   conversion information in conversion_info. This approach can be
   used to (1) save conversion information from a hierarchy,
   (2) write a forward_compatible_pdb file, (3) do something with the forward_compatible_pdb file that loses
   the header information, (4) read back the forward_compatible_pdb file that does not have
   REMARK records, and (5) restore the original structure in the new
   hierarchy.

  from iotbx.pdb.forward_compatible_pdb_cif_conversion import forward_compatible_pdb_cif_conversion
  from iotbx.pdb.forward_compatible_pdb_cif_conversion import hierarchy_as_forward_compatible_pdb_string
  from iotbx.pdb.forward_compatible_pdb_cif_conversion import pdb_or_mmcif_string_as_hierarchy

  # Get conversion information
  conversion_info = forward_compatible_pdb_cif_conversion(ph)

  # Get a forward_compatible_pdb string with no remarks
  forward_compatible_pdb_string =  hierarchy_as_forward_compatible_pdb_string(ph)
  forward_compatible_pdb_string_no_remarks = remove_remarks(forward_compatible_pdb_string)

  # convert back to hierarchy (this can be a new pdb string obtained
  #  after manipulations of the model but with residue names and chain id
  #  values matching the forward_compatible_pdb_string)
  ph = pdb_or_mmcif_string_as_hierarchy(forward_compatible_pdb_string_no_remarks).hierarchy

  # Apply the conversions to obtain a full representation in ph
  conversion_info.convert_hierarchy_to_full_representation(ph)
  '''

def hierarchy_as_forward_compatible_pdb_string(ph, conversion_info = None, *args, **kw):
  '''Convert a hierarchy into a forward_compatible_pdb compatible string, with any
     conversion information written as REMARK records

    parameters:
      ph: hierarchy object
      conversion_info: optional conversion_info object specifying conversion
      args, kw: any args and kw suitable for the hierarchy
          method ph.as_pdb_string()

    returns:  string
  '''

  if not conversion_info:
    conversion_info = forward_compatible_pdb_cif_conversion(hierarchy = ph)

  if (not conversion_info.conversion_required()):
    return ph.as_pdb_string(*args, **kw)
  else:
    ph_forward_compatible_pdb = ph.deep_copy()
    conversion_info.convert_hierarchy_to_forward_compatible_pdb_representation(ph_forward_compatible_pdb)
    remark_hetnam_string = conversion_info.conversion_as_remark_hetnam_string()
    forward_compatible_pdb_string = ph_forward_compatible_pdb.as_pdb_string(*args, **kw)
    full_string = remark_hetnam_string + forward_compatible_pdb_string
    return full_string

def pdb_or_mmcif_string_as_hierarchy(pdb_or_mmcif_string,
       conversion_info = None):
  '''Convert an mmcif string or a forward_compatible_pdb compatible string into a
      hierarchy object, using any conversion information written as
      REMARK records in the forward_compatible_pdb string, or using any supplied
      conversion information.

    parameters:
      pdb_or_mmcif_string: mmcif string or a forward_compatible_pdb compatible string
      conversion_info: optional forward_compatible_pdb_cif_conversion object to apply

    returns: group_args (hierarchy, pdb_inp, crystal_symmetry, conversion_info)
  '''
  import iotbx.pdb
  from iotbx.pdb.forward_compatible_pdb_cif_conversion import forward_compatible_pdb_cif_conversion
  inp = iotbx.pdb.input(lines=pdb_or_mmcif_string, source_info=None)
  remark_hetnam_string = "\n".join(inp.remark_section())
  hetnam_string = "\n".join(inp.heterogen_section())
  remark_hetnam_string += "\n"+ hetnam_string
  crystal_symmetry = inp.crystal_symmetry()

  if (not conversion_info):
    conversion_info = forward_compatible_pdb_cif_conversion()
    conversion_info.set_conversion_tables_from_remark_hetnam_records(
      remark_hetnam_records = remark_hetnam_string.splitlines())
  assert conversion_info.is_initialized()

  # Get the hierarchy
  ph = inp.construct_hierarchy()
  from libtbx import group_args
  result = group_args(
    group_args_type = 'hierarchy and crystal_symmetry from text string',
    hierarchy = ph,
    pdb_inp = inp,
    crystal_symmetry = crystal_symmetry,
    conversion_info = conversion_info)

  # Determine if this is already in full format
  if forward_compatible_pdb_cif_conversion(ph).conversion_required(): # already set
    assert not conversion_info.conversion_required(), \
      "Cannot apply forward_compatible_pdb conversions to a hierarchy that is not forward_compatible_pdb"
  elif conversion_info.conversion_required(): # convert it
    conversion_info.convert_hierarchy_to_full_representation(ph)
  else: # nothing to convert
    pass
  return result

class forward_compatible_pdb_cif_conversion:
  ''' Class to generate and save forward_compatible_pdb representation of 5-character
    residue names and n-character chain IDs. Used to convert between
    forward_compatible_pdb and mmcif formatting.

    NOTE 1: marked as self._is_initialized when values are available
    NOTE 2: hierarchy object that has been converted to forward_compatible_pdb compatible
    will be marked with the attribute
      self._is_forward_compatible_pdb_representation=True


    To modify these tables to add another field to check:
    1. Add new field to self._keys and self._max_chars_dict
    2. Add new methods like "def _unique_chain_ids_from_hierarchy"
    3. Use these new methods in "def _set_up_conversion_table"
    4. Add code at "Modify hierarchy here to convert to forward_compatible_pdb"
    5. Add code at "Modify hierarchy here to convert from forward_compatible_pdb"
    6. Add code to regression test at iotbx/regression/tst_hierarchy_forward_compatible_pdb.py
    '''

  def __init__(self, hierarchy = None,
     unique_values_dict = None,
     residue_conversion_as_remark = True,
     residue_conversion_as_hetnam = True,
     end_residue_names_with_tilde_if_possible = True,
     ):
    ''' Identify all unique chain_ids and residue names that are not compatible
        with forward_compatible_pdb. Generate dictionary relating original names and
        compatible names and for going backwards.

    parameters:  iotbx.pdb.hierarchy object (required unless unique_values_dict
           is supplied)
        unique_values_dict:  Optional dict with unique values for each key
        residue_conversion_as_remark:   read and write conversion for residue
            name as a REMARK
        residue_conversion_as_hetnam:   read and write conversion for residue
            name as a HETNAM record
        end_residue_names_with_tilde_if_possible:  try to make 3-char residue
                                                    names as 2 chars + "~"

    returns:  None

    '''


    # Fields in hierarchy that are limited in number of characters in forward_compatible_pdb
    self._keys = ['chain_id', 'resname']
    self._max_chars_dict = {'chain_id':2, 'resname':3}
    self._end_with_tilde_dict = {'chain_id':False, 'resname':True}

    if unique_values_dict is not None:
      for key in self._keys:
        assert key in list(unique_values_dict.keys())

    self._remark_keys = ['chain_id']
    self._hetnam_keys = []
    self._residue_conversion_as_remark = residue_conversion_as_remark
    if self._residue_conversion_as_remark:
      self._remark_keys.append('resname')

    self._residue_conversion_as_hetnam = residue_conversion_as_hetnam
    if self._residue_conversion_as_hetnam:
      self._hetnam_keys.append('resname')

    self._is_initialized = False

    if hierarchy is None and unique_values_dict is None:
      self._conversion_table_info_dict = None
      self._conversion_required = None
      return

    # Set up conversion tables
    self._conversion_table_info_dict = {}

    # Flag that indicates if any conversion is necessary
    self._conversion_required = False

    for key in self._keys:
      self._set_up_conversion_table(key, hierarchy,
        unique_values_dict = unique_values_dict)

    self._is_initialized = True

  def is_initialized(self):

    '''Public method to return True if this is initialized
    parameters:  None
    returns: True if initialized
    '''
    return self._is_initialized

  def conversion_required(self):

    '''Public method to return True if conversion for forward_compatible_pdb is necessary
    parameters:  None
    returns: True if conversion is necessary
    '''
    assert self.is_initialized(), "Need to initialize"
    return self._conversion_required

  def conversion_as_remark_hetnam_string(self):
    '''Public method to return a PDB REMARK/HETNAM string representing all the
    conversions that are necessary
    '''

    assert self.is_initialized(), "Need to initialize"

    if not self.conversion_required():
      return ""  # No info needed


    from six.moves import cStringIO as StringIO
    f = StringIO()
    print(
       "REMARK 987 PDB_V3_CONVERSION  CONVERSIONS MADE FOR PDB_V3 COMPATIBILITY",
           file = f)

    # Set up conversion info that goes in REMARK records
    for key in self._remark_keys:
      info =  self._conversion_table_info_dict[key]
      if info:
        for full_text, forward_compatible_pdb_text in zip(
           info.full_representation_list,
           info.forward_compatible_pdb_representation_list,
           ):
          print(
            "REMARK 987 PDB_V3_CONVERSION  %s: %s  PDB_V3_TEXT: %s" %(
              key.upper(),
              full_text,
              forward_compatible_pdb_text),
            file = f)
    print(file = f)

    # Set conversion info that goes in HETNAM records
    """
HETNAM
Overview

This record gives the chemical name of the compound with the given hetID.

Record Format

COLUMNS       DATA  TYPE    FIELD           DEFINITION
----------------------------------------------------------------------------
 1 -  6       Record name   "HETNAM"
 9 - 10       Continuation  continuation    Allows concatenation of multiple records.
12 - 14       LString(3)    hetID           Het identifier, right-justified.
16 - 70       String        text            Chemical name.
    """
    for key in self._hetnam_keys:
      info =  self._conversion_table_info_dict[key]
      if info:
        for full_text, forward_compatible_pdb_text in zip(
           info.full_representation_list,
           info.forward_compatible_pdb_representation_list,
           ):
          print("%6s  %2s %3s %55s%10s" %(
              "HETNAM".ljust(6),
              "".ljust(2),  # continuation chars
              forward_compatible_pdb_text.ljust(3),  # 3-char version
              "PDB_V3_CONVERSION (FULL NAME IN COLS 71-80)".ljust(55),  # any text for 55 chars
              full_text.ljust(10)),  # full version
            file = f)
    print(file = f)


    return f.getvalue()


  def convert_hierarchy_to_forward_compatible_pdb_representation(self, hierarchy):

    '''Public method to convert a hierarchy in place to forward_compatible_pdb compatible
       hierarchy using information in self._conversion_table_info_dict
    parameters: hierarchy (modified in place)
    output: None

    '''

    assert self.is_initialized(), "Need to initialize"
    assert hierarchy is not None, "Need hierarchy for conversion"

    if hasattr(hierarchy, '_is_forward_compatible_pdb_representation') and (
        hierarchy._is_forward_compatible_pdb_representation):
      return # nothing to do because it was already converted

    if not self.conversion_required():
      return # nothing to do because no conversion is necessary

    # Modify hierarchy here to convert to forward_compatible_pdb

    for model in hierarchy.models():
      for chain in model.chains():
        new_id = self.get_forward_compatible_pdb_text_from_full_text(
           key = 'chain_id',
           full_text = chain.id)
        if new_id and new_id != chain.id:
          chain.id = new_id  # Modify chain ID here

        for residue_group in chain.residue_groups():
          for atom_group in residue_group.atom_groups():
            new_resname = self.get_forward_compatible_pdb_text_from_full_text('resname',
                atom_group.resname)
            if new_resname and (new_resname != atom_group.resname):
              atom_group.resname = new_resname  # Modify residue name here

    hierarchy._is_forward_compatible_pdb_representation = True

  def convert_hierarchy_to_full_representation(self, hierarchy):
    '''Public method to convert a hierarchy in place from forward_compatible_pdb compatible
       hierarchy using information in self._conversion_table_info_dict
    parameters: hierarchy (modified in place)
    output: None

    '''
    assert hierarchy is not None, "Need hierarchy for conversion"
    assert self.is_initialized(), "Need to initialize"

    if hasattr(hierarchy, '_is_forward_compatible_pdb_representation') and (
        hierarchy._is_forward_compatible_pdb_representation):
      return # nothing to do because it was already converted

    if not self.conversion_required():
      return # nothing to do because no conversion is necessary

    # Modify hierarchy here to convert from forward_compatible_pdb

    for model in hierarchy.models():
      for chain in model.chains():
        new_id = self.get_full_text_from_forward_compatible_pdb_text(
          key = 'chain_id',
          forward_compatible_pdb_text = chain.id)
        if new_id and new_id != chain.id:
          chain.id = new_id  # Modify chain_id here

        for residue_group in chain.residue_groups():
          for atom_group in residue_group.atom_groups():
            new_resname = self.get_full_text_from_forward_compatible_pdb_text(
              key = 'resname',
              forward_compatible_pdb_text = atom_group.resname)
            if new_resname and (new_resname != atom_group.resname):
              atom_group.resname = new_resname # Modify residue name here

    hierarchy._is_forward_compatible_pdb_representation = False

  def set_conversion_tables_from_remark_hetnam_records(
       self, remark_hetnam_records, add_to_existing = False):
    ''' Public method to set conversion tables based on remarks and hetnam
        records written in standard form as by this class

    parameters:
      remark_hetnam_records:  list of lines, containing REMARK
                        and HETNAM lines with information
                        conversion_as_remark_hetnam_string
      add_to_existing: do not re-initialize if already initialized
    returns: None
    '''


    if not remark_hetnam_records:
      self._is_initialized = True
      return # nothing to do

    self._conversion_required = False

    if (self._is_initialized and add_to_existing):
      pass # keep existing dicts
    else: # usual...initialize
      full_representation_list_dict = {}
      forward_compatible_pdb_representation_list_dict = {}

      for key in self._keys:
        full_representation_list_dict[key] = []
        forward_compatible_pdb_representation_list_dict[key] = []
    self._is_initialized = True

    for line in remark_hetnam_records:
      if not line: continue
      spl = line.split()
      if (spl[0] == "REMARK") and (spl[1] == "987") and (spl[2] == "PDB_V3_CONVERSION"):
        if len(spl) != 7: continue
        key = spl[3].lower()[:-1] # take off ":"
        if not key in self._remark_keys: continue
        full = spl[4]
        forward_compatible_pdb = spl[6]
      elif self._residue_conversion_as_hetnam and (spl[0] == "HETNAM"):
        key = "resname"
        if not key in self._hetnam_keys: continue
        forward_compatible_pdb = line[11:14].strip()
        full = line[69:80].strip()
        if not forward_compatible_pdb: continue
        if not full: continue
      else:
        continue

      if not full in full_representation_list_dict[key]:
        full_representation_list_dict[key].append(full)
        forward_compatible_pdb_representation_list_dict[key].append(forward_compatible_pdb)

      # there was something needing conversion
      self._conversion_required = True

    self._is_initialized = True

    if not self._conversion_required: # nothing to do
      return

    self._conversion_table_info_dict = {}
    from libtbx import group_args
    for key in self._keys:
      self._conversion_table_info_dict[key] = group_args(
        group_args_type = 'conversion tables for %s' %(key),
        full_representation_list = full_representation_list_dict[key],
        forward_compatible_pdb_representation_list =  forward_compatible_pdb_representation_list_dict[key])


  def _set_up_conversion_table(self, key, hierarchy, unique_values_dict = None):
    ''' Private method to set up conversion table from a hierarchy for
        field named by key and put it in self._conversion_table_info_dict[key].
        Also set self._conversion_required if conversion is needed.
        also set self._is_initialized'''

    if unique_values_dict is not None:
      unique_values = unique_values_dict[key]  # use supplied values
    elif key == 'chain_id':
      unique_values = self._unique_chain_ids_from_hierarchy(hierarchy)
    elif key == 'resname':
      unique_values = self._unique_resnames_from_hierarchy(hierarchy)
    else:
      raise "NotImplemented"

    end_with_tilde = self._end_with_tilde_dict[key]

    max_chars = self._max_chars_dict[key]
    allowed_ids, ids_needing_conversion = self._choose_allowed_ids(
        unique_values,
        max_chars = max_chars)

    forward_compatible_pdb_representation_list = self._get_any_forward_compatible_pdb_representation(
        ids_needing_conversion, max_chars, exclude_list = allowed_ids,
        end_with_tilde = end_with_tilde)

    if ids_needing_conversion:
      assert len(ids_needing_conversion) == len(forward_compatible_pdb_representation_list)

    from libtbx import group_args
    self._conversion_table_info_dict[key] = group_args(
      group_args_type = 'conversion tables for %s' %(key),
      full_representation_list = ids_needing_conversion,
      forward_compatible_pdb_representation_list = forward_compatible_pdb_representation_list)

    if forward_compatible_pdb_representation_list:  # there was something needing conversion
      self._conversion_required = True

  def _unique_chain_ids_from_hierarchy(self, hierarchy):
    ''' Private method to identify all unique chain IDs in a hierarchy
    parameters:  hierarchy
    returns:  list of unique chain ids

    '''
    chain_ids = []
    for model in hierarchy.models():
      for chain in model.chains():
        if (not chain.id in chain_ids):
          chain_ids.append(chain.id)
    return chain_ids

  def _unique_resnames_from_hierarchy(self, hierarchy):
    ''' Private method to identify all unique residue names in a hierarchy
    parameters:  hierarchy
    returns:  list of unique residue names

    '''
    resnames = []
    for model in hierarchy.models():
      for chain in model.chains():
        for residue_group in chain.residue_groups():
          for atom_group in residue_group.atom_groups():
            if (not atom_group.resname in resnames):
              resnames.append(atom_group.resname)
    return resnames

  def _get_any_forward_compatible_pdb_representation(self, ids_needing_conversion,
     max_chars, exclude_list = None,
     end_with_tilde = None):
    '''Private method to try a few ways to generate unique forward_compatible_pdb
      representations for a set of strings.  Order to try:
      1. take first max_chars of each.
      2. if end_with_tilde, then generate max_chars-1 of numbers plus tilde,
      3. generate anything up to max_chars
    parameters:
      ids_needing_conversion:  list of strings to convert
      max_chars:  maximum characters in converted strings
      exclude_list: list of strings not to use as output
      end_with_tilde: try to end strings with a tilde ("~")
    returns:
      forward_compatible_pdb_representation_list: list of converted strings, same order and
        length as ids_needing_conversion
    '''

    if (not ids_needing_conversion):
       return [] # ok with nothing in it

    # Try just taking first n_chars of strings...ok if they are all unique
    forward_compatible_pdb_representation_list = self._get_forward_compatible_pdb_representation(
        ids_needing_conversion, max_chars, exclude_list = exclude_list,
        take_first_n_chars = True)
    if forward_compatible_pdb_representation_list:
      return forward_compatible_pdb_representation_list

    # Generate unique strings for all the ids needing conversion, preventing
    #   duplications of existing ids
    forward_compatible_pdb_representation_list = self._get_forward_compatible_pdb_representation(
        ids_needing_conversion, max_chars, exclude_list = exclude_list,
        end_with_tilde = end_with_tilde)
    if forward_compatible_pdb_representation_list:
      return forward_compatible_pdb_representation_list

    # Failed to get forward_compatible_pdb representation...
    from libtbx.utils import Sorry
    raise Sorry("Unable to generate forward_compatible_pdb representation of %s" %(key))

  def _get_forward_compatible_pdb_representation(self, ids, max_chars,
      exclude_list = None, take_first_n_chars = False,
      end_with_tilde = False):

    '''Private method to try and get forward_compatible_pdb representation of ids that fit in
       max_chars and do not duplicate anything in exclude_list
    parameters:
      ids:  strings to convert
      max_chars:  maximum characters in output
      exclude_list: strings to not include in output
      take_first_n_chars: just take the first max_chars if set
      end_with_tilde: try to end strings with a tilde ("~")

    returns:
      list of converted strings of same order and length as ids, if successful
      otherwise, None
    '''

    forward_compatible_pdb_representation_list = []
    for id in ids:
      if take_first_n_chars:  # Just take the first n_chars
        new_id = id[:max_chars]
        if new_id in exclude_list + forward_compatible_pdb_representation_list:
          return None # cannot do it this way
      else:  # generate a new id
        new_id = self._get_new_unique_id(id, max_chars,
           exclude_list + forward_compatible_pdb_representation_list,
           end_with_tilde = end_with_tilde)
        if not new_id:
          return None # could not do this
      forward_compatible_pdb_representation_list.append(new_id)
    return forward_compatible_pdb_representation_list

  def _get_new_unique_id(self, id, max_chars, exclude_list,
     end_with_tilde):
    ''' Private method to get a unique ID with up to max_chars that is not
    in exclude_list. Start with max_chars and work down and use reverse order
    so as to generally create codes that are unlikely for others to have used.
    Also start with numbers, then numbers and letters, then everything
    '''
    for z in (
        [False, False, True, False],
        [True, False, True, False],
        [True, True, True, False],
        [True, True, True, True],):
      include_upper, include_lower, include_numbers, include_special_chars = z

      if end_with_tilde:
        id = self._get_new_id(max_chars, exclude_list, end_with_tilde = True,
          include_upper = include_upper,
          include_lower = include_lower,
          include_numbers = include_numbers,
          include_special_chars = include_special_chars,)
        if id:
          return id

      for n_chars_inv in range(max_chars):
        n_chars = max_chars - n_chars_inv
        id = self._get_new_id(n_chars, exclude_list,
          include_upper = include_upper,
          include_lower = include_lower,
          include_numbers = include_numbers,
          include_special_chars = include_special_chars,)
        if id:
          return id

  def _get_new_id(self, n_chars, exclude_list, end_with_tilde = None,
      include_upper = True,
      include_lower = True,
      include_numbers = True,
      include_special_chars = True,
       ):
    ''' Private method to get a unique ID with exactly n_chars that is not
    in exclude_list
    '''
    from iotbx.pdb.utils import generate_n_char_string
    x = generate_n_char_string(n_chars = n_chars,
       reverse_order = (not end_with_tilde),
       end_with_tilde = end_with_tilde,
       include_upper = include_upper,
       include_lower = include_lower,
       include_numbers = include_numbers,
       include_special_chars = include_special_chars,
      )
    while 1:
      new_id = x.next()
      if (not new_id):
        return None # failed
      elif (not new_id in exclude_list):
        return new_id

  def _choose_allowed_ids(self, unique_values, max_chars):
    ''' Private method to separate unique_values into those that are and
        are not compatible with forward_compatible_pdb (i.e., have max_chars or fewer)
    '''
    allowed = []
    not_allowed = []
    for u in unique_values:
      if self._is_allowed(u, max_chars):
        allowed.append(u)
      else:
        not_allowed.append(u)
    return allowed, not_allowed

  def _is_allowed(self, u, max_chars):
    ''' Private method to identify whether the string u is or is not
        compatible with forward_compatible_pdb (i.e., has max_chars or fewer)
    '''
    if len(u) <= max_chars:
      return True
    else:
      return False


  def _get_conversion_table_info(self, key):
    ''' Private method to return conversion table info for
        specified key (e.g., chain_id, resname)
    '''

    if not key in self._keys:
      return None
    elif (not self._conversion_required):
      return None
    else:
      return self._conversion_table_info_dict[key]

  def get_full_text_from_forward_compatible_pdb_text(self, key = None, forward_compatible_pdb_text = None):
    '''Public method to return full text from forward_compatible_pdb_text based on
       conversion table

    parameters:
      key: field to convert (e.g., chain_id, resname)
      forward_compatible_pdb_text: text to convert from forward_compatible_pdb to full text
    '''

    assert key is not None
    assert forward_compatible_pdb_text is not None

    conversion_table_info = self._get_conversion_table_info(key)

    if conversion_table_info and (
        forward_compatible_pdb_text in conversion_table_info.forward_compatible_pdb_representation_list):
      index = conversion_table_info.forward_compatible_pdb_representation_list.index(
        forward_compatible_pdb_text)
      full_text = conversion_table_info.full_representation_list[index]
    else:
      full_text = forward_compatible_pdb_text

    return full_text

  def get_forward_compatible_pdb_text_from_full_text(self, key = None, full_text = None):
    '''Public method to return forward_compatible_pdb text from full text based on
       conversion table

    parameters:
      key: field to convert (e.g., chain_id, resname)
      full_text: text to convert to forward_compatible_pdb
    '''

    assert key is not None
    assert full_text is not None

    conversion_table_info = self._get_conversion_table_info(key)
    if conversion_table_info and (
        full_text in conversion_table_info.full_representation_list):
      index = conversion_table_info.full_representation_list.index(
        full_text)
      forward_compatible_pdb_text = conversion_table_info.forward_compatible_pdb_representation_list[index]
    else:
      forward_compatible_pdb_text = full_text

    # Make sure that the resulting text is allowed in forward_compatible_pdb
    assert self._is_allowed(forward_compatible_pdb_text, self._max_chars_dict[key])

    return forward_compatible_pdb_text


 *******************************************************************************


 *******************************************************************************
iotbx/pdb/pdbe.py
"""
PDBe web services
"""

from __future__ import absolute_import, division, print_function

from iotbx.pdb.web_service_api import FTPService, RESTService
from iotbx.pdb.download import openurl, NotFound

import json


class configurable_get_request(object):

  def __init__(self, opener):

    self.opener = opener


  def __call__(self, url, identifier):

    lcase = identifier.lower()

    stream = self.opener( url = ( url + lcase ).encode() )
    result_for = json.load( stream )
    stream.close()
    assert lcase in result_for
    return result_for[ lcase ]


class configurable_multiget_request(object):

  def __init__(self, opener):

    self.get_request = configurable_get_request( opener = opener )


  def __call__(self, url, identifiers):

    for ident in identifiers:
      try:
        result = get_request( url = url, identifier = ident )

      except NotFound:
        yield None

      else:
        yield result


class configurable_post_request(object):

  def __init__(self, opener):

    self.opener = opener


  def __call__(self, url, identifiers):

    if len( identifiers ) == 0:
      return ( i for i in () )

    try:
      stream = self.opener( url = url, data = ( ",".join( identifiers ) ).encode() )

    except NotFound:
      return ( None for i in identifiers )

    result_for = json.load( stream )
    stream.close()

    return ( result_for.get( ident.lower() ) for ident in identifiers )


# Define instances for default use
get_request = configurable_get_request( opener = openurl )
multiget_request = configurable_multiget_request( opener = openurl )
post_request = configurable_post_request( opener = openurl )


def identifier_to_pdb_entry_name(identifier):

  return "pdb%s.ent" % identifier.lower()


def identifier_to_cif_entry_name(identifier):

  return "%s.cif" % identifier.lower()


PDB_ENTRYFILE_PDB = FTPService(
  url = "http://www.ebi.ac.uk/pdbe/entry-files/",
  namer = identifier_to_pdb_entry_name,
  )

PDB_ENTRYFILE_CIF = FTPService(
  url = "http://www.ebi.ac.uk/pdbe/entry-files/",
  namer = identifier_to_cif_entry_name,
  )

PDBE_API_BASE = "http://www.ebi.ac.uk/pdbe/api/"

PDB_ENTRY_STATUS = RESTService(
  url = PDBE_API_BASE + "pdb/entry/status/",
  get = get_request,
  post = post_request,
  )

PDB_ENTRY_EXPERIMENT = RESTService(
  url = PDBE_API_BASE + "pdb/entry/experiment/",
  get = get_request,
  post = post_request,
  )

PDB_SIFTS_MAPPING_CATH = RESTService(
  url = PDBE_API_BASE + "mappings/cath/",
  get = get_request,
  post = multiget_request,
  )

PDB_SIFTS_MAPPING_SCOP = RESTService(
  url = PDBE_API_BASE + "mappings/scop/",
  get = get_request,
  post = multiget_request,
  )


class Redirections(object):
  """
  Use status queries
  """

  def __init__(self):

    self._currents = set()
    self._retracteds = set()
    self._replaced_by = {}


  def obsoleted(self, identifier):

    std = self._check_and_fetch( identifier = identifier )
    return std in self._replaced_by


  def replacement_for(self, identifier):

    std = self._check_and_fetch( identifier = identifier )
    return self._replaced_by[ std ]


  def retracted(self, identifier):

    std = self._check_and_fetch( identifier = identifier )
    return std in self._retracteds


  def seed(self, identifiers):

    from six.moves import zip

    blocks = PDB_ENTRY_STATUS.multiple( identifiers = identifiers )

    for ( code, data ) in zip( identifiers, blocks ):
      if not data:
        continue

      self._insert( code = self._standardize( identifier = code ), data = data )


  def _standardize(self, identifier):

    return identifier.lower()


  def _check_and_fetch(self, identifier):

    std = self._standardize( identifier = identifier )

    if std in self._currents or std in self._retracteds or std in self._replaced_by:
      return std

    data = PDB_ENTRY_STATUS.single( identifier = std )
    self._insert( code = std, data = data )
    return std


  def _insert(self, code, data):

    status = data[0][ "status_code" ]

    if status == "REL":
      self._currents.add( code )

    elif status == "OBS":
      successor = data[0][ "superceded_by" ][0]

      if successor is None:
        self._retracteds.add( code )

      else:
        self._replaced_by[ code ] = successor


from libtbx.object_oriented_patterns import lazy_initialization
redirection = lazy_initialization( func = Redirections )


 *******************************************************************************


 *******************************************************************************
iotbx/pdb/rcsb.py
"""Tools for working with RCSB API"""
from __future__ import absolute_import, division, print_function

from iotbx.pdb.web_service_api import FTPService
from iotbx.pdb.download import openurl, urlopener, gzip_encoding


def parse_pdb_redirections(lineiter):

  import re
  regex = re.compile(
    r"""
    ^ OBSLTE \s+
    [\w-]+ [ ]+
    ( \w{4} )
    (?: [ ]+ ( \w{4} ) )*
    \s* $
    """,
    re.VERBOSE | re.MULTILINE
    )

  replacement_for = {}

  for line in lineiter:
    hit = regex.match( line )

    if hit is not None:
      groups = hit.groups()
      obsoleted = groups[0].upper()
      assert 2 <= len( groups )

      if groups[1] is None:
        replacement = None

      else:
        replacement = groups[1].upper()

      replacement_for[ obsoleted ] = replacement

  return replacement_for


def end_of_chain(identifier, replacement_for):

  while True:
    if identifier is None or identifier not in replacement_for:
      break

    else:
      identifier = replacement_for[ identifier ]

  return identifier


class Redirections(object):
  """
  Downloads a list of redirections from the PDB
  """

  def __init__(self):

    stream = openurl(
      url = "https://files.wwpdb.org/pub/pdb/data/status/obsolete.dat",
      )

    chain_redirection_for = parse_pdb_redirections( lineiter = stream )

    self.redirection_for = {}

    for key in chain_redirection_for:
      self.redirection_for[ key ] = end_of_chain(
        identifier = key,
        replacement_for = chain_redirection_for,
        )


  def obsoleted(self, identifier):

    std = identifier.upper()
    return ( std in self.redirection_for and self.redirection_for[ std ] is not None )


  def replacement_for(self, identifier):

    std = identifier.upper()
    return self.redirection_for[ std ]


  def retracted(self, identifier):

    std = identifier.upper()
    return ( std in self.redirection_for and self.redirection_for[ std ] is None )


  def seed(self, identifiers):

    pass


from libtbx.object_oriented_patterns import lazy_initialization
redirection = lazy_initialization( func = Redirections )

# Special opener that disables gzip encoding (to fetch files already compressed)
gz_openurl = urlopener( extras = [] )
gz_encoding = gzip_encoding()

def identifier_to_pdb_entry_name(identifier):

  return "%s.pdb.gz" % identifier.lower()


def identifier_to_cif_entry_name(identifier):

  return "%s.cif.gz" % identifier.lower()


PDB_ENTRYFILE_PDB = FTPService(
  url = "http://files.rcsb.org/download/",
  namer = identifier_to_pdb_entry_name,
  opener = gz_openurl,
  postprocessing = gz_encoding,
  )

PDB_ENTRYFILE_CIF = FTPService(
  url = "http://www.ebi.ac.uk/pdbe/entry-files/",
  namer = identifier_to_cif_entry_name,
  opener = gz_openurl,
  postprocessing = gz_encoding,
  )


 *******************************************************************************


 *******************************************************************************
iotbx/pdb/records.py
"""Unpack information from PDB file"""
from __future__ import absolute_import, division, print_function
class FormatError(RuntimeError): pass

def chain_id_strip(id):
  if (id[0] == " "): return id[1]
  return id

class _base(object):

  def __init__(self, pdb_str):
    if (len(pdb_str) < 80): pdb_str += " " * (80-len(pdb_str))
    self.pdb_str = pdb_str

  def raise_FormatError(self, msg):
    raise FormatError('%s:\n  "%s"' % (msg, self.pdb_str))

class header(_base):

  def __init__(self, pdb_str):
    # 11 - 50  String(40)    classification  Classifies the molecule(s)
    # 51 - 59  Date          depDate         Deposition date.  This is the date
    #                                        the coordinates were received by
    #                                        the PDB
    # 63 - 66  IDcode        idCode          This identifier is unique within
    #                                        PDB
    _base.__init__(self, pdb_str)
    self.classification = self.pdb_str[10:50]
    self.depdate = self.pdb_str[50:59]
    self.idcode = self.pdb_str[62:66]

class expdta(_base):

  def __init__(self, pdb_str):
    #  9 - 10  Continuation   continuation  Allows concatenation of
    #                                       multiple records.
    # 11 - 70  List           technique     The experimental technique(s)
    #                                       with optional comment
    #                                       describing the sample
    #                                       or experiment.
    _base.__init__(self, pdb_str)
    self.continuation = self.pdb_str[8:10]
    self.technique = self.pdb_str[10:70]
    TECH = self.technique.upper()
    self.keywords = []
    for keyword in ("ELECTRON DIFFRACTION",
                    "FIBER DIFFRACTION",
                    "FLUORESCENCE TRANSFER",
                    "NEUTRON DIFFRACTION",
                    "NMR",
                    "THEORETICAL MODEL",
                    "X-RAY DIFFRACTION"):
      if (TECH.find(keyword) >= 0):
        self.keywords.append(keyword)

class remark_002(_base):

  def __init__(self, pdb_str):
    _base.__init__(self, pdb_str)
    self.text = self.pdb_str[11:70]
    flds = self.pdb_str[11:38].split()
    self.resolution = None
    if (    len(flds) == 3
        and flds[0].upper() == "RESOLUTION."
        and flds[2].upper() == "ANGSTROMS."):
      try: self.resolution = float(flds[1])
      except ValueError: pass

class cryst1(_base):

  def __init__(self, pdb_str):
    #  7 - 15       Real(9.3)      a             a (Angstroms).
    # 16 - 24       Real(9.3)      b             b (Angstroms).
    # 25 - 33       Real(9.3)      c             c (Angstroms).
    # 34 - 40       Real(7.2)      alpha         alpha (degrees).
    # 41 - 47       Real(7.2)      beta          beta (degrees).
    # 48 - 54       Real(7.2)      gamma         gamma (degrees).
    # 56 - 66       LString        sGroup        Space group.
    # 67 - 70       Integer        z             Z value.
    _base.__init__(self, pdb_str)
    ps = self.pdb_str
    self.ucparams = [ps[ 6:15], ps[15:24], ps[24:33],
                     ps[33:40], ps[40:47], ps[47:54]]
    self.sgroup = ps[55:66].strip()
    z = ps[66:70]
    if (len(" ".join(self.ucparams).strip()) == 0):
      self.ucparams = None
    else:
      try: self.ucparams = [float(u) for u in self.ucparams]
      except ValueError:
        self.raise_FormatError("Corrupt unit cell parameters")
    if (len(self.sgroup) == 0): self.sgroup = None
    if (z.strip()):
      try: self.z = int(z)
      except ValueError: self.raise_FormatError("Corrupt Z value")
    else: self.z = None

class scalen(_base):

  def __init__(self, pdb_str):
    #  1 -  6       Record name    "SCALEn"       n=1, 2, or 3
    # 11 - 20       Real(10.6)     s[n][1]        Sn1
    # 21 - 30       Real(10.6)     s[n][2]        Sn2
    # 31 - 40       Real(10.6)     s[n][3]        Sn3
    # 46 - 55       Real(10.5)     u[n]           Un
    _base.__init__(self, pdb_str)
    self.n = int(self.pdb_str[5])
    values = []
    for i in [10,20,30,45]:
      fld = self.pdb_str[i:i+10]
      if (len(fld.strip()) == 0):
        value = 0
      else:
        try: value = float(fld)
        except ValueError:
          raise self.raise_FormatError("Invalid floating-point value")
      values.append(value)
    self.sn1, self.sn2, self.sn3, self.un = values

class conect(_base):

  def __init__(self, pdb_str):
    #  7 - 11  Integer   serial          Atom serial number
    # 12 - 16  Integer   serial          Serial number of bonded atom
    # 17 - 21  Integer   serial          Serial number of bonded atom
    # 22 - 26  Integer   serial          Serial number of bonded atom
    # 27 - 31  Integer   serial          Serial number of bonded atom
    # 32 - 36  Integer   serial          Serial number of hydrogen bonded atom
    # 37 - 41  Integer   serial          Serial number of hydrogen bonded atom
    # 42 - 46  Integer   serial          Serial number of salt bridged atom
    # 47 - 51  Integer   serial          Serial number of hydrogen bonded atom
    # 52 - 56  Integer   serial          Serial number of hydrogen bonded atom
    # 57 - 61  Integer   serial          Serial number of salt bridged atom
    _base.__init__(self, pdb_str)
    self.serial = self.pdb_str[6:11]
    ba = []
    hba = []
    sba = []
    for i,l in [(11, ba), (16, ba), (21, ba), (26, ba),
                (31, hba), (36, hba), (41, sba),
                (46, hba), (51, hba), (56, sba)]:
      fld = self.pdb_str[i:i+5]
      if (fld != "     "): l.append(fld)
    self.serial_numbers_bonded_atoms = ba
    self.serial_numbers_hydrogen_bonded_atoms = hba
    self.serial_numbers_salt_bridged_atoms = sba

class link(_base):

  def __init__(self, pdb_str):
    # 13 - 16      Atom            name1       Atom name.
    # 17           Character       altLoc1     Alternate location indicator.
    # 18 - 20      Residue name    resName1    Residue name.
    # 21 - 22                      chainID1    Chain identifier.
    # 23 - 26                      resSeq1     Residue sequence number.
    # 27           AChar           iCode1      Insertion code.
    # 31 - 40      distance (REFMAC extension: F10.5)
    # 43 - 46      Atom            name2       Atom name.
    # 47           Character       altLoc2     Alternate location indicator.
    # 48 - 50      Residue name    resName2    Residue name.
    # 51 - 52                      chainID2    Chain identifier.
    # 53 - 56                      resSeq2     Residue sequence number.
    # 57           AChar           iCode2      Insertion code.
    # 60 - 65      SymOP           sym1        Symmetry operator for 1st atom.
    # 67 - 72      SymOP           sym2        Symmetry operator for 2nd atom.
    # 73 - 80      margin (REFMAC extension: _chem_link.id)
    _base.__init__(self, pdb_str)
    self.name1 = self.pdb_str[12:16]
    self.altloc1 = self.pdb_str[16]
    self.resname1 = self.pdb_str[17:20]
    self.chain_id1 = chain_id_strip(self.pdb_str[20:22])
    self.resseq1 = self.pdb_str[22:26]
    self.icode1 = self.pdb_str[26]
    try: self.distance = float(self.pdb_str[30:40])
    except ValueError: self.distance = None
    self.name2 = self.pdb_str[42:46]
    self.altloc2 = self.pdb_str[46]
    self.resname2 = self.pdb_str[47:50]
    self.chain_id2 = chain_id_strip(self.pdb_str[50:52])
    self.resseq2 = self.pdb_str[52:56]
    self.icode2 = self.pdb_str[56]
    self.sym1 = self.pdb_str[59:65]
    self.sym2 = self.pdb_str[66:72]
    self.margin = self.pdb_str[72:80]

class sltbrg(link): pass

class ssbond(_base):

  def __init__(self, pdb_str):
    #  8 - 10    Integer         serNum      Serial number.
    # 12 - 14    LString(3)      "CYS"       Residue name.
    # 15 - 16                    chainID1    Chain identifier.
    # 18 - 21                    seqNum1     Residue sequence number.
    # 22         AChar           icode1      Insertion code.
    # 26 - 28    LString(3)      "CYS"       Residue name.
    # 29 - 30                    chainID2    Chain identifier.
    # 32 - 35                    seqNum2     Residue sequence number.
    # 36         AChar           icode2      Insertion code.
    # 60 - 65    SymOP           sym1        Symmetry operator for 1st residue.
    # 67 - 72    SymOP           sym2        Symmetry operator for 2nd residue.
    # 73 - 80    margin (REFMAC extension: _chem_link.id)
    _base.__init__(self, pdb_str)
    self.sernum = self.pdb_str[7:10]
    self.resname1 = self.pdb_str[11:14]
    self.chain_id1 = chain_id_strip(self.pdb_str[14:16])
    self.resseq1 = self.pdb_str[17:21]
    self.icode1 = self.pdb_str[21]
    self.resname2 = self.pdb_str[25:28]
    self.chain_id2 = chain_id_strip(self.pdb_str[28:30])
    self.resseq2 = self.pdb_str[31:35]
    self.icode2 = self.pdb_str[35]
    self.sym1 = self.pdb_str[59:65]
    self.sym2 = self.pdb_str[66:72]
    self.margin = self.pdb_str[72:80]


 *******************************************************************************


 *******************************************************************************
iotbx/pdb/remark_290_interpretation.py
"""Extract information on symmetry from REMARK 290 records"""
from __future__ import absolute_import, division, print_function
from cctbx import sgtbx

example = """\
REMARK 290
REMARK 290
REMARK 290 CRYSTALLOGRAPHIC SYMMETRY
REMARK 290 SYMMETRY OPERATORS FOR SPACE GROUP: P 21 21 21
REMARK 290
REMARK 290      SYMOP   SYMMETRY
REMARK 290     NNNMMM   OPERATOR
REMARK 290       1555   X,Y,Z
REMARK 290       2555   1/2-X,-Y,1/2+Z
REMARK 290       3555   -X,1/2+Y,1/2-Z
REMARK 290       4555   1/2+X,1/2-Y,-Z
REMARK 290
REMARK 290     WHERE NNN -> OPERATOR NUMBER
REMARK 290           MMM -> TRANSLATION VECTOR
REMARK 290
REMARK 290 CRYSTALLOGRAPHIC SYMMETRY TRANSFORMATIONS
REMARK 290 THE FOLLOWING TRANSFORMATIONS OPERATE ON THE ATOM/HETATM
REMARK 290 RECORDS IN THIS ENTRY TO PRODUCE CRYSTALLOGRAPHICALLY
REMARK 290 RELATED MOLECULES.
REMARK 290   SMTRY1   1  1.000000  0.000000  0.000000        0.00000
REMARK 290   SMTRY2   1  0.000000  1.000000  0.000000        0.00000
REMARK 290   SMTRY3   1  0.000000  0.000000  1.000000        0.00000
REMARK 290   SMTRY1   2 -1.000000  0.000000  0.000000       36.30027
REMARK 290   SMTRY2   2  0.000000 -1.000000  0.000000        0.00000
REMARK 290   SMTRY3   2  0.000000  0.000000  1.000000       59.50256
REMARK 290   SMTRY1   3 -1.000000  0.000000  0.000000        0.00000
REMARK 290   SMTRY2   3  0.000000  1.000000  0.000000       46.45545
REMARK 290   SMTRY3   3  0.000000  0.000000 -1.000000       59.50256
REMARK 290   SMTRY1   4  1.000000  0.000000  0.000000       36.30027
REMARK 290   SMTRY2   4  0.000000 -1.000000  0.000000       46.45545
REMARK 290   SMTRY3   4  0.000000  0.000000 -1.000000        0.00000
REMARK 290
REMARK 290 REMARK: NULL
"""

def _nnnmmm_operator(record, expected_nnn):
  if (not record.startswith("REMARK 290     ")): return None
  flds = record.split()
  if (len(flds) != 4): return None
  nnnmmm = flds[2]
  if (len(nnnmmm) < 4): return None
  nnn,mmm = nnnmmm[:-3], nnnmmm[-3:]
  if (mmm != "555"): return None
  try: nnn = int(nnn)
  except ValueError: return None
  if (nnn != expected_nnn): return None
  try: return sgtbx.rt_mx(flds[3])
  except RuntimeError: return None

def extract_symmetry_operators(remark_290_records):
  symmetry_operators = []
  status = 0
  for record in remark_290_records:
    if (record.startswith("REMARK 290     ")):
      symmetry_operator = _nnnmmm_operator(
        record=record,
        expected_nnn=len(symmetry_operators)+1)
      if (symmetry_operator is not None):
        if (status == 2): return None
        symmetry_operators.append(symmetry_operator)
        status = 1
      elif (status == 1):
        status == 2
  if (len(symmetry_operators) == 0): return None
  return symmetry_operators

def get_link_symmetry_operator(symmetry_operators, link_sym):
  if (symmetry_operators is None): return None
  link_sym = link_sym.strip()
  if (len(link_sym) < 4): return None
  if (link_sym[-4] == "_"):
    link_sym = link_sym[:-4] + link_sym[-3:]
    if (len(link_sym) < 4): return None
  nnn = link_sym[:-3]
  try: nnn = int(nnn)
  except ValueError: return None
  if (nnn < 1 or nnn > len(symmetry_operators)): return None
  shifts = []
  for m in link_sym[-3:]:
    try: s = int(m)
    except ValueError: return None
    assert 0 <= s <= 9
    shifts.append(s-5)
  return symmetry_operators[nnn-1] + shifts


 *******************************************************************************


 *******************************************************************************
iotbx/pdb/remark_2_interpretation.py
"""Extract information on resolution from REMARK 2 records"""
from __future__ import absolute_import, division, print_function
def get_resolution(st):
  result = None
  q1 = (st.count("REMARK   2 ")==1 and st.count("RESOLUTION")>0 and
        st.count("ANGSTROM")>0)
  ch = st.split()
  if(q1):
    try:
      result = float(ch[3])
      assert result < 100.0 and result > 0.01
    except KeyboardInterrupt: raise
    except Exception:
      pass
  return result

def extract_resolution(remark_2_records):
  res_counter = 0
  resolutions = []
  for record in remark_2_records:
    result = get_resolution(record)
    if(result is not None):
      resolutions.append(result)
      res_counter +=1
  if(res_counter == 1):
    return resolutions
  else:
    return None


 *******************************************************************************


 *******************************************************************************
iotbx/pdb/remark_3_interpretation.py
"""Extract TLS from REMARK 3 records"""
from __future__ import absolute_import, division, print_function
import os
from libtbx import group_args

example_of_tls_parameters_in_remark_3 = """\
REMARK   3
REMARK   3  TLS DETAILS
REMARK   3   NUMBER OF TLS GROUPS  :    2
REMARK   3
REMARK   3   TLS GROUP :     1
REMARK   3    NUMBER OF COMPONENTS GROUP :    1
REMARK   3    COMPONENTS        C SSSEQI   TO  C SSSEQI
REMARK   3    RESIDUE RANGE :   A     1        A   348
REMARK   3    ORIGIN FOR THE GROUP (A):  40.4920  20.4900  77.3450
REMARK   3    T TENSOR
REMARK   3      T11:   0.0445 T22:   0.0094
REMARK   3      T33:   0.0759 T12:   0.0073
REMARK   3      T13:  -0.0019 T23:  -0.0026
REMARK   3    L TENSOR
REMARK   3      L11:   0.4084 L22:   0.4087
REMARK   3      L33:   0.4695 L12:   0.0456
REMARK   3      L13:   0.0685 L23:  -0.0388
REMARK   3    S TENSOR
REMARK   3      S11:  -0.0063 S12:   0.0233 S13:   0.0090
REMARK   3      S21:   0.0417 S22:  -0.0071 S23:   0.0132
REMARK   3      S31:  -0.0008 S32:  -0.0002 S33:   0.0134
REMARK   3
REMARK   3   TLS GROUP :     2
REMARK   3    NUMBER OF COMPONENTS GROUP :    1
REMARK   3    COMPONENTS        C SSSEQI   TO  C SSSEQI
REMARK   3    RESIDUE RANGE :   B XYZW1-       B XY348+
REMARK   3    ORIGIN FOR THE GROUP (A):  16.4920  -1.1810  59.3130
REMARK   3    T TENSOR
REMARK   3      T11:   0.0412 T22:   0.0118
REMARK   3      T33:   0.1002 T12:  -0.0082
REMARK   3      T13:  -0.0117 T23:  -0.0290
REMARK   3    L TENSOR
REMARK   3      L11:   0.3189 L22:   0.3849
REMARK   3      L33:   0.8975 L12:   0.0578
REMARK   3      L13:  -0.0398 L23:   0.1013
REMARK   3    S TENSOR
REMARK   3      S11:   0.0415 S12:   0.0183 S13:  -0.0365
REMARK   3      S21:   0.0087 S22:  -0.0145 S23:   0.0507
REMARK   3      S31:   0.1029 S32:  -0.0744 S33:  -0.0269
REMARK   3
"""

class tls(object):
  def __init__(self, t, l, s, origin, selection_string = None):
    self.t = t
    self.l = l
    self.s = s
    self.origin = origin
    self.selection_string = selection_string #XXX do this smarter

def has_tls(remark_3_records):
  result = False
  n_t = 0
  n_l = 0
  n_s = 0
  for line in remark_3_records:
    if(line.count("T11") or line.count("T 11")): n_t += 1
    if(line.count("T22") or line.count("T 22")): n_t += 1
    if(line.count("T33") or line.count("T 33")): n_t += 1
    if(line.count("T12") or line.count("T 12")): n_t += 1
    if(line.count("T13") or line.count("T 13")): n_t += 1
    if(line.count("T23") or line.count("T 23")): n_t += 1
    #
    if(line.count("L11") or line.count("L 11")): n_l += 1
    if(line.count("L22") or line.count("L 22")): n_l += 1
    if(line.count("L33") or line.count("L 33")): n_l += 1
    if(line.count("L12") or line.count("L 12")): n_l += 1
    if(line.count("L13") or line.count("L 13")): n_l += 1
    if(line.count("L23") or line.count("L 23")): n_l += 1
    #
    if(line.count("S11") or line.count("S 11")): n_s += 1
    if(line.count("S21") or line.count("S 21")): n_s += 1
    if(line.count("S31") or line.count("S 31")): n_s += 1
    if(line.count("S12") or line.count("S 12")): n_s += 1
    if(line.count("S22") or line.count("S 22")): n_s += 1
    if(line.count("S32") or line.count("S 32")): n_s += 1
    if(line.count("S13") or line.count("S 13")): n_s += 1
    if(line.count("S23") or line.count("S 23")): n_s += 1
    if(line.count("S33") or line.count("S 33")): n_s += 1
  if(n_t>0 and n_t%6==0 and n_l>0 and n_l%6==0 and n_t==n_l and n_s%9==0):
    return True
  return result

def prepocess_line(line):
  l0 = line.split()
  new_elements = []
  for l_ in l0:
    if(l_.isalpha() or l_.isdigit()): new_elements.append(l_)
    else:
      try:
        val = float(l_)
        new_elements.append(l_)
      except Exception:
        tmp = ""
        for i, c in enumerate(l_):
          if(i == 0): tmp+=c
          elif(c in ["+","-"]):
            if(not l_[i-1].isalpha()):
              new_elements.append(tmp)
              tmp = c
            else: tmp+=c
          else: tmp+=c
        new_elements.append(tmp)
  return " ".join(new_elements)

class extract_tls_parameters(object):

  def __init__(self, remark_3_records, chain_ids, pdb_hierarchy, file_name = ""):
    # T = (T11, T22, T33, T12, T13, T23)
    # L = (L11, L22, L33, L12, L13, L23)
    # S = (S11, S12, S13, S21, S22, S23, S31, S32, S33)
    self.pdb_hierarchy = pdb_hierarchy
    self.remark_3_records = remark_3_records
    self.chain_ids = chain_ids
    self.file_name = os.path.basename(file_name)
    self.tls_present = has_tls(remark_3_records = remark_3_records)
    self.tls_params = []
    self.error_string = None
    if(self.tls_present):
      self.extract()
    if(self.error_string is not None):
      self.tls_params = []

  def format_err(self, msg, rec=""):
    x = self.file_name.strip()+" : "
    y = " : "+rec.strip()
    if(len(y.strip())==1): y = ""
    if(len(x.strip())==1): x = ""
    self.error_string = x+msg.strip()+y

  def extract(self):
    T = []
    L = []
    S = []
    origin = []
    residue_range = []
    group_counter = 0
    record_start = False
    record_end = False
    one_tls_group_records = []
    all_tls_group_records = []
    for record in self.remark_3_records:
      assert record[0:10] == "REMARK   3"
      if(record.startswith("REMARK   3   TLS GROUP :")):
        group_number = None
        try:
          fields = record.split()
          if (len(fields) >= 6):
            group_number = int(fields[5])
          else :
            fields = record.split(":")
            if (len(fields) >= 2):
              group_number = int(fields[1].strip())
            else :
              raise ValueError()
        except ValueError:
          self.format_err(msg="Cannot extract TLS group number:", rec=record)
          return []
        record_start = True
      if(record.startswith("REMARK   3") and record.count("S31:")==1):
        record_end = True
        record_start = False
        one_tls_group_records.append(record)
        all_tls_group_records.append(one_tls_group_records)
        one_tls_group_records = []
      else:
        record_end = False
      if(record_start and not record_end):
        one_tls_group_records.append(record)
    def is_buster_selection(ss):
      result = []
      ss = ss.split()
      if(len(ss)==4):
        c0, r0, c1, r1 = ss[0].strip(),ss[1].strip(),ss[2].strip(),ss[3].strip()
        if(len(c0)<=2 and len(c1)<=2):
          try: r0 = int(r0)
          except Exception: ValueError
          try: r1 = int(r1)
          except Exception: ValueError
          if(type(r0)==type(1) and type(r1)==type(1)):
            result.append([c0,str(r0), c1,str(r1)])
      return result
    for one in all_tls_group_records:
      n_components = 0
      r_range = []
      origin = None
      T = None
      T11,T22,T33,T12,T13,T23 = [None]*6
      L11,L22,L33,L12,L13,L23 = [None]*6
      S11,S12,S13,S21,S22,S23,S31,S32,S33 = [None]*9
      sel_str = None
      for i_seq, rec in enumerate(one):
        if(rec.startswith("REMARK   3    NUMBER OF COMPONENTS GROUP :")):
          n_components = None
          try: n_components = int(rec.split()[7])
          except ValueError:
            self.format_err(msg="Cannot extract number of TLS components.", rec=rec)
            return []
        if(rec.startswith("REMARK   3    RESIDUE RANGE :")):
          if(len(rec.split())==9):
            rec_s = rec.split()
            ch1,res1,ch2,res2 = rec_s[5],rec_s[6],rec_s[7],rec_s[8]
          else:
            ch1,res1,ch2,res2 = rec[31:33], rec[34:40], rec[47:48], rec[49:55]
          r_range.append([ch1,res1,ch2,res2])
        # PHENIX or Buster selection
        if(rec.startswith("REMARK   3    SELECTION:")):
          sel_str = rec[rec.index(":")+1:]
          r_range = is_buster_selection(sel_str)
          if(len(r_range)>0):
            sel_str=None
          else:
            r_range = []
            if(sel_str.strip().upper() in ["NONE","NULL"]):
              self.format_err(msg="Bad TLS selection string1.", rec = rec)
              return []
            i = i_seq+1
            while ( one[i].startswith("REMARK   3             :") or
                    one[i].startswith("REMARK   3              ") ):
              if(one[i].count("ORIGIN")): break
              sel_str += " "+one[i][24:]
              i += 1
              if(sel_str.strip().upper() in ["NONE","NULL"]):
                self.format_err(msg="Bad TLS selection string2.", rec = rec)
                return []
            sel_str = " ".join(sel_str.split())
            ##
            sel_str_spl = sel_str.split()
            new_str = ""
            for ie, e in enumerate(sel_str_spl):
              if(ie==0): new_str = e
              else:
                if(new_str[len(new_str)-1]==":"):
                  if(e[0].isdigit()):
                    new_str += e
                  else:
                    new_str = new_str + " " + e
                elif(new_str[len(new_str)-1].isdigit()):
                  if(e[0]==":"):
                    new_str += e
                  else:
                    new_str = new_str + " " + e
                else:
                  new_str = new_str + " " + e
            sel_str = new_str
          ##
        #
        if(rec.startswith("REMARK   3    ORIGIN FOR THE GROUP (A):")):
          rec = prepocess_line(rec)
          try: x = float(rec.split()[7])
          except Exception:
            self.format_err(msg="Cannot extract origin.", rec=rec)
            return []
          try: y = float(rec.split()[8])
          except Exception:
            self.format_err(msg="Cannot extract origin.", rec=rec)
            return []
          try: z = float(rec.split()[9])
          except Exception:
            self.format_err(msg="Cannot extract origin.", rec=rec)
            return []
          origin = [x,y,z]
        if(rec.startswith("REMARK   3") and rec.count("T11:")==1):
          assert [T11, T22, T33, T12, T13, T23].count(None) == 6
          try: T11 = float(rec.split()[3])
          except Exception:
            self.format_err(msg="Cannot extract T.", rec=rec)
            return []
          try: T22 = float(rec.split()[5])
          except Exception:
            self.format_err(msg="Cannot extract T.", rec=rec)
            return []
        if(rec.startswith("REMARK   3") and rec.count("T33:")==1):
          assert [T11, T22, T33, T12, T13, T23].count(None) == 4
          try: T33 = float(rec.split()[3])
          except Exception:
            self.format_err(msg="Cannot extract T.", rec=rec)
            return []
          try: T12 = float(rec.split()[5])
          except Exception:
            self.format_err(msg="Cannot extract T.", rec=rec)
            return []
        if(rec.startswith("REMARK   3") and rec.count("T13:")==1):
          assert [T11, T22, T33, T12, T13, T23].count(None) == 2
          try: T13 = float(rec.split()[3])
          except Exception:
            self.format_err(msg="Cannot extract T.", rec=rec)
            return []
          try: T23 = float(rec.split()[5])
          except Exception:
            self.format_err(msg="Cannot extract T.", rec=rec)
            return []
          T=[T11, T22, T33, T12, T13, T23]
        if(rec.startswith("REMARK   3") and rec.count("L11:")==1):
          assert [L11, L22, L33, L12, L13, L23].count(None) == 6
          try: L11 = float(rec.split()[3])
          except Exception:
            try: L11 = float(rec[20:30])
            except Exception:
              self.format_err(msg="Cannot extract L.", rec=rec)
              return []
          try: L22 = float(rec.split()[5])
          except Exception:
            try: L22 = float(rec[34:44])
            except Exception:
              self.format_err(msg="Cannot extract L.", rec=rec)
              return []
        if(rec.startswith("REMARK   3") and rec.count("L33:")==1):
          assert [L11, L22, L33, L12, L13, L23].count(None) == 4
          try: L33 = float(rec.split()[3])
          except Exception:
            try: L33 = float(rec[20:30])
            except Exception:
              self.format_err(msg="Cannot extract L.", rec=rec)
              return []
          try: L12 = float(rec.split()[5])
          except Exception:
            try: L12 = float(rec[34:44])
            except Exception:
              self.format_err(msg="Cannot extract L.", rec=rec)
              return []
        if(rec.startswith("REMARK   3") and rec.count("L13:")==1):
          assert [L11, L22, L33, L12, L13, L23].count(None) == 2
          try: L13 = float(rec.split()[3])
          except Exception:
            try: L13 = float(rec[20:30])
            except Exception:
              self.format_err(msg="Cannot extract L.", rec=rec)
              return []
          try: L23 = float(rec.split()[5])
          except Exception:
            try: L23 = float(rec[34:44])
            except Exception:
              self.format_err(msg="Cannot extract L.", rec=rec)
              return []
          L=[L11, L22, L33, L12, L13, L23]
        if(rec.startswith("REMARK   3") and rec.count("S11:")==1):
          assert [S11, S12, S13, S21, S22, S23, S31, S32, S33].count(None) == 9
          try: S11 = float(rec.split()[3])
          except Exception:
            self.format_err(msg="Cannot extract S.", rec=rec)
            return []
          try: S12 = float(rec.split()[5])
          except Exception:
            self.format_err(msg="Cannot extract S.", rec=rec)
            return []
          try: S13 = float(rec.split()[7])
          except Exception:
            self.format_err(msg="Cannot extract S.", rec=rec)
            return []
        if(rec.startswith("REMARK   3") and rec.count("S21:")==1):
          assert [S11, S12, S13, S21, S22, S23, S31, S32, S33].count(None) == 6
          try: S21 = float(rec.split()[3])
          except Exception:
            self.format_err(msg="Cannot extract S.", rec=rec)
            return []
          try: S22 = float(rec.split()[5])
          except Exception:
            self.format_err(msg="Cannot extract S.", rec=rec)
            return []
          try: S23 = float(rec.split()[7])
          except Exception:
            self.format_err(msg="Cannot extract S.", rec=rec)
            return []
        if(rec.startswith("REMARK   3") and rec.count("S31:")==1):
          assert [S11, S12, S13, S21, S22, S23, S31, S32, S33].count(None) == 3
          try: S31 = float(rec.split()[3])
          except Exception:
            self.format_err(msg="Cannot extract S.", rec=rec)
            return []
          try: S32 = float(rec.split()[5])
          except Exception:
            self.format_err(msg="Cannot extract S.", rec=rec)
            return []
          try: S33 = float(rec.split()[7])
          except Exception:
            try:
              if(rec.split()[7].count("NULL")):
                S33 = - (S11 + S22)
            except Exception:
              self.format_err(msg="Cannot extract S.", rec=rec)
              return []
          S=[S11, S12, S13, S21, S22, S23, S31, S32, S33]
      if(len(r_range) > 0):
        assert sel_str is None
        for rr in r_range:
          ch1 = rr[0].strip()
          ch2 = rr[2].strip()
          res1 = rr[1].strip()
          res2 = rr[3].strip()
          if(len(res1)>0 and len(res2)>0):
            try: res1_ = int(res1)
            except ValueError:
              self.format_err(msg="Bad TLS selection string3.", rec = "".join(rr))
              return []
            try: res2_ = int(res2)
            except ValueError:
              self.format_err(msg="Bad TLS selection string4.", rec = "".join(rr))
              return []
            if(res1_ > res2_ and ch1 == ch2):
              self.format_err(msg="Bad TLS selection: start index > end index.")
              return []
          tmp = refmac_range_to_phenix_string_selection(
            pdb_hierarchy = self.pdb_hierarchy, chain_start = ch1,
            resseq_start = res1, chain_end = ch2, resseq_end = res2)
          if(sel_str is None): sel_str = tmp
          else: sel_str = sel_str +" or %s"%tmp
      if(sel_str is not None):
        if(sel_str.count("(") != sel_str.count(")")):
          self.format_err(msg="Bad TLS selection: missing ) or (", rec = sel_str)
          return []
        if(sel_str.upper() in ["NONE","NULL"]):
          sel_str = None
          self.format_err(msg="Bad TLS selection string5.", rec = sel_str)
          return []
      if(sel_str is not None or
         T.count(None) > 0 or
         L.count(None) > 0 or
         S.count(None) > 0):
        # Compatibility with previous versions of PHENIX: replace "-" with ":"
        new_c = ""
        for i,c in enumerate(sel_str):
          try: cl = sel_str[i-1]
          except Exception: cl = c
          if(c=="-" and cl.isdigit()): c = ":"
          new_c += c
        sel_str = new_c
        #
        self.tls_params.append(tls(
          t=T,l=L,s=S,origin=origin,selection_string=sel_str))
    if(self.tls_present and len(self.tls_params)==0):
      self.format_err(msg="TLS present but cannot be extracted.")
      return []
    #
    if(len(self.tls_params)>0):
      chain_ids_new_u = [c.upper() for c in self.chain_ids]
      chain_ids_new_l = [c.lower() for c in self.chain_ids]
      if(chain_ids_new_u!=self.chain_ids and chain_ids_new_l!=self.chain_ids):
        self.format_err(msg="Mixed chain ids detected.")
        return []
    #
    if(len(all_tls_group_records) != len(self.tls_params)):
      self.format_err(msg="TLS present but cannot be extracted.")
      return []
    #
    for t in self.tls_params:
      if(not(len(t.t) == len(t.l))):
        self.format_err(msg="TLS present but cannot be extracted.")
        return []
      if(len(t.t) == len(t.l) and len(t.l)>0 and
         (len(t.s)!=9 or len(t.origin)!=3)):
        self.format_err(msg="TLS present but cannot be extracted.")
        return []
    #
    return self.tls_params


def get_program(st):
  st = st.upper()
  program = None
  programs = ["SHELX", "CNS", "PROLSQ", "FROG", "REFMAC", "XPLOR", "X-PLOR",
              "NUCLSQ", "MOPRO", "MOLLY", "PROFFT", "TNT", "CORELS", "GROMOS"
              "XTALVIEW", "RESTRAIN", "GPRLSA", "ARP", "CNX", "EREF",
              "NMREF", "GSAS", "BUSTER","SOLVE", "CCP4", "NUCLIN", "MAIN",
              "PHENIX", "PHENIX.REFINE", "FFX","NCNS"]
  diamond = "DIAMOND"
  if(st[0:6] == "REMARK"):
    st_split = st.split()
    ch_1 = st.count("REMARK   3") == 1
    ch_2 = st.count("PROGRAM") == 1
    ch_3 = st.count("REFINEMENT") == 1
    if(ch_1 and ch_2):
      for item in st_split:
        for potential_program in programs:
          if(potential_program.count(",")):
            potential_program = potential_program[:-1]
          if(item.count(potential_program)):
            program = potential_program
    if(program is None and ch_1):
      refinement_or_program = st.count("REFINEMENT")>=1 or \
        st.count("PROGRAM")>=1 or \
        st.count("PROCEDURE")>=1
      j_and_l = st.count("JACK")==1 or st.count("LEVITT")
      if(refinement_or_program and j_and_l): program = "JACK_AND_LEVITT"
      if((ch_2 or ch_3) and st.count(diamond) != 0): program = diamond
      k_and_h = st.count("KONNERT")==1 or st.count("HENDRICKSON")==1
      k_and_h_1 = st.count("KONNERT AND HENDRICKSON")==1
      if((refinement_or_program and k_and_h) or k_and_h_1):
        program = "KONNERT_AND_HENDRICKSON"
      sa = st.count("SIMULATED")==1 and st.count("ANNEALING")
      if(refinement_or_program and sa): program = "X-PLOR"
      if(refinement_or_program and st.count("CORELS")==1): program = "CORELS"
      jones_and_liljas = "JONES_AND_A.LILJAS"
      if(st.count(jones_and_liljas) or st.count("T.A.JONES, L.LILJAS")):
        program = jones_and_liljas
  if(program == "XPLOR"): program = "X-PLOR"
  return program

def refmac_range_to_phenix_string_selection(pdb_hierarchy, chain_start,
                                            resseq_start, chain_end, resseq_end):
  chain_start  =  chain_start.strip()
  resseq_start =  resseq_start.strip()
  chain_end    =  chain_end.strip()
  resseq_end   =  resseq_end.strip()
  sel_str = None
  if(chain_start == chain_end):
    if(chain_start != ""):
      if([resseq_start,resseq_end].count("")==0):
        sel_str = "(chain %s and resseq %s:%s)"%(chain_start,resseq_start,resseq_end)
      else:
        sel_str = "(chain %s)"%(chain_start)
    else:
      if([resseq_start,resseq_end].count("")==0):
        sel_str = "(resseq %s:%s)"%(resseq_start,resseq_end)
  else:
    sel_str1 = None
    if(chain_start != ""):
      if(resseq_start != ""):
        sel_str1 = "(chain %s and resseq %s:)"%(chain_start,resseq_start)
      else:
        sel_str1 = "(chain %s)"%(chain_start)
    else:
      if(resseq_start != ""):
        sel_str1 = "(resseq %s:)"%(resseq_start)
    sel_str2 = None
    if(chain_end != ""):
      if(resseq_end != ""):
        sel_str2 = "(chain %s and resseq :%s)"%(chain_end,resseq_end)
      else:
        sel_str2 = "(chain %s)"%(chain_end)
    else:
      if(resseq_end != ""):
        sel_str2 = "(resseq :%s)"%(resseq_end)
    if([sel_str1,sel_str2].count(None)==0):
      sel_str = "%s or %s"%(sel_str1, sel_str2)
    elif(sel_str1 is not None): sel_str = sel_str1
    elif(sel_str2 is not None): sel_str = sel_str2
    if(sel_str is not None):
      models = pdb_hierarchy.models()
      if(len(models)>1): return None # XXX one model only
      start_collecting = False
      chain_ids = []
      for chain in models[0].chains():
        chain_id = chain.id.strip()
        if(chain_id == chain_end): break
        if(chain_id == chain_start):
          start_collecting = True
          continue
        if(start_collecting): chain_ids.append(chain_id)
      if(len(chain_ids)>0):
        for chain_id in chain_ids:
          sel_str = sel_str+" or chain %s"%chain_id
  if(sel_str is not None):
    return "%s"%sel_str
  else: return None


def format_name(program_names):
  try:
    new = program_names[0]
    first = program_names[0]
    for name in program_names:
      if(name != first):
        new +="/"+name
        first = name
  except Exception: return program_names
  return new


def extract_program_name(remark_3_records):
  p_counter = 0
  program_names = []
  for record in remark_3_records:
    result = get_program(record)
    if(result is not None):
      program_names.append(result)
      p_counter +=1
  if(p_counter != 0):
    return format_name(program_names)
  else:
    return None

def extract_f_model_core_constants(remark_3_records):
  # XXX phenix.refine specific
  k_sol            = None
  b_sol            = None
  b_cart           = [None]*6
  twin_fraction    = None
  twin_law         = None
  r_solv           = None
  r_shrink         = None
  grid_step_factor = None
  b11,b22,b33,b12,b13,b23 = [None,None,None,None,None,None]
  r_work           = None
  r_free           = None
  for l in remark_3_records:
    l = l.strip()
    ls= l.split()
    try:
      if(l.count("REMARK   3   K_SOL              :")): k_sol = float(ls[4])
      if(l.count("REMARK   3   B_SOL              :")): b_sol = float(ls[4])
      if(l.count("REMARK   3    B11")): b11 = float(ls[len(ls)-1])
      if(l.count("REMARK   3    B22")): b22 = float(ls[len(ls)-1])
      if(l.count("REMARK   3    B33")): b33 = float(ls[len(ls)-1])
      if(l.count("REMARK   3    B12")): b12 = float(ls[len(ls)-1])
      if(l.count("REMARK   3    B13")): b13 = float(ls[len(ls)-1])
      if(l.count("REMARK   3    B23")): b23 = float(ls[len(ls)-1])
      if(l.count("REMARK   3   FRACTION:")): twin_fraction = float(ls[3])
      if(l.count("REMARK   3   OPERATOR:")): twin_law      = ls[3]
      if(l.count("REMARK   3   SOLVENT RADIUS     :")): r_solv = float(ls[5])
      if(l.count("REMARK   3   SHRINKAGE RADIUS   :")): r_shrink = float(ls[5])
      if(l.count("REMARK   3   GRID STEP FACTOR   :")): grid_step_factor = float(ls[6])
      if(l.count("REMARK   3   R VALUE            (WORKING SET) ")): r_work = float(ls[7])
      if(l.count("REMARK   3   FREE R VALUE                     ")): r_free = float(ls[6])
    except Exception: pass
  if([b11,b22,b33,b12,b13,b23].count(None)==0): b_cart=[b11,b22,b33,b12,b13,b23]
  return group_args(
    k_sol            = k_sol,
    b_sol            = b_sol,
    b_cart           = b_cart,
    twin_fraction    = twin_fraction,
    twin_law         = twin_law,
    r_solv           = r_solv,
    r_shrink         = r_shrink,
    grid_step_factor = grid_step_factor,
    r_work           = r_work,
    r_free           = r_free)


 *******************************************************************************


 *******************************************************************************
iotbx/pdb/remediation/__init__.py


 *******************************************************************************


 *******************************************************************************
iotbx/pdb/remediation/remediator.py
from __future__ import absolute_import, division, print_function
#!/usr/bin/python
# remediator.py - version 1.61.110622 6/22/11
# Copyright 2007-2011, Jeffrey J. Headd and Robert Immormino

# revision 1.56 - JJH 070808 - added support for DU DNA base
#               - JJH 070808 - added compiled RE object for HN2 RES special case
#       - JJH 070815 - updated name of hash dictionary file
#       - JJH 070823 - added support for CNS Xplor and Coot RNA names
#       - JJH 070908 - added REMARK   4 comment addition
#               - JJH 070913 - added support for left-justified RNA/DNA old names
#       - JJH 070913 - added support for all left-justified residue names
#       - JJH 080328 - fixed REMARK   4 comment addition to work w/ PHENIX header info
# revision 1.57 - vbc 080620 - lines get output with at least 80 columns
#                              master_hash.txt not found bug fixed, must be in same directory
#                              as this script
# revision 1.58 - JJH 080630 - fixed --old functionality for DNA, this did not work previously
#                              and would sometimes incorrectly rename protein atoms
#               - JJH 080827 - fixed a bug in this version only by restricting the number of
#                              substitutions to 1 per line
# revision 1.59 - JJH 081015 - added a --dict flag, which allows user to input a custom
#                              dictionary for detecting non-standard atom names and converting
#                              them to version 3.0
# revision 1.60 - JJH 081120 - reorganized code into functions and cleaned up flag usage
# revision 1.61 - JJH 110622 - moved to iotbx
# revision 1.70 - VBC 201116 - switch over to using model object and chemical components library
# revision 1.71 - VBC 230923 - switch to using atom.parent() instead of relying on id_str()

import sys
import os
import re
try:
  from phenix.program_template import ProgramTemplate
except ImportError:
  from libtbx.program_template import ProgramTemplate

import libtbx.load_env
import iotbx.pdb
import iotbx.pdb.mmcif
import mmtbx.model
from libtbx.utils import null_out

import mmtbx.chemical_components as chemical_components

class Program(ProgramTemplate):

  description = """
Program for converting a PDB format file to version 3.2 or 2.3.

iotbx.pdb_remediator model.pdb [params.eff] [options ...]

Options:

  file_name     specify input pdb file
  output_file   specify output file name (defaults to stdout)
  version       3.2 (default) or 2.3
  dict          optionally supply custom definition file

Examples:

Convert version 2.3 file to version 3.2 naming:

  iotbx.pdb_remediator model.pdb > model_3.2.pdb
  iotbx.pdb_remediator file_name=model.pdb output_file=model_3.2.pdb_file

Convert version 3.2 file to version 2.3 naming:

  iotbx.pdb_remediator model.pdb version=2.3 > model_2.3.pdb_file
"""
  datatypes = ['model']
  results = ""

  def run(self):
    model = self.data_manager.get_model()
    convert_to_new = True
    if self.params.version == 2.3:
      convert_to_new = False
    remed = Remediator()
    self.results = remed.remediate_model_object(model, convert_to_new)

  def get_results(self):
    return self.results

  def validate(self):
    if not self.params.file_name or (not os.path.isfile(self.params.file_name)):
      raise Sorry("The file %s is missing" %(self.params.file_name))


#{{{ pre_screen_file
def pre_screen_file(filename, atom_exch, alt_atom_exch):
  count = 0
  res_count = 0
  if not os.path.isfile(filename):
    from libtbx.utils import Sorry
    raise Sorry("The file %s is missing" %(filename))
  pdb_file = open(filename)
  for line in pdb_file:
    line=line.rstrip()
    line=line.ljust(80)
    type_test = line[0:6]
    if type_test in ("ATOM  ", "HETATM", "TER   ", "ANISOU", "SIGATM", "SIGUIJ", "LINK  "):
      adjust_res = False
      #--make any left-justified residue names right-justified------------------
      if re.match(r'.{17}([a-zA-Z0-9])  ',line):
        line = re.sub(r'\A(.{17})(.)\s\s',r'\g<1>  \g<2>',line)
        adjust_res = True
      elif re.match(r'.{17}([a-zA-Z0-9][a-zA-Z0-9]) ',line):
        line = re.sub(r'\A(.{17})(..)\s',r'\g<1> \g<2>',line)
        adjust_res = True
      #-------------------------------------------------------------------------

      #--pre-screen for CNS Xplor RNA base names and Coot RNA base names--------
      if re.match(r'.{17}(GUA|ADE|CYT|THY|URI)',line):
        line = re.sub(r'\A(.{17})(.)..',r'\g<1>  \g<2>',line)
        adjust_res = True
      elif re.match(r'.{17}(OIP| Ar| Gr| Cr| Ur)',line):
        line = re.sub(r'\A(.{17}).(.).',r'\g<1>  \g<2>',line)
        adjust_res = True
      #-------------------------------------------------------------------------

      if adjust_res:
        res_count += 1
      entry = line[12:20]
      clean_entry = entry[0:4] + " " + entry[5:8]
      if clean_entry in atom_exch:
        if clean_entry in alt_atom_exch:
          pass
        else:
          count += 1
  return count, res_count
#}}}

#{{{ build_hash
#--Build Hash Table------------------------------------------------
def build_hash(remediated_out, custom_dict, user_dict):
  atom_exch = {}
  file_name = os.path.join(libtbx.env.dist_path("iotbx"), "pdb",
    "remediation", "remediation.dict")
  f = open(file_name, "r")
  if remediated_out == True: #converting to remediated
    for line in f:
      line=line.rstrip()
      new, old = line.split(':')
      atom_exch[old] = new
  else: #converting to old
    for line in f:
      new, old = line.split(':')
      atom_exch[new] = old
  if custom_dict == True:
    user_f = open(user_dict)
    if remediated_out == True: #converting to remediated
      for line in user_f:
        line=line.rstrip()
        new, old = line.split(':')
        atom_exch[old] = new
    else:
      for line in user_f:
        line=line.rstrip()
        new, old = line.split(':')
        atom_exch[new] = old
    user_f.close()
  f.close()
  return atom_exch
#}}}
#------------------------------------------------------------------

def justify_atom_names(atom_name):
  if (len(atom_name) == 4):
    return atom_name
  if (len(atom_name) == 3):
    if (atom_name == "H3T"):
      return atom_name.ljust(4)
    if (atom_name[0:2] == "NH"):
      return atom_name.rjust(4)
    if (atom_name[1] == "H"):
      return atom_name.ljust(4)
    return atom_name.rjust(4)
  else:
    return atom_name.center(4)

def get_model_from_file(file_path):
  pdb_inp = iotbx.pdb.input(file_name = file_path)
  model = mmtbx.model.manager(
    model_input = pdb_inp,
    stop_for_unknowns = False,
    log = null_out())
  model.process(make_restraints=False)
  return model

class Remediator():
  amino_acids = [ "ALA","ARG","ASN","ASP","ASX","CSE","CYS","GLN","GLU","GLX","GLY","HIS","ILE",
    "LEU","LYS","MET","MSE","PHE","PRO","SER","THR","TRP","TYR","VAL" ]
  na_bases = ["  A", "  C", "  T", "  G", "  I", "  U"]
  dna_bases = [" DA", " DC", " DT", " DG", " DI", " DU"]
  residues_dict = {}

  def build_hash_from_chem_components(self, residue_name, convert_to_new=True, build_all_atoms=False):
    atom_exch = {}
    old_atom_name_exceptions = ["1H  ", "2H  ", "3H  "]
    new_atom_name_exceptions = [" H1 ", " H2 ", " H3 "]
    if residue_name in self.amino_acids: # covers the case where the N terminal Hs aren't in chem comps
      for new_atom, old_atom in zip(new_atom_name_exceptions, old_atom_name_exceptions):
        if convert_to_new:
          atom_exch[old_atom+" "+residue_name] = new_atom+" "+residue_name
        else:
          atom_exch[new_atom+" "+residue_name] = old_atom+" "+residue_name
    na_old_atom_name_exceptions = ["H5T ","5HO*", "H3T ", "3HO*"] # it's difficult in 2020 to figure out the proper spacing for this old style atom name
    na_new_atom_name_exceptions = ["HO5'","HO5'", "HO3'", "HO3'"] # old remediator uses 5HO* but other examples on the web use H5T for HO5'
    residues_to_test = [ residue_name ]
    if (residue_name in self.na_bases) or (residue_name in self.dna_bases):
      if (residue_name in self.na_bases):
        residues_to_test.append(" D"+residue_name[2])
      else:
        residues_to_test.append("  "+residue_name[2])
      for nucleic_acid in residues_to_test:
        for new_atom, old_atom in zip(na_new_atom_name_exceptions, na_old_atom_name_exceptions):
          if convert_to_new:
            atom_exch[old_atom+" "+nucleic_acid] = new_atom+" "+nucleic_acid
          else:
            atom_exch[new_atom+" "+nucleic_acid] = old_atom+" "+nucleic_acid
    for residue in residues_to_test:
      if (chemical_components.is_code(residue)):
        #sys.stderr.write(residue_name+" is in chem_components\n")
        new_atom_names = chemical_components.get_atom_names(residue, alternate=False)
        old_atom_names = chemical_components.get_atom_names(residue, alternate=True)
        atom_type_symbols = chemical_components.get_atom_type_symbol(residue)
        if (len(new_atom_names) == len(old_atom_names)):
          for new_atom, old_atom, atom_type in zip(new_atom_names, old_atom_names, atom_type_symbols):
            if build_all_atoms or not (new_atom == old_atom):
              for res_name in residues_to_test:
                justified_old_atom = iotbx.pdb.mmcif.format_pdb_atom_name(old_atom, atom_type)
                justified_new_atom = iotbx.pdb.mmcif.format_pdb_atom_name(new_atom, atom_type)
                new_entry = justified_new_atom+" "+res_name
                old_entry = justified_old_atom+" "+res_name
                #print(old_entry + "->" + new_entry)
                if convert_to_new:
                  atom_exch[old_entry] = new_entry
                  #check for 1HA, 2HA, etc, which don't seem to always be in chem components as possible old names
                  if not build_all_atoms or re.match(r' H[A-Z]\d', justified_old_atom):
                    digit_first_hydrogen = justified_old_atom[3]+justified_old_atom[1:3]+" "
                    atom_exch[digit_first_hydrogen+" "+res_name] = new_entry
                else:
                  atom_exch[new_entry] = old_entry
    #print("finished making hash:"+str(atom_exch))
    return atom_exch

  def is_model_v3(self, model):
    self.residues_dict = {}
    pdb_hierarchy = model.get_hierarchy()
    atoms = pdb_hierarchy.atoms()
    non_v3_atoms_count = 0
    for atom in atoms:
      #print(atom.name)
      res_name = atom.parent().resname
      if not res_name in self.residues_dict:
        self.residues_dict[res_name] = self.build_hash_from_chem_components(res_name, convert_to_new=False, build_all_atoms=True)
        #print(res_name+"\n"+str(residues_dict[res_name]))
      atom_exch_dict = self.residues_dict[res_name]
      if not atom.name+" "+res_name in atom_exch_dict:
        #print(atom_exch_dict)
        print("|"+atom.name +"| does not seem to be v3 in "+res_name)
        non_v3_atoms_count=non_v3_atoms_count+1
      #print(atom.name)
    return non_v3_atoms_count == 0

  def remediate_model_object(self, model, convert_to_new):
    self.residues_dict = {}
    non_v3_atoms_count = 0
    pdb_hierarchy = model.get_hierarchy()
    #pdb_hierarchy.atoms().reset_i_seq()
    residue_groups = pdb_hierarchy.residue_groups()
    for residue_group in residue_groups:
      atom_groups = residue_group.atom_groups()
      for atom_group in atom_groups:
        res_name = atom_group.resname
        if res_name in self.dna_bases or res_name in self.na_bases:
          self.remediate_na_atom_group(atom_group, convert_to_new)
          res_name = atom_group.resname
        for atom in atom_group.atoms():
          if not res_name in self.residues_dict:
            self.residues_dict[res_name] = self.build_hash_from_chem_components(res_name, convert_to_new=convert_to_new, build_all_atoms=False)
          atom_exch_dict = self.residues_dict[res_name]
          #print(atom.name + str(atom.name+" "+res_name in atom_exch_dict))
          if atom.name+" "+res_name in atom_exch_dict:
            new_entry = atom_exch_dict.get(atom.name+" "+res_name)
            atom.set_name(new_entry[0:4])
            non_v3_atoms_count=non_v3_atoms_count+1
    pdb_hierarchy.sort_atoms_in_place()
    pdb_hierarchy.atoms_reset_serial()
    return model

  def remediate_na_atom_group(self, atom_group, convert_to_new):
    rna_group = False
    if convert_to_new:
      if atom_group.resname in self.na_bases:
        for atom in atom_group.atoms():
          if atom.name == " O2'" or atom.name == " O2*":
            rna_group = True
        if not rna_group:
          atom_group.resname = " D" + atom_group.resname[2]
    else:
      if atom_group.resname in self.dna_bases:
        atom_group.resname = "  " + atom_group.resname[2]


def remediate_atomic_line(line, atom_exch):
  #--make any left-justified residue names right-justified------------------
  if re.match(r'.{17}([a-zA-Z0-9])  ',line):
    line = re.sub(r'\A(.{17})(.)\s\s',r'\g<1>  \g<2>',line)
  elif re.match(r'.{17}([a-zA-Z0-9][a-zA-Z0-9]) ',line):
    line = re.sub(r'\A(.{17})(..)\s',r'\g<1> \g<2>',line)
  #-------------------------------------------------------------------------

  #--pre-screen for CNS Xplor RNA base names and Coot RNA base names--------
  if re.match(r'.{17}(GUA|ADE|CYT|THY|URI)',line):
    line = re.sub(r'\A(.{17})(.)..',r'\g<1>  \g<2>',line)
  elif re.match(r'.{17}(OIP| Ar| Gr| Cr| Ur)',line):
    line = re.sub(r'\A(.{17}).(.).',r'\g<1>  \g<2>',line)
  #-------------------------------------------------------------------------

  entry = line[12:20]
  clean_entry = entry[0:4] + " " + entry[5:8]
  if clean_entry in atom_exch:
    line = line.replace(clean_entry[0:4],atom_exch[clean_entry][0:4],1)
  return line

#{{{ remediate
#----PDB routine---------------------------------------------------
def remediate(filename, remediated_out, f=None):
  if remediated_out:
    remark4 = "REMARK   4 REMEDIATOR VALIDATED PDB VERSION 3.2 COMPLIANT".ljust(80)
  else:
    remark4 = "REMARK   4 REMEDIATOR VALIDATED PDB VERSION 2.3 COMPLIANT".ljust(80)

  if f == None:
    f = sys.stdout
  previous = None
  current = ""
  print_line = ""
  remark_flag = False
  remark_block = False

  pdb_file = open(filename)

  aa_re = re.compile(
    ' HN2 (ALA|ARG|ASN|ASP|ASX|CSE|CYS|GLN|GLU|GLX|GLY|HIS|ILE|'+
    'LEU|LYS|MET|MSE|PHE|PRO|SER|THR|TRP|UNK|TYR|VAL)')

  for line in pdb_file:
    line=line.rstrip()
    line=line.ljust(80)
    type_test = line[0:6]
    if remark_flag == False:
      if type_test == "REMARK":
        if remark_block == False:
          remark_block = True
        if re.search(remark4,line):
          remark_flag = True
        elif re.match(r'REMARK...\D',line):
          print_line += line + "\n"
          continue
        elif re.match('REMARK   4 REMEDIATOR',line):
          continue
        elif int(line[6:10]) > 4:
          print_line += remark4 + "\n"
          remark_flag = True
        else:
          print_line += line + "\n"
          continue

    if type_test in ("ATOM  ", "HETATM", "TER   ", "ANISOU", "SIGATM", "SIGUIJ", "LINK  "):
      if remark_flag == False:
        print_line += remark4 + "\n"
        remark_flag = True
      previous = current
      current = line[18:26]
      residue_name = line[17:20]
      remed = Remediator()
      chem_comp_atom_exch = remed.build_hash_from_chem_components(residue_name, remediated_out)
      #sys.stderr.write(str(chem_comp_atom_exch)+"\n")
      line = remediate_atomic_line(line, chem_comp_atom_exch)
    elif (remark_flag == False) and (remark_block == True): #deal with non-remark lines stuck in the top before main record types
      print_line += remark4 + "\n"
      remark_flag = True
    if previous == current:
      print_line += line + "\n"
    elif previous != current: # appears to check an entire residue for dna residue/atom names
      if re.search(r'^.{12}.\S..  .[ACTGIU]',print_line):
        if re.search(r'O2[\'|\*]   .',print_line) == None and previous != None:
          DNA_base = previous[1]
          if remediated_out == True:
            print_line = re.sub(r'(?m)(^.{12}.\S..)   '+DNA_base+' ',r'\g<1>  D'+DNA_base+' ',print_line)
            print_line = re.sub(r'(?m)(^TER.{15}) '+DNA_base+' ',r'\g<1>D'+DNA_base+' ',print_line)
          else:
            print_line = re.sub(r'(?m)(^.{12}.\S..)  D'+DNA_base+' ',r'\g<1>   '+DNA_base+' ',print_line)
            print_line = re.sub(r'(?m)(^TER.{15})D'+DNA_base+' ',r'\g<1> '+DNA_base+' ',print_line)

      if remediated_out == False:
        m = aa_re.search(print_line)
        if m:
          res = m.group(1)
          if re.search('1H   '+res,print_line) or re.search('2H   '+res,print_line):
            print_line = re.sub(' HN2 '+res,'2H   '+res,print_line)
      print_line=print_line.rstrip("\n")

      if not (print_line == ""):
        print(print_line, file=f)
      print_line = line + "\n"
  print_line=print_line.rstrip("\n")
  print(print_line, file=f)
  pdb_file.close()

#}}}

def remediator(params, log=None):
  if log == None:
    log = sys.stderr
  custom_dict = False
  remediated_out = True
  user_dict = ""
  file_name = params.file_name
  if params.version == "3.2":
    remediated_out = True
  elif params.version == "2.3":
    remediated_out = False
  if params.dict != None:
    custom_dict = True
    user_dict = params.dict
  if params.output_file != None:
    f = open(params.output_file, "w")
  else:
    f = sys.stdout
  atom_exch = build_hash(remediated_out,
                                  custom_dict,
                                  user_dict)
  if remediated_out:
    remediated_alt = False
  else:
    remediated_alt = True
  alt_atom_exch = build_hash(remediated_alt,
                                           custom_dict,
                                           user_dict)
  count, res_count = \
    pre_screen_file(file_name, atom_exch, alt_atom_exch)
  if count > 0 or res_count > 0:
    remediate(params.file_name, remediated_out, f)
    if params.output_file != None:
      f.close()
  else:
    if remediated_out:
      print("All atoms conform to PDB v3.x standard  **skipping**", file=log)
    else:
      print("All atoms conform to PDB v2.3 standard  **skipping**", file=log)


 *******************************************************************************


 *******************************************************************************
iotbx/pdb/remediation/tst_remediator.py
from __future__ import division
import time
import os
import difflib

import remediator
import iotbx.pdb
import libtbx
from iotbx.data_manager import DataManager
from libtbx import easy_run

multimodel_tst_str = """MODEL        1
ATOM     21  P     A C   2      33.564  25.095  49.676  0.50 58.86           P
ATOM     22  O1P   A C   2      32.847  23.800  49.561  0.50 58.82           O
ATOM     23  O2P   A C   2      33.003  26.303  49.020  0.50 59.18           O
ATOM     24  O5*   A C   2      33.761  25.416  51.223  0.50 57.39           O
ATOM     25  C5*   A C   2      34.402  24.477  52.075  0.50 54.86           C
ATOM     26  C4*   A C   2      34.562  25.046  53.463  0.50 53.12           C
ATOM     27  O4*   A C   2      35.472  26.177  53.431  0.50 52.63           O
ATOM     28  C3*   A C   2      33.304  25.614  54.097  0.50 51.70           C
ATOM     29  O3*   A C   2      32.491  24.581  54.644  0.50 48.48           O
ATOM     30  C2*   A C   2      33.889  26.530  55.163  0.50 51.30           C
ATOM     31  O2*   A C   2      34.349  25.814  56.290  0.50 51.90           O
ATOM     32  C1*   A C   2      35.090  27.121  54.420  0.50 51.50           C
ATOM     33  N9    A C   2      34.791  28.391  53.755  0.50 50.06           N
ATOM     34  C8    A C   2      34.427  28.594  52.447  0.50 49.53           C
ATOM     35  N7    A C   2      34.216  29.853  52.145  0.50 49.19           N
ATOM     36  C5    A C   2      34.462  30.526  53.334  0.50 48.49           C
ATOM     37  C6    A C   2      34.410  31.888  53.680  0.50 47.61           C
ATOM     38  N6    A C   2      34.090  32.858  52.824  0.50 47.71           N
ATOM     39  N1    A C   2      34.705  32.225  54.953  0.50 47.40           N
ATOM     40  C2    A C   2      35.031  31.253  55.813  0.50 48.40           C
ATOM     41  N3    A C   2      35.118  29.941  55.609  0.50 48.16           N
ATOM     42  C4    A C   2      34.817  29.639  54.334  0.50 48.73           C
ATOM      0 1H5*   A C   2      35.271  24.244  51.713  0.50 54.86           H   new
ATOM      0 2H5*   A C   2      33.882  23.659  52.113  0.50 54.86           H   new
ATOM      0  H4*   A C   2      34.865  24.283  53.979  0.50 53.12           H   new
ATOM      0  H3*   A C   2      32.712  26.073  53.481  0.50 51.70           H   new
ATOM      0  H2*   A C   2      33.246  27.170  55.506  0.50 51.30           H   new
ATOM      0 2HO*   A C   2      34.347  26.318  56.962  0.50 51.90           H   new
ATOM      0  H1*   A C   2      35.790  27.300  55.067  0.50 51.50           H   new
ATOM      0  H8    A C   2      34.338  27.902  51.832  0.50 49.53           H   new
ATOM      0 1H6    A C   2      34.073  33.676  53.088  0.50 47.71           H   new
ATOM      0 2H6    A C   2      33.900  32.666  52.008  0.50 47.71           H   new
ATOM      0  H2    A C   2      35.225  31.535  56.678  0.50 48.40           H   new
ATOM    210  P     U C  11       5.031  37.445  41.706  0.50 38.57           P
ATOM    211  O1P   U C  11       4.328  37.521  40.399  0.50 38.83           O
ATOM    212  O2P   U C  11       4.502  38.190  42.876  0.50 39.51           O
ATOM    213  O5*   U C  11       5.164  35.910  42.092  0.50 39.44           O
ATOM    214  C5*   U C  11       5.615  34.962  41.129  0.50 42.64           C
ATOM    215  C4*   U C  11       5.702  33.586  41.744  0.50 44.82           C
ATOM    216  O4*   U C  11       6.788  33.536  42.710  0.50 45.43           O
ATOM    217  C3*   U C  11       4.497  33.138  42.552  0.50 45.57           C
ATOM    218  O3*   U C  11       3.432  32.687  41.721  0.50 47.09           O
ATOM    219  C2*   U C  11       5.098  32.011  43.379  0.50 45.57           C
ATOM    220  O2*   U C  11       5.261  30.823  42.630  0.50 46.33           O
ATOM    221  C1*   U C  11       6.472  32.593  43.723  0.50 45.59           C
ATOM    222  N1    U C  11       6.468  33.279  45.023  0.50 45.31           N
ATOM    223  C2    U C  11       6.566  32.494  46.157  0.50 44.65           C
ATOM    224  O2    U C  11       6.667  31.278  46.111  0.50 43.78           O
ATOM    225  N3    U C  11       6.539  33.183  47.344  0.50 44.04           N
ATOM    226  C4    U C  11       6.431  34.550  47.509  0.50 45.77           C
ATOM    227  O4    U C  11       6.433  35.025  48.646  0.50 45.26           O
ATOM    228  C5    U C  11       6.341  35.294  46.286  0.50 45.83           C
ATOM    229  C6    U C  11       6.364  34.649  45.114  0.50 45.15           C
ATOM      0 1H5*   U C  11       6.484  35.226  40.789  0.50 42.64           H   new
ATOM      0 2H5*   U C  11       5.008  34.947  40.373  0.50 42.64           H   new
ATOM      0  H4*   U C  11       5.805  33.013  40.968  0.50 44.82           H   new
ATOM      0  H3*   U C  11       4.090  33.842  43.080  0.50 45.57           H   new
ATOM      0  H2*   U C  11       4.547  31.761  44.137  0.50 45.57           H   new
ATOM      0 2HO*   U C  11       5.328  30.169  43.153  0.50 46.33           H   new
ATOM      0  H1*   U C  11       7.121  31.874  43.775  0.50 45.59           H   new
ATOM      0  H3    U C  11       6.595  32.713  48.062  0.50 44.04           H   new
ATOM      0  H5    U C  11       6.267  36.221  46.308  0.50 45.83           H   new
ATOM      0  H6    U C  11       6.308  35.146  44.330  0.50 45.15           H   new
ENDMDL
MODEL        2
ATOM     21  P     A C   2      33.564  25.095  49.676  0.50 58.86           P
ATOM     22  O1P   A C   2      32.847  23.800  49.561  0.50 58.82           O
ATOM     23  O2P   A C   2      33.003  26.303  49.020  0.50 59.18           O
ATOM     24  O5*   A C   2      33.761  25.416  51.223  0.50 57.39           O
ATOM     25  C5*   A C   2      34.402  24.477  52.075  0.50 54.86           C
ATOM     26  C4*   A C   2      34.562  25.046  53.463  0.50 53.12           C
ATOM     27  O4*   A C   2      35.472  26.177  53.431  0.50 52.63           O
ATOM     28  C3*   A C   2      33.304  25.614  54.097  0.50 51.70           C
ATOM     29  O3*   A C   2      32.491  24.581  54.644  0.50 48.48           O
ATOM     30  C2*   A C   2      33.889  26.530  55.163  0.50 51.30           C
ATOM     31  O2*   A C   2      34.349  25.814  56.290  0.50 51.90           O
ATOM     32  C1*   A C   2      35.090  27.121  54.420  0.50 51.50           C
ATOM     33  N9    A C   2      34.791  28.391  53.755  0.50 50.06           N
ATOM     34  C8    A C   2      34.427  28.594  52.447  0.50 49.53           C
ATOM     35  N7    A C   2      34.216  29.853  52.145  0.50 49.19           N
ATOM     36  C5    A C   2      34.462  30.526  53.334  0.50 48.49           C
ATOM     37  C6    A C   2      34.410  31.888  53.680  0.50 47.61           C
ATOM     38  N6    A C   2      34.090  32.858  52.824  0.50 47.71           N
ATOM     39  N1    A C   2      34.705  32.225  54.953  0.50 47.40           N
ATOM     40  C2    A C   2      35.031  31.253  55.813  0.50 48.40           C
ATOM     41  N3    A C   2      35.118  29.941  55.609  0.50 48.16           N
ATOM     42  C4    A C   2      34.817  29.639  54.334  0.50 48.73           C
ATOM      0 1H5*   A C   2      35.271  24.244  51.713  0.50 54.86           H   new
ATOM      0 2H5*   A C   2      33.882  23.659  52.113  0.50 54.86           H   new
ATOM      0  H4*   A C   2      34.865  24.283  53.979  0.50 53.12           H   new
ATOM      0  H3*   A C   2      32.712  26.073  53.481  0.50 51.70           H   new
ATOM      0  H2*   A C   2      33.246  27.170  55.506  0.50 51.30           H   new
ATOM      0 2HO*   A C   2      34.347  26.318  56.962  0.50 51.90           H   new
ATOM      0  H1*   A C   2      35.790  27.300  55.067  0.50 51.50           H   new
ATOM      0  H8    A C   2      34.338  27.902  51.832  0.50 49.53           H   new
ATOM      0 1H6    A C   2      34.073  33.676  53.088  0.50 47.71           H   new
ATOM      0 2H6    A C   2      33.900  32.666  52.008  0.50 47.71           H   new
ATOM      0  H2    A C   2      35.225  31.535  56.678  0.50 48.40           H   new
ATOM    210  P     U C  11       5.031  37.445  41.706  0.50 38.57           P
ATOM    211  O1P   U C  11       4.328  37.521  40.399  0.50 38.83           O
ATOM    212  O2P   U C  11       4.502  38.190  42.876  0.50 39.51           O
ATOM    213  O5*   U C  11       5.164  35.910  42.092  0.50 39.44           O
ATOM    214  C5*   U C  11       5.615  34.962  41.129  0.50 42.64           C
ATOM    215  C4*   U C  11       5.702  33.586  41.744  0.50 44.82           C
ATOM    216  O4*   U C  11       6.788  33.536  42.710  0.50 45.43           O
ATOM    217  C3*   U C  11       4.497  33.138  42.552  0.50 45.57           C
ATOM    218  O3*   U C  11       3.432  32.687  41.721  0.50 47.09           O
ATOM    219  C2*   U C  11       5.098  32.011  43.379  0.50 45.57           C
ATOM    220  O2*   U C  11       5.261  30.823  42.630  0.50 46.33           O
ATOM    221  C1*   U C  11       6.472  32.593  43.723  0.50 45.59           C
ATOM    222  N1    U C  11       6.468  33.279  45.023  0.50 45.31           N
ATOM    223  C2    U C  11       6.566  32.494  46.157  0.50 44.65           C
ATOM    224  O2    U C  11       6.667  31.278  46.111  0.50 43.78           O
ATOM    225  N3    U C  11       6.539  33.183  47.344  0.50 44.04           N
ATOM    226  C4    U C  11       6.431  34.550  47.509  0.50 45.77           C
ATOM    227  O4    U C  11       6.433  35.025  48.646  0.50 45.26           O
ATOM    228  C5    U C  11       6.341  35.294  46.286  0.50 45.83           C
ATOM    229  C6    U C  11       6.364  34.649  45.114  0.50 45.15           C
ATOM      0 1H5*   U C  11       6.484  35.226  40.789  0.50 42.64           H   new
ATOM      0 2H5*   U C  11       5.008  34.947  40.373  0.50 42.64           H   new
ATOM      0  H4*   U C  11       5.805  33.013  40.968  0.50 44.82           H   new
ATOM      0  H3*   U C  11       4.090  33.842  43.080  0.50 45.57           H   new
ATOM      0  H2*   U C  11       4.547  31.761  44.137  0.50 45.57           H   new
ATOM      0 2HO*   U C  11       5.328  30.169  43.153  0.50 46.33           H   new
ATOM      0  H1*   U C  11       7.121  31.874  43.775  0.50 45.59           H   new
ATOM      0  H3    U C  11       6.595  32.713  48.062  0.50 44.04           H   new
ATOM      0  H5    U C  11       6.267  36.221  46.308  0.50 45.83           H   new
ATOM      0  H6    U C  11       6.308  35.146  44.330  0.50 45.15           H   new
ENDMDL
END
"""

def test_pdb_to_v3(old_pdb_file, v3_pdb_file):
  print("............checking conversion of "+old_pdb_file+" to new PDB format")
  remed = remediator.Remediator()
  old_pdb_model = remediator.get_model_from_file(old_pdb_file)
  #write_model_to_file(old_pdb_model.model_as_pdb(), "old_file.pdb")
  old_pdb_model_v3 = remed.remediate_model_object(old_pdb_model, True)
  v3_pdb_model = remediator.get_model_from_file(v3_pdb_file)
  #write_model_to_file(old_pdb_model_v3.model_as_pdb(), "old_file_remed.pdb")
  #write_model_to_file(v3_pdb_model.model_as_pdb(), "v3_file.pdb")
  old_pdb_model_v3_string = old_pdb_model_v3.get_hierarchy().as_pdb_string()
  v3_pdb_model_string = v3_pdb_model.get_hierarchy().as_pdb_string()
  #print(old_pdb_model_v3_string.splitlines(1))
  #print(v3_pdb_model_string.splitlines(1))
  if old_pdb_model_v3_string != v3_pdb_model_string:

    diff = list(difflib.unified_diff(old_pdb_model_v3_string.splitlines(), v3_pdb_model_string.splitlines()))
    assert False, "remediation test of "+old_pdb_file+" to v3 failed with these differences:\n" + '\n'.join(diff)

  print("OK")
  #print(old_pdb_model_v3.get_hierarchy().is_similar_hierarchy(v3_pdb_model.get_hierarchy()))
  #print(old_pdb_model.get_hierarchy().is_similar_hierarchy(v3_pdb_model.get_hierarchy()))

def test_pdb_to_v23(pdb_file):
  remediator.remediate(pdb_file, False)

def test_pdb_remediate_cycle(pdb_file):
  t0 = time.time()
  print("testing remediation cycle of "+pdb_file)
  remediatorObj = remediator.Remediator()
  pdb_model = remediator.get_model_from_file(pdb_file)
  pdb_model = remediatorObj.remediate_model_object(pdb_model, True)
  #write_model_to_file(pdb_model.model_as_pdb(), "input_model.pdb")
  print("............checking if remediated "+pdb_file+" is v3")
  assert remediatorObj.is_model_v3(pdb_model), "remediation of "+pdb_file+ " during remediation cycle test failed"
  pdb_model = remediatorObj.remediate_model_object(pdb_model, False)
  #write_model_to_file(pdb_model.model_as_pdb(), "old_model.pdb")
  pdb_model = remediatorObj.remediate_model_object(pdb_model, True)
  #write_model_to_file(pdb_model.model_as_pdb(), "reold_model.pdb")
  print("............checking remediated-to-v2.3 and remediated "+pdb_file+" is v3")
  assert remediatorObj.is_model_v3(pdb_model), "cycling of "+pdb_file+" from v3 to v2.3 back to v3 failed"
  print("OK. "+pdb_file+" time: %8.3f"%(time.time()-t0))

def tst_multi_model():
  print("............checking if multi-model files remediate properly")
  dm = DataManager()
  dm.process_model_str("1",multimodel_tst_str)
  m = dm.get_model("1")
  remediatorObj = remediator.Remediator()
  pdb_model = remediatorObj.remediate_model_object(m, True)
  assert remediatorObj.is_model_v3(pdb_model), "test if multimodel file converts to v3"

def write_model_to_file(model, file_name):
  with open("tst/"+file_name, 'w+') as fh:
    fh.write(model)

def get_regression_folder_file_path(file_name):
  regression_folder = os.path.join(libtbx.env.dist_path("iotbx"), 'pdb', 'remediation', 'tst')
  file_path = os.path.join(regression_folder, file_name)
  assert os.path.isfile(file_path), file_name + " test file is missing from " + regression_folder
  return file_path

def test_full_str_convert(old_file, new_file):
  print("testing full file remediation")
  prefix = os.path.basename(__file__).replace(".py","")
  assert not easy_run.call("iotbx.pdb_remediator %s > %s.pdb"%(old_file, prefix))
  with open(prefix+".pdb") as file:
    remediated_list = [line.rstrip() for line in file]
    remediated_pdb = "\n".join(remediated_list)
  with open(new_file) as file:
    new_list = [line.rstrip() for line in file]
    new_pdb = "\n".join(new_list)
  if new_pdb != remediated_pdb:

    diff = list(difflib.unified_diff(new_pdb.splitlines(), remediated_pdb.splitlines()))
    assert False, "remediation test of "+old_file+" to v3 failed with these differences:\n" + '\n'.join(diff)

  print("OK")

def run_tests():
  test_full_str_convert(get_regression_folder_file_path("dna-rna-testv23.pdb"), get_regression_folder_file_path("dna-rna-test.pdb"))
  test_pdb_to_v3(get_regression_folder_file_path("protein-dna-testv23.pdb"), get_regression_folder_file_path("protein-dna-test.pdb"))
  test_pdb_to_v3(get_regression_folder_file_path("dna-rna-testv23.pdb"), get_regression_folder_file_path("dna-rna-test.pdb"))

  test_pdb_remediate_cycle(get_regression_folder_file_path("protein-dna-test.pdb"))
  test_pdb_remediate_cycle(get_regression_folder_file_path("protein-dna-testv23.pdb"))
  test_pdb_remediate_cycle(get_regression_folder_file_path("dna-rna-test.pdb"))
  test_pdb_remediate_cycle(get_regression_folder_file_path("dna-rna-testv23.pdb"))
  test_pdb_remediate_cycle(get_regression_folder_file_path("mg_tst.pdb"))
  tst_multi_model()

if __name__=="__main__":
  if libtbx.env.find_in_repositories(relative_path='chem_data') is not None:
    run_tests()
    print("OK")
  else:
    print('chem_data is not available for remediator tests, skipping')


 *******************************************************************************


 *******************************************************************************
iotbx/pdb/rna_dna_detection.py
"""Tools for detection of RNA vs DNA"""
from __future__ import absolute_import, division, print_function
from scitbx import matrix
from libtbx import slots_getstate_setstate

# geostd values 2009-10-17
bond_distance_ideal_by_bond_atom_names_v3 = {
  "OP1 P":   1.485,
  "OP2 P":   1.485,
  "O5' P":   1.593,
  "C1' C2'": 1.521,
  "C1' O4'": 1.416,
  "C2' O2'": 1.407, # RNA only
  "C2' C3'": 1.529,
  "C3' O3'": 1.422,
  "C3' C4'": 1.528,
  "C4' O4'": 1.456,
  "C4' C5'": 1.511,
  "C5' O5'": 1.427}
bond_distance_ideal_by_bond_atom_names_v2 = {}
for key,value in bond_distance_ideal_by_bond_atom_names_v3.items():
  key = (key
    .replace("OP1", "O1P")
    .replace("OP2", "O2P")
    .replace("'", "*"))
  bond_distance_ideal_by_bond_atom_names_v2[key] = value

# 2009-10-17 geostd value for C1'-N, identical for A, C, G, U
c1p_outbound_distance_estimate = 1.463

class _make_tables(object):

  def __init__(O):
    O.cache = None

  def __call__(O, index):
    if (O.cache is None):
      O.cache = []
      atoms_t = ("C5'", "C4'", "O4'", "C1'", "C2'", "C3'", "O3'")
      atoms_s = tuple([a.replace("'","*") for a in atoms_t])
      for bonds_1 in [("OP1 P", "OP2 P"), ("O1P P", "O2P P")]:
        bonds_t = set(bonds_1 + (
          "O5' P",
          "C1' C2'", "C2' C3'", "C3' C4'", "C3' O3'", "C4' C5'", "C4' O4'",
          "C1' O4'", "C5' O5'"))
        O.cache.append((atoms_t, bonds_t))
        O.cache.append((atoms_s, set([b.replace("'","*") for b in bonds_t])))
    return O.cache[index]

_tables = _make_tables()

class atom_name_analysis(slots_getstate_setstate):

  __slots__ = [
    "sub_classification",
    "required_atoms",
    "required_bonds",
    "have_o2",
    "c2_o2",
    "c1",
    "have_all_required_atoms"]

  def __init__(O, atom_dict):
    O.sub_classification = None
    O.required_atoms = None
    O.required_bonds = None
    O.have_o2 = None
    O.c2_o2 = None
    O.c1 = None
    O.have_all_required_atoms = False
    if ("P" not in atom_dict): return
    if   ("OP1" in atom_dict):
      if ("OP2" not in atom_dict): return
      tab_offs = 0
    elif ("O1P" in atom_dict):
      if ("O2P" not in atom_dict): return
      tab_offs = 2
    else:
      return
    if   ("O5'" in atom_dict):
      O.required_atoms, O.required_bonds = _tables(index=tab_offs)
      O.have_o2 = "O2'" in atom_dict
      O.c2_o2 = "C2' O2'"
      O.c1 = "C1'"
    elif ("O5*" in atom_dict):
      O.required_atoms, O.required_bonds = _tables(index=tab_offs+1)
      O.have_o2 = "O2*" in atom_dict
      O.c2_o2 = "C2* O2*"
      O.c1 = "C1*"
    else:
      return
    if (tab_offs == 2 and O.c2_o2 == "C2* O2*"):
      O.sub_classification = ""
    elif (tab_offs == 2 and O.c2_o2 == "C2' O2'"):
      O.sub_classification = '_mixed'
    elif (tab_offs == 0 and O.c2_o2 == "C2' O2'"):
      O.sub_classification = "v3"
    else:
      return
    for required_atom in O.required_atoms:
      if (not required_atom in atom_dict): return
    O.have_all_required_atoms = True

  def bond_distance_ideal_by_bond_atom_names(O):
    if (O.sub_classification == "v3"):
      return bond_distance_ideal_by_bond_atom_names_v3
    return bond_distance_ideal_by_bond_atom_names_v2

def classification(atom_dict, bond_list):
  O = atom_name_analysis(atom_dict=atom_dict)
  if (not O.have_all_required_atoms):
    return None
  rna_indicator = False
  bonds_matched = set()
  for bond in bond_list:
    pair = [bond.atom_id_1, bond.atom_id_2]
    pair.sort()
    pair = " ".join(pair)
    if (pair in O.required_bonds):
      bonds_matched.add(pair)
    elif (O.have_o2 and pair == O.c2_o2):
      rna_indicator = True
  if (len(bonds_matched) != len(O.required_bonds)):
    return None
  if (rna_indicator): return "RNA"+O.sub_classification
  return "DNA"+O.sub_classification

deoxy_ribo_atom_keys = set("C1' C2' C3' O3' C4' O4' C5' O5'".split())

class residue_analysis(slots_getstate_setstate):

  __slots__ = [
    "problems",
    "atom_dict",
    "deoxy_ribo_atom_dict",
    "p_atom",
    "op_atoms",
    "c1p_outbound_candidates",
    "o2p_atom",
    "h_atoms",
    "is_terminus_with_o5p",
    "c1p_outbound_atom",
    "is_rna"]

  def __init__(O, residue_atoms, distance_tolerance=0.5):
    O.problems = []
    O.atom_dict = residue_atoms.build_dict(
      strip_names=True,
      upper_names=True,
      convert_stars_to_primes=True,
      throw_runtime_error_if_duplicate_keys=False)
    if (len(O.atom_dict) != len(residue_atoms)):
      O.problems.append("key_clash")
    deoxy_ribo_problems = []
    O.deoxy_ribo_atom_dict = {}
    for key in deoxy_ribo_atom_keys:
      atom = O.atom_dict.get(key)
      if (atom is None):
        deoxy_ribo_problems.append("missing_"+key)
        continue
      O.deoxy_ribo_atom_dict[key] = atom
      del O.atom_dict[key]
    O.p_atom = None
    for key in list(O.atom_dict.keys()):
      if (key.startswith("P")):
        if (key == "P"):
          O.p_atom = O.atom_dict[key]
          del O.atom_dict[key]
        else:
          O.problems.append("other_P")
    O.op_atoms = []
    for keys in [("OP1", "O1P"),
                 ("OP2", "O2P")]:
      for key in keys:
        atom = O.atom_dict.get(key)
        if (atom is not None):
          O.op_atoms.append(atom)
          del O.atom_dict[key]
          break
      else:
        O.op_atoms.append(None)
        if (O.p_atom is not None):
          O.problems.append("missing_"+keys[0])
    O.o2p_atom = O.atom_dict.get("O2'")
    if (O.o2p_atom is not None):
      del O.atom_dict["O2'"]
    O.h_atoms = {}
    for key in list(O.atom_dict.keys()):
      if (len(key) == 0):
        O.problems.append("blank_name")
        continue
      if (   "HD".find(key[0]) >= 0
          or (    len(key) > 1
              and "HD".find(key[1]) >= 0
              and "0123456789".find(key[0]) >= 0)):
        O.h_atoms[key] = O.atom_dict[key]
        del O.atom_dict[key]
    O.c1p_outbound_candidates = {}
    for key in list(O.atom_dict.keys()):
      if (key.find("'") >= 0):
        if key == "N2'":
          O.deoxy_ribo_atom_dict[key] = atom
          del O.atom_dict[key]
        else:
          O.problems.append("other_prime")
        continue
      if (key.startswith("N") or key.startswith("C")):
        O.c1p_outbound_candidates[key] = O.atom_dict[key]
        del O.atom_dict[key]
    O.is_terminus_with_o5p = False
    if (    len(O.problems) == 0
        and len(deoxy_ribo_problems) != 0
        and len(O.deoxy_ribo_atom_dict) == 1
        and O.deoxy_ribo_atom_dict.get("O5'") is not None):
      O.is_terminus_with_o5p = True
    else:
      O.problems.extend(deoxy_ribo_problems)
    def check_distance(key_pair, site_1, site_2):
      distance_model = abs(site_1 - site_2)
      distance_ideal = bond_distance_ideal_by_bond_atom_names_v3[key_pair]
      if (distance_model > distance_ideal + distance_tolerance):
        return False
      return True
    if (O.p_atom is not None):
      site_1 = matrix.col(O.p_atom.xyz)
      for i,key_pair in enumerate(["OP1 P", "OP2 P"]):
        atom = O.op_atoms[i]
        if (atom is None): continue
        site_2 = matrix.col(atom.xyz)
        if (not check_distance(key_pair, site_1, site_2)):
          O.problems.append("long_distance_P_OP%d" % (i+1))
      atom = O.deoxy_ribo_atom_dict.get("O5'")
      if (atom is not None):
        site_2 = matrix.col(atom.xyz)
        if (not check_distance("O5' P", site_1, site_2)):
          O.problems.append("long_distance_P_O5'")
    if (O.o2p_atom is not None):
      atom = O.deoxy_ribo_atom_dict.get("C2'")
      if (atom is not None):
        site_1 = matrix.col(atom.xyz)
        site_2 = matrix.col(O.o2p_atom.xyz)
        if (not check_distance("C2' O2'", site_1, site_2)):
          O.problems.append("long_distance_C2'_O2'")
    for key_pair in [
          "C1' C2'",
          "C1' O4'",
          "C2' C3'",
          "C3' O3'",
          "C3' C4'",
          "C4' O4'",
          "C4' C5'",
          "C5' O5'"]:
      sites = []
      for key in [key_pair[:3], key_pair[4:]]:
        atom = O.deoxy_ribo_atom_dict.get(key)
        if (atom is None): break
        sites.append(matrix.col(atom.xyz))
      else:
        if (not check_distance(key_pair, *sites)):
          O.problems.append("long_distance_"+key_pair.replace(" ","_"))
    O.c1p_outbound_atom = None
    if (len(O.c1p_outbound_candidates) != 0):
      atom = O.deoxy_ribo_atom_dict.get("C1'")
      if (atom is not None):
        closest_distance = c1p_outbound_distance_estimate + distance_tolerance
        site_1 = matrix.col(atom.xyz)
        for key,atom in O.c1p_outbound_candidates.items():
          site_2 = matrix.col(atom.xyz)
          distance = abs(site_1 - site_2)
          if (    closest_distance >= distance
              and (   closest_distance != distance
                   or O.c1p_outbound_atom is None)):
            O.c1p_outbound_atom = atom
            closest_distance = distance
    if (len(O.problems) == 0):
      O.problems = None
    O.is_rna = (O.o2p_atom is not None)


 *******************************************************************************


 *******************************************************************************
iotbx/pdb/secondary_structure.py
"""Tools for identification of secondary structure in a model"""
from __future__ import absolute_import, division, print_function
#
# Implemented based on PDB v3.2 specification at:
#   http://www.wwpdb.org/documentation/format32/sect5.html
#
# NOTE: the routines for extracting hydrogen bonds are not used in phenix.
# I have left them here because they're simpler (and probably still accurate
# in most cases), and useful for pymol-related I/O.
#
# Oleg on 3-26-2014:
# I'm commenting them out because they contain some bugs and duplicate the
# existing functionality (tested and used) from proteins.py
# pymol-related I/O used in phenix.secondary_structure_restraints is
# implemented and used from
#   mmtbx.geometry_restraints.hbond.as_pymol_dashes(...)
#
# The great part of relevant code which is actually in use
# (generation of hydrogen bonds, phil records) written as additional methods
# for annotation class (phil generation) and as stand-alone functions for
# h-bonds generation are in:
#   mmtbx/secondary_structure/proteins.py
#
# It might be useful to move all the relevant secondary-structure code
# somewhere in one place.
#
# Oleg on 11-25-2014
# Carried out massive refactoring of secondary_structure objects.
# proteins.py now contains only h-bond generation. Probably this is also
# should be methods of secondary structure classes. Then proteins.py could be
# eliminated completely.
#
#
# Oleg on 5-27-2015
# The purpose of these classes - store as precisely as possible the
# knowledge about secondary structure itself, without connection to a particular
# model file. Therefore these classes are just containers of information with
# methods to convert it to various formats, e.g. pdb HELIX/SHEET records,
# phil files, cif format (in future). No connection to model means no i_seqs,
# no integer or bool selections etc. Object of annotation should be passed
# whenever the SS information is needed to the places where particular pdb
# model is available.
# The outcome from this rationale is:
# 1. No phil parameters in the module.
# 2. Essentially only constructors from various formats and exports to
#    various formats here
# 3. Convenient access to the various data to work with in different parts
#    of code.
#

from libtbx.utils import Sorry

from libtbx import adopt_init_args
import sys
from iotbx.pdb.hybrid_36 import hy36encode, hy36decode
import iotbx.cif.model
import copy
from libtbx.utils import null_out
from six.moves import range
from six.moves import zip


def lists_have_comment_element(a,b):
  for x in a:
    if x in b:
      return True
  return False

def segments_are_similar(atom_selection_1=None,
   atom_selection_2=None,
   hierarchy=None,
   maximum_length_difference=None,
   minimum_overlap=None):

    "Return True if two segments selected are similar"
    "Different lengths optional."
    if minimum_overlap is None:
      minimum_overlap=4 # set default

    # Check to see if they overlap sufficiently and have similar lengths

    asc=hierarchy.atom_selection_cache()
    sel=asc.selection(string = atom_selection_1)
    try:
      h1=hierarchy.select(sel, copy_atoms=True)  # detach from original hierarchy
      number_self=h1.overall_counts().n_residues
    except Exception as e:
      return False

    asc=hierarchy.atom_selection_cache()
    sel=asc.selection(string = atom_selection_2)
    try:
      h2=hierarchy.select(sel, copy_atoms=True)
      number_other=h2.overall_counts().n_residues
    except Exception as e:
      return False

    if maximum_length_difference is not None and \
        abs(number_self-number_other)>maximum_length_difference:
      return False

    asc=hierarchy.atom_selection_cache()
    atom_selection="(%s) and (%s)" %(atom_selection_1,atom_selection_2)
    sel=asc.selection(string = atom_selection)
    try:
      h12=hierarchy.select(sel, copy_atoms=True)
      number_both=h12.overall_counts().n_residues
    except Exception as e:
      return False

    if number_both<minimum_overlap:
      return False

    return True

def choose_correct_cif_record(cif_dict, key1, key2, mandatory=True):
  if key1 in cif_dict:
    val = cif_dict[key1]
    if val != '?' and val != '.':
      return val
  if mandatory:
    return cif_dict[key2]
  else:
    return cif_dict.get(key2, '.')

class one_strand_pair_registration_atoms:
  def __init__(self,
      strand_a_atom=None,
      strand_b_atom=None,
      strand_a_position=None,
      strand_b_position=None,
      sense=None,
      log=sys.stdout):

    self.ok=True
    self.sense=sense
    self.std_position_a=None
    self.std_position_b=None

    #  If CA residue i of strand n matches with residue i' of strand n+1:
    #  For both parallel and antiparallel: N,O alternately H-bond to strands
    #    to right and to left

    #  For parallel strands:
    #  O of residue i in strand n H-bonds to N of residue i'+1 in strand n+1.
    #  N of residue i in strand n H-bonds to O of residue i'-1 in strand n+1.
    #  So... O 31 -- N 72  is equivalent to: (i=31  i'=71)
    #        O 33 -- N 74  (just naming residues 2 away)
    #        N 31 -- O 70  (naming N instead of O)
    #  and not equivalent to:
    #        O 32 -- N 73  (points the other way)

    #  For antiparallel strands:
    #  O of residue i in strand n H-bonds to N of residue i' in strand n+1
    #  N of residue i in strand n H-bonds to O of residue i' in strand n+1

    if not strand_a_atom or not strand_b_atom:
      return

    strand_a_atom=strand_a_atom.replace(" ","")
    strand_b_atom=strand_b_atom.replace(" ","")

    if (strand_a_atom=="O" and strand_b_atom != "N") or \
       (strand_a_atom=="N" and strand_b_atom != "O") or \
       (strand_a_atom!="N" and strand_b_atom != "N") or \
       (strand_a_atom!="O" and strand_b_atom != "O"):
          print("Cannot interpret H-bonding of %s to %s " %(
            strand_a_atom,strand_b_atom), file=log)
          ok=False
          return

    if not sense in [-1,1]:
      print("Cannot interpret bonding with sense not -1 or 1:", file=log)
      self.ok=False
      return

    if strand_a_position is None or strand_b_position is None:
      self.ok=False # can't do anything
      return

    if sense==1:  # parallel
      if strand_a_atom=="O":
        self.std_position_a=strand_a_position
        self.std_position_b=strand_b_position
      else:
        self.std_position_a=strand_a_position
        self.std_position_b=strand_b_position+2  # equiv to O bonding here
    else:  # antiparallel
      self.std_position_a=strand_a_position
      self.std_position_b=strand_b_position

  def is_equivalent_to(self,other=None):
    if not self.ok: return None  # can't tell

    #  Equivalent if std_position_a and std_position_b are the same for both
    #  or both offset by the same multiple of 2 (opposite multiple of 2 if
    #  antiparallel)

    if self.std_position_a is None and other.std_position_a is None:
      return None  # both are None
    elif self.std_position_a is None or other.std_position_a is None:
      return False # one is None
    if self.std_position_b is None and other.std_position_b is None:
      return None  # both are None
    elif self.std_position_b is None or other.std_position_b is None:
      return False # one is None

    delta_a=self.std_position_a-other.std_position_a
    if delta_a-2*(delta_a//2)!=0:  # must be multiple of 2
      return False
    delta_b=self.std_position_b-other.std_position_b
    if delta_b-2*(delta_b//2)!=0:  # must be multiple of 2
      return False
    if self.sense==1 and delta_a != delta_b:
      return False
    elif self.sense==-1 and delta_a != -delta_b:
      return False

    return True # all ok

class registration_atoms:
  def __init__(self,hierarchy=None,
      strand_1a=None,strand_1b=None,
      strand_2a=None,strand_2b=None,
      registration_1=None,registration_2=None,
      sense=None,
      log=sys.stdout):
    adopt_init_args(self, locals())

    self.ok=True
    # 1a,1b are the prev and cur strands for registration 1; 2a,2b for reg 2

    ok_a,strand_1a_position,strand_2a_position,\
      strand_1a_atom,strand_2a_atom=\
      self.get_positions_and_atoms_for_one_strand(prev=True)
    if not ok_a: ok=False

    ok_b,strand_1b_position,strand_2b_position, \
      strand_1b_atom,strand_2b_atom=\
      self.get_positions_and_atoms_for_one_strand(prev=False)
    if not ok_b: ok=False

    # pair_1 are the atoms and positions for one pair of strands
    registration_pair_1=one_strand_pair_registration_atoms(
      strand_a_atom=strand_1a_atom,
      strand_b_atom=strand_1b_atom,
      strand_a_position=strand_1a_position,
      strand_b_position=strand_1b_position,
      sense=sense,log=log)
    if not registration_pair_1.ok: self.ok=False


    registration_pair_2=one_strand_pair_registration_atoms(
      strand_a_atom=strand_2a_atom,
      strand_b_atom=strand_2b_atom,
      strand_a_position=strand_2a_position,
      strand_b_position=strand_2b_position,
      sense=sense,log=log)
    if not registration_pair_2.ok: self.ok=False

    if not self.ok:
      self.pairs_are_equivalent=None
    elif registration_pair_1.is_equivalent_to(registration_pair_2):
      self.pairs_are_equivalent=True
    else:
      self.pairs_are_equivalent=False

  def get_positions_and_atoms_for_one_strand(self,prev=True):
    if prev:
      atom_selection="( (%s) or (%s)) and name ca" %(
       self.strand_1a.as_atom_selections(), self.strand_2a.as_atom_selections())
    else:
      atom_selection="( (%s) or (%s)) and name ca" %(
       self.strand_1b.as_atom_selections(), self.strand_2b.as_atom_selections())
    asc=self.hierarchy.atom_selection_cache()
    sel=asc.selection(string = atom_selection)
    ph=self.hierarchy.select(sel, copy_atoms=True)  # detach from original hierarchy
    # now ph is list of CA by residue.  Find the position of our N or O

    if not self.registration_1 or not self.registration_2:
      return False,None,None,None,None

    strand_1_position=self.get_registration_position(
      hierarchy=ph,
      registration=self.registration_1,
      prev=prev)

    strand_2_position=self.get_registration_position(
      hierarchy=ph,
      registration=self.registration_2,
      prev=prev)

    if prev:
      strand_1_atom=self.registration_1.prev_atom
      strand_2_atom=self.registration_2.prev_atom
    else:
      strand_1_atom=self.registration_1.cur_atom
      strand_2_atom=self.registration_2.cur_atom

    ok=None
    if strand_1_position is None and strand_2_position is not None:
      ok=False
    if strand_2_position is None and strand_1_position is not None:
      ok=False
    if strand_1_position is not None and strand_2_position is not None:
      ok=True

    return ok,strand_1_position,strand_2_position,strand_1_atom,strand_2_atom

  def get_registration_position(self,hierarchy=None,
      registration=None,prev=False):
    if not registration:
      return None

    if prev:
      resname=registration.prev_resname
      resseq=registration.prev_resseq
      chain_id=registration.prev_chain_id
      icode=registration.prev_icode
    else:
      resname=registration.cur_resname
      resseq=registration.cur_resseq
      chain_id=registration.cur_chain_id
      icode=registration.cur_icode
    # Assumes that there are no alternate conformations
    pos=-1
    for model in hierarchy.models()[:1]:
      for chain in model.chains()[:1]:
        for conformer in chain.conformers()[:1]:
          for residue in conformer.residues():
            pos+=1
            if residue.resname==resname and \
               residue.resseq.replace(" ","")==resseq.replace(" ", "") and \
               chain.id==chain_id and residue.icode==icode:
              return pos
    return None


def get_amide_isel(asc, ss_element_string_selection):
  # There are cases where SS element can be only in B conformation
  # and split in A/B conformations
  sele_str_main_conf = "(%s) and (name N) and (altloc 'A' or altloc ' ')" % ss_element_string_selection
  amide_isel_main_conf = asc.iselection(sele_str_main_conf)
  if len(amide_isel_main_conf) > 0:
    return amide_isel_main_conf
  sele_str_all = "(%s) and (name N)" % ss_element_string_selection
  amide_isel_all = asc.iselection(sele_str_all)
  if len(amide_isel_all) > 0:
    return amide_isel_all
  error_msg = "Error in helix definition. It will be skipped.\n"
  error_msg += "String '%s' selected 0 atoms.\n" % sele_str_main_conf
  error_msg += "String '%s' selected 0 atoms.\n" % sele_str_all
  error_msg += "Most likely the definition of SS element does not match model.\n"
  return error_msg


class structure_base(object):

  def as_pdb_str(self):
    return None

  def as_pdb_or_mmcif_str(self):
    return None

  def __str__(self):
    return self.as_pdb_or_mmcif_str()

  @staticmethod
  def convert_resseq(resseq):
    if isinstance(resseq, str):
      return "%4s" % resseq[:4]
    elif isinstance(resseq, int):
      return hy36encode(4, resseq)

  @staticmethod
  def convert_id(id):
    if isinstance(id, str):
      return "%3s" % id[:3]
    elif isinstance(id, int):
      return hy36encode(3, id)

  def get_start_resseq_as_int(self):
    if self.start_resseq is not None:
      if isinstance(self.start_resseq, str):
        return hy36decode(4, self.start_resseq)
      elif isinstance(self.start_resseq, int):
        return self.start_resseq
    return None

  def get_end_resseq_as_int(self):
    if self.end_resseq is not None:
      if isinstance(self.end_resseq, str):
        return hy36decode(4, self.end_resseq)
      elif isinstance(self.end_resseq, int):
        return self.end_resseq
    return None

  @staticmethod
  def id_as_int(id):
    if isinstance(id, str):
      return hy36decode(min(len(id), 3), id)
    elif isinstance(id, int):
      return id
    raise ValueError("String or int is needed")

  @staticmethod
  def icode_to_cif(icode):
    if icode == ' ':
      return '?'
    else:
      return icode

  @staticmethod
  def parse_chain_id(chars):
    # assert len(chars) == 2
    if chars == "":
      # most likely somebody stripped empty chain id
      return " "
    if len(chars) == 1:
      return chars
    if chars == "  ":
      return " "
    else:
      return chars.strip()

  @staticmethod
  def parse_cif_insertion_code(chars):
    if chars == '?' or chars == '.' or chars == '':
      return ' '
    return chars

  @staticmethod
  def filter_helix_records(lines):
    result = []
    if lines is None:
      return result
    for line in lines:
      if line.startswith("HELIX"):
        result.append(line)
    return result

  @staticmethod
  def filter_and_split_sheet_records(lines):
    """Filter out only SHEET records and split them by sheet identifier.
    returns [[lines with equal sheetID], ... ,[lines with equal sheetID]]
    """
    result = []
    if lines is None:
      return result
    current_sh_lines = []
    current_sh_id = ""
    for line in lines:
      if line.startswith("SHEET"):
        line = "%-80s" % line # XXX: flex.split_lines strips each line
        sheet_id = line[11:14]
        if sheet_id == current_sh_id:
          current_sh_lines.append(line)
        else:
          if current_sh_lines != []:
            result.append(current_sh_lines)
            current_sh_lines = []
          current_sh_lines = [line]
          current_sh_id = sheet_id
    if current_sh_lines != []:
      result.append(current_sh_lines)
    return result

  def count_residues(self,hierarchy=None):
    "Count residues in this secondary structure"

    if hierarchy is None:
      raise AssertionError("Require hierarchy for count_residues")

    atom_selection=self.combine_atom_selections(self.as_atom_selections())
    if not atom_selection:
       return 0

    asc=hierarchy.atom_selection_cache()
    sel = asc.selection(string = atom_selection)
    ph=hierarchy.select(sel, copy_atoms=True)
    return ph.overall_counts().n_residues

  def present_in_hierarchy(self, hierarchy):
    start_present = False
    end_present = False
    start_resseq = self.start_resseq if isinstance(self.start_resseq, str) else self.convert_resseq(self.start_resseq)
    end_resseq = self.end_resseq if isinstance(self.end_resseq, str) else self.convert_resseq(self.end_resseq)
    if len(hierarchy.models()) == 0:
      return False
    for chain in hierarchy.models()[0].chains():
      if chain.id == self.start_chain_id:
        for rg in chain.residue_groups():
          resname = rg.atom_groups()[0].resname
          if rg.resseq == start_resseq and self.start_resname == resname:
            start_present = True
          if rg.resseq == end_resseq and self.end_resname == resname:
            end_present = True
          if start_present and end_present:
            return True
    return False

  def _change_residue_numbering_in_place_helper(self, renumbering_dictionary, se=["start", "end"]):
    """ Changes residue numbers and insertion codes in annotations.
    For cases where one needs to renumber hierarchy and keep annotations
    consisten with the new numbering.
    Not for cases where residues removed or added.
    Therefore residue name stays the same.

    Called from derived classes.

    Args:
        renumbering_dictionary (_type_): data structure to match old and new numbering:
        {'chain id':
          {(old resseq, icode):(new resseq, icode)}
        }
        se: labels for start and end. Also can be ["cur", "prev"] in sheet registrations
    """
    for start_end in se:
      chain_id_attr = "%s_chain_id" % start_end
      resseq_attr  = "%s_resseq" % start_end
      icode_attr = "%s_icode" % start_end
      chain_dic = renumbering_dictionary.get(getattr(self, chain_id_attr), None)
      if chain_dic is not None:
          resseq, icode = getattr(self, resseq_attr), getattr(self, icode_attr)
          new_resseq, new_icode = chain_dic.get((resseq, icode), (None, None))
          if new_resseq is not None and new_icode is not None:
              setattr(self, resseq_attr, new_resseq)
              setattr(self, icode_attr, new_icode)


  def count_h_bonds(self,hierarchy=None,
       max_h_bond_length=None,ss_by_chain=False):
    "Count good and poor H-bonds in this hierarchy"
    "This is generic version"
    "This uses the annotation in self"
    "Not appropriate for large structures unless you set ss_by_chain=True"
    "Use instead annotation.count_h_bonds in general."
    if hierarchy is None:
      raise AssertionError("Require hierarchy for count_h_bonds")

    atom_selection=self.combine_atom_selections(self.as_atom_selections())
    if not atom_selection:
       return 0,0

    asc=hierarchy.atom_selection_cache()
    sel = asc.selection(string = atom_selection)
    ph=hierarchy.select(sel, copy_atoms=True)

    from mmtbx.secondary_structure.find_ss_from_ca import \
      find_secondary_structure
    fss=find_secondary_structure(hierarchy=ph,
      user_annotation_text=self.as_pdb_or_mmcif_str(),
      force_secondary_structure_input=True,
      combine_annotations=False,
      ss_by_chain=ss_by_chain,
      max_h_bond_length=max_h_bond_length,out=null_out())
    return fss.number_of_good_h_bonds,fss.number_of_poor_h_bonds


  def is_similar_to(self,other=None,hierarchy=None,
      maximum_length_difference=None, minimum_overlap=None):
    "Return True if this annotation is similar to other."
    "Allow equivalent representations (different alignment atoms with)"
    "equivalent results, backwards order of strands in sheet, different "
    "lengths of strands optional."
    if self.is_same_as(other=other): return True  # always quicker

    if hierarchy is None:
      raise AssertionError("Require hierarchy for is_similar_to")

    # Check for different objects
    if type(self) != type(other): return False

    self_atom_selections=self.as_atom_selections()
    other_atom_selections=other.as_atom_selections()
    if type(self_atom_selections)==type([1,2,3]):
       assert len(self_atom_selections)==1
       self_atom_selections=self_atom_selections[0]
    if type(other_atom_selections)==type([1,2,3]):
       assert len(other_atom_selections)==1
       other_atom_selections=other_atom_selections[0]
    return segments_are_similar(
        atom_selection_1=self_atom_selections,
        atom_selection_2=other_atom_selections,
        hierarchy=hierarchy,
        maximum_length_difference=maximum_length_difference,
        minimum_overlap=minimum_overlap)

  def combine_atom_selections(self,atom_selections_list,require_all=False):
    if require_all:
      joiner="and"
    else:
      joiner="or"

    all_list=[]
    for x in atom_selections_list:
      if type(x)==type([1,2,3]):
        all_list+=x
      else:
        all_list.append(x)
    if not all_list: return ""

    atom_selection="(%s)" %(all_list[0])
    for x in all_list[1:]:
      atom_selection+=" %s (%s) " %(joiner,x)
    return atom_selection

  def has_chains_in_common(self,other=None):
    if lists_have_comment_element(
       self.chains_included(),
       other.chains_included()):
      return True
    else:
      return False

  def overlaps_with(self,other=None,hierarchy=None):
    assert hierarchy
    s1=self.as_atom_selections()
    s2=other.as_atom_selections()

    atom_selection=self.combine_atom_selections([s1,s2],require_all=True)
    if atom_selection:
      asc=hierarchy.atom_selection_cache()
      sel = asc.selection(string = atom_selection)
      ph=hierarchy.select(sel, copy_atoms=True)  #ph = needed part of hierarchy
      if ph.overall_counts().n_residues>0:
        return True
    return False



class annotation(structure_base):
  def __init__(self, helices=None, sheets=None):
    assert (not None in [helices, sheets])
    self.helices = helices
    self.sheets = sheets

  @classmethod
  def resseq_as_int(cls, resseq):
    if resseq is not None:
      if isinstance(resseq, str):
        return hy36decode(4, resseq)
      elif isinstance(resseq, int):
        return resseq
    return None

  @classmethod
  def from_records(cls, records=None, log=None):
    "Initialize annotation from pdb HELIX/SHEET records"
    helices = []
    sheets = []
    for line in cls.filter_helix_records(records):
      try:
        h = pdb_helix.from_pdb_record(line)
      except ValueError:
        print("Bad HELIX record, was skipped:\n%s" % line, file=log)
      else:
        helices.append(h)
    for sh_lines in cls.filter_and_split_sheet_records(records):
      try:
        sh = pdb_sheet.from_pdb_records(sh_lines)
      except ValueError as e:
        print("Bad SHEET records, were skipped:\n", file=log)
        for l in sh_lines:
          print("  %s" % l, file=log)
      else:
        sheets.append(sh)
    return cls(helices=helices, sheets=sheets)

  @classmethod
  def from_cif_block(cls, cif_block, log=None):
    "initialize annotation from cif HELIX/SHEET records"
    helices = []
    serial = 1
    helix_loop = cif_block.get_loop_or_row("_struct_conf")
    if helix_loop is not None:
      for helix_row in helix_loop.iterrows():
        try:
          h = pdb_helix.from_cif_row(helix_row, serial)
          if h is not None:
            helices.append(h)
            serial += 1
        except ValueError:
          print("Bad HELIX records!", file=log)
    sheets = []
    struct_sheet_loop = cif_block.get_loop_or_row("_struct_sheet")
    struct_sheet_order_loop = cif_block.get_loop_or_row("_struct_sheet_order")
    struct_sheet_range_loop = cif_block.get_loop_or_row("_struct_sheet_range")
    struct_sheet_hbond_loop = cif_block.get_loop_or_row("_pdbx_struct_sheet_hbond")
    if (struct_sheet_loop is not None
        and struct_sheet_order_loop is not None
        and struct_sheet_range_loop is not None
        and struct_sheet_hbond_loop is not None):
      # Check that keys are unique
      error_msg_list = []
      dups_sheet = struct_sheet_loop.check_key_is_unique(key_list=["_struct_sheet.id"])
      dups_order = struct_sheet_order_loop.check_key_is_unique(key_list=["_struct_sheet_order.sheet_id",
          "_struct_sheet_order.range_id_1", "_struct_sheet_order.range_id_2"])
      dups_range = struct_sheet_range_loop.check_key_is_unique(key_list=["_struct_sheet_range.sheet_id",
            "_struct_sheet_range.id"])
      dups_hbond = struct_sheet_hbond_loop.check_key_is_unique(key_list=["_pdbx_struct_sheet_hbond.sheet_id",
          "_pdbx_struct_sheet_hbond.range_id_1", "_pdbx_struct_sheet_hbond.range_id_2"])
      for e_str, dups in [("Duplication in _struct_sheet.id: %s", dups_sheet),
          ("Duplication in _struct_sheet_order: %s %s %s", dups_order),
          ("Duplication in _struct_sheet_range: %s %s", dups_range),
          ("Duplication in _pdbx_struct_sheet_hbond: %s %s %s", dups_hbond)]:
        for dup in dups:
          error_msg_list.append(e_str % dup)
      if len(error_msg_list) > 0:
        msg = "Error in sheet definitions:\n  " + "\n  ".join(error_msg_list)
        raise Sorry(msg)
      for sheet_row in struct_sheet_loop.iterrows():
        sheet_id = sheet_row['_struct_sheet.id']
        # we will count number_of_strands in from_cif_rows
        # number_of_strands = int(sheet_row['_struct_sheet.number_strands'])
        try:
          sh = pdb_sheet.from_cif_rows(sheet_id,
              struct_sheet_order_loop, struct_sheet_range_loop,
              struct_sheet_hbond_loop)
        except ValueError as e:
          print("Bad sheet records.\n", file=log)
        else:
          sheets.append(sh)
    return cls(helices=helices, sheets=sheets)

  @classmethod
  def from_phil(cls, phil_helices, phil_sheets, pdb_hierarchy,
      atom_selection_cache=None, log=None):
    helices = []
    sheets = []
    cache = atom_selection_cache
    if cache is None:
      cache = pdb_hierarchy.atom_selection_cache()
    h_atoms = pdb_hierarchy.atoms()
    for i, helix_param in enumerate(phil_helices):
      if helix_param.selection is not None:
        h = pdb_helix.from_phil_params(helix_param, pdb_hierarchy, h_atoms, cache, i, log)
        if h is not None:
          helices.append(h)
    for i, sheet_param in enumerate(phil_sheets):
      if sheet_param.first_strand is not None:
        sh = pdb_sheet.from_phil_params(sheet_param, pdb_hierarchy, h_atoms, cache, log)
        if sh is not None:
          sheets.append(sh)
    return cls(helices=helices, sheets=sheets)

  def deep_copy(self):
    return copy.deepcopy(self)

  def simple_elements(self):
    """ helices and strands"""
    all_elem_list = []
    all_elem_list += self.helices
    for sh in self.sheets:
      all_elem_list += sh.strands
    return all_elem_list

  def filter_annotation(
      self,
      hierarchy=None,
      asc=None,
      remove_short_annotations=True,
      remove_3_10_helices=True,
      remove_empty_annotations=True,
      concatenate_consecutive_helices=True,
      split_helices_with_prolines=True,
      filter_sheets_with_long_hbonds=True
      ):
    """ Returns filtered annotation"""
    result = self.deep_copy()
    if remove_short_annotations:
      result.remove_short_annotations()
    if asc is None and hierarchy is not None:
      asc = hierarchy.atom_selection_cache()
    if remove_3_10_helices:
      result.remove_3_10_helices()
    if hierarchy is not None:
      if asc is None:
        asc = hierarchy.atom_selection_cache()
      if remove_empty_annotations:
        result.remove_empty_annotations(hierarchy)
      if concatenate_consecutive_helices:
        result.concatenate_consecutive_helices(hierarchy, asc)
      if split_helices_with_prolines:
        result.split_helices_with_prolines(hierarchy, asc)
      if filter_sheets_with_long_hbonds:
        result.filter_sheets_with_long_hbonds(hierarchy, asc)
    return result

  def remove_empty_annotations(self, hierarchy):
    # returns annotation of deleted helices and sheets
    h_indeces_to_delete = []
    sh_indeces_to_delete = []
    for i, h in enumerate(self.helices):
      if not h.present_in_hierarchy(hierarchy):
        h_indeces_to_delete.append(i)
    for i, sh in enumerate(self.sheets):
      for st in sh.strands:
        if not st.present_in_hierarchy(hierarchy):
          if i not in sh_indeces_to_delete:
            sh_indeces_to_delete.append(i)
    deleted_helices = []
    deleted_sheets = []
    if len(h_indeces_to_delete) > 0:
      for i in reversed(h_indeces_to_delete):
        deleted_helices.append(self.helices[i])
        del self.helices[i]
    if len(sh_indeces_to_delete) > 0:
      for i in reversed(sh_indeces_to_delete):
        deleted_sheets.append(self.sheets[i])
        del self.sheets[i]
    return annotation(helices=deleted_helices, sheets=deleted_sheets)

  def remove_short_annotations(self,
      helix_min_len=5, sheet_min_len=3, keep_one_stranded_sheets=False):
    # returns nothing
    # Remove short annotations
    h_indeces_to_delete = []
    for i, h in enumerate(self.helices):
      if h.length < helix_min_len:
        h_indeces_to_delete.append(i)
    if len(h_indeces_to_delete) > 0:
      for i in reversed(h_indeces_to_delete):
        del self.helices[i]
    sh_indeces_to_delete = []
    for i, sh in enumerate(self.sheets):
      sh.remove_short_strands(size=sheet_min_len)
      if sh.n_strands < 2 and not keep_one_stranded_sheets:
        sh_indeces_to_delete.append(i)
    if len(sh_indeces_to_delete) > 0:
      for i in reversed(sh_indeces_to_delete):
        del self.sheets[i]

  def concatenate_consecutive_helices(self, hierarchy=None, asc=None):
    # also should divide them
    from mmtbx.command_line.angle import calculate_axes_and_angle_directional
    if asc is None and hierarchy is not None:
      asc = hierarchy.atom_selection_cache()
    concatenations_made = True
    while concatenations_made:
      concatenations_made = False
      new_helices = []
      if self.get_n_helices() < 2:
        return
      new_helices.append(self.helices[0])
      for i in range(1, len(self.helices)):
        # checking angle
        angle = 0
        if hierarchy is not None:
          h1_hierarchy = hierarchy.select(asc.selection(self.helices[i-1].as_atom_selections()[0]))
          h2_hierarchy = hierarchy.select(asc.selection(self.helices[i].as_atom_selections()[0]))
          xrs1 = h1_hierarchy.extract_xray_structure()
          xrs2 = h2_hierarchy.extract_xray_structure()
          if xrs1.sites_cart().size() < 2 or xrs2.sites_cart().size() < 2:
            new_helices.append(self.helices[i])
            continue
          a1, a2, angle = calculate_axes_and_angle_directional(
              xrs1,
              xrs2)
        # print "angle between '%s', '%s': %.2f" % (
        #     self.helices[i-1].as_pdb_str()[7:40],
        #     self.helices[i].as_pdb_str()[7:40],
        #     angle)
        if (new_helices[-1].end_chain_id == self.helices[i].start_chain_id and
            abs(new_helices[-1].get_end_resseq_as_int() - self.helices[i].get_start_resseq_as_int()) < 2):
            # helices are next to each other, so we will either
          if (angle < 15 and
              new_helices[-1].end_resname != "PRO" and
              self.helices[i].start_resname != "PRO"):
            # concatenate
            new_helices[-1].end_resname = self.helices[i].start_resname
            new_helices[-1].set_end_resseq(self.helices[i].get_end_resseq_as_int())
            new_helices[-1].end_icode = self.helices[i].start_icode
            new_helices[-1].length = (new_helices[-1].get_end_resseq_as_int() -
              new_helices[-1].get_start_resseq_as_int() + 1)
            concatenations_made = True
          else:
            # or separate them by moving borders by 1 residue
            h1_rg = [x for x in h1_hierarchy.residue_groups()][-2]
            h2_rgs = [x for x in h2_hierarchy.residue_groups()]
            if len(h2_rgs) > 3:
              # short one could creep here
              h2_rg = [x for x in h2_hierarchy.residue_groups()][1]

              new_helices[-1].end_resname = h1_rg.atom_groups()[0].resname
              new_helices[-1].set_end_resseq(h1_rg.resseq)
              new_helices[-1].end_icode = h1_rg.icode
              new_helices[-1].length -= 1

              self.helices[i].start_resname = h2_rg.atom_groups()[0].resname
              self.helices[i].set_start_resseq(h2_rg.resseq)
              self.helices[i].start_icode = h2_rg.icode
              self.helices[i].length -= 1

              new_helices.append(self.helices[i])

        else:
          new_helices.append(self.helices[i])
      self.helices = new_helices

  def split_helices_with_prolines(self, hierarchy, asc=None):
    # If hierarchy is present: break helices with PRO inside
    if asc is None:
      asc = hierarchy.atom_selection_cache()
    new_helices = []
    for i,h in enumerate(self.helices):
      selected_h = hierarchy.select(asc.selection(h.as_atom_selections()[0]))
      i_pro_res = []
      rgs = selected_h.only_model().only_chain().residue_groups()
      for j,rg in enumerate(rgs):
        if rg.atom_groups()[0].resname.strip() == "PRO":
          i_pro_res.append(j)
      for j in i_pro_res:
        if j > 0:
          h1 = h.deep_copy()
          h1.end_resname = rgs[j-1].atom_groups()[0].resname.strip()
          h1.end_resseq = rgs[j-1].resseq
          h1.end_icode = rgs[j-1].icode
          h1.length = j
          h1.erase_hbond_list()
          h.start_resname = rgs[j].atom_groups()[0].resname.strip()
          h.start_resseq = rgs[j].resseq
          h.start_icode = rgs[j].icode
          h.length = len(rgs) - j
          h.erase_hbond_list()
          if h1.length > 2:
            new_helices.append(h1)
      if h.length > 3:
        new_helices.append(h)
    for i, h in enumerate(new_helices):
      h.set_new_serial(serial=i+1, adopt_as_id=True)
    self.helices=new_helices

  def filter_sheets_with_long_hbonds(self, hierarchy, asc=None):
    """ Currently using from_ca method becuase ksdssp works wrong with
    partial model. In some cases it cannot detect chain break and extends
    strand e.g. from 191 to 226 (in 1ubf) when the strand ends at 194th residue."""
    assert hierarchy is not None, "Cannot measure bonds w/o hierarchy!"
    from mmtbx.secondary_structure.ss_validation import gather_ss_stats
    from mmtbx.secondary_structure import manager as ss_manager
    from mmtbx.secondary_structure import sec_str_master_phil
    if asc is None:
      asc = hierarchy.atom_selection_cache()
    ss_stats_obj = gather_ss_stats(pdb_h=hierarchy)
    new_sheets = []
    sh_indeces_to_delete = []
    asc = asc
    if asc is None:
      asc = hierarchy.atom_selection_cache()
    # prepare atom_selections for sheets
    sh_atom_selections = []
    for sh in self.sheets:
      atom_selections = sh.as_atom_selections()
      total_selection = " or ".join(["(%s)" % x for x in atom_selections])
      sel = asc.selection(total_selection)
      sh_atom_selections.append(sel)
    for i, sh in enumerate(self.sheets):
      if i not in sh_indeces_to_delete:
        sh_tuple = ([], [sh])
        (n_hbonds, n_bad_hbonds, n_mediocre_hbonds, hb_lens,
            n_outliers, n_wrong_region) = ss_stats_obj(sh_tuple)
        if n_bad_hbonds > 0:
          # SHEET is bad, work with it
          sh_indeces_to_delete.append(i)
          # check other sheets for intersection with this one
          for j in range(i+1, self.get_n_sheets()):
            intersect = sh_atom_selections[i].deep_copy()
            if j not in sh_indeces_to_delete: # don't check deleted sheets
              intersect &= sh_atom_selections[j]
              if not intersect.all_eq(False):
                sh_indeces_to_delete.append(j)
                sh_atom_selections[i] |= sh_atom_selections[j]
          # find new sheet structure for this sheet
          sh_hierarchy = hierarchy.select(sh_atom_selections[i])
          # sh_hierarchy.write_pdb_file(file_name="sheet_%d.pdb"%i)
          n_rgs = len(list(sh_hierarchy.residue_groups()))
          ss_def_pars = sec_str_master_phil.extract()
          ss_def_pars.secondary_structure.protein.search_method="from_ca"
          ss_m = ss_manager(
              pdb_hierarchy=sh_hierarchy,
              params=ss_def_pars.secondary_structure,
              log=null_out())
          fresh_sheets = ss_m.actual_sec_str.sheets
          # checking for bug occuring in 4a7h where one strand happens to be
          # too long
          # ksdssp_bug = False
          # for f_sh in fresh_sheets:
          #   for f_strand in f_sh.strands:
          #     if f_strand.get_end_resseq_as_int() - f_strand.get_start_resseq_as_int() > n_rgs:
          #       sh_hierarchy.write_pdb_file(file_name="ksdssp_failure.pdb")
          #       ksdssp_bug = True
          #       break
          #       # raise Sorry("It is 4a7h or ksdssp failed on another structure.")
          # if ksdssp_bug:
          #   ss_def_pars = sec_str_master_phil.extract()
          #   ss_def_pars.secondary_structure.protein.search_method="from_ca"
          #   ss_m = ss_manager(
          #       pdb_hierarchy=sh_hierarchy,
          #       params=ss_def_pars.secondary_structure,
          #       log=null_out())
          new_sheets += ss_m.actual_sec_str.sheets
    if len(sh_indeces_to_delete) > 0:
      for i in sorted(sh_indeces_to_delete, reverse=True):
        del self.sheets[i]
    self.sheets = self.sheets + new_sheets
    if len(new_sheets) > 0:
      self.reset_sheet_ids()
    self.remove_short_annotations()

  def reset_sheet_ids(self):
    import itertools, string
    ch = string.digits+string.ascii_uppercase
    ids = [''.join(x) for x in itertools.product(ch, repeat = 1)]
    if self.get_n_sheets() > len(ids):
      ids = [''.join(x) for x in itertools.product(ch, repeat = 2)]
    if self.get_n_sheets() > len(ids):
      ids = [''.join(x) for x in itertools.product(ch, repeat = 3)]
    if self.get_n_sheets() > len(ids):
      # Too many sheets for PDB format (max 3 chars for sheet id)
      raise Sorry("Too many sheets in annotations: %d" % self.get_n_sheets())
    for i, sh in enumerate(self.sheets):
      sh.sheet_id = ids[i]

  def renumber_helices(self):
    i = 1
    for helix in self.helices:
      helix.set_new_serial(i, adopt_as_id=True)
      i += 1

  def renumber_sheets(self):
    i = 1
    for sheet in self.sheets:
      sheet.sheet_id = self.convert_id(i)
      for strand in sheet.strands:
        strand.sheet_id = "%s" % self.convert_id(i)
      i += 1

  def renumber_helices_and_sheets(self):
    self.renumber_sheets()
    self.renumber_helices()

  def multiply_to_asu_2(self, chain_ids_dict):
    def get_new_chain_id(old_chain_id, n_copy):
      if n_copy == 0 and old_chain_id in chain_ids_dict:
        return old_chain_id
      return chain_ids_dict[old_chain_id][n_copy-1]
    n_copies = len(list(chain_ids_dict.values())[0]) + 1
    new_helices = []
    new_sheets = []
    new_h_serial = 0
    new_sheet_id = 0
    for n_copy in range(n_copies):
      for helix in self.helices:
        new_helix = copy.deepcopy(helix)
        new_helix.erase_hbond_list()
        try:
          new_h_serial += 1
          new_helix.set_new_chain_ids(
              get_new_chain_id(new_helix.start_chain_id, n_copy))
          new_helix.set_new_serial(new_h_serial, adopt_as_id=True)
          new_helices.append(new_helix)
        except KeyError:
          continue
      for sheet in self.sheets:
        new_sheet = copy.deepcopy(sheet)
        new_sheet_id += 1
        new_sheet.sheet_id = self.convert_id(new_sheet_id)
        new_sheet.erase_hbond_list()
        try:
          for strand in new_sheet.strands:
            strand.set_new_chain_ids(
                get_new_chain_id(strand.start_chain_id, n_copy))
            strand.sheet_id = "%s" % self.convert_id(new_sheet_id)
          for reg in new_sheet.registrations:
            if reg is not None:
              reg.set_new_cur_chain_id(
                  get_new_chain_id(reg.cur_chain_id, n_copy))
              reg.set_new_prev_chain_id(
                  get_new_chain_id(reg.prev_chain_id, n_copy))
        except KeyError:
          continue
        new_sheets.append(new_sheet)
    self.helices = new_helices
    self.sheets = new_sheets

  def remove_1hb_helices(self):
    filtered_helices = []
    for h in self.helices:
      if h.get_n_maximum_hbonds() <= 1:
        continue
      else:
        filtered_helices.append(h)
    self.helices = filtered_helices

  def remove_3_10_helices(self):
    filtered_helices = []
    for h in self.helices:
      if h.get_class_as_int() == 5:
        continue
      else:
        filtered_helices.append(h)
    self.helices = filtered_helices

  def change_residue_numbering_in_place(self, renumbering_dictionary):
    """ Changes residue numbers and insertion codes in annotations.
    For cases where one needs to renumber hierarchy and keep annotations
    consisten with the new numbering.
    Not for cases where residues removed or added.
    Therefore residue name stays the same.

    Args:
        renumbering_dictionary (_type_): data structure to match old and new numbering:
        {'chain id':
          {(old resseq, icode):(new resseq, icode)}
        }
    """
    for h in self.helices:
      h.change_residue_numbering_in_place(renumbering_dictionary)
    for sh in self.sheets:
      sh.change_residue_numbering_in_place(renumbering_dictionary)

  def as_cif_loops(self):
    """
    Returns list of loops needed to represent SS annotation. The first for
    helix, others for sheets. If there's no helix, there will be only sheet
    loops. Or empty list if there's nothing to output."""

    loops = []
    if self.get_n_helices() > 0:
      helix_info_prefix = '_struct_conf.'
      helix_info_cif_names = (
            'conf_type_id',
            'id',
            'pdbx_PDB_helix_id',
            'beg_label_comp_id',
            'beg_label_asym_id',
            'beg_label_seq_id',
            'pdbx_beg_PDB_ins_code',
            'end_label_comp_id',
            'end_label_asym_id',
            'end_label_seq_id',
            'pdbx_end_PDB_ins_code',
            'pdbx_PDB_helix_class',
            'details',
            'pdbx_PDB_helix_length')
      helix_loop = iotbx.cif.model.loop(header=(
          ["%s%s" % (helix_info_prefix, x) for x in helix_info_cif_names]))
      for h in self.helices:
        h_dict = h.as_cif_dict()
        row = []
        for cif_name in helix_info_cif_names:
          v = h_dict.get(cif_name, '?')
          row.append(v)
        helix_loop.add_row(row)
      loops.append(helix_loop)
      type_prefix = "_struct_conf_type."
      type_names = ["id", "criteria", "reference"]
      conf_type_ids = set(helix_loop["_struct_conf.conf_type_id"])
      type_loop = iotbx.cif.model.loop(header=(
          ["%s%s" % (type_prefix, x) for x in type_names]))
      for conf_type in conf_type_ids:
        type_loop.add_row([conf_type, "?", "?"])
      loops.append(type_loop)
    if self.get_n_sheets() > 0:
      struct_sheet_loop = iotbx.cif.model.loop(header=(
          '_struct_sheet.id',
          '_struct_sheet.type',
          '_struct_sheet.number_strands',
          '_struct_sheet.details'))
      struct_sheet_order_loop = iotbx.cif.model.loop(header=(
          '_struct_sheet_order.sheet_id',
          '_struct_sheet_order.range_id_1',
          '_struct_sheet_order.range_id_2',
          '_struct_sheet_order.offset',
          '_struct_sheet_order.sense'))
      struct_sheet_range_loop = iotbx.cif.model.loop(header=(
          '_struct_sheet_range.sheet_id',
          '_struct_sheet_range.id',
          '_struct_sheet_range.beg_label_comp_id',
          '_struct_sheet_range.beg_label_asym_id',
          '_struct_sheet_range.beg_label_seq_id',
          '_struct_sheet_range.pdbx_beg_PDB_ins_code',
          '_struct_sheet_range.end_label_comp_id',
          '_struct_sheet_range.end_label_asym_id',
          '_struct_sheet_range.end_label_seq_id',
          '_struct_sheet_range.pdbx_end_PDB_ins_code'))
      struct_sheet_hbond_loop = iotbx.cif.model.loop(header=(
          '_pdbx_struct_sheet_hbond.sheet_id',
          '_pdbx_struct_sheet_hbond.range_id_1',
          '_pdbx_struct_sheet_hbond.range_id_2',
          '_pdbx_struct_sheet_hbond.range_1_label_atom_id',
          '_pdbx_struct_sheet_hbond.range_1_label_comp_id',
          '_pdbx_struct_sheet_hbond.range_1_label_asym_id',
          '_pdbx_struct_sheet_hbond.range_1_label_seq_id',
          '_pdbx_struct_sheet_hbond.range_1_PDB_ins_code',
          '_pdbx_struct_sheet_hbond.range_2_label_atom_id',
          '_pdbx_struct_sheet_hbond.range_2_label_comp_id',
          '_pdbx_struct_sheet_hbond.range_2_label_asym_id',
          '_pdbx_struct_sheet_hbond.range_2_label_seq_id',
          '_pdbx_struct_sheet_hbond.range_2_PDB_ins_code'))
      for sh in self.sheets:
        sh_dict = sh.as_cif_dict()
        # parse it here and toss into loops
        struct_sheet_loop.add_row((
            sh_dict['_struct_sheet.id'],
            sh_dict['_struct_sheet.type'],
            sh_dict['_struct_sheet.number_strands'],
            sh_dict['_struct_sheet.details']))
        for struct_sheet_loop_row in zip(
            sh_dict['_struct_sheet_order.sheet_id'],
            sh_dict['_struct_sheet_order.range_id_1'],
            sh_dict['_struct_sheet_order.range_id_2'],
            sh_dict['_struct_sheet_order.offset'],
            sh_dict['_struct_sheet_order.sense']):
          struct_sheet_order_loop.add_row(struct_sheet_loop_row)
        for struct_sheet_range_row in zip(
            sh_dict['_struct_sheet_range.sheet_id'],
            sh_dict['_struct_sheet_range.id'],
            sh_dict['_struct_sheet_range.beg_label_comp_id'],
            sh_dict['_struct_sheet_range.beg_label_asym_id'],
            sh_dict['_struct_sheet_range.beg_label_seq_id'],
            sh_dict['_struct_sheet_range.pdbx_beg_PDB_ins_code'],
            sh_dict['_struct_sheet_range.end_label_comp_id'],
            sh_dict['_struct_sheet_range.end_label_asym_id'],
            sh_dict['_struct_sheet_range.end_label_seq_id'],
            sh_dict['_struct_sheet_range.pdbx_end_PDB_ins_code']):
          struct_sheet_range_loop.add_row(struct_sheet_range_row)
        for struct_sheet_hbond_row in zip(
            sh_dict['_pdbx_struct_sheet_hbond.sheet_id'],
            sh_dict['_pdbx_struct_sheet_hbond.range_id_1'],
            sh_dict['_pdbx_struct_sheet_hbond.range_id_2'],
            sh_dict['_pdbx_struct_sheet_hbond.range_1_label_atom_id'],
            sh_dict['_pdbx_struct_sheet_hbond.range_1_label_comp_id'],
            sh_dict['_pdbx_struct_sheet_hbond.range_1_label_asym_id'],
            sh_dict['_pdbx_struct_sheet_hbond.range_1_label_seq_id'],
            sh_dict['_pdbx_struct_sheet_hbond.range_1_PDB_ins_code'],
            sh_dict['_pdbx_struct_sheet_hbond.range_2_label_atom_id'],
            sh_dict['_pdbx_struct_sheet_hbond.range_2_label_comp_id'],
            sh_dict['_pdbx_struct_sheet_hbond.range_2_label_asym_id'],
            sh_dict['_pdbx_struct_sheet_hbond.range_2_label_seq_id'],
            sh_dict['_pdbx_struct_sheet_hbond.range_2_PDB_ins_code']):
          struct_sheet_hbond_loop.add_row(struct_sheet_hbond_row)
      loops.append(struct_sheet_loop)
      loops.append(struct_sheet_order_loop)
      loops.append(struct_sheet_range_loop)
      loops.append(struct_sheet_hbond_loop)
    return loops

  def as_mmcif_str(self, data_block_name=None):
    cif_object = iotbx.cif.model.cif()
    if data_block_name is None:
      data_block_name = "phenix"
    cif_object[data_block_name] = self.as_cif_block()
    from six.moves import cStringIO as StringIO
    f = StringIO()
    cif_object.show(out = f)
    return f.getvalue()

  def as_cif_block(self):
    cif_block = iotbx.cif.model.block()
    ss_cif_loops = self.as_cif_loops()
    for loop in ss_cif_loops:
      cif_block.add_loop(loop)
    return cif_block

  def fits_in_pdb_format(self):
    for helix in self.helices :
      if (not helix.fits_in_pdb_format()):  return False
    for sheet in self.sheets :
      if (not sheet.fits_in_pdb_format()):  return False
    return True

  def as_pdb_str(self):
    records = []
    for helix in self.helices :
      records.append(helix.as_pdb_str())
    for sheet in self.sheets :
      records.append(sheet.as_pdb_str())
    return "\n".join(records)

  def as_pdb_or_mmcif_str(self, target_format = 'pdb'):
    # Return str in target format if possible, otherwise in mmcif
    if target_format == 'pdb' and self.fits_in_pdb_format():
      return self.as_pdb_str()
    else:
      return self.as_mmcif_str()

  def as_restraint_groups(self, log=sys.stdout, prefix_scope="",
      add_segid=None):
    phil_strs = []
    for helix in self.helices :
      helix_phil = helix.as_restraint_group(log, prefix_scope, add_segid)
      if helix_phil is not None :
        phil_strs.append(helix_phil)
    for sheet in self.sheets :
      sheet_phil = sheet.as_restraint_group(log, prefix_scope, add_segid)
      if sheet_phil is not None :
        phil_strs.append(sheet_phil)
    return "\n".join(phil_strs)

  def as_atom_selections(self, add_segid=None):
    selections = []
    for helix in self.helices :
      try :
        selections.extend(helix.as_atom_selections(add_segid=add_segid))
      except RuntimeError as e :
        pass
    for sheet in self.sheets :
      selections.extend(sheet.as_atom_selections(add_segid=add_segid))
    return selections

  def overall_selection(self,add_segid=None,trim_ends_by=None):
    result = ""
    result = self.overall_helices_selection(add_segid=add_segid,trim_ends_by=trim_ends_by)
    s_s = self.overall_sheets_selection(add_segid=add_segid,trim_ends_by=trim_ends_by)
    if len(s_s) > 0 and len(result) > 0:
      result += " or %s" % s_s
      return result
    if len(result) == 0:
      return s_s
    else:
      return result

  def overall_helices_selection(self, add_segid=None,trim_ends_by=None):
    selections = []
    for helix in self.helices:
      try :
        selections.extend(helix.as_atom_selections(add_segid=add_segid,
         trim_ends_by=trim_ends_by))
      except RuntimeError as e :
        pass
    return "(" + ") or (".join(selections) + ")" if len(selections) > 0 else ""

  def overall_sheets_selection(self, add_segid=None,trim_ends_by=None):
    selections = []
    for sheet in self.sheets:
      try:
        selections.extend(sheet.as_atom_selections(add_segid=add_segid,
          trim_ends_by=trim_ends_by))
      except RuntimeError as e :
        pass
    return "(" + ") or (".join(selections) + ")" if len(selections) > 0 else ""


  def overall_helix_selection(self,
                              add_segid=None,
                              helix_types_selection=None,
                              n_hbonds_selection=None,
                              ):
    selections = []
    for helix in self.helices:
      if helix_types_selection is not None:
        if helix.helix_class not in helix_types_selection: continue
      if n_hbonds_selection is not None:
        if helix.get_n_maximum_hbonds()<n_hbonds_selection: continue
      try :
        selections.extend(helix.as_atom_selections(add_segid=add_segid))
      except RuntimeError as e :
        pass
    return "(" + ") or (".join(selections) + ")"

  def overall_sheet_selection(self, add_segid=None):
    selections = []
    for sheet in self.sheets:
      try:
        selections.extend(sheet.as_atom_selections(add_segid=add_segid))
      except RuntimeError as e :
        pass
    return "(" + ") or (".join(selections) + ")"

  def as_bond_selections(self):
    assert 0, "Probably is not used anywhere"
    bonded_atoms = self.extract_h_bonds(params)
    selections = []
    for (atom1, atom2) in bonded_atoms :
      selection_1 = "name %s and chain '%s' and resseq %d and icode '%s'" % (
        atom1.name, atom1.chain_id, atom1.resseq, atom1.icode)
      selection_2 = "name %s and chain '%s' and resseq %d and icode '%s'" % (
        atom2.name, atom2.chain_id, atom2.resseq, atom2.icode)
      selections.append((selection_1, selection_2))
    return selections

  def get_n_helices(self):
    return len(self.helices)

  def get_n_sheets(self):
    return len(self.sheets)

  def is_empty(self):
    return self.get_n_helices() + self.get_n_sheets() == 0

  def get_n_helix_residues(self):
    result = 0
    for h in self.helices:
      result += h.length
    return result

  def get_n_sheet_residues(self):
    result = 0
    for sh in self.sheets:
      result += sh.get_approx_size()
    return result

  def get_n_defined_hbonds(self):
    n_hb = 0
    if self.get_n_helices() > 0:
      for h in self.helices:
        n_hb += h.get_n_defined_hbonds()
    if self.get_n_sheets() > 0:
      for sh in self.sheets:
        n_hb += sh.get_n_defined_hbonds()
    return n_hb

  def split_sheets(self):
    "Split all multi-strand sheets into 2-strand sheets"
    new_sheets=[]
    for sheet in self.sheets:
      new_sheets+=sheet.split(starting_sheet_id_number=len(new_sheets)+1)
    return annotation(
      helices=copy.deepcopy(self.helices),
      sheets=new_sheets)

  def merge_sheets(self):
    "Group 2-strand sheets into larger sheets if identical component strands"
    # Assumes that all the sheets are non-overlapping
    # First run sheet.split() on all sheets or split_sheets on the annotation
    #  as this requires 2-strand sheets (not 3 or more)

    sheet_pointer_0={}
    sheet_pointer_1={}
    all_strands=[]
    for sheet in self.sheets:
      assert len(sheet.strands)==2 and len(sheet.registrations)==2
      strand_0_text=sheet.strands[0].as_atom_selections()
      strand_1_text=sheet.strands[1].as_atom_selections()
      sheet_pointer_0[strand_0_text]=sheet
      sheet_pointer_1[strand_1_text]=sheet
      if not sheet.strands[0] in all_strands:
        all_strands.append(sheet.strands[0])
      if not sheet.strands[1] in all_strands:
        all_strands.append(sheet.strands[1])

    used_strand_selections=[] # strands that have been used
    # Start with a strand that is in position 1 only (if any) and work through
    #   all strands in that sheet. Then on second iteration take all unused
    new_sheets=[]
    for iter in [0,1]:
      for strand in all_strands:
        key=strand.as_atom_selections()
        if key in used_strand_selections: continue
        if key in sheet_pointer_0.keys() and (
            iter==1 or not key in sheet_pointer_1.keys()):
          used_strand_selections.append(key)
          working_sheet=copy.deepcopy(sheet_pointer_0.get(key))
          new_sheets.append(working_sheet)  # now we will extend this sheet

          second_strand=working_sheet.strands[1] # second strand
          next_key=second_strand.as_atom_selections()
          if next_key in used_strand_selections: continue

          while next_key: # points to next sheet to merge 2nd strand
            # find sheet where next_strand is the first strand
            next_sheet=sheet_pointer_0.get(next_key)
            if not next_sheet: break
            used_strand_selections.append(next_key)

            next_strand=copy.deepcopy(next_sheet.strands[1])
            next_registration=copy.deepcopy(next_sheet.registrations[1])

            working_sheet.add_strand(next_strand)
            working_sheet.add_registration(next_registration)

            next_key=next_strand.as_atom_selections()
            if next_key in used_strand_selections: break
    # Now renumber the new sheets
    sheet_number=0
    for sheet in new_sheets:
      sheet_number+=1
      sheet.n_strands=len(sheet.strands)
      sheet.sheet_id="%d" %(sheet_number)
      strand_number=0
      for strand in sheet.strands:
        strand_number+=1
        strand.sheet_id=sheet.sheet_id
        strand.strand_id=strand_number

    # Now new_sheets has strands arranged in sheets.
    new_annotation=copy.deepcopy(self)
    new_annotation.sheets=new_sheets
    return new_annotation

  def add_helices_and_sheets_simple(self, other_annot):
    self.helices += other_annot.helices
    self.sheets += other_annot.sheets
    self.renumber_helices_and_sheets()

  def combine_annotations(self,hierarchy=None,other=None,
    keep_self=None,
    max_h_bond_length=None,
    maximize_h_bonds=True,
    maximum_length_difference=None,
    minimum_overlap=None,
    out=sys.stdout):
    "Create new annotation that is combination of self and other"

    # Hierarchy is required in order to identify what parts of annotation are
    #   in common for self and other and to identify which are better

    # Default behavior (keep_self=None):
    # For each annotation, keep the version that has the most information;
    #   if equal, keep self.  Then add things from other that are not present in
    #   self. Information is number of H-bonds (default) or total residues in
    #   secondary_structure.

    # Optional behavior (keep_self=True):
    # Keep all of self and add things from other not present in self.

    self.keep_self=keep_self
    self.maximize_h_bonds=maximize_h_bonds
    self.maximum_length_difference=maximum_length_difference
    self.minimum_overlap=minimum_overlap
    self.max_h_bond_length=max_h_bond_length

    # Select just the part of hierarchy we will need
    assert hierarchy
    s1=self.as_atom_selections()
    s2=other.as_atom_selections()
    atom_selection=self.combine_atom_selections([s1,s2])
    if not atom_selection:
      return None # nothing to do
    asc=hierarchy.atom_selection_cache()
    sel = asc.selection(string = atom_selection)
    ph=hierarchy.select(sel, copy_atoms=True)  #ph = needed part of hierarchy
    # Split all sheets into pairs
    a1=self.split_sheets()
    a2=other.split_sheets()

    # Find all pairs of overlapping annotations and all unique annotations in
    #  self and other

    #print >>out,"\nFinding matching and unique helices:"

    helices=self.get_unique_set(
       a1.helices,a2.helices,hierarchy=hierarchy,out=out)
    #print >>out,"\nFinding matching and unique sheets:"
    sheets=self.get_unique_set(a1.sheets,a2.sheets,hierarchy=hierarchy,out=out)

    new_annotation=annotation(sheets=sheets,helices=helices)
    new_annotation=new_annotation.merge_sheets()
    return new_annotation

  def score_pair(self,h1,h2,
      maximize_h_bonds=None,
      hierarchy=None,
      max_h_bond_length=None,
      rescore_if_zero_scores=True,
      poor_h_bond_weight=0.5,
      keep_self=None):

      assert h1 # h2 can be None
      score_1=None
      score_2=None
      if maximize_h_bonds:
        n_good_1,n_poor_1=h1.count_h_bonds(hierarchy=hierarchy,
          max_h_bond_length=self.max_h_bond_length)
        score_1=n_good_1+poor_h_bond_weight*n_poor_1
        if h2:
          n_good_2,n_poor_2=h2.count_h_bonds(hierarchy=hierarchy,
            max_h_bond_length=self.max_h_bond_length)
          score_2=n_good_2+poor_h_bond_weight*n_poor_2
        else:
          score_2=None

      if (score_1 is None and score_2 is None ) or \
         (rescore_if_zero_scores and score_1==0 and score_2==0): #nothing yet
        if keep_self:
          score_1=1
          score_2=0
        else: # take the one with more residues in secondary structure
          score_1=h1.count_residues(hierarchy=hierarchy)
          if h2:
            score_2=h2.count_residues(hierarchy=hierarchy)
          else:
            score_2=None
      return score_1,score_2

  def remove_overlapping_annotations(self,hierarchy=None,
     maximize_h_bonds=True,max_h_bond_length=None,
     maximum_length_difference=None,minimum_overlap=None):
    if not hierarchy:
      raise Sorry("Need hierarchy for remove_overlapping_annotations")
    self.maximum_length_difference=maximum_length_difference
    self.minimum_overlap=minimum_overlap
    self.maximize_h_bonds=maximize_h_bonds
    self.max_h_bond_length=max_h_bond_length

    self.sheets=self.split_sheets().sheets
    new_sheets=self.select_best_overlapping_annotations(hierarchy=hierarchy,
      sheet_or_helix_list=self.sheets)
    new_helices=self.select_best_overlapping_annotations(hierarchy=hierarchy,
      sheet_or_helix_list=self.helices)
    new_annotation=annotation(
      helices=copy.deepcopy(new_helices),
      sheets=copy.deepcopy(new_sheets))
    return new_annotation.merge_sheets()

  def select_best_overlapping_annotations(self,hierarchy=None,
      sheet_or_helix_list=None):
    assert hierarchy
    score_list=[]
    for s1 in sheet_or_helix_list:
      score_1,score_2=self.score_pair(s1,None,
         maximize_h_bonds=self.maximize_h_bonds,
         hierarchy=hierarchy,
         max_h_bond_length=self.max_h_bond_length,
         keep_self=None)
      score_list.append([score_1,s1])
    score_list = sorted(score_list, key = lambda s: s[0], reverse = True)

    remove_list=[]
    keep_list=[]
    for score1,s1 in score_list: # remove things that overlap and lower score
      if not s1 in remove_list and not s1 in keep_list:
        keep_list.append(s1)
      for score2,s2 in score_list:
        if s2==s1 or s2 in keep_list or s2 in remove_list: continue
        if not s1.has_chains_in_common(other=s2): continue
        if s1.overlaps_with(other=s2,hierarchy=hierarchy):
          remove_list.append(s2)
    return keep_list

  def get_unique_set(self,h1_list,h2_list,hierarchy=None,
     out=sys.stdout):
    # Sheets should be split before using get_unique_set
    pairs=[]
    overlapping_but_not_matching_pairs=[]
    unique_h1=[]
    unique_h2=[]
    used_h1=[]
    used_h2=[]
    for h1 in h1_list:
      for h2 in h2_list:
        if not h2.has_chains_in_common(other=h1):
          pass # nothting to do
        elif h2.is_similar_to(other=h1,hierarchy=hierarchy,
            maximum_length_difference=self.maximum_length_difference,
            minimum_overlap=self.minimum_overlap):
          if not h1 in used_h1: used_h1.append(h1)
          if not h2 in used_h2: used_h2.append(h2)
          pairs.append([h1,h2])
        elif h2.overlaps_with(other=h1,hierarchy=hierarchy):
          overlapping_but_not_matching_pairs.append([h1,h2])
          if not h1 in used_h1: used_h1.append(h1)
          if not h2 in used_h2: used_h2.append(h2)
    for h1 in h1_list:
      if not h1 in used_h1: unique_h1.append(h1)
    for h2 in h2_list:
      if not h2 in used_h2: unique_h2.append(h2)

    if pairs:
      #print >>out,"\nMatching pairs:"
      for [h1,h2] in pairs:

        score_1,score_2=self.score_pair(h1,h2,
          maximize_h_bonds=self.maximize_h_bonds,
          hierarchy=hierarchy,
          max_h_bond_length=self.max_h_bond_length,
          rescore_if_zero_scores=True,
          keep_self=self.keep_self)
        #print >>out,"SELF : %7.1f\n%s" %(score_1,h1.as_pdb_str())
        #print >>out,"OTHER: %7.1f\n%s" %(score_2,h2.as_pdb_str())

    if overlapping_but_not_matching_pairs:
      #print >>out,"\nOverlapping non-matching pairs:"
      for [h1,h2] in overlapping_but_not_matching_pairs:
        score_1,score_2=self.score_pair(h1,h2,
          maximize_h_bonds=self.maximize_h_bonds,
          hierarchy=hierarchy,
          rescore_if_zero_scores=True,
          max_h_bond_length=self.max_h_bond_length,
          keep_self=self.keep_self)
        #print >>out,"SELF : %7.1f\n%s" %(score_1,h1.as_pdb_str())
        #print >>out,"OTHER: %7.1f\n%s\n" %(score_2,h2.as_pdb_str())

    if unique_h1 or unique_h2:
      #print >>out,"\nNon-matching:"
      for h1 in unique_h1:
        score_1,score_2=self.score_pair(h1,None,
          maximize_h_bonds=self.maximize_h_bonds,
          rescore_if_zero_scores=True,
          hierarchy=hierarchy,
          max_h_bond_length=self.max_h_bond_length,
          keep_self=self.keep_self)
        #print >>out,"SELF : %7.1f\n%s" %(score_1,h1.as_pdb_str())
      for h2 in unique_h2:
        score_1,score_2=self.score_pair(h2,None,
          maximize_h_bonds=self.maximize_h_bonds,
          hierarchy=hierarchy,
          rescore_if_zero_scores=True,
          max_h_bond_length=self.max_h_bond_length,
          keep_self=self.keep_self)
        #print >>out,"OTHER: %7.1f\n%s" %(score_1,h2.as_pdb_str())

    unique=unique_h1+unique_h2

    # Now take the best (or self if desired) of pairs and add on unique
    final_list=[]
    for [h1,h2] in pairs+overlapping_but_not_matching_pairs:
      score_1,score_2=self.score_pair(h1,h2,
        maximize_h_bonds=self.maximize_h_bonds,
        rescore_if_zero_scores=True,
        hierarchy=hierarchy,
        max_h_bond_length=self.max_h_bond_length,
        keep_self=self.keep_self)

      #print "score 1 and 2: ",score_1,score_2
      if score_1>=score_2:
        if not h1 in final_list:final_list.append(h1)
        if h2 in final_list:final_list.remove(h2)
      else:
        if not h2 in final_list:final_list.append(h2)
        if h1 in final_list:final_list.remove(h1)

    for h in unique:
      final_list.append(h)

    return final_list

  def is_comparable_to(self,other=None):

    # Just see if self and other are remotely comparable

    if other is None: return False,None,None

    if type(self) != type(other):
      return False,None,None # Check for different objects

    # Check for completely different helices
    if len(self.helices) != len(other.helices):
      return False,None,None

    # Split all sheets so that everything is comparable
    a1=self.split_sheets()
    a2=other.split_sheets()
    # Check for completely different sheets
    if len(a1.sheets) != len(a2.sheets):
      return False,None,None

    return True,a1,a2

  def chains_included(self):
    "Report list of all chains involved "
    chain_list=[]
    for x in self.helices+self.sheets:
      for c in x.chains_included():
        if not x in chain_list: chain_list.append(x)
    return chain_list

  def overlaps_with(self,other=None,hierarchy=None):
    "Returns True if any element of the annotation overlap"

    a1=self.split_sheets()
    a2=other.split_sheets()
    assert hierarchy
    for h1 in a1.helices:
      for h2 in a2.helices:
        if h1.has_chains_in_common(other=h2) and \
          h1.overlaps_with(other=h2,hierarchy=hierarchy):
          return True

    for s1 in a1.sheets:
      for s2 in a2.sheets:
        if s1.has_chains_in_common(other=s2) and \
           s1.overlaps_with(other=s2,hierarchy=hierarchy):
          return True

    return False

  def is_similar_to(self,other=None,hierarchy=None,
      maximum_length_difference=None, minimum_overlap=None):
    "Return True if this annotation is similar to other."
    "Allow equivalent representations (different alignment atoms with)"
    "equivalent results, backwards order of strands in sheet, different "
    "lengths of strands optional."

    if self.is_same_as(other=other): return True  # always quicker

    is_comparable,a1,a2=self.is_comparable_to(other=other)
    if not is_comparable: return False

    # Now a1 and a2 both have split sheets

    # Check if helices are comparable
    for h1 in a1.helices:
      found=False
      for h2 in a2.helices:
        if h1.is_similar_to(other=h2,hierarchy=hierarchy,
            minimum_overlap=minimum_overlap,
            maximum_length_difference=maximum_length_difference):
          found=True
          break
      if not found: return False
    # Matched all the helices

    # Check if sheets are comparable. Note all sheets have 2 strands
    for s1 in a1.sheets:
      found=False
      for s2 in a2.sheets:
        if s1.is_similar_to(other=s2,hierarchy=hierarchy,
            minimum_overlap=minimum_overlap,
            maximum_length_difference=maximum_length_difference):
          found=True
          break
      if not found: return False
    # Matched all the sheets
    return True

  def is_same_as(self,other=None):
    "Return True if this annotation is the same as other. Allows different"
    "representations of same sheet (as pairs, as sheet)"
    "Does not allow different order of sheet or backwards"
    "Does not allow equivalent but different specifications of the alignment"

    is_comparable,a1,a2=self.is_comparable_to(other=other)
    if not is_comparable: return False

    def sort_strings(h):
      sorted=[]
      for x in h:
        sorted.append(x.as_pdb_str(set_id_zero=True, force_format = True))
      sorted.sort()
      return sorted

    # Sort helices and compare
    a1_helices=sort_strings(a1.helices)
    a2_helices=sort_strings(a2.helices)
    if a1_helices != a2_helices: return False

    # Sort sheets and compare
    a1_sheets=sort_strings(a1.sheets)
    a2_sheets=sort_strings(a2.sheets)
    if a1_sheets != a2_sheets: return False

    # Everything is the same
    return True

  def count_h_bonds(self,hierarchy=None,max_h_bond_length=None):
    # go through ss and count good and poor H-bonds
    number_of_good_h_bonds=0
    number_of_poor_h_bonds=0
    split_sheets=self.split_sheets() # annotation with split sheets
    for h in self.helices:
      n_good,n_poor=h.count_h_bonds(hierarchy=hierarchy,
        max_h_bond_length=max_h_bond_length)
      number_of_good_h_bonds+=n_good
      number_of_poor_h_bonds+=n_poor
    for s in self.sheets:
      n_good,n_poor=s.count_h_bonds(hierarchy=hierarchy,
        max_h_bond_length=max_h_bond_length)
      number_of_good_h_bonds+=n_good
      number_of_poor_h_bonds+=n_poor
    return number_of_good_h_bonds,number_of_poor_h_bonds


#=============================================================================
#        88        88 88888888888 88          88 8b        d8
#        88        88 88          88          88  Y8,    ,8P
#        88        88 88          88          88   `8b  d8'
#        88aaaaaaaa88 88aaaaa     88          88     Y88P
#        88""""""""88 88"""""     88          88     d88b
#        88        88 88          88          88   ,8P  Y8,
#        88        88 88          88          88  d8'    `8b
#        88        88 88888888888 88888888888 88 8P        Y8
#=============================================================================

class pdb_helix(structure_base):
  _helix_class_array = ['unknown','alpha', 'unknown', 'pi', 'unknown',
        '3_10', 'unknown', 'unknown', 'unknown', 'unknown', 'unknown']
  _cif_helix_classes = {'HELX_RH_AL_P': 1, 'HELX_RH_PI_P':3, 'HELX_RH_3T_P':5}

  def __init__(self,
        serial,
        helix_id,
        start_resname,
        start_chain_id,
        start_resseq,
        start_icode,
        end_resname,
        end_chain_id,
        end_resseq,
        end_icode,
        helix_class,
        comment,
        length,
        hbond_list=[], # list of (donor, acceptor) selecitons
        helix_selection=None,
        enabled=True,
        sigma=0.05,
        slack=0,
        top_out=False,
        ):
    adopt_init_args(self, locals())
    if (length <= 0):
      raise Sorry("Bad helix length. Check HELIX records.\n%s" % self.as_pdb_str())
    if isinstance(self.helix_class, int):
      self.helix_class = self._helix_class_array[helix_class]
    if self.helix_class not in self._helix_class_array:
      raise Sorry("Bad helix class: %s. Check HELIX records." % helix_class)

    self.start_chain_id = self.parse_chain_id(start_chain_id)
    self.end_chain_id = self.parse_chain_id(end_chain_id)
    if isinstance(self.helix_id, int):
      self.helix_id = "%s" % self.helix_id
      self.helix_id = self.helix_id[:3]
    elif self.helix_id is None or self.helix_id.strip()=="":
      self.adopt_serial_as_id()
    else:
      assert isinstance(self.helix_id, str)
    if self.start_chain_id != self.end_chain_id:
      raise Sorry("Don't know how to deal with helices with multiple "+
        "chain IDs ('%s' vs. '%s')." % (self.start_chain_id, self.end_chain_id))

  @classmethod
  def helix_class_to_int(cls, h_class):
    return cls._helix_class_array.index(h_class)

  @classmethod
  def helix_class_to_str(cls, h_class):
    return cls._helix_class_array[h_class]

  @classmethod
  def get_helix_class(cls, cif_row):
    conf_type_class = cif_row.get('_struct_conf.conf_type_id', None)
    if conf_type_class not in ['HELX_P', 'HELX_RH_AL_P', 'HELX_RH_PI_P', 'HELX_RH_3T_P']:
      return None
    if conf_type_class == 'HELX_P':
      pdbx_class = int(cif_row.get('_struct_conf.pdbx_PDB_helix_class', 0))
      return cls._helix_class_array[pdbx_class]
    else:
      return cls._helix_class_array[cls._cif_helix_classes[conf_type_class]]

  @classmethod
  def from_cif_row(cls, cif_row, serial):
    h_class = cls.get_helix_class(cif_row)
    if h_class is None:
      return None
    start_resname = choose_correct_cif_record(
        cif_row,
        '_struct_conf.beg_auth_comp_id',
        '_struct_conf.beg_label_comp_id')
    start_chain_id = cls.parse_chain_id(
        choose_correct_cif_record(
            cif_row,
            '_struct_conf.beg_auth_asym_id',
            '_struct_conf.beg_label_asym_id'))
    start_resseq = int(choose_correct_cif_record(
        cif_row,
        '_struct_conf.beg_auth_seq_id',
        '_struct_conf.beg_label_seq_id'))
    end_resname = choose_correct_cif_record(
        cif_row,
        '_struct_conf.end_auth_comp_id',
        '_struct_conf.end_label_comp_id')
    end_chain_id = cls.parse_chain_id(
        choose_correct_cif_record(
            cif_row,
            '_struct_conf.end_auth_asym_id',
            '_struct_conf.end_label_asym_id'))
    end_resseq = int(choose_correct_cif_record(
        cif_row,
        '_struct_conf.end_auth_seq_id',
        '_struct_conf.end_label_seq_id'))
    comment = ""
    if ('_struct_conf.details' in cif_row and
        cif_row['_struct_conf.details'] != '?' and
        cif_row['_struct_conf.details'] != '.'):
      comment = cif_row['_struct_conf.details']
    # this is not mandatory item in mmCIF, so we'll try to estimate it in case
    # it is absent. Since it is mmCIF, it should not contain hybrid36 notation,
    # so int() should be fine
    length = int(cif_row.get('_struct_conf.pdbx_PDB_helix_length', 0))
    if length == 0:
      length = end_resseq - start_resseq

    return cls(
      serial=serial,
      helix_id=cif_row.get('_struct_conf.pdbx_PDB_helix_id', None),
      start_resname=start_resname,
      start_chain_id=start_chain_id,
      start_resseq=cls.convert_resseq(start_resseq),
      start_icode=cls.parse_cif_insertion_code(cif_row.get('_struct_conf.pdbx_beg_PDB_ins_code', '.')),
      end_resname=end_resname,
      end_chain_id=end_chain_id,
      end_resseq=cls.convert_resseq(end_resseq),
      end_icode=cls.parse_cif_insertion_code(cif_row.get('_struct_conf.pdbx_end_PDB_ins_code', '.')),
      helix_class=h_class,
      helix_selection=None,
      comment=comment,
      length=length)

  @classmethod
  def from_pdb_record(cls, line):
    "Raises ValueError in case of corrupted HELIX record!!!"
    if len(line) < 76:
      line += " "*(80-len(line))
    return cls(
      serial=cls.convert_id(line[7:10]),
      helix_id=line[11:14].strip(),
      start_resname=line[15:18],
      start_chain_id=cls.parse_chain_id(line[18:20]),
      start_resseq=line[21:25],
      start_icode=line[25],
      end_resname=line[27:30],
      end_chain_id=cls.parse_chain_id(line[30:32]),
      end_resseq=line[33:37],
      end_icode=line[37],
      helix_class=cls.helix_class_to_str(int(line[38:40])),
      helix_selection=None,
      comment=line[40:70],
      length=int(line[71:76])) #string.atoi(line[71:76]))

  @classmethod
  def from_phil_params(cls, helix_params, pdb_hierarchy, h_atoms, cache, serial=0, log=None):
    if log is None:
      log = sys.stdout
    if helix_params.selection is None :
      print("Empty helix at serial %d." % (serial), file=log)
      # continue
    if helix_params.serial_number is not None:
      serial = helix_params.serial_number
    amide_isel = get_amide_isel(cache, helix_params.selection)
    if isinstance(amide_isel, str):
      print(amide_isel, file=log)
      return None
    start_atom = h_atoms[amide_isel[0]]
    end_atom = h_atoms[amide_isel[-1]]
    hbonds = []
    for hb in helix_params.hbond:
      if hb.donor is None:
        print("Donor selection in hbond cannot be None", file=log)
        continue
      if hb.acceptor is None:
        print("Acceptor selection in hbond cannot be None", file=log)
        continue
      hbonds.append((hb.donor, hb.acceptor))
    return cls(
      serial=serial,
      helix_id=helix_params.helix_identifier,
      start_resname=start_atom.parent().resname,
      start_chain_id=start_atom.parent().parent().parent().id,
      start_resseq=start_atom.parent().parent().resseq,
      start_icode=start_atom.parent().parent().icode,
      end_resname=end_atom.parent().resname,
      end_chain_id=end_atom.parent().parent().parent().id,
      end_resseq=end_atom.parent().parent().resseq,
      end_icode=end_atom.parent().parent().icode,
      helix_class=helix_params.helix_type,
      comment="",
      length=amide_isel.size(),
      hbond_list=hbonds,
      helix_selection=helix_params.selection,
      enabled=helix_params.enabled,
      sigma=helix_params.sigma,
      slack=helix_params.slack,
      top_out=helix_params.top_out,
      )

  def deep_copy(self):
    return copy.deepcopy(self)

  def get_class_as_int(self):
    return self.helix_class_to_int(self.helix_class)

  def get_class_as_str(self):
    return self.helix_class

  def set_start_resseq(self, resseq):
    self.start_resseq = self.convert_resseq(resseq)

  def set_end_resseq(self, resseq):
    self.end_resseq = self.convert_resseq(resseq)

  def set_new_serial(self, serial, adopt_as_id=False):
    self.serial = hy36encode(3, serial)
    if adopt_as_id:
      self.adopt_serial_as_id()

  def adopt_serial_as_id(self):
    self.helix_id = "%s" % self.serial

  def set_new_chain_ids(self, new_chain_id):
    self.start_chain_id = new_chain_id
    self.end_chain_id = new_chain_id

  def erase_hbond_list(self):
    self.hbond_list = []

  def as_restraint_group(self, log=sys.stdout, prefix_scope="",
      add_segid=None, show_hbonds=False):
    if self.start_chain_id != self.end_chain_id :
      print("Helix chain ID mismatch: starts in %s, ends in %s" % (
        self.start_chain_id, self.end_chain_id), file=log)
      return None
    sele = self.as_atom_selections(add_segid=add_segid)[0]
    if prefix_scope != "" and not prefix_scope.endswith("."):
      prefix_scope += "."
    serial_and_id = ""
    if self.serial is not None and self.id_as_int(self.serial) > 0:
      serial_and_id += "\n  serial_number = %s" % self.serial
    if self.helix_id is not None:
      serial_and_id += "\n  helix_identifier = %s" % self.helix_id
    hbond_restr = ""
    if show_hbonds:
      if self.get_n_defined_hbonds() > 0:
        for hb in self.hbond_list:
          hb_str = "\n  hbond {\n    donor = %s\n    acceptor = %s\n  }" % (
            hb[0], hb[1])
          hbond_restr += hb_str
    rg = """\
%sprotein.helix {%s
  selection = %s
  helix_type = %s%s
}""" % (prefix_scope, serial_and_id, sele,
        self.helix_class, hbond_restr)
    return rg

  def comment_to_cif(self):
    stripped = self.comment.strip()
    if len(stripped) == 0:
      return "?"
    else:
      return stripped

  def as_cif_dict(self):
    """Returns dict. keys - cif field names, values - appropriate values."""
    result = {}
    # XXX This is most default type of helix.
    # Refer here for the others:
    # http://mmcif.wwpdb.org/dictionaries/mmcif_mdb.dic/Items/_struct_conf_type.id.html
    # Note, that PDB itself does not assign proper helix types in mmCIF although it has
    # no problems doing it for PDB format and pretty pictures on the web-site.
    result['conf_type_id'] = "HELX_P"
    result['id'] = self.serial if isinstance(self.serial, int) else self.serial.strip()
    result['pdbx_PDB_helix_id'] = self.helix_id
    result['beg_label_comp_id'] = self.start_resname
    result['beg_label_asym_id'] = self.start_chain_id
    result['beg_label_seq_id'] =  "%d" % self.get_start_resseq_as_int()
    result['pdbx_beg_PDB_ins_code'] = self.icode_to_cif(self.start_icode)
    result['end_label_comp_id'] = self.end_resname
    result['end_label_asym_id'] = self.end_chain_id
    result['end_label_seq_id'] =  "%d" % self.get_end_resseq_as_int()
    result['pdbx_end_PDB_ins_code'] = self.icode_to_cif(self.end_icode)
    result['pdbx_PDB_helix_class'] = self.helix_class_to_int(self.helix_class)
    result['details'] = self.comment_to_cif()
    result['pdbx_PDB_helix_length'] = self.length
    return result

  def as_pdb_or_mmcif_str(self, target_format = 'pdb'):
    # Return str in target format if possible, otherwise in mmcif
    if target_format == 'pdb' and self.fits_in_pdb_format():
      return self.as_pdb_str()
    else:
      return self.as_mmcif_str()

  def fits_in_pdb_format(self):
    if len(self.start_resname.strip()) > 3: return False
    if len(self.end_resname.strip()) > 3: return False
    if len(self.start_chain_id.strip()) > 2: return False
    if len(self.end_chain_id.strip()) > 2: return False
    return True

  def as_mmcif_str(self):
    ann = annotation(helices = [self], sheets = [])
    text = ann.as_mmcif_str()
    return text

  def as_pdb_str(self, set_id_zero=False, force_format = False):
    if (not force_format) and (not self.fits_in_pdb_format()):
      raise AssertionError(
       "Helix does not fit in PDB format. "+
       "Please fix code to use as_pdb_or_mmcif_str instead of as_pdb_str")
    def h_class_to_pdb_int(h_class):
      h_class_int = self.helix_class_to_int(h_class)
      if h_class_int == 0:
        return 1
      return h_class_int
    format = "HELIX  %3s %3s %3s%2s %4s%1s %3s%2s %4s%1s%2d%30s %5d"
    if set_id_zero:
      serial=0
      helix_id=0
    else:
      serial=self.serial
      if self.helix_id is None:
        helix_id=self.serial
      else:
        helix_id=self.helix_id[:3]
    out = format % (self.convert_id(serial),
      helix_id,
      self.start_resname,
      self.start_chain_id, self.start_resseq, self.start_icode,
      self.end_resname, self.end_chain_id, self.end_resseq, self.end_icode,
      h_class_to_pdb_int(self.helix_class), self.comment, self.length)
    return out.strip()

  def as_atom_selections(self, add_segid=None, trim_ends_by=None):
    segid_extra = ""
    if add_segid is not None :
      segid_extra = "and segid '%s' " % add_segid
    if trim_ends_by and \
     self.get_end_resseq_as_int()-self.get_start_resseq_as_int()>2*trim_ends_by:
      resid_start = "%s%s" % (self.convert_resseq(
         self.get_start_resseq_as_int()+trim_ends_by), self.start_icode)
      resid_end = "%s%s" % (self.convert_resseq(
         self.get_end_resseq_as_int()-trim_ends_by), self.end_icode)
    else:  # usual
      resid_start = "%s%s" % (self.start_resseq, self.start_icode)
      resid_end = "%s%s" % (self.end_resseq, self.end_icode)
    sele = "chain '%s' %sand resid %s through %s" % (self.start_chain_id,
      segid_extra, resid_start, resid_end)
    return [sele]

  def get_n_defined_hbonds(self):
    if self.hbond_list is not None:
      return len(self.hbond_list)
    return 0

  def get_n_maximum_hbonds(self):
    if self.helix_class == 'alpha':
      return self.length-4
    elif self.helix_class=='3_10':
      return self.length-3
    elif self.helix_class=='pi':
      return self.length-5
    elif self.helix_class=='unknown':
      return 0
    else:
      # Should never happen
      assert 0, "Wrong helix_class creeped in object fields: %d" % (
          self.helix_class)

  def chains_included(self):
    "Report list of all chains involved in this helix"
    chain_list=[self.start_chain_id]
    if self.start_chain_id != self.start_chain_id:
      chain_list.append(self.end_chain_id)
    return chain_list

  def is_same_as(self,other=None):
    # return True if it self has the same values as other

    if not other: return False
    if type(self) != type(other): return False # Check for different objects

    for key in [ 'start_resname',
        'start_chain_id',
        'start_resseq',
        'start_icode',
        'end_resname',
        'end_chain_id',
        'end_resseq',
        'end_icode',
        'helix_class' ]:
      if getattr(self,key,None) != getattr(other,key,None):
        return False
    return True

  def change_residue_numbering_in_place(self, renumbering_dictionary):
    self._change_residue_numbering_in_place_helper(renumbering_dictionary)


#=============================================================================
#       ad88888ba  88        88 88888888888 88888888888 888888888888
#      d8"     "8b 88        88 88          88               88
#      Y8,         88        88 88          88               88
#      `Y8aaaaa,   88aaaaaaaa88 88aaaaa     88aaaaa          88
#        `"""""8b, 88""""""""88 88"""""     88"""""          88
#              `8b 88        88 88          88               88
#      Y8a     a8P 88        88 88          88               88
#       "Y88888P"  88        88 88888888888 88888888888      88
#=============================================================================

class pdb_strand(structure_base):
  def __init__(self,
      sheet_id,
      strand_id,
      start_resname,
      start_chain_id,
      start_resseq,
      start_icode,
      end_resname,
      end_chain_id,
      end_resseq,
      end_icode,
      sense):
    adopt_init_args(self, locals())
    # Python 3 prevents comparisons between str and int
    # assert (sheet_id > 0) and (strand_id > 0)
    assert (sheet_id is not None) and (strand_id is not None)
    if sense not in [-1, 0, 1]:
      raise Sorry("Bad sense in SHEET record: '%s'" % sense)
    self.start_chain_id = self.parse_chain_id(start_chain_id)
    self.end_chain_id = self.parse_chain_id(end_chain_id)
    self.set_start_resseq(self.start_resseq)
    self.set_end_resseq(self.end_resseq)
    if self.start_chain_id != self.end_chain_id:
      raise Sorry("Don't know how to deal with strands with different "+
        "chain IDs ('%s' vs. '%s')." % (self.start_chain_id, self.end_chain_id))
    self.approx_length = self.get_end_resseq_as_int() - self.get_start_resseq_as_int()

  @classmethod
  def from_pdb_record(cls, line):
    if len(line) < 76:
      line += " "*(80-len(line))
    return cls(sheet_id=line[11:14],
        strand_id=cls.convert_id(line[7:10]),
        start_resname=line[17:20],
        start_chain_id=cls.parse_chain_id(line[20:22]),
        start_resseq=line[22:26],
        start_icode=line[26],
        end_resname=line[28:31],
        end_chain_id=cls.parse_chain_id(line[31:33]),
        end_resseq=line[33:37],
        end_icode=line[37],
        sense=int(line[38:40]))

  @classmethod
  def from_cif_dict(cls, cif_dict, sense):
    start_resname = choose_correct_cif_record(
        cif_dict,
        '_struct_sheet_range.beg_auth_comp_id',
        '_struct_sheet_range.beg_label_comp_id')
    start_chain_id = choose_correct_cif_record(
        cif_dict,
        '_struct_sheet_range.beg_auth_asym_id',
        '_struct_sheet_range.beg_label_asym_id')
    start_resseq = int(choose_correct_cif_record(
        cif_dict,
        '_struct_sheet_range.beg_auth_seq_id',
        '_struct_sheet_range.beg_label_seq_id'))
    end_resname = choose_correct_cif_record(
        cif_dict,
        '_struct_sheet_range.end_auth_comp_id',
        '_struct_sheet_range.end_label_comp_id')
    end_chain_id = choose_correct_cif_record(
        cif_dict,
        '_struct_sheet_range.end_auth_asym_id',
        '_struct_sheet_range.end_label_asym_id')
    end_resseq = int(choose_correct_cif_record(
        cif_dict,
        '_struct_sheet_range.end_auth_seq_id',
        '_struct_sheet_range.end_label_seq_id'))
    return cls(
        sheet_id=cif_dict['_struct_sheet_range.sheet_id'],
        strand_id=int(cif_dict['_struct_sheet_range.id']),
        start_resname=start_resname,
        start_chain_id=cls.parse_chain_id(start_chain_id),
        start_resseq=cls.convert_resseq(start_resseq),
        start_icode=cls.parse_cif_insertion_code(
            cif_dict.get('_struct_sheet_range.pdbx_beg_PDB_ins_code','.')),
        end_resname=end_resname,
        end_chain_id=cls.parse_chain_id(end_chain_id),
        end_resseq=cls.convert_resseq(end_resseq),
        end_icode=cls.parse_cif_insertion_code(
            cif_dict.get('_struct_sheet_range.pdbx_end_PDB_ins_code', '.')),
        sense=sense,
      )
  def deep_copy(self):
    return copy.deepcopy(self)

  def set_new_chain_ids(self, new_chain_id):
    self.start_chain_id = new_chain_id
    self.end_chain_id = new_chain_id

  def change_residue_numbering_in_place(self, renumbering_dictionary):
    self._change_residue_numbering_in_place_helper(renumbering_dictionary)

  def sense_as_cif(self):
    if self.sense == 0:
      return '?'
    elif self.sense == 1:
      return "parallel"
    elif self.sense == -1:
      return "anti-parallel"
    else:
      raise Sorry("Invalid sense creeped in object: %s", self.sense)

  def set_start_resseq(self, resseq):
    self.start_resseq = self.convert_resseq(resseq)

  def set_end_resseq(self, resseq):
    self.end_resseq = self.convert_resseq(resseq)

  def as_atom_selections(self, add_segid=None,trim_ends_by=None):
    segid_extra = ""
    if add_segid is not None :
      segid_extra = "and segid '%s' " % add_segid
    if trim_ends_by and \
     self.get_end_resseq_as_int()-self.get_start_resseq_as_int()>2*trim_ends_by:
      resid_start = "%s%s" % (self.convert_resseq(
         self.get_start_resseq_as_int()+trim_ends_by), self.start_icode)
      resid_end = "%s%s" % (self.convert_resseq(
         self.get_end_resseq_as_int()-trim_ends_by), self.end_icode)
    else:  # usual
      resid_start = "%s%s" % (self.start_resseq, self.start_icode)
      resid_end = "%s%s" % (self.end_resseq, self.end_icode)

    sele = "chain '%s' %sand resid %s through %s" % (self.start_chain_id,
      segid_extra, resid_start, resid_end)
    return sele


  def is_same_as(self,other=None):
    # return True if it self has the same values as other
    # include sense (as it is relative to another strand)

    if not other: return False
    if type(self) != type(other): return False # Check for different objects

    key_list= [ 'start_resname',
        'start_chain_id',
        'start_resseq',
        'start_icode',
        'end_resname',
        'end_chain_id',
        'end_resseq',
        'end_icode',
        'sense']

    for key in key_list:
      if getattr(self,key,None) != getattr(other,key,None):
        return False
    return True

class pdb_strand_register(structure_base):
  def __init__(self,
      cur_atom,
      cur_resname,
      cur_chain_id,
      cur_resseq,
      cur_icode,
      prev_atom,
      prev_resname,
      prev_chain_id,
      prev_resseq,
      prev_icode):
    adopt_init_args(self, locals())

  @classmethod
  def from_pdb_record(cls, line):
    if len(line.strip()) < 67:
      return None
    return cls(cur_atom=line[41:45],
        cur_resname=line[45:48],
        cur_chain_id=cls.parse_chain_id(line[48:50]),
        cur_resseq=line[50:54],
        cur_icode=line[54],
        prev_atom=line[56:60],
        prev_resname=line[60:63],
        prev_chain_id=cls.parse_chain_id(line[63:65]),
        prev_resseq=line[65:69],
        prev_icode=line[69])

  @staticmethod
  def adjust_cif_atom_name(name):
    if len(name) == 1:
      return " %s  " % name
    elif len(name) == 2:
      return " %s " % name
    elif len(name) == 3:
      return "%s " % name
    return name

  @classmethod
  def from_cif_dict(cls, cif_dict):
    cur_atom = cls.adjust_cif_atom_name(
        choose_correct_cif_record(
            cif_dict,
            '_pdbx_struct_sheet_hbond.range_2_auth_atom_id',
            '_pdbx_struct_sheet_hbond.range_2_label_atom_id'))
    cur_resname = choose_correct_cif_record(
        cif_dict,
        '_pdbx_struct_sheet_hbond.range_2_auth_comp_id',
        '_pdbx_struct_sheet_hbond.range_2_label_comp_id',
        mandatory=False)
    cur_chain_id = choose_correct_cif_record(
        cif_dict,
        '_pdbx_struct_sheet_hbond.range_2_auth_asym_id',
        '_pdbx_struct_sheet_hbond.range_2_label_asym_id',
        mandatory=False)
    cur_resseq = int(choose_correct_cif_record(
        cif_dict,
        '_pdbx_struct_sheet_hbond.range_2_auth_seq_id',
        '_pdbx_struct_sheet_hbond.range_2_label_seq_id'))
    cur_icode = cls.parse_cif_insertion_code(
        cif_dict.get('_pdbx_struct_sheet_hbond.range_2_PDB_ins_code','.'))
    prev_atom = cls.adjust_cif_atom_name(
        choose_correct_cif_record(
          cif_dict,
          '_pdbx_struct_sheet_hbond.range_1_auth_atom_id',
          '_pdbx_struct_sheet_hbond.range_1_label_atom_id'))
    prev_resname = choose_correct_cif_record(
        cif_dict,
        '_pdbx_struct_sheet_hbond.range_1_auth_comp_id',
        '_pdbx_struct_sheet_hbond.range_1_label_comp_id',
        mandatory=False)
    prev_chain_id = choose_correct_cif_record(
        cif_dict,
        '_pdbx_struct_sheet_hbond.range_1_auth_asym_id',
        '_pdbx_struct_sheet_hbond.range_1_label_asym_id',
        mandatory=False)
    prev_resseq = int(choose_correct_cif_record(
        cif_dict,
        '_pdbx_struct_sheet_hbond.range_1_auth_seq_id',
        '_pdbx_struct_sheet_hbond.range_1_label_seq_id'))
    prev_icode = cls.parse_cif_insertion_code(
        cif_dict.get('_pdbx_struct_sheet_hbond.range_1_PDB_ins_code','.'))
    return cls(
        cur_atom=cur_atom,
        cur_resname=cur_resname,
        cur_chain_id=cls.parse_chain_id(cur_chain_id),
        cur_resseq=cls.convert_resseq(cur_resseq),
        cur_icode=cur_icode,
        prev_atom=prev_atom,
        prev_resname=prev_resname,
        prev_chain_id=cls.parse_chain_id(prev_chain_id),
        prev_resseq=cls.convert_resseq(prev_resseq),
        prev_icode=prev_icode)

  def get_cur_resseq_as_int(self):
    if self.cur_resseq is not None:
      return hy36decode(4, self.cur_resseq)
    return None

  def get_prev_resseq_as_int(self):
    if self.prev_resseq is not None:
      return hy36decode(4, self.prev_resseq)
    return None

  def set_cur_resseq(self, resseq):
    self.cur_resseq = self.convert_resseq(resseq)

  def set_prev_resseq(self, resseq):
    self.prev_resseq = self.convert_resseq(resseq)

  def set_new_cur_chain_id(self, new_chain_id):
    self.cur_chain_id = new_chain_id

  def set_new_prev_chain_id(self, new_chain_id):
    self.prev_chain_id = new_chain_id

  def as_atom_selections(self, add_segid=None):
    segid_extra = ""
    if add_segid is not None :
      segid_extra = "and segid '%s' " % add_segid
    sele_base = "chain '%s' %sand resid %s and name %s"
    resid_curr = "%s%s" % (self.cur_resseq,self.cur_icode)
    resid_prev = "%s%s" % (self.prev_resseq,
        self.prev_icode)
    sele_curr = sele_base % (self.cur_chain_id, segid_extra,
        resid_curr, self.cur_atom.strip())
    sele_prev = sele_base % (self.prev_chain_id,segid_extra,
        resid_prev, self.prev_atom.strip())
    return sele_curr, sele_prev

  def change_residue_numbering_in_place(self, renumbering_dictionary):
    self._change_residue_numbering_in_place_helper(renumbering_dictionary, se=["cur", "prev"])

  def is_same_as(self,other=None):

    if not other: return False
    if type(self) != type(other): return False # Check for different objects

    for key in [ 'cur_atom',
        'cur_resname',
        'cur_chain_id',
        'cur_resseq',
        'cur_icode',
        'prev_atom',
        'prev_resname',
        'prev_chain_id',
        'prev_resseq',
        'prev_icode']:
      if getattr(self,key,None) != getattr(other,key,None):
        return False
    return True


  def reversed(self):
    # swap cur and prev
    new_register=copy.deepcopy(self)
    for key in [
        'atom',
        'resname',
        'chain_id',
        'resseq',
        'icode']:
      setattr(new_register,'prev_'+key,getattr(self,'cur_'+key))
      setattr(new_register,'cur_'+key,getattr(self,'prev_'+key))
    return new_register



class pdb_sheet(structure_base):
  def __init__(self,
      sheet_id,
      n_strands,
      strands,
      registrations,
      hbond_list=[], # list of (donor, acceptor) selections
      ):
    adopt_init_args(self, locals())
    if isinstance(self.sheet_id, int):
      self.sheet_id = hy36encode(3, self.sheet_id)
    else:
      assert isinstance(self.sheet_id, str)

  @classmethod
  def from_pdb_records(cls,records):
    "Raises ValueError in case of bad SHEET record"
    assert len(records) > 0
    sheet_id = records[0][11:14]
    n_strands = int(records[0][14:16])
    if n_strands != len(records):
      # correcting wrong n_strands
      n_strands = len(records)
    strands = []
    registrations = []
    for r in records:
      s = pdb_strand.from_pdb_record(r)
      reg = None
      sense = int(r[38:40])
      reg = pdb_strand_register.from_pdb_record(r)
      # do we really need to stop on this?
      if sense == 0 and reg is not None:
        raise Sorry("Sense should be 1 or -1 for non-first strand:\n%s" % r)
      strands.append(s)
      registrations.append(reg)
    return cls(sheet_id=sheet_id,
               n_strands=n_strands,
               strands=strands,
               registrations=registrations)

  @classmethod
  def from_cif_rows(cls, sheet_id,
            struct_sheet_order_loop, struct_sheet_range_loop,
            struct_sheet_hbond_loop):
    strands = []
    registrations = []
    number_of_strands = 0
    # counting number of strands
    number_of_strands = struct_sheet_range_loop['_struct_sheet_range.sheet_id'].\
        count(sheet_id)
    for i in range(1, number_of_strands+1):
      f_rows = struct_sheet_range_loop.find_row(kv_dict={
          '_struct_sheet_range.sheet_id' : sheet_id,
          '_struct_sheet_range.id' : str(i),
        })
      if i == 1:
        # the first strand, no sense, no hbond
        strands.append(pdb_strand.from_cif_dict(f_rows[0], 0))
        registrations.append(None)
      else:
        # all the rest
        res_range = None
        registration = None
        sense = None
        if len(f_rows) == 1:
          res_range = f_rows[0]
        sense_rows = struct_sheet_order_loop.find_row(kv_dict = {
            '_struct_sheet_order.sheet_id':sheet_id,
            '_struct_sheet_order.range_id_1':str(i-1),
            '_struct_sheet_order.range_id_2':str(i)
          })
        if len(sense_rows) >= 1:
          sense = 0
          str_sense = sense_rows[0].get('_struct_sheet_order.sense', '.')
          if str_sense == 'parallel':
            sense = 1
          elif str_sense == 'anti-parallel':
            sense = -1
        registration_rows = struct_sheet_hbond_loop.find_row(kv_dict = {
              '_pdbx_struct_sheet_hbond.sheet_id':sheet_id,
              '_pdbx_struct_sheet_hbond.range_id_1':str(i-1),
              '_pdbx_struct_sheet_hbond.range_id_2':str(i)
          })
        if len(registration_rows) >= 1:
          registration = registration_rows[0]
        if (res_range is not None and
            registration is not None and sense is not None):
          strands.append(pdb_strand.from_cif_dict(res_range, sense))
          registrations.append(pdb_strand_register.from_cif_dict(registration))
    return cls(sheet_id=sheet_id,
               n_strands=number_of_strands,
               strands=strands,
               registrations=registrations)

  @classmethod
  def from_phil_params(cls, sheet_params, pdb_hierarchy, h_atoms, cache, log=None):
    if log is None:
      log = sys.stdout
    if sheet_params.first_strand is None:
      raise Sorry("Empty first strand selection")
    sheet_id="1"
    if sheet_params.sheet_id is not None:
      sheet_id =  "%3s" % sheet_params.sheet_id[:3]
    n_strands = len(sheet_params.strand) + 1
    amide_isel = get_amide_isel(cache, sheet_params.first_strand)
    if isinstance(amide_isel, str):
      print(amide_isel, file=log)
      return None
    start_atom = h_atoms[amide_isel[0]]
    end_atom = h_atoms[amide_isel[-1]]
    first_strand = pdb_strand(
        sheet_id=sheet_id,
        strand_id=1,
        start_resname=start_atom.parent().resname,
        start_chain_id=start_atom.parent().parent().parent().id,
        start_resseq=start_atom.parent().parent().resseq,
        start_icode=start_atom.parent().parent().icode,
        end_resname=end_atom.parent().resname,
        end_chain_id=end_atom.parent().parent().parent().id,
        end_resseq=end_atom.parent().parent().resseq,
        end_icode=end_atom.parent().parent().icode,
        sense=0)
    strands = [first_strand]
    registrations = [None]
    for i, strand_param in enumerate(sheet_params.strand):
      amide_isel = get_amide_isel(cache, strand_param.selection)
      if isinstance(amide_isel, str):
        print(amide_isel, file=log)
        return None
      start_atom = h_atoms[amide_isel[0]]
      end_atom = h_atoms[amide_isel[-1]]
      sense = cls.sense_to_int(strand_param.sense)
      strand = pdb_strand(
          sheet_id=sheet_id,
          strand_id=i+2,
          start_resname=start_atom.parent().resname,
          start_chain_id=start_atom.parent().parent().parent().id,
          start_resseq=start_atom.parent().parent().resseq,
          start_icode=start_atom.parent().parent().icode,
          end_resname=end_atom.parent().resname,
          end_chain_id=end_atom.parent().parent().parent().id,
          end_resseq=end_atom.parent().parent().resseq,
          end_icode=end_atom.parent().parent().icode,
          sense=sense)
      reg_cur_atom = None
      if strand_param.bond_start_current is not None:
        reg_cur_sel = cache.iselection(strand_param.bond_start_current)
        if reg_cur_sel is None or len(reg_cur_sel) == 0:
          error_msg = "Error in sheet definition. Whole sheet will be skipped.\n"
          error_msg += "String '%s' selected 0 atoms.\n" % strand_param.bond_start_current
          error_msg += "Most likely the definition of sheet does not match model.\n"
          print(error_msg, file=log)
          return None
        reg_cur_atom = h_atoms[reg_cur_sel[0]]
      if reg_cur_atom is None: # No current atom in registration
        pass
        # raise Sorry("This bond_start_current yields 0 atoms:\n %s" % strand_param.bond_start_current)
      reg_prev_atom = None
      if strand_param.bond_start_previous is not None:
        reg_prev_sel = cache.iselection(strand_param.bond_start_previous)
        if reg_prev_sel is None or len(reg_prev_sel) == 0:
          error_msg = "Error in sheet definition. Whole sheet will be skipped.\n"
          error_msg += "String '%s' selected 0 atoms.\n" % strand_param.bond_start_previous
          error_msg += "Most likely the definition of sheet does not match model.\n"
          print(error_msg, file=log)
          return None
        reg_prev_atom = h_atoms[reg_prev_sel[0]]
      if reg_prev_atom is None: # No previous atom in registration
        pass
        # raise Sorry("This bond_start_previous yields 0 atoms:\n %s" % strand_param.bond_start_previous)
      reg = None
      if reg_cur_atom is not None and reg_prev_atom is not None:
        reg = pdb_strand_register(
            cur_atom=reg_cur_atom.name,
            cur_resname=reg_cur_atom.parent().resname,
            cur_chain_id=reg_cur_atom.parent().parent().parent().id,
            cur_resseq=reg_cur_atom.parent().parent().resseq,
            cur_icode=reg_cur_atom.parent().parent().icode,
            prev_atom=reg_prev_atom.name,
            prev_resname=reg_prev_atom.parent().resname,
            prev_chain_id=reg_prev_atom.parent().parent().parent().id,
            prev_resseq=reg_prev_atom.parent().parent().resseq,
            prev_icode=reg_prev_atom.parent().parent().icode)
      strands.append(strand)
      registrations.append(reg)
    hbonds = []
    for hb in sheet_params.hbond:
      if hb.donor is None:
        print("Donor selection in hbond cannot be None", file=log)
        continue
      if hb.acceptor is None:
        print("Acceptor selection in hbond cannot be None", file=log)
        continue
      hbonds.append((hb.donor, hb.acceptor))
    return cls(sheet_id=sheet_id,
               n_strands=n_strands,
               strands=strands,
               registrations=registrations,
               hbond_list=hbonds)
  @staticmethod
  def sense_to_int(str_sense):
    sense = 0
    if str_sense == "parallel":
      sense = 1
    elif str_sense == "antiparallel":
      sense = -1
    return sense

  def get_approx_size(self):
    s = 0
    for strand in self.strands:
      s += strand.approx_length
    return s

  def remove_short_strands(self, size=3):
    strand_indices_to_delete = []
    # need to make sure we are not deleting strands from the middle and
    # brake registrations...
    cont = True
    i = 0
    while cont and i < self.n_strands:
      if self.strands[i].approx_length < size:
        if i not in strand_indices_to_delete:
          strand_indices_to_delete.append(i)
      else:
        cont = False
      i += 1
    cont = True
    i = len(self.strands) - 1
    while cont and i >= 0:
      if self.strands[i].approx_length < size:
        if i not in strand_indices_to_delete:
          strand_indices_to_delete.append(i)
      else:
        cont = False
      i -= 1
    if len(strand_indices_to_delete) == self.n_strands:
      self.sheet_id=0
      self.n_strands=0
      self.strands=[]
      self.registrations=[]
    else:
      for i in reversed(sorted(strand_indices_to_delete)):
        del self.strands[i]
        del self.registrations[i]
        self.n_strands -= 1
      self.registrations[0] = None
      self.strands[0].sense = 0
      if len(strand_indices_to_delete) > 0:
        self.renumber_strands()
        self.erase_hbond_list()

  def change_residue_numbering_in_place(self, renumbering_dictionary):
    self.erase_hbond_list()
    for st in self.strands:
      st.change_residue_numbering_in_place(renumbering_dictionary)
    for reg in self.registrations:
      if reg is not None:
        reg.change_residue_numbering_in_place(renumbering_dictionary)

  def deep_copy(self):
    return copy.deepcopy(self)

  def renumber_strands(self):
    i = 1
    for s in self.strands:
      s.strand_id = i
      i += 1

  def erase_hbond_list(self):
    self.hbond_list = []

  def add_strand(self, strand):
    self.strands.append(strand)

  def add_registration(self, registration):
    self.registrations.append(registration)

  def as_atom_selections(self, trim_ends_by=None, add_segid=None):
    strand_selections = []
    for strand in self.strands:
      strand_selections.append(strand.as_atom_selections(add_segid=add_segid,
       trim_ends_by=trim_ends_by))
    return strand_selections

  def get_n_defined_hbonds(self):
    if self.hbond_list is not None:
      return len(self.hbond_list)
    return 0

  def as_pdb_or_mmcif_str(self, target_format = 'pdb'):
    # Return str in target format if possible, otherwise in mmcif
    if target_format == 'pdb' and self.fits_in_pdb_format():
      return self.as_pdb_str()
    else:
      return self.as_mmcif_str()

  def fits_in_pdb_format(self):
    for strand in self.strands:
      if len(strand.start_resname.strip()) > 3: return False
      if len(strand.end_resname.strip()) > 3: return False
      if len(strand.start_chain_id.strip()) > 2: return False
      if len(strand.end_chain_id.strip()) > 2: return False
    return True

  def as_mmcif_str(self):
    ann = annotation(helices = [], sheets = [self])
    text = ann.as_mmcif_str()
    return text

  def as_pdb_str(self, strand_id=None, set_id_zero=False, force_format = False):
    if (not force_format) and (not self.fits_in_pdb_format()):
      raise AssertionError("Sheet does not fit in PDB format"+
       "Please fix code to use as_pdb_or_mmcif_str instead of as_pdb_str")
    assert len(self.strands) == len(self.registrations)
    lines = []
    for strand, reg in zip(self.strands, self.registrations):
      format1 = "SHEET  %3s %3s%2d %3s%2s%4s%1s %3s%2s%4s%1s%2d"
      format2 = "%4s%3s%2s%4s%1s %4s%3s%2s%4s%1s"
      # print "STRAND, REG", strand, reg
      if set_id_zero:
        strand_id_for_print=0
        sheet_id_for_print=self.convert_id(0)
      else:
        strand_id_for_print=strand.strand_id
        sheet_id_for_print=self.convert_id(strand.sheet_id)

      line = format1 % (strand_id_for_print, sheet_id_for_print, self.n_strands,
        strand.start_resname, strand.start_chain_id, strand.start_resseq,
        strand.start_icode, strand.end_resname, strand.end_chain_id,
        strand.end_resseq, strand.end_icode, strand.sense)
      if reg is not None :
        line += " "
        line += format2 % (reg.cur_atom, reg.cur_resname, reg.cur_chain_id,
          reg.cur_resseq, reg.cur_icode, reg.prev_atom, reg.prev_resname,
          reg.prev_chain_id, reg.prev_resseq, reg.prev_icode)
      else :
        pass
        #assert strand.sense == 0
      if strand_id is not None and strand_id == strand.strand_id:
        return line
      lines.append(line.strip())
    return "\n".join(lines)

  def as_cif_dict(self):
    """Returns dict. keys - cif field names, values - appropriate values,
    lists where needed."""
    sheet_id_to_output = self.sheet_id if isinstance(self.sheet_id, int) else self.sheet_id.strip()
    result = {}
    result['_struct_sheet.id'] = sheet_id_to_output
    result['_struct_sheet.type'] = '?'
    result['_struct_sheet.number_strands'] = self.n_strands
    result['_struct_sheet.details'] = '?'

    result['_struct_sheet_order.sheet_id'] = []
    result['_struct_sheet_order.range_id_1'] = []
    result['_struct_sheet_order.range_id_2'] = []
    result['_struct_sheet_order.offset'] = []
    result['_struct_sheet_order.sense'] = []

    result['_struct_sheet_range.sheet_id'] = []
    result['_struct_sheet_range.id'] = []
    result['_struct_sheet_range.beg_label_comp_id'] = []
    result['_struct_sheet_range.beg_label_asym_id'] = []
    result['_struct_sheet_range.beg_label_seq_id'] = []
    result['_struct_sheet_range.pdbx_beg_PDB_ins_code'] = []
    result['_struct_sheet_range.end_label_comp_id'] = []
    result['_struct_sheet_range.end_label_asym_id'] = []
    result['_struct_sheet_range.end_label_seq_id'] = []
    result['_struct_sheet_range.pdbx_end_PDB_ins_code'] = []

    result['_pdbx_struct_sheet_hbond.sheet_id'] = []
    result['_pdbx_struct_sheet_hbond.range_id_1'] = []
    result['_pdbx_struct_sheet_hbond.range_id_2'] = []
    result['_pdbx_struct_sheet_hbond.range_1_label_atom_id'] = []
    result['_pdbx_struct_sheet_hbond.range_1_label_comp_id'] = []
    result['_pdbx_struct_sheet_hbond.range_1_label_asym_id'] = []
    result['_pdbx_struct_sheet_hbond.range_1_label_seq_id'] = []
    result['_pdbx_struct_sheet_hbond.range_1_PDB_ins_code'] = []
    result['_pdbx_struct_sheet_hbond.range_2_label_atom_id'] = []
    result['_pdbx_struct_sheet_hbond.range_2_label_comp_id'] = []
    result['_pdbx_struct_sheet_hbond.range_2_label_asym_id'] = []
    result['_pdbx_struct_sheet_hbond.range_2_label_seq_id'] = []
    result['_pdbx_struct_sheet_hbond.range_2_PDB_ins_code'] = []

    for i, strand, registration in zip(range(self.n_strands), self.strands, self.registrations):
      # _struct_sheet_order
      if i != 0:
        result['_struct_sheet_order.sheet_id'].append(sheet_id_to_output)
        result['_struct_sheet_order.range_id_1'].append(i)
        result['_struct_sheet_order.range_id_2'].append(i+1)
        result['_struct_sheet_order.offset'].append('?')
        result['_struct_sheet_order.sense'].append(strand.sense_as_cif())

      if strand is not None: # should always be True
        result['_struct_sheet_range.sheet_id'].append(sheet_id_to_output)
        result['_struct_sheet_range.id'].append(i+1)
        result['_struct_sheet_range.beg_label_comp_id'].append(strand.start_resname)
        result['_struct_sheet_range.beg_label_asym_id'].append(strand.start_chain_id)
        result['_struct_sheet_range.beg_label_seq_id'].append(annotation.resseq_as_int(strand.start_resseq))
        result['_struct_sheet_range.pdbx_beg_PDB_ins_code'].append(self.icode_to_cif(strand.start_icode))
        result['_struct_sheet_range.end_label_comp_id'].append(strand.end_resname)
        result['_struct_sheet_range.end_label_asym_id'].append(strand.end_chain_id)
        result['_struct_sheet_range.end_label_seq_id'].append(annotation.resseq_as_int(strand.end_resseq))
        result['_struct_sheet_range.pdbx_end_PDB_ins_code'].append(self.icode_to_cif(strand.end_icode))

      if registration is not None:
        result['_pdbx_struct_sheet_hbond.sheet_id'].append(sheet_id_to_output)
        result['_pdbx_struct_sheet_hbond.range_id_1'].append(i)
        result['_pdbx_struct_sheet_hbond.range_id_2'].append(i+1)
        result['_pdbx_struct_sheet_hbond.range_1_label_atom_id'].append(registration.prev_atom.strip())
        result['_pdbx_struct_sheet_hbond.range_1_label_comp_id'].append(registration.prev_resname)
        result['_pdbx_struct_sheet_hbond.range_1_label_asym_id'].append(registration.prev_chain_id)
        result['_pdbx_struct_sheet_hbond.range_1_label_seq_id'].append(annotation.resseq_as_int(registration.prev_resseq))
        result['_pdbx_struct_sheet_hbond.range_1_PDB_ins_code'].append(self.icode_to_cif(registration.prev_icode))
        result['_pdbx_struct_sheet_hbond.range_2_label_atom_id'].append(registration.cur_atom.strip())
        result['_pdbx_struct_sheet_hbond.range_2_label_comp_id'].append(registration.cur_resname)
        result['_pdbx_struct_sheet_hbond.range_2_label_asym_id'].append(registration.cur_chain_id)
        result['_pdbx_struct_sheet_hbond.range_2_label_seq_id'].append(annotation.resseq_as_int(registration.cur_resseq))
        result['_pdbx_struct_sheet_hbond.range_2_PDB_ins_code'].append(self.icode_to_cif(registration.cur_icode))
    return result


  def as_restraint_group(self, log=sys.stdout, prefix_scope="",
      add_segid=None, show_hbonds=False):
    if len(self.strands) == 0 :
      return None
    selections = []
    senses = []
    reg_curr = []
    reg_prev = []
    for (strand,registration) in zip(self.strands, self.registrations):
      sele = strand.as_atom_selections(add_segid=add_segid)
      selections.append(sele)
      if strand.sense == 0 :
        senses.append("unknown")
      elif strand.sense == -1 :
        senses.append("antiparallel")
      elif strand.sense == 1 :
        senses.append("parallel")
      else :
        raise Sorry("Sense must be 0, 1, or -1.")
      if registration is not None :
        curr, prev = registration.as_atom_selections(add_segid=add_segid)
        reg_curr.append(curr)
        reg_prev.append(prev)
      else :
        reg_curr.append(None)
        reg_prev.append(None)
    # print "selections", selections
    # print "reg_curr", reg_curr
    # print "reg_prev", reg_prev
    # STOP()
    n = 0
    first_strand = None
    strands = []
    for (sele, sense, curr, prev) in zip(selections,senses,reg_curr,reg_prev):
      if n == 0 :
        first_strand = sele
      else :
        strands.append("""\
  strand {
    selection = %s
    sense = %s
    bond_start_current = %s
    bond_start_previous = %s
  }""" % (sele, sense, curr, prev))
      n += 1
    assert first_strand is not None
    if prefix_scope != "" and not prefix_scope.endswith("."):
      prefix_scope += "."
    hbond_restr = ""
    if show_hbonds:
      if self.get_n_defined_hbonds() > 0:
        for hb in self.hbond_list:
          hb_str = "\n  hbond {\n    donor = %s\n    acceptor = %s\n  }" % (
            hb[0], hb[1])
          hbond_restr += hb_str
    phil_str = """
%sprotein.sheet {
  sheet_id = "%s"
  first_strand = %s
%s%s
}""" % (prefix_scope, self.sheet_id, first_strand, "\n".join(strands), hbond_restr)
    # print "phil_str", phil_str
    return phil_str

  def split(self,starting_sheet_id_number=1):
    assert len(self.strands)==len(self.registrations)
    new_sheets=[]
    for s1,s2,r2 in zip(
      self.strands[:-1],
      self.strands[1:],self.registrations[1:]):
      self_id="%d" %(len(new_sheets)+1)
      new_s1=copy.deepcopy(s1)
      new_s1.sense=0
      new_s1.strand_id=1
      new_s1.sheet_id="%d" %(starting_sheet_id_number)
      new_s2=copy.deepcopy(s2)
      new_s2.strand_id=2
      new_s2.sheet_id="%d" %(starting_sheet_id_number)
      new_r2=copy.deepcopy(r2)
      new_sheet=pdb_sheet(
             sheet_id="%d" %(starting_sheet_id_number),
             n_strands=2,
             strands=[new_s1,new_s2],
             registrations=[None,new_r2])
      new_sheets.append(new_sheet)
      starting_sheet_id_number+=1
    return new_sheets

  def chains_included(self):
    "Report list of all chains involved in this sheet"
    chain_list=[]
    for strand in self.strands:
      if not strand.start_chain_id in chain_list:
        chain_list.append(strand.start_chain_id)
      if strand.start_chain_id != strand.start_chain_id and \
         not strand.end_chain_id in chain_list:
          chain_list.append(strand.end_chain_id)
    return chain_list

  def overlaps_with(self,other=None,hierarchy=None):
    "For sheet, overlaps if both members of any strand pair overlap"
    assert hierarchy

    if len(self.strands)<2: return False
    if len(other.strands)<2: return False
    self_sheets=[self]
    other_sheets=[other]
    if len(self.strands)>2 or len(other.strands)>2: # break down
      self_sheets=self.split()
      other_sheets=other.split()
    asc=hierarchy.atom_selection_cache()
    for self_sheet in self_sheets:
      for other_sheet in other_sheets:
        assert len(self_sheet.strands)==2
        assert len(other_sheet.strands)==2

        s1a=self_sheet.strands[0].as_atom_selections()
        s1b=self_sheet.strands[1].as_atom_selections()
        s2a=other_sheet.strands[0].as_atom_selections()
        s2b=other_sheet.strands[1].as_atom_selections()
        for pair_1,pair_2 in [[[s1a,s2a],[s1b,s2b]],[[s1a,s2b],[s1b,s2a]] ]:
          atom_selection=self.combine_atom_selections(pair_1,require_all=True)
          if not atom_selection: continue
          sel = asc.selection(string = atom_selection)
          ph=hierarchy.select(sel, copy_atoms=True)  #ph = needed part of hierarchy
          if ph.overall_counts().n_residues==0: continue

          atom_selection=self.combine_atom_selections(pair_2,require_all=True)
          if not atom_selection: continue
          sel = asc.selection(string = atom_selection)
          ph=hierarchy.select(sel, copy_atoms=True)  #ph = needed part of hierarchy
          if ph.overall_counts().n_residues>0:
            return True  # both match
    return False

  def is_similar_to(self,other=None,hierarchy=None,
      maximum_length_difference=None, minimum_overlap=None,log=sys.stdout):
    # Two sheets are similar if their strands are similar and their
    # alignments are similar, or if reversing the order, they are similar
    if self.is_same_as(other=other): return True  # always quicker

    # Check if two strands are similar to the other two strands
    s1a=self.strands[0]
    s1b=self.strands[1]
    s2a=other.strands[0]
    s2b=other.strands[1]

    # both must be parallel or antiparallel
    if s1b.sense!=s2b.sense: return False

    match_forward=None
    if s1a.is_similar_to(other=s2a,hierarchy=hierarchy,
        maximum_length_difference=maximum_length_difference,
        minimum_overlap=minimum_overlap) and \
       s1b.is_similar_to(other=s2b,hierarchy=hierarchy,
        maximum_length_difference=maximum_length_difference,
        minimum_overlap=minimum_overlap):
      match_forward=True
    elif s1a.is_similar_to(other=s2b,hierarchy=hierarchy,
        maximum_length_difference=maximum_length_difference,
        minimum_overlap=minimum_overlap) and \
       s1b.is_similar_to(other=s2a,hierarchy=hierarchy,
        maximum_length_difference=maximum_length_difference,
        minimum_overlap=minimum_overlap):
      match_forward=False
    if match_forward is None:
      return False

    assert len(self.registrations)==len(other.registrations)
    reg1=self.registrations[1]
    reg2=other.registrations[1]

    if reg1 is None and reg2 is None:
      return True # seems ok
    elif reg1 is None or reg2 is None:
      return False # one has it and other does not

    # otherwise check the two for similarity
    if not match_forward:  # swap other so they should be comparable
      xx=s2a
      s2a=s2b
      s2b=xx
      reg2=reg2.reversed()

    # Now check alignments


    # Find the registration atom positions
    ra=registration_atoms(
      hierarchy=hierarchy,
      strand_1a=s1a,strand_1b=s1b,strand_2a=s2a,strand_2b=s2b,
      registration_1=reg1,registration_2=reg2,
      sense=s1b.sense,log=log)

    return ra.pairs_are_equivalent

  def is_same_as(self,other=None):

    if not other: return False
    if type(self) != type(other): return False # Check for different objects

    if self.n_strands != other.n_strands: return False
    for s1,s2 in zip(self.strands,other.strands):
      if not s1.is_same_as(s2): return False
    for r1,r2 in zip(self.registrations,other.registrations):
      if r1 is None and r2 is None: continue # special case..can be None
      if r1 is None or r2 is None: return False
      if not r1.is_same_as(r2): return False
    return True

if __name__ == "__main__" :
  pass


 *******************************************************************************
