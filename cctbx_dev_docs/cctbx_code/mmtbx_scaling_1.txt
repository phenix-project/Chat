

 *******************************************************************************
mmtbx/scaling/plan_sad_experiment.py

from __future__ import absolute_import, division, print_function
import mmtbx.scaling
from iotbx.bioinformatics import any_sequence_format
from libtbx.utils import Sorry, null_out
from libtbx import table_utils
import math
import sys,os
from six.moves import zip
from six.moves import range

def get_vol_per_residue(chain_type='PROTEIN'):
  if chain_type=='PROTEIN':
    vol_per_residue=135.3
  else:
    vol_per_residue=495.
  return vol_per_residue

def get_atoms_per_residue(chain_type='PROTEIN'):
  if chain_type=='PROTEIN':
    atoms_per_residue=7
  else:
    atoms_per_residue=26
  return atoms_per_residue

def get_nrefl(i_obs=None,
   residues=200,dmin=2.5,solvent_fraction=0.5,chain_type='PROTEIN'):
  if i_obs:  # use actual number of reflections:
    i_obs=i_obs.resolution_filter(d_min=dmin)
    nrefl=i_obs.data().size()
  else:
    nrefl=2.*3.14159*0.3333*residues*\
        get_vol_per_residue(chain_type=chain_type)/ \
        (1.-solvent_fraction)*(1./dmin**3)

  return nrefl

def get_sigf(nrefl,nsites,natoms,z,fpp,target_s_ano=15.,ntries=1000,
     min_cc_ano=None,
     fa2=None,fb2=None,disorder_parameter=None,
     fo_list=None,fo_number_list=None,occupancy=None,include_zero=True,
     ratio_for_failure=0.95,
     resolution=None,
     cc_ano_estimators=None,
     signal_estimators=None,
     max_i_over_sigma=None,
     f_ratio=None):
  closest_sigf=None
  closest_dist2=None
  start_value=1
  if include_zero: start_value=0
  for i in range(start_value,ntries):
    sigf=i*1./float(ntries)
    if max_i_over_sigma and get_i_over_sigma_from_sigf(sigf)>max_i_over_sigma:
      continue
    s_ano,s_ano_sig,cc_ano,cc_ano_sig,cc_half,cc_half_sig,fpp_weak,\
        cc_ano_weak,cc_half_weak,i_over_sigma=\
      get_values_from_sigf(nrefl,nsites,natoms,z,fpp,sigf,
          fa2=fa2,fb2=fb2,disorder_parameter=disorder_parameter,
          fo_list=fo_list,fo_number_list=fo_number_list,occupancy=occupancy,
          get_fpp_weak=False,resolution=resolution,
          cc_ano_estimators=cc_ano_estimators,
          signal_estimators=signal_estimators,
          f_ratio=f_ratio)

    if min_cc_ano is not None and cc_ano_weak < min_cc_ano:
      continue

    dist2=(s_ano-target_s_ano)**2
    if closest_dist2 is None or dist2<closest_dist2:
     closest_dist2=dist2
     closest_sigf=sigf
  if closest_sigf==0:  # try with target of 90% of maximum available
    s_ano,s_ano_sig,cc_ano,cc_ano_sig,cc_half,cc_half_sig,\
       fpp_weak,cc_ano_weak,\
       cc_half_weak,i_over_sigma=\
       get_values_from_sigf(nrefl,nsites,natoms,z,fpp,closest_sigf,
          fa2=fa2,fb2=fb2,disorder_parameter=disorder_parameter,
          fo_list=fo_list,fo_number_list=fo_number_list,occupancy=occupancy,
          get_fpp_weak=False,resolution=resolution,
          cc_ano_estimators=cc_ano_estimators,
          signal_estimators=signal_estimators,
          f_ratio=f_ratio)
    closest_sigf=get_sigf(
     nrefl,nsites,natoms,z,fpp,target_s_ano=s_ano*ratio_for_failure,
     ntries=ntries,
     min_cc_ano=min_cc_ano,
     fa2=fa2,fb2=fb2,disorder_parameter=disorder_parameter,
     fo_list=fo_list,fo_number_list=fo_number_list,
     occupancy=occupancy,include_zero=False,resolution=resolution,
     cc_ano_estimators=cc_ano_estimators,
     signal_estimators=signal_estimators,
     max_i_over_sigma=max_i_over_sigma,
     f_ratio=f_ratio)
  return closest_sigf

def get_sano(nrefl,nsites,natoms,z,fpp,sigf,
       fa2=None,fb2=None,disorder_parameter=None,
       fo_list=None,fo_number_list=None,occupancy=None,
       f_ratio=None):

  # nrefl = number of anomalous data
  # nsites = number of anomalously scattering atoms in asymmetric unit
  # natoms = number of non-anomalously scattering atoms in asymmetric unit
  # z = typical Z of non-anomalously scattering atoms
  # sigf = rms(sigF)/rms(F) overall
  # delta_b_ano=B for anom atoms minus B for rest of structure.
  #  Ignoring this effect here but including f_ratio 2014-12-08
  #sano2=f_ratio*(nrefl/nsites)/(1.+(natoms/nsites)*(z*sigf/fpp)**2)

  # more detailed calculation with fa2 and fb2 and disorder_parameter:
  # sano=cc_ano*f_ratio*nrefl/nsites)
  cc_ano=get_cc_ano(nrefl,nsites,natoms,z,fpp,sigf,
      fa2=fa2,fb2=fb2,disorder_parameter=disorder_parameter,
      fo_list=fo_list,fo_number_list=fo_number_list,occupancy=occupancy)
  sano_new=cc_ano*f_ratio*math.sqrt(nrefl/nsites)
  return sano_new

def get_cc_ano(nrefl,nsites,natoms,z,fpp,sigf,
      fa2=None,fb2=None,disorder_parameter=None,
      fo_list=None,fo_number_list=None,occupancy=None):
  "cc_ano**2=1/[1 + <sigF**2>/(n/N)(Za/Z)**2<F**2>]"
  #ccano2=1./(1.+sigf**2*natoms*z**2/(nsites*fpp**2))

  # recalculate fa2 and fb2 based on fpp value:
  target_fpp_list=[fpp]
  target_fpp_number_list=[nsites]

  local_fa2=get_normalized_scattering(
      fpp_list=target_fpp_list,
      fpp_number_list=target_fpp_number_list,
      fo_list=fo_list,
      fo_number_list=fo_number_list,
      occupancy=occupancy)

  #  more detailed: cc_ano**2=1/(1+Q+(fb2/fa2)+(sigf2/fa2))
  ccano2_new=1./(1.+disorder_parameter+(fb2/local_fa2)+(sigf**2/local_fa2))

  return math.sqrt(ccano2_new)

def get_cc_half(cc_ano,disorder_parameter=None):
  "cc_half=cc_ano_perf**2/(2-cc_ano_perf**2)"

  #cc_half=cc_ano*cc_ano/(2.-cc_ano*cc_ano)

  # with disorder_parameter, cc**_ano (useful cc_ano)=cc_ano/sqrt(1+Q)
  #   or cc_ano=cc**_ano*sqrt(1+Q), where cc**_ano is cc_ano_perf, useful cc_ano
  ccano2=min(1.,cc_ano*cc_ano*(1+disorder_parameter))
  cc_half_new=ccano2/(2.-ccano2)
  return cc_half_new

def estimate_fpp_weak(nrefl,nsites,natoms,z,fpp,sigf,
      fa2=None,fb2=None,disorder_parameter=None,
      fo_list=None,fo_number_list=None,occupancy=None,
      f_ratio=None):
  # estimate fpp that would give half the sano value
  sano=get_sano(nrefl,nsites,natoms,z,fpp,sigf,
      fa2=fa2,fb2=fb2,disorder_parameter=disorder_parameter,
      fo_list=fo_list,fo_number_list=fo_number_list,occupancy=occupancy,
      f_ratio=f_ratio)

  target=sano/2.
  best_scale=1.0
  for i in range(1,101):
    scale=float(i)*0.01
    ss=get_sano(nrefl,nsites,natoms,z,fpp*scale,sigf,
      fa2=fa2,fb2=fb2,disorder_parameter=disorder_parameter,
      fo_list=fo_list,fo_number_list=fo_number_list,occupancy=occupancy,
      f_ratio=f_ratio)
    if ss >=target and scale<best_scale:
      best_scale=scale
      break


  return fpp*best_scale

def get_i_over_sigma_from_sigf(sigf):
  # <I/sigI> = 0.88 * rms(F)/rms(sigF) r**2=0.82
  if sigf>0:
    return 0.88/sigf
  else:
    return 999.

def get_sigf_from_i_over_sigma(i_over_sigma):
  # just inverse of get_i_over_sigma_from_sigf
  #  ios=.88/sigf  -> sigf=0.88/ios
  if i_over_sigma > 0:
     sigf=0.88/i_over_sigma
  else:
     sigf=0.001
  return sigf

def get_i_over_sigma(i_obs):
  data=i_obs.data().select(i_obs.sigmas() > 0)
  sigmas=i_obs.sigmas().select(i_obs.sigmas() > 0)
  iover=data/sigmas
  return iover.min_max_mean().mean

def get_values_from_sigf(nrefl,nsites,natoms,z,fpp,sigf,
       fa2=None,fb2=None,disorder_parameter=None,
       fo_list=None,fo_number_list=None,occupancy=None,
       get_fpp_weak=True,resolution=None,
       cc_ano_estimators=None,signal_estimators=None,
       f_ratio=None):
    # 2014-11-04 add capability for Bayesian estimation (update) of
    #   signal and cc_ano
    s_ano=get_sano(nrefl,nsites,natoms,z,fpp,sigf,
      fa2=fa2,fb2=fb2,disorder_parameter=disorder_parameter,
      fo_list=fo_list,fo_number_list=fo_number_list,occupancy=occupancy,
      f_ratio=f_ratio)
    cc_ano=get_cc_ano(nrefl,nsites,natoms,z,fpp,sigf,
      fa2=fa2,fb2=fb2,disorder_parameter=disorder_parameter,
      fo_list=fo_list,fo_number_list=fo_number_list,occupancy=occupancy)

    if cc_ano_estimators:
      cc_ano,cc_ano_sig=cc_ano_estimators.apply_estimators(
         value_list=[cc_ano],data_items=['pred_cc_perfect'],
         resolution=resolution)
    else:
      cc_ano_sig=None
    if signal_estimators:
      s_ano,s_ano_sig=signal_estimators.apply_estimators(
         value_list=[s_ano],data_items=['pred_signal'],
         resolution=resolution)
    else:
      s_ano_sig=None

    cc_half=get_cc_half(cc_ano,disorder_parameter=disorder_parameter)

    if cc_ano_estimators:
      cc_half_high=get_cc_half(cc_ano+cc_ano_sig,
         disorder_parameter=disorder_parameter)
      cc_half_low=get_cc_half(max(0,cc_ano-cc_ano_sig),
         disorder_parameter=disorder_parameter)
      cc_half_sig=0.5*(cc_half_high-cc_half_low)
    else:
      cc_half_sig=None

    if get_fpp_weak:
      fpp_weak=estimate_fpp_weak(nrefl,nsites,natoms,z,fpp,sigf,
        fa2=fa2,fb2=fb2,disorder_parameter=disorder_parameter,
        fo_list=fo_list,fo_number_list=fo_number_list,occupancy=occupancy,
        f_ratio=f_ratio)
      cc_ano_weak=get_cc_ano(nrefl,nsites,natoms,z,fpp_weak,sigf,
        fa2=fa2,fb2=fb2,disorder_parameter=disorder_parameter,
        fo_list=fo_list,fo_number_list=fo_number_list,occupancy=occupancy)

      if cc_ano_estimators:
        cc_ano_weak,cc_ano_weak_sig=cc_ano_estimators.apply_estimators(
           value_list=[cc_ano_weak],data_items=['pred_cc_perfect'],
           resolution=resolution)

      cc_half_weak=get_cc_half(cc_ano_weak,
          disorder_parameter=disorder_parameter)

      i_over_sigma=get_i_over_sigma_from_sigf(sigf)
      return s_ano,s_ano_sig,cc_ano,cc_ano_sig,cc_half,cc_half_sig,fpp_weak,\
          cc_ano_weak,cc_half_weak,i_over_sigma
    else:
       return s_ano,s_ano_sig,cc_ano,cc_ano_sig,cc_half,cc_half_sig,\
         fpp,cc_ano,cc_half,0.0

def get_fp_fdp(atom_type=None,wavelength=None,out=sys.stdout):
  if not atom_type or not wavelength:
    raise Sorry("Please specify either f_double_prime or " +
      "atom_type and wavelength")
  from cctbx.eltbx import sasaki
  try:
    table = sasaki.table(atom_type)
    fp_fdp = table.at_angstrom(wavelength)
  except Exception:
    return None
#
  if not fp_fdp.is_valid():
    from cctbx.eltbx import henke
    try:
      table = henke.table(atom_type)
      fp_fdp = table.at_angstrom(wavelength)
    except Exception:
      return None
    #print "Using Henke tables for scattering factors"
  if fp_fdp.is_valid():
    return fp_fdp
  else:
    return None

def get_fo(atom_type=None,wavelength=None,out=sys.stdout):
  if not atom_type or not wavelength:
    raise Sorry("Please specify either f_double_prime or " +
      "atom_type and wavelength")
  from cctbx.eltbx import sasaki
  try:
    table = sasaki.table(atom_type)
    fo=table.atomic_number()
  except ValueError :
    #ab=b
    raise Sorry("Unable to get scattering factors for %s" %(atom_type))
  return fo

def get_aa_and_met(sequence):
  sequence=sequence.upper().replace(" ","")
  n_aa=len(sequence)
  n_met=sequence.count("M")
  n_cys=sequence.count("C")
  return n_aa,n_met,n_cys

def get_number_of_sites(atom_type=None,n_met=0,n_cys=0,
      n_aa=0,ncs_copies=1,out=sys.stdout):
    # guess number of sites:
    number_of_sites_lowres=None
    if atom_type is None:
      print("No heavy atom type set, so no sites estimated", file=out)
      number_of_sites=None
    elif atom_type.lower() in ['se']:
      number_of_sites=max(1,ncs_copies*n_met)
    elif atom_type.lower() in ['s']:
      number_of_sites=max(1,ncs_copies*(n_met+n_cys))
      number_of_sites_lowres=ncs_copies*(n_met+int(float(n_cys)/2.))
    elif atom_type.lower() in ['br','i']: # 1 per 20 up to 10 per chain
      number_of_sites=ncs_copies*max(1,min(10,1+int(float(n_aa)/20.)))
    else: # general ha  1 per 100 up to 2 per chain
      number_of_sites=ncs_copies*max(1,min(2,1+int(float(n_aa)/100.)))
    if number_of_sites:
      print("\nBest guess of number of %s sites: %d" %(
        atom_type.upper(),number_of_sites), file=out)
    if number_of_sites_lowres is None: number_of_sites_lowres=number_of_sites
    return number_of_sites,number_of_sites_lowres

def get_residues_and_ha(seq_file=None,atom_type=None,
      chain_type=None,data=None,solvent_fraction=None,
      ncs_copies=None,out=sys.stdout):

  if not seq_file or not os.path.isfile(seq_file):
    raise Sorry("Please supply number of residues or a sequence file")
  objects, non_compliant = any_sequence_format(seq_file)
  if non_compliant or (objects is None):
    raise Sorry("Sorry, unable to read the sequence file %s" %(seq_file))
  n_aa, n_met, n_cys = 0, 0, 0
  for seq_obj in objects :
    n_aa_,n_met_,n_cys_ = get_aa_and_met(sequence=seq_obj.sequence)
    n_aa += n_aa_
    n_met += n_met_
    n_cys += n_cys_
  number_of_s=n_met+n_cys
  number_of_sites,number_of_sites_lowres=get_number_of_sites(
      atom_type=atom_type,n_met=n_met,n_cys=n_cys,
      n_aa=n_aa,ncs_copies=1,out=null_out())

  # if data file is specified, use it to get crystal_symmetry and then estimate
  # residues and ha using that information and seq_file. Otherwise guess
  if data and os.path.isfile(data):
    try:
      from phenix.command_line.ncs_and_number_of_ha import ncs_and_number_of_ha
    except Exception as e: # not available
      return n_aa,number_of_sites,number_of_s,solvent_fraction,ncs_copies
    args=["data=%s" %(data)]
    if seq_file: args.append("seq_file=%s" %(seq_file))
    if atom_type: args.append("atom_type=%s" %(atom_type))
    if chain_type: args.append("chain_type=%s" %(chain_type))
    if ncs_copies: args.append("ncs_copies=%s" %(ncs_copies))
    args.append("log=None")
    args.append("params_out=None")
    na=ncs_and_number_of_ha(args=args,out=null_out())
    return na.ncs_copies*n_aa,na.number_of_sites,na.ncs_copies*number_of_s,\
      na.solvent_fraction,na.ncs_copies
  else:
    return n_aa,number_of_sites,number_of_s,solvent_fraction,ncs_copies

def get_disorder_parameter(ideal_cc_anom=None):
  if not ideal_cc_anom:
    return 0.  #
  else:
    # E=1+Q; ideal_cc_anom=1/sqrt(E) -> Q=1/ideal_cc_anom**2 - 1
    cc2=ideal_cc_anom**2
    return (1.-cc2)/cc2

def include_intrinsic_scatterers(
   intrinsic_scatterers_list,target_fpp,wavelength,
    minimum_ratio=0.25):
  all_weak=True # noise if all are weak
  for x in intrinsic_scatterers_list:
    fp_fdp=get_fp_fdp(atom_type=x,wavelength=wavelength)
    if fp_fdp is not None:
      fpp=fp_fdp.fdp()
      if fpp >= minimum_ratio*target_fpp:
        all_weak=False
    else:
      fpp=None
  if all_weak:
    return True
  else:
    return False


def get_fo_list(chain_type="PROTEIN",wavelength=1.0,
    residues=1,
    target_fpp=None,target_atom_type=None,target_n=None,
    intrinsic_scatterers_as_noise=None,
    include_weak_anomalous_scattering=None,
    number_of_s=0.,out=sys.stdout):

  if chain_type=="PROTEIN":
     intrinsic_scatterers_list=['S']
     if number_of_s is None:
       number_of_s=0.044*residues # typical s content
     intrinsic_scatterers_number_list=[number_of_s/residues]
     atoms_list=['C','N','O']
     atoms_number_list=[5.15,1.37,1.58] # average (for a2u-globulin)
  else:
     intrinsic_scatterers_list=['P']
     intrinsic_scatterers_number_list=[1]
     atoms_list=['C','N','O']
     atoms_number_list=[8.75,3.75,6.]  # average of DNA
     if chain_type=="RNA":
       atoms_number_list[2]+=1 # an extra O in RNA

  # decide if we include the intrinsic scatterers
  if intrinsic_scatterers_as_noise is None:
    force_include=include_intrinsic_scatterers(
          intrinsic_scatterers_list,target_fpp,wavelength)
  else:
    force_include=False
  if intrinsic_scatterers_as_noise or force_include:
       atoms_list+=intrinsic_scatterers_list
       atoms_number_list+=intrinsic_scatterers_number_list

  fo_list=[]
  fo_number_list=[]
  fpp_list=[]
  fpp_number_list=[]
  noise_table_rows = []
  n_atoms=0
  for x,n in zip(atoms_list,atoms_number_list):
    fp_fdp=get_fp_fdp(atom_type=x,wavelength=wavelength)
    if fp_fdp is not None:
      fpp=fp_fdp.fdp()
    else:
      fpp=None
    fo=get_fo(atom_type=x,wavelength=wavelength)
    fo_list.append(fo)
    fo_number_list.append(n*residues)
    n_atoms+=n*residues
    if include_weak_anomalous_scattering and fpp is not None:
      fpp_list.append(fpp)
      fpp_number_list.append(n*residues)
      contribution=fpp*math.sqrt(n*residues)
      noise_table_rows.append(
         (x,fpp,n*residues,contribution)
      )
  return fo_list,fo_number_list,fpp_list,fpp_number_list,\
      n_atoms,noise_table_rows

def get_normalized_scattering(
      fpp_list=[],
      fpp_number_list=[],
      fo_list=[],
      fo_number_list=[],
      occupancy=1.,
      return_total=False):
  # estimate ratio of anomalous to real scattering for these atoms
  # sum(Z_b_i**2 N_b_i)/sum(Z_i**2 N_i)
  sum_real=0.
  for fo,fo_n in zip(fo_list,fo_number_list):
    sum_real+=fo**2*fo_n
  sum_anom=0.
  for fpp,fpp_n in zip(fpp_list,fpp_number_list):
    sum_anom+=(occupancy*fpp)**2*fpp_n
  if sum_real <=0:
    sum_real=1.
  if return_total:
    return sum_real
  else:
    return sum_anom/sum_real

def get_local_file_name(estimator_type):
  no_resolution=False

  # Estimators from experimental data

  if estimator_type=='cc_star':  # cc_perfect from cc_st_square skew e
    local_file_name='cc_ano_data.dat'
  elif estimator_type=='cc_exptl':  # cc_exptl from cc_perfect
    local_file_name='cc_exptl_data.dat'
  elif estimator_type=='b_from_dmin':  # Wilson B estimated from dmin
    local_file_name='b_from_dmin.dat'
    no_resolution=True
  elif estimator_type=='dmin_from_b':  # dmin estimated from Wilson B
    local_file_name='dmin_from_b.dat'
    no_resolution=True
  elif estimator_type=='ha_b_from_wilson': # B of anom atom est from Wilson B
    local_file_name='ha_b_from_wilson.dat'
    no_resolution=True

  # Estimators of targets from theoretical estimates

  elif estimator_type=='signal':  # signal from pred_signal
    local_file_name='signal_from_est_signal.dat'
  elif estimator_type=='cc_ano': # cc_perfect from pred_cc_perfect
    local_file_name='cc_anom_from_cc_anom_star.dat'
  elif estimator_type=='solved':
    local_file_name='percent_solved_vs_signal.dat'
  else:
    raise Sorry("No estimator type %s" %(estimator_type))
  return local_file_name,no_resolution

class interpolator:
  # basic interpolator. Call with list of pairs of (predictor,target)
  # list can have extra data (ignored)
  # then apply with (predictor) and get an estimate of target
  # If off the end in either direction, use last available value (do not
  #   extrapolate)
  def __init__(self,target_predictor_list,extrapolate=False,
      require_monotonic_increase=False):
    assert not extrapolate # (not programmed)
    # make a dict
    self.target_dict={}
    self.keys=[]
    for values in target_predictor_list:
       predictor=values[0]
       target=values[1]
       self.target_dict[predictor]=target
       self.keys.append(predictor)
    self.keys.sort()
    if require_monotonic_increase:
      self.set_monotonic_increase()
    from copy import deepcopy
    self.reverse_keys=deepcopy(self.keys)
    self.reverse_keys.reverse()  # so we can go down too

  def set_monotonic_increase(self):
    # require never to decrease value with increasing key
    # start at middle and make sure that value never goes above
    #  mean of remaining or below previous
    # verify that overall means for each end do not change
    n=len(self.keys)
    if n<2: return
    i_middle=n//2
    from copy import deepcopy
    value_list=[]
    for key in self.keys:
      value_list.append(self.target_dict[key])
    new_values=deepcopy(value_list)
    for i in range(i_middle+1,n):
      prev_value=new_values[i-1]
      remainder=value_list[i:]
      value=value_list[i]
      mean_remainder=self.get_mean(remainder)
      new_values[i]=max(prev_value,min(mean_remainder,value))
    for i in range(i_middle-1,-1,-1):
      prev_value=new_values[i+1]
      remainder=value_list[:i+1]
      value=value_list[i]
      mean_remainder=self.get_mean(remainder)
      new_values[i]=min(prev_value,max(mean_remainder,value))
    from cctbx.array_family import flex
    a=flex.double()
    b=flex.double()
    delta_list=[]
    for key,value in zip(self.keys,new_values):
      delta_list.append(value-self.target_dict[key])
    # adjust delta mean to zero in each region
    offset_low=self.get_mean(delta_list[:i_middle])
    offset_high=self.get_mean(delta_list[i_middle+1:])
    for i in range(i_middle+1,n):
      new_values[i]=new_values[i]-offset_high
    for i in range(i_middle-1,-1,-1):
      new_values[i]=new_values[i]-offset_low
    for key,value in zip(self.keys,new_values):
      self.target_dict[key]=value

  def get_mean(self,remainder):
    n=len(remainder)
    if n<1: return 0
    a=0.
    for x in remainder: a+=x
    return a/n



  def interpolate(self,predictor):
    lower_pred=None
    dist_from_lower=None
    higher_pred=None
    dist_from_higher=None
    if predictor <= self.keys[0]:
      return self.target_dict[self.keys[0]]
    elif predictor >= self.keys[-1]:
      return self.target_dict[self.keys[-1]]
    else:  # between two keys. Find highest key < predictor and lowest > pred
      for key in self.reverse_keys: # high to low values of keys
        if predictor > key:
          lower_pred=self.target_dict[key]
          dist_from_lower=predictor-key
          break
      for key in self.keys: # low to high values of keys
        if predictor < key:
          higher_pred=self.target_dict[key]
          dist_from_higher=key-predictor
          break
      if lower_pred is None or higher_pred is None:
        raise Sorry("Unable to interpolate...")
      dist=(dist_from_lower+dist_from_higher)
      value=lower_pred+(higher_pred-lower_pred)*dist_from_lower/dist
      return value


def get_interpolator(estimator_type='solved',predictor_variable='PredSignal',
       require_monotonic_increase=None,out=sys.stdout):
  local_file_name,no_resolution=get_local_file_name(estimator_type)
  print("\nSetting up interpolator for %s" %(estimator_type), file=out)
  import libtbx.load_env
  file_name=libtbx.env.find_in_repositories(
    relative_path=os.path.join("mmtbx","scaling",local_file_name),
      test=os.path.isfile)
  # data looks like:
  """
Signal  %Solved   N  PredSignal  %Solved  N   BayesEstSignal  %Solved  N
   1.0       2    44        1.0       1    69        1.0       0     6
   3.0       0   123        3.0       0    95        3.0       9   104
  """
  # Pick up 2 columns starting with predictor variable

  from mmtbx.scaling.bayesian_estimator import get_table_as_list
  prediction_values_as_list,info_values_as_list,\
     dummy_target_variable,dummy_data_items,dummy_info_items=\
    get_table_as_list(file_name=file_name, select_only_complete=False,
    start_column_header=predictor_variable,out=out)
  return interpolator(prediction_values_as_list,
       require_monotonic_increase=require_monotonic_increase)


def get_estimators(estimator_type='signal',
    min_in_bin=50,
    resolution_cutoffs=None,out=sys.stdout):
  # get estimators for signal from est_signal or cc_ano from cc*_ano
  local_file_name,no_resolution=get_local_file_name(estimator_type)

  import libtbx.load_env

  print("\nSetting up estimator for %s" %(estimator_type), file=out)
  file_name=libtbx.env.find_in_repositories(
    relative_path=os.path.join("mmtbx","scaling",local_file_name),
      test=os.path.isfile)

  from mmtbx.scaling.bayesian_estimator import estimator_group
  if no_resolution: resolution_cutoffs=None
  estimators=estimator_group(
    min_in_bin=min_in_bin,
    resolution_cutoffs=resolution_cutoffs,out=out)
  estimators.set_up_estimators(
    file_name=file_name,select_only_complete=False)
  estimators.show_summary()
  return estimators

def get_b_value_anomalous_and_resolution_from_file(data=None,data_labels=None):
  import iotbx.merging_statistics
  i_obs=iotbx.merging_statistics.select_data( allow_amplitudes=True,
           file_name=data,
           data_labels=data_labels,
           log=null_out())
  i_obs=i_obs.merge_equivalents().array()
  d_max,d_min=i_obs.d_max_min()
  assert i_obs.is_xray_intensity_array()
  b_aniso_mean=get_b_aniso_mean(i_obs)
  return b_aniso_mean,d_min,i_obs

def get_b_aniso_mean(i_obs):
  from mmtbx.scaling import absolute_scaling
  aniso_scale_and_b = absolute_scaling.ml_aniso_absolute_scaling(
        miller_array = i_obs,
        n_residues = 200,
        n_bases = 0)
  try: b_cart=aniso_scale_and_b.b_cart
  except AttributeError as e:
        raise Sorry("\nCannot correct the column %s for anisotropy\n"%(
          best_label))
  b_aniso_mean=0.
  for k in [0,1,2]:
    b_aniso_mean+=b_cart[k]
  return b_aniso_mean/3.0

def get_b_value_anomalous_and_resolution(
     b_value=None,
     resolution=None,
     data=None,
     data_labels=None,
     b_value_anomalous=None,
     min_in_bin=50,
     i_obs=None):
  # use bayesian estimator to get b_value from resolution or vice_versa
  # (need one or the other)
  if data:  # get b_value and resolution from file if provided
     b_value_from_file,resolution_from_file,i_obs=\
       get_b_value_anomalous_and_resolution_from_file(data=data,
       data_labels=data_labels)
     if b_value is None: b_value=b_value_from_file
     if resolution is None: resolution=resolution_from_file
  if b_value is None and resolution is None:
    raise Sorry("Sorry need either Wilson B value or resolution")
  elif b_value is not None and resolution is not None:
    pass # do nothing
  elif b_value is None and resolution is not None:
     b_value_estimators=get_estimators(
       min_in_bin=min_in_bin,
       estimator_type='b_from_dmin',out=null_out())
     b_value,sig_b_value=b_value_estimators.apply_estimators(
         value_list=[resolution],data_items=['dmin_resol'])
  else:
     dmin_estimators=get_estimators(estimator_type='dmin_from_b',
       min_in_bin=min_in_bin,
       out=sys.stdout)
     resolution,sig_resolution=dmin_estimators.apply_estimators(
         value_list=[b_value],data_items=['B'])

  if b_value_anomalous is None:  # estimate it from b_value
     b_value_anomalous_estimators=get_estimators(
       min_in_bin=min_in_bin,
       estimator_type='ha_b_from_wilson',out=null_out())
     b_value_anomalous,sig_b_value_anomalous=\
         b_value_anomalous_estimators.apply_estimators(
         value_list=[b_value],data_items=['B-overall'])
  return b_value_anomalous,resolution,i_obs


def get_dmin_ranges(resolution=None,target_list=[6,5,3,2.5,2,1.5]):
  # get ranges from target_list starting with something near resolution:
#  best_match=None
#  best_dist=None
#  for x in target_list:
#    dist=abs(x-resolution)
#    if best_match is None or dist<best_dist:
#      best_dist=dist
#      best_match=x
  new_list=[]
  for x in target_list:
    if x > resolution:
      new_list.append(x)
    #if best_match==x: break
  new_list.append(resolution)
  return new_list


class estimate_necessary_i_sigi(mmtbx.scaling.xtriage_analysis):
  def __init__(self,
      chain_type='PROTEIN',
      residues=250,
      number_of_s=0,
      solvent_fraction=0.50,
      nsites=5,
      wavelength=1.0,
      atom_type=None,
      fpp=3.8,
      target_s_ano=30,
      min_cc_ano=0.15,
      resolution=None,  # estimated overall resolution of data
      b_value=None,     # estimated Wilson B of dataset
      b_value_anomalous=None,     # estimated B for anomalous atoms
      fixed_resolution=None,
      occupancy=1.,
      ideal_cc_anom=0.76,
      bayesian_updates=True,
      include_weak_anomalous_scattering=True,
      intrinsic_scatterers_as_noise=None,
      ratio_for_failure=0.95,
      i_over_sigma=None,
      max_i_over_sigma=None,
      min_in_bin=50,
      data=None,
      data_labels=None,
      quiet=False):
    self.min_in_bin = min_in_bin
    self.chain_type = chain_type
    self.residues = residues
    self.solvent_fraction = solvent_fraction
    self.nsites = nsites
    self.fpp = fpp
    self.target_s_ano = target_s_ano
    self.min_cc_ano = min_cc_ano
    self.wavelength = wavelength
    self.max_i_over_sigma = max_i_over_sigma
    self.bayesian_updates = bayesian_updates
    if atom_type is None:
      self.atom_type='-'
    else:
      self.atom_type=atom_type
    self.occupancy=occupancy
    self.ratio_for_failure=ratio_for_failure

    b_value_anomalous,resolution,i_obs=\
         get_b_value_anomalous_and_resolution(
     b_value=b_value,
     resolution=resolution,
     data=data,
     min_in_bin=self.min_in_bin,
     data_labels=data_labels,
     b_value_anomalous=b_value_anomalous)
    self.resolution=resolution
    self.b_value_anomalous=b_value_anomalous

    vol_per_residue = get_vol_per_residue(chain_type=chain_type)

    # q (disorder_parameter) = normalized mean square anom diff not due
    #   to target atoms, E=1+Q
    self.disorder_parameter=get_disorder_parameter(ideal_cc_anom=ideal_cc_anom)

    # atoms in structure and their numbers, normal and anomalous scattering
    fo_list,fo_number_list,fpp_list,fpp_number_list,\
       self.natoms,self.noise_table_rows=get_fo_list(
        residues=residues,
        chain_type=chain_type,
        wavelength=self.wavelength,
        target_fpp=self.fpp,
        target_atom_type=atom_type,
        target_n=nsites,
        intrinsic_scatterers_as_noise=intrinsic_scatterers_as_noise,
        include_weak_anomalous_scattering=include_weak_anomalous_scattering,
        number_of_s=number_of_s)


    # scattering from target anomalous atoms:
    # fa2=Occ**2*sum(Z_a_i**2 N_a_i)/sum(Z_i**2 N_i)
    target_fpp_list=[self.fpp]
    target_fpp_number_list=[self.nsites]

    self.fa2=get_normalized_scattering(
      fpp_list=target_fpp_list,
      fpp_number_list=target_fpp_number_list,
      fo_list=fo_list,
      fo_number_list=fo_number_list,
      occupancy=occupancy)

    # scattering from all other anomalous atoms
    # fb2=sum(Z_b_i**2 N_b_i)/sum(Z_i**2 N_i)

    self.fb2=get_normalized_scattering(
      fpp_list=fpp_list,
      fpp_number_list=fpp_number_list,
      fo_list=fo_list,
      fo_number_list=fo_number_list)


    # get real scattering from anomalous substructure
    if not self.atom_type or self.atom_type=="-":
     fo=0.
    else:
      fo=get_fo(atom_type=self.atom_type,wavelength=self.wavelength)
      if self.occupancy: fo=fo*self.occupancy
    self.fc2=get_normalized_scattering(
      fpp_list=[],
      fpp_number_list=[],
      fo_list=[fo],
      fo_number_list=[self.nsites],
      return_total=True)
    # and real scattering from entire molecule including substructure
    self.fd2=get_normalized_scattering(
      fpp_list=[],
      fpp_number_list=[],
      fo_list=fo_list,
      fo_number_list=fo_number_list,
      return_total=True)

    z=6.7 # Hendrickson, W.A. (2014) Quarterly Rev. Biophys. 47, 49-93.
    self.dmin_ranges=get_dmin_ranges(resolution=self.resolution) # changed 2014-12-26 to always get it
    self.table_rows = []
    self.representative_values = None
    self.skipped_resolutions = []
    self.missed_target_resolutions = []
    self.input_i_over_sigma=i_over_sigma
    self.used_max_i_over_sigma=True

    if self.bayesian_updates:

       # set up estimators of cc_ano from cc_*_ano and signal from est_signal

       # estimate cc_perfect from pred_cc_perfect
       self.cc_ano_estimators=get_estimators(estimator_type='cc_ano',
         min_in_bin=self.min_in_bin,
         resolution_cutoffs=self.dmin_ranges,out=null_out())

       # estimate true signal from estimated signal
       self.signal_estimators=get_estimators(estimator_type='signal',
         min_in_bin=self.min_in_bin,
         resolution_cutoffs=self.dmin_ranges,out=null_out())

    else:
      self.cc_ano_estimators=None
      self.signal_estimators=None

    # estimate fom of phasing from cc_perfect
    # key    d_min cc_exptl cc_perfect
    self.fom_estimators=get_estimators(estimator_type='cc_exptl',
      min_in_bin=self.min_in_bin,
      resolution_cutoffs=self.dmin_ranges,out=null_out())

    # solved (probability of hyss finding >=50% of sites) is just a table
    #  so interpolate the probability.
    self.solved_interpolator=get_interpolator(estimator_type='solved',
       require_monotonic_increase=True,
       predictor_variable='PredSignal',out=null_out())

    if fixed_resolution:
      dmin_ranges=self.dmin_ranges[-1:]
    else:
      dmin_ranges=self.dmin_ranges
    for dmin in dmin_ranges:
      # Guess reflections from residues, dmin, solvent fraction

      # get f_ratio:
      from mmtbx.scaling.mean_f_rms_f import ratio_mean_f_to_rms_f
      f_ratio=ratio_mean_f_to_rms_f(dmin,b_value_anomalous)

      nrefl=get_nrefl(i_obs=i_obs,
         residues=residues,dmin=dmin,solvent_fraction=solvent_fraction)

      # identify rms(sigF)/rms(F) necessary to get target_s_ano with this
      # many reflections, sites, atoms, f" value

      if not i_over_sigma:
        sigf=get_sigf(nrefl,nsites,self.natoms,z,fpp,target_s_ano=target_s_ano,
          min_cc_ano=min_cc_ano,
          fa2=self.fa2,fb2=self.fb2,disorder_parameter=self.disorder_parameter,
          fo_list=fo_list,fo_number_list=fo_number_list,occupancy=occupancy,
          ratio_for_failure=self.ratio_for_failure,resolution=dmin,
          cc_ano_estimators=None,
          signal_estimators=None,
          max_i_over_sigma=self.max_i_over_sigma,
          f_ratio=f_ratio)
        if sigf is None: # failed so far...
          self.used_max_i_over_sigma=True
          sigf=get_sigf_from_i_over_sigma(self.max_i_over_sigma)
      else:  # input i_over_sigma...estimate sigf
        sigf=get_sigf_from_i_over_sigma(i_over_sigma)
      # what are expected signal, useful cc_ano, cc_half-dataset, <I>/<sigI>


      s_ano,s_ano_sig,cc_ano,cc_ano_sig,cc_half,cc_half_sig,\
          fpp_weak,cc_ano_weak,\
          cc_half_weak,local_i_over_sigma=\
        get_values_from_sigf(nrefl,nsites,self.natoms,z,fpp,sigf,
          fa2=self.fa2,fb2=self.fb2,disorder_parameter=self.disorder_parameter,
          fo_list=fo_list,fo_number_list=fo_number_list,occupancy=occupancy,
          get_fpp_weak=True,
          resolution=dmin,
          cc_ano_estimators=self.cc_ano_estimators,
          signal_estimators=self.signal_estimators,
          f_ratio=f_ratio)


      if local_i_over_sigma>=999:
        self.skipped_resolutions.append(dmin)
        continue  # hopeless
      if s_ano<0.95*target_s_ano:  # must not be able to get target s_ano
        self.missed_target_resolutions.append(dmin)

      solved=self.solved_interpolator.interpolate(s_ano)
      if solved is None: solved=0.0
      if s_ano_sig is None: s_ano_sig=0.0
      if cc_half_sig is None: cc_half_sig=0.0
      if cc_ano_sig is None: cc_ano_sig=0.0
      s_ano_weak=max(0,s_ano-s_ano_sig)
      cc_half_weak=max(0,cc_half-cc_half_sig)
      cc_ano_weak=max(0.,cc_ano-cc_ano_sig)
      fom,s_fom=self.fom_estimators.apply_estimators(
       value_list=[cc_ano],
       data_items=['cc_perfect'],
         resolution=dmin)

      self.table_rows.append([
        "%5.2f" % dmin,
        "%7d" % nrefl,
        "%6.0f" % local_i_over_sigma,
        "%7.1f" % (100.*sigf),
        "%5.2f" % (cc_half),
        "%5.2f" % ( cc_ano),
        "%3.0f" % ( s_ano),
        "%3.0f" % (solved),
        "%3.2f" % (fom),
      ])
      if ((self.representative_values is None) or
          len(self.dmin_ranges) < 2 or dmin == self.dmin_ranges[-2]):
        self.representative_values = [dmin,nsites,nrefl,fpp,local_i_over_sigma,
           sigf,cc_half_weak,cc_half,cc_ano_weak,cc_ano,s_ano,solved,fom]


  def representative_dmin(self):
    return self.representative_values[0]

  def representative_nsites(self):
    return self.representative_values[1]

  def representative_nrefl(self):
    return self.representative_values[2]

  def representative_fpp(self):
    return self.representative_values[3]

  def representative_i_over_sigma(self):
    return self.representative_values[4]

  def representative_sigf(self):
    return self.representative_values[5]

  def representative_cc_half_weak(self):
    return self.representative_values[6]

  def representative_cc_half(self):
    return self.representative_values[7]

  def representative_cc_ano_weak(self):
    return self.representative_values[8]

  def representative_cc_ano(self):
    return self.representative_values[9]

  def representative_s_ano(self):
    return self.representative_values[10]

  def representative_solved(self):
    return self.representative_values[11]

  def representative_fom(self):
    return self.representative_values[12]

  def show_summary(self):
    if self.is_solvable():
      print("""
I/sigI: %7.1f
Dmin:      %5.2f
cc_half:   %5.2f - %5.2f
cc*_anom:  %5.2f - %5.2f
Signal:   %5.1f - %5.1f
p(Substr):%3d %%
FOM:      %3.2f
""" %(
 self.representative_i_over_sigma(),
 self.representative_dmin(),
 self.representative_cc_half_weak(),
 self.representative_cc_half(),
 self.representative_cc_ano_weak(),
 self.representative_cc_ano(),
 self.representative_s_ano()/2,
 self.representative_s_ano(),
 int(0.5+self.representative_solved()),
 self.representative_fom(),
 ))

  def is_solvable(self):
    return (self.representative_values is not None)

  def show_characteristics(self, out):
    from mmtbx.scaling import printed_output
    if out is None:
      out=sys.stdout
    if (not isinstance(out, printed_output)):
      out = printed_output(out)

    out.show_paragraph_header(
      "\nDataset characteristics:")

    out.show_preformatted_text("""\
  Target anomalous signal: %(target_s_ano)7.1f
  Residues: %(residues)d
  Chain-type: %(chain_type)s
  Solvent_fraction: %(solvent_fraction)7.2f
  Atoms: %(natoms)d
  Anomalously-scattering atom: %(atom_type)s
  Wavelength: %(wavelength)7.4f A
  Sites: %(nsites)d
  f-double-prime: %(fpp)7.2f
  Resolution: %(resolution)5.1f A
  B-value for anomalously-scattering atoms: %(b_value_anomalous)4.0f
""" % self.__dict__)

    if self.atom_type:
       t=self.atom_type
    else:
       t='-'
    contribution=self.fpp*math.sqrt(self.nsites)
    out.show_preformatted_text("""\
Target anomalous scatterer:
  Atom: %2s  f": %4.2f  n:%5.0f   rmsF:%7.1f  rmsF/rms(Total F) (%%):%5.1f""" %(
         t,self.fpp,self.nsites,contribution,100.*math.sqrt(self.fa2)))
    if self.noise_table_rows:
      out.show_preformatted_text("""\

Other anomalous scatterers in the structure:""")
      for row in self.noise_table_rows :
        out.show_preformatted_text(
    '  Atom: %2s  f": %4.2f  n:%5.0f   rmsF:%7.1f' %tuple(row))

      fa=100.*math.sqrt(self.fa2)
      fb=100.*math.sqrt(self.fb2)
      fab=math.sqrt(self.fa2/(self.fa2+self.fb2))
      out.show_preformatted_text("""\

Normalized anomalous scattering:
  From target anomalous atoms rms(x**2)/rms(F**2):  %7.2f
  From other anomalous atoms rms(e**2)/rms(F**2):   %7.2f
  Correlation of useful to total anomalous scattering: %4.2f
""" % (fa,fb,fab))

  def _show_impl(self, out):
    out.show_header("SAD experiment planning")
    out.show_sub_header(
      "Dataset overall I/sigma required to solve a structure")

    self.show_characteristics(out=out)

    out.show_preformatted_text("""
-------Targets for entire dataset-------  ----------Likely outcome-----------""")

    if (len(self.table_rows) == 0):
      out.show_text("SAD solution unlikely with the given parameters.")
      return
    if (not out.gui_output):
      out.show_preformatted_text("""
                              Anomalous    Useful    Useful
                            Half-dataset  Anom CC   Anomalous
 Dmin   N     I/sigI sigF/F     CC       (cc*_anom)  Signal   P(Substr)   FOM
                      (%)                                        (%)
""")
      for row in self.table_rows :
        out.show_preformatted_text(
        "%s%s%s%s     %s       %s      %s        %s       %s" %
          tuple(row))
    else :
      table = table_utils.simple_table(
        table_rows=self.table_rows,
        column_headers=["d_min", "N", "I/sigI", "sigF/F (%)",
          "Half-dataset CC_ano", "CC*_ano", "Anom. signal","P(Substr)","FOM"])
      out.show_table(table)
    (dmin,nsites,nrefl,fpp,i_over_sigma,sigf,cc_half_weak,cc_half,cc_ano_weak,
      cc_ano,s_ano,solved,fom) = tuple(self.representative_values)

    if self.missed_target_resolutions:
      self.missed_target_resolutions.sort()
      extra_note=""
      if self.used_max_i_over_sigma:
        extra_note="I/sigma shown is value of max_i_over_sigma."
      elif not self.input_i_over_sigma:
        extra_note="I/sigma shown achieves about %3.0f%% of \nmaximum anomalous signal." %(self.ratio_for_failure*100.)
      out.show_text("""
Note: Target anomalous signal not achievable with tested I/sigma (up to %d )
for resolutions of %5.2f A and lower. %s
""" % (int(self.max_i_over_sigma),self.missed_target_resolutions[0],extra_note))

    if self.skipped_resolutions:
      self.skipped_resolutions.sort()
      out.show_text("""
Note: No plausible values of I/sigma found for  resolutions of %5.2f A
and lower.
""" % (self.skipped_resolutions[0]))


    out.show_text("""
This table says that if you collect your data to a resolution of %5.1f A with
an overall <I>/<sigma> of about %3.0f then the half-dataset anomalous
correlation should be about %5.2f (typically within a factor of 2).  This
should lead to a correlation of your anomalous data to true anomalous
differences (CC*_ano) of about %5.2f, and a useful anomalous signal around
%3.0f (again within a factor of about two). With this value of estimated
anomalous signal the probability of finding the anomalous substructure is
about %3d%% (based on estimated anomalous signal and actual outcomes for
real structures), and the estimated figure of merit of phasing is %3.2f.""" % (dmin, i_over_sigma,  cc_half,  cc_ano,
        s_ano, int(solved), fom))
    out.show_text("""
The value of sigF/F (actually rms(sigF)/rms(F)) is approximately the inverse
of I/sigma. The calculations are based on rms(sigF)/rms(F).

Note that these values assume data measured with little radiation damage or at
least with anomalous pairs measured close in time. The values also assume that
the anomalously-scattering atoms are nearly as well-ordered as other atoms.
If your crystal does not fit these assumptions it may be necessary to collect
data with even higher I/sigma than indicated here.

Note also that anomalous signal is roughly proportional to the anomalous
structure factors at a given resolution. That means that if you have 50%
occupancy of your anomalous atoms, the signal will be 50% of what it otherwise
would be.  Also it means that if your anomalously scattering atoms only
contribute to 5 A, you should only consider data to 5 A in this analysis.
""")
    out.show_paragraph_header("""What to do next:""")
    out.show_text("""
1. Collect your data, trying to obtain a value of I/sigma for the whole dataset
   at least as high as your target.""")
    out.show_text("""\
2. Scale and analyze your unmerged data with phenix.scale_and_merge to get
   accurate scaled and merged data as well as two half-dataset data files
   that can be used to estimate the quality of your data.""")
    out.show_text("""\
3. Analyze your anomalous data (the scaled merged data and the two half-datdaset
   data files) with phenix.anomalous_signal to estimate the anomalous signal
   in your data. This tool will again guess the fraction of the substructure
   that can be obtained with your data, this time with knowledge of the
   actual anomalous signal.  It will also estimate the figure of merit of
   phasing that you can obtain once you solve the substruture. """)
    out.show_text("""\
4. Compare the anomalous signal in your measured data with the
   estimated values in the table above. If they are lower than expected
   you may need to collect more data to obtain the target anomalous signal.""")


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/pre_scale.py
from __future__ import absolute_import, division, print_function
from cctbx import miller
import mmtbx.scaling
from mmtbx.scaling import absolute_scaling
from mmtbx.scaling import matthews
from mmtbx.scaling import data_statistics


class pre_scaler(object):
  def __init__(self,
               miller_array,
               pre_scaling_protocol,
               basic_info,
               out=None):
    ## Make deep copy of the miller array of interest
    self.x1 = miller_array.deep_copy()
    self.options=pre_scaling_protocol
    self.basic_info= basic_info

    ## Determine unit_cell contents
    print(file=out)
    print("Matthews analyses", file=out)
    print("-----------------", file=out)
    print(file=out)
    print("Inspired by: Kantardjieff and Rupp. Prot. Sci. 12(9): 1865-1871 (2003).", file=out)
    matthews_analyses = matthews.matthews_rupp(
      crystal_symmetry = self.x1,
      n_residues = self.basic_info.n_residues,
      n_bases = self.basic_info.n_bases,
      out=out, verbose=1)
    n_residues=matthews_analyses[0]
    n_bases=matthews_analyses[1]
    n_copies_solc=matthews_analyses[2]

    if (self.basic_info.n_residues==None):
      self.basic_info.n_residues = n_residues
    if (self.basic_info.n_bases == None):
      self.basic_info.n_bases = n_bases


    ## apply resolution cut
    print(file=out)
    print("Applying resolution cut", file=out)
    print("-----------------------", file=out)

    if self.options.low_resolution is None:
      if self.options.high_resolution is None:
        print("No resolution cut is made", file=out)

    low_cut=float(1e6)
    if self.options.low_resolution is not None:
      low_cut = self.options.low_resolution
      print("Specified low resolution limit: %3.2f"%(
       float(self.options.low_resolution) ), file=out)

    high_cut = 0
    if self.options.high_resolution is not None:
      high_cut = self.options.high_resolution
      print("Specified high resolution limit: %3.2f"%(
       float(self.options.high_resolution) ), file=out)

    ## perform outlier analyses
    ##
    ## Do a simple outlier analyses please
    print(file=out)
    print("Wilson statistics based outlier analyses", file=out)
    print("----------------------------------------", file=out)
    print(file=out)
    native_outlier = data_statistics.possible_outliers(
      miller_array = self.x1,
      prob_cut_ex = self.options.outlier_level_extreme,
      prob_cut_wil = self.options.outlier_level_wilson )
    native_outlier.show(out=out)

    self.x1 = native_outlier.remove_outliers(
      self.x1 )

    ## apply anisotropic scaling  (final B-value will be set to b_add)!
    if self.options.aniso_correction:

      b_final = self.options.b_add
      if b_final is None:
        b_final = 0.0

      print(file=out)
      print("Anisotropic absolute scaling of data", file=out)
      print("--------------------------------------", file=out)
      print(file=out)

      aniso_correct = absolute_scaling.ml_aniso_absolute_scaling(
        miller_array = self.x1,
        n_residues = n_residues*\
        self.x1.space_group().order_z()*n_copies_solc,
        n_bases = n_bases*\
        self.x1.space_group().order_z()*n_copies_solc)
      aniso_correct.show(out=out,verbose=1)
      print(file=out)
      print("  removing anisotropy for native  ", file=out)
      print(file=out)
      u_star_correct_nat = aniso_correct.u_star
      self.x1 = absolute_scaling.anisotropic_correction(
        self.x1,
        aniso_correct.p_scale,
        u_star_correct_nat  )


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/random_omit.py
"""
the parameters should have this scope
  omit {
    perform_omit = True
    fraction = 0.15
    max_number = 1e5
    number_of_sets = 100
    root_name = 'omit_'
  }
"""
from __future__ import absolute_import, division, print_function
from six.moves import range

class random_omit_data(object):
  def __init__(self,
               miller_array,
               parameters):
    self.miller_array = miller_array
    self.parameters = parameters

  def write_datasets(self):

    comfile = open(self.parameters.root_name+"exec.com", "w")

    for nth in range(self.parameters.number_of_sets):
      file_name = self.parameters.root_name + str(nth)+".mtz"
      print("__REPLACE_1__%s__REPLACE_2__ > %s.log"%(file_name,nth), file=comfile)
      tmp_select = self.miller_array.generate_r_free_flags(
        fraction = self.parameters.fraction ,
        max_free =  self.parameters.max_number,
        use_lattice_symmetry = False)
      tmp_miller = self.miller_array.select( ~tmp_select.data() )
      tmp_mtz_dataset = tmp_miller.as_mtz_dataset(
        column_root_label="Fdelta")
      tmp_mtz_dataset.mtz_object().write(
        file_name=file_name)


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/relative_scaling.py
from __future__ import absolute_import, division, print_function
from cctbx.array_family import flex
from mmtbx import scaling
from mmtbx.scaling import absolute_scaling
from cctbx import adptbx
from cctbx import crystal
from cctbx import miller
from libtbx.utils import Sorry
from scitbx.minimizers import newton_more_thuente_1994
from scitbx import matrix
import math
import sys
from six.moves import range

# 2009-04-15, cctbx svn rev. 8940:
#   there are no unit tests for this module, but is is used by these commands:
#     phenix.model_vs_data, phenix.refine, phenix.real_space_correlation,
#     phenix.twin_map_utils, phenix.xmanip

class refinery:

  def __init__(self,
               miller_native,
               miller_derivative,
               use_intensities=True,
               scale_weight=False,
               use_weights=False,
               mask=[1,1],
               start_values=None ):


    ## This mask allows one to refine only scale factor and only B values
    self.mask = mask ## multiplier for gradients of [scale factor, u tensor]

    ## make deep copies just to avoid any possible problems
    self.native = miller_native.deep_copy().set_observation_type(
      miller_native)

    if not self.native.is_real_array():
      raise Sorry("A real array is need for ls scaling")
    self.derivative = miller_derivative.deep_copy().set_observation_type(
      miller_derivative)
    if not self.derivative.is_real_array():
      raise Sorry("A real array is need for ls scaling")


    if use_intensities:
      if not self.native.is_xray_intensity_array():
        self.native = self.native.f_as_f_sq()
      if not self.derivative.is_xray_intensity_array():
        self.derivative = self.derivative.f_as_f_sq()
    if not use_intensities:
      if self.native.is_xray_intensity_array():
        self.native = self.native.f_sq_as_f()
      if self.derivative.is_xray_intensity_array():
        self.derivative = self.derivative.f_sq_as_f()

    ## Get the common sets
    self.native, self.derivative = self.native.map_to_asu().common_sets(
       self.derivative.map_to_asu() )

    ## Get the required information
    self.hkl = self.native.indices()

    self.i_or_f_nat =  self.native.data()
    self.sig_nat = self.native.sigmas()
    if self.sig_nat is None:
      self.sig_nat = self.i_or_f_nat*0 + 1

    self.i_or_f_der = self.derivative.data()
    self.sig_der = self.derivative.sigmas()
    if self.sig_der is None:
      self.sig_der = self.i_or_f_der*0+1

    self.unit_cell = self.native.unit_cell()

    # Modifiy the weights if required
    if not use_weights:
      self.sig_nat = self.sig_nat*0.0 + 1.0
      self.sig_der = self.sig_der*0.0


    ## Set up the minimiser 'cache'
    self.minimizer_object = None
    if use_intensities:
      if scale_weight:
        self.minimizer_object = scaling.least_squares_on_i_wt(
          self.hkl,
          self.i_or_f_nat,
          self.sig_nat,
          self.i_or_f_der,
          self.sig_der,
          0,
          self.unit_cell,
          [0,0,0,0,0,0])
      else :
        self.minimizer_object = scaling.least_squares_on_i(
          self.hkl,
          self.i_or_f_nat,
          self.sig_nat,
          self.i_or_f_der,
          self.sig_der,
          0,
          self.unit_cell,
          [0,0,0,0,0,0])
    else:
      if scale_weight:
        self.minimizer_object = scaling.least_squares_on_f_wt(
          self.hkl,
          self.i_or_f_nat,
          self.sig_nat,
          self.i_or_f_der,
          self.sig_der,
          0,
          self.unit_cell,
          [0,0,0,0,0,0])
      else :
        self.minimizer_object = scaling.least_squares_on_f(
          self.hkl,
          self.i_or_f_nat,
          self.sig_nat,
          self.i_or_f_der,
          self.sig_der,
          0,
          self.unit_cell,
          [0,0,0,0,0,0])

    ## Symmetry related issues
    self.sg = self.native.space_group()
    self.adp_constraints = self.sg.adp_constraints()
    self.dim_u = self.adp_constraints.n_independent_params
    ## Setup number of parameters
    assert self.dim_u()<=6
    ## Optimisation stuff
    x0 = flex.double(self.dim_u()+1, 0.0) ## B-values and scale factor!
    if start_values is not None:
      assert( start_values.size()==self.x.size() )
      x0 = start_values

    minimized = newton_more_thuente_1994(
      function=self, x0=x0, gtol=0.9e-6, eps_1=1.e-6, eps_2=1.e-6,
      matrix_symmetric_relative_epsilon=1.e-6)


    Vrwgk = math.pow(self.unit_cell.volume(),2.0/3.0)
    self.p_scale = minimized.x_star[0]
    self.u_star = self.unpack( minimized.x_star )
    self.u_star = list( flex.double(self.u_star) / Vrwgk )
    self.b_cart = adptbx.u_as_b(adptbx.u_star_as_u_cart(self.unit_cell,
                                                        self.u_star))
    self.u_cif = adptbx.u_star_as_u_cif(self.unit_cell,
                                        self.u_star)


  def pack(self,grad_tensor):
    grad_independent = [ grad_tensor[0]*float(self.mask[0]) ]+\
      list( float(self.mask[1])*
            flex.double(self.adp_constraints.independent_gradients(
              list(grad_tensor[1:])))
            )
    return flex.double(grad_independent)

  def unpack(self,x):
    u_tensor = self.adp_constraints.all_params( list(x[1:]) )
    return u_tensor

  def functional(self, x):
    ## unpack the u-tensor
    u_full = self.unpack(x)
    ## place the params in the whatever
    self.minimizer_object.set_params(
      x[0],
      u_full)
    return self.minimizer_object.get_function()

  def gradients(self, x):
    u_full = self.unpack(x)
    self.minimizer_object.set_params(
      x[0],
      u_full)
    g_full = self.minimizer_object.get_gradient()
    g = self.pack( g_full )
    return g

  def hessian(self, x, eps=1.e-6):

    u_full = self.unpack(x)
    self.minimizer_object.set_params(
      x[0],
      u_full)
    result = self.minimizer_object.hessian_as_packed_u()
    result = result.matrix_packed_u_as_symmetric()
    result = self.hessian_transform(result,self.adp_constraints )
    return(result)

  ## This function is *only* for hessian with scale + utensor components
  def hessian_transform(self,
                        original_hessian,
                        adp_constraints):
    constraint_matrix_tensor = matrix.rec(
      adp_constraints.gradient_sum_matrix(),
      adp_constraints.gradient_sum_matrix().focus())

    hessian_matrix = matrix.rec( original_hessian,
                                 original_hessian.focus())
      ## now create an expanded matrix
    rows=adp_constraints.gradient_sum_matrix().focus()[0]+1
    columns=adp_constraints.gradient_sum_matrix().focus()[1]+1
    expanded_constraint_array = flex.double(rows*columns,0)
    count_new=0
    count_old=0
    for ii in range(rows):
      for jj in range(columns):
        if (ii>0):
          if (jj>0):
            expanded_constraint_array[count_new]=\
               constraint_matrix_tensor[count_old]
            count_old+=1
        count_new+=1
      ## place the first element please
    expanded_constraint_array[0]=1
    result=matrix.rec(  expanded_constraint_array,
                        (rows, columns) )
    #print result.mathematica_form()
    new_hessian = result *  hessian_matrix * result.transpose()
    result = flex.double(new_hessian)
    result.resize(flex.grid( new_hessian.n ) )
    return(result)



class ls_rel_scale_driver(object):
  def __init__(self,
               miller_native,
               miller_derivative,
               use_intensities=True,
               scale_weight=True,
               use_weights=True):
    self.native = miller_native.deep_copy().map_to_asu()
    self.derivative = miller_derivative.deep_copy().map_to_asu()

    lsq_object = refinery(self.native,
                          self.derivative,
                          use_intensities=use_intensities,
                          scale_weight=scale_weight,
                          use_weights=use_weights)


    self.p_scale = lsq_object.p_scale
    self.b_cart = lsq_object.b_cart
    self.u_star = lsq_object.u_star

    ## very well, all done and set.
    ## apply the scaling on the data please and compute some r values
    tmp_nat, tmp_der = self.native.common_sets(self.derivative)

    self.r_val_before = flex.sum( flex.abs(tmp_nat.data()-tmp_der.data()) )
    if flex.sum( flex.abs(tmp_nat.data()+tmp_der.data()) ) > 0:
      self.r_val_before /=flex.sum( flex.abs(tmp_nat.data()+tmp_der.data()) )/2.0

    self.derivative = absolute_scaling.anisotropic_correction(
      self.derivative,self.p_scale,self.u_star )

    self.scaled_original_derivative = self.derivative.deep_copy().set_observation_type(
      self.derivative ).map_to_asu()

    tmp_nat = self.native
    tmp_der = self.derivative

    tmp_nat, tmp_der = self.native.map_to_asu().common_sets(self.derivative.map_to_asu())
    self.r_val_after = flex.sum( flex.abs( tmp_nat.data()-
                                           tmp_der.data()   )
                               )
    if (flex.sum( flex.abs(tmp_nat.data()) ) +
                        flex.sum( flex.abs(tmp_der.data()) )) > 0:
      self.r_val_after /=(flex.sum( flex.abs(tmp_nat.data()) ) +
                          flex.sum( flex.abs(tmp_der.data()) ))/2.0

    self.native=tmp_nat
    self.derivative=tmp_der

    ## All done

  def show(self, out=None):
    if out is None:
      out=sys.stdout

    print(file=out)
    print("p_scale                    : %5.3f"%(self.p_scale), file=out)
    print("                            (%5.3f)"%(math.exp( self.p_scale ) ), file=out)
    print("B_cart trace               : %5.3f, %5.3f, %5.3f"%(
      self.b_cart[0],
      self.b_cart[1],
      self.b_cart[2]), file=out)
    print(file=out)
    print("R-value before LS scaling  : %5.3f"%(self.r_val_before), file=out)
    print("R-value after LS scaling   : %5.3f"%(self.r_val_after), file=out)
    print(file=out)




class local_scaling_driver(object):
  def __init__(self,
               miller_native,
               miller_derivative,
               local_scaling_dict,
               use_intensities=True,
               use_weights=False,
               max_depth=10,
               target_neighbours=1000,
               sphere=1,
               threshold=1.0,
               out=None):

    if out == None:
      out = sys.stdout


    self.native = miller_native.deep_copy().set_observation_type(
      miller_native).map_to_asu()
    self.derivative = miller_derivative.deep_copy().set_observation_type(
      miller_derivative).map_to_asu()

    assert self.native.observation_type() != None
    assert self.derivative.observation_type() != None

    self.native, self.derivative = self.native.common_sets(
      self.derivative)

    ## Here we change things into intensities or amplitudes as asked

    if use_intensities:
      if not self.native.is_xray_intensity_array():
        self.native = self.native.f_as_f_sq()
      if not self.derivative.is_xray_intensity_array():
        self.derivative = self.derivative.f_as_f_sq()

    if not use_intensities:
      if self.native.is_xray_intensity_array():
        self.native = self.native.f_sq_as_f()
      if self.derivative.is_xray_intensity_array():
        self.derivative = self.derivative.f_sq_as_f()


    ## In order to avoid problems with abseces due to lattice
    ## types, we transform the system to the primitive setting

    ## make new symmetry object
    self.nat_primset=self.native.change_basis(
     self.native.change_of_basis_op_to_niggli_cell()
     ).set_observation_type( self.native ).map_to_asu()

    self.der_primset=self.derivative.change_basis(
      self.derivative.change_of_basis_op_to_niggli_cell()
      ).set_observation_type(  self.derivative ).map_to_asu()

    ## Get the symmetry of the intensity group
    ## this is to make suree systematic absense are present in the hkl list
    intensity_group = self.nat_primset.space_group() \
      .build_derived_reflection_intensity_group(
      anomalous_flag=self.nat_primset.anomalous_flag() )

    ## We need to define a master set
    ## This is a miller array that encompasses
    ## both native and derivative *and* has systematic absences present.
    tmp_reso_lim = self.nat_primset.d_min()-0.1
    ## the 0.1 limit is a bit of a hack, but is needed
    ## to ensure that the master set is equal or slightkly larger than
    ## the working sets
    assert( tmp_reso_lim > 0 )

    self.master_set = miller.build_set(
      crystal_symmetry=crystal.symmetry(
      unit_cell=self.nat_primset.unit_cell(),
      space_group=intensity_group),
      anomalous_flag=self.nat_primset.anomalous_flag(),
      d_min=tmp_reso_lim)

    self.local_scaler=None
    self.sphere=sphere
    self.max_depth=max_depth
    self.target_neighbours=target_neighbours
    self.use_weights=use_weights
    self.threshold=threshold

    # Moment based scaling
    if local_scaling_dict['local_moment']:
      self.local_moment_scaling(out)

    # then it will be lsq based local scaling
    if local_scaling_dict['local_lsq']:
      self.local_lsq_scaling(out)

    # or nikonov scaling
    if local_scaling_dict['local_nikonov']:
      self.local_nikonov_scaling(out)


    scales=self.local_scaler.get_scales()
    stats=self.local_scaler.stats()

    print("Mean number of neighbours           : %8.3f"%(stats[2]), file=out)
    print("Minimum number of neighbours        : %8i"%(stats[0]), file=out)
    print("Maximum number of neighbours        : %8i"%(stats[1]), file=out)
    print(file=out)
    print("Mean local scale                    : %8.3f"%(
      flex.mean(scales) ), file=out)
    print("Standard deviation of local scale   : %8.3f"%(
      math.sqrt(   flex.mean(scales*scales)
                 - flex.mean(scales)*flex.mean(scales))), file=out)
    print("Minimum local scale                 : %8.3f"%(
      flex.min( scales ) ), file=out)
    print("Maximum local scale                 : %8.3f"%(
      flex.max( scales ) ), file=out)

    self.der_primset = self.der_primset.customized_copy(
       data = self.der_primset.data()*scales,
       sigmas = self.der_primset.sigmas()*scales
      ).set_observation_type( self.der_primset )

    ## We now have to transform the thing back please

    self.derivative = self.der_primset.change_basis(
        self.native.change_of_basis_op_to_niggli_cell().inverse()
      ).set_observation_type( self.der_primset ).map_to_asu()

    del self.der_primset
    del self.nat_primset

  def r_value(self,out):
    top = flex.abs(self.der_primset.data()-
                   self.nat_primset.data())
    bottom = flex.abs(self.der_primset.data() +
                      self.nat_primset.data())/2.0
    top=flex.sum(top)
    bottom=flex.sum(bottom)
    print("Current R value: %4.3f"%(top/bottom), file=out)


  def local_moment_scaling(self,out):
    print(file=out)
    print("Moment based local scaling", file=out)
    print("Maximum depth        : %8i"%(self.max_depth), file=out)
    print("Target neighbours    : %8i"%(self.target_neighbours), file=out)
    print("neighbourhood sphere : %8i"%(self.sphere), file=out)
    print(file=out)
    self.local_scaler = scaling.local_scaling_moment_based(
      hkl_master=self.master_set.indices(),
      hkl_sets=self.nat_primset.indices(),
      data_set_a=self.nat_primset.data(),
      sigma_set_a=self.nat_primset.sigmas(),
      data_set_b=self.der_primset.data(),
      sigma_set_b=self.der_primset.sigmas(),
      space_group=self.nat_primset.space_group(),
      anomalous_flag=self.nat_primset.anomalous_flag(),
      radius=self.sphere,
      depth=self.max_depth,
      target_ref=self.target_neighbours,
      use_experimental_sigmas=self.use_weights)


  def local_lsq_scaling(self,out):
    print(file=out)
    print("Least squares based local scaling", file=out)
    print("Maximum depth        : %8i"%(self.max_depth), file=out)
    print("Target neighbours    : %8i"%(self.target_neighbours), file=out)
    print("neighbourhood sphere : %8i"%(self.sphere), file=out)
    print(file=out)
    self.local_scaler = scaling.local_scaling_ls_based(
      hkl_master=self.master_set.indices(),
      hkl_sets=self.nat_primset.indices(),
      data_set_a=self.nat_primset.data(),
      sigma_set_a=self.nat_primset.sigmas(),
      data_set_b=self.der_primset.data(),
      sigma_set_b=self.der_primset.sigmas(),
      space_group=self.nat_primset.space_group(),
      anomalous_flag=self.nat_primset.anomalous_flag(),
      radius=self.sphere,
      depth=self.max_depth,
      target_ref=self.target_neighbours,
      use_experimental_sigmas=self.use_weights)

  def local_nikonov_scaling(self,out):
    print(file=out)
    print("Nikonev based local scaling", file=out)
    print("Maximum depth        : %8i"%(self.max_depth), file=out)
    print("Target neighbours    : %8i"%(self.target_neighbours), file=out)
    print("neighbourhood sphere : %8i"%(self.sphere), file=out)
    print(file=out)

    if self.der_primset.is_xray_intensity_array():
      raise Sorry(" For Nikonev target in local scaling, amplitudes must be used")
      assert (False)

    self.local_scaler = scaling.local_scaling_nikonov(
      hkl_master=self.master_set.indices(),
      hkl_sets=self.nat_primset.indices(),
      data_set_a=self.nat_primset.data(),
      data_set_b=self.der_primset.data(),
      epsilons=self.der_primset.epsilons().data().as_double(),
      centric=flex.bool(self.der_primset.centric_flags().data()),
      threshold=self.threshold,
      space_group=self.nat_primset.space_group(),
      anomalous_flag=self.nat_primset.anomalous_flag(),
      radius=self.sphere,
      depth=self.max_depth,
      target_ref=self.target_neighbours)


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/relative_wilson.py
from __future__ import absolute_import, division, print_function
from mmtbx.scaling import absolute_scaling
import mmtbx.scaling
from iotbx import data_plots
from scitbx.array_family import flex
from scitbx.math import scale_curves
from scitbx import simplex
from scitbx.math import chebyshev_polynome
from libtbx.utils import Sorry
from libtbx import table_utils
import sys,math
from six.moves import zip
from six.moves import range

low_lim = 0.00142857142857
high_lim = 0.914857142857

mean_coefs = flex.double([-0.43300151026105321, 0.33427752644857256, -0.36429668743046412, 0.32015342663362861, -0.33187593112421665, 0.25808435074324798, -0.21097660027094089, 0.19245214093632027, -0.16848242519036311, 0.15291501268526811, -0.12910039774102444, 0.099417816273142834, -0.088720990827720211, 0.095701367393211653, -0.10629070210548312, 0.10981060473882812, -0.098166077509690655, 0.081269422162739482, -0.070505309537945482, 0.062534977493702376, -0.052309784988656668, 0.043125649942558686, -0.033948257000056978, 0.025057256942873911, -0.017846896900315476, 0.013015472828176888, -0.0090867510653793015, 0.005271961028977065, -0.00066906935937178775, -0.001734805197078687, 0.0019473391336692384, -0.00077699241439945844, -0.00012166448304738755, 0.0022859384260305684, -0.0041703347977807507, 0.0048616662471392688, -0.0049936320989681648, 0.0062946025455139906, -0.0054911210539810868, 0.0040222680071224075, -0.0033087237096459769, 0.0042634379859494151, -0.0037060560347156168, 0.0026770515762505058, -0.0020954947717182685, 0.0035512064084911323, -0.0028854530832642875, 0.002100343979509825, -0.0014536705634179688, 0.0024292695349115174])

std_coefs = flex.double([0.22609807236783072, -0.051083004382385722, 0.10050223345603099, -0.059797469000968342, 0.078452075293119358, -0.061376061912225756, 0.046001019180730129, -0.046818252753277688, 0.037535878728343949, -0.031883497361025873, 0.031132854775465228, -0.026248228833806508, 0.025229855893282804, -0.022987539515026915, 0.018603638709078982, -0.020688685663116515, 0.021490882895477355, -0.019155463126466928, 0.018694555361791723, -0.017919220545523508, 0.01688432732243788, -0.016177982330096936, 0.013523772618558827, -0.011497460798395623, 0.010090183879313928, -0.0077311745570573494, 0.0069765868372828593, -0.0085904927919333608, 0.0079389398144072369, -0.0063106616713442193, 0.0072030470015979342, -0.0082688707324504833, 0.0075456582719407002, -0.0078122483377159966, 0.007131121698384397, -0.004898714984268495, 0.0045473543279292298, -0.0055478491205527948, 0.0041818036356804219, -0.0032442174724577502, 0.0035282617908206485, -0.0026738719276938735, 0.0012942832126333331, -0.001864418991069073, 0.001979588643470585, -0.001413729012970848, 0.00074827896319899767, -0.00089235624086152622, 0.00061639083311362331, -0.0007922443411235876])

class relative_wilson(mmtbx.scaling.xtriage_analysis):
  def __init__(self,
      miller_obs,
      miller_calc,
      min_d_star_sq=0.0,
      max_d_star_sq=2.0,
      n_points=2000,
      level=6.0):
    assert miller_obs.indices().all_eq(miller_calc.indices())
    if (miller_obs.is_xray_amplitude_array()):
      miller_obs = miller_obs.f_as_f_sq()
    if (miller_calc.is_xray_amplitude_array()):
      miller_calc = miller_calc.f_as_f_sq()
    self.obs  = miller_obs.deep_copy()
    self.calc = miller_calc.deep_copy()
    self.mind = min_d_star_sq
    self.maxd = max_d_star_sq
    self.m    = n_points
    self.n    = 2
    self.level = level

    norma_obs  = absolute_scaling.kernel_normalisation(
      miller_array=self.obs,
      auto_kernel=True,
      n_bins=45,
      n_term=17)
    norma_calc = absolute_scaling.kernel_normalisation(
      miller_array=self.calc,
      auto_kernel=True,
      n_bins=45,
      n_term=17)

    obs_d_star_sq  = norma_obs.d_star_sq_array
    calc_d_star_sq = norma_calc.d_star_sq_array
    sel_calc_obs = norma_calc.bin_selection.select(norma_obs.bin_selection)
    sel_obs_calc = norma_obs.bin_selection.select(norma_calc.bin_selection)
    sel  = ((obs_d_star_sq > low_lim) & (obs_d_star_sq < high_lim) &
            (norma_obs.mean_I_array > 0))
    sel = sel.select(sel_calc_obs)

    self.obs_d_star_sq  = obs_d_star_sq.select( sel )
    self.calc_d_star_sq = calc_d_star_sq.select( sel_obs_calc ).select(sel)
    self.mean_obs       = norma_obs.mean_I_array.select(sel)
    self.mean_calc      = norma_calc.mean_I_array.select(
                            sel_obs_calc).select(sel)
    self.var_obs        = norma_obs.var_I_array.select(sel)
    self.var_calc       = norma_calc.var_I_array.select(
      sel_obs_calc).select(sel)

    # make an interpolator object please
    self.interpol = scale_curves.curve_interpolator( self.mind, self.maxd,
      self.m)
    # do the interpolation
    tmp_obs_d_star_sq  , self.mean_obs,self.obs_a  , self.obs_b  = \
      self.interpol.interpolate(self.obs_d_star_sq,self.mean_obs)
    self.obs_d_star_sq , self.var_obs,self.obs_a   , self.obs_b  = \
      self.interpol.interpolate(self.obs_d_star_sq, self.var_obs)
    tmp_calc_d_star_sq , self.mean_calc,self.calc_a, self.calc_b = \
      self.interpol.interpolate(self.calc_d_star_sq,self.mean_calc)
    self.calc_d_star_sq, self.var_calc,self.calc_a , self.calc_b = \
      self.interpol.interpolate(self.calc_d_star_sq,self.var_calc)

    self.mean_ratio_engine = chebyshev_polynome( mean_coefs.size(),
      low_lim-1e-3, high_lim+1e-3,mean_coefs)
    self.std_ratio_engine = chebyshev_polynome( std_coefs.size(),
      low_lim-1e-3, high_lim+1e-3,std_coefs)

    self.x = flex.double([0,0])

    self.low_lim_for_scaling = 1.0/(4.0*4.0) #0.0625
    selection = (self.calc_d_star_sq > self.low_lim_for_scaling)
    if (selection.count(True) == 0):
      raise Sorry("No reflections within required resolution range after "+
        "filtering.")
    self.weight_array = selection.as_double() / (2.0 * self.var_obs)
    assert (not self.weight_array.all_eq(0.0))

    self.mean   = flex.double( [1.0/(flex.sum(self.mean_calc) /
                                flex.sum(self.mean_obs)), 0.0 ] )
    self.sigmas = flex.double( [0.5, 0.5] )

    s = 1.0/(flex.sum(self.weight_array*self.mean_calc)/
             flex.sum(self.weight_array*self.mean_obs))
    b = 0.0
    self.sart_simplex = [ flex.double([s,b]), flex.double([s+0.1,b+1.1]),
                          flex.double([s-0.1,b-1.1]) ]
    self.opti = simplex.simplex_opt( 2, self.sart_simplex, self)

    sol = self.opti.get_solution()
    self.scale   = abs(sol[0])
    self.b_value = sol[1]

    self.modify_weights()
    self.all_bad_z_scores = self.weight_array.all_eq(0.0)
    if (not self.all_bad_z_scores):
      s = 1.0/(flex.sum(self.weight_array*self.mean_calc) /
               flex.sum(self.weight_array*self.mean_obs))
      b = 0.0
      self.sart_simplex = [ flex.double([s,b]), flex.double([s+0.1,b+1.1]),
                            flex.double([s-0.1,b-1.1]) ]
      self.opti = simplex.simplex_opt( 2, self.sart_simplex, self)
    #self.mean_calc = self.mean_calc*self.scale*flex.exp(self.calc_d_star_sq*self.b_value)

  def summary(self):
    i_scaled = flex.exp( self.calc_d_star_sq*self.b_value ) * \
                self.mean_calc * self.scale
    sel = (self.mean_obs > 0).iselection()
    ratio  = flex.log(i_scaled.select(sel) / self.mean_obs.select(sel))
    ratio_ = flex.double(self.mean_obs.size(), 0)
    ratio_.set_selected(sel, ratio)
    curves = [
      self.calc_d_star_sq,
      -ratio_, # observed
      self.curve( self.calc_d_star_sq ), # expected
      self.get_z_scores(self.scale, self.b_value)
    ]
    return summary(
      all_curves=curves,
      level=self.level,
      all_bad_z_scores=self.all_bad_z_scores)

  def modify_weights(self,level=5):
    z_scores = self.get_z_scores(self.scale, self.b_value)
    sel  = flex.double(list(flex.bool(z_scores<level)))
    self.weight_array = self.weight_array*sel

  def get_z_scores(self, scale, b_value):
    i_scaled = flex.exp( self.calc_d_star_sq*b_value )*self.mean_calc*scale
    sel = ((self.mean_obs > 0) & (i_scaled > 0)) .iselection()
    ratio  = self.mean_obs.select(sel) / i_scaled.select(sel)
    mean = self.curve( self.calc_d_star_sq ).select(sel)
    assert ratio.all_gt(0) # FIXME need to filter first!
    ratio = flex.log(ratio)
    var = self.std(self.calc_d_star_sq).select(sel)
    d_star_sq = self.calc_d_star_sq.select(sel)
    assert var.all_ne(0)
    z = flex.abs(ratio-mean)/var
    z_ = flex.double(self.mean_obs.size(), -1)
    z_.set_selected(sel, z)
    return z_

  def target(self,vector):
    v=1.0
    scale = abs(vector[0])
    b_value = vector[1]
    if b_value > 200.0:
      b_value = 200.0
    if b_value < -200.0:
      b_value = -200.0
    i_scaled = flex.exp( self.calc_d_star_sq*b_value )*self.mean_calc*scale
    ratio  = i_scaled / self.mean_obs
    curve = self.curve( self.calc_d_star_sq )
    result = ratio - flex.exp(curve)
    if (flex.max(result) > math.sqrt(sys.float_info.max)):
      raise OverflowError("Result array exceeds floating-point limit.")
    result = result*result
    wmax = flex.max(self.weight_array)
    assert (wmax != 0)
    if (wmax > 1) and (flex.max(result) > sys.float_info.max / wmax):
      raise OverflowError("Weighted result array will exceed floating-point "+
        "limit: %e" % flex.max(result))
    result = result*self.weight_array
    result = flex.sum( result )
    return result

  def curve(self,d_star_sq):
    result =  self.mean_ratio_engine.f( d_star_sq )
    return result

  def std(self,d_star_sq):
    result = self.std_ratio_engine.f( d_star_sq )
    return result

  def show_summary(self, out):
    return self.summary().show(out=out)

class summary(mmtbx.scaling.xtriage_analysis):
  def __init__(self,
      all_curves,
      level=6.0,
      all_bad_z_scores=False):
    self.table = data_plots.table_data(
      title="Relative Wilson plot",
      column_labels=["Max. resolution", "log(I_exp/I_obs)", "Reference curve",
        "Z-score"],
      graph_names=["Relative Wilson plot"],
      graph_labels=[("High resolution", "")],
      graph_columns=[list(range(4))],
      x_is_inverse_d_min=True,
      data=[ list(array) for array in all_curves ])
    self.cutoff = level
    self.all_bad_z_scores = all_bad_z_scores

  def n_outliers(self):
    ss,rr,ii,zz = self.data_as_flex_arrays()
    flagged = zz > self.cutoff
    return flagged.count(True)

  def data_as_flex_arrays(self):
    return [ flex.double(column) for column in self.table.data ]

  def _show_impl(self, out):
    ss,rr,ii,zz = self.data_as_flex_arrays()
    flagged = zz > self.cutoff
    sel_ss = ss.select(flagged)
    sel_z = zz.select(flagged)
    sel_r = rr.select(flagged)
    sel_i = ii.select(flagged)
    out.show_sub_header("Relative Wilson plot")
    out.show_text("""\
The relative Wilson plot compares the mean intensity of the observed data with
the mean intensity computed from the model, as a function of resolution.  This
curve is expected to fall off at low resolution if no contribution for bulk
solvent is provided for the calculated intensities, because the presence of
bulk solvent reduces the observed intensities at low resolution by reducing
the contrast.  At high resolution, the curve should be a straight line with a
slope that reflects the difference in overall B-factor between the model and
the data.  Compared to the normal Wilson plot, the relative Wilson plot is
more linear because the influence of favored distances between atoms, caused
by bonding and secondary structure, is cancelled out.
""")
    out.show_plot(self.table)
    if (self.all_bad_z_scores):
      out.warn("""\
All resolution shells have Z-scores above %4.2f sigma.  This is indicative of
severe problems with the input data, including processing errors or ice rings.
We recommend checking the logs for data processing and inspecting the raw
images.\n""" % self.cutoff)
    else :
      out.show_text("""\
All relative wilson plot outliers above %4.2f sigma are reported.
""" % self.cutoff)
    out.newline()
    rows = []
    if len(sel_ss) > 0:
      for s,z,r,i in zip(sel_ss,sel_z,sel_r,sel_i):
        sss = math.sqrt(1.0/s)
        rows.append([ "%8.2f" % sss, "%9.3e" % r, "%9.3e" % i, "%5.2f" % z ])
      table = table_utils.simple_table(
        column_headers=["d-spacing", "Obs. Log[ratio]", "Expected Log[ratio]",
          "Z-score"],
        table_rows=rows)
      out.show_table(table)
    else:
      out.show("The Relative wilson plot doesn't indicate any serious errors.")


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/remove_outliers.py
from __future__ import absolute_import, division, print_function
from cctbx import crystal
from cctbx import sgtbx
from cctbx import xray
from cctbx.array_family import flex
from libtbx.utils import Sorry, multi_out
import iotbx.phil
from iotbx import reflection_file_utils
from iotbx import crystal_symmetry_from_any
from iotbx import pdb
import mmtbx.scaling
from mmtbx.scaling import outlier_rejection
from mmtbx import f_model
from libtbx.str_utils import StringIO
import sys, os



master_params = iotbx.phil.parse("""\
outlier_utils{
  input{
    unit_cell=None
    .type=unit_cell
    space_group=None
    .type=space_group
    xray_data{
      file_name=None
      .type=path
      obs_labels=None
      .type=str
      free_flags=None
      .type=str
    }
    model{
      file_name=None
      .type=path
    }
  }
  outlier_detection{
    protocol=basic *extreme beamstop model
    .type=choice
    parameters{
      basic_wilson{
        level=1E-6
        .type=float
      }
      extreme_wilson{
        level=0.01
        .type=float
      }
      beamstop{
        level=0.001
        .type=float
        d_min=10.0
        .type=float
      }
      model_based{
        level=0.01
        .type=float
      }
    }
  }
  additional_parameters{
    free_flag_generation{
      fraction = 0.10
      .type=float
      max_number = 2000
      .type=float
      lattice_symmetry_max_delta=5.0
      .type=float
      use_lattice_symmetry=true
      .type=bool
    }
  }

  output{
    logfile=outlier_tools.log
    .type=str
    hklout=None
    .type=path
  }
}
""")

def print_help(command_name):
  print("""
usage: %(command_name)s <options>

Options are defined by the following phil scope:

outlier_utils{
  input{
    unit_cell=None                   << Unit cell. Needed for CNS and SHELX formatted Fobs/Iobs files
    space_group=None                 << space group.  Needed for CNS and SHELX formatted Fobs/Iobs files
    xray_data{
      file_name=None                 << reflection file in any format.
      obs_labels=None                << The labels of the the observed labels (CNS and MTZ)
      free_flags=None                << The labels of the Free Flags (CNS and MTZ)
    }
    model{
      file_name=None                 << A PDB file of the model corresponding to the provided xray data
    }
  }
  outlier_detection{
    protocol=basic *extreme beamstop model   << outlier protocol. See below.
    parameters{
      basic_wilson{
        level=1E-6                   << Outlier rejection level for protocol basic
      }
      extreme_wilson{
        level=0.01                   << Outlier rejection level for protocol extreme
      }
      model_based{
        level=0.01                   << outlier rejection level for protocol model
      }
    }
  }
  additional_parameters{
    free_flag_generation{            << If no free flags are provided, they will be generated
      fraction = 0.10                << Fraction of free flags
      max_number = 2000              << Maximum number of free flags
      lattice_symmetry_max_delta=5.0 << Lattice symmetry maximum delta (no need to touch this)
      use_lattice_symmetry=true      << whether or not to use the lattice symmetry in free flag generation
    }
  }

  output{
    logfile=outlier_tools.log        << The output logfile. Has a copy of the screen dump and (possibly) added plots
    hklout=None                      << an mtz file with observations that are not outliers according to the selected protocol
  }
}


A basic run looks like:

%(command_name)s data.file=my_precious_data.mtz \\
  data.obs=FOBS data.free=TEST \\
  model.file=my_baby.pdb proto=model hklout=new.mtz

A short description of the 3 outlier protocols are described below

1. protocol: basic
   Outliers are reflection for which 1 - P(E^2=>E_obs^2) < level
   For the default value, this results in (approximately) designating
   reflections with E value larger then 3.8 as an outlier.
    see Read, Acta Cryst. (1999). D55, 1759-1764 for details.

2. protocol: extreme
   Outliers are reflections for which 1 - [P(E^2=>E_obs^2)]^Nobs < level
   Nobs is the number of observations. In this manner, the size of the dataset
   is taken into account in the decision if something is an outlier.
   The basic and extrenme protocol usually result in similar outliers.

3. protocol: model
   Calculated amplitudes and estimated values of alpha and beta
   are used to compute the log-likelihood of the observed amplitude.
   The method is inspired by Read, Acta Cryst. (1999). D55, 1759-1764.
   Outliers are rejected on the basis of the assumption that the log
   likelihood differnce log[P(Fmode)]-log[P(Fobs)] is distributed
   according to a Chi-square distribution
   (see http://en.wikipedia.org/wiki/Likelihood-ratio_test ).
   The outlier threshold of relates to the p-value of the
   extreme value distribution of the chi-square distribution.

""" % vars())

def run(args, command_name="phenix.remove_outliers"):
  if (len(args)==0 or "--help" in args or "--h" in args or "-h" in args):
    print_help(command_name=command_name)
  else:
    log = multi_out()
    plot_out = None
    if (not "--quiet" in args):
      log.register(label="stdout", file_object=sys.stdout)
    string_buffer = StringIO()
    string_buffer_plots = StringIO()
    log.register(label="log_buffer", file_object=string_buffer)

    phil_objects = []
    argument_interpreter = master_params.command_line_argument_interpreter(
      home_scope="outlier_detection")

    for arg in args:
      command_line_params = None
      arg_is_processed = False
      # is it a file?
      if arg=="--quiet":
        arg_is_processed = True
      if (os.path.isfile(arg)): ## is this a file name?
        # check if it is a phil file
        try:
          command_line_params = iotbx.phil.parse(file_name=arg)
          if command_line_params is not None:
            phil_objects.append(command_line_params)
            arg_is_processed = True
        except KeyboardInterrupt: raise
        except Exception : pass
      else:
        try:
          command_line_params = argument_interpreter.process(arg=arg)
          if command_line_params is not None:
            phil_objects.append(command_line_params)
            arg_is_processed = True
        except KeyboardInterrupt: raise
        except Exception : pass

      if not arg_is_processed:
        print("##----------------------------------------------##", file=log)
        print("## Unknown file or keyword:", arg, file=log)
        print("##----------------------------------------------##", file=log)
        print(file=log)
        raise Sorry("Unknown file or keyword: %s" % arg)

    effective_params = master_params.fetch(sources=phil_objects)
    params = effective_params.extract()
    if not os.path.exists( params.outlier_utils.input.xray_data.file_name ):
      raise Sorry("File %s can not be found"%(params.outlier_utils.input.xray_data.file_name) )
    if params.outlier_utils.input.model.file_name is not None:
      if not os.path.exists( params.outlier_utils.input.model.file_name ):
        raise Sorry("File %s can not be found"%(params.outlier_utils.input.model.file_name) )



    # now get the unit cell from the pdb file

    hkl_xs = None
    if params.outlier_utils.input.xray_data.file_name is not None:
      hkl_xs = crystal_symmetry_from_any.extract_from(
        file_name=params.outlier_utils.input.xray_data.file_name)
    pdb_xs = None
    if params.outlier_utils.input.model.file_name is not None:
      pdb_xs = crystal_symmetry_from_any.extract_from(
        file_name=params.outlier_utils.input.model.file_name)

    phil_xs = crystal.symmetry(
      unit_cell=params.outlier_utils.input.unit_cell,
      space_group_info=params.outlier_utils.input.space_group  )

    phil_xs.show_summary()
    hkl_xs.show_summary()


    combined_xs = crystal.select_crystal_symmetry(
      None,phil_xs, [pdb_xs],[hkl_xs])

    # inject the unit cell and symmetry in the phil scope please
    params.outlier_utils.input.unit_cell = combined_xs.unit_cell()
    params.outlier_utils.input.space_group = \
      sgtbx.space_group_info( group = combined_xs.space_group() )

    new_params =  master_params.format(python_object=params)
    new_params.show(out=log)

    if params.outlier_utils.input.unit_cell is None:
      raise Sorry("unit cell not specified")
    if params.outlier_utils.input.space_group is None:
      raise Sorry("space group not specified")
    if params.outlier_utils.input.xray_data.file_name is None:
      raise Sorry("Xray data not specified")
    if params.outlier_utils.input.model.file_name is None:
      print("PDB file not specified. Basic wilson outlier rejections only.")



    #-----------------------------------------------------------
    #
    # step 1: read in the reflection file
    #
    phil_xs = crystal.symmetry(
      unit_cell=params.outlier_utils.input.unit_cell,
      space_group_info=params.outlier_utils.input.space_group  )

    xray_data_server =  reflection_file_utils.reflection_file_server(
      crystal_symmetry = phil_xs,
      force_symmetry = True,
      reflection_files=[])

    miller_array = None

    miller_array = xray_data_server.get_xray_data(
      file_name = params.outlier_utils.input.xray_data.file_name,
      labels = params.outlier_utils.input.xray_data.obs_labels,
      ignore_all_zeros = True,
      parameter_scope = 'outlier_utils.input.xray_data',
      parameter_name = 'obs_labels'
      )

    info = miller_array.info()

    miller_array = miller_array.map_to_asu()

    miller_array = miller_array.select(
      miller_array.indices() != (0,0,0))

    #we have to check if the sigma's make any sense at all
    if not miller_array.sigmas_are_sensible():
      miller_array = miller_array.customized_copy(
        data = miller_array.data(),
        sigmas=None).set_observation_type(miller_array)
    miller_array = miller_array.select(
      miller_array.data() > 0 )
    if  miller_array.sigmas() is not None:
      miller_array = miller_array.select(
        miller_array.sigmas() > 0 )

    if (miller_array.is_xray_intensity_array()):
      miller_array = miller_array.f_sq_as_f()
    elif (miller_array.is_complex_array()):
      miller_array = abs(miller_array)

    miller_array.set_info(info)
    merged_anomalous=False
    if miller_array.anomalous_flag():
      miller_array = miller_array.average_bijvoet_mates().set_observation_type(
        miller_array )
      merged_anomalous=True
    miller_array = miller_array.map_to_asu()

    # get the free reflections please
    free_flags = None
    if params.outlier_utils.input.xray_data.free_flags is None:
      free_flags = miller_array.generate_r_free_flags(
         fraction=params.outlier_utils.\
           additional_parameters.free_flag_generation.fraction,
         max_free=params.outlier_utils.\
           additional_parameters.free_flag_generation.max_number,
         lattice_symmetry_max_delta=params.outlier_utils.\
           additional_parameters.free_flag_generation.lattice_symmetry_max_delta,
         use_lattice_symmetry=params.outlier_utils.\
           additional_parameters.free_flag_generation.use_lattice_symmetry
        )
    else:
      free_flags = xray_data_server.get_xray_data(
        file_name = params.outlier_utils.input.xray_data.file_name,
        labels = params.outlier_utils.input.xray_data.free_flags,
        ignore_all_zeros = True,
        parameter_scope = 'outlier_utils.input.xray_data',
        parameter_name = 'free_flags'
        )

      if free_flags.anomalous_flag():
        free_flags = free_flags.average_bijvoet_mates()
        merged_anomalous=True
      free_flags = free_flags.customized_copy(
        data = flex.bool( free_flags.data() == 1 ))
      free_flags = free_flags.map_to_asu()
      free_flags = free_flags.common_set( miller_array )


    print(file=log)
    print("Summary info of observed data", file=log)
    print("=============================", file=log)
    miller_array.show_summary(f=log)
    if merged_anomalous:
      print("For outlier detection purposes, the Bijvoet pairs have been merged.", file=log)
    print(file=log)

    print("Constructing an outlier manager", file=log)
    print("===============================", file=log)
    print(file=log)
    outlier_manager = outlier_rejection.outlier_manager(
      miller_array,
      free_flags,
      out=log)

    basic_array = None
    extreme_array = None
    model_based_array = None

    basic_array = outlier_manager.basic_wilson_outliers(
      p_basic_wilson = params.outlier_utils.outlier_detection.\
                       parameters.basic_wilson.level,
      return_data = True)

    extreme_array = outlier_manager.extreme_wilson_outliers(
      p_extreme_wilson = params.outlier_utils.outlier_detection.parameters.\
                         extreme_wilson.level,
      return_data = True)

    beamstop_array = outlier_manager.beamstop_shadow_outliers(
      level = params.outlier_utils.outlier_detection.parameters.\
               beamstop.level,
      d_min = params.outlier_utils.outlier_detection.parameters.\
               beamstop.d_min,
      return_data=True)



    #----------------------------------------------------------------
    # Step 2: get an xray structure from the PDB file
    #
    if params.outlier_utils.input.model.file_name is not None:
      model = pdb.input(file_name=params.outlier_utils.input.model.file_name).xray_structure_simple(
        crystal_symmetry=phil_xs)
      print("Atomic model summary", file=log)
      print("====================", file=log)
      model.show_summary(f=log)
      print(file=log)


      # please make an f_model object for bulk solvent scaling etc etc

      f_model_object = f_model.manager(
        f_obs = miller_array,
        r_free_flags = free_flags,
        xray_structure = model )
      print("Bulk solvent scaling of the data", file=log)
      print("================================", file=log)
      print("Maximum likelihood bulk solvent scaling.", file=log)
      print(file=log)
      f_model_object.update_all_scales(log=log, remove_outliers=False)
      plot_out = StringIO()
      model_based_array = outlier_manager.model_based_outliers(
        f_model_object.f_model(),
        level=params.outlier_utils.outlier_detection.parameters.model_based.level,
        return_data=True,
        plot_out=plot_out)
    #check what needs to be put out please
    if params.outlier_utils.output.hklout is not None:
      if params.outlier_utils.outlier_detection.protocol == "model":
        if params.outlier_utils.input.model.file_name == None:
          print("Model based rejections requested. No model was supplied.", file=log)
          print("Switching to writing out rejections based on extreme value Wilson statistics.", file=log)
          params.outlier_utils.outlier_detection.protocol="extreme"

      output_array = None
      print(file=log)
      if params.outlier_utils.outlier_detection.protocol == "basic":
        print("Non-outliers found by the basic wilson statistics", file=log)
        print("protocol will be written out.", file=log)
        output_array = basic_array
        new_set_of_free_flags = free_flags.common_set( basic_array )

      if params.outlier_utils.outlier_detection.protocol == "extreme":
        print("Non-outliers found by the extreme value wilson statistics", file=log)
        print("protocol will be written out.", file=log)
        output_array = extreme_array
        new_set_of_free_flags = free_flags.common_set( extreme_array )

      if params.outlier_utils.outlier_detection.protocol == "model":
        print("Non-outliers found by the model based", file=log)
        print("protocol will be written out to the file:", file=log)
        print(params.outlier_utils.output.hklout, file=log)
        print(file=log)
        output_array = model_based_array
        new_set_of_free_flags = free_flags.common_set( model_based_array )

      if params.outlier_utils.outlier_detection.protocol == "beamstop":
        print("Outliers found for the beamstop shadow", file=log)
        print("problems detection protocol will be written to the file:", file=log)
        print(params.outlier_utils.output.hklout, file=log)
        print(file=log)
        output_array = model_based_array
        new_set_of_free_flags = free_flags.common_set( model_based_array )

      mtz_dataset = output_array.as_mtz_dataset(
        column_root_label="FOBS")
      mtz_dataset = mtz_dataset.add_miller_array(
        miller_array = new_set_of_free_flags,
        column_root_label = "Free_R_Flag"
        )
      mtz_dataset.mtz_object().write(
        file_name=params.outlier_utils.output.hklout)

    if (params.outlier_utils.output.logfile is not None):
      final_log = StringIO()
      print(string_buffer.getvalue(), file=final_log)
      print(file=final_log)
      if plot_out is not None:
        print(plot_out.getvalue(), file=final_log)
      outfile = open( params.outlier_utils.output.logfile, 'w' )
      outfile.write( final_log.getvalue() )
      print(file=log)
      print("A logfile named %s was created."%(
        params.outlier_utils.output.logfile), file=log)
      print("This logfile contains the screen output and", file=log)
      print("(possibly) some ccp4 style loggraph plots", file=log)


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/sigmaa_estimation.py
from __future__ import absolute_import, division, print_function
from mmtbx.scaling import absolute_scaling
from mmtbx.scaling import ext
from cctbx.array_family import flex
from mmtbx import max_lik
import scitbx.lbfgs
from scitbx.math import chebyshev_polynome
from scitbx.math import chebyshev_lsq_fit
from libtbx.math_utils import iround
import math
import sys
import iotbx.phil
from libtbx import group_args
from six.moves import zip
from six.moves import range

sigmaa_estimator_params = iotbx.phil.parse("""\
  kernel_width_free_reflections = 100
    .type = int
  kernel_on_chebyshev_nodes = True
    .type = bool
  number_of_sampling_points = 20
    .type = int
  number_of_chebyshev_terms = 10
    .type = int
  use_sampling_sum_weights = True
    .type = bool
""")


class sigmaa_point_estimator(object):
  def __init__(self,
               target_functor,
               h):
    self.functor = target_functor
    self.h = h
    self.f = None
    self.x = flex.double( [-1.0] )
    self.max = 0.99
    self.min = 0.01
    term_parameters = scitbx.lbfgs.termination_parameters(
      max_iterations = 100 )
    # the parametrisation as a sigmoid makes things difficult at the edges
    exception_handling_parameters = scitbx.lbfgs.exception_handling_parameters(
      ignore_line_search_failed_step_at_lower_bound=True,
      ignore_line_search_failed_step_at_upper_bound=True)
    self.minimizer = scitbx.lbfgs.run(target_evaluator=self,
                                      termination_params=term_parameters,
                                      exception_handling_params=exception_handling_parameters)
    self.sigmaa = self.min+(self.max-self.min)/(1.0+math.exp(-self.x[0]))

  def compute_functional_and_gradients(self):
    sigmaa = self.min+(self.max-self.min)/(1.0+math.exp(-self.x[0]))
    # chain rule bit for sigmoidal function
    dsdx = (self.max-self.min)*math.exp(-self.x[0])/(
      (1.0+math.exp(-self.x[0]))**2.0 )
    f,g = self.functor.target_and_gradient( self.h,
                                             sigmaa)
    self.f = -f
    return -f, flex.double([-g*dsdx])

def sigmaa_estimator_kernel_width_d_star_cubed(
      r_free_flags,
      kernel_width_free_reflections):
  assert kernel_width_free_reflections > 0
  n_refl = r_free_flags.size()
  n_free = r_free_flags.data().count(True)
  n_refl_per_bin = kernel_width_free_reflections
  if (n_free != 0):
    n_refl_per_bin *= n_refl / n_free
  n_refl_per_bin = min(n_refl, iround(n_refl_per_bin))
  n_bins = max(1, n_refl / max(1, n_refl_per_bin))
  dsc_min, dsc_max = [dss**(3/2) for dss in r_free_flags.min_max_d_star_sq()]
  return (dsc_max - dsc_min) / n_bins

class sigmaa_estimator(object):
  def __init__(self,
               miller_obs,
               miller_calc,
               r_free_flags,
               kernel_width_free_reflections=None,
               kernel_width_d_star_cubed=None,
               kernel_in_bin_centers=False,
               kernel_on_chebyshev_nodes=True,
               n_sampling_points=20,
               n_chebyshev_terms=10,
               use_sampling_sum_weights=False,
               make_checks_and_clean_up=True):
    assert [kernel_width_free_reflections, kernel_width_d_star_cubed].count(None) == 1

    self.miller_obs = miller_obs
    self.miller_calc = abs(miller_calc)
    self.r_free_flags = r_free_flags
    self.kernel_width_free_reflections = kernel_width_free_reflections
    self.kernel_width_d_star_cubed = kernel_width_d_star_cubed
    self.n_chebyshev_terms = n_chebyshev_terms

    if make_checks_and_clean_up:
      self.miller_obs = self.miller_obs.map_to_asu()
      self.miller_calc = self.miller_calc.map_to_asu()
      self.r_free_flags = self.r_free_flags.map_to_asu()
      assert self.r_free_flags.indices().all_eq(
        self.miller_obs.indices() )
      self.miller_calc = self.miller_calc.common_set(
        self.miller_obs )
      assert self.r_free_flags.indices().all_eq(
        self.miller_calc.indices() )
      assert self.miller_obs.is_real_array()

      if self.miller_obs.is_xray_intensity_array():
        self.miller_obs = self.miller_obs.f_sq_as_f()
      assert self.miller_obs.observation_type() is None or \
             self.miller_obs.is_xray_amplitude_array()

    if self.miller_calc.observation_type() is None:
      self.miller_calc = self.miller_calc.set_observation_type(
        self.miller_obs)

    # get normalized data please
    self.normalized_obs_f = absolute_scaling.kernel_normalisation(
      self.miller_obs, auto_kernel=True)
    self.normalized_obs =self.normalized_obs_f.normalised_miller_dev_eps.f_sq_as_f()

    self.normalized_calc_f = absolute_scaling.kernel_normalisation(
      self.miller_calc, auto_kernel=True)
    self.normalized_calc =self.normalized_calc_f.normalised_miller_dev_eps.f_sq_as_f()

    # get the 'free data'

    if(self.r_free_flags.data().count(True) == 0):
      self.r_free_flags = self.r_free_flags.array(
        data = ~self.r_free_flags.data())

    self.free_norm_obs = self.normalized_obs.select( self.r_free_flags.data() )
    self.free_norm_calc= self.normalized_calc.select( self.r_free_flags.data() )

    if self.free_norm_obs.data().size() <= 0:
      raise RuntimeError("No free reflections.")

    if (self.kernel_width_d_star_cubed is None):
      self.kernel_width_d_star_cubed=sigmaa_estimator_kernel_width_d_star_cubed(
        r_free_flags=self.r_free_flags,
        kernel_width_free_reflections=self.kernel_width_free_reflections)

    self.sigma_target_functor = ext.sigmaa_estimator(
      e_obs     = self.free_norm_obs.data(),
      e_calc    = self.free_norm_calc.data(),
      centric   = self.free_norm_obs.centric_flags().data(),
      d_star_cubed = self.free_norm_obs.d_star_cubed().data() ,
      width=self.kernel_width_d_star_cubed)

    d_star_cubed_overall = self.miller_obs.d_star_cubed().data()
    self.min_h = flex.min( d_star_cubed_overall )
    self.max_h = flex.max( d_star_cubed_overall )
    self.h_array = None
    if (kernel_in_bin_centers):
      self.h_array = flex.double( range(1,n_sampling_points*2,2) )*(
        self.max_h-self.min_h)/(n_sampling_points*2)+self.min_h
    else:
      self.min_h *= 0.99
      self.max_h *= 1.01
      if kernel_on_chebyshev_nodes:
        self.h_array = chebyshev_lsq_fit.chebyshev_nodes(
          n=n_sampling_points,
          low=self.min_h,
          high=self.max_h,
          include_limits=True)
      else:
        self.h_array = flex.double( range(n_sampling_points) )*(
          self.max_h-self.min_h)/float(n_sampling_points-1.0)+self.min_h
    assert self.h_array.size() == n_sampling_points
    self.sigmaa_array = flex.double()
    self.sigmaa_array.reserve(self.h_array.size())
    self.sum_weights = flex.double()
    self.sum_weights.reserve(self.h_array.size())

    for h in self.h_array:
      stimator = sigmaa_point_estimator(self.sigma_target_functor, h)
      self.sigmaa_array.append( stimator.sigmaa )
      self.sum_weights.append(
        self.sigma_target_functor.sum_weights(d_star_cubed=h))

    # fit a smooth function
    reparam_sa = -flex.log( 1.0/self.sigmaa_array -1.0 )
    if (use_sampling_sum_weights):
      w_obs = flex.sqrt(self.sum_weights)
    else:
      w_obs = None
    fit_lsq = chebyshev_lsq_fit.chebyshev_lsq_fit(
      n_terms=self.n_chebyshev_terms,
      x_obs=self.h_array,
      y_obs=reparam_sa,
      w_obs=w_obs)

    cheb_pol = chebyshev_polynome(
        self.n_chebyshev_terms,
        self.min_h,
        self.max_h,
        fit_lsq.coefs)
    def reverse_reparam(values): return 1.0/(1.0 + flex.exp(-values))
    self.sigmaa_fitted = reverse_reparam(cheb_pol.f(self.h_array))
    self.sigmaa_miller_array = reverse_reparam(cheb_pol.f(d_star_cubed_overall))
    assert flex.min(self.sigmaa_miller_array) >= 0
    assert flex.max(self.sigmaa_miller_array) <= 1
    self.sigmaa_miller_array = self.miller_obs.array(data=self.sigmaa_miller_array)

    self.alpha = None
    self.beta = None
    self.fom_array = None

  def sigmaa(self):
    return self.sigmaa_miller_array

  def sigmaa_model_error(self):
    x = 0.25*flex.pow( self.h_array, 2.0/3.0 )  # h was in d*^-3 !!!
    y = flex.log( self.sigmaa_fitted )
    #compute the slope please
    result = flex.linear_regression( x, y )
    result = -(result.slope()/math.pi*3)
    if result < 0:
      result = None
    else:
      result = math.sqrt( result )
    return result

  def fom(self):
    if self.fom_array is None:
      tmp_x = self.sigmaa_miller_array.data()*self.normalized_calc.data()*self.normalized_obs.data()
      tmp_x = tmp_x / (1.0-self.sigmaa_miller_array.data()*self.sigmaa_miller_array.data())
      centric_fom = flex.tanh( tmp_x )
      acentric_fom = scitbx.math.bessel_i1_over_i0( 2.0*tmp_x )
      # we need to make sure centric and acentrics are not mixed up ...
      centrics = self.sigmaa_miller_array.centric_flags().data()
      centric_fom  = centric_fom.set_selected( ~centrics, 0 )
      acentric_fom = acentric_fom.set_selected( centrics, 0 )
      final_fom =  centric_fom + acentric_fom
      self.fom_array = self.sigmaa_miller_array.customized_copy(data=final_fom)
    return self.fom_array

  def phase_errors(self):
    alpha, beta = self.alpha_beta()
    result = max_lik.fom_and_phase_error(
      f_obs          = self.miller_obs.data(),
      f_model        = flex.abs(self.miller_calc.data()),
      alpha          = alpha.data(),
      beta           = beta.data(),
      epsilons       = self.miller_obs.epsilons().data().as_double(),
      centric_flags  = self.miller_obs.centric_flags().data()).phase_error()
    result =  self.miller_obs.customized_copy(data=result)
    return result

  def alpha_beta(self):
    if self.alpha is None:
      self.alpha = self.sigmaa_miller_array.data()*flex.sqrt(
        self.normalized_obs_f.normalizer_for_miller_array/
        self.normalized_calc_f.normalizer_for_miller_array)
      self.beta = (1.0-self.sigmaa_miller_array.data()*self.sigmaa_miller_array.data())*\
                  self.normalized_obs_f.normalizer_for_miller_array
      self.alpha = self.miller_obs.array(data=self.alpha)
      self.beta = self.miller_obs.array(data=self.beta)

    return self.alpha, self.beta

  def show(self, out=None):
    if out is None:
      out = sys.stdout
    print(file=out)
    print("SigmaA estimation summary", file=out)
    print("-------------------------", file=out)
    print("Kernel width d* cubed     :  %.6g" % \
      self.kernel_width_d_star_cubed, file=out)
    print("Kernel width free refl.   : ", \
      self.kernel_width_free_reflections, file=out)
    print("Number of sampling points : ", self.h_array.size(), file=out)
    print("Number of Chebyshev terms : ", self.n_chebyshev_terms, file=out)
    print(file=out)
    print("1/d^3      d    sum weights  sigmaA   fitted    diff", file=out)
    for h,w,sa,saf in zip(
          self.h_array,
          self.sum_weights,
          self.sigmaa_array,
          self.sigmaa_fitted):
      if (h == 0):
        d = " "*7
      else:
        d = "%7.4f" % (1.0/h)**(1/3)
      print("%5.4f  %s  %8.2f   %7.4f  %7.4f  %7.4f" % (
        h,d,w,sa,saf,saf-sa), file=out)
    print(file=out)
    print(file=out)

  def show_short(self, out=None, silent=False):
    if(out is None): out = sys.stdout
    if(not silent): print(file=out)
    if(not silent): print("SigmaA vs Resolution", file=out)
    if(not silent): print("--------------------", file=out)
    if(not silent): print("1/d^3      d    sum weights  sigmaA", file=out)
    resolution = []
    sigmaa = []
    for h, sa in zip(self.h_array, self.sigmaa_array):
      if(h == 0): d = " "*7
      else: d = "%7.4f" % (1.0/h)**(1/3)
      if(not silent): print("%s %7.4f" % (d, sa), file=out)
      resolution.append(d)
      sigmaa.append(sa)
    if(not silent): print(file=out)
    if(not silent): print(file=out)
    return group_args(resolution = resolution, sigmaa = sigmaa)


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/ta_alpha_beta_calc.py
from __future__ import absolute_import, division, print_function
from mmtbx.scaling import absolute_scaling
from mmtbx.scaling import ext
from cctbx.array_family import flex
from mmtbx import max_lik
import scitbx.lbfgs
from scitbx.math import chebyshev_polynome
from scitbx.math import chebyshev_lsq_fit
from libtbx.math_utils import iround
import math
import sys
import iotbx.phil
from six.moves import zip

sigmaa_estimator_params = iotbx.phil.parse("""\
  kernel_width_free_reflections = 100
    .type = int
  kernel_on_chebyshev_nodes = True
    .type = bool
  number_of_sampling_points = 20
    .type = int
  number_of_chebyshev_terms = 10
    .type = int
  use_sampling_sum_weights = True
    .type = bool
""")


class sigmaa_point_estimator(object):
  def __init__(self,
               target_functor,
               h):
    self.functor = target_functor
    self.h = h
    self.f = None
    self.x = flex.double( [-1.0] )
    self.max = 0.99
    self.min = 0.01
    term_parameters = scitbx.lbfgs.termination_parameters(
      max_iterations = 100 )
    # the parametrisation as a sigmoid makes things difficult at the edges
    exception_handling_parameters = scitbx.lbfgs.exception_handling_parameters(
      ignore_line_search_failed_step_at_lower_bound=True,
      ignore_line_search_failed_step_at_upper_bound=True)
    self.minimizer = scitbx.lbfgs.run(target_evaluator=self,
                                      termination_params=term_parameters,
                                      exception_handling_params=exception_handling_parameters)
    self.sigmaa = self.min+(self.max-self.min)/(1.0+math.exp(-self.x[0]))

  def compute_functional_and_gradients(self):
    sigmaa = self.min+(self.max-self.min)/(1.0+math.exp(-self.x[0]))
    # chain rule bit for sigmoidal function
    dsdx = (self.max-self.min)*math.exp(-self.x[0])/(
      (1.0+math.exp(-self.x[0]))**2.0 )
    f,g = self.functor.target_and_gradient( self.h,
                                             sigmaa)
    self.f = -f
    return -f, flex.double([-g*dsdx])

def sigmaa_estimator_kernel_width_d_star_cubed(
      r_free_flags,
      kernel_width_free_reflections):
  assert kernel_width_free_reflections > 0
  n_refl = r_free_flags.size()
  n_free = r_free_flags.data().count(True)
  n_refl_per_bin = kernel_width_free_reflections
  if (n_free != 0):
    n_refl_per_bin *= n_refl / n_free
  n_refl_per_bin = min(n_refl, iround(n_refl_per_bin))
  n_bins = max(1, n_refl / max(1, n_refl_per_bin))
  dsc_min, dsc_max = [dss**(3/2) for dss in r_free_flags.min_max_d_star_sq()]
  return (dsc_max - dsc_min) / n_bins

class ta_alpha_beta_calc(object):
  def __init__(self,
               miller_obs,
               miller_calc,
               r_free_flags,
               ta_d,
               kernel_width_free_reflections=None,
               kernel_width_d_star_cubed=None,
               kernel_in_bin_centers=False,
               kernel_on_chebyshev_nodes=True,
               n_sampling_points=20,
               n_chebyshev_terms=10,
               use_sampling_sum_weights=False,
               make_checks_and_clean_up=True):
    assert [kernel_width_free_reflections, kernel_width_d_star_cubed].count(None) == 1

    self.miller_obs = miller_obs
    self.miller_calc = abs(miller_calc)
    self.r_free_flags = r_free_flags
    self.kernel_width_free_reflections = kernel_width_free_reflections
    self.kernel_width_d_star_cubed = kernel_width_d_star_cubed
    self.n_chebyshev_terms = n_chebyshev_terms
    self.ta_d = ta_d


    if make_checks_and_clean_up:
      self.miller_obs = self.miller_obs.map_to_asu()
      self.miller_calc = self.miller_calc.map_to_asu()
      self.r_free_flags = self.r_free_flags.map_to_asu()
      assert self.r_free_flags.indices().all_eq(
        self.miller_obs.indices() )
      self.miller_calc = self.miller_calc.common_set(
        self.miller_obs )
      assert self.r_free_flags.indices().all_eq(
        self.miller_calc.indices() )
      assert self.miller_obs.is_real_array()

      if self.miller_obs.is_xray_intensity_array():
        self.miller_obs = self.miller_obs.f_sq_as_f()
      assert self.miller_obs.observation_type() is None or \
             self.miller_obs.is_xray_amplitude_array()

    if self.miller_calc.observation_type() is None:
      self.miller_calc = self.miller_calc.set_observation_type(
        self.miller_obs)

    # get normalized data please
    self.normalized_obs_f = absolute_scaling.kernel_normalisation(
      self.miller_obs, auto_kernel=True)
    self.normalized_obs =self.normalized_obs_f.normalised_miller_dev_eps.f_sq_as_f()

    self.normalized_calc_f = absolute_scaling.kernel_normalisation(
      self.miller_calc, auto_kernel=True)
    self.normalized_calc =self.normalized_calc_f.normalised_miller_dev_eps.f_sq_as_f()

    # get the 'free data'
    self.free_norm_obs = self.normalized_obs.select( self.r_free_flags.data() )
    self.free_norm_calc= self.normalized_calc.select( self.r_free_flags.data() )

    if self.free_norm_obs.data().size() <= 0:
      raise RuntimeError("No free reflections.")

#    if (self.kernel_width_d_star_cubed is None):
#      self.kernel_width_d_star_cubed=sigmaa_estimator_kernel_width_d_star_cubed(
#        r_free_flags=self.r_free_flags,
#        kernel_width_free_reflections=self.kernel_width_free_reflections)

#    self.sigma_target_functor = ext.sigmaa_estimator(
#      e_obs     = self.free_norm_obs.data(),
#      e_calc    = self.free_norm_calc.data(),
#      centric   = self.free_norm_obs.centric_flags().data(),
#      d_star_cubed = self.free_norm_obs.d_star_cubed().data() ,
#      width=self.kernel_width_d_star_cubed)

#    d_star_cubed_overall = self.miller_obs.d_star_cubed().data()
#    self.min_h = flex.min( d_star_cubed_overall )
#    self.max_h = flex.max( d_star_cubed_overall )
#    self.h_array = None
#    if (kernel_in_bin_centers):
#      self.h_array = flex.double( range(1,n_sampling_points*2,2) )*(
#        self.max_h-self.min_h)/(n_sampling_points*2)+self.min_h
#    else:
#      self.min_h *= 0.99
#      self.max_h *= 1.01
#      if kernel_on_chebyshev_nodes:
#        self.h_array = chebyshev_lsq_fit.chebyshev_nodes(
#          n=n_sampling_points,
#          low=self.min_h,
#          high=self.max_h,
#          include_limits=True)
#      else:
#        self.h_array = flex.double( range(n_sampling_points) )*(
#          self.max_h-self.min_h)/float(n_sampling_points-1.0)+self.min_h

#    assert self.h_array.size() == n_sampling_points
#    self.sigmaa_array = flex.double()
#    self.sigmaa_array.reserve(self.h_array.size())
#    self.sum_weights = flex.double()
#    self.sum_weights.reserve(self.h_array.size())

#    for h in self.h_array:
#      stimator = sigmaa_point_estimator(self.sigma_target_functor, h)
#      self.sigmaa_array.append( stimator.sigmaa )
#      self.sum_weights.append(
#        self.sigma_target_functor.sum_weights(d_star_cubed=h))

#    # fit a smooth function
#    reparam_sa = -flex.log( 1.0/self.sigmaa_array -1.0 )
#    if (use_sampling_sum_weights):
#      w_obs = flex.sqrt(self.sum_weights)
#    else:
#      w_obs = None
#    fit_lsq = chebyshev_lsq_fit.chebyshev_lsq_fit(
#      n_terms=self.n_chebyshev_terms,
#      x_obs=self.h_array,
#      y_obs=reparam_sa,
#      w_obs=w_obs)

#    cheb_pol = chebyshev_polynome(
#        self.n_chebyshev_terms,
#        self.min_h,
#        self.max_h,
#        fit_lsq.coefs)
#    def reverse_reparam(values): return 1.0/(1.0 + flex.exp(-values))
#    self.sigmaa_fitted = reverse_reparam(cheb_pol.f(self.h_array))
#    self.sigmaa_miller_array = reverse_reparam(cheb_pol.f(d_star_cubed_overall))
#    assert flex.min(self.sigmaa_miller_array) >= 0
#    assert flex.max(self.sigmaa_miller_array) <= 1
#    self.sigmaa_miller_array = self.miller_obs.array(data=self.sigmaa_miller_array)

    self.alpha = None
    self.beta = None
    self.fom_array = None
    self.ta_d_miller = self.miller_obs.array(data=self.ta_d)


  def sigmaa(self):
    return self.sigmaa_miller_array

  def sigmaa_model_error(self):
    x = 0.25*flex.pow( self.h_array, 2.0/3.0 )  # h was in d*^-3 !!!
    y = flex.log( self.sigmaa_fitted )
    #compute the slope please
    result = flex.linear_regression( x, y )
    result = -(result.slope()/math.pi*3)
    if result < 0:
      result = None
    else:
      result = math.sqrt( result )
    return result

  def fom(self):
    if self.fom_array is None:
      tmp_x = self.ta_d*self.normalized_calc.data()*self.normalized_obs.data()
      # tmp_x = tmp_x / ta_sumvar
      tmp_x = tmp_x / (1.0-self.ta_d*self.ta_d)
      centric_fom = flex.tanh( tmp_x )
      acentric_fom = scitbx.math.bessel_i1_over_i0( 2.0*tmp_x )
      # we need to make sure centric and acentrics are not mixed up ...
      centrics = self.ta_d_miller.centric_flags().data()
      centric_fom  = centric_fom.set_selected( ~centrics, 0 )
      acentric_fom = acentric_fom.set_selected( centrics, 0 )
      final_fom =  centric_fom + acentric_fom
      self.fom_array = self.ta_d_miller.customized_copy(data=final_fom)
    return self.fom_array

  def phase_errors(self):
    alpha, beta = self.alpha_beta()
    result = max_lik.fom_and_phase_error(
                           f_obs          = self.miller_obs.data(),
                           f_model        = flex.abs(self.miller_calc.data()),
                           alpha          = alpha.data(),
                           beta           = beta.data(),
                           space_group    = self.miller_obs.space_group(),
                           miller_indices = self.miller_obs.indices() ).phase_error()
    result =  self.miller_obs.customized_copy(data=result)
    return result

  def alpha_beta(self):
    if self.alpha is None:
      print("re calc a/b")
      self.alpha = self.ta_d*flex.sqrt(
        self.normalized_obs_f.normalizer_for_miller_array/
        self.normalized_calc_f.normalizer_for_miller_array)
      self.beta = (1.0-self.ta_d*self.ta_d)*\
                  self.normalized_obs_f.normalizer_for_miller_array
      self.alpha = self.miller_obs.array(data=self.alpha)
      self.beta = self.miller_obs.array(data=self.beta)
    return self.alpha, self.beta

  def eobs_and_ecalc_miller_array_normalizers(self):
    return self.normalized_obs_f.normalizer_for_miller_array, self.normalized_calc_f.normalizer_for_miller_array

  def show(self, out=None):
    if out is None:
      out = sys.stdout
    print(file=out)
    print("SigmaA estimation summary", file=out)
    print("-------------------------", file=out)
    print("Kernel width d* cubed     :  %.6g" % \
      self.kernel_width_d_star_cubed, file=out)
    print("Kernel width free refl.   : ", \
      self.kernel_width_free_reflections, file=out)
    print("Number of sampling points : ", self.h_array.size(), file=out)
    print("Number of Chebyshev terms : ", self.n_chebyshev_terms, file=out)
    print(file=out)
    print("1/d^3      d    sum weights  sigmaA   fitted    diff", file=out)
    for h,w,sa,saf in zip(
          self.h_array,
          self.sum_weights,
          self.sigmaa_array,
          self.sigmaa_fitted):
      if (h == 0):
        d = " "*7
      else:
        d = "%7.4f" % (1.0/h)**(1/3)
      print("%5.4f  %s  %8.2f   %7.4f  %7.4f  %7.4f" % (
        h,d,w,sa,saf,saf-sa), file=out)
    print(file=out)
    print(file=out)

  def show_short(self, out=None):
    if(out is None): out = sys.stdout
    print(file=out)
    print("SigmaA vs Resolution", file=out)
    print("--------------------", file=out)
    print("1/d^3      d    sum weights  sigmaA", file=out)
    for h, sa in zip(self.h_array, self.sigmaa_array):
      if(h == 0): d = " "*7
      else: d = "%7.4f" % (1.0/h)**(1/3)
      print("%s %7.4f" % (d, sa), file=out)
    print(file=out)
    print(file=out)


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/thorough_outlier_test.py
from __future__ import absolute_import, division, print_function
from cctbx.array_family import flex
import mmtbx.f_model
from cctbx.development import random_structure
from cctbx.development import debug_utils
from cctbx import sgtbx
import random
import sys
from mmtbx.scaling import outlier_rejection
from cctbx.xray import observation_types
from cctbx.development import debug_utils

if (1):
  random.seed(0)
  flex.set_random_seed(value=0)

def exercise(d_min            = 3.5,
             k_sol            = 0.3,
             b_sol            = 60.0,
             b_cart           = [0,0,0,0,0,0],
             anomalous_flag   = False,
             scattering_table = "it1992",
             space_group_info = None):
  space_groups = [ str(space_group_info) ]
  for sg in space_groups:
      ### get random structure
      xray_structure = random_structure.xray_structure(
                          space_group_info       = sgtbx.space_group_info(sg),
                          elements               = (("O","C","N")*50),
                          volume_per_atom        = 100,
                          min_distance           = 1.5,
                          general_positions_only = True,
                          random_u_iso           = True)
      xray_structure.scattering_type_registry(table = scattering_table)
      ### Get FOBS
      for scale in [0.0001, 1.0, 1000.0]:
          dummy = abs(xray_structure.structure_factors(
                                   d_min          = d_min,
                                   anomalous_flag = anomalous_flag).f_calc())
          flags = dummy.generate_r_free_flags(fraction = 0.1,
                                              max_free = 99999999)
          fmodel = mmtbx.f_model.manager(xray_structure   = xray_structure,
                                         r_free_flags     = flags,
                                         target_name      = "ls_wunit_k1",
                                         f_obs            = dummy,
                                         k_sol            = k_sol,
                                         b_sol            = b_sol,
                                         b_cart           = b_cart)

          fmodel.update_xray_structure(xray_structure = xray_structure,
                                       update_f_calc = True,
                                       update_f_mask = True)
          f_obs = abs(fmodel.f_model())
          f_obs = f_obs.array(data = f_obs.data()*scale)
          f_obs.set_observation_type(observation_type = observation_types.amplitude())
          ### look at non-model based outliers detection
          om = outlier_rejection.outlier_manager(miller_obs   = f_obs,
                                                 r_free_flags = flags,
                                                 out          = "silent")
          tmp1 = om.basic_wilson_outliers()
          tmp2 = om.extreme_wilson_outliers()
          tmp3 = om.beamstop_shadow_outliers()
          # start loop over distorted models
          for error in [0.0,  0.8]:
              for fraction in [0.0,0.5]:
                # get distorted model
                xrs_dc = xray_structure.deep_copy_scatterers()
                sel = xrs_dc.random_remove_sites_selection(fraction = fraction)
                xrs_dc = xrs_dc.select(sel)
                xrs_dc.shake_sites_in_place(rms_difference=error)
                xrs_dc.scattering_type_registry(table = scattering_table)
                for k_sol in [0.50,]:
                  for b_sol in [60.,]:
                    fmodel = mmtbx.f_model.manager(
                       xray_structure = xrs_dc,
                       r_free_flags   = flags,
                       target_name    = "ls_wunit_k1",
                       f_obs          = f_obs,
                       k_sol          = k_sol,
                       b_sol          = b_sol,
                       b_cart         = b_cart)
                    a,b = fmodel.alpha_beta()
                    o_sel =  om.model_based_outliers(f_model = fmodel.f_model())
                    n_out = o_sel.data().count(False)
                    assert (n_out < 5)

def run_call_back(flags, space_group_info):
  exercise(space_group_info=space_group_info)

def run():
  debug_utils.parse_options_loop_space_groups(sys.argv[1:], run_call_back)


if (__name__ == "__main__"):
  run()


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/tst_absences.py

"""
Regression tests for mmtbx.scaling.absences
"""

from __future__ import absolute_import, division, print_function
from mmtbx.scaling.absences import *
from cctbx.development import random_structure
from cctbx import sgtbx
from scitbx.array_family import flex
import libtbx.load_env
import os.path as op
import random

def exercise_abs_list():
  tmp = absences()
  assert tmp.check( "2_1 (c)", (0,0,1),True ) == (True, False)
  assert tmp.check( "2_1 (c)", (0,0,4),True ) == (True, True)
  assert tmp.check( "4_1 (a)", (4,0,0),True ) == (True, True)
  assert tmp.check( "3_1 (c)", (0,0,3),True ) == (True, True)

  tmp = sgi_iterator(chiral = True,
    crystal_system = None,
    intensity_symmetry = sgtbx.space_group_info(
      "P222").group().build_derived_reflection_intensity_group(False))
  sg_list = []
  abs_list = []
  for sg in tmp.list():
    sg_list.append( str(sg) )
    if str(sg)== "P 21 21 21":
      for s in sg.group():
        abs_list.append( conditions_for_operator( s ).absence_type() )
  assert "2_1 (a)" in abs_list
  assert "2_1 (b)" in abs_list
  assert "2_1 (c)" in abs_list

  assert "P 2 2 2" in sg_list
  assert "P 21 2 2" in sg_list
  assert "P 2 21 2" in sg_list
  assert "P 2 2 21" in sg_list
  assert "P 21 21 2" in sg_list
  assert "P 21 2 21" in sg_list
  assert "P 2 21 21" in sg_list
  assert "P 21 21 21" in sg_list

def exercise_analyze_absences():
  """
  Looping over common space groups in macromolecular structures, generate
  synthetic data converted to the derived intensity group with absent
  reflections present (I=0), and run the analysis to check that the actual
  space group is the best-scoring.
  """
  # space groups found in structures with data in the PDB as of Sep. 2011,
  # minus a few unusual ones
  symbols = ['C 2 2 21', 'P 31 1 2', 'P 62 2 2', 'P 2 3', 'P 3', 'P 1', 'P 63',
    'C 1 2 1', 'F 2 2 2', 'A 1 2 1', 'B 1 1 2', 'P 61', 'P 61 2 2', 'P 6 2 2',
    'P 2 21 21', 'P 21 21 21', 'R 3 2 :R', 'P 4 3 2', 'P 1 21 1', 'P 32 2 1',
    'P 42 2 2', 'P 43 21 2', 'P 21 2 21', 'I 2 3', 'P 43 3 2', 'I 41 3 2',
    'P 3 1 2', 'P 2 2 2', 'R 3 :H', 'P 21 21 2', 'P 65 2 2',
    'P 21 21 2', 'F 2 3', 'I 41 2 2', 'P 65', 'P 1 1 21', 'P 2 2 21',
    'P 4 2 2', 'I 2 2 2', 'I 4 3 2', 'R 3 :R', 'P 4', 'P 42', 'P 64 2 2',
    'I 1 2 1', 'P 64', 'C 1 2 1', 'I 41', 'P 63 2 2', 'P 3 2 1', 'P 41 2 2',
    'P 62', 'P 1', 'P 32 1 2', 'F 4 3 2', 'P 31 2 1', 'I 4 2 2', 'P 21 3',
    'P 43 2 2', 'C 2 2 2', 'P 4 21 2', 'B 2 21 2', 'P 41 21 2', 'P 31', 'P 41',
    'P 41 3 2', 'P 32', 'P 6', 'P 1 2 1', 'I 21 3', 'F 41 3 2',
    'P 42 3 2', 'P 42 21 2', 'P 43', 'I 21 21 21', 'R 3 2 :H', 'I 4']
  random.seed(987654321)
  flex.set_random_seed(987654321) #12345)
  for symbol in  ["P 42 3 2"] : #symbols :
    xrs = random_structure.xray_structure(
      space_group_symbol=symbol,
      elements=['O']*200,
      n_scatterers=200)
    fc = abs(xrs.structure_factors(d_min=1.5).f_calc())
    fc.set_observation_type_xray_amplitude()
    i_calc = fc.f_as_f_sq()
    space_group_info = i_calc.crystal_symmetry().space_group_info()
    n_absent = i_calc.sys_absent_flags().data().count(True)
    assert (n_absent == 0)
    ig = space_group_info.group().build_derived_reflection_intensity_group(
      False)
    complete_set = i_calc.complete_set()
    missing_set = complete_set.lone_set(i_calc)
    assert missing_set.size() == 0 # should be complete so far
    i_calc_orig = i_calc.deep_copy().customized_copy(
      sigmas=flex.double(i_calc.size(), 0.01))
    symm = i_calc.crystal_symmetry().customized_copy(
      space_group_info=ig.info())
    i_calc = i_calc.customized_copy(crystal_symmetry=symm)
    #i_calc.show_summary()
    complete_set = i_calc.complete_set()
    missing_set = complete_set.lone_set(i_calc) # these are the absences
    data = flex.double(missing_set.size(), 0.01)
    sys_abs = missing_set.array(data=data)
    i_calc = i_calc.concatenate(sys_abs)
    sigmas = flex.double(i_calc.size(), 0.01)
    i_calc = i_calc.customized_copy(sigmas=sigmas)
    i_calc = i_calc.set_observation_type_xray_intensity()
    # i_calc is now complete with respect to the derived reflection intensity
    # group, with I/sigma=1 for all systematic absences
    psgc = protein_space_group_choices(miller_array=i_calc,
      original_data=i_calc_orig)
    table = psgc.suggest_likely_candidates()
    min_score = min([ row[-1] for row in table ])
    expected_failures = ["I 4", "I 4 2 2", "P 42 3 2"]
    # The actual space group won't always be the first on the list (if the
    # absence rules are the same for more than one related space group), but
    # it should always have the lowest score.  (I think...)
    for row in table :
      group = row[0]
      score = row[-1]
      n_abs_viol = int(row[4])
      n_pres_viol = int(row[5])
      if (group == str(space_group_info)):
        assert (n_abs_viol == 0), row
        if (n_pres_viol != 0):
          print(group, n_pres_viol)
          psgc.show()
        # XXX Currently this fails for the following space groups:
        #     I 4
        #     I 4 2 2
        #     P 42 3 2
        assert ((score == min_score) or
                (str(space_group_info) in expected_failures))

# XXX depends on phenix_regression
def exercise_2():
  pdb_file = libtbx.env.find_in_repositories(
    relative_path="phenix_regression/pdb/x_001_HD_ricardo_leal.pdb",
    test=op.isfile)

if __name__ == "__main__":
  exercise_abs_list()
  exercise_analyze_absences()
  print("OK")


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/tst_bayesian_estimator.py

from __future__ import absolute_import, division, print_function
import sys
from libtbx.test_utils import approx_equal
from libtbx.utils import null_out

def exercise(args):
  if 'verbose' in args:
    out=sys.stdout
  else:
    out=null_out()

  print("Running basic estimator....", end=' ')
  from mmtbx.scaling.bayesian_estimator import bayesian_estimator
  run=bayesian_estimator(out=out)
  result_list=run.exercise()
  assert approx_equal(result_list,[0.975, 0.973, 0.746, 0.784],eps=1.e-3)
  print("OK")

  print("Running basic estimator with randomized data....", end=' ')
  cc=run.exercise_2(out=out)
  assert approx_equal(cc,0.988,eps=1.e-3)
  print("OK")

  print("Running jacknifed estimators...", end=' ')
  from mmtbx.scaling.bayesian_estimator import run_jacknife
  cc,dummy_target_list,dummy_result_list=run_jacknife(args=[],out=out)
  assert approx_equal(cc,0.859,eps=1.e-2)
  print("OK")

  print("Running group of estimators...", end=' ')
  from mmtbx.scaling.bayesian_estimator import exercise_group
  cc1,cc2=exercise_group(out=out)
  assert approx_equal(cc1,0.867,eps=1.e-2)
  assert approx_equal(cc2,-0.186,eps=1.e-3)
  print("OK")



if (__name__ == "__main__"):
  exercise(sys.argv[1:])
  print("OK")


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/tst_massage_data.py

from __future__ import absolute_import, division, print_function
from mmtbx.command_line import massage_data
from iotbx import file_reader
from cctbx.development import random_structure
from scitbx.array_family import flex
from libtbx.test_utils import approx_equal
from libtbx.utils import null_out
import os.path as op
import random
from six.moves import zip

def exercise_twin_detwin():
  random.seed(12345)
  flex.set_random_seed(12345)
  xrs = random_structure.xray_structure(
    unit_cell=(12,5,12,90,90,90),
    space_group_symbol="P1",
    n_scatterers=12,
    elements="random")
  fc = abs(xrs.structure_factors(d_min=1.5).f_calc())
  fc = fc.set_observation_type_xray_amplitude()
  mtz_file = "tmp_massage_in.mtz"
  fc.as_mtz_dataset(column_root_label="F").mtz_object().write(mtz_file)
  massage_data.run(
    args=[
      mtz_file,
      "aniso.action=None",
      "outlier.action=None",
      "symmetry.action=twin",
      "twin_law='l,-k,h'",
      "fraction=0.3",
      "hklout=tmp_massage_twinned.mtz",
    ],
    out=null_out())
  assert op.isfile("tmp_massage_twinned.mtz")
  mtz_in = file_reader.any_file("tmp_massage_twinned.mtz")
  fc_twin = mtz_in.file_server.miller_arrays[0].f_sq_as_f()
  fc_twin, fc_tmp = fc_twin.common_sets(other=fc)
  for hkl, f1, f2 in zip(fc_tmp.indices(), fc_tmp.data(), fc_twin.data()):
    if (abs(hkl[0]) != abs(hkl[2])):
      assert not approx_equal(f1, f2, eps=0.01, out=null_out()), (hkl, f1, f2)
  massage_data.run(
    args=[
      mtz_file,
      "aniso.action=None",
      "outlier.action=None",
      "symmetry.action=twin",
      "twin_law='l,-k,h'",
      "fraction=0.3",
      "hklout=tmp_massage_twinned.sca",
    ],
    out=null_out())
  assert op.isfile("tmp_massage_twinned.sca")
  massage_data.run(
    args=[
      "tmp_massage_twinned.mtz",
      "aniso.action=None",
      "outlier.action=None",
      "symmetry.action=detwin",
      "twin_law='l,-k,h'",
      "fraction=0.3",
      "hklout=tmp_massage_detwinned.mtz",
    ],
    out=null_out())
  mtz_in = file_reader.any_file("tmp_massage_detwinned.mtz")
  fc_detwin = mtz_in.file_server.miller_arrays[0].f_sq_as_f()
  fc_detwin, fc_tmp = fc_detwin.common_sets(other=fc)
  # XXX we appear to lose some accuracy here, possibly due to the use of
  # MTZ format
  for hkl, f1, f2 in zip(fc_tmp.indices(), fc_tmp.data(), fc_detwin.data()):
    assert approx_equal(f1, f2, eps=0.01), hkl

if (__name__ == "__main__"):
  exercise_twin_detwin()
  print("OK")


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/tst_mean_f_rms_f.py

from __future__ import absolute_import, division, print_function
import sys
#from libtbx.test_utils import approx_equal
from libtbx.utils import null_out

def exercise(args):
  if 'verbose' in args:
    out=sys.stdout
  else:
    out=null_out()

  print("Testing mean_f_rms_f....", end=' ')
  from mmtbx.scaling.mean_f_rms_f import test
  test(out=out)

if (__name__ == "__main__"):
  exercise(sys.argv[1:])
  print("OK")


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/tst_outlier.py
from __future__ import absolute_import, division, print_function
from mmtbx import scaling
from cctbx.array_family import flex
from libtbx.test_utils import approx_equal
from six.moves import zip
from six.moves import range


def tst_outliers_compare_mode_mean():
  fobs  = flex.double( range(1000) )/300.0
  fcalc = flex.double( range(1000) )/300.0
  sigmas = None
  epsilon = flex.double( range(1000) )*0 +1.0
  centric = flex.bool( [False]*1000 )

  for ii in range(50):
    a = ii/50.0
    b = 1.0-a*a
    alpha = flex.double( [a]*1000 )
    beta  = flex.double( [b]*1000 )
    tmp_object = scaling.likelihood_ratio_outlier_test(
      fobs,
      sigmas,
      fcalc,
      epsilon,
      centric,
      alpha,
      beta )
    mean = tmp_object.mean_fobs()
    std  = tmp_object.std_fobs()
    mode = tmp_object.posterior_mode()
    sdmo = tmp_object.posterior_mode_snd_der()
    for a,mm,m,v in zip(alpha,mean,mode,sdmo):
      assert  (-1.0/v>0)
      if (a>0.9):
        assert approx_equal(mm,m,eps=1e-1)

  for ii in range(1,50):
    a = ii/50.0
    b = 1.0-a*a
    alpha = flex.double( [a]*1000 )
    beta  = flex.double( [b]*1000 )
    tmp_object = scaling.likelihood_ratio_outlier_test(
      fobs,
      sigmas,
      fcalc,
      epsilon,
      ~centric,
      alpha,
      beta )
    mean = tmp_object.mean_fobs()
    std  = tmp_object.std_fobs()
    mode = tmp_object.posterior_mode()
    sdmo = tmp_object.posterior_mode_snd_der()
    for a,b,fc,mm,m,v in zip(alpha,beta,fcalc,mean,mode,sdmo):
      assert  (-1.0/v>0)
      if (a>0.9):
        if (fc>1.0):
          assert approx_equal(mm,m,eps=1e-1)




def tst_outliers_find_posterior_mode():
  # first check if we can find the posterior mode for the acentric
  # when alpha is close to 1, this should be very close to fcalc
  fobs  =  flex.double( [3]*10 )
  fcalc =  flex.double( range(10) )*10 + 10
  sigmas = flex.double( [0]*10 )
  epsilon = flex.double( [1]*10 )
  centric = flex.bool( [False]*10 )
  beta = flex.double( [1]*10 )
  alpha = flex.double( [0.99]*10 )
  tmp_object = scaling.likelihood_ratio_outlier_test(
     fobs,
     sigmas,
     fcalc,
     epsilon,
     centric,
     alpha,
     beta)
  posterior_mode = tmp_object.posterior_mode()
  for f, m in zip(fcalc,posterior_mode):
    assert approx_equal(f/m, 1, eps=0.05)

  # have a look at centrics
  fobs  =  flex.double( [3]*10 )
  fcalc =  flex.double( range(10) )*100 + 100
  sigmas = flex.double( [0]*10 )
  epsilon = flex.double( [1]*10 )
  centric = flex.bool( [True]*10 )
  beta = flex.double( [1]*10 )
  alpha = flex.double( [0.099]*10 )
  tmp_object = scaling.likelihood_ratio_outlier_test(
     fobs,
     sigmas,
     fcalc,
     epsilon,
     centric,
     alpha,
     beta)
  posterior_mode = tmp_object.posterior_mode()
  for f, m in zip(fcalc,posterior_mode):
    assert approx_equal(m/f, 0.099, eps=0.001)

def tst_loglikelihoods():
  fobs  =  flex.double( range(1000) )/200
  fcalc =  flex.double( [1]*1000 )
  sigmas = flex.double( [0]*1000 )
  epsilon = flex.double( [1]*1000 )
  centric = flex.bool( [False]*1000 )
  beta = flex.double( [1]*1000 )
  alpha = flex.double( [0.99]*1000 )
  tmp_object = scaling.likelihood_ratio_outlier_test(
     fobs,
     sigmas,
     fcalc,
     epsilon,
     centric,
     alpha,
     beta)
  cur_lik = tmp_object.log_likelihood()
  pm_lik  = tmp_object.posterior_mode_log_likelihood()
  mode    = tmp_object.posterior_mode()
  level   = 4.5
  flags   = tmp_object.flag_potential_outliers( 2.0*level )
  for fl,pl,l,m,fo in zip(flags,pm_lik,cur_lik,mode,fobs):
    if pl-l < level*2.0:
      assert fl
    else:
      assert not fl

def run():
  tst_outliers_compare_mode_mean()
  tst_outliers_find_posterior_mode()
  tst_loglikelihoods()
  print("OK")

if (__name__ == "__main__"):
  run()


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/tst_plan_sad_experiment.py

from __future__ import absolute_import, division, print_function
from mmtbx.command_line import plan_sad_experiment
from libtbx.test_utils import approx_equal, Exception_expected
from libtbx.utils import null_out, Sorry

def exercise():
  # Generic SeMet protein (actually Rv0577)
  args = [
    "resolution=2.2",
    "atom_type=Se",
    "residues=300",
    "wavelength=0.9794",
    "include_weak_anomalous_scattering=False",
    "sites=12",
  ]
  result = plan_sad_experiment.run(args=args, out=null_out()).show(null_out())
  #print result.representative_values[:-1]
  assert approx_equal(result.representative_values[:-1],
   [2.5, 12, 10880.374304954881, 3.8438000679016113, 97.77777777777779, 0.009, 0.9467311684652722, 0.9467311684652722, 0.7396676207890701, 0.7396676207890701, 20.9016001033, 98.3151904665], eps=0.01)
  assert (95 < result.representative_values[-2] < 100)
  # Insulin S-SAD
  with open("tst_plan_sad_experiment.fa", "w") as f:
    f.write("""
>1ZNI:A|PDBID|CHAIN|SEQUENCE
GIVEQCCTSICSLYQLENYCN
>1ZNI:B|PDBID|CHAIN|SEQUENCE
FVNQHLCGSHLVEALYLVCGERGFFYTPKA
>1ZNI:C|PDBID|CHAIN|SEQUENCE
GIVEQCCTSICSLYQLENYCN
>1ZNI:D|PDBID|CHAIN|SEQUENCE
FVNQHLCGSHLVEALYLVCGERGFFYTPKA
""")
  args = [
    "seq_file=tst_plan_sad_experiment.fa",
    "atom_type=S",
    "resolution=1.2",
    "wavelength=1.54"
  ]
  result = plan_sad_experiment.run(args=args, out=null_out())
  #print result.representative_values[:-1]
  assert approx_equal(result.representative_values[:-1],
  [1.5, 12, 17126.515109651198, 0.5562999844551086, 97.77777777777779, 0.009, 0.5095890245676317, 0.5095890245676317, 0.616249995680551, 0.616249995680551, 17.36855053251538, 89.82473404335383], eps=0.01)
#  [2, 12, 7225.2485618841, 0.5562999844551086, 97.77777777777779, 0.009, 0.5095890245676317, 0.5095890245676317, 0.616249995680551, 0.616249995680551, 13.5861823956, 82.6264245311 ], eps=0.01)
  assert (86 < result.representative_values[-2] < 92)
  # now with worse resolution
  args = [
    "seq_file=tst_plan_sad_experiment.fa",
    "atom_type=S",
    "resolution=3.0",
    "wavelength=1.54"
  ]
  result = plan_sad_experiment.run(args=args, out=null_out())
  #print result.representative_values[:-1]
  assert approx_equal(result.representative_values[:-1],
  [5, 12, 462.41590796058244, 0.5562999844551086, 97.77777777777779, 0.009, 0.5095890245676317, 0.5095890245676317, 0.616249995680551, 0.616249995680551, 3.55312435872, 16.8013009611], eps=0.01)

  # Error handling
  args = [
    "resolution=2.2",
    "atom_type=Se",
    "wavelength=0.9794",
    "sites=12",
  ]
  try :
    result = plan_sad_experiment.run(args=args, out=null_out())
  except Sorry :
    pass
  else :
    raise Exception_expected

if (__name__ == "__main__"):
  exercise()
  print("OK")


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/tst_scaling.py
from __future__ import absolute_import, division, print_function
## Peter Zwart July 5, 2005
from libtbx.test_utils import raises

from cctbx.array_family import flex
from cctbx import crystal
from cctbx import miller
from cctbx import xray
from cctbx import sgtbx
from cctbx import uctbx
from mmtbx import scaling
from libtbx.test_utils import approx_equal
from mmtbx.scaling import absolute_scaling
from mmtbx.scaling import twin_analyses as t_a
import mmtbx.scaling

from scitbx.python_utils import random_transform
import random
import math
import time
from libtbx.str_utils import StringIO
from libtbx.utils import format_cpu_times
from six.moves import zip
from six.moves import range

random.seed(0)
flex.set_random_seed(0)

import scitbx.math as sm


##testing quick erf and quick eio
def test_luts():
  qerf = mmtbx.scaling.very_quick_erf(0.001)
  qeio = mmtbx.scaling.quick_ei0(5000)
  for i in range(-1000,1000):
    x=i/100.0
    assert approx_equal( qerf.erf(x), sm.erf(x), eps=1e-5 )
    if (x>=0):
      assert approx_equal( qeio.ei0(x), math.exp(-x)*sm.bessel_i0(x) , eps=1e-5 )
  number_of_iterations = 15000000
  for optimized in [False, True]:
    t0 = time.time()
    zero = qerf.loop_for_timings(number_of_iterations, optimized=optimized)
    print("very_quick_erf*%d optimized=%s: %.2f s" % (
      number_of_iterations, str(optimized), time.time()-t0))
    assert approx_equal(zero, 0)
  number_of_iterations = 5000000
  for optimized in [False, True]:
    t0 = time.time()
    zero = qeio.loop_for_timings(number_of_iterations, optimized=optimized)
    print("quick_ei0*%d optimized=%s: %.2f s" % (
      number_of_iterations, str(optimized), time.time()-t0))
    assert approx_equal(zero, 0)

## Testing Wilson parameters
def test_gamma_prot():
  gamma_prot_test = scaling.gamma_prot(0.011478)
  assert approx_equal(gamma_prot_test,-0.349085)

  gamma_prot_test = scaling.gamma_prot(0.028868)
  assert approx_equal(gamma_prot_test,-0.585563)

  d_star_sq = flex.double([0.011478,0.028868,1.0,0.0])
  gamma_array_test = scaling.get_gamma_prot(d_star_sq)
  assert approx_equal(gamma_array_test[0],-0.349085)
  assert approx_equal(gamma_array_test[1],-0.585563)
  assert approx_equal(gamma_array_test[2], 0.0)
  assert approx_equal(gamma_array_test[3], 0.0)

def test_sigma_prot():
  z_0 = scaling.sigma_prot_sq(0.0,1.0)
  z_0_theory = + 8.0*1.0*1.0 \
               + 5.0*6.0*6.0 \
               + 1.5*7.0*7.0 \
               + 1.2*8.0*8.0
  assert approx_equal(z_0,z_0_theory,eps=1e-0)

  d_star_sq = flex.double([0.0])
  z_0_array = scaling.get_sigma_prot_sq(d_star_sq,1.0)
  assert approx_equal(z_0_array[0],z_0)


## Testing isotropic wilson scaling
def finite_diffs_iso(p_scale=0.0,p_B_wilson=0.0,centric=False,h=0.0001):

  d_star_sq = flex.double(10,0.25)
  f_obs =  flex.double(10,1.0)
  centric_array = flex.bool(10,centric)
  sigma_f_obs = f_obs/10.0
  sigma_sq = flex.double(10,1.0)
  epsilon = flex.double(10,1.0)
  gamma =flex.double(10,0.0)

  stmp1 = scaling.wilson_total_nll(d_star_sq = d_star_sq,
                                    f_obs = f_obs,
                                    sigma_f_obs = sigma_f_obs,
                                    epsilon = epsilon,
                                    sigma_sq = sigma_sq,
                                    gamma_prot = gamma,
                                    centric = centric_array,
                                    p_scale = p_scale-h,
                                    p_B_wilson = p_B_wilson )

  stmp2 = scaling.wilson_total_nll(d_star_sq = d_star_sq,
                                    f_obs = f_obs,
                                    sigma_f_obs = sigma_f_obs,
                                    epsilon = epsilon,
                                    sigma_sq = sigma_sq,
                                    gamma_prot = gamma,
                                    centric = centric_array,
                                    p_scale = p_scale+h,
                                    p_B_wilson = p_B_wilson)

  s_grad_diff = (stmp1-stmp2)/(-2.0*h)

  btmp1 = scaling.wilson_total_nll(d_star_sq = d_star_sq,
                                    f_obs = f_obs,
                                    sigma_f_obs = sigma_f_obs,
                                    epsilon = epsilon,
                                    sigma_sq = sigma_sq,
                                    gamma_prot = gamma,
                                    centric = centric_array,
                                    p_scale = p_scale,
                                    p_B_wilson = p_B_wilson-h)

  btmp2 = scaling.wilson_total_nll(d_star_sq = d_star_sq,
                                    f_obs = f_obs,
                                    sigma_f_obs = sigma_f_obs,
                                    epsilon = epsilon,
                                    sigma_sq = sigma_sq,
                                    gamma_prot = gamma,
                                    centric = centric_array,
                                    p_scale = p_scale,
                                    p_B_wilson = p_B_wilson+h)

  b_grad_diff = (btmp1-btmp2)/(-2.0*h)

  grad  = scaling.wilson_total_nll_gradient(d_star_sq = d_star_sq,
                                             f_obs = f_obs,
                                             sigma_f_obs = sigma_f_obs,
                                             epsilon = epsilon,
                                             sigma_sq = sigma_sq,
                                             gamma_prot = gamma,
                                             centric = centric_array,
                                             p_scale = p_scale,
                                             p_B_wilson = p_B_wilson)
  assert approx_equal(s_grad_diff, grad[0])
  assert approx_equal(b_grad_diff, grad[1])


def test_likelihood_iso():
  d_star_sq = flex.double(10,0.250)
  f_obs = flex.double(10,1.0)
  sigma_f_obs = flex.double(10,0.0000)
  sigma_sq = flex.double(10,1.0)
  epsilon = flex.double(10,1.0)
  gamma = flex.double(10,0.0)
  centric = flex.bool(10,True)
  acentric = flex.bool(10,False)
  p_scale = 0.0
  p_B_wilson = 0.0



  centric_single_trans = scaling.wilson_single_nll(
    d_star_sq = d_star_sq[0],
    f_obs = f_obs[0],
    sigma_f_obs = sigma_f_obs[0],
    epsilon = epsilon[0],
    sigma_sq = sigma_sq[0],
    gamma_prot = gamma[0],
    centric = centric[0],
    p_scale = p_scale,
    p_B_wilson = p_B_wilson,
    transform = True)

  centric_single_no_trans = scaling.wilson_single_nll(
    d_star_sq = d_star_sq[0],
    f_obs = f_obs[0],
    sigma_f_obs = sigma_f_obs[0],
    epsilon = epsilon[0],
    sigma_sq = sigma_sq[0],
    gamma_prot = gamma[0],
    centric = centric[0],
    p_scale = 1.0,
    p_B_wilson = p_B_wilson,
    transform = False)

  assert approx_equal(centric_single_trans,  1.072364 ) ## from Mathematica
  assert approx_equal(centric_single_trans, centric_single_no_trans)

  acentric_single_trans = scaling.wilson_single_nll(
    d_star_sq = d_star_sq[0],
    f_obs = f_obs[0],
    sigma_f_obs = sigma_f_obs[0],
    epsilon = epsilon[0],
    sigma_sq = sigma_sq[0],
    gamma_prot = gamma[0],
    centric = acentric[0],
    p_scale = p_scale,
    p_B_wilson = p_B_wilson)

  acentric_single_no_trans  = scaling.wilson_single_nll(
    d_star_sq = d_star_sq[0],
    f_obs = f_obs[0],
    sigma_f_obs = sigma_f_obs[0],
    epsilon = epsilon[0],
    sigma_sq = sigma_sq[0],
    gamma_prot = gamma[0],
    centric = acentric[0],
    p_scale = 1.0,
    p_B_wilson =p_B_wilson,
    transform = False)

  assert approx_equal(acentric_single_trans, 0.306853) ## from Mathematica
  assert approx_equal(acentric_single_trans, acentric_single_no_trans)


  centric_total = scaling.wilson_total_nll(
    d_star_sq = d_star_sq,
    f_obs = f_obs,
    sigma_f_obs = sigma_f_obs,
    epsilon = epsilon,
    sigma_sq = sigma_sq,
    gamma_prot = gamma,
    centric = centric,
    p_scale = p_scale,
    p_B_wilson = p_B_wilson)

  acentric_total = scaling.wilson_total_nll(
    d_star_sq = d_star_sq,
    f_obs = f_obs,
    sigma_f_obs = sigma_f_obs,
    epsilon = epsilon,
    sigma_sq = sigma_sq,
    gamma_prot = gamma,
    centric = acentric,
    p_scale = p_scale,
    p_B_wilson = p_B_wilson)

  assert approx_equal(centric_total, centric_single_trans*10.0)
  assert approx_equal(acentric_total, acentric_single_trans*10.0)



def test_gradients_iso():
  ## Centrics
  finite_diffs_iso(p_scale=3.0,
               p_B_wilson=10.0,
               centric=True,h=0.000001)
  finite_diffs_iso(p_scale=-3.0,
               p_B_wilson=-10.0,
               centric=True,h=0.000001)
  finite_diffs_iso(p_scale=90.0,
               p_B_wilson=-10.0,
               centric=True,h=0.000001)
  finite_diffs_iso(p_scale=-90.0,
               p_B_wilson=10.0,
               centric=True,h=0.000001)
   ## Acentrics
  finite_diffs_iso(p_scale=3.0,
               p_B_wilson=10.0,
               centric=False,h=0.000001)
  finite_diffs_iso(p_scale=-3.0,
               p_B_wilson=-10.0,
               centric=False,h=0.000001)
  finite_diffs_iso(p_scale=90.0,
               p_B_wilson=-10.0,
               centric=True,h=0.000001)
  finite_diffs_iso(p_scale=-90.0,
               p_B_wilson=10.0,
               centric=True,h=0.000001)




## Testing anisotropic wilson scaling
def finite_diffs_aniso(p_scale,
                       u_star,
                       centric=False,
                       h=0.0001):
  d_star_sq = flex.double(2,0.25)
  f_obs =  flex.double(2,1.0)
  centric_array = flex.bool(2,centric)
  sigma_f_obs = f_obs/10.0
  sigma_sq = flex.double(2,1.0)
  epsilon = flex.double(2,1.0)
  gamma =flex.double(2,0.0)
  unit_cell = uctbx.unit_cell('20, 30, 40, 90.0, 90.0, 90.0')
  mi = flex.miller_index(((1,2,3), (1,2,3)))
  xs = crystal.symmetry((20,30,40), "P 2 2 2")
  ms = miller.set(xs, mi)

  nll_norm = scaling.wilson_single_nll_aniso(ms.indices()[0],
                                             f_obs[0],
                                             sigma_f_obs[0],
                                             epsilon[0],
                                             sigma_sq[0],
                                             gamma[0],
                                             centric_array[0],
                                             p_scale,
                                             unit_cell,
                                             u_star)

  nll_scale = scaling.wilson_single_nll_aniso(ms.indices()[0],
                                             f_obs[0],
                                             sigma_f_obs[0],
                                             epsilon[0],
                                             sigma_sq[0],
                                             gamma[0],
                                             centric_array[0],
                                             p_scale+h,
                                             unit_cell,
                                             u_star)
  u_star[0]+=h
  nll_u11 = scaling.wilson_single_nll_aniso(ms.indices()[0],
                                             f_obs[0],
                                             sigma_f_obs[0],
                                             epsilon[0],
                                             sigma_sq[0],
                                             gamma[0],
                                             centric_array[0],
                                             p_scale,
                                             unit_cell,
                                             u_star)
  u_star[0]-=h
  u_star[1]+=h
  nll_u22 = scaling.wilson_single_nll_aniso(ms.indices()[0],
                                             f_obs[0],
                                             sigma_f_obs[0],
                                             epsilon[0],
                                             sigma_sq[0],
                                             gamma[0],
                                             centric_array[0],
                                             p_scale,
                                             unit_cell,
                                             u_star)
  u_star[1]-=h
  u_star[2]+=h
  nll_u33 = scaling.wilson_single_nll_aniso(ms.indices()[0],
                                             f_obs[0],
                                             sigma_f_obs[0],
                                             epsilon[0],
                                             sigma_sq[0],
                                             gamma[0],
                                             centric_array[0],
                                             p_scale,
                                             unit_cell,
                                             u_star)
  u_star[2]-=h
  u_star[3]+=h
  nll_u12 = scaling.wilson_single_nll_aniso(ms.indices()[0],
                                             f_obs[0],
                                             sigma_f_obs[0],
                                             epsilon[0],
                                             sigma_sq[0],
                                             gamma[0],
                                             centric_array[0],
                                             p_scale,
                                             unit_cell,
                                             u_star)
  u_star[3]-=h
  u_star[4]+=h
  nll_u13 = scaling.wilson_single_nll_aniso(ms.indices()[0],
                                             f_obs[0],
                                             sigma_f_obs[0],
                                             epsilon[0],
                                             sigma_sq[0],
                                             gamma[0],
                                             centric_array[0],
                                             p_scale,
                                             unit_cell,
                                             u_star)
  u_star[4]-=h
  u_star[5]+=h
  nll_u23 = scaling.wilson_single_nll_aniso(ms.indices()[0],
                                            f_obs[0],
                                            sigma_f_obs[0],
                                            epsilon[0],
                                            sigma_sq[0],
                                            gamma[0],
                                            centric_array[0],
                                            p_scale,
                                            unit_cell,
                                            u_star)


  g = scaling.wilson_single_nll_aniso_gradient(ms.indices()[0],
                                               f_obs[0],
                                               sigma_f_obs[0],
                                               epsilon[0],
                                               sigma_sq[0],
                                               gamma[0],
                                               centric_array[0],
                                               p_scale,
                                               unit_cell,
                                               u_star)

  g2 = scaling.wilson_total_nll_aniso_gradient(ms.indices(),
                                               f_obs,
                                               sigma_f_obs,
                                               epsilon,
                                               sigma_sq,
                                               gamma,
                                               centric_array,
                                               p_scale,
                                               unit_cell,
                                               u_star)
  ds=(nll_norm-nll_scale)/-h
  du11=(nll_norm-nll_u11)/-h
  du22=(nll_norm-nll_u22)/-h
  du33=(nll_norm-nll_u33)/-h
  du12=(nll_norm-nll_u12)/-h
  du13=(nll_norm-nll_u13)/-h
  du23=(nll_norm-nll_u23)/-h
  assert approx_equal(ds,g[0]), (ds,g[0])
  assert approx_equal(du11,g[1]), (du11,g[1])
  assert approx_equal(du22,g[2])
  assert approx_equal(du33,g[3])
  assert approx_equal(du12,g[4])
  assert approx_equal(du13,g[5])
  assert approx_equal(du23,g[6])

  assert approx_equal(ds,g2[0]/2.0)
  assert approx_equal(du11,g2[1]/2.0)
  assert approx_equal(du22,g2[2]/2.0)
  assert approx_equal(du33,g2[3]/2.0)
  assert approx_equal(du12,g2[4]/2.0)
  assert approx_equal(du13,g2[5]/2.0)
  assert approx_equal(du23,g2[6]/2.0)


def test_likelihood_aniso():
  u_star = [0,0,0,0,0,0]
  d_star_sq = flex.double(2,0.25)
  f_obs =  flex.double(2,1.0)
  centric_array = flex.bool(2,True)
  sigma_f_obs = f_obs/10.0
  sigma_sq = flex.double(2,1.0)
  epsilon = flex.double(2,1.0)
  gamma =flex.double(2,0.0)
  unit_cell = uctbx.unit_cell('20, 30, 40, 90.0, 90.0, 90.0')
  mi = flex.miller_index(((1,2,3), (1,2,3)))
  xs = crystal.symmetry((20,30,40), "P 2 2 2")
  ms = miller.set(xs, mi)
  nll_centric_aniso = scaling.wilson_single_nll_aniso(ms.indices()[0],
                                                      f_obs[0],
                                                      sigma_f_obs[0],
                                                      epsilon[0],
                                                      sigma_sq[0],
                                                      gamma[0],
                                                      centric_array[0],
                                                      0.0,
                                                      unit_cell,
                                                      u_star)


  assert approx_equal(nll_centric_aniso,  1.07239 ) ## from Mathematica
  nll_acentric_aniso = scaling.wilson_single_nll_aniso(ms.indices()[0],
                                                       f_obs[0],
                                                       sigma_f_obs[0],
                                                       epsilon[0],
                                                       sigma_sq[0],
                                                       gamma[0],
                                                       centric_array[0],
                                                       0.0,
                                                       unit_cell,
                                                       u_star)
  centric_array = flex.bool(2,False)
  nll_acentric_aniso = scaling.wilson_single_nll_aniso(ms.indices()[0],
                                                       f_obs[0],
                                                       sigma_f_obs[0],
                                                       epsilon[0],
                                                       sigma_sq[0],
                                                       gamma[0],
                                                       centric_array[0],
                                                       0.0,
                                                       unit_cell,
                                                       u_star)
  assert approx_equal(nll_acentric_aniso,0.306902 ) ## from Mathematica

  centric_array = flex.bool(2,True)
  u_star = [1,1,1,0,0,0]
  nll_centric_aniso = scaling.wilson_single_nll_aniso(ms.indices()[0],
                                                      f_obs[0],
                                                      sigma_f_obs[0],
                                                      epsilon[0],
                                                      sigma_sq[0],
                                                      gamma[0],
                                                      centric_array[0],
                                                      0.0,
                                                      unit_cell,
                                                      u_star)
  assert approx_equal(nll_centric_aniso,  1.535008 ) ## from Mathematica
  centric_array = flex.bool(2,False)
  nll_acentric_aniso = scaling.wilson_single_nll_aniso(ms.indices()[0],
                                                      f_obs[0],
                                                      sigma_f_obs[0],
                                                      epsilon[0],
                                                      sigma_sq[0],
                                                      gamma[0],
                                                      centric_array[0],
                                                      0.0,
                                                      unit_cell,
                                                      u_star)
  assert approx_equal(nll_acentric_aniso,  0.900003 ) ## from Mathematica

  centric_array[1]=True
  nll_total_aniso = scaling.wilson_total_nll_aniso(ms.indices(),
                                                   f_obs,
                                                   sigma_f_obs,
                                                   epsilon,
                                                   sigma_sq,
                                                   gamma,
                                                   centric_array,
                                                   0.0,
                                                   unit_cell,
                                                   u_star)
  assert approx_equal(nll_total_aniso,  2.435011)


def test_grads_aniso():
  finite_diffs_aniso(0.0,[0.0,0.0,0.0,0.0,0.0,0.0],True, 0.0000001)
  finite_diffs_aniso(0.0,[0.0,0.0,0.0,2.0,0.0,0.0],False, 0.0000001)
  finite_diffs_aniso(0.0,[1.0,2.0,3.0,4.0,5.0,6.0],True, 0.0000001)
  finite_diffs_aniso(0.0,[1.0,2.0,3.0,4.0,5.0,6.0],False, 0.0000001)
  finite_diffs_aniso(-10.0,[1.0,2.0,3.0,4.0,5.0,6.0],True, 0.0000001)
  finite_diffs_aniso(-10.0,[1.0,2.0,3.0,4.0,5.0,6.0],False, 0.0000001)
  finite_diffs_aniso(10.0,[1.0,2.0,3.0,4.0,5.0,6.0],True, 0.0000001)
  finite_diffs_aniso(10.0,[1.0,2.0,3.0,4.0,5.0,6.0],False, 0.0000001)
  finite_diffs_aniso(10.0,[10.0,20.0,30.0,40.0,50.0,60.0],True, 0.0000001)
  finite_diffs_aniso(10.0,[10.0,20.0,30.0,40.0,50.0,60.0],False, 0.0000001)


## Testing relative scaling summats

class scaling_tester(object):
  def __init__(self):
    self.data_obs1 = flex.double(2,1.0)
    self.data_obs2 = flex.double(2,3.0)
    self.sigma_obs1 = flex.double(2,0.1)
    self.sigma_obs2 = flex.double(2,1)
    self.unit_cell = uctbx.unit_cell('20, 30, 40, 90.0, 90.0, 90.0')
    #mi = flex.miller_index(((1,2,3), (1,2,3)))
    self.mi = flex.miller_index(((1,2,3), (5,6,7)))
    self.xs = crystal.symmetry((20,30,40), "P 2 2 2")
    self.ms = miller.set(self.xs, self.mi)
    self.u = [1,2,3,4,5,6]
    self.p_scale = 0.40
    #self.u = [0,0,0,0,0,0]
    #self.p_scale = 0.00

    self.ls_i_wt = scaling.least_squares_on_i_wt(
      self.mi,
      self.data_obs1,
      self.sigma_obs1,
      self.data_obs2,
      self.sigma_obs2,
      self.p_scale,
      self.unit_cell,
      self.u)
    self.ls_i = scaling.least_squares_on_i(
      self.mi,
      self.data_obs1,
      self.sigma_obs1,
      self.data_obs2,
      self.sigma_obs2,
      self.p_scale,
      self.unit_cell,
      self.u)
    self.ls_f_wt = scaling.least_squares_on_f_wt(
      self.mi,
      self.data_obs1,
      self.sigma_obs1,
      self.data_obs2,
      self.sigma_obs2,
      self.p_scale,
      self.unit_cell,
      self.u)
    self.ls_f = scaling.least_squares_on_f(
      self.mi,
      self.data_obs1,
      self.sigma_obs1,
      self.data_obs2,
      self.sigma_obs2,
      self.p_scale,
      self.unit_cell,
      self.u)

    self.tst_ls_f_wt()
    self.tst_ls_i_wt()
    self.tst_ls_f()
    self.tst_ls_i()
    self.tst_hes_ls_i_wt()
    self.tst_hes_ls_f_wt()
    self.tst_hes_ls_i()
    self.tst_hes_ls_f()

  def tst_ls_i_wt(self, h=0.0000001):
    ## This function tests the gradients
    tmp = self.ls_i_wt.get_function()
    before = flex.double(7,tmp)
    after = flex.double(7,0)
    ## Test the pscale
    self.ls_i_wt.set_p_scale(self.p_scale+h)
    tmp = self.ls_i_wt.get_function()
    after[0]=tmp
    self.ls_i_wt.set_p_scale(self.p_scale)
    for ii in range(6):
      u_tmp=list(flex.double(self.u).deep_copy())
      u_tmp[ii]+=h
      self.ls_i_wt.set_u_rwgk(u_tmp)
      tmp = self.ls_i_wt.get_function()
      after[ii+1]=tmp
      self.ls_i_wt.set_u_rwgk(self.u)
      grads=self.ls_i_wt.get_gradient()
    f = max(1, flex.max(flex.abs(grads)))
    assert approx_equal(grads/f, (after-before)/h/f)

  def tst_ls_f_wt(self, h=0.0000001):
    ## This function tests the gradients
    tmp = self.ls_f_wt.get_function()
    before = flex.double(7,tmp)
    after = flex.double(7,0)
    ## Test the pscale
    self.ls_f_wt.set_p_scale(self.p_scale+h)
    tmp = self.ls_f_wt.get_function()
    after[0]=tmp
    self.ls_f_wt.set_p_scale(self.p_scale)
    for ii in range(6):
      u_tmp=list(flex.double(self.u).deep_copy())
      u_tmp[ii]+=h
      self.ls_f_wt.set_u_rwgk(u_tmp)
      tmp = self.ls_f_wt.get_function()
      after[ii+1]=tmp
      self.ls_f_wt.set_u_rwgk(self.u)
    grads=self.ls_f_wt.get_gradient()
    f = max(1, flex.max(flex.abs(grads)))
    assert approx_equal(grads/f, (after-before)/h/f)

  def tst_ls_f(self, h=0.0000001):
    ## This function tests the gradients
    tmp = self.ls_f.get_function()
    before = flex.double(7,tmp)
    after = flex.double(7,0)
    ## Test the pscale
    self.ls_f.set_p_scale(self.p_scale+h)
    tmp = self.ls_f.get_function()
    after[0]=tmp
    self.ls_f.set_p_scale(self.p_scale)
    for ii in range(6):
      u_tmp=list(flex.double(self.u).deep_copy())
      u_tmp[ii]+=h
      self.ls_f.set_u_rwgk(u_tmp)
      tmp = self.ls_f.get_function()
      after[ii+1]=tmp
      self.ls_f.set_u_rwgk(self.u)
    grads=self.ls_f.get_gradient()
    f = max(1, flex.max(flex.abs(grads)))
    assert approx_equal(grads/f, (after-before)/h/f)

  def tst_ls_i(self, h=0.0000001):
    ## This function tests the gradients
    tmp = self.ls_i.get_function()
    before = flex.double(7,tmp)
    after = flex.double(7,0)
    ## Test the pscale
    self.ls_i.set_p_scale(self.p_scale+h)
    tmp = self.ls_i.get_function()
    after[0]=tmp
    self.ls_i.set_p_scale(self.p_scale)
    #aniso tensor components
    for ii in range(6):
      u_tmp=list(flex.double(self.u).deep_copy())
      u_tmp[ii]+=h
      self.ls_i.set_u_rwgk(u_tmp)
      tmp = self.ls_i.get_function()
      after[ii+1]=tmp
      self.ls_i.set_u_rwgk(self.u)
    grads=self.ls_i.get_gradient()
    f = max(1, flex.max(flex.abs(grads)))
    assert approx_equal(grads/f, (after-before)/h/f)

  def tst_hes_ls_i_wt(self,h=0.0000001):

    hes_anal = self.ls_i_wt.hessian_as_packed_u()
    hes_anal=hes_anal.matrix_packed_u_as_symmetric()

    grads = self.ls_i_wt.get_gradient()

    self.ls_i_wt.set_p_scale(self.p_scale+h)
    tmp = self.ls_i_wt.get_gradient()
    tmp = list( (grads-tmp)/-h )
    tmp_hess=[]
    tmp_hess.append( tmp )
    self.ls_i_wt.set_p_scale(self.p_scale)

    for ii in range(6):
      u_tmp=list(flex.double(self.u).deep_copy())
      u_tmp[ii]+=h
      self.ls_i_wt.set_u_rwgk(u_tmp)
      tmp = self.ls_i_wt.get_gradient()
      tmp = (grads - tmp)/-h
      tmp_hess.append( list(tmp)  )
      self.ls_i_wt.set_u_rwgk(self.u)

    f = max(1, flex.max(flex.abs(hes_anal)))
    count=0
    for ii in range(7):
      for jj in range(7):
        assert approx_equal(tmp_hess[ii][jj]/f, hes_anal[count]/f)
        count+=1

  def tst_hes_ls_f_wt(self,h=0.0000001):

    hes_anal = self.ls_f_wt.hessian_as_packed_u()
    hes_anal=hes_anal.matrix_packed_u_as_symmetric()

    grads = self.ls_f_wt.get_gradient()

    self.ls_f_wt.set_p_scale(self.p_scale+h)
    tmp = self.ls_f_wt.get_gradient()
    tmp = list( (grads-tmp)/-h )
    tmp_hess=[]
    tmp_hess.append( tmp )
    self.ls_f_wt.set_p_scale(self.p_scale)

    for ii in range(6):
      u_tmp=list(flex.double(self.u).deep_copy())
      u_tmp[ii]+=h
      self.ls_f_wt.set_u_rwgk(u_tmp)
      tmp = self.ls_f_wt.get_gradient()
      tmp = (grads - tmp)/-h
      tmp_hess.append( list(tmp)  )
      self.ls_f_wt.set_u_rwgk(self.u)

    f = max(1, flex.max(flex.abs(hes_anal)))
    count=0
    for ii in range(7):
      for jj in range(7):
        assert approx_equal(tmp_hess[ii][jj]/f, hes_anal[count]/f)
        count+=1

  def tst_hes_ls_i(self,h=0.0000001):

    hes_anal = self.ls_i.hessian_as_packed_u()
    hes_anal=hes_anal.matrix_packed_u_as_symmetric()

    grads = self.ls_i.get_gradient()

    self.ls_i.set_p_scale(self.p_scale+h)
    tmp = self.ls_i.get_gradient()
    tmp = list( (grads-tmp)/-h )
    tmp_hess=[]
    tmp_hess.append( tmp )
    self.ls_i.set_p_scale(self.p_scale)

    for ii in range(6):
      u_tmp=list(flex.double(self.u).deep_copy())
      u_tmp[ii]+=h
      self.ls_i.set_u_rwgk(u_tmp)
      tmp = self.ls_i.get_gradient()
      tmp = (grads - tmp)/-h
      tmp_hess.append( list(tmp)  )
      self.ls_i.set_u_rwgk(self.u)

    count=0
    for ii in range(7):
      for jj in range(7):
        assert approx_equal(tmp_hess[ii][jj]/hes_anal[count], 1 , eps=1e-5)
        count+=1

  def tst_hes_ls_f(self,h=0.0000001):

    hes_anal = self.ls_f.hessian_as_packed_u()
    hes_anal=hes_anal.matrix_packed_u_as_symmetric()

    grads = self.ls_f.get_gradient()

    self.ls_f.set_p_scale(self.p_scale+h)
    tmp = self.ls_f.get_gradient()
    tmp = list( (grads-tmp)/-h )
    tmp_hess=[]
    tmp_hess.append( tmp )
    self.ls_f.set_p_scale(self.p_scale)

    for ii in range(6):
      u_tmp=list(flex.double(self.u).deep_copy())
      u_tmp[ii]+=h
      self.ls_f.set_u_rwgk(u_tmp)
      tmp = self.ls_f.get_gradient()
      tmp = (grads - tmp)/-h
      tmp_hess.append( list(tmp)  )
      self.ls_f.set_u_rwgk(self.u)

    count=0
    for ii in range(7):
      for jj in range(7):
        assert approx_equal(tmp_hess[ii][jj]/hes_anal[count], 1 , eps=1e-5)
        count+=1




def random_data(B_add=35,
                n_residues=585.0,
                d_min=3.5):
  unit_cell = uctbx.unit_cell( (81.0,  81.0,  61.0,  90.0,  90.0, 120.0) )
  xtal = crystal.symmetry(unit_cell, " P 3 ")
  ## In P3 I do not have to worry about centrics or reflections with different
  ## epsilons.
  miller_set = miller.build_set(
    crystal_symmetry = xtal,
    anomalous_flag = False,
    d_min = d_min)
  ## Now make an array with d_star_sq values
  d_star_sq = miller_set.d_spacings().data()
  d_star_sq = 1.0/(d_star_sq*d_star_sq)
  asu = {"H":8.0*n_residues*1.0,
         "C":5.0*n_residues*1.0,
         "N":1.5*n_residues*1.0,
         "O":1.2*n_residues*1.0}
  scat_info = absolute_scaling.scattering_information(
    asu_contents = asu,
    fraction_protein=1.0,
    fraction_nucleic=0.0)
  scat_info.scat_data(d_star_sq)
  gamma_prot = scat_info.gamma_tot
  sigma_prot = scat_info.sigma_tot_sq
  ## The number of residues is multriplied by the Z of the spacegroup
  protein_total = sigma_prot * (1.0+gamma_prot)
  ## add a B-value of 35 please
  protein_total = protein_total*flex.exp(-B_add*d_star_sq/2.0)
  ## Now that has been done,
  ## We can make random structure factors
  normalised_random_intensities = \
     random_transform.wilson_intensity_variate(protein_total.size())
  random_intensities = normalised_random_intensities*protein_total*math.exp(6)
  std_dev = random_intensities*5.0/100.0
  noise = random_transform.normal_variate(N=protein_total.size())
  noise = noise*std_dev
  random_intensities=noise+random_intensities
  ## STuff the arrays in the miller array
  miller_array = miller.array(miller_set,
                              data=random_intensities,
                              sigmas=std_dev)
  miller_array=miller_array.set_observation_type(
    xray.observation_types.intensity())
  miller_array = miller_array.f_sq_as_f()
  return (miller_array)


def test_scaling_on_random_data(B_add):
  miller_array = random_data(B_add,n_residues=100.0)
  scale_object_iso = absolute_scaling.ml_iso_absolute_scaling(
    miller_array,
    n_residues=100.0)

  ## compare the results please
  assert approx_equal(B_add, scale_object_iso.b_wilson, eps=5)

  scale_object_aniso = absolute_scaling.ml_aniso_absolute_scaling(
    miller_array,
    n_residues=100.0)

  assert approx_equal(B_add, scale_object_aniso.b_cart[0], eps=5)
  assert approx_equal(B_add, scale_object_aniso.b_cart[1], eps=5)
  assert approx_equal(B_add, scale_object_aniso.b_cart[2], eps=5)



def test_scattering_info():
  miller_array = random_data(35.0, d_min=2.5 )
  d_star_sq = miller_array.d_spacings().data()
  d_star_sq = 1.0/(d_star_sq*d_star_sq)

  asu = {"H":8.0*585.0,"C":5.0*585.0,"N":1.5*585.0, "O":1.2*585.0}
  scat_info = absolute_scaling.scattering_information(
    asu_contents = asu,
    fraction_protein=1.0,
    fraction_nucleic=0.0)
  scat_info.scat_data(d_star_sq)

  scat_info2 = absolute_scaling.scattering_information(
    n_residues=585.0)
  scat_info2.scat_data(d_star_sq)

  sigma_prot = scaling.get_sigma_prot_sq(d_star_sq,195.0*3.0)
  # Testing for consistency
  for ii in range(d_star_sq.size()):
    assert approx_equal(scat_info.sigma_tot_sq[ii],
                        scat_info2.sigma_tot_sq[ii],
                        eps=1e-03)
    assert approx_equal(scat_info.sigma_tot_sq[ii],
                        sigma_prot[ii],
                        eps=0.5)


def twin_the_data_and_analyse(twin_operator,twin_fraction=0.2):
  out_string = StringIO()

  miller_array = random_data(35).map_to_asu()
  miller_array = miller_array.f_as_f_sq()

  cb_op =  sgtbx.change_of_basis_op( twin_operator )

  miller_array_mod, miller_array_twin = miller_array.common_sets(
    miller_array.change_basis( cb_op ).map_to_asu() )
  twinned_miller = miller_array_mod.customized_copy(
    data = (1.0-twin_fraction)*miller_array_mod.data()
    + twin_fraction*miller_array_twin.data(),
    sigmas = flex.sqrt(
    flex.pow( ((1.0-twin_fraction)*miller_array_mod.sigmas()),2.0)+\
    flex.pow( ((twin_fraction)*miller_array_twin.sigmas()),2.0))
    )

  twinned_miller.set_observation_type( miller_array.observation_type())
  twin_anal_object = t_a.twin_analyses(twinned_miller,
                                       out=out_string,
                                       verbose=-100)

  index = twin_anal_object.twin_summary.most_worrysome_twin_law

  assert approx_equal(
    twin_anal_object.twin_summary.britton_alpha[index],
    twin_fraction,eps=0.1)

  assert approx_equal(twin_anal_object.twin_law_dependent_analyses[index].ml_murray_rust.estimated_alpha,
                      twin_fraction, eps=0.1)

  ## Untwinned data standards
  if twin_fraction==0:
    ## L-test
    assert approx_equal(twin_anal_object.l_test.mean_l, 0.50,eps=0.1)
    ## Wilson ratios
    assert approx_equal(twin_anal_object.twin_summary.i_ratio,
                        2.00,
                        eps=0.1)
    ## H-test
    assert approx_equal(
      twin_anal_object.twin_law_dependent_analyses[index].h_test.mean_h,
      0.50,eps=0.1)


  ## Perfect twin standards
  if twin_fraction==0.5:
    assert approx_equal(twin_anal_object.l_test.mean_l, 0.375,eps=0.1)
    assert approx_equal(twin_anal_object.twin_summary.i_ratio,
                        1.50,eps=0.1)
    assert approx_equal(
      twin_anal_object.twin_law_dependent_analyses[index].h_test.mean_h,
      0.00,eps=0.1)
  ## Just make sure we actually detect significant twinning
  if twin_fraction > 0.10:
    assert (twin_anal_object.twin_summary.maha_l > 3.0)
  ## The patterson origin peak should be smallish ...
  assert (twin_anal_object.twin_summary.patterson_p_value > 0.01)
  # and the brief test should be passed as well
  answer = t_a.twin_analyses_brief( twinned_miller,out=out_string,verbose=-100 )
  if twin_fraction > 0.10:
    assert answer is True


def test_twin_r_value(twin_operator):
  miller_array = random_data(35).map_to_asu()
  miller_array = miller_array.f_as_f_sq()

  for twin_fraction, expected_r_abs,expected_r_sq in zip(
     [0,0.1,0.2,0.3,0.4,0.5],
     [0.50,0.40,0.30,0.20,0.10,0.0],
     [0.333,0.213,0.120,0.0533,0.0133,0.00]):
    cb_op =  sgtbx.change_of_basis_op( twin_operator )

    miller_array_mod, miller_array_twin = miller_array.common_sets(
      miller_array.change_basis( cb_op ).map_to_asu() )
    twinned_miller = miller_array_mod.customized_copy(
      data = (1.0-twin_fraction)*miller_array_mod.data()
      + twin_fraction*miller_array_twin.data(),
      sigmas = flex.sqrt(
      flex.pow( ((1.0-twin_fraction)*miller_array_mod.sigmas()),2.0)+\
      flex.pow( ((twin_fraction)*miller_array_twin.sigmas()),2.0))
      )

    twinned_miller.set_observation_type( miller_array.observation_type())

    twin_r = scaling.twin_r( twinned_miller.indices(),
                             twinned_miller.data(),
                             twinned_miller.space_group(),
                             twinned_miller.anomalous_flag(),
                             cb_op.c().r().as_double()[0:9] )
    assert approx_equal(twin_r.r_abs_value(), expected_r_abs, 0.08)
    assert approx_equal(twin_r.r_sq_value(), expected_r_sq, 0.08)


def test_constant():
  # this is to make sure that the tmp_const in the class  symmetry_issues
  # does not result in any overflow problems
  math.log(1e-250)


def test_kernel_based_normalisation():
  miller_array = random_data(35.0, d_min=2.5 )
  normalizer = absolute_scaling.kernel_normalisation(
    miller_array, auto_kernel=True)
  z_values = normalizer.normalised_miller.data()/\
             normalizer.normalised_miller.epsilons().data().as_double()
  z_values = flex.mean(z_values)
  assert approx_equal(1.0,z_values,eps=0.05)
  # This should raise an error rather than enter an infinite loop
  with raises(AssertionError) as e:
    absolute_scaling.kernel_normalisation(
      miller_array[:1].set_observation_type_xray_amplitude(), auto_kernel=True)


def test_ml_murray_rust():
  miller_array = random_data(35.0, d_min=4.5 )
  ml_mr_object = scaling.ml_murray_rust(
    miller_array.data(),
    miller_array.data(),
    miller_array.indices(),
    miller_array.space_group(),
    miller_array.anomalous_flag(),
    (1,0,0,0,1,0,0,0,1),
    6 )

  for ii in range(5,30):
    for jj in range(5,30):
       p1 = ml_mr_object.p_raw(ii/3.0, jj/3.0, 0.25)
       p2 = ml_mr_object.num_int(ii/3.0, 1e-13, jj/3.0, 1e-13, -5, 5,0.25, 20)
       assert approx_equal( p1, p2, eps=0.01)

def test_all():
  test_luts()
  test_ml_murray_rust()
  test_likelihood_iso()
  test_gradients_iso()
  test_gamma_prot()
  test_sigma_prot()
  test_likelihood_aniso()
  test_grads_aniso()
  test_scaling_on_random_data(10)
  test_scaling_on_random_data(20)
  test_scaling_on_random_data(40)
  test_scaling_on_random_data(70)
  test_scaling_on_random_data(80)
  scaling_tester()
  twin_the_data_and_analyse('h+k,-k,-l',0.00)
  twin_the_data_and_analyse('h+k,-k,-l',0.10)
  twin_the_data_and_analyse('h+k,-k,-l',0.20)
  twin_the_data_and_analyse('h+k,-k,-l',0.30)
  twin_the_data_and_analyse('h+k,-k,-l',0.50)
  test_scattering_info()
  test_kernel_based_normalisation()
  test_twin_r_value('h+k,-k,-l')
  test_constant()

def run(args):
  assert len(args) == 0
  test_all()
  print(format_cpu_times())

if (__name__ == "__main__"):
  import sys
  run(args=sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/tst_sigmaa.py
from __future__ import absolute_import, division, print_function
from mmtbx import scaling
from cctbx.array_family import flex
import mmtbx.scaling
import time
from libtbx.test_utils import approx_equal
from six.moves import range

def tst_sigmaa():
  eo = flex.double([1])
  ec = flex.double([1.2])
  dsc = flex.double([0.5])
  centric = flex.bool( [False] )

  tmp_a = scaling.sigmaa_estimator(
    e_obs     = eo,
    e_calc    = ec,
    centric   = centric,
    d_star_cubed = dsc,
    width=0.1)
  assert approx_equal(tmp_a.sum_weights(d_star_cubed=0.5), 1)
  # number obtained from mathematica
  assert approx_equal(tmp_a.target(0.5, 0.5), -0.272899, eps=1e-4 )

  assert approx_equal(tmp_a.target_and_gradient(0.5,0.5)[0], tmp_a.target(0.5, 0.5) )
  assert approx_equal(tmp_a.target_and_gradient(0.5,0.5)[1], tmp_a.dtarget(0.5, 0.5) )

  N=100000
  start = time.time()
  for ii in range(N):
    tmp_a.target(0.5, 0.5)
    tmp_a.dtarget(0.5, 0.5)

  end = time.time()
  print(end-start)

  start = time.time()
  for ii in range(N):
    tmp_a.target_and_gradient(0.5, 0.5)
  end = time.time()
  print(end-start)

  tmp_c = scaling.sigmaa_estimator(
    e_obs     = eo,
    e_calc    = ec,
    centric   = ~centric,
    d_star_cubed = dsc,
    width=0.1)
  assert approx_equal(tmp_a.sum_weights(d_star_cubed=0.5), 1)
  # number obtained from mathematica
  assert approx_equal(tmp_c.target(0.5, 0.5), -0.697863,  eps=1e-4 )

  assert approx_equal(tmp_c.target_and_gradient(0.5,0.5)[0], tmp_c.target(0.5, 0.5) )
  assert approx_equal(tmp_c.target_and_gradient(0.5,0.5)[1], tmp_c.dtarget(0.5, 0.5) )


  # timings for large arrays
  N=100000
  eo = flex.double([1]*N)
  ec = flex.double([1.2]*N)
  dsc = flex.double([0.5]*N)
  centric = flex.bool( [False]*N )
  tmp_large = scaling.sigmaa_estimator(
    e_obs     = eo,
    e_calc    = ec,
    centric   = centric,
    d_star_cubed = dsc,
    width=0.1)

  start = time.time()
  for trial in range(100):
    a = tmp_large.target(0.5,0.5)
    a = tmp_large.dtarget(0.5,0.5)
  end = time.time()
  print(end-start)

  tmp_large = scaling.sigmaa_estimator(
    e_obs     = eo,
    e_calc    = ec,
    centric   = centric,
    d_star_cubed = dsc,
    width=0.1)


  start = time.time()
  for trial in range(100):
    a = tmp_large.target_and_gradient(0.5,0.5)
  end = time.time()
  print(end-start)







  h=0.5
  d=0.000001
  for a in flex.double(range(20))/20.0:
    fa  = tmp_a.target(h,a)
    fa1 = tmp_a.target(h,a+d)
    ga  = tmp_a.dtarget(h,a)
    ga1 = (fa1-fa)/d

    fc  = tmp_c.target(h,a)
    fc1 = tmp_c.target(h,a+d)
    gc  = tmp_c.dtarget(h,a)
    gc1 = (fc1-fc)/d

    assert approx_equal(ga,ga1, eps=1e-3)
    assert approx_equal(gc,gc1, eps=1e-3)

def run():
  tst_sigmaa()
  print("OK")

if __name__=="__main__":
  run()


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/tst_xtriage.py

from __future__ import absolute_import, division, print_function
from mmtbx.scaling import data_statistics as ds
from mmtbx.scaling import xtriage
from mmtbx.command_line import fmodel
from iotbx import file_reader
import iotbx.pdb
from cctbx import crystal
from cctbx import miller
from cctbx import sgtbx
from cctbx.array_family import flex
from libtbx.test_utils import approx_equal, Exception_expected, show_diff
from libtbx.development import show_pickle_sizes
from libtbx.easy_pickle import dumps, loads
from libtbx.utils import null_out, Sorry
import libtbx.load_env
from six.moves import cStringIO as StringIO
import warnings
import os.path
import sys
from six.moves import range

# synthetic data
def exercise_1():
  pdb_raw = """\
CRYST1   23.000    6.666   25.000  90.00 107.08  90.00 P 1 21 1      2
ATOM      1  N   GLY A   1      -9.009   4.612   6.102  1.00 16.77           N
ATOM      2  CA  GLY A   1      -9.052   4.207   4.651  1.00 16.57           C
ATOM      3  C   GLY A   1      -8.015   3.140   4.419  1.00 16.16           C
ATOM      4  O   GLY A   1      -7.523   2.521   5.381  1.00 16.78           O
ATOM      5  N   ASN A   2      -7.656   2.923   3.155  1.00 15.02           N
ATOM      6  CA  ASN A   2      -6.522   2.038   2.831  1.00 14.10           C
ATOM      7  C   ASN A   2      -5.241   2.537   3.427  1.00 13.13           C
ATOM      8  O   ASN A   2      -4.978   3.742   3.426  1.00 11.91           O
ATOM      9  CB  ASN A   2      -6.346   1.881   1.341  1.00 15.38           C
ATOM     10  CG  ASN A   2      -7.584   1.342   0.692  1.00 14.08           C
ATOM     11  OD1 ASN A   2      -8.025   0.227   1.016  1.00 17.46           O
ATOM     12  ND2 ASN A   2      -8.204   2.155  -0.169  1.00 11.72           N
ATOM     13  N   ASN A   3      -4.438   1.590   3.905  1.00 12.26           N
ATOM     14  CA  ASN A   3      -3.193   1.904   4.589  1.00 11.74           C
ATOM     15  C   ASN A   3      -1.955   1.332   3.895  1.00 11.10           C
ATOM     16  O   ASN A   3      -1.872   0.119   3.648  1.00 10.42           O
ATOM     17  CB  ASN A   3      -3.259   1.378   6.042  1.00 12.15           C
ATOM     18  CG  ASN A   3      -2.006   1.739   6.861  1.00 12.82           C
ATOM     19  OD1 ASN A   3      -1.702   2.925   7.072  1.00 15.05           O
ATOM     20  ND2 ASN A   3      -1.271   0.715   7.306  1.00 13.48           N
ATOM     21  N   MET A   4      -1.005   2.228   3.598  1.00 10.29           N
ATOM     22  CA  MET A   4       0.384   1.888   3.199  1.00 10.53           C
ATOM     23  C   MET A   4       1.435   2.606   4.088  1.00 10.24           C
ATOM     24  O   MET A   4       1.547   3.843   4.115  1.00  8.86           O
ATOM     25  CB  MET A   4       0.616   2.241   1.729  1.00 20.00           C
ATOM     26  CG  MET A   4      -0.207   1.416   0.754  1.00 20.00           C
ATOM     27  SD  MET A   4       0.132  -0.349   0.876  1.00 20.00           S
ATOM     28  CE  MET A   4       1.822  -0.411   0.285  1.00 20.00           C
ATOM     29  N   GLN A   5       2.154   1.821   4.871  1.00 10.38           N
ATOM     30  CA  GLN A   5       3.270   2.361   5.640  1.00 11.39           C
ATOM     31  C   GLN A   5       4.594   1.768   5.172  1.00 11.52           C
ATOM     32  O   GLN A   5       4.768   0.546   5.054  1.00 12.05           O
ATOM     33  CB  GLN A   5       3.056   2.183   7.147  1.00 11.96           C
ATOM     34  CG  GLN A   5       1.829   2.950   7.647  1.00 10.81           C
ATOM     35  CD  GLN A   5       1.344   2.414   8.954  1.00 13.10           C
ATOM     36  OE1 GLN A   5       0.774   1.325   9.002  1.00 10.65           O
ATOM     37  NE2 GLN A   5       1.549   3.187  10.039  1.00 12.30           N
ATOM     38  N   ASN A   6       5.514   2.664   4.856  1.00 11.99           N
ATOM     39  CA  ASN A   6       6.831   2.310   4.318  1.00 12.30           C
ATOM     40  C   ASN A   6       7.854   2.761   5.324  1.00 13.40           C
ATOM     41  O   ASN A   6       8.219   3.943   5.374  1.00 13.92           O
ATOM     42  CB  ASN A   6       7.065   3.016   2.993  1.00 12.13           C
ATOM     43  CG  ASN A   6       5.961   2.735   2.003  1.00 12.77           C
ATOM     44  OD1 ASN A   6       5.798   1.604   1.551  1.00 14.27           O
ATOM     45  ND2 ASN A   6       5.195   3.747   1.679  1.00 10.07           N
ATOM     46  N   TYR A   7       8.292   1.817   6.147  1.00 14.70           N
ATOM     47  CA  TYR A   7       9.159   2.144   7.299  1.00 15.18           C
ATOM     48  C   TYR A   7      10.603   2.331   6.885  1.00 15.91           C
ATOM     49  O   TYR A   7      11.041   1.811   5.855  1.00 15.76           O
ATOM     50  CB  TYR A   7       9.061   1.065   8.369  1.00 15.35           C
ATOM     51  CG  TYR A   7       7.665   0.929   8.902  1.00 14.45           C
ATOM     52  CD1 TYR A   7       6.771   0.021   8.327  1.00 15.68           C
ATOM     53  CD2 TYR A   7       7.210   1.756   9.920  1.00 14.80           C
ATOM     54  CE1 TYR A   7       5.480  -0.094   8.796  1.00 13.46           C
ATOM     55  CE2 TYR A   7       5.904   1.649  10.416  1.00 14.33           C
ATOM     56  CZ  TYR A   7       5.047   0.729   9.831  1.00 15.09           C
ATOM     57  OH  TYR A   7       3.766   0.589  10.291  1.00 14.39           O
ATOM     58  OXT TYR A   7      11.358   2.999   7.612  1.00 17.49           O
TER      59      TYR A   7
HETATM    1 CA    CA A   8      10.431   1.858   3.216  1.00 30.00          CA
HETATM   60  O   HOH A   9      -6.471   5.227   7.124  1.00 22.62           O
HETATM   62  O   HOH A  10     -11.286   1.756  -1.468  1.00 17.08           O
HETATM   63  O   HOH A  11      11.808   4.179   9.970  1.00 23.99           O
HETATM   64  O   HOH A  12      13.605   1.327   9.198  1.00 26.17           O
HETATM   65  O   HOH A  13      -2.749   3.429  10.024  1.00 39.15           O
HETATM   66  O   HOH A  14      -1.500   0.682  10.967  1.00 43.49           O
END
"""
  pdb_file = "tst_xtriage_in.pdb"
  with open(pdb_file, "w") as f:
    f.write(pdb_raw)
  fmodel_args = [
    pdb_file,
    "high_resolution=1.5",
    "k_sol=0.35",
    "b_sol=20",
    "wavelength=1.54",
    "add_random_error_to_amplitudes_percent=3",
    "random_seed=12345",
    "output.type=real",
    "output.label=F",
    "output.file_name=tst_xtriage_fmodel.mtz",
  ]

  #  read it instead so python3 will be the same
  #  fmodel.run(args=fmodel_args, log=null_out())
  hkl_file = libtbx.env.find_in_repositories(
    relative_path="mmtbx/regression/mtz/tst_xtriage_fmodel.mtz",
    test=os.path.isfile)
  mtz_in = file_reader.any_file(
    hkl_file).assert_file_type("hkl")
  f_obs = mtz_in.file_server.miller_arrays[0].remove_cone(0.1)
  data = f_obs.data()
  # add some outliers
  #data[17] = 20
  #data[334] = 26
  #data[1908] = 13
  # and sigmas
  sigf = flex.double(f_obs.size(), 0.1) + (f_obs.data() * 0.03)
  f_obs = f_obs.customized_copy(sigmas=sigf)
  mtz_file = "tst_xtriage_in.mtz"
  f_obs.as_mtz_dataset(column_root_label="F").mtz_object().write(mtz_file)
  seq_file = "tst_xtriage_in.fa"
  with open(seq_file, "w") as f:
    f.write("> tst_xtriage\nGNNMQNY")

  # check with completeness_as_non_anomalous=True

  xtriage_args = [
    mtz_file,
    pdb_file,
    seq_file,
    "log=tst_xtriage_1.log",
    "l_test_dhkl=2,2,2",
    "completeness_as_non_anomalous=True",
  ]
  result = xtriage.run(args=xtriage_args, out=null_out())
  test_pickle_consistency_and_size(result)
  assert (result.matthews.n_copies == 1)
  assert (str(result.matthews.table) == """\
Solvent content analysis
Copies             Solvent content    Matthews coeff.    P(solvent content)
1                  0.472              2.33               1.000
""")
  data_strength = result.data_strength_and_completeness
  assert approx_equal(data_strength.data_strength.resolution_cut, 1.5351,
    eps=0.001)
  out1 = data_strength.low_resolution_completeness.format()
  assert (out1 == """\
---------------------------------------------------------
| Resolution range  | N(obs)/N(possible) | Completeness |
---------------------------------------------------------
| 21.9858 - 10.4368 | [6/7]              | 0.857        |
| 10.4368 -  8.4369 | [3/3]              | 1.000        |
|  8.4369 -  7.4172 | [3/4]              | 0.750        |
|  7.4172 -  6.7606 | [4/4]              | 1.000        |
|  6.7606 -  6.2882 | [5/5]              | 1.000        |
|  6.2882 -  5.9252 | [3/4]              | 0.750        |
|  5.9252 -  5.6337 | [7/7]              | 1.000        |
|  5.6337 -  5.3922 | [5/5]              | 1.000        |
|  5.3922 -  5.1874 | [4/4]              | 1.000        |
|  5.1874 -  5.0106 | [4/4]              | 1.000        |
---------------------------------------------------------"""), out1
  # ANOMALOUS SIGNAL
  a_meas = result.anomalous_info.measurability
  #assert approx_equal(a_meas.high_d_cut, 4.7636, eps=0.0001) # Why it's None?
  assert approx_equal(a_meas.low_d_cut, 2.3566, eps=0.0001)
  # ABSOLUTE SCALING
  ws = result.wilson_scaling
  assert ("%.2f" % ws.iso_p_scale) == "0.65",ws.iso_p_scale
  assert ("%.2f" % ws.iso_b_wilson) == "14.42", ws.iso_b_wilson
  # FIXME these may need to be adjusted for different hardware/OS
  assert approx_equal(ws.aniso_p_scale, 0.64723, eps=0.001)
  assert approx_equal(ws.aniso_u_star, [0.00034229, 0.00475982, 0.000285989,
                                        -0.0, 8.95386085999e-05, 0.0])
  assert approx_equal(ws.aniso_b_cart, (13.218423, 16.840142, 12.948426,
    1.0354e-15, -0.0685311, -7.92862e-16), 0.3)
  # convenience methods for GUI
  assert approx_equal(result.aniso_b_min, 12.895580)
  assert approx_equal(result.aniso_range_of_b, 3.804215)
  #
  assert approx_equal(ws.outlier_shell_table.data[0], # d_spacing
    [9.865131, 8.369653, 4.648634])
  assert approx_equal(ws.outlier_shell_table.data[1], # z_score
    [5.306713, 18.068284, 5.319230])
  assert (len(ws.outliers.acentric_outliers_table.data[0]) == 2)
  assert (ws.outliers.acentric_outliers_table.data[1] == [(0,-1,-1), (0,1,1)])
  assert approx_equal(ws.outliers.acentric_outliers_table.data[2],
    [3.507247, 3.315550])
  assert (ws.outliers.centric_outliers_table.data is None)
  assert (len(ws.ice_rings.table._rows) == 10)
  assert (ws.ice_rings.table._rows[0] ==
          ['    3.897', '     1.000', '   0.76', '   1.00']), \
          ws.ice_rings.table._rows[0]
  tw = result.twin_results
  wm = tw.wilson_moments
  out = StringIO()
  wm.show(out)
  assert not show_diff(out.getvalue(), """
                  ----------Wilson ratio and moments----------

Acentric reflections:


   <I^2>/<I>^2    :2.063   (untwinned: 2.000; perfect twin 1.500)
   <F>^2/<F^2>    :0.778   (untwinned: 0.785; perfect twin 0.885)
   <|E^2 - 1|>    :0.745   (untwinned: 0.736; perfect twin 0.541)

Centric reflections:


   <I^2>/<I>^2    :3.076   (untwinned: 3.000; perfect twin 2.000)
   <F>^2/<F^2>    :0.628   (untwinned: 0.637; perfect twin 0.785)
   <|E^2 - 1|>    :0.999   (untwinned: 0.968; perfect twin 0.736)

""")
  # XXX PDB validation server
  assert approx_equal(result.iso_b_wilson, 14.51, eps=0.1)
  assert approx_equal(result.aniso_b_ratio, 0.271, eps=0.1)
  assert (result.number_of_wilson_outliers == 2)
  assert approx_equal(result.l_test_mean_l, 0.481, eps=0.1)
  assert approx_equal(result.l_test_mean_l_squared, 0.322, eps=0.1)
  assert approx_equal(result.i_over_sigma_outer_shell, 10.71, eps=0.01)
  assert ("indicating pseudo-translationa" in result.patterson_verdict)
  # check relative Wilson
  # FIXME
  #result.relative_wilson.show()
  #assert (result.relative_wilson.n_outliers() == 0)
  #show_pickled_object_sizes(result)
  #

  # check with completeness_as_non_anomalous=False

  xtriage_args = [
    mtz_file,
    pdb_file,
    seq_file,
    "log=tst_xtriage_1.log",
    "l_test_dhkl=2,2,2",
    "completeness_as_non_anomalous=False",
  ]
  result = xtriage.run(args=xtriage_args, out=null_out())
  test_pickle_consistency_and_size(result)
  assert (result.matthews.n_copies == 1)
  assert (str(result.matthews.table) == """\
Solvent content analysis
Copies             Solvent content    Matthews coeff.    P(solvent content)
1                  0.472              2.33               1.000
""")
  data_strength = result.data_strength_and_completeness
  assert approx_equal(data_strength.data_strength.resolution_cut, 1.5351,
    eps=0.001)
  out1 = data_strength.low_resolution_completeness.format()
  assert (out1 == """\
---------------------------------------------------------
| Resolution range  | N(obs)/N(possible) | Completeness |
---------------------------------------------------------
| 21.9858 - 10.4368 | [ 6/7 ]            | 0.857        |
| 10.4368 -  8.4369 | [ 3/3 ]            | 1.000        |
|  8.4369 -  7.4172 | [ 3/4 ]            | 0.750        |
|  7.4172 -  6.7606 | [ 4/4 ]            | 1.000        |
|  6.7606 -  6.2882 | [ 8/8 ]            | 1.000        |
|  6.2882 -  5.9252 | [ 4/5 ]            | 0.800        |
|  5.9252 -  5.6337 | [11/11]            | 1.000        |
|  5.6337 -  5.3922 | [ 7/7 ]            | 1.000        |
|  5.3922 -  5.1874 | [ 6/6 ]            | 1.000        |
|  5.1874 -  5.0106 | [ 7/7 ]            | 1.000        |
---------------------------------------------------------"""), out1
  # ANOMALOUS SIGNAL
  a_meas = result.anomalous_info.measurability
  #assert approx_equal(a_meas.high_d_cut, 4.7636, eps=0.0001) # Why?
  assert approx_equal(a_meas.low_d_cut, 2.3565, eps=0.0001)
  # ABSOLUTE SCALING
  ws = result.wilson_scaling
  assert ("%.2f" % ws.iso_p_scale) == "0.65", ws.iso_p_scale
  assert ("%.2f" % ws.iso_b_wilson) == "14.42", ws.iso_b_wilson
  # FIXME these may need to be adjusted for different hardware/OS
  assert approx_equal(ws.aniso_p_scale, 0.64723, eps=0.001)
  assert approx_equal(ws.aniso_u_star, [0.00034473, 0.00479983, 0.000287162,
                                        -0.0, 9.00962e-05, 0.0], 6.e-5)
  assert approx_equal(ws.aniso_b_cart, [13.12, 16.69, 12.89,
    0, -0.08, 0], 0.01)
  # convenience methods for GUI
  assert approx_equal(result.aniso_b_min, 12.9, 0.1)
  assert approx_equal(result.aniso_range_of_b, 3.8, 0.1)
  #
  assert approx_equal(ws.outlier_shell_table.data[0], # d_spacing
    [9.86, 8.36, 4.64], 0.02)
  assert approx_equal(ws.outlier_shell_table.data[1], # z_score
    [5.30, 18.06, 5.31], 0.01)
  assert (len(ws.outliers.acentric_outliers_table.data[0]) == 2)
  assert (ws.outliers.acentric_outliers_table.data[1] == [(0,-1,-1), (0,1,1)])
  assert approx_equal(ws.outliers.acentric_outliers_table.data[2],
    [3.5, 3.3], 0.1)
  assert (ws.outliers.centric_outliers_table.data is None)
  assert (len(ws.ice_rings.table._rows) == 10)
  assert (ws.ice_rings.table._rows[0] ==
          ['    3.897', '     1.000', '   0.76', '   1.00']), \
          ws.ice_rings.table._rows[0]
  tw = result.twin_results
  wm = tw.wilson_moments
  out = StringIO()
  wm.show(out)
  assert not show_diff(out.getvalue(), """
                  ----------Wilson ratio and moments----------

Acentric reflections:


   <I^2>/<I>^2    :2.063   (untwinned: 2.000; perfect twin 1.500)
   <F>^2/<F^2>    :0.778   (untwinned: 0.785; perfect twin 0.885)
   <|E^2 - 1|>    :0.745   (untwinned: 0.736; perfect twin 0.541)

Centric reflections:


   <I^2>/<I>^2    :3.076   (untwinned: 3.000; perfect twin 2.000)
   <F>^2/<F^2>    :0.628   (untwinned: 0.637; perfect twin 0.785)
   <|E^2 - 1|>    :0.999   (untwinned: 0.968; perfect twin 0.736)

""")
  # XXX PDB validation server
  assert approx_equal(result.iso_b_wilson, 14.51, eps=0.1)
  assert approx_equal(result.aniso_b_ratio, 0.271, eps=0.1)
  assert (result.number_of_wilson_outliers == 2)
  assert approx_equal(result.l_test_mean_l, 0.481, eps=0.1)
  assert approx_equal(result.l_test_mean_l_squared, 0.322, eps=0.1)
  assert approx_equal(result.i_over_sigma_outer_shell, 10.71, eps=0.01)
  assert ("indicating pseudo-translationa" in result.patterson_verdict)
  # check relative Wilson
  # FIXME
  #result.relative_wilson.show()
  #assert (result.relative_wilson.n_outliers() == 0)
  #show_pickled_object_sizes(result)
  #
  # test without sigmas
  f_obs_2 = f_obs.customized_copy(sigmas=None)
  mtz_file = "tst_xtriage_in_2.mtz"
  f_obs_2.as_mtz_dataset(column_root_label="F").mtz_object().write(mtz_file)
  xtriage_args = [
    mtz_file,
    pdb_file,
    seq_file,
    "log=tst_xtriage_1.log",
  ]


  result = xtriage.run(args=xtriage_args, out=null_out())
  result.summarize_issues()
  # test in lower symmetry
  f_obs_3 = f_obs.expand_to_p1()
  mtz_file = "tst_xtriage_in_3.mtz"
  f_obs_3.as_mtz_dataset(column_root_label="F").mtz_object().write(mtz_file)
  xtriage_args = [
    mtz_file,
    seq_file,
    "log=tst_xtriage_2.log",
  ]
  result = xtriage.run(args=xtriage_args, out=null_out())
  assert ((1, 'One or more symmetry operators suggest that the data has a higher crystallographic symmetry (P 2 1 1).', 'Point group and R-factor analysis') in result.summarize_issues()._issues)
  # test with elliptical truncation
  f_obs_3 = f_obs.customized_copy(
    crystal_symmetry=crystal.symmetry((23,5,20,90,107.8,90), "P 21"))
  f_obs_3 = f_obs_3.resolution_filter(d_min=1.5)
  f_obs_3 = f_obs_3.customized_copy(crystal_symmetry=f_obs.crystal_symmetry())
  reso = ds.analyze_resolution_limits(f_obs_3)
  out = StringIO()
  reso.show(out=out)
  assert ("max. difference between axes = 0.652" in out.getvalue()), \
    out.getvalue()
  assert ("elliptically truncated" in out.getvalue())
  # make sure the elliptical truncation detection still works in higher space
  # groups - we only need a miller.set for this
  miller_set = miller.build_set(
    crystal_symmetry=crystal.symmetry((20,20,20,90,90,90), "P422"),
    d_min=1.5,
    anomalous_flag=False)
  reso = ds.analyze_resolution_limits(miller_set)
  out = StringIO()
  reso.show(out=out)
  assert ("Resolution limits are within expected tolerances" in out.getvalue())
  # log binning
  out = StringIO()
  log_binned = ds.log_binned_completeness(f_obs_3)
  log_binned.show(out=out)
  assert ("""| 1.9724 - 1.5094  | 368/1230    | 29.9%        |""" in
          out.getvalue()), out.getvalue()
  # test with no acentrics
  cf = f_obs.centric_flags().data()
  centrics = f_obs.select(cf)
  acentrics = f_obs.select(~cf)
  mtz_file = "tst_xtriage_in_3.mtz"
  centrics.as_mtz_dataset(column_root_label="F").mtz_object().write(mtz_file)
  args = [
    mtz_file,
    pdb_file,
    seq_file,
    "log=tst_xtriage_3.log",
  ]
  try :
    xtriage.run(args=args, out=null_out())
  except Sorry :
    pass
  else :
    raise Exception_expected
  # with only a handful of acentrics
  sel = flex.bool(acentrics.size(), False)
  for i in range(10):
    sel[i] = True
  f_obs_4 = centrics.concatenate(acentrics.select(sel))
  f_obs_4.as_mtz_dataset(column_root_label="F").mtz_object().write(mtz_file)
  try :
    xtriage.run(args=args, out=null_out())
  except Sorry :
    pass
  else :
    raise Exception_expected

# XXX code for debugging pickle size issues
def show_pickled_object_sizes(result):
  result_pkl = dumps(result)
  print("result", len(result_pkl))
  show_pickle_sizes(result, "  ")

# test consistency of output after pickling and unpickling
def test_pickle_consistency_and_size(result):
  all_out = StringIO()
  result.show(out=all_out)
  result_pkl_str = dumps(result)
  pkl_size = len(result_pkl_str)
  if (pkl_size >= 100000):
    print("Oversized pickle:", pkl_size)
    show_pickled_object_sizes(result)
    raise OverflowError()
  assert (pkl_size < 100000), pkl_size # limit pickle size
  result_pkl = loads(result_pkl_str)
  all_out_pkl = StringIO()
  result_pkl.show(out=all_out_pkl)
  assert not show_diff(all_out.getvalue(), all_out_pkl.getvalue())

def exercise_analyze_resolution_limits():
  for x in range(1, 231):
    sg = sgtbx.space_group_info(number=x)
    #sg = sgtbx.space_group_info("P222")
    uc = sg.any_compatible_unit_cell(80000)
    ms = miller.build_set(
      crystal_symmetry=crystal.symmetry(
        space_group_info=sg,
        unit_cell=uc),
      anomalous_flag=True,
      d_min=1.5)
    arl = ds.analyze_resolution_limits(ms)
    if (x > 2):
      assert (arl.max_d_min_delta() < 0.1)

# real data
def exercise_2():
  hkl_file = libtbx.env.find_in_repositories(
    relative_path="phenix_regression/wizards/data/p9_se_w2.sca",
    test=os.path.isfile)
  if (hkl_file is None):
    warnings.warn("phenix_regression not available, skipping test")
    return
  hkl_in = file_reader.any_file(hkl_file).assert_file_type("hkl")
  i_obs_raw = hkl_in.file_object.as_miller_arrays(
    merge_equivalents=False,
    crystal_symmetry=crystal.symmetry(
      space_group_symbol="I4",
      unit_cell=(113.949,113.949,32.474,90,90,90)))[0]
  i_obs = i_obs_raw.merge_equivalents().array()
  # completeness and data strength
  cstats = ds.i_sigi_completeness_stats(i_obs)
  d_min_cut = cstats.resolution_cut
  assert approx_equal(d_min_cut, 2.150815)
  ws = ds.wilson_scaling(
    miller_array=i_obs,
    n_residues=120)
  # outliers - this shouldn't actually work, since it requires additional
  # processing steps on the input data
  try :
    outliers = ds.possible_outliers(i_obs)
  except AssertionError :
    pass
  else :
    raise Exception_expected
  ######################################################################
  # OVERALL ANALYSIS
  pdb_file = libtbx.env.find_in_repositories(
    relative_path="phenix_examples/p9-build/p9.pdb",
    test=os.path.isfile)
  f_calc = None
  if (pdb_file is not None):
    pdb_in = iotbx.pdb.input(pdb_file)
    hierarchy = pdb_in.construct_hierarchy()
    xrs = pdb_in.xray_structure_simple(
      crystal_symmetry=i_obs)
    f_calc = xrs.structure_factors(d_min=i_obs.d_min()).f_calc()
    f_calc = abs(f_calc).generate_bijvoet_mates()
    f_calc = f_calc.set_observation_type_xray_amplitude()
    i_obs, f_calc = i_obs.common_sets(other=f_calc)
    with open("tmp_xtriage.pdb", "w") as f:
      f.write(hierarchy.as_pdb_string(crystal_symmetry=i_obs))
    pdb_file = "tmp_xtriage.pdb"
  params = xtriage.master_params.extract()
  params.scaling.input.asu_contents.n_residues = 141
  text_out = open("logfile3.log", "w")
  result = xtriage.xtriage_analyses(
    miller_obs=i_obs,
    miller_calc=f_calc,
    params=params,
    unmerged_obs=i_obs_raw,
    text_out=text_out)#sys.stdout)
  text_out.close()
  # XXX there appears to be some system-dependence here, hence sloppy limits
  assert (15.5 < result.aniso_b_min < 15.9)
  assert (10 < result.aniso_range_of_b < 11)
  # check relative Wilson
  if (pdb_file is not None):
    assert (result.relative_wilson is not None)
    # FIXME
    #assert (result.relative_wilson.n_outliers() == 34)
  #show_pickled_object_sizes(result)
  test_pickle_consistency_and_size(result)
  # XXX PDB validation server
  assert approx_equal(result.iso_b_wilson, 18.33, eps=0.1)
  assert approx_equal(result.aniso_b_ratio, 0.546, eps=0.1)
  assert (result.number_of_wilson_outliers == 0)
  assert approx_equal(result.l_test_mean_l, 0.493, eps=0.1)
  assert approx_equal(result.l_test_mean_l_squared, 0.326, eps=0.1)
  assert approx_equal(result.i_over_sigma_outer_shell, 3.25, eps=0.1)
  assert approx_equal(result.overall_i_sig_i,10.34,eps=0.1)
  assert approx_equal(result.anomalous_info.plan_sad_experiment_stats.get_overall(
      item="i_over_sigma_dict"),10.61,eps=0.1)
  assert approx_equal(result.anomalous_info.plan_sad_experiment_stats.get_overall(
      item="anom_signal_dict"),15.35,eps=0.1)
  assert ("No significant pseudotranslation is detected" in
          result.patterson_verdict)
  # test consistency of output after pickling and unpickling
  try :
    from phenix_dev.phenix_cloud import xtriage_json
  except ImportError :
    pass
  else :
    json_out = xtriage_json.json_output("p9.sca")
    result.show(out=json_out)
    with open("xtriage.json", "w") as f:
      f.write(json_out.export())
  # unmerged data
  assert result.merging_stats is not None
  out = StringIO()
  result.merging_stats.show(out=out)
  assert ("R-merge: 0.073" in out.getvalue())
  assert approx_equal(result.estimate_d_min(min_i_over_sigma=10), 1.9645,
    eps=0.001)
  # FIXME PDB doesn't actually have unit cell!
  # test detection of symmetry in reference file
  if (pdb_file is not None):
    args = [hkl_file, pdb_file]
    result = xtriage.run(args=args, out=null_out())

def exercise_loggraph():
  mtz_file = libtbx.env.find_in_repositories(
    relative_path="mmtbx/regression/mtz/tst_xtriage_fmodel.mtz",
    test=os.path.isfile)
  result = xtriage.run(
    args=[mtz_file, 'scaling.input.parameters.reporting.loggraphs=True'],
    out=null_out()
  )

if (__name__ == "__main__"):
  #exercise_2()
  exercise_1()
  exercise_analyze_resolution_limits()
  exercise_loggraph()
  print("OK")


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/tst_xtriage_twin_analyses.py

# XXX This overlaps heavily with tst_scaling.py, which is much more
# comprehensive.  Maybe remove this one?

from __future__ import absolute_import, division, print_function
from mmtbx.scaling import twin_analyses
import iotbx.pdb
from scitbx.array_family import flex
from libtbx.test_utils import approx_equal, show_diff
from libtbx.utils import null_out
from six.moves import cStringIO as StringIO
import sys

def exercise_twin_detection(verbose=False):
  # simple model with translational pseudosymmetry
  # XXX one big disadvantage: no centric reflections!
  pdb_str = """\
CRYST1   12.000    8.000   12.000  90.02  89.96  90.05 P 1           1
ATOM     39  N   ASN A   6       5.514   2.664   4.856  1.00 11.99           N
ATOM     40  CA  ASN A   6       6.831   2.310   4.318  1.00 12.30           C
ATOM     41  C   ASN A   6       7.854   2.761   5.324  1.00 13.40           C
ATOM     42  O   ASN A   6       8.219   3.943   5.374  1.00 13.92           O
ATOM     43  CB  ASN A   6       7.065   3.016   2.993  1.00 12.13           C
ATOM     44  CG  ASN A   6       5.961   2.735   2.003  1.00 12.77           C
ATOM     45  OD1 ASN A   6       5.798   1.604   1.551  1.00 14.27           O
ATOM     46  ND2 ASN A   6       5.195   3.747   1.679  1.00 10.07           N
ATOM     47  N   TYR A   7       8.292   1.817   6.147  1.00 14.70           N
ATOM     48  CA  TYR A   7       9.159   2.144   7.299  1.00 15.18           C
ATOM     49  C   TYR A   7      10.603   2.331   6.885  1.00 15.91           C
ATOM     50  O   TYR A   7      11.041   1.811   5.855  1.00 15.76           O
ATOM     51  CB  TYR A   7       9.061   1.065   8.369  1.00 15.35           C
ATOM     52  CG  TYR A   7       7.665   0.929   8.902  1.00 14.45           C
ATOM     53  CD1 TYR A   7       6.771   0.021   8.327  1.00 15.68           C
ATOM     54  CD2 TYR A   7       7.210   1.756   9.920  1.00 14.80           C
ATOM     55  CE1 TYR A   7       5.480  -0.094   8.796  1.00 13.46           C
ATOM     56  CE2 TYR A   7       5.904   1.649  10.416  1.00 14.33           C
ATOM     57  CZ  TYR A   7       5.047   0.729   9.831  1.00 15.09           C
ATOM     58  OH  TYR A   7       3.766   0.589  10.291  1.00 14.39           O
ATOM     59  OXT TYR A   7      11.358   2.999   7.612  1.00 17.49           O
TER
ATOM      1  N   ASN B   6       1.414   5.113   6.019  1.00 12.99           N
ATOM      2  CA  ASN B   6       2.720   4.776   5.445  1.00 13.30           C
ATOM      3  C   ASN B   6       3.763   5.209   6.438  1.00 14.40           C
ATOM      4  O   ASN B   6       4.125   6.391   6.507  1.00 14.92           O
ATOM      5  CB  ASN B   6       2.922   5.513   4.131  1.00 13.13           C
ATOM      6  CG  ASN B   6       1.798   5.250   3.160  1.00 13.77           C
ATOM      7  OD1 ASN B   6       1.629   4.129   2.686  1.00 15.27           O
ATOM      8  ND2 ASN B   6       1.022   6.266   2.875  1.00 11.07           N
ATOM      9  N   TYR B   7       4.222   4.248   7.230  1.00 15.70           N
ATOM     10  CA  TYR B   7       5.113   4.552   8.370  1.00 16.18           C
ATOM     11  C   TYR B   7       6.547   4.754   7.929  1.00 16.91           C
ATOM     12  O   TYR B   7       6.964   4.259   6.878  1.00 16.76           O
ATOM     13  CB  TYR B   7       5.042   3.449   9.417  1.00 16.35           C
ATOM     14  CG  TYR B   7       3.659   3.296   9.977  1.00 15.45           C
ATOM     15  CD1 TYR B   7       2.756   2.398   9.402  1.00 16.68           C
ATOM     16  CD2 TYR B   7       3.224   4.098  11.023  1.00 15.80           C
ATOM     17  CE1 TYR B   7       1.476   2.267   9.896  1.00 14.46           C
ATOM     18  CE2 TYR B   7       1.929   3.975  11.545  1.00 15.33           C
ATOM     19  CZ  TYR B   7       1.063   3.065  10.959  1.00 16.09           C
ATOM     20  OH  TYR B   7      -0.207   2.910  11.443  1.00 15.39           O
ATOM     21  OXT TYR B   7       7.316   5.408   8.654  1.00 18.49           O
END
"""
  pdb_in = iotbx.pdb.input(source_info=None, lines=pdb_str)
  xrs = pdb_in.xray_structure_simple()
  fc = abs(xrs.structure_factors(d_min=2.5).f_calc())
  fc = fc.set_observation_type_xray_amplitude()
  sigf = flex.double(fc.size(), 0.1) + (fc.data() * 0.03)
  fc = fc.customized_copy(sigmas=sigf)
  # and now add twinning
  fc_twin = fc.twin_data(twin_law='-l,-k,-h', alpha=0.4)
  fc_twin.as_mtz_dataset(column_root_label="F").mtz_object().write(
    "tmp_xtriage_twinned.mtz")
  # twin_laws
  laws = twin_analyses.twin_laws(miller_array=fc_twin)
  assert (len(laws.operators) == 7)
  delta_santoro = [ tl.delta_santoro for tl in laws.operators ]
  assert approx_equal(delta_santoro,
    [0.104655, 0.104655, 0.066599, 0.104655, 0.076113, 0.066599, 0.028542])
  delta_le_page = [ tl.delta_le_page for tl in laws.operators ]
  assert approx_equal(delta_le_page,
    [0.053839, 0.053839, 0.053839, 0.064020, 0.044706, 0.049480, 0.021221])
  delta_lebedev = [ tl.delta_lebedev for tl in laws.operators ]
  assert approx_equal(delta_lebedev,
    [0.000787, 0.000787, 0.000767, 0.000912, 0.000637, 0.000705, 0.000302])
  # TNCS
  tps = twin_analyses.detect_pseudo_translations(
    miller_array=fc_twin, out=null_out())
  assert ([tps.mod_h, tps.mod_k, tps.mod_l] == [3,2,3])
  assert approx_equal(tps.high_peak, 47.152352)
  assert approx_equal(tps.high_p_value, 0.000103)
  # L test
  fc_norm_acentric = twin_analyses.wilson_normalised_intensities(
    miller_array=fc_twin, normalise=True, out=null_out()).acentric
  ltest = twin_analyses.l_test(miller_array=fc_norm_acentric,
    parity_h=3, parity_l=3)
  assert approx_equal(ltest.mean_l, 0.498974)
  assert approx_equal(ltest.mean_l2, 0.371674)
  # we need to go through the wrapper class for a lot of these...
  out = StringIO()
  tw = twin_analyses.twin_analyses(miller_array=fc_twin, out=out)
  tw2 = twin_analyses.twin_analyses(miller_array=fc_twin, out=out,
    d_hkl_for_l_test=[3,2,3])
  # Wilson moments
  wm = tw.wilson_moments
  assert ([wm.centric_i_ratio, wm.centric_f_ratio, wm.centric_e_sq_minus_one]
          == [None, None, None])
  assert approx_equal(wm.acentric_i_ratio, 2.878383)
  assert approx_equal(wm.acentric_f_ratio, 0.717738)
  assert approx_equal(wm.acentric_abs_e_sq_minus_one, 0.808899)
  assert (not wm.centric_present)
  # Overall analyses
  out = StringIO()
  tw.show(out=out)
  assert ("significantly different than is expected" in out.getvalue())
  summary = tw.twin_summary
  assert approx_equal(summary.l_mean, 0.4989738)
  assert approx_equal(summary.patterson_height, 47.152352)
  for k, twin_r_factor in enumerate(summary.r_obs):
    if (k == 6) : assert twin_r_factor < 0.11
    else : assert twin_r_factor > 0.3
  out2 = StringIO()
  tw.l_test.show(out=out2)
  assert ("""\
  Mean |L|   :0.499  (untwinned: 0.500; perfect twin: 0.375)
  Mean  L^2  :0.372  (untwinned: 0.333; perfect twin: 0.200)
""" in out2.getvalue()), out2.getvalue()
  out3 = StringIO()
  tw2.l_test.show(out=out3)
  assert not show_diff(out2.getvalue(), out3.getvalue())
  if (verbose):
    print(out.getvalue())
  # twin_results_interpretation object via cctbx.miller.array API extension
  # XXX I get slightly different numbers here versus running through the
  # twin_analyses call above - this seems to be caused by the resolution
  # limits passed here.  Need to confirm these with PHZ.
  result = fc_twin.analyze_intensity_statistics()
  assert approx_equal(result.maha_l, 27.580674)
  assert approx_equal(result.r_obs,
    [0.514901, 0.514901, 0.397741, 0.580353, 0.579294, 0.420914, 0.114999])
  assert approx_equal(result.h_alpha,
    [0.019980, 0.019980, 0.211788, 0.073926, 0.079920, 0.150849, 0.389610])
  assert result.has_twinning()

if (__name__ == "__main__"):
  exercise_twin_detection(verbose=("--verbose" in sys.argv))
  print("OK")


 *******************************************************************************


 *******************************************************************************
mmtbx/scaling/twin_analyses.py

from __future__ import absolute_import, division, print_function
from mmtbx.scaling import absolute_scaling
from mmtbx import scaling
from iotbx import data_plots
from cctbx.sgtbx import pointgroup_tools
from cctbx.array_family import flex
import cctbx.sgtbx.lattice_symmetry
from cctbx import crystal
import cctbx.sgtbx.cosets
from cctbx import maptbx
from cctbx import sgtbx
import cctbx.xray
import scitbx.math
from scitbx import matrix
from libtbx import slots_getstate_setstate, Auto
from libtbx.str_utils import format_value
from libtbx.utils import Sorry, null_out
from libtbx import table_utils
from six.moves import cStringIO as StringIO
import math
import sys
from six.moves import zip
from six.moves import range

# some cutoffs that may need to be adjusted
TWIN_FRAC_SIGNIFICANT = 0.05
PATT_HEIGHT_MISSED_CENTERING = 75
PATT_HEIGHT_TNCS = 0.2

class obliquity(slots_getstate_setstate):
  __slots__ = ["type", "u", "h", "t", "tau", "delta"]
  def __init__(self, reduced_cell, rot_mx, deg=True):
    orth = matrix.sqr(reduced_cell.orthogonalization_matrix())
    frac = matrix.sqr(reduced_cell.fractionalization_matrix())
    r_info = rot_mx.info()
    self.type = r_info.type()
    self.u = rot_mx.info().ev()
    self.h = rot_mx.transpose().inverse().info().ev()
    self.t = orth * matrix.col(self.u)
    self.tau = matrix.row(self.h) * frac
    self.delta = self.t.accute_angle(self.tau, deg=deg)

class twin_law_quality(object):
  """
  Various scores for a potential twin law given the crystal lattice.
  """
  def __init__(self,
               xs,
               twin_law):
    self.xs = xs
    self.xs_niggli = xs.change_basis(
      xs.change_of_basis_op_to_niggli_cell())
    self.twin_in_nig = xs.change_of_basis_op_to_niggli_cell().apply(
      twin_law)
    self.cb_op = sgtbx.change_of_basis_op(
      symbol = self.twin_in_nig.r().as_xyz() )
    self.niggli_cell = self.xs_niggli.unit_cell()
    self.new_niggli_cell = self.xs_niggli.change_basis( self.cb_op ).unit_cell()

  def delta_santoro(self):
    # santoros measure for quality of a twin law
    # relative (default) or absolute is possible
    old = self.niggli_cell.metrical_matrix()
    new = self.new_niggli_cell.metrical_matrix()
    old = flex.double( old )
    new = flex.double( new )
    delta = flex.abs(old - new)
    result = 0
    top = 0
    bottom = 0
    for oi, ni in zip(old,new):
      tmp = math.fabs(oi-ni)
      top += tmp
      bottom += math.fabs(oi)
    assert (bottom>0)
    delta = 100.0*top/bottom
    return( delta )

  def delta_le_page(self):
    rot_mx_current = self.cb_op.c().r().new_denominator(1)
    obl = obliquity(self.niggli_cell,
                    rot_mx_current )
    type_string = str(obl.type)+"-fold"
    return obl.delta, type_string

  def delta_lebedev(self):
    return self.twin_in_nig.r().lebedev_2005_perturbation(
      reduced_cell=self.niggli_cell)

  def strain_tensor(self):
    """
    this gives a tensor describing the deformation of the unit cell needed
    to obtain a perfect match. the sum of diagonal elements describes
    the change in volume, off diagonal components measure associated shear.
    """
    x1 = matrix.col( (1,0,0) )
    x2 = matrix.col( (0,1,0) )
    x3 = matrix.col( (0,0,1) )
    f1 = matrix.sqr(self.niggli_cell.fractionalization_matrix() )*x1
    f2 = matrix.sqr(self.niggli_cell.fractionalization_matrix() )*x2
    f3 = matrix.sqr(self.niggli_cell.fractionalization_matrix() )*x3
    xn1 =  matrix.sqr(self.new_niggli_cell.orthogonalization_matrix() )*f1
    xn2 =  matrix.sqr(self.new_niggli_cell.orthogonalization_matrix() )*f2
    xn3 =  matrix.sqr(self.new_niggli_cell.orthogonalization_matrix() )*f3
    u1 = math.sqrt( xn1.dot(xn1) )-1.0
    u2 = math.sqrt( xn2.dot(xn2) )-1.0
    u3 = math.sqrt( xn3.dot(xn3) )-1.0
    e11 = u1/1.0
    e22 = u2/1.0
    e33 = u3/1.0
    e12 = (u1 + u2)/2.0
    e13 = (u1 + u3)/2.0
    e23 = (u2 + u3)/2.0
    result=matrix.sqr( [e11,e12,e13,
                        e12,e22,e23,
                        e13,e23,e33 ] )
    return result

## python routines copied from iotbx.reflection_statistics.
## Should be moved but are (for now) in a conveniant place.
class twin_law(slots_getstate_setstate):
  """
  Basic container for information about a possible twin law, with scores for
  fit to crystal lattice.
  """
  __slots__ = ["operator", "twin_type", "delta_santoro", "delta_le_page",
               "delta_lebedev", "axis_type"]
  def __init__(self,
               op,
               pseudo_merohedral_flag,
               axis_type,
               delta_santoro,
               delta_le_page,
               delta_lebedev):
    self.operator =  op
    self.twin_type = pseudo_merohedral_flag
    self.delta_santoro = delta_santoro
    self.delta_le_page = delta_le_page
    self.delta_lebedev = delta_lebedev
    self.axis_type = axis_type

  def __str__(self):
    return str(self.operator.r().as_hkl())

class twin_laws(scaling.xtriage_analysis):
  """
  Container for all possible twin laws given a crystal lattice and space
  group.
  """
  def __init__(self,
               miller_array,
               lattice_symmetry_max_delta=3.0,
               out=None):
    input = miller_array.eliminate_sys_absent(integral_only=True,
        log=out)
    self.change_of_basis_op_to_niggli_cell = \
      input.change_of_basis_op_to_niggli_cell()
    minimum_cell_symmetry = input.crystal_symmetry().change_basis(
      cb_op=self.change_of_basis_op_to_niggli_cell)
    self.lattice_group = sgtbx.lattice_symmetry.group(
      reduced_cell=minimum_cell_symmetry.unit_cell(),
      max_delta=lattice_symmetry_max_delta)
    intensity_symmetry = minimum_cell_symmetry.reflection_intensity_symmetry(
        anomalous_flag=input.anomalous_flag())
    self.euclid = intensity_symmetry.space_group_info().type()\
      .expand_addl_generators_of_euclidean_normalizer(flag_k2l=True,
                                                      flag_l2n=True )
    self.operators = []
    self.m = 0  # number of merohedral twin laws
    self.pm = 0 # number of pseudo-merohedral twin laws
    cb_op = self.change_of_basis_op_to_niggli_cell.inverse()
    for partition in sgtbx.cosets.left_decomposition(
        g = self.lattice_group,
        h = intensity_symmetry.space_group()
            .build_derived_acentric_group()
            .make_tidy()).partitions[1:] :
      if (partition[0].r().determinant() > 0):
        is_pseudo_merohedral=False
        twin_type =str("  M")
        self.m+=1
        euclid_check = sgtbx.space_group( self.euclid )
        try:
          euclid_check.expand_smx( partition[0] )
        except KeyboardInterrupt: raise
        except Exception:
          is_pseudo_merohedral=True
          twin_type = str(" PM")
          self.pm+=1
          self.m-=1
        if ( euclid_check.order_z() != self.euclid.order_z() ):
          is_pseudo_merohedral=True
          if is_pseudo_merohedral:
            twin_type = str(" PM")
            self.pm+=1
            self.m-=1
        tlq = twin_law_quality(
          xs=miller_array,
          twin_law=cb_op.apply(partition[0]))
        tl = twin_law(
          op=cb_op.apply(partition[0]),
          pseudo_merohedral_flag=str(twin_type),
          axis_type=tlq.delta_le_page()[1],
          delta_santoro=tlq.delta_santoro(),
          delta_le_page=tlq.delta_le_page()[0],
          delta_lebedev=tlq.delta_lebedev())
        self.operators.append(tl)
    table_rows = []
    for tl in self.operators:
      table_rows.append([
        tl.twin_type,
        tl.axis_type,
        "%5.3f"%(tl.delta_santoro),
        "%5.3f"%(tl.delta_le_page),
        "%5.3f"%(tl.delta_lebedev),
        str(tl.operator.r().as_hkl())
      ])
    self.table = table_utils.simple_table(
      column_headers=['Type', 'Axis', 'R metric (%)', 'delta (le Page)',
                      'delta (Lebedev)', 'Twin law'],
      table_rows=table_rows,
      comments="""\
M:  Merohedral twin law
PM: Pseudomerohedral twin law""")
    self.coset_table = sgtbx.cosets.left_decomposition(self.lattice_group,
      intensity_symmetry.space_group().build_derived_acentric_group()\
            .make_tidy())
    self.coset_decomposition = False
    try:
      tmp = self.lattice_group.change_basis(
          self.change_of_basis_op_to_niggli_cell.inverse() )
      self.coset_decomposition = True
    except Exception:
      pass

  def _show_impl(self, out):
    assert (self.m + self.pm)==len(self.operators)
    out.show_sub_header("Twin law identification")
    if (len(self.operators) == 0):
      out.show("\nNo twin laws are possible for this crystal lattice.\n")
    else:
      out.show_paragraph_header("Possible twin laws:")
      out.show_table(self.table, indent=2)
      out.show_lines("""
%(m)-3.0f merohedral twin operators found
%(pm)-3.0f pseudo-merohedral twin operators found
In total, %(n_op)3.0f twin operators were found""" %
        ({ "m" : self.m, "pm" : self.pm, "n_op" : len(self.operators) }))
      out.show("""
 Please note that the possibility of twin laws only means that the lattice
 symmetry permits twinning; it does not mean that the data are actually
 twinned.  You should only treat the data as twinned if the intensity
 statistics are abnormal.""")
    # XXX disabled 2014-06-28
    if False :
      out.show("""
 The presence of twin laws indicates that the symmetry of the lattice (unit
 cell) is higher (has more elements) than the point group of the assigned
 space group.""")
      out.show("""
 There are four likely scenarios associated with the presence of twin laws:""")
      out.show_lines("""
    i.  The assigned space group is incorrect (too low).
   ii.  The assigned space group is correct and the data *are not* twinned.
  iii.  The assigned space group is correct and the data *are* twinned.
   iv.  The assigned space group is not correct (too low) and at the same time,
        the data *are* twinned.""")
      out.show("""
 Xtriage tries to distinguish between these cases by inspecting the intensity
 statistics.  It never hurts to carefully inspect statistics yourself and make
 sure that the automated interpretation is correct.""")

      if self.coset_decomposition :
        out.show_sub_header("Details of automated twin law derivation")
        out.show("""
Below, the results of the coset decomposition are given.  Each coset represents
a single twin law, and all symmetry equivalent twin laws are given.  For each
coset, the operator in (x,y,z) and (h,k,l) notation are given.   The direction
of the axis (in fractional coordinates), the type and possible offsets are
given as well.  Furthermore, the result of combining a certain coset with the
input space group is listed.

This table can be usefull when comparing twin laws generated by xtriage with
those listed in lookup tables.  In the table subgroup H denotes the *presumed
intensity symmetry*. Group G is the symmetry of the lattice.
""")
        coset_out = StringIO()
        self.coset_table.show(out=coset_out,
          cb_op=self.change_of_basis_op_to_niggli_cell.inverse())
        out.show_preformatted_text(coset_out.getvalue())
        out.show("""\
Note that if group H is centered (C,P,I,F), elements corresponding to centering
operators are omitted.  (This is because internally the calculations are done
with the symmetry of the reduced cell.)
""")
      else: # lattice_group.change_basis() failed
        out.show("""
The present crystal symmetry does not allow to have the its lattice symmetry
expressed in the setting desired.  Becasue of this, a full coset table cannot
be produced. Working with the data in the reduced cell will solve this.
""")

def get_twin_laws(miller_array):
  """
  Convenience method for getting a list of twin law operators (as strings)
  """
  return [ str(tl) for tl in twin_laws(miller_array).operators ]

class wilson_normalised_intensities(scaling.xtriage_analysis):
  """ making centric and acentric cut """
  def __init__(self,
               miller_array,
               normalise=True,
               out=None,
               verbose=0):
    if out is None:
      out = sys.stdout
    assert not miller_array.space_group().is_centric()
    if not miller_array.is_xray_intensity_array():
      miller_array = miller_array.f_as_f_sq()
    work_array =  miller_array.deep_copy()
    if normalise:
      normalizer = absolute_scaling.kernel_normalisation(
        miller_array, auto_kernel=True)
      work_array = normalizer.normalised_miller.deep_copy()
      work_array = work_array.select(work_array.data()>0)
    else:
      work_array = miller_array.deep_copy().set_observation_type(miller_array)
      work_array = work_array.select(work_array.data()>0)
    self.all = work_array
    self.acentric = work_array.select_acentric().as_intensity_array()
    self.centric = work_array.select_centric().as_intensity_array()
    if (self.acentric.indices().size()<=0):
      raise Sorry("No acentric reflections available. Check your input")

  def _show_impl(self, out):
    out.show("Splitting data in centrics and acentrics")
    out.show("  Number of centrics  : %d" % self.centric.data().size())
    out.show("  Number of acentrics : %d", self.acentric.data().size())

########################################################################
# DIAGNOSTIC TESTS
########################################################################

class detect_pseudo_translations(scaling.xtriage_analysis):
  """
  Analyze the Patterson map to identify off-origin peaks that are a significant
  fraction of the origin peak height.
  """
  def __init__(self,
      miller_array, # ideally amplitudes
      low_limit=10.0,
      high_limit=5.0,
      max_sites=100,
      height_cut=0.0,
      distance_cut=15.0,
      p_value_cut=0.05,
      completeness_cut=0.75,
      cut_radius=3.5,
      min_cubicle_edge=5.0,
      completeness_as_non_anomalous=None,
      out=None,
      verbose=0):
    if out is None:
      out=sys.stdout
    if miller_array.is_xray_intensity_array():
      miller_array = miller_array.f_sq_as_f()
    work_array = miller_array.resolution_filter(low_limit,high_limit)
    work_array = work_array.select(work_array.data()>0).set_observation_type(
      miller_array)
    self.space_group = miller_array.space_group()
    self.unit_cell = miller_array.unit_cell()
    self.d_max = low_limit
    self.d_min = high_limit
    self.completeness_in_range = work_array.completeness(
      as_non_anomalous_array = completeness_as_non_anomalous)
    self.xs = crystal.symmetry(unit_cell=miller_array.unit_cell(),
                               space_group=miller_array.space_group())

    if work_array.completeness(
      as_non_anomalous_array = completeness_as_non_anomalous) \
         <completeness_cut:
      print(file=out)
      print(" WARNING (twin_analysis):", file=out)
      print("  The completeness is only %3.2f between %3.1f and %3.1f A."% (
        work_array.completeness(
          as_non_anomalous_array = completeness_as_non_anomalous
          ), low_limit, high_limit), file=out)
      print("  This might not be enough to obtain a good estimate", file=out)
      print("  of the presence or absence of pseudo translational", file=out)
      print("  symmetry.", file=out)
    if work_array.indices().size()==0:
      raise Sorry("No low resolution reflections")

    if work_array.anomalous_flag():
      work_array = work_array.average_bijvoet_mates().set_observation_type(
        miller_array)
    everything_okay = True
    if (work_array.indices().size()<0):
      print("The number of reflection between %3.1f and %3.1f Angstrom" \
         %( low_limit,
            high_limit ), file=out)
      print("is equal to %i" %(work_array.indices().size()), file=out)
      print(" ##  This is not enough to obtain a reasonable estimate of", file=out)
      print(" ##  the presence of translational NCS", file=out)
      everything_okay = False

    if everything_okay:
      patterson_map = work_array.patterson_map(
        symmetry_flags=maptbx.use_space_group_symmetry).apply_sigma_scaling()
      peak_search_parameters = maptbx.peak_search_parameters(
        peak_search_level=1,
        interpolate=True,
        min_distance_sym_equiv=1e-4,
        general_positions_only=False,
        effective_resolution=None,
        min_cross_distance=cut_radius,
        min_cubicle_edge=min_cubicle_edge)
      cluster_analysis = patterson_map.peak_search(
        parameters=peak_search_parameters)
      peak_list = cluster_analysis.all(max_clusters=1000)
      max_height = peak_list.heights()[0]
      sym_equiv_origin = sgtbx.sym_equiv_sites(
        unit_cell=patterson_map.unit_cell(),
        space_group=patterson_map.space_group(),
        original_site=(0,0,0))
      self.suspected_peaks = []
      if max_sites > peak_list.sites().size():
        max_sites = peak_list.sites().size()
      for i_peak in range(max_sites):
        height = peak_list.heights()[i_peak]/max_height*100.0
        site = peak_list.sites()[i_peak]
        dist_info = sgtbx.min_sym_equiv_distance_info(sym_equiv_origin, site)
        if (dist_info.dist() >= distance_cut):
          if (height >= height_cut):
            p_value = self.p_value(height)
            self.suspected_peaks.append( [dist_info.sym_op()*site,
                                          height, p_value,
                                          dist_info.dist()] )
      if len(self.suspected_peaks)==0:
        print(file=out)
        print("No Patterson vectors with a length larger than", file=out)
        print("%5.2f found. removing distance constraint"%(distance_cut), file=out)
        print(file=out)
        distance_cut = 1e-3
        for i_peak in range(max_sites):
          height = peak_list.heights()[i_peak]/max_height*100.0
          site = peak_list.sites()[i_peak]
          dist_info = sgtbx.min_sym_equiv_distance_info(sym_equiv_origin, site)
          if (dist_info.dist() >= distance_cut):
            if (height >= height_cut):
              p_value = self.p_value(height)
              self.suspected_peaks.append( [dist_info.sym_op()*site,
                                            height, p_value,
                                            dist_info.dist()] )
      self.p_value_cut = p_value_cut
      self.mod_h = 2
      self.mod_k = 2
      self.mod_l = 2
      self.suggested_space_groups_table = None
      if everything_okay:
        self.high_peak = self.suspected_peaks[0][1]
        self.high_peak_distance = self.suspected_peaks[0][3]
        self.high_peak_xyz = self.suspected_peaks[0][0]
        self.high_p_value = self.suspected_peaks[0][2]
        if( self.high_p_value <= self.p_value_cut):
          self.guesstimate_mod_hkl()

  def suggest_new_space_groups(self,t_den=144,out=None):
    symops = []
    sgs = []
    with_operator = []
    new_cell = []
    for peak in self.suspected_peaks:
      if peak[2] < self.p_value_cut:
        xyz = peak[0]
        dx = self.closest_rational( xyz[0] )
        dy = self.closest_rational( xyz[1] )
        dz = self.closest_rational( xyz[2] )
        additional_symop = None
        if ([dx,dy,dz]).count(None)==0:
          additional_symop = "x%s, y%s, z%s"%(
            dx, dy, dz )
          if additional_symop is not None:
            symops.append( additional_symop )
    tmp_space_group = sgtbx.space_group_info( group=self.space_group )
    try:
      tmp_space_group = sgtbx.space_group_info( str(tmp_space_group),
        space_group_t_den=t_den)
    except KeyboardInterrupt: raise
    except Exception : pass

    for so in symops:
      sg_str = None
      try:
        new_tmp_sg = tmp_space_group.group().make_tidy()
        smx = sgtbx.rt_mx( so )
        smx = smx.new_denominators( new_tmp_sg.r_den(), new_tmp_sg.t_den() )
        new_tmp_sg.expand_smx( smx )
        sg_str = str( sgtbx.space_group_info( group = new_tmp_sg,
          space_group_t_den=t_den  ) )
        to_ref_set = sgtbx.space_group_info(
          group=new_tmp_sg,
          space_group_t_den=t_den).change_of_basis_op_to_reference_setting()
      except Exception: pass
      if sg_str not in sgs:
        if sg_str is not None:
          sgs.append( sg_str )
          with_operator.append( so )
          new_cell.append( self.unit_cell.change_basis(
            to_ref_set.c_inv().r().as_double() ) )
        else:
          sgs.append( None )
          new_cell.append( self.unit_cell )
    number_of_new_sgs = len(symops)-sgs.count(None)
    if number_of_new_sgs>0:
      table_rows = []
      for sg,op,uc in zip(sgs,with_operator,new_cell):
        if sg is not None:
          table_rows.append([str(sg), str(op),
            "%5.2f, %5.2f, %5.2f,  %5.2f, %5.2f, %5.2f)"% uc.parameters() ])
      self.suggested_space_groups_table = table_utils.simple_table(
        table_rows=table_rows,
        column_headers=("Space group", "Operator",
          "Unit cell of reference setting"))

  def guesstimate_mod_hkl(self):
    tmp_mod_h = 1.0/(self.high_peak_xyz[0]+1.0e-6)
    tmp_mod_k = 1.0/(self.high_peak_xyz[1]+1.0e-6)
    tmp_mod_l = 1.0/(self.high_peak_xyz[2]+1.0e-6)
    tmp_mod_h = int(math.fabs(tmp_mod_h)+0.5)
    if (tmp_mod_h>=8):
      tmp_mod_h = 2
    tmp_mod_k = int(math.fabs(tmp_mod_k)+0.5)
    if (tmp_mod_k>=8):
      tmp_mod_k = 2
    tmp_mod_l = int(math.fabs(tmp_mod_l)+0.5)
    if (tmp_mod_l>=8):
      tmp_mod_l = 2
    self.mod_h = tmp_mod_h
    self.mod_k = tmp_mod_k
    self.mod_l = tmp_mod_l

  def p_value(self, peak_height):
    x= peak_height/100.0
    result=None
    if x<1.0:
      x = x/(1.0-x)
      a = 0.06789
      b = 3.5628
      result = 1.0 - math.exp(- ((x/a)**(-b)) )
    else:
      result=0.0
    return result

  def closest_rational(self, fraction, eps=0.02,return_text=True):
    tmp_fraction = abs(fraction)
    sign = "+"
    if fraction < 0:
      sign = "-"

    num = None
    den = None
    den_list = [2,3,4,5,6]
    fraction_trials = []
    min_del=10.0
    best_frac=None
    for trial_den in den_list:
      start_num=0
      if trial_den > 2:
        start_num = 1
      for trial_num in range(start_num,trial_den):
        tmp = (sign, trial_num, trial_den)
        tmp_frac = trial_num/trial_den
        delta = abs(tmp_frac - tmp_fraction )
        if delta < min_del:
          best_frac = tmp
          min_del = float( delta )
    result = None
    if min_del < eps:
      if return_text:
        if best_frac[1]==0:
          result = ""
        else:
          result = "%s%s/%s"%(best_frac[0],best_frac[1],best_frac[2])
      else:
        result = best_frac
    return result

  def _show_impl(self, out):
    out.show_sub_header("Patterson analyses")
    out.show(" Largest Patterson peak with length larger than 15 Angstrom:")
    out.show_preformatted_text(" Frac. coord.              : %8.3f %8.3f %8.3f"
      % self.high_peak_xyz)
    out.show_preformatted_text("""\
 Distance to origin        : %8.3f
 Height relative to origin : %8.3f %%
 p_value(height)           : %12.3e
""" % (self.high_peak_distance,self.high_peak, self.high_p_value))
    out.show_paragraph_header("Explanation")
    out.show("""\
 The p-value, the probability that a peak of the specified height or larger
 is found in a Patterson function of a macromolecule that does not have any
 translational pseudo-symmetry, is equal to %.3e.  p_values smaller than
 0.05 might indicate weak translational pseudo symmetry, or the self vector of
 a large anomalous scatterer such as Hg, whereas values smaller than 1e-3 are
 a very strong indication for the presence of translational pseudo symmetry.
""" % self.high_p_value)
    if (self.high_peak >= 20):
      out.show_text("""\
 Translational pseudo-symmetry is very likely present in these data.  Be
 aware that this will change the intensity statistics and may impact subsequent
 analyses, and in practice may lead to higher R-factors in refinement.
""")
    if (self.high_p_value <= self.p_value_cut):
      table_rows = []
      for ii in range(len(self.suspected_peaks)):
        if self.suspected_peaks[ii][2] > self.p_value_cut:
          break
        table_rows.append([
          "%6.3f,%6.3f,%6.3f" % self.suspected_peaks[ii][0],
          "%8.3f" % self.suspected_peaks[ii][1],
          "%9.3e" % self.suspected_peaks[ii][2] ])
      if (len(table_rows) > 1):
        out.show(" The full list of Patterson peaks is:")
        table = table_utils.simple_table(
          table_rows=table_rows,
          column_headers=["XYZ", "height", "p-value(height)"])
        out.show_table(table, indent=2)
      if (self.suggested_space_groups_table is not None):
        out.show("""\
 If the observed pseudo translationals are crystallographic, the following
 spacegroups and unit cells are possible:
""")
        out.show_table(self.suggested_space_groups_table, indent=2)

class wilson_moments(scaling.xtriage_analysis):
  centric_i_ratio_library= [3.0,2.0]
  centric_f_ratio_library = [0.637,0.785]
  centric_e_sq_minus_one_library = [0.968,0.736]
  acentric_i_ratio_library= [2.0,1.5]
  acentric_f_ratio_library = [0.785,0.885]
  acentric_e_sq_minus_one_library = [0.736, 0.541]
  def __init__(self,
               acentric_z,
               centric_z):
    self.centric_i_ratio = None
    self.centric_f_ratio = None
    self.centric_e_sq_minus_one = None
    self.acentric_i_ratio = None
    self.acentric_f_ratio = None
    self.acentric_abs_e_sq_minus_one = None
    self.compute_ratios(
      acentric_z.data()/acentric_z.epsilons().data().as_double(),
      centric_z.data()/centric_z.epsilons().data().as_double())
    self.centric_present = True
    if centric_z.data().size()==0:
      self.centric_present=False

  def compute_ratios(self, ac, c):
    if (ac.size()>0):
      mean_z = flex.mean( ac )
      mean_z_sq = flex.mean( ac*ac )
      mean_e = flex.mean( flex.sqrt(ac) )
      self.acentric_i_ratio = mean_z_sq / (mean_z*mean_z)
      self.acentric_f_ratio = mean_e*mean_e/mean_z
      self.acentric_abs_e_sq_minus_one = flex.mean( flex.abs(ac - 1.0) )
    if (c.size()>0):
      mean_z = flex.mean( c )
      mean_z_sq = flex.mean( c*c )
      mean_e = flex.mean( flex.sqrt(c) )
      self.centric_i_ratio = mean_z_sq / (mean_z*mean_z)
      self.centric_f_ratio = mean_e*mean_e/mean_z
      self.centric_abs_e_sq_minus_one = flex.mean( flex.abs(c - 1.0) )

  def _show_impl(self, out):
    out.show_sub_header("Wilson ratio and moments")
    out.show("""Acentric reflections:\n""")
    out.show_preformatted_text("""
   <I^2>/<I>^2    :%4.3f   (untwinned: %4.3f; perfect twin %4.3f)
   <F>^2/<F^2>    :%4.3f   (untwinned: %4.3f; perfect twin %4.3f)
   <|E^2 - 1|>    :%4.3f   (untwinned: %4.3f; perfect twin %4.3f)
""" % (self.acentric_i_ratio,
       self.acentric_i_ratio_library[0],
       self.acentric_i_ratio_library[1],
       self.acentric_f_ratio,
       self.acentric_f_ratio_library[0],
       self.acentric_f_ratio_library[1],
       self.acentric_abs_e_sq_minus_one,
       self.acentric_e_sq_minus_one_library[0],
       self.acentric_e_sq_minus_one_library[1]))
    if self.centric_present:
      out.show("""Centric reflections:\n""")
      out.show_preformatted_text("""
   <I^2>/<I>^2    :%4.3f   (untwinned: %4.3f; perfect twin %4.3f)
   <F>^2/<F^2>    :%4.3f   (untwinned: %4.3f; perfect twin %4.3f)
   <|E^2 - 1|>    :%4.3f   (untwinned: %4.3f; perfect twin %4.3f)
""" % (self.centric_i_ratio,
       self.centric_i_ratio_library[0],
       self.centric_i_ratio_library[1],
       self.centric_f_ratio,
       self.centric_f_ratio_library[0],
       self.centric_f_ratio_library[1],
       self.centric_abs_e_sq_minus_one,
       self.centric_e_sq_minus_one_library[0],
       self.centric_e_sq_minus_one_library[1]))
    # TODO explanation, citation?

class n_z_test(scaling.xtriage_analysis):
  def __init__(self,
               normalised_acentric,
               normalised_centric):
    centric_available = True
    acentric_available = True
    if normalised_centric.data().size() == 0:
      centric_available = False
    if normalised_acentric.data().size() == 0:
      acentric_available = False
    n_z = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]
    ac_theory = flex.double([0.0000, 0.0952, 0.1813, 0.2592, 0.3297, 0.3935,
                             0.4512, 0.5034, 0.5507, 0.5934, 0.6321])
    c_theory =  flex.double([0.0000, 0.2481, 0.3453, 0.4187, 0.4738, 0.5205,
                             0.5614, 0.5972, 0.6289, 0.6572, 0.6833])
    ac_obs = flex.double(11,0)
    c_obs = flex.double(11,0)
    for ii in range(10):
      ac_obs[ii+1] = (
        (normalised_acentric.data() < (ii+1.0)/10.0) ).count(True)
      if (centric_available):
        c_obs[ii+1] = (
          (normalised_centric.data() < (ii+1.0)/10.0) ).count(True)
    if acentric_available:
      ac_obs = ac_obs/float(normalised_acentric.data().size())
    if centric_available:
      c_obs = c_obs/float(normalised_centric.data().size())
    max_deviation_ac = flex.max( flex.abs(ac_obs-ac_theory)  )
    max_deviation_c = flex.max( flex.abs(c_obs-c_theory) )
    n_z_less_then_one_ac = (
      normalised_acentric.data() < 1.0  ).count(True)
    n_z_less_then_one_c =(
      normalised_centric.data() < 1.0  ).count(True)
    d_kolmogorov_smirnov_ac = max_deviation_ac/ac_theory[10]*math.sqrt(
      n_z_less_then_one_ac )
    d_kolmogorov_smirnov_c = max_deviation_c/c_theory[10]*math.sqrt(
      n_z_less_then_one_c  )
    z = flex.double(range(11))/10.0
    self.z = z
    self.ac_obs = ac_obs
    self.ac_untwinned = ac_theory
    self.frac_ac_lt_1 = 0
    if normalised_acentric.data().size()>0:
      self.frac_ac_lt_1 = n_z_less_then_one_ac/normalised_acentric.data().size()
    self.max_diff_ac = max_deviation_ac
    self.kolmogorov_smirnoff_ac = max_deviation_ac/ac_theory[10]*math.sqrt(
      n_z_less_then_one_ac )
    self.c_obs = c_obs
    self.c_untwinned = c_theory
    self.frac_c_lt_1 = 0
    if normalised_centric.data().size()>0:
      self.frac_c_lt_1 = n_z_less_then_one_c/normalised_centric.data().size()
    self.max_diff_c = max_deviation_c
    self.kolmogorov_smirnoff_c = max_deviation_c/c_theory[10]*math.sqrt(
      n_z_less_then_one_c )
    self.mean_diff_ac = flex.sum(self.ac_obs - self.ac_untwinned)/11.0
    self.mean_diff_c = flex.sum(self.c_obs - self.c_untwinned)/11.0

  @property
  def table(self):
    return data_plots.table_data(
      title = "NZ test",
      column_labels=["z", "Acentric observed", "Acentric untwinned",
        "Centric observed", "Centric untwinned"],
      graph_names=["NZ test"],
      graph_labels=[("Z", "P(Z<=z)")],
      graph_columns=[list(range(5))],
      # make sure these aren't flex arrays - JSON can't handle them
      data=[list(self.z), list(self.ac_obs), list(self.ac_untwinned),
            list(self.c_obs), list(self.c_untwinned) ])

  def _show_impl(self, out):
    out.show_sub_header("NZ test for twinning and TNCS")
    out.show("""
The NZ test is diagnostic for both twinning and translational NCS.  Note
however that if both are present, the effects may cancel each other out,
therefore the results of the Patterson analysis and L-test also need to be
considered.
""")
    self.sign_ac = '+'
    if self.mean_diff_ac < 0:
      self.sign_ac = '-'
    self.sign_c = '+'
    if self.mean_diff_c < 0:
      self.sign_c = '-'
    out.show_preformatted_text("""
  Maximum deviation acentric      :  %(max_diff_ac)4.3f
  Maximum deviation centric       :  %(max_diff_c)4.3f

  <NZ(obs)-NZ(twinned)>_acentric  : %(sign_ac)1s%(mean_diff_ac)4.3f
  <NZ(obs)-NZ(twinned)>_centric   : %(sign_c)1s%(mean_diff_c)4.3f
""" % { "max_diff_ac" : self.max_diff_ac,
        "max_diff_c" : self.max_diff_c,
        "sign_ac" : self.sign_ac,
        "sign_c" : self.sign_c,
        "mean_diff_ac" : math.fabs(self.mean_diff_ac),
        "mean_diff_c" : math.fabs(self.mean_diff_c), })
    if (out.gui_output):
      out.show_plot(self.table)
      out.show_table(self.table)
    else :
      out.show_table(self.table)

class l_test(scaling.xtriage_analysis):
  """
  Implementation of:

  J. Padilla & T. O. Yeates. A statistic for local intensity differences:
  robustness to anisotropy and pseudo-centering and utility for detecting
  twinning. Acta Crystallogr. D59, 1124-30, 2003.

  This is complementary to the NZ test, but is insensitive to translational
  pseuo-symmetry.
  """
  def __init__(self,
      miller_array,
      parity_h=2,
      parity_k=2,
      parity_l=2):
    acentric_data = miller_array.select_acentric().set_observation_type(
      miller_array)
    if not miller_array.is_xray_intensity_array():
      acentric_data = acentric_data.f_as_f_sq()
    self.parity_h = parity_h
    self.parity_k = parity_k
    self.parity_l = parity_l
    l_stats = scaling.l_test(
      miller_indices=acentric_data.indices(),
      intensity=acentric_data.data()/\
        acentric_data.epsilons().data().as_double(),
      space_group=acentric_data.space_group(),
      anomalous_flag=acentric_data.anomalous_flag(),
      parity_h=parity_h,
      parity_k=parity_k,
      parity_l=parity_l,
      max_delta_h=8);
    self.mean_l = l_stats.mean_l()
    self.mean_l2 = l_stats.mean_l2()
    self.l_cumul = l_stats.cumul()
    self.l_values = flex.double(range(self.l_cumul.size()))/float(
      self.l_cumul.size())
    self.l_cumul_untwinned = self.l_values
    self.l_cumul_perfect_twin = self.l_values*(
      3.0-self.l_values*self.l_values)/2.0
    self.ml_alpha = l_stats.ml_alpha()

  @property
  def table(self):
    return data_plots.table_data(
      title="L test, acentric data",
      column_labels=["|l|", "Observed", "Acentric theory",
                     "Acentric theory, perfect twin"],
      graph_names=["L test"],
      graph_labels=[("|l|", "P(L>=l)")],
      graph_columns=[list(range(4))],
      data=[list(self.l_values), list(self.l_cumul),
            list(self.l_cumul_untwinned), list(self.l_cumul_perfect_twin)])

  def _show_impl(self, out):
    out.show_sub_header("L test for acentric data")
    out.show("""Using difference vectors (dh,dk,dl) of the form:""")
    out.show_preformatted_text(
      """    (%(parity_h)ihp, %(parity_k)ikp, %(parity_l)ilp)""" %
      { "parity_h" : self.parity_h,
        "parity_k" : self.parity_k,
        "parity_l" : self.parity_l })
    out.show("""where hp, kp, and lp are random signed integers such that""")
    out.show_preformatted_text("""    2 <= |dh| + |dk| + |dl| <= 8""")
    out.show_lines("""\
  Mean |L|   :%(mean_l)4.3f  (untwinned: 0.500; perfect twin: 0.375)
  Mean  L^2  :%(mean_l2)4.3f  (untwinned: 0.333; perfect twin: 0.200)""" %
      { "mean_l" : self.mean_l,
        "mean_l2" : self.mean_l2, })
    out.show("""
 The distribution of |L| values indicates a twin fraction of
 %(ml_alpha)3.2f. Note that this estimate is not as reliable as obtained
 via a Britton plot or H-test if twin laws are available.
""" % { "ml_alpha" : self.ml_alpha, })
    if (out.gui_output):
      out.show_plot(self.table)
      out.show_table(self.table)
    else :
      out.show_table(self.table)

    out.show("""\
 Reference:
  J. Padilla & T. O. Yeates. A statistic for local intensity differences:
  robustness to anisotropy and pseudo-centering and utility for detecting
  twinning. Acta Crystallogr. D59, 1124-30, 2003.
""")

########################################################################
# TWIN LAW-DEPENDENT TESTS
########################################################################

class britton_test(scaling.xtriage_analysis):
  def __init__(self,
               twin_law,
               miller_array, # ideally intensities!
               cc_cut_off=0.995,
               verbose=0):
    self.twin_law = twin_law
    self.max_iter=1000
    result = [0.5,1.0,0,0]
    miller_array = miller_array.select(
      miller_array.data() > 0).set_observation_type(miller_array)
    if not miller_array.is_xray_intensity_array():
      miller_array = miller_array.f_as_f_sq()
    britton_plot_array = []
    britton_plot_alpha = flex.double(range(50))/101.0
    detwin_object = scaling.detwin(miller_array.indices(),
                                   miller_array.data(),
                                   miller_array.sigmas(),
                                   miller_array.space_group(),
                                   miller_array.anomalous_flag(),
                                   twin_law)
    for ii in range(50):
      alpha = (ii)/101.0
      negative_fraction = detwin_object.detwin_with_alpha(alpha)
      britton_plot_array.append(negative_fraction)
    britton_plot_array = flex.double(britton_plot_array)
    britton_plot_array = britton_plot_array - britton_plot_array[0]
    if flex.min( britton_plot_array )==flex.max( britton_plot_array ):
      not_done=False
      estimated_alpha = 0.5
    else:
      estimated_alpha = 0.0
      not_done=True
    icount=0
    while not_done:
      icount+=1
      for ii in range(48):
        alpha = ii/101.0
        britton_range = (flex.double(range(ii,50)))/101.0
        britton_obs = britton_plot_array[ii:50]
        result = self.get_alpha(britton_range,britton_obs)
        if result[1]>=cc_cut_off:
          estimated_alpha = result[0]
          not_done=False
          break
      cc_cut_off-=0.005
      if icount > self.max_iter: # a nasty fail safe
        break
    ## reset the cc_cut_off one step back
    cc_cut_off+=0.005
    self.alpha_cut = ii/101.0
    britton_plot_fit = flex.double(50,0)
    for ii in range(50):
      alpha = ii/101.0
      if (alpha<estimated_alpha):
        britton_plot_fit[ii]=0.0
      else:
        britton_plot_fit[ii]= result[2] + alpha*result[3]
    self.estimated_alpha = estimated_alpha
    self.correlation = result[1]
    self.britton_alpha =  britton_plot_alpha
    self.britton_obs = britton_plot_array
    self.britton_fit = britton_plot_fit

  def get_alpha(self, x, y):
    assert x.size() == y.size()
    mean_x = flex.mean(x)
    mean_y = flex.mean(y)
    var_x = flex.mean(x*x)-mean_x*mean_x
    var_y = flex.mean(y*y)-mean_y*mean_y
    covar_xy = flex.mean(x*y)-mean_x*mean_y
    N = float(x.size())
    m = flex.sum(x*x)- N*mean_x*mean_x
    b = covar_xy/(var_x+1.0e-6)
    a = mean_y - b*mean_x
    correlation = covar_xy/(math.sqrt(abs(var_x*var_y))+1.0e-6)
    return [-a/(b+1.0e-6) ,  correlation, a, b]

  @property
  def table(self):
    return data_plots.table_data(
        title = "Britton plot for twin law %s" % str(self.twin_law),
        column_labels=["alpha", "percentage negatives", "fit"],
        graph_names=["Britton plot"],
        graph_labels=[("alpha", "percentage negatives")],
        graph_columns=[[0,1,2]],
        reference_marks=[[self.estimated_alpha]],
        data=[self.britton_alpha, self.britton_obs, self.britton_fit])

  def _show_impl(self, out):
    out.show_paragraph_header("Britton analyses")
    out.show("""
  Extrapolation performed on  %(alpha_cut)3.2f < alpha < 0.495
  Estimated twin fraction: %(estimated_alpha)4.3f
  Correlation: %(correlation)5.4f
""" % { "alpha_cut" : self.alpha_cut,
        "estimated_alpha" : self.estimated_alpha,
        "correlation" : self.correlation })


class h_test(scaling.xtriage_analysis):
  def __init__(self,
               twin_law,
               miller_array, # ideally intensities!
               fraction=0.50):
    self.fraction = fraction
    self.twin_law = twin_law
    miller_array = miller_array.select(
      miller_array.data()>0).set_observation_type(miller_array)
    if not miller_array.is_xray_intensity_array():
      miller_array = miller_array.f_as_f_sq()
    if miller_array.is_real_array():
      acentric_data =  miller_array.select_acentric().set_observation_type(
        miller_array)
      try :
        h_test_object  = scaling.h_test(
          miller_indices=acentric_data.indices(),
          intensity=acentric_data.data(),
          sigma=acentric_data.sigmas(),
          space_group=acentric_data.space_group(),
          anomalous_flag=acentric_data.anomalous_flag(),
          twin_law=twin_law,
          fraction=fraction)
      except ValueError:
        if miller_array.completeness() < 0.05 : # XXX could check for anomalous
          raise Sorry("These data are severely incomplete, which breaks the "+
            "H-test for twinning.  We recommend that you use a full data set "+
            "in Xtriage, otherwise the statistical analyses may be invalid.")
        else :
          raise
      self.mean_h = h_test_object.mean_h()
      self.mean_h2 = h_test_object.mean_h2()
      self.estimated_alpha = h_test_object.alpha()
      self.alpha_from_mean_h = (self.mean_h*2.0-1.0)/-2.0
      self.h_array = h_test_object.h_array()
      self.h_values = h_test_object.h_values()
      self.cumul_obs = h_test_object.h_cumul_obs()
      self.cumul_fit = h_test_object.h_cumul_fit()

  @property
  def table(self):
    return data_plots.table_data(
        title="H test for possible twin law %s" % str(self.twin_law),
        column_labels=["H", "Observed S(H)", "Fitted S(H)"],
        graph_names=["H test for acentric data"],
        graph_labels=[("H", "S(H)")],
        graph_columns=[[0,1,2]],
        reference_marks=[[self.estimated_alpha]],
        data=[self.h_array, self.cumul_obs, self.cumul_fit])

  def _show_impl(self, out):
    out.show_paragraph_header("""H-test on acentric data""")
    out.show_lines("""\
Only %(fraction)3.1f %% of the strongest twin pairs were used.

  mean |H| : %(mean_h)4.3f  (0.50: untwinned; 0.0: 50%% twinned)
  mean H^2 : %(mean_h2)4.3f  (0.33: untwinned; 0.0: 50%% twinned)

Estimation of twin fraction via mean |H|: %(alpha_from_mean_h)4.3f
Estimation of twin fraction via cum. dist. of H: %(estimated_alpha)4.3f
""" % { "fraction" : self.fraction*100.0,
        "mean_h" : self.mean_h,
        "mean_h2" : self.mean_h2,
        "alpha_from_mean_h" : self.alpha_from_mean_h,
        "estimated_alpha" : self.estimated_alpha })

  def __getstate__(self):
    """
    Pickling optimization - see note below
    """
    self.h_values = None
    return self.__dict__

class ml_murray_rust_with_ncs(object):
  def __init__(self,
               miller_array,
               twin_law,
               out,
               n_bins=10,
               calc_data=None,
               start_alpha=None):
    if out == None:
      out = sys.stdout
    self.twin_law = twin_law
    self.twin_cap = 0.45
    self.d_cap = 0.95
    assert miller_array.is_xray_intensity_array()
    assert miller_array.sigmas() is not None
    non_zero_sigma_sel = miller_array.sigmas() > 0
    tmp_miller_array = miller_array.deep_copy().select(
      non_zero_sigma_sel).set_observation_type(miller_array)
    # make a binner
    self.binner = tmp_miller_array.setup_binner( n_bins=n_bins )
    self.out = out
    self.f = None
    self.cycle = 0
    self.ml_object = scaling.ml_twin_with_ncs(
      tmp_miller_array.data(),
      tmp_miller_array.sigmas(),
      tmp_miller_array.indices(),
      self.binner.bin_indices(),
      tmp_miller_array.space_group(),
      tmp_miller_array.anomalous_flag(),
      twin_law,
      tmp_miller_array.unit_cell(),
      4 )
    self.n = 1+n_bins
    self.x = flex.double([-3])
    if start_alpha is not None:
      if start_alpha > 0.45:
         start_alpha = 0.40
      if start_alpha <=0 :
         start_alpha = 0.05
      tmp = -math.log(self.twin_cap/start_alpha-1.0)
      self.x = flex.double([tmp])

    for ii in range(n_bins):
      self.x.append( -1.0 - ii/20.0 )
    self._show_info(out=out)
    term_parameters = scitbx.lbfgs.termination_parameters(max_iterations=1000)
    self.minimizer = scitbx.lbfgs.run(target_evaluator=self,
      termination_params=term_parameters)
    scitbx.lbfgs.run(target_evaluator=self)
    self.log_likelihood = self.f
    self.twin_fraction = self.twin_cap/(1+math.exp(-(self.x[0])))
    table = []
    for ii in range(self.x.size()-1):
      d_ncs = self.x[ii+1]
      d_ncs = self.d_cap/( 1.0+math.exp(-d_ncs) )
      low = self.binner.bin_d_range(ii+1)[0]
      high = self.binner.bin_d_range(ii+1)[1]
      table.append(["%5.4f - %5.4f" % (low, high), "%4.3f" % d_ncs])
    self.table = table_utils.simple_table(
      table_rows=table,
      column_headers=["Resolution", "D_ncs"])
    self._show_results(out)
    if calc_data is not None:
      self.calc_correlation(
        obs=tmp_miller_array,
        calc=calc_data,
        out=out)

  def string_it(self,x):
    return str("%4.3f"%(x))

  def calc_correlation(self,obs,calc, out=sys.stdout):
    if calc.is_xray_amplitude_array():
      calc = calc.f_as_f_sq()
    calc = calc.common_set( other=obs )
    calc.use_binning_of( obs  )
    print("""
 The correlation of the calculated F^2 should be similar to the estimated
 values.

 Observed correlation between twin related, untwinned calculated F^2
 in resolutiuon ranges, as well as ewstimates D_ncs^2 values:
""", file=out)
    # now loop over all resolution bins, get twin related intensities and get
    # twin related intensities please
    print(" Bin    d_max     d_min     CC_obs   D_ncs^2 ", file=out)
    for i_bin in calc.binner().range_used():
      tmp_array = calc.select( calc.binner().bin_indices() == i_bin )
      tmp_r_class = scaling.twin_r( tmp_array.indices(),
                                    tmp_array.data(),
                                    tmp_array.space_group(),
                                    tmp_array.anomalous_flag(),
                                    self.twin_law )
      low = self.binner.bin_d_range(i_bin)[0]
      high = self.binner.bin_d_range(i_bin)[1]
      d_theory_sq = tmp_r_class.correlation()
      d_ncs_sq = self.x[i_bin]
      d_ncs_sq = self.d_cap/( 1.0+math.exp(-d_ncs_sq) )
      d_ncs_sq = d_ncs_sq*d_ncs_sq
      print("%3i)    %5.4f -- %5.4f :: %6s   %6s" % (i_bin, low,
        high, self.string_it(d_theory_sq), self.string_it(d_ncs_sq)), file=out)

  def compute_functional_and_gradients(self):
    tmp = self.ml_object.p_tot_given_t_and_coeff( self.x[0],
                                                  self.x[1:] )
    f = tmp[0]
    g = tmp[1:]

    self.f=f
    self.cycle+=1
    if self.cycle%5==0:
      print("%3i "%(self.cycle), end=' ', file=self.out)
      #self.print_it()
    else:
      print(".", end=' ', file=self.out)
    if self.cycle%30==0:
      print(file=self.out)
    self.out.flush()
    return f,flex.double(g)

  def _show_info(self, out):
    if (not isinstance(out, scaling.xtriage_output)):
      out = scaling.printed_output(out)
    out.show_paragraph_header("Maximum Likelihood twin fraction determination")
    out.show("""
 Estimation of twin fraction, while taking into account the effects of
 possible NCS parallel to the twin axis.
     (Zwart, Read, Grosse-Kunstleve & Adams, to be published.)

 A parameters D_ncs will be estimated as a function of resolution, together
 with a global twin fraction.  D_ncs is an estimate of the correlation
 coefficient between untwinned, error-free, twin related, normalized
 intensities.  Large values (0.95) could indicate an incorrect point group.
 Value of D_ncs larger than say, 0.5, could indicate the presence of NCS. The
 twin fraction should be smaller or similar to other estimates given elsewhere.

 The refinement can take some time.  For numerical stability issues, D_ncs is
 limited between 0 and 0.95.  The twin fraction is allowed to vary between 0
 and 0.45.  Refinement cycle numbers are printed out to keep you entertained.
""")

  def _show_results(self, out):
    if (not isinstance(out, scaling.xtriage_output)):
      out = scaling.printed_output(out)
    out.show("""
  Cycle : %(cycle)3i
  Log[likelihood]: %(log_likelihood)15.3f
  twin fraction: %(twin_fraction)4.3f
  D_ncs in resolution ranges:
"""%{"cycle":self.cycle,"log_likelihood":self.log_likelihood,"twin_fraction":self.twin_fraction})
    out.show_table(self.table, indent=2)

  def _show_impl(self, out):
    self._show_info(out)
    self._show_results(out)

class ml_murray_rust(scaling.xtriage_analysis):
  """
  Maximum-likelihood twin fraction estimation (Zwart, Read, Grosse-Kunstleve &
  Adams, to be published).
  """
  def __init__(self,
               miller_array, # must be intensities
               twin_law,
               n_points = 4):
    assert miller_array.is_xray_intensity_array()
    assert miller_array.sigmas() is not None
    non_zero_sigma_sel = miller_array.sigmas() > 0
    tmp_array = miller_array.select(non_zero_sigma_sel)
    ml_murray_rust_object = scaling.ml_murray_rust(
      z=tmp_array.data(),
      sig_z=tmp_array.sigmas(),
      indices=tmp_array.indices(),
      space_group=tmp_array.space_group(),
      anomalous_flag=tmp_array.anomalous_flag(),
      twin_law=twin_law,
      n_hermite=n_points)
    self.twin_law = twin_law
    self.twin_fraction = []
    self.nll = []
    for ii in range(1,23):
      t=ii/46.0
      self.twin_fraction.append( t )
      self.nll.append( - ml_murray_rust_object.fast_log_p_given_t( t )  )
    i_t_max = flex.min_index( flex.double( self.nll ) )
    self.estimated_alpha = None
    if (i_t_max >= 1) and (i_t_max < len(self.nll)-1 ):
      tmp_t = [0,0,0]
      tmp_nll = [0,0,0]
      tmp_t[0] = self.twin_fraction[ i_t_max - 1 ]
      tmp_t[1] = self.twin_fraction[ i_t_max     ]
      tmp_t[2] = self.twin_fraction[ i_t_max + 1 ]
      tmp_nll[0] = self.nll[ i_t_max - 1 ]
      tmp_nll[1] = self.nll[ i_t_max ]
      tmp_nll[2] = self.nll[ i_t_max + 1 ]
      tmp_top = (tmp_t[2]**2.0)*(tmp_nll[0] - tmp_nll[1]) + \
                (tmp_t[1]**2.0)*(tmp_nll[1] - tmp_nll[2]) + \
                (tmp_t[0]**2.0)*(tmp_nll[2] - tmp_nll[0])
      tmp_bottom = (tmp_t[2])*(tmp_nll[0] - tmp_nll[1]) + \
                   (tmp_t[1])*(tmp_nll[1] - tmp_nll[2]) + \
                   (tmp_t[0])*(tmp_nll[2] - tmp_nll[0])
      self.estimated_alpha= tmp_top/(2.0*tmp_bottom)
      if ((self.estimated_alpha < tmp_t[0]) or
          (self.estimated_alpha > tmp_t[2])):
        self.estimated_alpha = self.twin_fraction[ i_t_max ]
    else:
      self.estimated_alpha = self.twin_fraction[ i_t_max ]

  @property
  def table(self):
    return data_plots.table_data(
      title="Likelihood based twin fraction estimation for twin law %s"%\
            str(self.twin_law),
      column_labels=["alpha", "-Log[Likelihood]"],
      graph_names=["Likelihood-based twin fraction estimate"],
      graph_labels=[("alpha", "-Log[Likelihood]")],
      graph_columns=[[0,1]],
      reference_marks=[[self.estimated_alpha]],
      data=[self.twin_fraction, self.nll])

  def _show_impl(self, out):
    out.show_paragraph_header("Maximum Likelihood twin fraction determination")
    out.show("""\
  Zwart, Read, Grosse-Kunstleve & Adams, to be published.
  The estimated twin fraction is equal to %4.3f
"""%(self.estimated_alpha))

def weighted_cc(x,y,w):
  """
  Utility function for correlation_analyses class.
  """
  tw = flex.sum( w )
  meanx = flex.sum(w*x)/tw
  meany = flex.sum(w*y)/tw
  meanxx = flex.sum(w*x*x)/tw
  meanyy = flex.sum(w*y*y)/tw
  meanxy = flex.sum(w*x*y)/tw
  top = meanxy-meanx*meany
  bottom = math.sqrt(  (meanxx - meanx*meanx)*(meanyy - meany*meany) )
  if math.fabs(bottom) > 0:
    return top/bottom
  else:
    return 0.0

class correlation_analyses(scaling.xtriage_analysis):
  def __init__(self,
               miller_obs,
               miller_calc,
               twin_law,
               d_weight=0.1):
    self.twin_law=sgtbx.change_of_basis_op( twin_law )
    self.twin_law= self.twin_law.new_denominators(
      r_den=miller_calc.space_group().r_den(),
      t_den=miller_calc.space_group().t_den() )
    obs=miller_obs.deep_copy()
    calc=miller_calc.deep_copy()
    # the incomming data is normalized, normalize the calculated data as well please
    normalizer = absolute_scaling.kernel_normalisation(calc, auto_kernel=True)
    calc = normalizer.normalised_miller.deep_copy()
    calc = calc.select(calc.data()>0)
    calc_part2 = calc.change_basis( self.twin_law )
    calc_part2 = calc.customized_copy(
      indices = calc_part2.indices(),
      data = calc_part2.data()).map_to_asu()
    # make sure we have common sets of everything
    calc, calc_part2 = calc.common_sets( calc_part2 )
    obs,  calc = obs.common_sets( calc )
    obs,  calc_part2 = obs.common_sets( calc_part2 )
    self.d = d_weight
    self.alpha = []
    self.cc = []
    def twin_the_data_and_compute_cc(alpha):
      wx = obs.sigmas()
      x = obs.data()
      dd = obs.d_star_sq().data()
      dd = flex.exp( -7.7*self.d*self.d*dd )
      wy = 1.0-dd*dd
      y = (1-alpha)*calc.data() + alpha*calc_part2.data()
      y = dd*dd*y
      w_tot = wx*wx + wy*wy
      cc = weighted_cc( x,y,w_tot )
      return(cc)
    if (obs.data().size() > 0):
      for ii in range(50):
        alpha=ii/100.0
        self.alpha.append( alpha )
        cc = twin_the_data_and_compute_cc(alpha)
        self.cc.append(cc)
    self.max_alpha, self.max_cc = self.find_maximum()
    del self.d
    del self.alpha

  def find_maximum(self):
    max_cc = flex.max( flex.double( self.cc ) )
    max_alpha = flex.max_index( flex.double( self.cc ) )
    max_alpha =  self.alpha[ max_alpha ]
    return max_alpha, max_cc

  def _show_impl(self, out):
    out.show_paragraph_header("Correlation analyses")
    out.show("""
  The supplied calculated data are normalized and artificially twinned;
  subsequently a correlation with the observed data is computed.
""")
    out.show("Correlation : %.3f" % self.max_cc)
    out.show("Estimated twin fraction : %.3f" % self.max_alpha)

#
# WRAPPER CLASS
#
class twin_law_dependent_twin_tests(scaling.xtriage_analysis):
  """Twin law dependent test results"""
  def __init__(self,
      twin_law,
      miller_array,
      out,
      verbose=0,
      miller_calc=None,
      normalized_intensities=None,
      ncs_test=None,
      n_ncs_bins=None):
    if n_ncs_bins is None:
      n_ncs_bins = 7
    acentric_data = miller_array.select_acentric().set_observation_type(
      miller_array)
    self.twin_law = twin_law
    tmp_detwin_object = scaling.detwin(
      miller_indices=acentric_data.indices(),
      intensity=acentric_data.data(),
      sigma=acentric_data.sigmas(),
      space_group=acentric_data.space_group(),
      anomalous_flag=acentric_data.anomalous_flag(),
      twin_law=twin_law.operator.as_double_array()[0:9])
    self.twin_completeness = tmp_detwin_object.completeness()
    self.results_available = False
    self.h_test = self.h_test_table = None
    self.britton_test = self.britton_test_table = None
    self.r_values=None
    self.correlation=None
    self.ml_murray_rust = self.ml_murray_rust_table = None
    self.ml_murry_rust_with_ncs=None
    if self.twin_completeness > 0:
      self.results_available = True
      self.h_test = h_test(
        twin_law=twin_law.operator.as_double_array()[0:9],
        miller_array = acentric_data)
      self.britton_test = britton_test(
        twin_law=twin_law.operator.as_double_array()[0:9],
        miller_array=acentric_data,
        verbose=verbose)
      self.r_values = r_values(
        miller_obs=miller_array,
        twin_law=twin_law.operator.as_double_array()[0:9],
        miller_calc=miller_calc)
      if miller_calc is not None:
        ## Magic number to control influence of hiugh resolutiuon reflections
        d_weight=0.25
        try:
          self.correlation = correlation_analyses(
            miller_obs=normalized_intensities,
            miller_calc=miller_calc,
            twin_law=twin_law.operator,
            d_weight=d_weight)
        except KeyboardInterrupt: raise
        except Exception: pass
      if normalized_intensities.sigmas() is not None:
        self.ml_murray_rust = ml_murray_rust(
          miller_array=normalized_intensities,
          twin_law=twin_law.operator.as_double_array()[0:9])
        if ncs_test:
          self.ml_murry_rust_with_ncs = ml_murray_rust_with_ncs(
            miller_array=normalized_intensities,
            twin_law=twin_law.operator.as_double_array()[0:9],
            out=out,
            n_bins=n_ncs_bins,
            calc_data=miller_calc,
            start_alpha=self.ml_murray_rust.estimated_alpha)

  @property
  def h_frac(self):
    if (self.h_test is not None):
      return self.h_test.estimated_alpha

  @property
  def britton_frac(self):
    if (self.britton_test is not None):
      return self.britton_test.estimated_alpha

  @property
  def ml_frac(Self):
    if (self.ml_murray_rust is not None):
      return self.ml_murray_rust.estimated_alpha

  def _show_impl(self, out):
    out.show_sub_header("Analysis of twin law %s" % self.twin_law)
    if (self.results_available):
      self.h_test.show(out)
      self.britton_test.show(out)
      self.r_values.show(out)
      if (self.correlation is not None):
        self.correlation.show(out)
      tables = [self.h_test.table, self.britton_test.table]
      if (self.ml_murray_rust is not None):
        tables.append(self.ml_murray_rust.table)
      out.show_plots_row(tables)
    else:
      out.show(("The twin completeness is only %3.2f. Twin law dependent "+
        "tests not performed.") % (self.twin_completeness))

########################################################################
# OTHER SYMMETRY PROBLEMS
########################################################################

class symmetry_issues(scaling.xtriage_analysis):
  def __init__(self,
               miller_array,
               max_delta=3.0,
               r_cut=0.05,
               sigma_inflation = 1.25,
               out=None):
    self.sigma_warning = None
    if out == None:
      out = sys.stdout
    self.inflate_sigma = sigma_inflation
    self.miller_array = miller_array
    self.xs_input = crystal.symmetry(miller_array.unit_cell(),
                                     space_group=miller_array.space_group() )
    self.explore_sg = pointgroup_tools.space_group_graph_from_cell_and_sg(
      self.xs_input.unit_cell(),
      self.xs_input.space_group(),
      max_delta=max_delta)
    self.pg_of_input_sg = self.xs_input.space_group().build_derived_group(
      False,False)
    self.pg_input_name = str(sgtbx.space_group_info(group=self.pg_of_input_sg))
    self.pg_low_prim_set = self.explore_sg.pg_low_prim_set
    self.pg_low_prim_set_name = str(sgtbx.space_group_info(
      group=self.explore_sg.pg_low_prim_set))
    self.pg_lattice_name = str( sgtbx.space_group_info(
      group = self.explore_sg.pg_high ) )
    self.miller_niggli =self.miller_array.change_basis(
      self.xs_input.change_of_basis_op_to_niggli_cell())

    self.ops_and_r_pairs = {}
    self.pg_r_used_table = {}
    self.pg_max_r_used_table = {}
    self.pg_r_unused_table = {}

    self.pg_r_unused_split = {}
    self.pg_r_used_split = {}

    self.pg_min_r_unused_table = {}
    self.pg_scores = {}
    self.pg_choice = None
    self.sg_possibilities = []

    self.legend = None
    self.table_data = []
    self.table = None

    self.make_r_table()
    self.make_pg_r_table()
    # loop over all pg's
    for pg in self.pg_max_r_used_table:
      tmp = self.miller_niggli
      if tmp.is_xray_amplitude_array():
        tmp = tmp.f_as_f_sq()
      sigmas = tmp.sigmas()
      if tmp.sigmas() is None:
         sigmas = tmp.d_star_sq().data()
         # lets assume a sigma of 5% at 0 and a sigma of 20% at the high resolution
         Berror = -math.log(0.20/0.05)/flex.max(sigmas)
         sigmas = 0.05*flex.exp( -sigmas*Berror )
         sigmas = tmp.data()*sigmas
         self.sigma_warning = """  ----> WARNING: NO SIGMAS FOUND  <----
          Sigmas now modeled as 0.05*|F|*exp( -Berror/d^2 )
          with Berror=%6.3f"""%(Berror)
      else :
        non_zero_sigmas_sel = sigmas != 0
        tmp = tmp.select(non_zero_sigmas_sel)
        sigmas = tmp.sigmas()

      merger = cctbx.xray.merger( tmp.indices(),
                                  tmp.data(),
                                  sigmas*self.inflate_sigma ,
                                  sgtbx.space_group_info(pg).group(),
                                  tmp.anomalous_flag(),
                                  tmp.unit_cell() )
      final_score = -merger.bic()
      self.pg_scores.update( {pg:final_score} )
    # create output table
    legend = ['Point group', 'mean R_used', 'max R_used', 'mean R_unused',
              'min R_unused', 'BIC', 'choice']
    table_data = []
    min_score = 2e+9
    def as_string(val, fmt):
      return format_value(fmt, val, replace_none_with="None")
    for pg in self.pg_scores:
      tmp = [pg,
             as_string(self.pg_r_used_table[pg], "%4.3f"),
             as_string(self.pg_max_r_used_table[pg],"%4.3f"),
             as_string(self.pg_r_unused_table[pg],"%4.3f"),
             as_string(self.pg_min_r_unused_table[pg],"%4.3f"),
             as_string(self.pg_scores[pg],"%6.3e"),
             "    " ]
      table_data.append( tmp )
      if self.pg_scores[ pg ] < min_score:
        min_score = self.pg_scores[ pg ]
        self.pg_choice = pg
    for row in self.table_data:
      if self.pg_choice == row[0]:
        row[ 6 ] = "<---"
    self.table = table_utils.simple_table(
      column_headers=legend,
      table_rows=table_data)
    # store the possible spacegroups and the change of basis ops
    # for the most likely pg
    for xs in self.explore_sg.pg_graph.graph.node_objects[
      self.pg_choice ].allowed_xtal_syms:
      self.sg_possibilities.append( xs )
    self.xs_with_pg_choice_in_standard_setting = crystal.symmetry(
      unit_cell   = self.miller_niggli.unit_cell(),
      space_group = self.pg_choice,
      assert_is_compatible_unit_cell=False).as_reference_setting()

  def make_r_table(self):
    tmp_buffer=StringIO()
    # please find all missing sym ops
    start = str(sgtbx.space_group_info(group=self.pg_low_prim_set))
    # FIXME, ordering keys/values changes depending on py2/3
    tmp_key = list(self.explore_sg.pg_graph.graph.edge_objects[ start ].keys())[0]
    tmp_edge = self.explore_sg.pg_graph.graph.edge_objects[ start ][tmp_key]
    for symop in tmp_edge.return_used():
      # get the r value please for this symop
      r_value = r_values(
        miller_obs=self.miller_niggli,
        twin_law=symop.as_double_array()[0:9]).show(out=tmp_buffer)
      top = r_value.r_abs_top_obs
      bottom = r_value.r_abs_bottom_obs
      self.ops_and_r_pairs.update( {symop.r().as_hkl():
                                    (top, bottom)} )
    for symop in tmp_edge.return_unused():
      r_value = r_values(
        miller_obs=self.miller_niggli,
        twin_law=symop.as_double_array()[0:9]).show(out=tmp_buffer)
      top = r_value.r_abs_top_obs
      bottom = r_value.r_abs_bottom_obs
      self.ops_and_r_pairs.update( {symop.r().as_hkl():
                                    (top, bottom)} )

  def make_pg_r_table(self):
    start = str(sgtbx.space_group_info(group=self.pg_low_prim_set))
    end = str(sgtbx.space_group_info(group=self.explore_sg.pg_high))

    for pg in self.explore_sg.pg_graph.graph.node_objects:
      tot_in, max_in, tot_out, min_out, r_in, r_out = \
        self.get_r_value_total(start,pg)
      self.pg_r_used_table.update( {pg:tot_in} )
      self.pg_max_r_used_table.update( {pg:max_in} )
      self.pg_r_used_split.update( {pg:r_in} )
      self.pg_r_unused_table.update( {pg:tot_out} )
      self.pg_min_r_unused_table.update( {pg:min_out} )
      self.pg_r_unused_split.update( {pg:r_out} )

  def get_r_value_total(self,
                        start_pg,
                        end_pg):
    # do a coset decompostion on start and end
    spg = sgtbx.space_group_info( start_pg ).group()
    epg = sgtbx.space_group_info( end_pg ).group()
    cosets = cctbx.sgtbx.cosets.left_decomposition( epg, spg )
    coset_ops = []
    # get the coset operations in a nice list
    for cs in cosets.partitions:
      for op in cs:
        coset_ops.append(  op.r().as_hkl() )
    r_in = flex.double()
    r_out = flex.double()
    tot_r_in  = [0,0]
    tot_r_out = [0,0]
    tmp_keys = list(self.ops_and_r_pairs.keys())
    tmp_values = list(self.ops_and_r_pairs.values())
    tmp = self.ops_and_r_pairs.copy()
    for op in coset_ops:
      if op in tmp_keys:  #tmp.has_key( op ):
        # Work around for absence of pop in python 2.2
        iii = tmp_keys.index( op )
        a = tmp_values[ iii ]
        # now we have to pop these guys from the array
        tmp_tmp_keys = []
        tmp_tmp_values = []
        for ii in range( len(tmp_keys) ):
          if ii != iii:
            tmp_tmp_keys.append( tmp_keys[ii] )
            tmp_tmp_values.append( tmp_values[ii] )
        tmp_keys = list(tmp_tmp_keys)
        tmp_values = list(tmp_tmp_values)
        r_in.append( a[0]/max(a[1],1e-8) )
        tot_r_in[0]+=a[0]
        tot_r_in[1]+=a[1]
    for a in tmp_values:
      r_out.append( a[0]/max(a[1],1e-8) )
      tot_r_out[0]+=a[0]
      tot_r_out[1]+=a[1]
    tot_in  = None
    max_in  = None
    tot_out = None
    min_out = None
    if r_in.size() > 0:
      tot_in = tot_r_in[0]/max(1e-8,tot_r_in[1] )
      max_in = flex.max( r_in )
    if r_out.size() > 0:
      tot_out = tot_r_out[0]/max( 1e-8,tot_r_out[1] )
      min_out = flex.min(r_out)
    return tot_in, max_in, tot_out, min_out, r_in, r_out

  def return_point_groups(self):
    #please return
    # miller_array of niggli stuff
    # Preffered symmetry
    # Highest symmetry
    miller_niggli = self.miller_niggli
    if miller_niggli.is_xray_amplitude_array():
      miller_niggli = miller_niggli.f_as_f_sq()
    return [ miller_niggli, self.pg_low_prim_set_name, self.pg_choice,
             self.pg_lattice_name ]

  def _show_impl(self, out):
    out.show_header("Exploring higher metric symmetry")
    if self.sigma_warning is not None:
      out.show_text(self.sigma_warning)
    if out.gui_output : # XXX GUI only
      out.show_sub_header("Point group and R-factor analysis")
    out.show_lines("""
The point group of data as dictated by the space group is %s
The point group in the niggli setting is %s
The point group of the lattice is %s
A summary of R values for various possible point groups follow.
""" % (str(self.pg_input_name),
       str(sgtbx.space_group_info(group=self.pg_low_prim_set)),
       str(self.pg_lattice_name)))
    out.show_table(self.table, indent=2)
    # Phenix GUI hack
    if hasattr(out, "add_change_symmetry_button"):
      out.add_change_symmetry_button()
    out.show_lines("""
R_used: mean and maximum R value for symmetry operators *used* in this point group
R_unused: mean and minimum R value for symmetry operators *not used* in this point group
""")
    out.show("""
An automated point group suggestion is made on the basis of the BIC (Bayesian
information criterion).
""")
    out.show("The likely point group of the data is: %s\n" % self.pg_choice)
    out.show("Possible space groups in this point group are:")
    for sg in self.sg_possibilities:
      sg_out = StringIO()
      sg[0].show_summary(f=sg_out, prefix= "   ")
      out.show_preformatted_text(sg_out.getvalue())
    out.show("""
Note that this analysis does not take into account the effects of twinning.
If the data are (almost) perfectly twinned, the symmetry will appear to be
higher than it actually is.
""")


class r_values(scaling.xtriage_analysis):
  def __init__(self,
               miller_obs,
               twin_law,
               miller_calc=None,
               n_reflections=400):
    self.obs = miller_obs.deep_copy()
    self.calc=None
    self.n_reflections=n_reflections
    if miller_calc is not None:
      self.calc = miller_calc.deep_copy()
      self.obs, self.calc = self.obs.common_sets( self.calc )
    self.twin_law = twin_law
    self.rvsr_interpretation=None
    self.table=None
    self.r_abs_obs=None
    self.r_abs_calc=None
    self.r_sq_obs=None
    self.r_sq_calc=None
    self.r_abs_top_obs=None
    self.r_abs_bottom_obs=None
    self.r_sq_top_obs = None
    self.r_sq_bottom_obs = None
    self.d_star_sq = []
    self.r_sq_obs_reso = []
    self.r_sq_calc_reso = []
    self.guess=None
    #self.resolution_dependent_r_values()
    self.r_vs_r( self.obs,self.calc)
    self.r_vs_r_classification()
    del self.obs
    del self.calc

  def resolution_dependent_r_values(self):
    # bin the data please in the specified number of bins
    self.obs.setup_binner(reflections_per_bin=self.n_reflections)
    # now we have to loop over all bins please
    for bin_number in self.obs.binner().range_used():
      selection =  self.obs.binner().selection( bin_number ).iselection()
      tmp_obs = self.obs.select( selection )
      tmp_calc=None
      if self.calc is not None:
        tmp_calc = self.calc.select( selection )
      else:
        self.r_sq_calc_reso = None
      self.r_vs_r(tmp_obs, tmp_calc)
      up = self.obs.binner().bin_d_range(bin_number)[0]
      down = self.obs.binner().bin_d_range(bin_number)[1]
      self.d_star_sq.append( 0.5/(up*up) + 0.5/(down*down) )
      self.r_sq_obs_reso.append( self.r_sq_obs )
      if self.calc is not None:
        self.r_sq_calc_reso.append(self.r_sq_calc )

  def r_vs_r(self, input_obs, input_calc):
    # in order to avoid certain issues, take common sets please
    obs = None
    calc = None
    if input_calc is not None:
      obs, calc = input_obs.common_sets( input_calc )
    else:
      obs = input_obs.deep_copy()
    # make sure we have intensities
    if not obs.is_xray_intensity_array():
      obs = obs.f_as_f_sq()
    if calc is not None:
      if not calc.is_xray_intensity_array():
        calc = calc.f_as_f_sq()
    obs_obj = scaling.twin_r( obs.indices(),
                              obs.data(),
                              obs.space_group(),
                              obs.anomalous_flag(),
                              self.twin_law )
    self.r_abs_obs = obs_obj.r_abs_value()
    self.r_sq_obs = obs_obj.r_sq_value()
    self.r_sq_top_obs, self.r_sq_bottom_obs = obs_obj.r_sq_pair()
    self.r_abs_top_obs, self.r_abs_bottom_obs = obs_obj.r_abs_pair()
    if calc is not None:
      calc_obj =  scaling.twin_r( calc.indices(),
                                  calc.data(),
                                  calc.space_group(),
                                  calc.anomalous_flag(),
                                  self.twin_law )
      self.r_abs_calc = calc_obj.r_abs_value()
      self.r_sq_calc = calc_obj.r_sq_value()

  def r_vs_r_classification(self):
    self.rvsr_interpretation = [
      [ "0.1", "0.1", "Misspecified (too low) crystal symmetry \n ",None,""] ,
      [ "0.1", "0.3", "Twin with NCS parallel to twin operator \n ",None,""],
      [ "0.1", "0.5", "Close to perfect twinning \n ",None,""],
      [ "0.3", "0.3", "Data is not twinned, but NCS is parallel \n   to putative twin operator",None,""],
      [ "0.3", "0.5", "Partially twinned data \n ",None,""],
      [ "0.5", "0.5", "No twinning or parallel NCS",None,""]
      ]
    guess = None
    min=1.0
    count=0
    if self.r_abs_calc is not None:
      for test_class in  self.rvsr_interpretation:
        tmp_obs = (float(test_class[0])-self.r_abs_obs)**2.0
        tmp_calc = (float(test_class[1])-self.r_abs_calc)**2.0
        d =  math.sqrt( tmp_obs+tmp_calc )
        test_class[3]="%4.3f"%(d)
        if d < min:
          min = d
          guess = count
        count+=1
    self.guess=guess
    if guess is not None:
      self.rvsr_interpretation[ guess ][4]="<---"

  def _show_impl(self, out=sys.stdout):
    out.show_paragraph_header("R vs R statistics")
    out.show("  R_abs_twin = <|I1-I2|>/<|I1+I2|>")
    out.show("    (Lebedev, Vagin, Murshudov. Acta Cryst. (2006). D62, 83-95)")
    out.show("  R_abs_twin observed data   : %4.3f" % (self.r_abs_obs))
    if self.r_abs_calc is not None:
      out.show("   R_abs_twin calculated data : %4.3f" % self.r_abs_calc)
    out.show("  R_sq_twin = <(I1-I2)^2>/<(I1+I2)^2>")
    out.show("  R_sq_twin observed data    : %4.3f" % self.r_sq_obs)
    if self.r_abs_calc is not None :
      out.show("  R_sq_twin calculated data  : %4.3f" % self.r_sq_calc)
    else:
      out.show("  No calculated data available.")
      out.show("  R_twin for calculated data not determined.")

########################################################################
# OVERALL ANALYSES
########################################################################

class twin_results_interpretation(scaling.xtriage_analysis):
  def __init__(self,
               nz_test,
               wilson_ratios,
               l_test,
               translational_pseudo_symmetry=None,
               twin_law_related_test=None,
               symmetry_issues=None,
               maha_l_cut=3.5,
               patterson_p_cut=0.01,
               out=None):
    self.maha_l_cut = maha_l_cut
    self.patterson_p_cut = patterson_p_cut
    # patterson analyses
    self.patterson_height = self.patterson_p_value = None
    if translational_pseudo_symmetry is not None:
      self.patterson_height = translational_pseudo_symmetry.high_peak
      self.patterson_p_value = translational_pseudo_symmetry.high_p_value
    # wilson statistics moments, etc
    self.i_ratio = wilson_ratios.acentric_i_ratio
    self.f_ratio = wilson_ratios.acentric_f_ratio
    self.e_sq_minus_1 = wilson_ratios.acentric_abs_e_sq_minus_one
    # L test
    self.l_mean = l_test.mean_l
    self.l_sq_mean = l_test.mean_l2
    self.compute_maha_l()
    # twin dependent tests
    self.n_twin_laws = len(twin_law_related_test)
    self.twin_laws = []
    self.twin_law_type = []
    self.r_obs = []
    self.r_calc = []
    # twin fraction estimates
    self.britton_alpha = []
    self.h_alpha = []
    self.murray_rust_alpha = []
    have_twin_test_results = False
    for twin_item in twin_law_related_test:
      self.twin_laws.append( twin_item.twin_law.operator.r().as_hkl() )
      self.twin_law_type.append( str(twin_item.twin_law.twin_type) )
      if twin_item.results_available:
        have_twin_test_results = True
        self.r_obs.append(twin_item.r_values.r_abs_obs)
        self.r_calc.append(twin_item.r_values.r_abs_calc)
        self.britton_alpha.append(twin_item.britton_test.estimated_alpha)
        self.h_alpha.append(twin_item.h_test.estimated_alpha)
        if twin_item.ml_murray_rust is not None:
          self.murray_rust_alpha.append(
            twin_item.ml_murray_rust.estimated_alpha)
        else:
          self.murray_rust_alpha.append(None)
      else:
        self.r_obs.append(None)
        self.r_calc.append(None)
        self.britton_alpha.append(None)
        self.h_alpha.append(None)
        self.murray_rust_alpha.append(None)
    if self.n_twin_laws > 0 :
      if have_twin_test_results :
        max_tf = -1
        max_index = -1
        for ii, twin_fraction in enumerate(self.britton_alpha):
          if (twin_fraction is not None) and (float(twin_fraction) > max_tf):
            max_tf = twin_fraction
            max_index = ii
        self.most_worrysome_twin_law = max_index
      else:
        tmp_tf = []
        for twin_fraction in self.britton_alpha:
          if twin_fraction is None:
            tmp_tf.append( -100 )
          else:
            tmp_tf.append( twin_fraction )
        self.most_worrysome_twin_law = flex.max_index( flex.double(tmp_tf) )
      self.suspected_point_group = symmetry_issues.pg_choice
      self.possible_sgs = symmetry_issues.sg_possibilities
      self.input_point_group = symmetry_issues.pg_low_prim_set_name
    else:
      self.input_point_group = None
      self.most_worrysome_twin_law = None
      self.suspected_point_group = None
      self.possible_sgs = None

  def compute_maha_l(self):
    # Mean vector: 0.48513455414 0.316418789809
    # Variancxe /covriance: 0.000143317086266 0.000192434487707 0.000165215146913
    # Its inverse {{679728., -583582.}, {-583582., 506232.}}
    # These numbers have been obtained from roughly 1200 datasets
    # They were selected using
    # strong high resolution limit (over 85% complete, for i/sigi>3.0) is smaller than 2.0
    # no detected pseudo translation
    # <|L|> > 0.45 (limit decided based on histogram, mainly  for removing some outliers)
    maha_l   = 679728.0 #old value 117820.0
    maha_l2  = 506232.0 #old value 106570
    maha_ll2 = 2.0*-583582.0 #old value -212319
    maha_mean_l = 0.48513455414  #old value: 0.487758242
    mama_mean_l2 = 0.316418789809 #old value: 0.322836996
    tmp_l = self.l_mean - maha_mean_l
    tmp_l2 = self.l_sq_mean - mama_mean_l2
    maha_distance_l = tmp_l*tmp_l*maha_l +\
                      tmp_l2*tmp_l2*maha_l2 +\
                      tmp_l*tmp_l2*maha_ll2
    maha_distance_l = math.sqrt(maha_distance_l)
    self.maha_l = maha_distance_l

  def has_pseudo_translational_symmetry(self):
    if (self.patterson_p_value is None):
      return None
    return (self.patterson_p_value <= self.patterson_p_cut)

  def has_abnormal_intensity_statistics(self):
    return ((self.maha_l >= self.maha_l_cut) and (self.l_mean < 0.5))

  def has_twinning(self):
    return self.has_abnormal_intensity_statistics() and (self.n_twin_laws > 0)

  def has_higher_symmetry(self):
    return (self.input_point_group != self.suspected_point_group)

  def patterson_verdict(self):
    verdict = ""
    if self.has_pseudo_translational_symmetry():
      verdict = """\
The analyses of the Patterson function reveals a significant off-origin
peak that is %3.2f %% of the origin peak, indicating pseudo-translational
symmetry.  The chance of finding a peak of this or larger height by random
in a structure without pseudo-translational symmetry is equal to %5.4e.""" % \
        (self.patterson_height, self.patterson_p_value)
      if self.i_ratio > 2:
        verdict += """
The detected translational NCS is most likely also responsible for the
elevated intensity ratio.  See the relevant section of the logfile for more
details."""
    elif (self.patterson_p_value is not None):
      verdict = """\
The largest off-origin peak in the Patterson function is %3.2f%% of the
height of the origin peak. No significant pseudotranslation is detected.""" % \
        self.patterson_height
    return verdict

  def show_verdict(self, out):
    # First, TNCS verdict
    out.show("\n%s\n" % self.patterson_verdict())
    # And now the rest:
    # Case 1: intensity statistics are suspicious
    if self.maha_l >= self.maha_l_cut:
      # Case 1a: looks like twinning...
      if self.l_mean < 0.5 :
        out.show("""\
The results of the L-test indicate that the intensity statistics
are significantly different than is expected from good to reasonable,
untwinned data.""")
        # [1a] ...and we have twin laws!
        if (self.n_twin_laws > 0):
          out.show("""
As there are twin laws possible given the crystal symmetry, twinning could
be the reason for the departure of the intensity statistics from normality.
It might be worthwhile carrying out refinement with a twin specific target
function.

Please note however that R-factors from twinned refinement cannot be directly
compared to R-factors without twinning, as they will always be lower when a
twin law is used.  You should also use caution when interpreting the maps from
refinement, as they will have significantly more model bias.
""")
          self.twinning_short=True
          if self.has_higher_symmetry ():
            out.show("""
Note that the symmetry of the intensities suggest that the assumed space group
is too low. As twinning is however suspected, it is not immediately clear if
this is the case.  Careful reprocessing and (twin)refinement for all cases
might resolve this question.""")
        # [1a] ...but no twin laws possible
        else :
          out.show("""
As there are no twin laws possible given the crystal symmetry, there could be
a number of reasons for the departure of the intensity statistics from
normality.  Overmerging pseudo-symmetric or twinned data, intensity to
amplitude conversion problems as well as bad data quality might be possible
reasons.  It could be worthwhile considering reprocessing the data.""")
          self.twinning_short=None

      # Case 1b: not twinning?
      else:
        self.twinning_short=None
        out.show("""\
The results of the L-test indicate that the intensity statistics show more
centric character than is expected for acentric data.""")
        if self.has_pseudo_translational_symmetry():
          out.show("""\
This behavior might be explained by the presence of the detected pseudo
translation.""")
          self.twinning_short=False
    # Case 2: no twinning suspected
    else:
      self.twinning_short=False
      out.show("""\
The results of the L-test indicate that the intensity statistics behave as
expected. No twinning is suspected.""")
      if self.has_higher_symmetry():
        out.show("""\
The symmetry of the lattice and intensity however suggests that the input
input space group is too low. See the relevant sections of the log file for
more details on your choice of space groups.""")

      if (self.n_twin_laws > 0):
        if (not self.has_higher_symmetry()):
          if (self.most_worrysome_twin_law is not None):
            if (self.britton_alpha[ self.most_worrysome_twin_law ]>
                TWIN_FRAC_SIGNIFICANT):
              out.show("""\
The correlation between the intensities related by the twin law
%(twin_law)s with an estimated twin fraction of %(alpha)3.2f is most likely
due to an NCS axis parallel to the twin axis. This can be verified by
supplying calculated data as well.
""" % { "twin_law" :  self.twin_laws[ self.most_worrysome_twin_law ],
        "alpha" : self.britton_alpha[ self.most_worrysome_twin_law ] })
        else:
          out.show("""\
As the symmetry is suspected to be incorrect, it is advisable to reconsider
data processing.""")
          if self.has_pseudo_translational_symmetry():
            out.show("""\
Note however that the presence of translational NCS (and possible rotational
pseudo symmetry parallel to the twin axis) can make the detection of twinning
difficult.  Trying various space groups and twinning hypotheses in structure
refinement might provide an answer.
""")

  def make_sym_op_table(self):
    def as_string(x):
      return format_value("%4.3f", x).strip()
    legend = None
    table_data = []
    if self.r_calc[0]==None:
      legend = ('Operator', 'type', 'R obs.', 'Britton alpha', 'H alpha',
                'ML alpha')
      for item in range( len(self.twin_laws) ):
        tmp = [ self.twin_laws[item],
                self.twin_law_type[item],
                as_string( self.r_obs[item]),
                as_string( self.britton_alpha[item]),
                as_string( self.h_alpha[item]),
                as_string( self.murray_rust_alpha[item]) ]
        table_data.append( tmp )
    else:
      legend = ('Operator', 'type', 'R_abs obs.', 'R_abs calc.',
                'Britton alpha', 'H alpha', 'ML alpha')
      for item in range( len(self.twin_laws) ):
        tmp = [ self.twin_laws[item],
                self.twin_law_type[item],
                as_string(self.r_obs[item]),
                as_string(self.r_calc[item]),
                as_string(self.britton_alpha[item]),
                as_string(self.h_alpha[item]),
                as_string(self.murray_rust_alpha[item]) ]
        table_data.append( tmp )
    self.table = table_utils.simple_table(
      column_headers=legend,
      table_rows=table_data)

  def _show_impl(self, out):
    out.show_header("Twinning and intensity statistics summary")
    out.show_sub_header("Final verdict")
    self.show_verdict(out=out)
    out.show_sub_header("Statistics independent of twin laws")
    out.show_lines("""\
  <I^2>/<I>^2 : %(i_ratio)5.3f  (untwinned: 2.0, perfect twin: 1.5)
  <F>^2/<F^2> : %(f_ratio)5.3f  (untwinned: 0.785, perfect twin: 0.885)
  <|E^2-1|>   : %(e_sq_minus_1)5.3f  (untwinned: 0.736, perfect twin: 0.541)
  <|L|>       : %(l_mean)5.3f  (untwinned: 0.500; perfect twin: 0.375)
  <L^2>       : %(l_sq_mean)5.3f  (untwinned: 0.333; perfect twin: 0.200)
  Multivariate Z score L-test: %(maha_l)5.3f
""" % { "i_ratio" : self.i_ratio,
        "f_ratio" : self.f_ratio,
        "e_sq_minus_1" : self.e_sq_minus_1,
        "l_mean" : self.l_mean,
        "l_sq_mean" : self.l_sq_mean,
        "maha_l" : self.maha_l })
    out.show("""
 The multivariate Z score is a quality measure of the given spread in
 intensities. Good to reasonable data are expected to have a Z score lower
 than 3.5.  Large values can indicate twinning, but small values do not
 necessarily exclude it.  Note that the expected values for perfect twinning
 are for merohedrally twinned structures, and deviations from untwinned will
 be larger for perfect higher-order twinning.
""")
    if len(self.twin_laws)>0:
      out.show_sub_header("Statistics depending on twin laws")
      self.make_sym_op_table()
      out.show_table(self.table, indent=2)
    else:
      out.show("\nNo (pseudo)merohedral twin laws were found.\n")

  # FIXME this uses the Britton test, but the PDB validation server appears to
  # use the H test.  Which is correct?
  def max_twin_fraction(self):
    if (self.most_worrysome_twin_law is not None):
      return self.britton_alpha[ self.most_worrysome_twin_law ]
    return 0

  def summarize_issues(self):
    issues = []
    bad_data = False
    if self.has_abnormal_intensity_statistics():
      if ((self.n_twin_laws > 0) and
          (self.max_twin_fraction() > TWIN_FRAC_SIGNIFICANT)):
        issues.append((2, "Intensity statistics suggest twinning "+
          "(intensities are significantly different from expected for "+
          "normal data) and one or more twin operators show a significant "+
          "twin fraction.", "Statistics depending on twin laws"))
      else :
        issues.append((1, "The intensity statistics look unusual, but "+
          "twinning is not indicated or not possible in the given space "+
          "group.", "Wilson ratio and moments"))
    else :
      issues.append((0, "The intensity statistics look normal, indicating "+
        "that the data are not twinned.", "Wilson ratio and moments"))
      if (self.max_twin_fraction() > TWIN_FRAC_SIGNIFICANT):
        issues.append((1, "One or more twin operators show a significant "+
          "twin fraction but since the intensity statistics do not indicate "+
          "twinning, you may have an NCS rotation axis parallel to a "+
          "crystallographic axis.", "Statistics depending on twin laws"))
      if self.has_higher_symmetry():
        issues.append((1, "One or more symmetry operators suggest that the "+
          "data has a higher "+
          "crystallographic symmetry (%s)." % str(self.suspected_point_group),
          "Point group and R-factor analysis"))
    if self.patterson_height and self.patterson_height > 75:
      issues.append((2, "Translational NCS is present at a level that might "+
        "be a result of a missed centering operation (one or more peaks "+
        "greater than 75% of the origin).", "Patterson analyses"))
    elif self.patterson_height and self.patterson_height > 20:
      issues.append((2, "Translational NCS is present at a level that may "+
        "complicate refinement (one or more peaks greater than 20% of the "+
        "origin)", "Patterson analyses"))
    else :
      issues.append((0, "Translational NCS does not appear to be present.",
        "Patterson analyses"))
    return issues

class twin_analyses(scaling.xtriage_analysis):
  """Perform various twin related tests"""
  def __init__(self,
               miller_array,
               d_star_sq_low_limit=None,
               d_star_sq_high_limit=None,
               d_hkl_for_l_test=None,
               normalise=True, ## If normalised is true, normalisation is done
               out=None,
               out_plots = None,
               verbose = 1,
               miller_calc=None,
               additional_parameters=None,
               original_data=None,
               completeness_as_non_anomalous=None,
                ):
    assert miller_array.is_unique_set_under_symmetry()
    if out is None:
      out = sys.stdout
    self.max_delta = 3.0
    symm_issue_table = [0.08, 75, 0.08, 75]
    perform_ncs_analyses=False
    n_ncs_bins=7
    sigma_inflation = 1.25
    if additional_parameters is not None:
      sigma_inflation = additional_parameters.missing_symmetry.sigma_inflation
      perform_ncs_analyses = \
        additional_parameters.twinning_with_ncs.perform_analyses
      n_ncs_bins = additional_parameters.twinning_with_ncs.n_bins

    ## If resolution limits are not specified
    ## use full resolution limit
    if  d_star_sq_high_limit is None:
      d_star_sq_high_limit = flex.min(miller_array.d_spacings().data())
      d_star_sq_high_limit = d_star_sq_high_limit**2.0
      d_star_sq_high_limit = 1.0/d_star_sq_high_limit
    if  d_star_sq_low_limit is None:
      d_star_sq_low_limit = flex.max(miller_array.d_spacings().data())
      d_star_sq_low_limit = d_star_sq_low_limit**2.0
      d_star_sq_low_limit = 1.0/d_star_sq_low_limit
    self.d_max = math.sqrt(1./d_star_sq_low_limit)
    self.d_min = math.sqrt(1./d_star_sq_high_limit)
    self.d_hkl_for_l_test = d_hkl_for_l_test

    miller_array = miller_array.resolution_filter(
      math.sqrt(1.0/d_star_sq_low_limit),
      math.sqrt(1.0/d_star_sq_high_limit))
    ## Make sure that we actually have some data
    if (miller_array.indices().size() == 0):
      raise Sorry("No suitable data available after resolution cuts")
    if miller_array.observation_type() is None:
      raise RuntimeError("Observation type unknown")

    # minimize the number of conversions between amplitude and intensities
    # by preparing both in advance
    f_obs = i_obs = miller_array
    if miller_array.is_real_array():
      if miller_array.is_xray_intensity_array():
        f_obs = miller_array.f_sq_as_f()
      else :
        i_obs = miller_array.f_as_f_sq()
    else:
      raise RuntimeError("Observations should be a real array.")
    # now do the same for calculated data
    f_calc = i_calc = miller_calc
    if (miller_calc is not None):
      if miller_calc.is_xray_amplitude_array():
        i_calc = miller_calc.f_as_f_sq()
      else :
        f_calc = miller_calc.f_sq_as_f()

    ## Determine possible twin laws
    possible_twin_laws = twin_laws(
      miller_array=f_obs,
      lattice_symmetry_max_delta=self.max_delta)
    ##-----------------------------
    self.possible_twin_laws = possible_twin_laws
    self.normalised_intensities = wilson_normalised_intensities(
      miller_array=i_obs,
      normalise=normalise,
      out=out,
      verbose=verbose)
    ## Try to locat e pseudo translational symm.
    ## If no refls are available at low reso,
    ## an exception is thrown and caught here not to disturb things too much
    self.translational_pseudo_symmetry = None
    try:
      self.translational_pseudo_symmetry = detect_pseudo_translations(
        miller_array=f_obs,
        out=out,
        completeness_as_non_anomalous=completeness_as_non_anomalous,
        verbose=verbose)
    except Sorry: pass

    self.abs_sg_anal = None
    try:
      if miller_array.sigmas() is not None:
        # Look at systematic absences please
        #from mmtbx.scaling import absences
        from mmtbx.scaling import absences
        self.abs_sg_anal = absences.protein_space_group_choices(
          miller_array = self.normalised_intensities.all,
          threshold = 3.0,
          sigma_inflation=sigma_inflation,
          original_data=original_data)#.show(out)
    except Sorry: pass

    centric_cut = self.normalised_intensities.centric
    acentric_cut = self.normalised_intensities.acentric
    self.wilson_moments = wilson_moments(
      acentric_cut,
      centric_cut)
    self.nz_test = n_z_test(
      normalised_acentric=acentric_cut,
      normalised_centric=centric_cut)
    self.l_test=None
    parity_h = parity_k = parity_l = 2
    if (not d_hkl_for_l_test in [None, Auto]):
      parity_h, parity_k, parity_l = d_hkl_for_l_test
    if self.translational_pseudo_symmetry is not None:
      if (d_hkl_for_l_test in [None, Auto]):
        parity_h = self.translational_pseudo_symmetry.mod_h
        parity_k = self.translational_pseudo_symmetry.mod_k
        parity_l = self.translational_pseudo_symmetry.mod_l
      self.l_test = l_test(
        miller_array=acentric_cut,
        parity_h=parity_h,
        parity_k=parity_k,
        parity_l=parity_l)
    else:
      self.l_test = l_test(
        miller_array=acentric_cut,
        parity_h=parity_h,
        parity_k=parity_k,
        parity_l=parity_l)
    ##--------------------------
    self.n_twin_laws = len(possible_twin_laws.operators)
    self.twin_law_dependent_analyses = []
    self.twin_law_names = []
    for ii in range(self.n_twin_laws):
      twin_law_name = possible_twin_laws.operators[ii].operator.r().as_hkl()
      self.twin_law_names.append(twin_law_name)
      tmp_twin_law_stuff = twin_law_dependent_twin_tests(
        twin_law=possible_twin_laws.operators[ii],
        miller_array=i_obs,
        out=out,
        verbose=verbose,
        miller_calc=i_calc,
        normalized_intensities=self.normalised_intensities.acentric,
        ncs_test=perform_ncs_analyses,
        n_ncs_bins=n_ncs_bins)
      self.twin_law_dependent_analyses.append( tmp_twin_law_stuff )
    # now we can check for space group related issues
    self.check_sg = None
    self.suggested_space_group=None
    if self.n_twin_laws > 0:
      self.check_sg = symmetry_issues(
        miller_array=i_obs,
        max_delta=self.max_delta,
        out=out,
        sigma_inflation=sigma_inflation)
      nig_data, pg_this_one, pg_choice, pg_high = \
        self.check_sg.return_point_groups()
      xs_choice = crystal.symmetry(
        unit_cell=nig_data.unit_cell(),
        space_group_symbol=pg_choice,
        assert_is_compatible_unit_cell=False )
      xs_high   = crystal.symmetry(
        unit_cell=nig_data.unit_cell(),
        space_group_symbol=pg_high,
        assert_is_compatible_unit_cell=False )
      if pg_choice != pg_high:
        # FIXME
        merge_data_and_guess_space_groups(
          miller_array=nig_data,
          xs=xs_high,
          out=out,
          txt="Merging in *highest possible* point group %s.\n ***** THIS MIGHT NOT BE THE BEST POINT GROUP SYMMETRY *****  "%pg_high,
          check_absences=False)

      if pg_choice != pg_this_one:
        suggested_space_group = merge_data_and_guess_space_groups(
          miller_array=nig_data,
          xs=xs_choice,
          out=out,
          txt="Merging in *suggested* point group %s "%pg_choice,
          check_absences=False)
    ##--------------------------
    self.twin_summary = twin_results_interpretation(
      nz_test=self.nz_test,
      wilson_ratios=self.wilson_moments,
      l_test=self.l_test,
      translational_pseudo_symmetry=self.translational_pseudo_symmetry,
      twin_law_related_test=self.twin_law_dependent_analyses,
      symmetry_issues=self.check_sg)

  def _show_impl(self, out):
    if (self.abs_sg_anal):
      self.abs_sg_anal.show(out)
    out.show_header("Diagnostic tests for twinning and pseudosymmetry")
    out.show("Using data between %4.2f to %4.2f Angstrom." % (self.d_max,
      self.d_min))
    if (self.translational_pseudo_symmetry is not None):
      self.translational_pseudo_symmetry.show(out)
    self.wilson_moments.show(out)
    self.nz_test.show(out)
    if (self.l_test is not None):
      self.l_test.show(out)
    out.show_header("Twin laws")
    self.possible_twin_laws.show(out=out)
    if (self.n_twin_laws > 0):
      out.show_sub_header("Twin law-specific tests")
      out.show("""\
 The following tests analyze the input data with each of the possible twin
 laws applied.  If twinning is present, the most appropriate twin law will
 usually have a low R_abs_twin value and a consistent estimate of the twin
 fraction (significantly above 0) from each test.  The results are also
 compiled in the summary section.

 WARNING: please remember that the possibility of twin laws, and the results
 of the specific tests, does not guarantee that twinning is actually present
 in the data.  Only the presence of abnormal intensity statistics (as judged
 by the Wilson moments, NZ-test, and L-test) is diagnostic for twinning.
""")
      for twin_tests in self.twin_law_dependent_analyses :
        twin_tests.show(out=out)
      if (self.check_sg is not None):
        self.check_sg.show(out=out)
    self.twin_summary.show(out=out)

  # XXX grossness ahead, beware
  # Objects of this class are relatively bulky due to the storage of multiple
  # derived Miller arrays that may be used elsewhere.  Since we do not need
  # these arrays for simply displaying the results (in the Phenix GUI or
  # elsewhere), they are deleted prior to pickling to reduce the amount of
  # data that needs to be transfered or saved.  It is not necessary to
  # implement __setstate__, since we are still just pickling self.__dict__.
  def __getstate__(self):
    """
    Pickling function with storage efficiency optimizations.
    """
    if (self.abs_sg_anal is not None):
      self.abs_sg_anal.miller_array = None
      self.abs_sg_anal.absences_table.miller_array = None
    if (self.check_sg is not None):
      self.check_sg.miller_niggli = None
      self.check_sg.miller_array = None
    self.normalised_intensities = None
    return self.__dict__

def merge_data_and_guess_space_groups(miller_array, txt, xs=None,out=None,
    sigma_inflation=1.0, check_absences=True):
  tmp_ma = miller_array.deep_copy()
  if xs is None:
    xs = tmp_ma.crystal_symmetry()
  tmp_ma = miller_array.customized_copy( crystal_symmetry=xs )
  merge_obj = tmp_ma.change_basis(
      xs.space_group_info().change_of_basis_op_to_reference_setting()
    ).merge_equivalents()
  tmp_ma = merge_obj.array()
  r_lin = merge_obj.r_linear()
  normalizer = absolute_scaling.kernel_normalisation(tmp_ma, auto_kernel=True)
  work_array = normalizer.normalised_miller.deep_copy()
  abs_sg_anal = None
  if tmp_ma.sigmas() is not None and (check_absences):
    print(file=out)
    print(file=out)
    print("-"*len(txt), file=out)
    print(txt, file=out)
    print("-"*len(txt), file=out)
    print(file=out)
    merge_obj.show_summary(out=out)
    print(file=out)
    print("Suggesting various space group choices on the basis of systematic absence analyses", file=out)
    print(file=out)
    print(file=out)
    this_worked=False
    try:
      if miller_array.sigmas() is not None:
        # Look at systematic absences please
        #from mmtbx.scaling import absences
        from mmtbx.scaling import absences
        abs_sg_anal = absences.protein_space_group_choices(
          miller_array = work_array,
          threshold = 3.0,
          print_all=False,
          sigma_inflation=sigma_inflation).show(out)
    except Sorry:
      print("Systematic absence analyses failed", file=out)
  return (merge_obj, abs_sg_anal)

########################################################################
# MILLER ARRAY EXTENSIONS
# Injector class to extend the Miller array class with the intensity analyses
# contained in this module.
def analyze_intensity_statistics(self, d_min=2.5,
    completeness_as_non_anomalous=None, log=None):
  """
  Detect translational pseudosymmetry and twinning.  Returns a
  twin_law_interpretation object.
  """
  if (log is None) : log = null_out()
  if self.space_group().is_centric():
    return None
  tmp_array = self.resolution_filter(d_min=d_min)
  if (not self.sigmas_are_sensible()):
    tmp_array = tmp_array.customized_copy(
      indices=tmp_array.indices(),
      data=tmp_array.data(),
      sigmas=None).set_observation_type( tmp_array )
  twin_results = twin_analyses(
    miller_array=tmp_array,
    d_star_sq_low_limit=1.0/100.0, # XXX need to confirm
    d_star_sq_high_limit=1.0/(0.001**2.0), # XXX need to confirm
    completeness_as_non_anomalous=completeness_as_non_anomalous,
    out = null_out(),
    out_plots = null_out(),
    verbose=False)
  summary = twin_results.twin_summary
  summary.show(out=log)
  return summary

def twin_analyses_brief(miller_array,
                        cut_off=2.5,
                        completeness_as_non_anomalous=None,
                        out = None,
                        verbose=0):
  """
  A very brief twin analyses and tries to answer the question whether or
  not the data are twinned.
  possible outputs and the meaning:
  - False: data are not twinned
  - True : data do not behave as expected. One possible explanantion
           is twinning
  - None : data do not behave as expected, and might or might not be
           due to twinning.
           Also gives none when something messes up.
  """
  if (out is None) and (verbose != 0):
    out = sys.stdout
  summary = miller_array.analyze_intensity_statistics(d_min=cut_off,
    completeness_as_non_anomalous=completeness_as_non_anomalous,
    log=out, )
  return summary.has_twinning()


 *******************************************************************************
