

 *******************************************************************************
xfel/command_line/FEE_average_plot.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from psana import *
import numpy as np
from libtbx import easy_pickle
import iotbx.phil, sys
import libtbx.load_env
from libtbx.utils import Sorry, Usage

master_phil = """
  dispatch{
    events_begin = None
    .type = int
    .help = If not specified, process all events. Otherwise, process events beginning at this number.
    events_end = None
    .type = int
    .help = If not specified, process all events. Otherwise, process events ending at this number.
    max_events = None
    .type = int
    .help = If not specified, process all events. Otherwise, only process this many
    events_accepted = False
    .type = bool
    .help = Plot average of filtered events
    events_rejected = False
    .type = bool
    .help = Plot average of rejected events
    events_all = False
    .type = bool
    .help = Plot average of all events
  }
  input {
    cfg = None
    .type = str
    .help = Path to psana config file
    experiment = None
    .type = str
    help = Experiment identifier, e.g. cxi84914
    run_num = None
    .type = int
    .help = Run number or run range to process
    address = None
    .type = str
    .help = FEE detector address, e.g. FEE-SPEC0
    dark = None
    .type = str
    .help = Path to FEE dark pickle file
    pixel_to_eV{
      energy_per_px = None
      .type = float
      .help = Energy per pixel conversion if known
      x_coord_one = None
      .type = int
      .help = Pixel valued x coordinate of known energy y.
       y_coord_one = None
      .type = int
      .help = Energy in eV of y coordinate of known x pixel position.
      x_coord_two = None
      .type = int
      .help = Pixel valued x coordinate of known energy y.
       y_coord_two = None
      .type = int
      .help = Energy in eV of y coordinate of known x pixel position.
    }
  }
  output {
    output_dir = .
    .type = str
    .help = Directory output files will be placed
  }
"""

def run(args):
  phil = iotbx.phil.process_command_line(args=args, master_string=master_phil).show()
  usage = \
  """ %s input.experiment=experimentname input.run_num=N input.address=address
  """%libtbx.env.dispatcher_name

  params = phil.work.extract()
  if not os.path.exists(params.output.output_dir):
    raise Sorry("Output path not found:" + params.output.output_dir)

  if params.input.experiment is None or \
  params.input.run_num is None or \
  params.input.address is None:
    raise Usage(usage)
  # set up psana
  if params.dispatch.events_accepted or params.dispatch.events_rejected:
    assert params.input.cfg is not None
    setConfigFile(params.input.cfg)

  dataset_name = "exp=%s:run=%s:idx"%(params.input.experiment,params.input.run_num)
  ds = DataSource(dataset_name)
  src = Source('DetInfo(%s)'%params.input.address)
  # set up multiprocessing with MPI
  from libtbx.mpi4py import MPI
  comm = MPI.COMM_WORLD
  rank = comm.Get_rank() # each process in MPI has a unique id, 0-indexed
  size = comm.Get_size() # size: number of processes running in this job

  if params.dispatch.max_events is None:
    max_events = sys.maxsize
  else:
    max_events = params.dispatch.max_events
  if params.input.dark is not None:
    dark = easy_pickle.load('%s'%params.input.dark)
  for run in ds.runs():
    times = run.times()
    if (params.dispatch.events_begin is None and params.dispatch.events_end is None):
      times = times[:]
    elif (params.dispatch.events_begin is not None and params.dispatch.events_end is None):
      times = times[params.dispatch.events_begin:]
    elif (params.dispatch.events_begin is None and params.dispatch.events_end is not None):
      times = times[:params.dispatch.events_end]
    elif (params.dispatch.events_begin is not None and params.dispatch.events_end is not None):
      times = times[params.dispatch.events_begin:params.dispatch.events_end]
    nevents = min(len(times),max_events)
  # chop the list into pieces, depending on rank.  This assigns each process
  # events such that the get every Nth event where N is the number of processes
    mytimes = [times[i] for i in range(nevents) if (i+rank)%size == 0]
    print(len(mytimes))
    #mytimes = mytimes[len(mytimes)-1000:len(mytimes)]
    totals = np.array([0.0])
    print("initial totals", totals)

    for i, t in enumerate(mytimes):
      print("Event", i, "of", len(mytimes), end=' ')
      evt = run.event(t)
      if params.dispatch.events_accepted or params.dispatch.events_all:
        if evt.get("skip_event")==True:
          continue
      elif params.dispatch.events_rejected:
        if evt.get("skip_event")==False:
          continue
      try:
        data = evt.get(Camera.FrameV1,src)
      except ValueError as e:
        src = Source('BldInfo(%s)'%params.input.address)
        data = evt.get(Bld.BldDataSpectrometerV1, src)
      if data is None:
        print("No data")
        continue
      #set default to determine FEE data type
      two_D=False
      #check attribute of data for type
      try:
        data = np.array(data.data16().astype(np.int32))
        two_D=True
      except AttributeError as e:
        data = np.array(data.hproj().astype(np.float64))

      if two_D:
        if 'dark' in locals():
          data = data - dark
        one_D_data = np.sum(data,0)/data.shape[0]
        two_D_data = np.double(data)
      else:
      #used to fix underflow problem that was present in earlier release of psana and pressent for LH80
        for i in range(len(data)):
          if data[i]>1000000000:
            data[i]=data[i]-(2**32)
        if 'dark' in locals():
          data = data - dark
        one_D_data = data

      totals[0] += 1
      print("total good:", totals[0])

      if not 'fee_one_D' in locals():
        fee_one_D = one_D_data
      else:
        fee_one_D += one_D_data
      if ('two_D_data' in locals() and not 'fee_two_D' in locals()):
        fee_two_D = two_D_data
      elif 'fee_two_D' in locals():
        fee_two_D += two_D_data

    acceptedtotals = np.zeros(totals.shape)
    acceptedfee1 = np.zeros((fee_one_D.shape))
    if 'fee_two_D' in locals():
      acceptedfee2 = np.zeros((fee_two_D.shape))
    print("Synchronizing rank", rank)
  comm.Reduce(fee_one_D,acceptedfee1)
  comm.Reduce(totals,acceptedtotals)
  if 'acceptedfee2' in locals():
    comm.Reduce(fee_two_D,acceptedfee2)
  print("number averaged", acceptedtotals[0])
  if rank == 0:
    if acceptedtotals[0] > 0:
      acceptedfee1 /= acceptedtotals[0]
      if 'acceptedfee2' in locals():
        acceptedfee2 /= acceptedtotals[0]

    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    from pylab import savefig,close
    from matplotlib.backends.backend_pdf import PdfPages
    import matplotlib.pyplot as plt
    from matplotlib import cm

    if params.dispatch.events_accepted:
      easy_pickle.dump(os.path.join(params.output.output_dir,"fee_avg_1_D_"+'r%s'%params.input.run_num+"_accepted.pickle"), acceptedfee1)
      pp1 = PdfPages(os.path.join(params.output.output_dir,"fee_avg_1_D_"+'r%s'%params.input.run_num+"_accepted.pdf"))
      if 'acceptedfee2' in locals():
        easy_pickle.dump(os.path.join(params.output.output_dir,"fee_avg_2_D_"+'r%s'%params.input.run_num+"_accepted.pickle"), acceptedfee2)
        pp2 = PdfPages(os.path.join(params.output.output_dir,"fee_avg_2_D_"+'r%s'%params.input.run_num+"_accepted.pdf"))
    if params.dispatch.events_all:
      easy_pickle.dump(os.path.join(params.output.output_dir,"fee_avg_1_D_"+'r%s'%params.input.run_num+"_all.pickle"), acceptedfee1)
      pp1 = PdfPages(os.path.join(params.output.output_dir,"fee_avg_1_D_"+'r%s'%params.input.run_num+"_all.pdf"))
      if 'acceptedfee2' in locals():
        easy_pickle.dump(os.path.join(params.output.output_dir,"fee_avg_2_D_"+'r%s'%params.input.run_num+"_all.pickle"), acceptedfee2)
        pp2 = PdfPages(os.path.join(params.output.output_dir,"fee_avg_2_D_"+'r%s'%params.input.run_num+"_all.pdf"))
    if params.dispatch.events_rejected:
      easy_pickle.dump(os.path.join(params.output.output_dir,"fee_avg_1_D_"+'r%s'%params.input.run_num+"_rejected.pickle"), acceptedfee1)
      pp1 = PdfPages(os.path.join(params.output.output_dir,"fee_avg_1_D_"+'r%s'%params.input.run_num+"_rejected.pdf"))
      if 'acceptedfee2' in locals():
        easy_pickle.dump(os.path.join(params.output.output_dir,"fee_avg_2_D_"+'r%s'%params.input.run_num+"_rejected.pickle"), acceptedfee2)
        pp2 = PdfPages(os.path.join(params.output.output_dir,"fee_avg_2_D_"+'r%s'%params.input.run_num+"_rejected.pdf"))
    print("Done")
   #plotting result
   # matplotlib needs a different backend when run on the cluster nodes at SLAC
    # these two lines not needed when working interactively at SLAC, or on mac or on viper

    if params.input.pixel_to_eV.energy_per_px is not None:
      xvals = (np.array(range(acceptedfee1.shape[0]))-params.input.pixel_to_eV.x_coord_one)*params.input.pixel_to_eV.energy_per_px+params.input.pixel_to_eV.y_coord_one
      xvals = xvals[::-1]

    if params.input.pixel_to_eV.x_coord_two is not None:
      eV_per_px = (params.input.pixel_to_eV.y_coord_two-params.input.pixel_to_eV.y_coord_one)/(params.input.pixel_to_eV.x_coord_two-params.input.pixel_to_eV.x_coord_one)
      xvals = (np.array(range(acceptedfee1.shape[0]))-params.input.pixel_to_eV.x_coord_one)*eV_per_px+params.input.pixel_to_eV.y_coord_one
      xvals = xvals[::-1]

    if params.input.pixel_to_eV.x_coord_two is None and params.input.pixel_to_eV.energy_per_px is None:
      xvals=np.arange(0,len(acceptedfee1),1)

    yvals = acceptedfee1
    def OneD_plot(X,Y):
      plt.figure()
      plt.clf()
      plt.plot(X,Y)
      if params.dispatch.events_accepted:
        plt.title('Accepted Shots FEE Spectrum Run %s'%params.input.run_num)
      elif params.dispatch.events_all:
        plt.title('All Shots FEE Spectrum Run %s'%params.input.run_num)
      elif params.dispatch.events_rejected:
        plt.title('Rejected Shots FEE Spectrum Run %s'%params.input.run_num)
      if params.input.pixel_to_eV.x_coord_one is not None:
        plt.xlabel('eV', fontsize = 13)
      else:
        plt.xlabel('pixels', fontsize = 13)
      plt.ylabel('pixels', fontsize = 13)
      pp1.savefig()

    def TwoD_plot(data):
      plt.figure()
      ax = plt.gca()
     #  use specified range 0, 50 to plot runs 117 - 201
     #min=0, vmax=50
      cax=ax.imshow(data, interpolation='nearest',origin='lower',cmap=cm.coolwarm)
      plt.colorbar(cax, fraction=0.014, pad=0.04)
      if params.dispatch.events_accepted:
        ax.set_title('Accepted 2-D FEE Spectrum Run %s'%params.input.run_num)
      elif params.dispatch.events_all:
        ax.set_title('All 2-D FEE Spectrum Run %s'%params.input.run_num)
      elif params.dispatch.events_rejected:
        ax.set_title('Rejected 2-D FEE Spectrum Run %s'%params.input.run_num)
        pp2.savefig()

    OneD_plot(xvals,yvals)
    pp1.close()
    if 'acceptedfee2' in locals():
      TwoD_plot(acceptedfee2)
      pp2.close()



if __name__ == "__main__":
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/command_line/auto_submit.py
from __future__ import absolute_import, division, print_function

#-----------------------------------------------------------------------
# Monitor a directory for streams and submit them
#-----------------------------------------------------------------------

import libtbx.phil
try:
  from cxi_xdr_xes.cftbx.cspad_ana import db as db
except ImportError: pass
import os
import sys
import time
import libtbx
from libtbx.utils import Usage, Sorry

master_phil = libtbx.phil.parse("""
  xtc_dir = None
    .type = path
  output_dir = None
    .type = path
  trial_id = None
    .type = int
  stream_count = 5
    .type = int
  num_procs = 12
    .type = int
  queue = 'psfehmpiq'
    .type = str
  start_run = None
    .type = int
  end_run = None
    .type = int
  config_file = 'onlyhitfind.cfg'
    .type = str
  experiment = None
    .type = str
    .help = Optional. If blank, auto_submit will use data in the xtc_dir path
  submit_as_group = True
    .type = bool
  use_in_progress = False
    .type = bool
""")

submitted_runs = []

class _run:
  def __init__(self, id):
    self.id = id
    self.files = []

  def __eq__(self, other): return other == self.id
  def __ne__(self, other): return not __eq__(self,other)

  def __repr__(self): return f"Run: {self.id}, files: {','.join(self.files)}"

  def max_chunks(self):
    streams = {}
    for file in self.files:
      for str in file.split("-"):
        try:
          if 'c' in str:
            c = int(str.split(".")[0].strip('c'))
          if 'r' in str:
            r = int(str.strip('r'))
          elif 's' in str:
            s = int(str.strip('s'))
        except ValueError:
          pass
      assert r is not None and s is not None and c is not None # and c == 0:
      if s not in streams:
        streams[s] = 0
      streams[s] += 1
    return max([streams[key] for key in streams])


def match_runs(dir,use_in_progress):
  runs = []
  files = os.listdir(dir)
  for file in files:
    r = s = c = None
    if "s80" in file or "s81" in file:
      continue
    if not use_in_progress and ".inprogress" in file:
      continue
    for str in file.split("-"):
      try:
        if 'c' in str:
          c = int(str.split(".")[0].strip('c'))
        if 'r' in str:
          r = int(str.strip('r'))
        elif 's' in str:
          s = int(str.strip('s'))
      except ValueError:
        pass
    if r is not None and s is not None and c is not None: # and c == 0:
      foundIt = False
      for rn in runs:
        if rn.id == r:
          foundIt = True
          if not file in rn.files:
            rn.files.append(file)
      if not foundIt:
        rn = _run(r)
        rn.files.append(file)
        runs.append(rn)
  return runs

def run (args) :
  path = os.getcwd()
  if os.path.basename(path) != "myrelease":
    raise Sorry("You must run this script from within your pyana myrelease directory.")

  user_phil = []
  # TODO: replace this stuff with iotbx.phil.process_command_line_with_files
  # as soon as I can safely modify it
  for arg in args :
    if (os.path.isdir(arg)) :
      user_phil.append(libtbx.phil.parse("""xtc_dir=\"%s\"""" % arg))
    elif (not "=" in arg) :
      try :
        user_phil.append(libtbx.phil.parse("""trial_id=%d""" % int(arg)))
      except ValueError as e :
        raise Sorry("Unrecognized argument '%s'" % arg)
    else :
      try :
        user_phil.append(libtbx.phil.parse(arg))
      except RuntimeError as e :
        raise Sorry("Unrecognized argument '%s' (error: %s)" % (arg, str(e)))
  params = master_phil.fetch(sources=user_phil).extract()
  if (params.trial_id is None) :
    master_phil.show()
    raise Usage("trial_id must be defined (either trial_id=XXX, or the integer "+
      "ID alone).")
  if (params.xtc_dir is None) :
     master_phil.show()
     raise Usage("xtc_dir must be defined!")
  elif (not os.path.isdir(params.xtc_dir)) :
     raise Sorry("%s does not exist or is not a directory!" % params.xtc_dir)

  if (params.output_dir is None) :
     master_phil.show()
     raise Usage("output_dir must be defined!")
  elif (not os.path.isdir(params.output_dir)) :
     raise Sorry("%s does not exist or is not a directory!" % params.output_dir)

  if (params.config_file is None) :
     master_phil.show()
     raise Usage("config_file must be defined!")
  elif (not os.path.isfile(params.config_file)) :
     raise Sorry("%s does not exist or is not a file!" % params.config_file)

  assert (params.stream_count is not None) and (params.stream_count > 0)
  assert (params.num_procs is not None) and (params.num_procs > 0)
  assert (params.queue is not None)
  assert (params.submit_as_group is not None)
  assert (params.use_in_progress is not None)
  if params.experiment is None:
    input_str = "-i %s"%params.xtc_dir
  else:
    input_str = "-x %s"%params.experiment

  submitted_runs = []
  submitted_files = [] # used in single stream submit mode

  print("Note, it is not recommended that you run this program while you have new jobs pending.")
  print("Starting indefinite loop to scan directory '%s'"%params.xtc_dir)

  if params.submit_as_group:
    try:
      while(True):
        rs = match_runs(params.xtc_dir,params.use_in_progress)
        add_runs = []
        for r in rs:
          if not ((params.start_run is not None and r.id < params.start_run) or (params.end_run is not None and r.id > params.end_run)):
            if not db.run_in_trial(r.id, params.trial_id):
              doit = True
              for test in submitted_runs:
                if test.id == r.id:
                  doit = False
                  break
              if doit: add_runs.append(r)

        submitted_a_run = False
        if len(add_runs) > 0:
          for r in add_runs:
            if len(r.files) < params.stream_count * r.max_chunks():
              print("Waiting to queue run %s.  %s/%s streams ready."% \
                (r.id,len(r.files),params.stream_count * r.max_chunks()))
              continue

            print("Preparing to queue run %s into trial %s"%(r.id,params.trial_id))
            cmd = "cxi.lsf -c %s -p %s %s -o %s -t %s -r %s -q %s"%(params.config_file,params.num_procs,input_str,
              params.output_dir,params.trial_id,r.id,params.queue)
            print("Command to execute: %s"%cmd)
            os.system(cmd)
            print("Run %s queued."%r.id)

            submitted_a_run = True
            submitted_runs.append(r)
        if not submitted_a_run:
          print("No new data... sleepy...")

        time.sleep(10)
    except KeyboardInterrupt:
      pass

  else: # submit streams singly
    try:
      while (True):
        submitted_a_run = False
        files = os.listdir(params.xtc_dir)
        for f in files:
          r = s = c = None
          if not params.use_in_progress and ".inprogress" in f:
            continue
          for str in f.split("-"):
            try:
              if 'c' in str:
                c = int(str.split(".")[0].strip('c'))
              if 'r' in str:
                r = int(str.strip('r'))
              elif 's' in str:
                s = int(str.strip('s'))
            except ValueError:
              pass
          if r is not None and s is not None and c is not None and c == 0:
            if not ((params.start_run is not None and r.id < params.start_run) or (params.end_run is not None and r.id > params.end_run)):
            #  if not db.run_in_trial(r.id, params.trial_id):  can't check for this when queueing streams.  can lead to duplicate data.
              if not f in submitted_files:

                print("Preparing to queue stream %s into trial %s"%(os.path.basename(f),params.trial_id))
                cmd = "./single_lsf.sh -c %s -p %s %s -o %s -t %s -r %s -q %s -s %s"%(params.config_file,params.num_procs,input_str,
                  params.output_dir,params.trial_id,r,params.queue,s)

                print("Command to execute: %s"%cmd)
                os.system(cmd)
                print("Run %s stream %s queued."%(r,s))

                submitted_a_run = True
                submitted_files.append(f)
                if '.inprogress' in f:
                  submitted_files.append(f.rstrip(".inprogress"))
        if not submitted_a_run:
          print("No new data... sleepy...")

        time.sleep(10)
    except KeyboardInterrupt:
      pass



if (__name__ == "__main__") :
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/beamcenter.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# LIBTBX_SET_DISPATCHER_NAME xpp.beamcenter
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1

import sys,os
from scitbx.array_family import flex
from scitbx.matrix import sqr, col
from math import sin, cos, pi
import dxtbx
from xfel.metrology.legacy_scale import quadrant_self_correlation
import libtbx.phil

master_phil = libtbx.phil.parse("""
  beam_center_fast = None
    .type = int
    .help = Initial estimate of beam center (fast coordinate)
  beam_center_slow = None
    .type = int
    .help = Initial estimate of beam center (slow coordinate)
  px_max = None
    .type = int
    .help = Only test pixels this distance or less from the initial beam center estimate
  px_min = None
    .type = int
    .help = Only test pixels this distance or more from the initial beam center estimate
  show_plots = False
    .type = bool
    .help = If True, show the pixels that will be tested and the rotational self-correlations from the grid search
""")

if (__name__ == "__main__"):
  files = [arg for arg in sys.argv[1:] if os.path.isfile(arg)]
  arguments = [libtbx.phil.parse(arg) for arg in sys.argv[1:] if not os.path.isfile(arg)]
  params = master_phil.fetch(sources=arguments).extract()

  rot45 = sqr((sin(pi/4.),-cos(pi/4.),cos(pi/4.),sin(pi/4.)))

  for file in files:
    message="""Based on the file %s, this program will compute incremental translations to circularize
    powder rings.  The algorithm  scores based on self-correlation upon 45-degree rotation.
    Increments are determined ON TOP OF beam center value in image header.  Output is given in the form of
    delta to that value."""%file
    print(message)

    img = dxtbx.load(file)
    beam = img.get_beam()
    s0 = beam.get_s0()

    raw_data = img.get_raw_data()
    if not isinstance(raw_data, tuple):
      raw_data = (raw_data,)

    for panel_id, panel in enumerate(img.get_detector()):
      beam_center = col(panel.get_beam_centre_px(s0))
      data = raw_data[panel_id]

      print("Assembling mask...", end=' '); sys.stdout.flush()
      mask = panel.get_trusted_range_mask(data)
      trusted_min = panel.get_trusted_range()[0]

      mask_center = col((params.beam_center_slow,params.beam_center_fast))
      px_max = params.px_max
      px_min = params.px_min
      data = data[mask_center[0] - px_max:mask_center[0] + px_max, mask_center[1] - px_max:mask_center[1] + px_max]
      mask = mask[mask_center[0] - px_max:mask_center[0] + px_max, mask_center[1] - px_max:mask_center[1] + px_max]
      panel_origin = col((mask_center[0] - px_max,mask_center[1] - px_max))

      for y in range(mask.focus()[1]):
        for x in range(mask.focus()[0]):
          l = (col((x-px_max+mask_center[0],y-px_max+mask_center[1])) - mask_center).length()
          if l < px_min or l > px_max:
            mask[x,y] = False

      data.set_selected(~mask, trusted_min-1)
      print("done")
      if params.show_plots:
        from matplotlib import pyplot as plt
        plt.imshow(data.as_numpy_array())
        plt.show()

      grid_radius = 20
      mapp = flex.double(flex.grid(2*grid_radius+1, 2*grid_radius+1))
      print(mapp.focus())

      gmax = 0.0
      coordmax = (0,0)
      for xi in range(-grid_radius, grid_radius+1):
        for yi in range(-grid_radius, grid_radius+1):
          test_bc = beam_center + col((xi,yi))
          print("Testing beam center", test_bc.elems, end=' ')
          REF,ROT = quadrant_self_correlation(data,panel_origin,test_bc,rot45,trusted_min)
          CCRR = flex.linear_correlation(REF,ROT)
          VV = CCRR.coefficient()
          if VV>gmax:
            gmax = VV
            coordmax = col((xi,yi))
          mapp[(xi+grid_radius,yi+grid_radius)]=VV
          print(VV)

      print("max cc %7.4F is at "%gmax, (beam_center + coordmax).elems, "(slow, fast). Delta:", coordmax.elems)
      if params.show_plots:
        npy = mapp.as_numpy_array()
        from matplotlib import pyplot as plt
        plt.imshow(npy, cmap="hot")
        plt.plot([coordmax[1]+grid_radius],[coordmax[0]+grid_radius],"k.")
        plt.show()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/brehm_diederichs.py
# LIBTBX_SET_DISPATCHER_NAME cxi.brehm_diederichs
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1
from __future__ import absolute_import, division, print_function

import iotbx.phil
from cctbx.crystal import symmetry
from libtbx.utils import Usage # implicit import
from libtbx.utils import multi_out
import sys
from xfel.cxi.util import is_odd_numbered # implicit import
from xfel.command_line.cxi_merge import master_phil
#-----------------------------------------------------------------------
from xfel.command_line.cxi_xmerge import xscaling_manager

extra_phil = """
asymmetric = 3
  .type = int(value_min=0, value_max=3)
show_plot = True
  .type = bool
save_plot = False
  .type = bool
"""

#-----------------------------------------------------------------------
def run(args):
  phil = iotbx.phil.process_command_line(args=args, master_string=master_phil + "\n" + extra_phil).show()
  work_params = phil.work.extract()
  log = open("%s_%s_merging.log" % (work_params.output.prefix,work_params.scaling.algorithm), "w")
  out = multi_out()
  out.register("log", log, atexit_send_to=None)
  out.register("stdout", sys.stdout)

  print("Target unit cell and space group:", file=out)
  print("  ", work_params.target_unit_cell, file=out)
  print("  ", work_params.target_space_group, file=out)

  miller_set = symmetry(
      unit_cell=work_params.target_unit_cell,
      space_group_info=work_params.target_space_group
    ).build_miller_set(
      anomalous_flag=not work_params.merge_anomalous,
      d_min=work_params.d_min)
  from xfel.merging.general_fcalc import random_structure
  i_model = random_structure(work_params)

# ---- Augment this code with any special procedures for x scaling
  scaler = xscaling_manager(
    miller_set=miller_set,
    i_model=i_model,
    params=work_params,
    log=out)
  scaler.read_all_mysql()
  print("finished reading the database")
  sg = miller_set.space_group()

  hkl_asu = scaler.observations["hkl_id"]
  imageno = scaler.observations["frame_id"]
  intensi = scaler.observations["i"]
  lookup = scaler.millers["merged_asu_hkl"]
  origH = scaler.observations["H"]
  origK = scaler.observations["K"]
  origL = scaler.observations["L"]

  from cctbx.array_family import flex

  print("# observations from the database",len(scaler.observations["hkl_id"]))
  hkl = flex.miller_index(flex.select(lookup,hkl_asu))
  from cctbx import miller

  hkl_list = miller_set.customized_copy(indices = hkl)

  ARRAY = miller.array(miller_set = hkl_list, data = intensi)
  LATTICES = miller.array(miller_set = hkl_list, data = imageno)

  from cctbx.merging.brehm_diederichs import run_multiprocess, run

  L = (ARRAY, LATTICES) # tuple(data,lattice_id)
  from libtbx import easy_pickle
  presort_file = work_params.output.prefix+"_intensities_presort.pickle"
  print("pickling these intensities to", presort_file)
  easy_pickle.dump(presort_file,L)

    ######  INPUTS #######
    #       data = miller array: ASU miller index + intensity (sigmas not implemented yet)
    #       lattice_id = flex double: assignment of each miller index to a lattice number
    ######################
  if work_params.nproc < 5:
    print("Sorting the lattices with 1 processor")
    result = run(L, asymmetric=work_params.asymmetric, nproc=1, verbose=True,
                 show_plot=work_params.show_plot, save_plot=work_params.save_plot)
  else:
    print("Sorting the lattices with %d processors"%work_params.nproc)
    result = run_multiprocess(L, asymmetric=work_params.asymmetric, nproc=work_params.nproc, verbose=False,
                              show_plot=work_params.show_plot, save_plot=work_params.save_plot)
  for key in result.keys():
    print(key,len(result[key]))

  # 2) pickle the postsort (reindexed) ARRAY, LATTICES XXX not done yet; not clear if needed

  reverse_lookup = {}
  frame_id_list = list(scaler.frames_mysql["frame_id"])
  for key in result.keys():
    for frame in result[key]:
      frame_idx = frame_id_list.index(frame)
      reverse_lookup[scaler.frames_mysql["unique_file_name"][frame_idx]] = key

  lookup_file = work_params.output.prefix+"_lookup.pickle"
  reverse_lookup_file = work_params.output.prefix+"_reverse_lookup.pickle"
  easy_pickle.dump(lookup_file, result)
  easy_pickle.dump(reverse_lookup_file, reverse_lookup)


if (__name__ == "__main__"):
  import sys
  result = run(args = sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/calc_gain_ratio.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.calc_gain_ratio
#
import dxtbx, sys
from libtbx import phil
from libtbx.utils import Sorry
import psana
from scitbx.array_family import flex
from six.moves import zip

help_message = """
Program to compute the gain ratio for the CSPAD detector in mixed gain mode. For each pair of adjacent pixels, if one is in high gain mode and the other in low gain mode, then the ratio between them is computed. These ratios are averaged for each panel and the average over the whole detector is reported.

Inputs: requires an averaged run in CBF format and access to psana (to extract the original gain mask for the run)

Example usage:
calc_gain_ratio.py average=cxid9114_avg-r0097.cbf experiment=cxid9114 run=97 address=CxiDs2.0:Cspad.0
"""

phil_str = """
  experiment = None
    .type = str
    .help = LCLS experiment name
  run=None
    .type = int
    .help = Run number
  average = None
    .type = str
    .help = Averaged image (can be maximum projection instead)
  address = None
    .type = str
    .help = Detector address, eg CxiDs1.0:Cspad.0
"""

phil_scope = phil.parse(phil_str)

def run(args):
  if "-c" in args or "-h" in args or "--help" in args:
    print(help_message)
  user_phil = []
  for arg in args :
    try :
      user_phil.append(phil.parse(arg))
    except RuntimeError as e :
      raise Sorry("Unrecognized argument '%s' (error: %s)" % (arg, str(e)))
  params = phil_scope.fetch(sources=user_phil).extract()

  img = dxtbx.load(params.average)
  dataset_name = "exp=%s:run=%s:idx"%(params.experiment,params.run)
  ds = psana.DataSource(dataset_name)
  run = next(ds.runs())

  psana_det = psana.Detector(params.address, ds.env())
  psana_gain_mask = psana_det.gain_mask()
  psana_gain_mask = flex.bool(psana_gain_mask==1)

  gain_masks = []
  assert psana_gain_mask.focus() == (32, 185, 388)
  for i in range(32):
    gain_masks.append(psana_gain_mask[i:i+1,:,:194])
    gain_masks[-1].reshape(flex.grid(185,194))
    gain_masks.append(psana_gain_mask[i:i+1,:,194:])
    gain_masks[-1].reshape(flex.grid(185,194))

  ratios = flex.double()
  counts = flex.int()
  for panel_id, (data, mask) in enumerate(zip(img.get_raw_data(), gain_masks)):
    if mask.all_eq(True) or mask.all_eq(False):
      continue

    panel_sum = 0
    panel_count = 0
    for s in range(data.focus()[1]):
      for f in range(data.focus()[0]):
        if f+1 == data.focus()[0]:
          continue
        if (not mask[f,s]) and mask[f+1,s] and data[f+1,s] != 0:
          panel_sum += data[f,s]/data[f+1,s]
          panel_count += 1
        elif mask[f,s] and not mask[f+1,s] and data[f,s] != 0:
          panel_sum += data[f+1,s]/data[f,s]
          panel_count += 1
    if panel_count > 0:
      ratio = panel_sum/panel_count
      ratios.append(ratio)
      counts.append(panel_count)
      print("Panel", panel_id, "ratio:", ratio, "N pairs", panel_count)

  if len(ratios) <= 1:
    return
  print("Mean:", flex.mean(ratios))
  print("Standard deviation", flex.mean_and_variance(ratios).unweighted_sample_standard_deviation())

  stats = flex.mean_and_variance(ratios, counts.as_double())
  print("Weighted mean:", stats.mean())
  print("Weighted standard deviation", stats.gsl_stats_wsd())

if __name__ == "__main__":
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cbf_average.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.cspad_average
#
# $Id

from __future__ import absolute_import, division, print_function
from six.moves import range

import sys, copy
from six.moves import zip

def run(argv=None):
  """Compute mean, standard deviation, and maximum projection images
  from a set of CSPAD cbf images given on the command line.

  @param argv Command line argument list
  @return     @c 0 on successful termination, @c 1 on error, and @c 2
              for command line syntax errors
  """

  import libtbx.load_env

  from libtbx import option_parser
  from scitbx.array_family import flex
  import dxtbx.format.Registry
  from xfel.cftbx.detector.cspad_cbf_tbx import cbf_file_to_basis_dict, write_cspad_cbf
#  from xfel.cxi.cspad_ana import cspad_tbx
#  from iotbx.detectors.cspad_detector_formats import reverse_timestamp

  if argv is None:
    argv = sys.argv
  command_line = (option_parser.option_parser(
    usage="%s [-v] [-a PATH] [-m PATH] [-s PATH] " \
    "image1 image2 [image3 ...]" % libtbx.env.dispatcher_name)
                  .option(None, "--average-path", "-a",
                          type="string",
                          default=None,
                          dest="avg_path",
                          metavar="PATH",
                          help="Write average image to PATH")
                  .option(None, "--maximum-path", "-m",
                          type="string",
                          default=None,
                          dest="max_path",
                          metavar="PATH",
                          help="Write maximum projection image to PATH")
                  .option(None, "--stddev-path", "-s",
                          type="string",
                          default=None,
                          dest="stddev_path",
                          metavar="PATH",
                          help="Write standard deviation image to PATH")
                  .option(None, "--verbose", "-v",
                          action="store_true",
                          default=False,
                          dest="verbose",
                          help="Print more information about progress")
                  ).process(args=argv[1:])

  # Note that it is not an error to omit the output paths, because
  # certain statistics could still be printed, e.g. with the verbose
  # option.
  paths = command_line.args
  if len(paths) == 0:
    command_line.parser.print_usage(file=sys.stderr)
    return 2

  # Loop over all images and accumulate statistics.
  nfail = 0
  nmemb = 0
  for path in paths:
    if command_line.options.verbose:
      sys.stdout.write("Processing %s...\n" % path)

    try:
      # Promote the image to double-precision floating point type.
      # All real-valued flex arrays have the as_double() function.
      # Warn if the header items across the set of images do not match
      # up.  Note that discrepancies regarding the image size are
      # fatal.
      if not 'reader' in locals():
        reader = dxtbx.format.Registry.get_format_class_for_file(path)
      img = reader(path)
      if 'detector' in locals():
        test_detector = img.get_detector()
        if len(test_detector) != len(detector):
          sys.stderr.write("Detectors do not have the same number of panels\n")
          return 1
        for t, d in zip(test_detector, detector):
          if t.get_image_size() != d.get_image_size():
            sys.stderr.write("Panel sizes do not match\n")
            return 1
          if t.get_pixel_size() != d.get_pixel_size():
            sys.stderr.write("Pixel sizes do not match\n")
            return 1
          if t.get_d_matrix() != d.get_d_matrix():
            sys.stderr.write("Detector panels are not all in the same location. The average will use the positions of the first image.\n")
        detector = test_detector
      else:
        detector = img.get_detector()

      data = [img.get_raw_data()[i].as_1d().as_double() for i in range(len(detector))]
      wavelength = img.get_beam().get_wavelength()
      distance = flex.mean(flex.double([d.get_directed_distance() for d in detector]))

    except Exception:
      nfail += 1
      continue

    # The sum-of-squares image is accumulated using long integers, as
    # this delays the point where overflow occurs.  But really, this
    # is just a band-aid...
    if nmemb == 0:
      max_img = copy.deepcopy(data)
      sum_distance = distance
      sum_img = copy.deepcopy(data)
      ssq_img = [flex.pow2(d) for d in data]
      sum_wavelength = wavelength
      metro = cbf_file_to_basis_dict(path)

    else:
      sel = [(d > max_d).as_1d() for d, max_d in zip(data, max_img)]
      for d, max_d, s in zip(data, max_img, sel): max_d.set_selected(s, d.select(s))

      sum_distance += distance
      for d, sum_d in zip(data, sum_img): sum_d += d
      for d, ssq_d in zip(data, ssq_img): ssq_d += flex.pow2(d)
      sum_wavelength += wavelength

    nmemb += 1

  # Early exit if no statistics were accumulated.
  if command_line.options.verbose:
    sys.stderr.write("Processed %d images (%d failed)\n" % (nmemb, nfail))
  if nmemb == 0:
    return 0

  # Calculate averages for measures where other statistics do not make
  # sense.  Note that avg_img is required for stddev_img.
  avg_img = [sum_d.as_double() / nmemb for sum_d in sum_img]
  avg_distance = sum_distance / nmemb
  avg_wavelength = sum_wavelength / nmemb

  def make_tiles(data, detector):
    """
    Assemble a tiles dictionary as required by write_cspad_cbf, consisting of 4 arrays of shape 8x185x388.
    Assumes the order in the data array matches the order of the enumerated detector panels.
    """
    assert len(data) == 64
    tiles = {}
    s, f = 185, 194

    for q_id in range(4):
      tiles[0,q_id] = flex.double((flex.grid(s*8, f*2)))
      for s_id in range(8):
        for a_id in range(2):
          asic_idx = (q_id*16) + (s_id*2) + a_id
          asic = data[asic_idx]
          asic.reshape(flex.grid((s, f)))

          tiles[0, q_id].matrix_paste_block_in_place(asic, s_id*s, a_id*f)
      tiles[0, q_id].reshape(flex.grid((8, s, f*2)))

    return tiles


  # Output the average image, maximum projection image, and standard
  # deviation image, if requested.
  if command_line.options.avg_path is not None:
    tiles = make_tiles(avg_img, detector)
    write_cspad_cbf(tiles, metro, 'cbf', None, command_line.options.avg_path, avg_wavelength, avg_distance)

  if command_line.options.max_path is not None:
    tiles = make_tiles(max_img, detector)
    write_cspad_cbf(tiles, metro, 'cbf', None, command_line.options.max_path, avg_wavelength, avg_distance)

  if command_line.options.stddev_path is not None:
    stddev_img = [ssq_d.as_double() - sum_d.as_double() * avg_d for ssq_d, sum_d, avg_d in zip(ssq_img, sum_img, avg_img)]

    # Accumulating floating-point numbers introduces errors, which may
    # cause negative variances.  Since a two-pass approach is
    # unacceptable, the standard deviation is clamped at zero.
    for stddev_d in stddev_img:
      stddev_d.set_selected(stddev_d < 0, 0)

    if nmemb == 1:
      stddev_img = [flex.sqrt(stddev_d) for stddev_d in stddev_img]
    else:
      stddev_img = [flex.sqrt(stddev_d / (nmemb - 1)) for stddev_d in stddev_img]

    tiles = make_tiles(stddev_img, detector)
    write_cspad_cbf(tiles, metro, 'cbf', None, command_line.options.stddev_path, avg_wavelength, avg_distance)

  return 0


if __name__ == '__main__':
  sys.exit(run())


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cctbx_integration_pickle_viewer.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# LIBTBX_SET_DISPATCHER_NAME cctbx.integration_pickle_viewer
from cctbx.array_family import flex # implicit dependency
from matplotlib import pyplot as plt
from six.moves import cPickle as pickle

def get_CSPAD_active_areas(image, version_phil):
  from libtbx.phil import parse
  from iotbx.detectors.npy import NpyImage
  from spotfinder.applications.xfel.cxi_phil import cxi_basic_start
  data = pickle.load(open(image, "rb"))
  scope = parse(file_name=version_phil)
  basic_scope = cxi_basic_start()
  new_scope = basic_scope.phil_scope.fetch(source=scope)
  phil = new_scope.extract()
  img = NpyImage("dummy", source_data=data)
  img.readHeader(phil)
  tm = img.get_tile_manager(phil)
  return list(tm.effective_tiling_as_flex_int())

LG36_active_areas = [724, 448, 918, 633, 527, 448, 721, 633, 724, 655, 918, 840, 527, 655, 721, 840, 524, 23, 709, 217, 524, 220, 709, 414, 731, 24, 916, 218, 731, 221, 916, 415, 100, 230, 294, 415, 297, 230, 491, 415, 99, 20, 293, 205, 296, 20, 490, 205, 119, 447, 304, 641, 119, 644, 304, 838, 328, 450, 513, 644, 328, 647, 513, 841, 446, 856, 631, 1050, 446, 1053, 631, 1247, 654, 856, 839, 1050, 654, 1053, 839, 1247, 26, 1065, 220, 1250, 223, 1065, 417, 1250, 25, 858, 219, 1043, 222, 858, 416, 1043, 232, 1475, 417, 1669, 232, 1278, 417, 1472, 24, 1476, 209, 1670, 24, 1279, 209, 1473, 448, 1463, 642, 1648, 645, 1463, 839, 1648, 448, 1258, 642, 1443, 645, 1258, 839, 1443, 855, 1143, 1049, 1328, 1052, 1143, 1246, 1328, 855, 937, 1049, 1122, 1052, 937, 1246, 1122, 1060, 1558, 1245, 1752, 1060, 1361, 1245, 1555, 856, 1558, 1041, 1752, 856, 1361, 1041, 1555, 1473, 1359, 1667, 1544, 1276, 1359, 1470, 1544, 1482, 1566, 1676, 1751, 1285, 1566, 1479, 1751, 1463, 1131, 1648, 1325, 1463, 934, 1648, 1128, 1257, 1132, 1442, 1326, 1257, 935, 1442, 1129, 1142, 729, 1327, 923, 1142, 532, 1327, 726, 937, 729, 1122, 923, 937, 532, 1122, 726, 1558, 531, 1752, 716, 1361, 531, 1555, 716, 1557, 734, 1751, 919, 1360, 734, 1554, 919, 1359, 106, 1544, 300, 1359, 303, 1544, 497, 1573, 101, 1758, 295, 1573, 298, 1758, 492, 1133, 127, 1327, 312, 936, 127, 1130, 312, 1133, 335, 1327, 520, 936, 335, 1130, 520]

def plot_preds(pdata, active_areas=LG36_active_areas):
  try:
    preds = pdata['mapped_predictions'][0]
    preds_flat = preds.as_double()
    preds_fast = preds_flat[0::2]
    preds_slow = preds_flat[1::2]
  except KeyError:
    print("pickle may not be an integration pickle! skipping...")
    return
  plt.scatter(preds_slow, preds_fast, c='blue', marker='.')
  for i in range(64):
    aa = active_areas[4*i:4*i+4]
    plt.plot(
      [aa[1], aa[1], aa[3], aa[3], aa[1]],
      [aa[0], aa[2], aa[2], aa[0], aa[0]]
            )
  ax = plt.gca()
  ax.set_xlim(0, 1800)
  ax.set_ylim(0, 1800)
  ax.invert_yaxis()
  plt.axes().set_aspect('equal')
  plt.show()

if __name__ == "__main__":
  import sys
  from libtbx.utils import Sorry
  import libtbx.option_parser
  cmd_line = (libtbx.option_parser.option_parser(
    usage="%s [--detector_version_phil phil] [--image image] integration_pickle(s)" % libtbx.env.dispatcher_name)
    .option(None, "--detector_version_phil", "-d",
            type="string",
            default=None,
            dest="det_phil",
            help="detector version phil for the CSPAD")
    .option(None, "--image", "-i",
            type="string",
            default=None,
            dest="det_image",
            help="image matching the detector version phil")
    ).process(args=sys.argv[1:])
  if cmd_line.options.det_phil is not None and cmd_line.options.det_image is not None:
    print("extracting active areas...")
    active_areas = get_CSPAD_active_areas(
      cmd_line.options.det_image,
      cmd_line.options.det_phil)
  elif cmd_line.options.det_phil is None and cmd_line.options.det_image is None:
    print("using active areas from LG36 CSPAD metrology")
    active_areas = LG36_active_areas
  else:
    raise Sorry("Specify both a detector version phil and an example image to extract active areas.")
  for arg in cmd_line.args:
    file = open(arg, "rb")
    data = pickle.load(file)
    file.close()
    plot_preds(data, active_areas=active_areas)


 *******************************************************************************


 *******************************************************************************
xfel/command_line/convert_gain_map.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# LIBTBX_SET_DISPATCHER_NAME cxi.gain_map
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT

import sys,time,math
import numpy

from libtbx import easy_pickle
import libtbx.load_env
from libtbx.option_parser import option_parser
from scitbx.array_family import flex
from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import parse_calib
from iotbx.detectors.cspad_detector_formats import address_and_timestamp_from_detector_format_version
from xfel.cxi.cspad_ana.cspad_tbx import evt_timestamp

# Fake objects to emulate the minimal functionality so that we can reuse the
# functions CsPad2x2Image and CsPadDetector in cspad_tbx.py
def fake_get_config(address, env):
  return fake_config()

cspad_tbx.getConfig = fake_get_config

class fake_cspad_ElementV2(object):
  def __init__(self, data, quad):
    self._data = data
    self._quad = quad

  def data(self):
    return self._data

  def quad(self):
    return self._quad

class fake_config(object):

  def sections(self, i):
    return list(range(8))

  def quadMask(self):
    return 15

  def roiMask(self, i):
    return 255

class fake_env(object):
  # XXX Not tested!

  def __init__(self, config):
    self._config = config

  def getConfig(self, Id, address):
    return self._config

class fake_evt(object):
  # XXX Not tested!

  def __init__(self, data3d):
    self._data3d = data3d

  def getCsPadQuads(self, address, env):
    return self._data3d

  def getTime(self):
    class fakeTime(object):
      def __init__(self):
        t = time.time()
        s = int(math.floor(t))
        self.s = s
        self.n = int(round((t - s) * 1000))

      def seconds(self): return self.s
      def nanoseconds(self): return self.n
    return fakeTime()

def run(args):
  command_line = (option_parser()
                  .option("-o", "--output_filename",
                          action="store",
                          type="string",
                          help="Filename for the output pickle file",
                          default="gain_map.pickle")
                  .option("-f", "--detector_format_version",
                          action="store",
                          type="string",
                          help="Detector format version to use for generating active areas and laying out tiles",
                          default=None)
                  .option("-m", "--optical_metrology_path",
                          action="store",
                          type="string",
                          help="Path to slac optical metrology file. If not set, use Run 4 metrology",
                          default=None)
                  .option("-d", "--distance",
                          action="store",
                          type="int",
                          help="Detector distance put into the gain pickle file. Not needed for processing.",
                          default="0")
                  .option("-w", "--wavelength",
                          action="store",
                          type="float",
                          help="Incident beam wavelength put into the gain pickle file. Not needed for processing.",
                          default="0")
                     ).process(args=args)
  output_filename = command_line.options.output_filename
  detector_format_version = command_line.options.detector_format_version
  if detector_format_version is None or 'XPP' not in detector_format_version:
    beam_center_x = None
    beam_center_y = None
  else:
    beam_center_x = 1765 // 2 * 0.11
    beam_center_y = 1765 // 2 * 0.11
  address, timestamp = address_and_timestamp_from_detector_format_version(detector_format_version)

  # if no detector format version is provided, make sure to write no address to the image pickle
  # but CsPadDetector (called later), needs an address, so give it a fake one
  save_address = address is not None
  if not save_address:
    address = "CxiDs1-0|Cspad-0" # time stamp will still be None
  timestamp = evt_timestamp((timestamp,0))
  args = command_line.args
  assert len(args) == 1
  if args[0].endswith('.npy'):
    data = numpy.load(args[0])
    det, active_areas = convert_2x2(data, detector_format_version, address)
  elif args[0].endswith('.txt') or args[0].endswith('.gain'):
    raw_data = numpy.loadtxt(args[0])
    assert raw_data.shape in [(5920, 388), (11840, 194)]
    det, active_areas = convert_detector(raw_data, detector_format_version, address, command_line.options.optical_metrology_path)
  img_diff = det
  img_sel = (img_diff > 0).as_1d()
  gain_map = flex.double(img_diff.accessor(), 0)
  gain_map.as_1d().set_selected(img_sel.iselection(), 1/img_diff.as_1d().select(img_sel))
  gain_map /= flex.mean(gain_map.as_1d().select(img_sel))

  if not save_address:
    address = None
  d = cspad_tbx.dpack(data=gain_map, address=address, active_areas=active_areas, timestamp=timestamp,
    distance=command_line.options.distance,wavelength=command_line.options.wavelength,
    beam_center_x = beam_center_x, beam_center_y = beam_center_y)
  easy_pickle.dump(output_filename, d)


def convert_detector(raw_data, detector_format_version, address, optical_metrology_path=None):
  # https://confluence.slac.stanford.edu/display/PCDS/CSPad+metrology+and+calibration+files%2C+links
  data3d = []
  if raw_data.shape == (5920,388):
    asic_start = 0
    if optical_metrology_path is None:
      calib_dir = libtbx.env.find_in_repositories("xfel/metrology/CSPad/run4/CxiDs1.0_Cspad.0")
      sections = parse_calib.calib2sections(calib_dir)
    else:
      sections = parse_calib.calib2sections(optical_metrology_path)
    for i_quad in range(4):
      asic_size = 185 * 388
      section_size = asic_size * 2
      quad_start = i_quad * section_size * 4
      quad_asics = []
      for i_2x2 in range(4):
        for i_asic in range(2):
          asic_end = asic_start + 185
          quad_asics.append(raw_data[asic_start:asic_end, :])
          asic_start = asic_end
      quad_data = numpy.dstack(quad_asics)
      quad_data = numpy.rollaxis(quad_data, 2,0)
      data3d.append(fake_cspad_ElementV2(quad_data, i_quad))
    env = fake_env(fake_config())
    evt = fake_evt(data3d)
    return flex.double(cspad_tbx.CsPadDetector(address, evt, env, sections).astype(numpy.float64)), None
  else:
    asic_start = 0
    if detector_format_version is not None and 'XPP' in detector_format_version:
      from xfel.cxi.cspad_ana.cspad_tbx import xpp_active_areas
      rotations = xpp_active_areas[detector_format_version]['rotations']
      active_areas = xpp_active_areas[detector_format_version]['active_areas']
      det = flex.double([0]*(1765*1765))
      det.reshape(flex.grid((1765,1765)))
      for i in range(64):
        row = active_areas[i*4]
        col = active_areas[i*4 + 1]
        block = flex.double(raw_data[i * 185:(i+1)*185, :])
        det.matrix_paste_block_in_place(block.matrix_rot90(rotations[i]), row, col)
      return det, active_areas

    else:
      if optical_metrology_path is None:
        calib_dir = libtbx.env.find_in_repositories("xfel/metrology/CSPad/run4/CxiDs1.0_Cspad.0")
        sections = parse_calib.calib2sections(calib_dir)
      else:
        sections = parse_calib.calib2sections(optical_metrology_path)
      for i_quad in range(4):
        asic_size = 185 * 194
        section_size = asic_size * 4
        quad_start = i_quad * section_size * 4
        quad_asics = []
        for i_2x2 in range(4):
          for i_asic in range(2):
            asic_end = asic_start + 185
            a = raw_data[asic_start:asic_end, :]
            asic_start = asic_end

            asic_end = asic_start + 185
            b = raw_data[asic_start:asic_end, :]
            asic_start = asic_end

            quad_asics.append(numpy.concatenate((a,b),axis=1))
        quad_data = numpy.dstack(quad_asics)
        quad_data = numpy.rollaxis(quad_data, 2,0)
        data3d.append(fake_cspad_ElementV2(quad_data, i_quad))

      env = fake_env(fake_config())
      evt = fake_evt(data3d)
      beam_center, active_areas = cspad_tbx.cbcaa(fake_config(),sections)
      return flex.double(cspad_tbx.CsPadDetector(address, evt, env, sections).astype(numpy.float64)), active_areas


def convert_2x2(data):
  config = fake_config()
  sections = [[parse_calib.Section(90, (185 / 2 + 0,   (2 * 194 + 3) / 2)),
               parse_calib.Section(90, (185 / 2 + 185, (2 * 194 + 3) / 2))]]
  if data.shape == (2, 185, 388):
    data = numpy.rollaxis(data, 0, 3)
  elif data.shape == (370, 388):
    data = numpy.dstack((data[:185, :], data[185:, :]))
  else:
    raise RuntimeError("Shape of numpy array is %s: I don't know what to do with an array this shape!")
  return cspad_tbx.CsPad2x2Image(data, config, sections)


if __name__ == '__main__':
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/convert_gain_map_cbf.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.gain_map_cbf
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT
#
# $Id
#
# Convert an LCLS gain mask to a CBF for use with converting XTC streams
# to gain-corrected CSPAD CBF images
#

import sys, os, numpy

from libtbx.option_parser import option_parser
from scitbx.array_family import flex
from libtbx.utils import Usage
from xfel.cftbx.detector.cspad_cbf_tbx import cbf_file_to_basis_dict, write_cspad_cbf
from six.moves import range

def run(args):
  command_line = (option_parser()
                  .option("-o", "--output_filename",
                          action="store",
                          type="string",
                          help="Filename for the output cbf file",
                          default="gain_map.cbf")
                  .option("-m", "--metrology",
                          action="store",
                          type="string",
                          help="CBF or DEF file",
                          default=None)
                  .option("-d", "--distance",
                          action="store",
                          type="int",
                          help="Detector distance put into the gain cbf file. Not needed for processing.",
                          default="0")
                  .option("-w", "--wavelength",
                          action="store",
                          type="float",
                          help="Incident beam wavelength put into the gain cbf file. Not needed for processing.",
                          default="0")
                     ).process(args=args)

  output_filename = command_line.options.output_filename
  metrology = command_line.options.metrology
  assert metrology is not None and os.path.isfile(metrology)

  args = command_line.args

  assert len(args) == 1
  if args[0].endswith('.txt') or args[0].endswith('.gain'):
    raw_data = numpy.loadtxt(args[0])
    assert raw_data.shape in [(5920, 388), (11840, 194)]
    tiles = convert_detector(raw_data)
  else:
    raise Usage("Gain input file should be a text file with extension .txt or .gain")

  metro = cbf_file_to_basis_dict(metrology)
  write_cspad_cbf(tiles, metro, 'cbf', None, output_filename,
                  command_line.options.wavelength, command_line.options.distance)



def convert_detector(raw_data):
  # https://confluence.slac.stanford.edu/display/PCDS/CSPad+metrology+and+calibration+files%2C+links
  data3d = []
  if raw_data.shape == (5920,388):
    raise NotImplementedError("Please contact the authors.  This is an older gain map format.")

  asic_start = 0

  # input dictionary for writing the CBF
  tiles = {}

  # iterate through and extract the data, converting it to asic-sized chunks
  for i_quad in range(4):
    asic_size = 185 * 194
    section_size = asic_size * 4
    quad_start = i_quad * section_size * 4
    quad_asics = []

    # the data is laid out by quadrants, and then by 4 sections per quadrant, each with 2 sensors,
    # each with two asics
    for i_2x2 in range(4):
      for i_sensor in range(2):
        asic_end = asic_start + 185
        a = raw_data[asic_start:asic_end, :]
        asic_start = asic_end
        tiles[(0,i_quad,(i_2x2*2)+i_sensor,0)] = flex.int(a.astype(numpy.int32))

        asic_end = asic_start + 185
        b = raw_data[asic_start:asic_end, :]
        asic_start = asic_end
        tiles[(0,i_quad,(i_2x2*2)+i_sensor,1)] = flex.int(b.astype(numpy.int32))

  return tiles

if __name__ == '__main__':
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cspad_cbf_metrology.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cspad.cbf_metrology
#
from __future__ import absolute_import, division, print_function
from six.moves import range
import os, sys, random
from iotbx.phil import parse
from libtbx import easy_run
from libtbx.utils import Sorry
import six
from six.moves import zip

phil_scope = parse("""
  method = *hierarchical expanding
    .type = choice
  reflections = reindexedstrong *indexed integrated
    .type = choice
    .help = Which subset of reflections
  tag = cspad
    .type = str
    .help = Name of this refinement run. Output filenames will use this tag.
  start_at_hierarchy_level = 0
    .type = int
    .help = Start refinement at this hierarchy level
  refine_to_hierarchy_level = 2
    .type = int
    .help = maximum level to refine cspad to
  refine_distance = True
    .type = bool
    .help = If true, allow root hierarchy level to refine in Z. Otherwise fix this \
            axis. Regardless, higher hierarchy levels will refine in Z.
  refine_energy = False
    .type = bool
    .help = If true, when refining level 0, also refine beam energy. Subsequent hierarchy \
            levels will fix the energy in place.
  flat_refinement = False
    .type = bool
    .help = If True, do not refine tilt (Tau2 and Tau3) when refining panel positions. Further, \
            don't refine distance at levels 1 or higher (respects refine_distance for level 0).
  flat_refinement_with_distance = False
    .type = bool
    .help = If True, and if using flat refinement, then use constraints to allow disance \
            to refine at levels 1 and higher.
  n_subset = None
    .type = int
    .help = Refine a random subset of the provided files
  split_dataset = False
    .type = bool
    .help = After refining the full set of images, if split_dataset is True, the \
            data will be split in two using odd and even file numbers and each half \
            will be refined independently. For each half, _1 or _2 is appended to \
            the tag. If used with n_subset, each half will have n_subset/2 images.
  data_phil = None
    .type = str
    .help = Optional phil file with all experiments and reflections for use during \
            refinement.  If not provided, the program will use whatever directories \
            were specified.
  rmsd_filter {
    enable = True
      .type = bool
      .help = If enabled, between each round of hierarchical refinement, filter \
              the images by positional RMSD
    iqr_multiplier = 1.5
      .type = float
      .help = Interquartile multiplier
  }
  n_subset_method = *random n_refl significance_filter
    .type = choice
    .help = Algorithm to be used for choosing the n_subset images/experiments for \
            refinement.  n_refl chooses the set with the largest numbers of reflections \
            listed in the reflection table files, thus giving maximal coverage of the detector tiles \
            with the fewest refineable parameters. Significance_filter chooses the subset of \
            images with maximum reflections above an I/sigI cutoff
  n_refl_panel_list = None
    .type = ints
    .help = If n_subset_method is n_refl, specify which panels to search on.
  panel_filter = None
    .type = ints
    .help = Specify a list of panels to include during refinement. Default (None) is to use \
            all panels.
  output_lcls_geometry = True
    .type = bool
    .help = If True, convert final refined geometry to LCLS format
""", process_includes=True)

refine_defaults_scope = parse("""
output.include_unused_reflections=False
refinement {
  refinery.engine = SparseLevMar
  parameterisation {
    beam.fix=all
    auto_reduction {
      action=remove
      min_nref_per_parameter=3
    }
  }
  reflections {
    outlier {
      algorithm=sauter_poon
      separate_panels=True
      separate_experiments=False
    }
  }
}
""")

def is_even(filename):
  import re
  return int(re.findall(r'\d+', filename)[-1][-1]) % 2 == 0

refine_scope = parse("""
  include scope dials.command_line.refine.phil_scope
""", process_includes=True)

def run(args):
  print("Parsing input...")
  if "-c" in args or "-h" in args or "--help" in args:
    phil_scope.show(attributes_level=2)
    return
  user_phil = []
  paths = []
  refine_phil_file = None
  for arg in args:
    if os.path.isfile(arg):
      try:
        if os.path.splitext(arg)[1] == ".phil":
          refine_phil_file = arg
          continue
      except Exception as e:
        raise Sorry("Unrecognized file %s"%arg)
    if os.path.isdir(arg):
      paths.append(arg)
    else:
      try:
        user_phil.append(parse(arg))
      except Exception as e:
        raise Sorry("Unrecognized argument: %s"%arg)

  params = phil_scope.fetch(sources=user_phil).extract()

  merged_scope = refine_scope.fetch(refine_defaults_scope)
  if refine_phil_file is not None:
    merged_scope = merged_scope.fetch(parse(file_name = refine_phil_file))

  print("Gathering file names...")
  all_exp = []
  all_ref = []

  if params.data_phil is None:
    for path in paths:
      exp, ref = find_files(path, params.reflections)
      all_exp.extend(exp)
      all_ref.extend(ref)

    if params.split_dataset:
      even_exp = []
      odd_exp = []
      even_ref = []
      odd_ref = []
      for exp, ref in zip(all_exp, all_ref):
        if is_even(exp):
          even_exp.append(exp)
          even_ref.append(ref)
        else:
          odd_exp.append(exp)
          odd_ref.append(ref)

      base_tag = params.tag
      base_n_subset = params.n_subset

      params.n_subset = base_n_subset // 2

      params.tag = base_tag + "_1"
      odd_combine_phil = write_combine_phil(params, odd_exp, odd_ref)
      params.tag = base_tag + "_2"
      even_combine_phil = write_combine_phil(params, even_exp, even_ref)

      params.tag = base_tag
      params.n_subset = base_n_subset
      full_combine_phil = write_combine_phil(params, odd_exp+even_exp, odd_ref+even_ref)

      print("Refining full dataset using tag", params.tag)
      refine(params, merged_scope, full_combine_phil)

      params.tag = base_tag + "_1"
      print("Refining odd numbered data using tag", params.tag)
      refine(params, merged_scope, odd_combine_phil)

      params.tag = base_tag + "_2"
      print("Refining even numbered data using tag", params.tag)
      refine(params, merged_scope, even_combine_phil)
    else:
      combine_phil = write_combine_phil(params, all_exp, all_ref)
      refine(params, merged_scope, combine_phil)
  else:
    assert len(paths) == 0
    assert params.n_subset is None
    print("Refining full dataset using tag", params.tag)
    refine(params, merged_scope, params.data_phil)
    if params.split_dataset:

      input_scope = parse("""
        input {
          experiments = None
            .type = str
            .multiple = True
            .help = "The experiment list file path"
          reflections = None
            .type = str
            .multiple = True
            .help = "The reflection table file path"
        }
        """)
      input_params = input_scope.fetch(parse(file_name = params.data_phil)).extract()
      even_exp = []
      odd_exp = []
      even_ref = []
      odd_ref = []
      for f in input_params.input.experiments:
        if is_even(f):
          even_exp.append(f)
        else:
          odd_exp.append(f)
      for f in input_params.input.reflections:
        if is_even(f):
          even_ref.append(f)
        else:
          odd_ref.append(f)
      base_tag = params.tag
      params.tag = base_tag + "_1"
      odd_combine_phil = write_combine_phil(params, odd_exp, odd_ref)
      params.tag = base_tag + "_2"
      even_combine_phil = write_combine_phil(params, even_exp, even_ref)

      params.tag = base_tag + "_1"
      print("Refining odd numbered data using tag", params.tag)
      refine(params, merged_scope, odd_combine_phil)

      params.tag = base_tag + "_2"
      print("Refining even numbered data using tag", params.tag)
      refine(params, merged_scope, even_combine_phil)

def find_files(path, reflections):
  all_exp = []
  all_ref = []
  for filename in os.listdir(path):
    if reflections in filename:
      extension = os.path.splitext(filename)[1]
      if extension not in ['.pickle', '.mpack', '.refl']: continue
      if extension == ".pickle":
        exp_path = os.path.join(path, filename.rstrip("_%s%s"%(reflections, extension)) + "_refined_experiments.json")
      else:
        exp_path = os.path.join(path, filename.rstrip("_%s%s"%(reflections, extension)) + "_refined.expt")
      if not os.path.exists(exp_path):
        if extension == ".pickle":
          exp_path = os.path.join(path, filename.rstrip("_%s%s"%(reflections, extension)) + "_experiments.json")
        else:
          exp_path = os.path.join(path, filename.rstrip("_%s%s"%(reflections, extension)) + "_indexed.expt")
        if not os.path.exists(exp_path): continue
      all_exp.append(exp_path)
      all_ref.append(os.path.join(path, filename))
  return all_exp, all_ref

def write_combine_phil(params, all_exp, all_ref):
  combine_phil = "%s_combine.phil"%params.tag
  f = open(combine_phil, 'w')
  for exp_path, ref_path in zip(all_exp, all_ref):
    f.write("input {\n")
    f.write("  experiments = %s\n"%exp_path)
    f.write("  reflections = %s\n"%ref_path)
    f.write("}\n")
  f.close()

  return combine_phil

def refine(params, merged_scope, combine_phil):
  print("Combining experiments...")
  command = "dials.combine_experiments reference_from_experiment.average_detector=True reference_from_experiment.average_hierarchy_level=0 output.experiments_filename=%s_combined.expt output.reflections_filename=%s_combined.refl %s"%(params.tag, params.tag, combine_phil)
  if params.n_subset is not None:
    command += " n_subset=%d n_subset_method=%s"%(params.n_subset, params.n_subset_method)
    if params.n_refl_panel_list is not None:
      command += " n_refl_panel_list=%s"%(",".join(["%d"%p for p in params.n_refl_panel_list]))

  if params.refine_energy:
    command += " reference_from_experiment.beam=0"
  print(command)
  result = easy_run.fully_buffered(command=command).raise_if_errors()
  result.show_stdout()

  if params.method == 'hierarchical':
    refine_hierarchical(params, merged_scope, combine_phil)
  elif params.method == 'expanding':
    refine_expanding(params, merged_scope, combine_phil)

def refine_hierarchical(params, merged_scope, combine_phil):
  if params.panel_filter is not None:
    from libtbx import easy_pickle
    print("Filtering out all reflections except those on panels %s"%(", ".join(["%d"%p for p in params.panel_filter])))
    combined_path = "%s_combined.refl"%params.tag
    data = easy_pickle.load(combined_path)
    sel = None
    for panel_id in params.panel_filter:
      if sel is None:
        sel = data['panel'] == panel_id
      else:
        sel |= data['panel'] == panel_id
    print("Retaining", len(data.select(sel)), "out of", len(data), "reflections")
    easy_pickle.dump(combined_path, data.select(sel))

  for i in range(params.start_at_hierarchy_level, params.refine_to_hierarchy_level+1):
    if params.rmsd_filter.enable:
      input_name = "filtered"
    else:
      if i == params.start_at_hierarchy_level:
        input_name = "combined"
      else:
        input_name = "refined"

    if params.rmsd_filter.enable:
      command = "cctbx.xfel.filter_experiments_by_rmsd %s %s output.filtered_experiments=%s output.filtered_reflections=%s"
      if i == params.start_at_hierarchy_level:
        command = command%("%s_combined.expt"%params.tag, "%s_combined.refl"%params.tag,
                           "%s_filtered.expt"%params.tag, "%s_filtered.refl"%params.tag)
      else:
        command = command%("%s_refined_level%d.expt"%(params.tag, i-1), "%s_refined_level%d.refl"%(params.tag, i-1),
                           "%s_filtered_level%d.expt"%(params.tag, i-1), "%s_filtered_level%d.refl"%(params.tag, i-1))
      command += " iqr_multiplier=%f"%params.rmsd_filter.iqr_multiplier
      print(command)
      result = easy_run.fully_buffered(command=command).raise_if_errors()
      result.show_stdout()

    print("Refining at hierarchy level", i)
    refine_phil_file = "%s_refine_level%d.phil"%(params.tag, i)
    if i == 0:
      fix_list = ['Tau1'] # fix detector rotz
      if not params.refine_distance:
        fix_list.append('Dist')
      if params.flat_refinement:
        fix_list.extend(['Tau2','Tau3'])

      diff_phil = "refinement.parameterisation.detector.fix_list=%s\n"%",".join(fix_list)
      if params.refine_energy:
        diff_phil += " refinement.parameterisation.beam.fix=in_spindle_plane+out_spindle_plane\n" # allow energy to refine
    else:
      # Note, always need to fix something, so pick a panel group and fix its Tau1 (rotation around Z) always
      if params.flat_refinement and params.flat_refinement_with_distance:
        diff_phil = "refinement.parameterisation.detector.fix_list=Group1Tau1,Tau2,Tau3\n" # refine distance, rotz and xy translation
        diff_phil += "refinement.parameterisation.detector.constraints.parameter=Dist\n" # constrain distance to be refined identically for all panels at this hierarchy level
      elif params.flat_refinement:
        diff_phil = "refinement.parameterisation.detector.fix_list=Dist,Group1Tau1,Tau2,Tau3\n" # refine only rotz and xy translation
      else:
        diff_phil = "refinement.parameterisation.detector.fix_list=Group1Tau1\n" # refine almost everything

    if i == params.start_at_hierarchy_level:
      command = "dials.refine %s %s_%s.expt %s_%s.refl"%(refine_phil_file, params.tag, input_name, params.tag, input_name)
    else:
      command = "dials.refine %s %s_%s_level%d.expt %s_%s_level%d.refl"%(refine_phil_file, params.tag, input_name, i-1, params.tag, input_name, i-1)

    diff_phil += "refinement.parameterisation.detector.hierarchy_level=%d\n"%i

    command += " output.experiments=%s_refined_level%d.expt output.reflections=%s_refined_level%d.refl"%( \
      params.tag, i, params.tag, i)

    scope = merged_scope.fetch(parse(diff_phil))
    f = open(refine_phil_file, 'w')
    f.write(refine_scope.fetch_diff(scope).as_str())
    f.close()

    print(command)
    result = easy_run.fully_buffered(command=command).raise_if_errors()
    result.show_stdout()

  output_geometry(params)

def refine_expanding(params, merged_scope, combine_phil):
  assert params.start_at_hierarchy_level == 0
  if params.rmsd_filter.enable:
    input_name = "filtered"
    command = "cctbx.xfel.filter_experiments_by_rmsd %s %s output.filtered_experiments=%s output.filtered_reflections=%s"
    command = command%("%s_combined.expt"%params.tag, "%s_combined.refl"%params.tag,
                       "%s_filtered.expt"%params.tag, "%s_filtered.refl"%params.tag)
    command += " iqr_multiplier=%f"%params.rmsd_filter.iqr_multiplier
    print(command)
    result = easy_run.fully_buffered(command=command).raise_if_errors()
    result.show_stdout()
  else:
    input_name = "combined"
  # --------------------------
  if params.panel_filter is not None:
    from libtbx import easy_pickle
    print("Filtering out all reflections except those on panels %s"%(", ".join(["%d"%p for p in params.panel_filter])))
    combined_path = "%s_combined.refl"%params.tag
    data = easy_pickle.load(combined_path)
    sel = None
    for panel_id in params.panel_filter:
      if sel is None:
        sel = data['panel'] == panel_id
      else:
        sel |= data['panel'] == panel_id
    print("Retaining", len(data.select(sel)), "out of", len(data), "reflections")
    easy_pickle.dump(combined_path, data.select(sel))
  # ----------------------------------
  # this is the order to refine the CSPAD in
  steps = {}
  steps[0] = [2, 3]
  steps[1] = steps[0] + [0, 1]
  steps[2] = steps[1] + [14, 15]
  steps[3] = steps[2] + [6, 7]
  steps[4] = steps[3] + [4, 5]
  steps[5] = steps[4] + [12, 13]
  steps[6] = steps[5] + [8, 9]
  steps[7] = steps[6] + [10, 11]

  for s, panels in six.iteritems(steps):
    rest = []
    for p in panels:
      rest.append(p+16)
      rest.append(p+32)
      rest.append(p+48)
    panels.extend(rest)

  levels = {0: (0,1)} # levels 0 and 1
  for i in range(7):
    levels[i+1] = (2,) # level 2

  previous_step_and_level = None
  for j in range(8):
    from libtbx import easy_pickle
    print("Filtering out all reflections except those on panels %s"%(", ".join(["%d"%p for p in steps[j]])))
    combined_path = "%s_%s.refl"%(params.tag, input_name)
    output_path = "%s_step%d.refl"%(params.tag, j)
    data = easy_pickle.load(combined_path)
    sel = None
    for panel_id in steps[j]:
      if sel is None:
        sel = data['panel'] == panel_id
      else:
        sel |= data['panel'] == panel_id
    print("Retaining", len(data.select(sel)), "out of", len(data), "reflections")
    easy_pickle.dump(output_path, data.select(sel))

    for i in levels[j]:
      print("Step", j , "refining at hierarchy level", i)
      refine_phil_file = "%s_refine_step%d_level%d.phil"%(params.tag, j, i)
      if i == 0:
        if params.refine_distance:
          diff_phil = "refinement.parameterisation.detector.fix_list=Tau1" # fix detector rotz
        else:
          diff_phil = "refinement.parameterisation.detector.fix_list=Dist,Tau1" # fix detector rotz, distance
        if params.flat_refinement:
          diff_phil += ",Tau2,Tau3" # Also fix x and y rotations
        diff_phil += "\n"
        if params.refine_energy:
          diff_phil += "refinement.parameterisation.beam.fix=in_spindle_plane+out_spindle_plane\n" # allow energy to refine
      else:
        # Note, always need to fix something, so pick a panel group and fix its Tau1 (rotation around Z) always
        if params.flat_refinement and params.flat_refinement_with_distance:
          diff_phil = "refinement.parameterisation.detector.fix_list=Group1Tau1,Tau2,Tau3\n" # refine distance, rotz and xy translation
          diff_phil += "refinement.parameterisation.detector.constraints.parameter=Dist\n" # constrain distance to be refined identically for all panels at this hierarchy level
        elif params.flat_refinement:
          diff_phil = "refinement.parameterisation.detector.fix_list=Dist,Group1Tau1,Tau2,Tau3\n" # refine only rotz and xy translation
        else:
          diff_phil = "refinement.parameterisation.detector.fix_list=Group1Tau1\n" # refine almost everything

      if previous_step_and_level is None:
        command = "dials.refine %s %s_%s.expt %s_step%d.refl"%( \
          refine_phil_file, params.tag, input_name, params.tag, j)
      else:
        p_step, p_level = previous_step_and_level
        if p_step == j:
          command = "dials.refine %s %s_refined_step%d_level%d.expt %s_refined_step%d_level%d.refl"%( \
            refine_phil_file, params.tag, p_step, p_level, params.tag, p_step, p_level)
        else:
          command = "dials.refine %s %s_refined_step%d_level%d.expt %s_step%d.refl"%( \
            refine_phil_file, params.tag, p_step, p_level, params.tag, j)


      diff_phil += "refinement.parameterisation.detector.hierarchy_level=%d\n"%i

      output_experiments = "%s_refined_step%d_level%d.expt"%(params.tag, j, i)
      command += " output.experiments=%s output.reflections=%s_refined_step%d_level%d.refl"%( \
        output_experiments, params.tag, j, i)

      scope = merged_scope.fetch(parse(diff_phil))
      f = open(refine_phil_file, 'w')
      f.write(refine_scope.fetch_diff(scope).as_str())
      f.close()

      print(command)
      result = easy_run.fully_buffered(command=command).raise_if_errors()
      result.show_stdout()

      # In expanding mode, if using flat refinement with distance, after having refined this step as a block, unrefined
      # panels will have been left behind.  Read back the new metrology, compute the shift applied to the panels refined
      # in this step,and apply that shift to the unrefined panels in this step
      if params.flat_refinement and params.flat_refinement_with_distance and i > 0:
        from dxtbx.model.experiment_list import ExperimentListFactory
        from serialtbx.detector import iterate_detector_at_level, iterate_panels
        from scitbx.array_family import flex
        from scitbx.matrix import col
        from libtbx.test_utils import approx_equal
        experiments = ExperimentListFactory.from_json_file(output_experiments, check_format=False)
        assert len(experiments.detectors()) == 1
        detector = experiments.detectors()[0]
        # Displacements: deltas along the vector normal to the detector
        displacements = flex.double()
        # Iterate through the panel groups at this level
        for panel_group in iterate_detector_at_level(detector.hierarchy(), 0, i):
          # Were there panels refined in this step in this panel group?
          if params.panel_filter:
            test = [list(detector).index(panel) in steps[j] for panel in iterate_panels(panel_group) if list(detector).index(panel) in params.panel_filter]
          else:
            test = [list(detector).index(panel) in steps[j] for panel in iterate_panels(panel_group)]
          if not any(test): continue
          # Compute the translation along the normal of this panel group.  This is defined as distance in dials.refine
          displacements.append(col(panel_group.get_local_fast_axis()).cross(col(panel_group.get_local_slow_axis())).dot(col(panel_group.get_local_origin())))

        # Even though the panels are constrained to move the same amount, there is a bit a variation.
        stats = flex.mean_and_variance(displacements)
        displacement = stats.mean()
        print("Average displacement along normals: %f +/- %f"%(stats.mean(), stats.unweighted_sample_standard_deviation()))

        # Verify the variation isn't significant
        for k in range(1, len(displacements)):
          assert approx_equal(displacements[0], displacements[k])
        # If all of the panel groups in this level moved, no need to do anything.
        if len(displacements) != len(list(iterate_detector_at_level(detector.hierarchy(), 0, i))):
          for panel_group in iterate_detector_at_level(detector.hierarchy(), 0, i):
            if params.panel_filter:
              test = [list(detector).index(panel) in steps[j] and list(detector).index(panel) in params.panel_filter for panel in iterate_panels(panel_group)]
            else:
              test = [list(detector).index(panel) in steps[j] for panel in iterate_panels(panel_group)]
            # If any of the panels in this panel group moved, no need to do anything
            if any(test): continue

            # None of the panels in this panel group moved in this step, so need to apply displacement from other panel
            # groups at this level
            fast = col(panel_group.get_local_fast_axis())
            slow = col(panel_group.get_local_slow_axis())
            ori = col(panel_group.get_local_origin())
            normal = fast.cross(slow)
            panel_group.set_local_frame(fast, slow, (ori.dot(fast)*fast) + (ori.dot(slow)*slow) + (normal*displacement))

        # Check the new displacements. Should be the same across all panels.
        displacements = []
        for panel_group in iterate_detector_at_level(detector.hierarchy(), 0, i):
          displacements.append(col(panel_group.get_local_fast_axis()).cross(col(panel_group.get_local_slow_axis())).dot(col(panel_group.get_local_origin())))

        for k in range(1, len(displacements)):
          assert approx_equal(displacements[0], displacements[k])

        experiments.as_file(output_experiments)

      previous_step_and_level = j,i

  output_geometry(params)

def output_geometry(params):
  print("Creating files to deploy to psana calibration directory...")
  if params.refine_to_hierarchy_level > 2:
    deploy_level = 2
  else:
    deploy_level = params.refine_to_hierarchy_level

  if params.method == 'hierarchical':
    command = "cxi.experiment_json_to_cbf_def %s_refined_level%d.expt output_def_file=%s_refined_detector_level%d.def"%(params.tag, deploy_level, params.tag, deploy_level)
  elif params.method == 'expanding':
    command = "cxi.experiment_json_to_cbf_def %s_refined_step7_level%d.expt output_def_file=%s_refined_detector_level%d.def"%(params.tag, deploy_level, params.tag, deploy_level)
  print(command)
  result = easy_run.fully_buffered(command=command).raise_if_errors()
  result.show_stdout()

  if params.output_lcls_geometry:
    command = "cxi.cbfheader2slaccalib cbf_header=%s_refined_detector_level%d.def out_metrology_file=0-end.data.%s"%(params.tag, deploy_level, params.tag)
    print(command)
    result = easy_run.fully_buffered(command=command)
    errmsg = "\n".join(result.stderr_lines)
    # py2/py3
    if ("ImportError" in errmsg and "PSCalib.GeometryAccess" in errmsg) or \
       ("ModuleNotFoundError" in errmsg and "PSCalib" in errmsg):
      print("Not converting to LCLS geometry as PSDM is not available")
      print("Done.")
    else:
      result.raise_if_errors()
      result.show_stdout()
      print("Done. Soft link 0-end.data.%s to 0-end.data in the geometry folder of your calibration folder for your experiment to deploy this metrology."%params.tag)

if __name__ == "__main__":
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cspad_circular_gain_mask.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# LIBTBX_SET_DISPATCHER_NAME cspad.circular_gain_mask
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1
# $Id
#

"""
This command line function generates a gain ascii file suitable for use
by CXI.  The pixels out to the specified resolution in a circular pattern
will be set to low gain.
"""

import sys, numpy, math
import libtbx.phil
from libtbx.utils import Sorry
from xfel.command_line.make_mask import point_inside_circle

master_phil = libtbx.phil.parse("""
resolution = None
  .type = float
  .help = Low gain pixels will be set out to this resolution. If using an annulus, instead, pixels higher than this resolution will be set to low gain
annulus_inner = None
  .type = float
  .help = Use a low gain annulus instead of masking out all the pixels below the given resolution.
annulus_outer = None
  .type = float
  .help = Use a low gain annulus instead of masking out all the pixels below the given resolution.
distance = None
  .type = float
  .help = Detector distance
wavelength = None
  .type = float
  .help = Beam wavelength
out = circle.gain
  .type = str
  .help = Output file path
optical_metrology_path = None
  .type = str
  .help = Path to slac optical metrology file. If not set, use Run 4 metrology
""")

if (__name__ == "__main__") :
  user_phil = []
  for arg in sys.argv[1:]:
    try :
      user_phil.append(libtbx.phil.parse(arg))
    except RuntimeError as e :
      raise Sorry("Unrecognized argument '%s' (error: %s)" % (arg, str(e)))

  params = master_phil.fetch(sources=user_phil).extract()
  assert params.resolution is not None or (params.annulus_inner is not None and params.annulus_outer is not None)
  assert params.distance is not None
  assert params.wavelength is not None

  annulus = (params.annulus_inner is not None and params.annulus_outer is not None)

  if annulus and params.resolution is not None:
    assert params.resolution < params.annulus_outer

  if annulus:
    if params.resolution is None:
      print("Generating annular gain mask between %f and %f angstroms, assuming a distance %s mm and wavelength %s angstroms" % \
        (params.annulus_inner, params.annulus_outer, params.distance, params.wavelength))
    else:
      print("Generating annular gain mask between %f and %f angstroms, assuming a distance %s mm and wavelength %s angstroms. Also, pixels higher than %f angstroms will be set to low gain." % \
        (params.annulus_inner, params.annulus_outer, params.distance, params.wavelength, params.resolution))
  elif params.resolution is not None:
    print("Generating circular gain mask %s angstroms, assuming a distance %s mm and wavelength %s angstroms" % \
      (params.resolution, params.distance, params.wavelength))

  from serialtbx.detector.cspad import read_slac_metrology
  from xfel.cftbx.detector.cspad_cbf_tbx import get_cspad_cbf_handle
  from dxtbx.format.FormatCBFCspad import FormatCBFCspadInMemory
  from dxtbx.model import BeamFactory
  metro = read_slac_metrology(path = params.optical_metrology_path)
  cbf = get_cspad_cbf_handle(None, metro, 'cbf', None, "test", None, params.distance, verbose = True, header_only = True)
  img = FormatCBFCspadInMemory(cbf)
  beam = BeamFactory.simple(params.wavelength).get_s0()
  beam_center = (0,0)

  data = numpy.zeros((11840,194))

  if annulus:
    inner = params.distance * math.tan(2*math.sinh(params.wavelength/(2*params.annulus_inner)))
    outer = params.distance * math.tan(2*math.sinh(params.wavelength/(2*params.annulus_outer)))
    print("Inner (mm):", inner)
    print("Outer (mm):", outer)
  if params.resolution is not None:
    radius = params.distance * math.tan(2*math.sinh(params.wavelength/(2*params.resolution)))
    print("Radius (mm):", radius)

  print("Panel:", end=' '); sys.stdout.flush()
  for p_id, panel in enumerate(img.get_detector()):
    print(p_id, end=' '); sys.stdout.flush()
    for y in range(185):
      for x in range(194):
        lx, ly, lz = panel.get_pixel_lab_coord((x,y))
        if annulus:
          if not point_inside_circle(lx,ly,beam_center[0],beam_center[1],outer) or point_inside_circle(lx,ly,beam_center[0],beam_center[1],inner):
            data[(p_id*185)+y,x] = 1
        if params.resolution is not None:
          if annulus:
            if not point_inside_circle(lx,ly,beam_center[0],beam_center[1],radius):
              data[(p_id*185)+y,x] = 0
          else:
            if not point_inside_circle(lx,ly,beam_center[0],beam_center[1],radius):
              data[(p_id*185)+y,x] = 1
  print()
  print("Masked out %d pixels out of %d (%.2f%%)"%(data.size-int(data.sum()), data.size, 100*(data.size-int(data.sum()))/data.size))
  numpy.savetxt(params.out, data, fmt="%d")


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cspad_detector_congruence.py
#!/usr/bin/env python
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# detector_congruence.py
#
#  Copyright (C) 2016 Lawrence Berkeley National Laboratory (LBNL)
#
#  Author: Aaron Brewster
#
#  This code is distributed under the X license, a copy of which is
#  included in the root directory of this package.
#
# LIBTBX_SET_DISPATCHER_NAME cspad.detector_congruence
#
from __future__ import absolute_import, division, print_function
from six.moves import range
from dials.array_family import flex
from dials.util import show_mail_on_error
from scitbx.matrix import col
import numpy as np
from libtbx.phil import parse
import libtbx.load_env
import math
from six.moves import zip
from serialtbx.detector import iterate_detector_at_level, iterate_panels, id_from_name, get_center

try:
  from matplotlib import pyplot as plt
  from matplotlib.patches import Polygon
  from matplotlib.colors import Normalize
  from matplotlib import cm
except ImportError:
  pass # Can happen on non-GUI nodes

help_message = '''

This program is used to calculate statisical measurements of consistency
between two detectors.

Example:

  %s experiment1.expt experiment2.expt reflections1.refl reflections2.refl
''' % libtbx.env.dispatcher_name

# Create the phil parameters
phil_scope = parse('''
tag = None
  .type = str
  .help = Used in the plot titles
hierarchy_level=0
  .type=int
  .help=Provide congruence statistics for detector modules at the given hierarchy level.
colormap=RdYlGn_r
  .type=str
  .help=matplotlib color map. See e.g.: \
        http://matplotlib.org/examples/color/colormaps_reference.html
show_plots=True
  .type=bool
  .help=Whether to show congruence plots
draw_normal_arrows=False
  .type=bool
  .help=Whether to draw the XY components of each panel group's normal vector. Useful \
        for visualizing tilt.
''')

def get_bounds(root, pg):
  """ Find the max extent of the panel group pg, projected onto the fast/slow plane of root """
  def panel_bounds(root, panel):
    size = panel.get_image_size()
    p0 = col(panel.get_pixel_lab_coord((0,0)))
    p1 = col(panel.get_pixel_lab_coord((size[0]-1,0)))
    p2 = col(panel.get_pixel_lab_coord((size[0]-1,size[1]-1)))
    p3 = col(panel.get_pixel_lab_coord((0,size[1]-1)))

    rn = col(root.get_normal())
    rf = col(root.get_fast_axis())
    rs = col(root.get_slow_axis())

    return [col((p.dot(rf), + p.dot(rs),0)) for p in [p0, p1, p2, p3]]

  if pg.is_group():
    minx = miny = float('inf')
    maxx = maxy = float('-inf')
    for panel in iterate_panels(pg):
      bounds = panel_bounds(root, panel)
      for v in bounds:
        if v[0] < minx:
          minx = v[0]
        if v[0] > maxx:
          maxx = v[0]
        if v[1] < miny:
          miny = v[1]
        if v[1] > maxy:
          maxy = v[1]
    return [col((minx, miny, 0)),
            col((maxx, miny, 0)),
            col((maxx, maxy, 0)),
            col((minx, maxy, 0))]

  else:
    return panel_bounds(root, pg)

def detector_plot_dict(params, detector, data, title, units_str, show=True, reverse_colormap=False):
  """
  Use matplotlib to plot a detector, color coding panels according to data
  @param detector detector reference detector object
  @param data python dictionary of panel names as keys and numbers as values
  @param title title string for plot
  @param units_str string with a formatting statment for units on each panel
  """
  # initialize the color map
  values = flex.double(list(data.values()))
  norm = Normalize(vmin=flex.min(values), vmax=flex.max(values))
  cmap = plt.get_cmap(params.colormap + ("_r" if reverse_colormap else ''))
  sm = cm.ScalarMappable(norm=norm, cmap=cmap)
  if len(values) == 0:
    print("no values")
    return
  elif len(values) == 1:
    sm.set_array(np.arange(values[0], values[0], 1)) # needed for colorbar
  else:
    sm.set_array(np.arange(flex.min(values), flex.max(values), (flex.max(values)-flex.min(values))/20)) # needed for colorbar

  fig = plt.figure()
  ax = fig.add_subplot(111, aspect='equal')
  max_dim = 0
  root = detector.hierarchy()
  rf = col(root.get_fast_axis())
  rs = col(root.get_slow_axis())
  for pg_id, pg in enumerate(iterate_detector_at_level(root, 0, params.hierarchy_level)):
    if pg.get_name() not in data:
      continue
    # get panel coordinates
    p0, p1, p2, p3 = get_bounds(root, pg)

    v1 = p1-p0
    v2 = p3-p0
    vcen = ((v2/2) + (v1/2)) + p0

    # add the panel to the plot
    ax.add_patch(Polygon((p0[0:2],p1[0:2],p2[0:2],p3[0:2]), closed=True, color=sm.to_rgba(data[pg.get_name()]), fill=True))
    ax.annotate("%d %s"%(pg_id, units_str%data[pg.get_name()]), vcen[0:2], ha='center')

    if params.draw_normal_arrows:
      pgn = col(pg.get_normal())
      v = col((rf.dot(pgn), rs.dot(pgn), 0))
      v *= 10000
      ax.arrow(vcen[0], vcen[1], v[0], v[1], head_width=5.0, head_length=10.0, fc='k', ec='k')

    # find the plot maximum dimensions
    for p in [p0, p1, p2, p3]:
      for c in p[0:2]:
        if abs(c) > max_dim:
          max_dim = abs(c)

  # plot the results
  ax.set_xlim((-max_dim,max_dim))
  ax.set_ylim((-max_dim,max_dim))
  ax.set_xlabel("mm")
  ax.set_ylabel("mm")
  fig.colorbar(sm)
  plt.title(title)
  if show:
    plt.show()

class Script(object):
  ''' Class to parse the command line options. '''

  def __init__(self):
    ''' Set the expected options. '''
    from dials.util.options import ArgumentParser
    import libtbx.load_env

    # Create the option parser
    usage = "usage: %s experiment1.expt experiment2.expt reflections1.refl reflections2.refl" % libtbx.env.dispatcher_name
    self.parser = ArgumentParser(
      usage=usage,
      sort_options=True,
      phil=phil_scope,
      read_experiments=True,
      read_reflections=True,
      epilog=help_message)

  def run(self):
    ''' Parse the options. '''
    from dials.util.options import flatten_experiments, flatten_reflections
    # Parse the command line arguments
    params, options = self.parser.parse_args(show_diff_phil=True)
    self.params = params
    experiments = flatten_experiments(params.input.experiments)
    reflections = flatten_reflections(params.input.reflections)

    # Find all detector objects
    detectors = []
    detectors.extend(experiments.detectors())

    # Verify inputs
    if len(detectors) != 2:
      print("Please provide two experiments for comparison")
      return

    # These lines exercise the iterate_detector_at_level and iterate_panels functions
    # for a detector with 4 hierarchy levels
    """
    print "Testing iterate_detector_at_level"
    for level in range(4):
      print "iterating at level", level
      for panelg in iterate_detector_at_level(detectors[0].hierarchy(), 0, level):
        print panelg.get_name()

    print "Testing iterate_panels"
    for level in range(4):
      print "iterating at level", level
      for panelg in iterate_detector_at_level(detectors[0].hierarchy(), 0, level):
        for panel in iterate_panels(panelg):
          print panel.get_name()
    """
    tmp = []
    for refls in reflections:
      print("N reflections total:", len(refls))
      refls = refls.select(refls.get_flags(refls.flags.used_in_refinement))
      print("N reflections used in refinement", len(refls))
      print("Reporting only on those reflections used in refinement")

      refls['difference_vector_norms'] = (refls['xyzcal.mm']-refls['xyzobs.mm.value']).norms()
      tmp.append(refls)
    reflections = tmp

    # Iterate through the detectors, computing the congruence statistics
    delta_normals = {}
    z_angles = {}
    f_deltas = {}
    s_deltas = {}
    z_deltas = {}
    o_deltas = {} # overall
    z_offsets_d = {}
    refl_counts = {}
    all_delta_normals = flex.double()
    all_rdelta_normals = flex.double()
    all_tdelta_normals = flex.double()
    all_z_angles = flex.double()
    all_f_deltas = flex.double()
    all_s_deltas = flex.double()
    all_z_deltas = flex.double()
    all_deltas = flex.double()
    all_refls_count = flex.int()

    all_normal_angles = flex.double()
    all_rnormal_angles = flex.double()
    all_tnormal_angles = flex.double()
    pg_normal_angle_sigmas = flex.double()
    pg_rnormal_angle_sigmas = flex.double()
    pg_tnormal_angle_sigmas = flex.double()
    all_rot_z = flex.double()
    pg_rot_z_sigmas = flex.double()
    pg_bc_dists = flex.double()
    all_bc_dist = flex.double()
    all_f_offsets = flex.double()
    all_s_offsets = flex.double()
    all_z_offsets = flex.double()
    pg_f_offset_sigmas = flex.double()
    pg_s_offset_sigmas = flex.double()
    pg_z_offset_sigmas = flex.double()
    pg_offset_sigmas = flex.double()
    all_weights = flex.double()

    congruence_table_data = []
    detector_table_data = []
    rmsds_table_data = []
    root1 = detectors[0].hierarchy()
    root2 = detectors[1].hierarchy()

    s0 = col(flex.vec3_double([col(b.get_s0()) for b in experiments.beams()]).mean())

    # Compute a set of radial and transverse displacements for each reflection
    print("Setting up stats...")
    tmp_refls = []
    for refls, expts in zip(reflections, [wrapper.data for wrapper in params.input.experiments]):
      tmp = flex.reflection_table()
      assert len(expts.detectors()) == 1
      dect = expts.detectors()[0]
      # Need to construct a variety of vectors
      for panel_id, panel in enumerate(dect):
        panel_refls = refls.select(refls['panel'] == panel_id)
        bcl = flex.vec3_double()
        # Compute the beam center in lab space (a vector pointing from the origin to where the beam would intersect
        # the panel, if it did intersect the panel)
        for expt_id in set(panel_refls['id']):
          beam = expts[expt_id].beam
          s0 = beam.get_s0()
          expt_refls = panel_refls.select(panel_refls['id'] == expt_id)
          beam_centre = panel.get_beam_centre_lab(s0)
          bcl.extend(flex.vec3_double(len(expt_refls), beam_centre))
        panel_refls['beam_centre_lab'] = bcl

        # Compute obs in lab space
        x, y, _ = panel_refls['xyzobs.mm.value'].parts()
        c = flex.vec2_double(x, y)
        panel_refls['obs_lab_coords'] = panel.get_lab_coord(c)
        # Compute deltaXY in panel space. This vector is relative to the panel origin
        x, y, _ = (panel_refls['xyzcal.mm'] - panel_refls['xyzobs.mm.value']).parts()
        # Convert deltaXY to lab space, subtracting off of the panel origin
        panel_refls['delta_lab_coords'] = panel.get_lab_coord(flex.vec2_double(x,y)) - panel.get_origin()
        tmp.extend(panel_refls)
      refls = tmp
      # The radial vector points from the center of the reflection to the beam center
      radial_vectors = (refls['obs_lab_coords'] - refls['beam_centre_lab']).each_normalize()
      # The transverse vector is orthogonal to the radial vector and the beam vector
      transverse_vectors = radial_vectors.cross(refls['beam_centre_lab']).each_normalize()
      # Compute the raidal and transverse components of each deltaXY
      refls['radial_displacements']     = refls['delta_lab_coords'].dot(radial_vectors)
      refls['transverse_displacements'] = refls['delta_lab_coords'].dot(transverse_vectors)

      tmp_refls.append(refls)
    reflections = tmp_refls

    for pg_id, (pg1, pg2) in enumerate(zip(iterate_detector_at_level(root1, 0, params.hierarchy_level),
                                           iterate_detector_at_level(root2, 0, params.hierarchy_level))):
      """ First compute statistics for detector congruence """
      # Count up the number of reflections in this panel group pair for use as a weighting scheme
      total_refls = 0
      pg1_refls = 0
      pg2_refls = 0
      for p1, p2 in zip(iterate_panels(pg1), iterate_panels(pg2)):
        r1 = len(reflections[0].select(reflections[0]['panel'] == id_from_name(detectors[0], p1.get_name())))
        r2 = len(reflections[1].select(reflections[1]['panel'] == id_from_name(detectors[1], p2.get_name())))
        total_refls += r1 + r2
        pg1_refls += r1
        pg2_refls += r2
      if pg1_refls == 0 and pg2_refls == 0:
        print("No reflections on panel group", pg_id)
        continue

      assert pg1.get_name() == pg2.get_name()
      refl_counts[pg1.get_name()] = total_refls

      row = ["%d"%pg_id]
      for pg, refls, det in zip([pg1, pg2], reflections, detectors):
        pg_refls = flex.reflection_table()
        for p in iterate_panels(pg):
          pg_refls.extend(refls.select(refls['panel'] == id_from_name(det, p.get_name())))
        if len(pg_refls) == 0:
          rmsd = r_rmsd = t_rmsd = 0
        else:
          rmsd = math.sqrt(flex.sum_sq(pg_refls['difference_vector_norms'])/len(pg_refls))*1000
          r_rmsd = math.sqrt(flex.sum_sq(pg_refls['radial_displacements'])/len(pg_refls))*1000
          t_rmsd = math.sqrt(flex.sum_sq(pg_refls['transverse_displacements'])/len(pg_refls))*1000

        row.extend(["%6.1f"%rmsd, "%6.1f"%r_rmsd, "%6.1f"%t_rmsd, "%8d"%len(pg_refls)])
      rmsds_table_data.append(row)

      # Angle between normals of pg1 and pg2
      delta_norm_angle = col(pg1.get_normal()).angle(col(pg2.get_normal()), deg=True)
      all_delta_normals.append(delta_norm_angle)

      # compute radial and transverse components of the delta between normal angles
      pgo = (get_center(pg1)+get_center(pg2))/2
      ro = (get_center(root1)+get_center(root2))/2
      rn = (col(root1.get_normal())+col(root2.get_normal()))/2
      rf = (col(root1.get_fast_axis())+col(root2.get_fast_axis()))/2
      rs = (col(root1.get_slow_axis())+col(root2.get_slow_axis()))/2

      ro_pgo = pgo - ro # vector from the detector origin to the average panel group origin
      if ro_pgo.length() == 0:
        radial = col((0,0,0))
        transverse = col((0,0,0))
      else:
        radial = ((rf.dot(ro_pgo) * rf) + (rs.dot(ro_pgo) * rs)).normalize() # component of ro_pgo in rf rs plane
        transverse = rn.cross(radial).normalize()
      # now radial and transverse are vectors othogonal to each other and the detector normal, such that
      # radial points at the panel group origin
      # v1 and v2 are the components of pg 1 and 2 normals in the rn radial plane
      v1 = (radial.dot(col(pg1.get_normal())) * radial) + (rn.dot(col(pg1.get_normal())) * rn)
      v2 = (radial.dot(col(pg2.get_normal())) * radial) + (rn.dot(col(pg2.get_normal())) * rn)
      rdelta_norm_angle = v1.angle(v2, deg=True)
      if v1.cross(v2).dot(transverse) < 0:
        rdelta_norm_angle = -rdelta_norm_angle
      all_rdelta_normals.append(rdelta_norm_angle)
      # v1 and v2 are the components of pg 1 and 2 normals in the rn transverse plane
      v1 = (transverse.dot(col(pg1.get_normal())) * transverse) + (rn.dot(col(pg1.get_normal())) * rn)
      v2 = (transverse.dot(col(pg2.get_normal())) * transverse) + (rn.dot(col(pg2.get_normal())) * rn)
      tdelta_norm_angle = v1.angle(v2, deg=True)
      if v1.cross(v2).dot(radial) < 0:
        tdelta_norm_angle = -tdelta_norm_angle
      all_tdelta_normals.append(tdelta_norm_angle)

      # compute the angle between fast axes of these panel groups
      z_angle = col(pg1.get_fast_axis()[0:2]).angle(col(pg2.get_fast_axis()[0:2]), deg=True)
      all_z_angles.append(z_angle)
      z_angles[pg1.get_name()] = z_angle

      all_refls_count.append(total_refls)
      all_weights.append(pg1_refls)
      all_weights.append(pg2_refls)


      """ Now compute statistics measuring the reality of the detector. For example, instead of the distance between two things,
      we are concerned with the location of those things relative to laboratory space """
      # Compute distances between panel groups and beam center
      # Also compute offset along Z axis
      dists = flex.double()
      f_offsets = flex.double()
      s_offsets = flex.double()
      z_offsets = flex.double()
      for pg, r in zip([pg1, pg2], [root1, root2]):
        bc = col(pg.get_beam_centre_lab(s0))
        ori = get_center(pg)

        dists.append((ori-bc).length())

        rori = col(r.get_origin())
        delta_ori = ori-rori
        r_norm = col(r.get_normal())
        r_fast = col(r.get_fast_axis())
        r_slow = col(r.get_slow_axis())
        f_offsets.append(r_fast.dot(delta_ori)*1000)
        s_offsets.append(r_slow.dot(delta_ori)*1000)
        z_offsets.append(r_norm.dot(delta_ori)*1000)

      fd = abs(f_offsets[0]-f_offsets[1])
      sd = abs(s_offsets[0]-s_offsets[1])
      zd = abs(z_offsets[0]-z_offsets[1])
      od = math.sqrt(fd**2+sd**2+zd**2)
      f_deltas[pg1.get_name()] = fd
      s_deltas[pg1.get_name()] = sd
      z_deltas[pg1.get_name()] = zd
      o_deltas[pg1.get_name()] = od
      all_f_deltas.append(fd)
      all_s_deltas.append(sd)
      all_z_deltas.append(zd)
      all_deltas.append(od)

      all_f_offsets.extend(f_offsets)
      all_s_offsets.extend(s_offsets)
      all_z_offsets.extend(z_offsets)

      # Compute angle between detector normal and panel group normal
      # Compute rotation of panel group around detector normal
      pg_rotz = flex.double()
      norm_angles = flex.double()
      rnorm_angles = flex.double()
      tnorm_angles = flex.double()
      for pg, r in zip([pg1, pg2], [root1, root2]):

        pgo = get_center(pg)
        pgn = col(pg.get_normal())
        pgf = col(pg.get_fast_axis())

        ro = get_center(r)
        rn = col(r.get_normal())
        rf = col(r.get_fast_axis())
        rs = col(r.get_slow_axis())

        norm_angle = rn.angle(pgn, deg=True)
        norm_angles.append(norm_angle)
        all_normal_angles.append(norm_angle)

        ro_pgo = pgo - ro # vector from the detector origin to the panel group origin
        if ro_pgo.length() == 0:
          radial = col((0,0,0))
          transverse = col((0,0,0))
        else:
          radial = ((rf.dot(ro_pgo) * rf) + (rs.dot(ro_pgo) * rs)).normalize() # component of ro_pgo in rf rs plane
          transverse = rn.cross(radial).normalize()
        # now radial and transverse are vectors othogonal to each other and the detector normal, such that
        # radial points at the panel group origin
        # v is the component of pgn in the rn radial plane
        v = (radial.dot(pgn) * radial) + (rn.dot(pgn) * rn)
        angle = rn.angle(v, deg=True)
        if rn.cross(v).dot(transverse) < 0:
          angle = -angle
        rnorm_angles.append(angle)
        all_rnormal_angles.append(angle)
        # v is the component of pgn in the rn transverse plane
        v = (transverse.dot(pgn) * transverse) + (rn.dot(pgn) * rn)
        angle = rn.angle(v, deg=True)
        if rn.cross(v).dot(radial) < 0:
          angle = -angle
        tnorm_angles.append(angle)
        all_tnormal_angles.append(angle)

        # v is the component of pgf in the rf rs plane
        v = (rf.dot(pgf) * rf) + (rs.dot(pgf) * rs)
        angle = rf.angle(v, deg=True)
        angle = angle-(round(angle/90)*90) # deviation from 90 degrees
        pg_rotz.append(angle)
        all_rot_z.append(angle)

      # Set up table rows using stats aggregated from above
      pg_weights = flex.double([pg1_refls, pg2_refls])
      if 0 in pg_weights:
        dist_m = dist_s = norm_angle_m = norm_angle_s = rnorm_angle_m = rnorm_angle_s = 0
        tnorm_angle_m = tnorm_angle_s = rotz_m = rotz_s = 0
        fo_m = fo_s = so_m = so_s = zo_m = zo_s = o_s = 0

      else:
        stats = flex.mean_and_variance(dists, pg_weights)
        dist_m = stats.mean()
        dist_s = stats.gsl_stats_wsd()

        stats = flex.mean_and_variance(norm_angles, pg_weights)
        norm_angle_m = stats.mean()
        norm_angle_s = stats.gsl_stats_wsd()

        stats = flex.mean_and_variance(rnorm_angles, pg_weights)
        rnorm_angle_m = stats.mean()
        rnorm_angle_s = stats.gsl_stats_wsd()

        stats = flex.mean_and_variance(tnorm_angles, pg_weights)
        tnorm_angle_m = stats.mean()
        tnorm_angle_s = stats.gsl_stats_wsd()

        stats = flex.mean_and_variance(pg_rotz, pg_weights)
        rotz_m = stats.mean()
        rotz_s = stats.gsl_stats_wsd()

        stats = flex.mean_and_variance(f_offsets, pg_weights)
        fo_m = stats.mean()
        fo_s = stats.gsl_stats_wsd()
        stats = flex.mean_and_variance(s_offsets, pg_weights)
        so_m = stats.mean()
        so_s = stats.gsl_stats_wsd()
        stats = flex.mean_and_variance(z_offsets, pg_weights)
        zo_m = stats.mean()
        zo_s = stats.gsl_stats_wsd()

        o_s = math.sqrt(fo_s**2+so_s**2+zo_s**2)

      pg_bc_dists.append(dist_m)
      all_bc_dist.extend(dists)
      pg_normal_angle_sigmas.append(norm_angle_s)
      pg_rnormal_angle_sigmas.append(rnorm_angle_s)
      pg_tnormal_angle_sigmas.append(tnorm_angle_s)
      pg_rot_z_sigmas.append(rotz_s)
      pg_f_offset_sigmas.append(fo_s)
      pg_s_offset_sigmas.append(so_s)
      pg_z_offset_sigmas.append(zo_s)
      pg_offset_sigmas.append(o_s)
      z_offsets_d[pg1.get_name()] = zo_m

      congruence_table_data.append(["%d"%pg_id, "%5.1f"%dist_m, #"%.4f"%dist_s,
                                    "%.4f"%delta_norm_angle, "%.4f"%rdelta_norm_angle,
                                    "%.4f"%tdelta_norm_angle, "%.4f"%z_angle,
                                    "%4.1f"%fd, "%4.1f"%sd, "%4.1f"%zd, "%4.1f"%od, "%6d"%total_refls])
      detector_table_data.append(["%d"%pg_id, "%5.1f"%dist_m, #"%.4f"%dist_s,
                                  "%.4f"%norm_angle_m, "%.4f"%norm_angle_s,
                                  "%.4f"%rnorm_angle_m, "%.4f"%rnorm_angle_s,
                                  "%.4f"%tnorm_angle_m, "%.4f"%tnorm_angle_s,
                                  "%10.6f"%rotz_m, "%.6f"%rotz_s,
                                  #"%9.1f"%fo_m, "%5.3f"%fo_s,
                                  #"%9.1f"%so_m, "%5.3f"%so_s,
                                  "%9.3f"%fo_s,
                                  "%9.3f"%so_s,
                                  "%9.1f"%zo_m, "%9.1f"%zo_s, "%9.3f"%o_s, "%6d"%total_refls])

    # Set up table output
    table_d = {d:row for d, row in zip(pg_bc_dists, congruence_table_data)}
    table_header = ["PanelG","Dist","Normal","RNormal","TNormal","Z rot","Delta","Delta","Delta","Delta","N"]
    table_header2 = ["Id","","Angle","Angle","Angle","Angle","F","S","Z","O","Refls"]
    table_header3 = ["", "(mm)","(mm)","(deg)","(deg)","(microns)","(microns)","(microns)","(microns)","(microns)",""]
    congruence_table_data = [table_header, table_header2, table_header3]
    congruence_table_data.extend([table_d[key] for key in sorted(table_d)])

    table_d = {d:row for d, row in zip(pg_bc_dists, detector_table_data)}
    table_header = ["PanelG","Dist","Normal","Normal","RNormal","RNormal","TNormal","TNormal","RotZ", "RotZ","F Offset","S Offset","Z Offset","Z Offset","Offset","N"]
    table_header2 = ["Id","","","Sigma","","Sigma","","Sigma","","Sigma","Sigma","Sigma","","Sigma","Sigma","Refls"]
    table_header3 = ["", "(mm)","(deg)","(deg)","(deg)","(deg)","(deg)","(deg)","(deg)","(deg)","(microns)","(microns)","(microns)","(microns)","(microns)",""]
    detector_table_data = [table_header, table_header2, table_header3]
    detector_table_data.extend([table_d[key] for key in sorted(table_d)])

    table_d = {d:row for d, row in zip(pg_bc_dists, rmsds_table_data)}
    table_header = ["PanelG"]
    table_header2 = ["Id"]
    table_header3 = [""]
    for i in range(len(detectors)):
      table_header.extend(["D%d"%i]*4)
      table_header2.extend(["RMSD", "rRMSD", "tRMSD", "N refls"])
      table_header3.extend(["(microns)"]*3)
      table_header3.append("")
    rmsds_table_data = [table_header, table_header2, table_header3]
    rmsds_table_data.extend([table_d[key] for key in sorted(table_d)])

    if len(all_refls_count) > 1:
      r1 = ["Weighted mean"]
      r2 = ["Weighted stddev"]
      r1.append("")
      r2.append("")
      #r1.append("")
      #r2.append("")
      stats = flex.mean_and_variance(all_delta_normals, all_refls_count.as_double())
      r1.append("%.4f"%stats.mean())
      r2.append("%.4f"%stats.gsl_stats_wsd())
      stats = flex.mean_and_variance(all_rdelta_normals, all_refls_count.as_double())
      r1.append("%.4f"%stats.mean())
      r2.append("%.4f"%stats.gsl_stats_wsd())
      stats = flex.mean_and_variance(all_tdelta_normals, all_refls_count.as_double())
      r1.append("%.4f"%stats.mean())
      r2.append("%.4f"%stats.gsl_stats_wsd())
      stats = flex.mean_and_variance(all_z_angles, all_refls_count.as_double())
      r1.append("%.4f"%stats.mean())
      r2.append("%.4f"%stats.gsl_stats_wsd())
      stats = flex.mean_and_variance(all_f_deltas, all_refls_count.as_double())
      r1.append("%4.1f"%stats.mean())
      r2.append("%4.1f"%stats.gsl_stats_wsd())
      stats = flex.mean_and_variance(all_s_deltas, all_refls_count.as_double())
      r1.append("%4.1f"%stats.mean())
      r2.append("%4.1f"%stats.gsl_stats_wsd())
      stats = flex.mean_and_variance(all_z_deltas, all_refls_count.as_double())
      r1.append("%4.1f"%stats.mean())
      r2.append("%4.1f"%stats.gsl_stats_wsd())
      stats = flex.mean_and_variance(all_deltas, all_refls_count.as_double())
      r1.append("%4.1f"%stats.mean())
      r2.append("%4.1f"%stats.gsl_stats_wsd())
      r1.append("")
      r2.append("")
      congruence_table_data.append(r1)
      congruence_table_data.append(r2)
      congruence_table_data.append(["Mean", "", "", "","","", "", "", "", "", "", "%6.1f"%flex.mean(all_refls_count.as_double())])

    from libtbx import table_utils
    print("Congruence statistics, I.E. the differences between the input detectors:")
    print(table_utils.format(congruence_table_data,has_header=3,justify='center',delim=" "))

    print("PanelG Id: panel group id or panel id, depending on hierarchy_level. For each panel group, statistics are computed between the matching panel groups between the two input experiments.")
    print("Dist: distance from center of panel group to the beam center")
    print("Dist Sigma: weighted standard deviation of the measurements used to compute Dist")
    print("Normal angle: angle between the normal vectors of matching panel groups.")
    print("RNormal angle: radial component of the angle between the normal vectors of matching panel groups")
    print("TNormal angle: transverse component of the angle between the normal vectors of matching panel groups")
    print("Z rot: angle between the XY components of the fast axes of the panel groups.")
    print("Delta F: shift between matching panel groups along the detector fast axis.")
    print("Delta S: shift between matching panel groups along the detector slow axis.")
    print("Delta Z: Z shift between matching panel groups along the detector normal.")
    print("Delta O: Overall shift between matching panel groups along the detector normal.")
    print("N refls: number of reflections summed between both matching panel groups. This number is used as a weight when computing means and standard deviations.")
    print()
    print()


    if len(all_weights) > 1:
      r1 = ["All"]
      r2 = ["Mean"]
      for data, weights, fmt in [[None,None,None],
                                 #[None,None,None],
                                 [all_normal_angles,       all_weights.as_double(),     "%.4f"],
                                 [pg_normal_angle_sigmas,  all_refls_count.as_double(), "%.4f"],
                                 [all_rnormal_angles,      all_weights.as_double(),     "%.4f"],
                                 [pg_rnormal_angle_sigmas, all_refls_count.as_double(), "%.4f"],
                                 [all_tnormal_angles,      all_weights.as_double(),     "%.4f"],
                                 [pg_tnormal_angle_sigmas, all_refls_count.as_double(), "%.4f"],
                                 [all_rot_z,               all_weights.as_double(),     "%10.6f"],
                                 [pg_rot_z_sigmas,         all_refls_count.as_double(), "%.6f"],
                                 #[all_f_offsets,           all_weights.as_double(),     "%9.1f"],
                                 [pg_f_offset_sigmas,      all_refls_count.as_double(), "%9.3f"],
                                 #[all_s_offsets,           all_weights.as_double(),     "%9.1f"],
                                 [pg_s_offset_sigmas,      all_refls_count.as_double(), "%9.3f"],
                                 [all_z_offsets,           all_weights.as_double(),     "%9.1f"],
                                 [pg_z_offset_sigmas,      all_refls_count.as_double(), "%9.1f"],
                                 [pg_offset_sigmas,        all_refls_count.as_double(), "%9.1f"]]:

        r2.append("")
        if data is None and weights is None:
          r1.append("")
          continue
        stats = flex.mean_and_variance(data, weights)
        r1.append(fmt%stats.mean())

      r1.append("")
      r2.append("%6.1f"%flex.mean(all_refls_count.as_double()))
      detector_table_data.append(r1)
      detector_table_data.append(r2)

    print("Detector statistics, I.E. measurements of parameters relative to the detector plane:")
    print(table_utils.format(detector_table_data,has_header=3,justify='center',delim=" "))

    print("PanelG Id: panel group id or panel id, depending on hierarchy_level. For each panel group, weighted means and weighted standard deviations (Sigmas) for the properties listed below are computed using the matching panel groups between the input experiments.")
    print("Dist: distance from center of panel group to the beam center")
    print("Dist Sigma: weighted standard deviation of the measurements used to compute Dist")
    print("Normal Angle: angle between the normal vector of the detector at its root hierarchy level and the normal of the panel group")
    print("RNormal Angle: radial component of Normal Angle")
    print("TNormal Angle: transverse component of Normal Angle")
    print("RotZ: deviation from 90 degrees of the rotation of each panel group around the detector normal")
    print("F Offset: offset of panel group along the detector's fast axis")
    print("S Offset: offset of panel group along the detector's slow axis")
    print("Z Offset: offset of panel group along the detector normal")
    print("Offset: offset of panel group in F,S,Z space. Sigma is F, S, Z offset sigmas summed in quadrature.")
    print("N refls: number of reflections summed between both matching panel groups. This number is used as a weight when computing means and standard deviations.")
    print("All: weighted mean of the values shown")
    print()
    print("Sigmas in this table are computed using the standard deviation of 2 measurements (I.E. a panel's Z Offset is measured twice, once in each input dataset). This is related by a factor of sqrt(2)/2 to the mean of the Delta Z parameter in the congruence statistics table above, which is the difference between Z parameters.")
    print()

    row = ["Overall"]
    for refls in reflections:
      row.append("%6.1f"%(math.sqrt(flex.sum_sq(refls['difference_vector_norms'])/len(refls))*1000))
      row.append("%6.1f"%(math.sqrt(flex.sum_sq(refls['radial_displacements'])/len(refls))*1000))
      row.append("%6.1f"%(math.sqrt(flex.sum_sq(refls['transverse_displacements'])/len(refls))*1000))
      row.append("%8d"%len(refls))
    rmsds_table_data.append(row)

    print("RMSDs by detector number")
    print(table_utils.format(rmsds_table_data,has_header=3,justify='center',delim=" "))
    print("PanelG Id: panel group id or panel id, depending on hierarchy_level")
    print("RMSD: root mean squared deviation between observed and predicted spot locations")
    print("rRMSD: RMSD of radial components of the observed-predicted vectors")
    print("tRMSD: RMSD of transverse components of the observed-predicted vectors")
    print("N refls: number of reflections")

    # Show stats for detector hierarchy root
    def _print_vector(v):
      for i in v:
        print("%10.5f"%i, end=' ')
      print()
    for d_id, d in enumerate(detectors):
      ori = d.hierarchy().get_origin()
      norm = d.hierarchy().get_normal()
      fast = d.hierarchy().get_fast_axis()
      slow = d.hierarchy().get_slow_axis()
      print("Detector", d_id, "origin:   ", end=' '); _print_vector(ori)
      print("Detector", d_id, "normal:   ", end=' '); _print_vector(norm)
      print("Detector", d_id, "fast axis:", end=' '); _print_vector(fast)
      print("Detector", d_id, "slow axis:", end=' '); _print_vector(slow)

    # Unit cell statstics
    lengths = flex.vec3_double()
    angles = flex.vec3_double()
    weights = flex.double()
    for refls, expts in zip(reflections, [d.data for d in params.input.experiments]):
      for crystal_id, crystal in enumerate(expts.crystals()):
        lengths.append(crystal.get_unit_cell().parameters()[0:3])
        angles.append(crystal.get_unit_cell().parameters()[3:6])
        weights.append(len(refls.select(refls['id'] == crystal_id)))

    print("Unit cell stats (angstroms and degrees), weighted means and standard deviations")
    for subset, tags in zip([lengths, angles], [["Cell a", "Cell b", "Cell c"],["Cell alpha", "Cell beta", "Cell gamma"]]):
      for data, tag in zip(subset.parts(), tags):
        stats = flex.mean_and_variance(data, weights)
        print("%s %5.1f +/- %6.3f"%(tag, stats.mean(), stats.gsl_stats_wsd()))

    if params.tag is None:
      tag = ""
    else:
      tag = "%s "%params.tag

    if params.show_plots:
      # Plot the results
      detector_plot_dict(self.params, detectors[0], refl_counts, u"%sN reflections"%tag, u"%6d", show=False)
      #detector_plot_dict(self.params, detectors[0], delta_normals, u"%sAngle between normal vectors (\N{DEGREE SIGN})"%tag, u"%.2f\N{DEGREE SIGN}", show=False)
      detector_plot_dict(self.params, detectors[0], z_angles, u"%sZ rotation angle between panels (\N{DEGREE SIGN})"%tag, u"%.2f\N{DEGREE SIGN}", show=False)
      detector_plot_dict(self.params, detectors[0], f_deltas, u"%sFast displacements between panels (microns)"%tag, u"%4.1f", show=False)
      detector_plot_dict(self.params, detectors[0], s_deltas, u"%sSlow displacements between panels (microns)"%tag, u"%4.1f", show=False)
      detector_plot_dict(self.params, detectors[0], z_offsets_d, u"%sZ offsets along detector normal (microns)"%tag, u"%4.1f", show=False)
      detector_plot_dict(self.params, detectors[0], z_deltas, u"%sZ displacements between panels (microns)"%tag, u"%4.1f", show=False)
      detector_plot_dict(self.params, detectors[0], o_deltas, u"%sOverall displacements between panels (microns)"%tag, u"%4.1f", show=False)
      plt.show()

if __name__ == '__main__':
  with show_mail_on_error():
    script = Script()
    script.run()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cspad_detector_congruence2.py
#!/usr/bin/env python
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# detector_congruence.py
#
#  Copyright (C) 2016 Lawrence Berkeley National Laboratory (LBNL)
#
#  Author: Aaron Brewster
#
#  This code is distributed under the X license, a copy of which is
#  included in the root directory of this package.
#
# LIBTBX_SET_DISPATCHER_NAME cspad.detector_congruence2
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.detector_congruence
#
from __future__ import absolute_import, division, print_function
from six.moves import range
from dials.array_family import flex
from dials.util import show_mail_on_error
from scitbx.matrix import col, sqr
from matplotlib import pyplot as plt
from matplotlib.patches import Polygon
from matplotlib.colors import Normalize
from matplotlib import cm
import numpy as np
from libtbx.phil import parse
import libtbx.load_env
import math
from six.moves import zip

help_message = '''

This program is used to calculate statisical measurements of consistency
between two detectors.

Example:

  %s experiment1.expt experiment2.expt reflections1.refl reflections2.refl
''' % libtbx.env.dispatcher_name

# Create the phil parameters
phil_scope = parse('''
tag = None
  .type = str
  .help = Used in the plot titles
hierarchy_level=0
  .type=int
  .help=Provide congruence statistics for detector modules at the given hierarchy level.
colormap=RdYlGn_r
  .type=str
  .help=matplotlib color map. See e.g.: \
        http://matplotlib.org/examples/color/colormaps_reference.html
show_plots=False
  .type=bool
  .help=Whether to show congruence plots
draw_normal_arrows=False
  .type=bool
  .help=Whether to draw the XY components of each panel group's normal vector. Useful \
        for visualizing tilt.
''')

def iterate_detector_at_level(item, depth = 0, level = 0):
  """
  Iterate through all panel groups or panels of a detector object at a given
  hierarchy level
  @param item panel group or panel. Use detector.hierarchy().
  @param depth current dept for recursion. Should be 0 for initial call.
  @param level iterate groups at this level
  @return next panel or panel group object
  """
  if level == depth:
    yield item
  else:
    for child in item:
      for subitem in iterate_detector_at_level(child, depth+1, level):
        yield subitem

def iterate_panels(panelgroup):
  """
  Find and iterate all panels in the given panel group, regardless of the hierarchly level
  of this panelgroup
  @param panelgroup the panel group of interest
  @return the next panel
  """
  if panelgroup.is_group():
    for child in panelgroup:
      for subitem in iterate_panels(child):
        yield subitem
  else:
    yield panelgroup

def id_from_name(detector, name):
  """ Jiffy function to get the id of a panel using its name
  @param detector detector object
  @param name panel name
  @return index of panel in detector
  """
  return [p.get_name() for p in detector].index(name)

def get_center(pg):
  """ Find the center of a panel group pg, projected on its fast/slow plane """
  if pg.is_group():
    # find the average center of all this group's children
    children_center = col((0,0,0))
    count = 0
    for p in iterate_panels(pg):
      children_center += get_center(p)
      count += 1
    children_center /= count

    # project the children center onto the plane of the panel group
    pgf = col(pg.get_fast_axis())
    pgs = col(pg.get_slow_axis())
    pgn = col(pg.get_normal())
    pgo = col(pg.get_origin())

    return (pgf.dot(children_center) * pgf) + (pgs.dot(children_center) * pgs) + (pgn.dot(pgo) * pgn)
  else:
    s = pg.get_image_size()
    return col(pg.get_pixel_lab_coord((s[0]/2, s[1]/2)))

def get_center_lab(pg):
  """ Find the center of a panel group pg in lab space """
  if pg.is_group():
    # find the average center of all this group's children
    children_center = col((0,0,0))
    count = 0
    for p in iterate_panels(pg):
      children_center += get_center(p)
      count += 1
    children_center /= count

    return children_center
  else:
    s = pg.get_image_size()
    return col(pg.get_pixel_lab_coord((s[0]/2, s[1]/2)))

def get_bounds(root, pg):
  """ Find the max extent of the panel group pg, projected onto the fast/slow plane of root """
  def panel_bounds(root, panel):
    size = panel.get_image_size()
    p0 = col(panel.get_pixel_lab_coord((0,0)))
    p1 = col(panel.get_pixel_lab_coord((size[0]-1,0)))
    p2 = col(panel.get_pixel_lab_coord((size[0]-1,size[1]-1)))
    p3 = col(panel.get_pixel_lab_coord((0,size[1]-1)))

    rn = col(root.get_normal())
    rf = col(root.get_fast_axis())
    rs = col(root.get_slow_axis())

    return [col((p.dot(rf), + p.dot(rs),0)) for p in [p0, p1, p2, p3]]

  if pg.is_group():
    minx = miny = float('inf')
    maxx = maxy = float('-inf')
    for panel in iterate_panels(pg):
      bounds = panel_bounds(root, panel)
      for v in bounds:
        if v[0] < minx:
          minx = v[0]
        if v[0] > maxx:
          maxx = v[0]
        if v[1] < miny:
          miny = v[1]
        if v[1] > maxy:
          maxy = v[1]
    return [col((minx, miny, 0)),
            col((maxx, miny, 0)),
            col((maxx, maxy, 0)),
            col((minx, maxy, 0))]

  else:
    return panel_bounds(root, pg)


class Script(object):
  ''' Class to parse the command line options. '''

  def __init__(self):
    ''' Set the expected options. '''
    from dials.util.options import ArgumentParser
    import libtbx.load_env

    # Create the option parser
    usage = "usage: %s experiment1.expt experiment2.expt reflections1.refl reflections2.refl" % libtbx.env.dispatcher_name
    self.parser = ArgumentParser(
      usage=usage,
      sort_options=True,
      phil=phil_scope,
      read_experiments=True,
      read_reflections=True,
      check_format=False,
      epilog=help_message)

  def run(self):
    ''' Parse the options. '''
    # Parse the command line arguments
    params, options = self.parser.parse_args(show_diff_phil=True)
    self.params = params
    experiments = [wrapper.data for wrapper in params.input.experiments]
    reflections = [wrapper.data for wrapper in params.input.reflections]

    # Find all detector objects
    detectors = []
    beams = []
    for expts in experiments:
      detectors.extend(expts.detectors())
      beams.extend(expts.beams())

    # Verify inputs
    if len(detectors) != 2:
      print("Please provide two experiments for comparison")
      return

    # These lines exercise the iterate_detector_at_level and iterate_panels functions
    # for a detector with 4 hierarchy levels
    """
    print "Testing iterate_detector_at_level"
    for level in range(4):
      print "iterating at level", level
      for panelg in iterate_detector_at_level(detectors[0].hierarchy(), 0, level):
        print panelg.get_name()

    print "Testing iterate_panels"
    for level in range(4):
      print "iterating at level", level
      for panelg in iterate_detector_at_level(detectors[0].hierarchy(), 0, level):
        for panel in iterate_panels(panelg):
          print panel.get_name()
    """
    tmp = []
    for refls in reflections:
      print("N reflections total:", len(refls))
      sel = refls.get_flags(refls.flags.used_in_refinement)
      if sel.count(True) > 0:
        refls = refls.select(sel)
        print("N reflections used in refinement", len(refls))
        print("Reporting only on those reflections used in refinement")

      refls['difference_vector_norms'] = (refls['xyzcal.mm']-refls['xyzobs.mm.value']).norms()
      tmp.append(refls)
    reflections = tmp

    s0 = col(flex.vec3_double([col(b.get_s0()) for b in beams]).mean())

    # Compute a set of radial and transverse displacements for each reflection
    print("Setting up stats...")
    tmp_refls = []
    for refls, expts in zip(reflections, experiments):
      tmp = flex.reflection_table()
      assert len(expts.detectors()) == 1
      dect = expts.detectors()[0]
      # Need to construct a variety of vectors
      for panel_id, panel in enumerate(dect):
        panel_refls = refls.select(refls['panel'] == panel_id)
        bcl = flex.vec3_double()
        # Compute the beam center in lab space (a vector pointing from the origin to where the beam would intersect
        # the panel, if it did intersect the panel)
        for expt_id in set(panel_refls['id']):
          beam = expts[expt_id].beam
          s0_ = beam.get_s0()
          expt_refls = panel_refls.select(panel_refls['id'] == expt_id)
          beam_centre = panel.get_beam_centre_lab(s0_)
          bcl.extend(flex.vec3_double(len(expt_refls), beam_centre))
        panel_refls['beam_centre_lab'] = bcl

        # Compute obs in lab space
        x, y, _ = panel_refls['xyzobs.mm.value'].parts()
        c = flex.vec2_double(x, y)
        panel_refls['obs_lab_coords'] = panel.get_lab_coord(c)
        # Compute deltaXY in panel space. This vector is relative to the panel origin
        x, y, _ = (panel_refls['xyzcal.mm'] - panel_refls['xyzobs.mm.value']).parts()
        # Convert deltaXY to lab space, subtracting off of the panel origin
        panel_refls['delta_lab_coords'] = panel.get_lab_coord(flex.vec2_double(x,y)) - panel.get_origin()
        tmp.extend(panel_refls)
      refls = tmp
      # The radial vector points from the center of the reflection to the beam center
      radial_vectors = (refls['obs_lab_coords'] - refls['beam_centre_lab']).each_normalize()
      # The transverse vector is orthogonal to the radial vector and the beam vector
      transverse_vectors = radial_vectors.cross(refls['beam_centre_lab']).each_normalize()
      # Compute the raidal and transverse components of each deltaXY
      refls['radial_displacements']     = refls['delta_lab_coords'].dot(radial_vectors)
      refls['transverse_displacements'] = refls['delta_lab_coords'].dot(transverse_vectors)

      tmp_refls.append(refls)
    reflections = tmp_refls

    # storage for plots
    refl_counts = {}

    # Data for all tables
    pg_bc_dists = flex.double()
    root1 = detectors[0].hierarchy()
    root2 = detectors[1].hierarchy()
    all_weights = flex.double()
    all_refls_count = flex.int()

    # Data for lab space table
    lab_table_data = []
    lab_delta_table_data = []
    all_lab_x = flex.double()
    all_lab_y = flex.double()
    all_lab_z = flex.double()
    pg_lab_x_sigmas = flex.double()
    pg_lab_y_sigmas = flex.double()
    pg_lab_z_sigmas = flex.double()
    all_rotX = flex.double()
    all_rotY = flex.double()
    all_rotZ = flex.double()
    pg_rotX_sigmas = flex.double()
    pg_rotY_sigmas = flex.double()
    pg_rotZ_sigmas = flex.double()
    all_delta_x = flex.double()
    all_delta_y = flex.double()
    all_delta_z = flex.double()
    all_delta_xy = flex.double()
    all_delta_xyz = flex.double()
    all_delta_r = flex.double()
    all_delta_t = flex.double()
    all_delta_norm = flex.double()

    if params.hierarchy_level > 0:
      # Data for local table
      local_table_data = []
      local_delta_table_data = []
      all_local_x = flex.double()
      all_local_y = flex.double()
      all_local_z = flex.double()
      pg_local_x_sigmas = flex.double()
      pg_local_y_sigmas = flex.double()
      pg_local_z_sigmas = flex.double()
      all_local_rotX = flex.double()
      all_local_rotY = flex.double()
      all_local_rotZ = flex.double()
      pg_local_rotX_sigmas = flex.double()
      pg_local_rotY_sigmas = flex.double()
      pg_local_rotZ_sigmas = flex.double()
      all_local_delta_x = flex.double()
      all_local_delta_y = flex.double()
      all_local_delta_z = flex.double()
      all_local_delta_xy = flex.double()
      all_local_delta_xyz = flex.double()

    # Data for RMSD table
    rmsds_table_data = []

    for pg_id, (pg1, pg2) in enumerate(zip(iterate_detector_at_level(root1, 0, params.hierarchy_level),
                                           iterate_detector_at_level(root2, 0, params.hierarchy_level))):
      # Count up the number of reflections in this panel group pair for use as a weighting scheme
      total_refls = 0
      pg1_refls = 0
      pg2_refls = 0
      for p1, p2 in zip(iterate_panels(pg1), iterate_panels(pg2)):
        r1 = len(reflections[0].select(reflections[0]['panel'] == id_from_name(detectors[0], p1.get_name())))
        r2 = len(reflections[1].select(reflections[1]['panel'] == id_from_name(detectors[1], p2.get_name())))
        total_refls += r1 + r2
        pg1_refls += r1
        pg2_refls += r2
      if pg1_refls == 0 and pg2_refls == 0:
        print("No reflections on panel group", pg_id)
        continue
      all_refls_count.append(total_refls)
      all_weights.append(pg1_refls)
      all_weights.append(pg2_refls)

      assert pg1.get_name() == pg2.get_name()
      refl_counts[pg1.get_name()] = total_refls

      # Compute RMSDs
      row = ["%d"%pg_id]
      for pg, refls, det in zip([pg1, pg2], reflections, detectors):
        pg_refls = flex.reflection_table()
        for p in iterate_panels(pg):
          pg_refls.extend(refls.select(refls['panel'] == id_from_name(det, p.get_name())))
        if len(pg_refls) == 0:
          rmsd = r_rmsd = t_rmsd = 0
        else:
          rmsd = math.sqrt(flex.sum_sq(pg_refls['difference_vector_norms'])/len(pg_refls))*1000
          r_rmsd = math.sqrt(flex.sum_sq(pg_refls['radial_displacements'])/len(pg_refls))*1000
          t_rmsd = math.sqrt(flex.sum_sq(pg_refls['transverse_displacements'])/len(pg_refls))*1000

        row.extend(["%6.1f"%rmsd, "%6.1f"%r_rmsd, "%6.1f"%t_rmsd, "%8d"%len(pg_refls)])
      rmsds_table_data.append(row)

      dists = flex.double()
      lab_x = flex.double()
      lab_y = flex.double()
      lab_z = flex.double()
      rot_X = flex.double()
      rot_Y = flex.double()
      rot_Z = flex.double()

      for pg in [pg1, pg2]:
        bc = col(pg.get_beam_centre_lab(s0))
        ori = get_center(pg)

        dists.append((ori-bc).length())

        ori_lab = pg.get_origin()
        lab_x.append(ori_lab[0])
        lab_y.append(ori_lab[1])
        lab_z.append(ori_lab[2])

        f = col(pg.get_fast_axis())
        s = col(pg.get_slow_axis())
        n = col(pg.get_normal())
        basis = sqr([f[0], s[0], n[0],
                     f[1], s[1], n[1],
                     f[2], s[2], n[2]])
        rotX, rotY, rotZ = basis.r3_rotation_matrix_as_x_y_z_angles(deg=True)
        rot_X.append(rotX)
        rot_Y.append(rotY)
        rot_Z.append(rotZ)

      all_lab_x.extend(lab_x)
      all_lab_y.extend(lab_y)
      all_lab_z.extend(lab_z)
      all_rotX.extend(rot_X)
      all_rotY.extend(rot_Y)
      all_rotZ.extend(rot_Z)

      pg_weights = flex.double([pg1_refls, pg2_refls])
      if 0 in pg_weights:
        dist_m = dist_s = 0
        lx_m = lx_s = ly_m = ly_s = lz_m = lz_s = 0
        lrx_m = lrx_s = lry_m = lry_s = lrz_m = lrz_s = 0
        dx = dy = dz = dxy = dxyz = dr = dt = dnorm = 0
      else:
        stats = flex.mean_and_variance(dists, pg_weights)
        dist_m = stats.mean()
        dist_s = stats.gsl_stats_wsd()

        stats = flex.mean_and_variance(lab_x, pg_weights)
        lx_m = stats.mean()
        lx_s = stats.gsl_stats_wsd()

        stats = flex.mean_and_variance(lab_y, pg_weights)
        ly_m = stats.mean()
        ly_s = stats.gsl_stats_wsd()

        stats = flex.mean_and_variance(lab_z, pg_weights)
        lz_m = stats.mean()
        lz_s = stats.gsl_stats_wsd()

        stats = flex.mean_and_variance(rot_X, pg_weights)
        lrx_m = stats.mean()
        lrx_s = stats.gsl_stats_wsd()

        stats = flex.mean_and_variance(rot_Y, pg_weights)
        lry_m = stats.mean()
        lry_s = stats.gsl_stats_wsd()

        stats = flex.mean_and_variance(rot_Z, pg_weights)
        lrz_m = stats.mean()
        lrz_s = stats.gsl_stats_wsd()

        dx = lab_x[0] - lab_x[1]
        dy = lab_y[0] - lab_y[1]
        dz = lab_z[0] - lab_z[1]
        dxy = math.sqrt(dx**2+dy**2)
        dxyz = math.sqrt(dx**2+dy**2+dz**2)

        delta = col([lab_x[0], lab_y[0], lab_z[0]]) - col([lab_x[1], lab_y[1], lab_z[1]])
        pg1_center = get_center_lab(pg1).normalize()
        transverse = s0.cross(pg1_center).normalize()
        radial = transverse.cross(s0).normalize()
        dr = delta.dot(radial)
        dt = delta.dot(transverse)
        dnorm = col(pg1.get_normal()).angle(col(pg2.get_normal()), deg=True)

      pg_bc_dists.append(dist_m)
      pg_lab_x_sigmas.append(lx_s)
      pg_lab_y_sigmas.append(ly_s)
      pg_lab_z_sigmas.append(lz_s)
      pg_rotX_sigmas.append(lrx_s)
      pg_rotY_sigmas.append(lry_s)
      pg_rotZ_sigmas.append(lrz_s)
      all_delta_x.append(dx)
      all_delta_y.append(dy)
      all_delta_z.append(dz)
      all_delta_xy.append(dxy)
      all_delta_xyz.append(dxyz)
      all_delta_r.append(dr)
      all_delta_t.append(dt)
      all_delta_norm.append(dnorm)

      lab_table_data.append(["%d"%pg_id, "%5.1f"%dist_m,
                             "%9.3f"%lx_m, "%9.3f"%lx_s,
                             "%9.3f"%ly_m, "%9.3f"%ly_s,
                             "%9.3f"%lz_m, "%9.3f"%lz_s,
                             "%9.3f"%lrx_m, "%9.3f"%lrx_s,
                             "%9.3f"%lry_m, "%9.3f"%lry_s,
                             "%9.3f"%lrz_m, "%9.3f"%lrz_s,
                             "%6d"%total_refls])

      lab_delta_table_data.append(["%d"%pg_id, "%5.1f"%dist_m,
                                   "%9.1f"%(dx*1000), "%9.1f"%(dy*1000), "%9.3f"%dz, "%9.1f"%(dxy*1000), "%9.3f"%dxyz,
                                   "%9.1f"%(dr*1000), "%9.1f"%(dt*1000), "%9.3f"%dnorm,
                                   "%6d"%total_refls])

      if params.hierarchy_level > 0:
        local_x = flex.double()
        local_y = flex.double()
        local_z = flex.double()
        l_rot_X = flex.double()
        l_rot_Y = flex.double()
        l_rot_Z = flex.double()
        l_dx = flex.double()
        l_dy = flex.double()
        l_dz = flex.double()
        l_dxy = flex.double()
        l_dxyz = flex.double()

        for pg in [pg1, pg2]:

          l_ori = pg.get_local_origin()
          local_x.append(l_ori[0])
          local_y.append(l_ori[1])
          local_z.append(l_ori[2])

          f = col(pg.get_local_fast_axis())
          s = col(pg.get_local_slow_axis())
          n = f.cross(s)
          basis = sqr([f[0], s[0], n[0],
                       f[1], s[1], n[1],
                       f[2], s[2], n[2]])
          rotX, rotY, rotZ = basis.r3_rotation_matrix_as_x_y_z_angles(deg=True)
          l_rot_X.append(rotX)
          l_rot_Y.append(rotY)
          l_rot_Z.append(rotZ)

        all_local_x.extend(local_x)
        all_local_y.extend(local_y)
        all_local_z.extend(local_z)
        all_local_rotX.extend(l_rot_X)
        all_local_rotY.extend(l_rot_Y)
        all_local_rotZ.extend(l_rot_Z)

        pg_weights = flex.double([pg1_refls, pg2_refls])
        if 0 in pg_weights:
          lx_m = lx_s = ly_m = ly_s = lz_m = lz_s = 0
          lrx_m = lrx_s = lry_m = lry_s = lrz_m = lrz_s = 0
          ldx = ldy = ldz = ldxy = ldxyz = 0
        else:
          stats = flex.mean_and_variance(local_x, pg_weights)
          lx_m = stats.mean()
          lx_s = stats.gsl_stats_wsd()

          stats = flex.mean_and_variance(local_y, pg_weights)
          ly_m = stats.mean()
          ly_s = stats.gsl_stats_wsd()

          stats = flex.mean_and_variance(local_z, pg_weights)
          lz_m = stats.mean()
          lz_s = stats.gsl_stats_wsd()

          stats = flex.mean_and_variance(l_rot_X, pg_weights)
          lrx_m = stats.mean()
          lrx_s = stats.gsl_stats_wsd()

          stats = flex.mean_and_variance(l_rot_Y, pg_weights)
          lry_m = stats.mean()
          lry_s = stats.gsl_stats_wsd()

          stats = flex.mean_and_variance(l_rot_Z, pg_weights)
          lrz_m = stats.mean()
          lrz_s = stats.gsl_stats_wsd()

          ldx = local_x[0] - local_x[1]
          ldy = local_y[0] - local_y[1]
          ldz = local_z[0] - local_z[1]
          ldxy = math.sqrt(ldx**2+ldy**2)
          ldxyz = math.sqrt(ldx**2+ldy**2+ldz**2)

        pg_local_x_sigmas.append(lx_s)
        pg_local_y_sigmas.append(ly_s)
        pg_local_z_sigmas.append(lz_s)
        pg_local_rotX_sigmas.append(lrx_s)
        pg_local_rotY_sigmas.append(lry_s)
        pg_local_rotZ_sigmas.append(lrz_s)
        all_local_delta_x.append(ldx)
        all_local_delta_y.append(ldy)
        all_local_delta_z.append(ldz)
        all_local_delta_xy.append(ldxy)
        all_local_delta_xyz.append(ldxyz)

        local_table_data.append(["%d"%pg_id, "%5.1f"%dist_m,
                               "%9.3f"%lx_m, "%9.3f"%lx_s,
                               "%9.3f"%ly_m, "%9.3f"%ly_s,
                               "%9.3f"%lz_m, "%9.3f"%lz_s,
                               "%9.3f"%lrx_m, "%9.3f"%lrx_s,
                               "%9.3f"%lry_m, "%9.3f"%lry_s,
                               "%9.3f"%lrz_m, "%9.3f"%lrz_s,
                               "%6d"%total_refls])

        local_delta_table_data.append(["%d"%pg_id, "%5.1f"%dist_m,
                                       "%9.1f"%(ldx*1000), "%9.1f"%(ldy*1000), "%9.3f"%ldz, "%9.1f"%(ldxy*1000), "%9.3f"%ldxyz,
                                       "%6d"%total_refls])

    # Set up table output, starting with lab table
    table_d = {d:row for d, row in zip(pg_bc_dists, lab_table_data)}
    table_header = ["PanelG","Radial","Lab X","Lab X","Lab Y","Lab Y","Lab Z","Lab Z","Rot X","Rot X","Rot Y","Rot Y","Rot Z","Rot Z","N"]
    table_header2 = ["Id","Dist","","Sigma","","Sigma","","Sigma","","Sigma","","Sigma","","Sigma","Refls"]
    table_header3 = ["","(mm)","(mm)","(mm)","(mm)","(mm)","(mm)","(mm)","(deg)","(deg)","(deg)","(deg)","(deg)","(deg)",""]
    lab_table_data = [table_header, table_header2, table_header3]
    lab_table_data.extend([table_d[key] for key in sorted(table_d)])

    if len(all_weights) > 1:
      r1 = ["All"]
      r2 = ["Mean"]
      for data, weights, fmt in [[None,None,None],
                                 [all_lab_x,               all_weights.as_double(),     "%9.3f"],
                                 [pg_lab_x_sigmas,         all_refls_count.as_double(), "%9.3f"],
                                 [all_lab_y,               all_weights.as_double(),     "%9.3f"],
                                 [pg_lab_y_sigmas,         all_refls_count.as_double(), "%9.3f"],
                                 [all_lab_z,               all_weights.as_double(),     "%9.3f"],
                                 [pg_lab_z_sigmas,         all_refls_count.as_double(), "%9.3f"],
                                 [all_rotX,                all_weights.as_double(),     "%9.3f"],
                                 [pg_rotX_sigmas,          all_refls_count.as_double(), "%9.3f"],
                                 [all_rotY,                all_weights.as_double(),     "%9.3f"],
                                 [pg_rotY_sigmas,          all_refls_count.as_double(), "%9.3f"],
                                 [all_rotZ,                all_weights.as_double(),     "%9.3f"],
                                 [pg_rotZ_sigmas,          all_refls_count.as_double(), "%9.3f"]]:
        r2.append("")
        if data is None and weights is None:
          r1.append("")
          continue
        stats = flex.mean_and_variance(data, weights)
        r1.append(fmt%stats.mean())

      r1.append("")
      r2.append("%6.1f"%flex.mean(all_refls_count.as_double()))
      lab_table_data.append(r1)
      lab_table_data.append(r2)

    from libtbx import table_utils
    print("Detector statistics relative to lab origin")
    print(table_utils.format(lab_table_data,has_header=3,justify='center',delim=" "))
    print("PanelG Id: panel group id or panel id, depending on hierarchy_level. For each panel group, weighted means and weighted standard deviations (Sigmas) for the properties listed below are computed using the matching panel groups between the input experiments.")
    print("Radial dist: distance from center of panel group to the beam center")
    print("Lab X, Y and Z: mean coordinate in lab space")
    print("Rot X, Y and Z: rotation of panel group around lab X, Y and Z axes")
    print("N refls: number of reflections summed between both matching panel groups. This number is used as a weight when computing means and standard deviations.")
    print("All: weighted mean of the values shown")
    print()

    # Next, deltas in lab space
    table_d = {d:row for d, row in zip(pg_bc_dists, lab_delta_table_data)}
    table_header = ["PanelG","Radial","Lab dX","Lab dY","Lab dZ","Lab dXY","Lab dXYZ","Lab dR","Lab dT","Lab dNorm","N"]
    table_header2 = ["Id","Dist","","","","","","","","","Refls"]
    table_header3 = ["","(mm)","(microns)","(microns)","(mm)","(microns)","(mm)","(microns)","(microns)","(deg)",""]
    lab_delta_table_data = [table_header, table_header2, table_header3]
    lab_delta_table_data.extend([table_d[key] for key in sorted(table_d)])

    if len(all_weights) > 1:
      r1 = ["WMean"]
      r2 = ["WStddev"]
      r3 = ["Mean"]
      for data, weights, fmt in [[None,None,None],
                                 [all_delta_x*1000,          all_refls_count.as_double(),     "%9.1f"],
                                 [all_delta_y*1000,          all_refls_count.as_double(),     "%9.1f"],
                                 [all_delta_z,               all_refls_count.as_double(),     "%9.3f"],
                                 [all_delta_xy*1000,         all_refls_count.as_double(),     "%9.1f"],
                                 [all_delta_xyz,             all_refls_count.as_double(),     "%9.3f"],
                                 [all_delta_r*1000,          all_refls_count.as_double(),     "%9.1f"],
                                 [all_delta_t*1000,          all_refls_count.as_double(),     "%9.1f"],
                                 [all_delta_norm,            all_refls_count.as_double(),     "%9.3f"]]:
        r3.append("")
        if data is None and weights is None:
          r1.append("")
          r2.append("")
          continue
        stats = flex.mean_and_variance(data, weights)
        r1.append(fmt%stats.mean())
        if len(data) > 1:
          r2.append(fmt%stats.gsl_stats_wsd())
        else:
          r2.append("-")

      r1.append("")
      r2.append("")
      r3.append("%6.1f"%flex.mean(all_refls_count.as_double()))
      lab_delta_table_data.append(r1)
      lab_delta_table_data.append(r2)
      lab_delta_table_data.append(r3)

    print("Detector deltas in lab space")
    print(table_utils.format(lab_delta_table_data,has_header=3,justify='center',delim=" "))
    print("PanelG Id: panel group id or panel id, depending on hierarchy_level. For each panel group, weighted means and weighted standard deviations (Sigmas) for the properties listed below are computed using the matching panel groups between the input experiments.")
    print("Radial dist: distance from center of panel group to the beam center")
    print("Lab dX, dY and dZ: delta between X, Y and Z coordinates in lab space")
    print("Lab dR, dT and dZ: radial and transverse components of dXY in lab space")
    print("Lab dNorm: angle between normal vectors in lab space")
    print("N refls: number of reflections summed between both matching panel groups. This number is used as a weight when computing means and standard deviations.")
    print("WMean: weighted mean of the values shown")
    print("WStddev: weighted standard deviation of the values shown")
    print("Mean: mean of the values shown")
    print()

    if params.hierarchy_level > 0:
      # Local table
      table_d = {d:row for d, row in zip(pg_bc_dists, local_table_data)}
      table_header = ["PanelG","Radial","Local X","Local X","Local Y","Local Y","Local Z","Local Z","Rot X","Rot X","Rot Y","Rot Y","Rot Z","Rot Z","N"]
      table_header2 = ["Id","Dist","","Sigma","","Sigma","","Sigma","","Sigma","","Sigma","","Sigma","Refls"]
      table_header3 = ["","(mm)","(mm)","(mm)","(mm)","(mm)","(mm)","(mm)","(deg)","(deg)","(deg)","(deg)","(deg)","(deg)",""]
      local_table_data = [table_header, table_header2, table_header3]
      local_table_data.extend([table_d[key] for key in sorted(table_d)])

      if len(all_weights) > 1:
        r1 = ["All"]
        r2 = ["Mean"]
        for data, weights, fmt in [[None,None,None],
                                   [all_local_x,               all_weights.as_double(),     "%9.3f"],
                                   [pg_local_x_sigmas,         all_refls_count.as_double(), "%9.3f"],
                                   [all_local_y,               all_weights.as_double(),     "%9.3f"],
                                   [pg_local_y_sigmas,         all_refls_count.as_double(), "%9.3f"],
                                   [all_local_z,               all_weights.as_double(),     "%9.3f"],
                                   [pg_local_z_sigmas,         all_refls_count.as_double(), "%9.3f"],
                                   [all_local_rotX,            all_weights.as_double(),     "%9.3f"],
                                   [pg_local_rotX_sigmas,      all_refls_count.as_double(), "%9.3f"],
                                   [all_local_rotY,            all_weights.as_double(),     "%9.3f"],
                                   [pg_local_rotY_sigmas,      all_refls_count.as_double(), "%9.3f"],
                                   [all_local_rotZ,            all_weights.as_double(),     "%9.3f"],
                                   [pg_local_rotZ_sigmas,      all_refls_count.as_double(), "%9.3f"]]:
          r2.append("")
          if data is None and weights is None:
            r1.append("")
            continue
          stats = flex.mean_and_variance(data, weights)
          r1.append(fmt%stats.mean())

        r1.append("")
        r2.append("%6.1f"%flex.mean(all_refls_count.as_double()))
        local_table_data.append(r1)
        local_table_data.append(r2)

      print("Detector statistics in local frame of each panel group")
      print(table_utils.format(local_table_data,has_header=3,justify='center',delim=" "))
      print("PanelG Id: panel group id or panel id, depending on hierarchy_level. For each panel group, weighted means and weighted standard deviations (Sigmas) for the properties listed below are computed using the matching panel groups between the input experiments.")
      print("Radial dist: distance from center of panel group to the beam center")
      print("Lab X, Y and Z: mean coordinate in relative to parent panel group")
      print("Rot X, Y and Z: rotation of panel group around parent panel group X, Y and Z axes")
      print("N refls: number of reflections summed between both matching panel groups. This number is used as a weight when computing means and standard deviations.")
      print("All: weighted mean of the values shown")
      print()

      # Next, deltas in local space
      table_d = {d:row for d, row in zip(pg_bc_dists, local_delta_table_data)}
      table_header = ["PanelG","Radial","Local dX","Local dY","Local dZ","Local dXY","Local dXYZ","N"]
      table_header2 = ["Id","Dist","","","","","","Refls"]
      table_header3 = ["","(mm)","(microns)","(microns)","(mm)","(microns)","(mm)",""]
      local_delta_table_data = [table_header, table_header2, table_header3]
      local_delta_table_data.extend([table_d[key] for key in sorted(table_d)])

      if len(all_weights) > 1:
        r1 = ["WMean"]
        r2 = ["WStddev"]
        r3 = ["Mean"]
        for data, weights, fmt in [[None,None,None],
                                   [all_local_delta_x*1000,          all_refls_count.as_double(),     "%9.1f"],
                                   [all_local_delta_y*1000,          all_refls_count.as_double(),     "%9.1f"],
                                   [all_local_delta_z,               all_refls_count.as_double(),     "%9.3f"],
                                   [all_local_delta_xy*1000,         all_refls_count.as_double(),     "%9.1f"],
                                   [all_local_delta_xyz,             all_refls_count.as_double(),     "%9.3f"]]:
          r3.append("")
          if data is None and weights is None:
            r1.append("")
            r2.append("")
            continue
          stats = flex.mean_and_variance(data, weights)
          r1.append(fmt%stats.mean())
          r2.append(fmt%stats.gsl_stats_wsd())

        r1.append("")
        r2.append("")
        r3.append("%6.1f"%flex.mean(all_refls_count.as_double()))
        local_delta_table_data.append(r1)
        local_delta_table_data.append(r2)
        local_delta_table_data.append(r3)

      print("Detector deltas relative to panel group origin")
      print(table_utils.format(local_delta_table_data,has_header=3,justify='center',delim=" "))
      print("PanelG Id: panel group id or panel id, depending on hierarchy_level. For each panel group, weighted means and weighted standard deviations (Sigmas) for the properties listed below are computed using the matching panel groups between the input experiments.")
      print("Radial dist: distance from center of panel group to the beam center")
      print("Local dX, dY and dZ: delta between X, Y and Z coordinates in the local frame of the panel group")
      print("N refls: number of reflections summed between both matching panel groups. This number is used as a weight when computing means and standard deviations.")
      print("All: weighted mean of the values shown")
      print()

    #RMSD table
    table_d = {d:row for d, row in zip(pg_bc_dists, rmsds_table_data)}
    table_header = ["PanelG"]
    table_header2 = ["Id"]
    table_header3 = [""]
    for i in range(len(detectors)):
      table_header.extend(["D%d"%i]*4)
      table_header2.extend(["RMSD", "rRMSD", "tRMSD", "N refls"])
      table_header3.extend(["(microns)"]*3)
      table_header3.append("")
    rmsds_table_data = [table_header, table_header2, table_header3]
    rmsds_table_data.extend([table_d[key] for key in sorted(table_d)])

    row = ["Overall"]
    for refls in reflections:
      row.append("%6.1f"%(math.sqrt(flex.sum_sq(refls['difference_vector_norms'])/len(refls))*1000))
      row.append("%6.1f"%(math.sqrt(flex.sum_sq(refls['radial_displacements'])/len(refls))*1000))
      row.append("%6.1f"%(math.sqrt(flex.sum_sq(refls['transverse_displacements'])/len(refls))*1000))
      row.append("%8d"%len(refls))
    rmsds_table_data.append(row)

    print("RMSDs by detector number")
    print(table_utils.format(rmsds_table_data,has_header=3,justify='center',delim=" "))
    print("PanelG Id: panel group id or panel id, depending on hierarchy_level")
    print("RMSD: root mean squared deviation between observed and predicted spot locations")
    print("rRMSD: RMSD of radial components of the observed-predicted vectors")
    print("tRMSD: RMSD of transverse components of the observed-predicted vectors")
    print("N refls: number of reflections")

    if params.tag is None:
      tag = ""
    else:
      tag = "%s "%params.tag

    if params.show_plots:
      # Plot the results
      self.detector_plot_dict(detectors[0], refl_counts, u"%sN reflections"%tag, u"%6d", show=False)

  def detector_plot_dict(self, detector, data, title, units_str, show=True, reverse_colormap=False):
    """
    Use matplotlib to plot a detector, color coding panels according to data
    @param detector detector reference detector object
    @param data python dictionary of panel names as keys and numbers as values
    @param title title string for plot
    @param units_str string with a formatting statment for units on each panel
    """
    # initialize the color map
    values = flex.double(data.values())
    norm = Normalize(vmin=flex.min(values), vmax=flex.max(values))
    cmap = plt.get_cmap(self.params.colormap + ("_r" if reverse_colormap else ''))
    sm = cm.ScalarMappable(norm=norm, cmap=cmap)
    if len(values) == 0:
      print("no values")
      return
    elif len(values) == 1:
      sm.set_array(np.arange(values[0], values[0], 1)) # needed for colorbar
    else:
      sm.set_array(np.arange(flex.min(values), flex.max(values), (flex.max(values)-flex.min(values))/20)) # needed for colorbar

    fig = plt.figure()
    ax = fig.add_subplot(111, aspect='equal')
    max_dim = 0
    root = detector.hierarchy()
    rf = col(root.get_fast_axis())
    rs = col(root.get_slow_axis())
    for pg_id, pg in enumerate(iterate_detector_at_level(root, 0, self.params.hierarchy_level)):
      if pg.get_name() not in data:
        continue
      # get panel coordinates
      p0, p1, p2, p3 = get_bounds(root, pg)

      v1 = p1-p0
      v2 = p3-p0
      vcen = ((v2/2) + (v1/2)) + p0

      # add the panel to the plot
      ax.add_patch(Polygon((p0[0:2],p1[0:2],p2[0:2],p3[0:2]), closed=True, color=sm.to_rgba(data[pg.get_name()]), fill=True))
      ax.annotate("%d %s"%(pg_id, units_str%data[pg.get_name()]), vcen[0:2], ha='center')

      if self.params.draw_normal_arrows:
        pgn = col(pg.get_normal())
        v = col((rf.dot(pgn), rs.dot(pgn), 0))
        v *= 10000
        ax.arrow(vcen[0], vcen[1], v[0], v[1], head_width=5.0, head_length=10.0, fc='k', ec='k')

      # find the plot maximum dimensions
      for p in [p0, p1, p2, p3]:
        for c in p[0:2]:
          if abs(c) > max_dim:
            max_dim = abs(c)

    # plot the results
    ax.set_xlim((-max_dim,max_dim))
    ax.set_ylim((-max_dim,max_dim))
    ax.set_xlabel("mm")
    ax.set_ylabel("mm")
    fig.colorbar(sm)
    plt.title(title)
    if show:
      plt.show()

if __name__ == '__main__':
  with show_mail_on_error():
    script = Script()
    script.run()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cspad_detector_shifts.py
#!/usr/bin/env python
#
# cspad_detector_shifts.py
#
#  Copyright (C) 2016 Lawrence Berkeley National Laboratory (LBNL)
#
#  Author: Aaron Brewster
#
#  This code is distributed under the X license, a copy of which is
#  included in the root directory of this package.
#
# LIBTBX_SET_DISPATCHER_NAME cspad.detector_shifts
# LIBTBX_SET_DISPATCHER_NAME cctbx.xfel.detector_shifts
#
from __future__ import absolute_import, division, print_function
from six.moves import range
from dials.util import show_mail_on_error
from scitbx.array_family import flex
from scitbx.matrix import col
from libtbx.phil import parse
from libtbx.utils import Sorry
from serialtbx.detector import get_center
import libtbx.load_env
from six.moves import zip

help_message = '''

This program is used to show differences between a reference and a moving set of detectors
Example:

  %s experiment1.expt experiment2.expt reflections1.refl reflections2.refl
''' % libtbx.env.dispatcher_name

# Create the phil parameters
phil_scope = parse('''
max_hierarchy_level=Auto
  .type = int
  .help = Maximum hierarchy level to compute shifts to
''', process_includes=True)

from serialtbx.detector import iterate_detector_at_level, iterate_panels, id_from_name
from xfel.command_line.cspad_detector_congruence import Script as ParentScript

class Script(ParentScript):
  ''' Class to parse the command line options. '''

  def __init__(self):
    ''' Set the expected options. '''
    from dials.util.options import ArgumentParser
    import libtbx.load_env

    # Create the option parser
    usage = "usage: %s experiment1.expt experiment2.expt reflections1.refl reflections2.refl" % libtbx.env.dispatcher_name
    self.parser = ArgumentParser(
      usage=usage,
      sort_options=True,
      phil=phil_scope,
      read_experiments=True,
      read_reflections=True,
      check_format=False,
      epilog=help_message)

  def run(self):
    ''' Parse the options. '''
    # Parse the command line arguments
    params, options = self.parser.parse_args(show_diff_phil=True)
    self.params = params

    if (len(params.input.experiments), len(params.input.reflections)) != (2,2):
      raise Sorry("Please provide a reference and a moving set of experiments")
    experiments1 = params.input.experiments[0].data
    experiments2 = params.input.experiments[1].data
    reflections1 = params.input.reflections[0].data
    reflections2 = params.input.reflections[1].data

    # Find all detector objects
    detectors = []
    detectors.extend(experiments1.detectors())
    detectors.extend(experiments2.detectors())

    # Verify inputs
    if len(detectors) != 2:
      raise Sorry("Please provide a reference and a moving set of experiments")

    detector = detectors[1]

    if not hasattr(detector, 'hierarchy'):
      raise Sorry("Script intended for hierarchical detectors")

    if params.max_hierarchy_level is None or str(params.max_hierarchy_level).lower() == 'auto':
      params.max_hierarchy_level = 0
      root = detector.hierarchy()
      while root.is_group():
        root = root[0]
        params.max_hierarchy_level += 1
      print("Found", params.max_hierarchy_level+1, "hierarchy levels")

    reference_root = detectors[0].hierarchy()
    moving_root = detector.hierarchy()
    rori = get_center(reference_root)
    rf = col(reference_root.get_fast_axis())
    rs = col(reference_root.get_slow_axis())
    r_norm = col(reference_root.get_normal())
    all_beams = experiments1.beams() + experiments2.beams()
    s0 = col(flex.vec3_double([col(b.get_s0()) for b in all_beams]).mean())

    summary_table_header = ["Hierarchy","Delta XY","Delta XY","R Offsets","R Offsets","T Offsets","T Offsets","Z Offsets","Z Offsets","dR Norm","dR Norm","dT Norm","dT Norm","Local dNorm", "Local dNorm", "Rot Z","Rot Z"]
    summary_table_header2 = ["Level","","Sigma","","Sigma","","Sigma","","Sigma","","Sigma","","Sigma","","Sigma","","Sigma"]
    summary_table_header3 = ["","(microns)","(microns)","(microns)","(microns)","(microns)","(microns)","(microns)","(microns)","(deg)","(deg)","(deg)","(deg)","(deg)","(deg)","(deg)","(deg)"]
    summary_table_data = []
    summary_table_data.append(summary_table_header)
    summary_table_data.append(summary_table_header2)
    summary_table_data.append(summary_table_header3)

    table_header = ["PanelG","BC dist","Delta XY","R Offsets","T Offsets","Z Offsets","dR Norm","dT Norm","Local dNorm","Rot Z","N Refls"]
    table_header2 = ["ID","(mm)","(microns)","(microns)","(microns)","(microns)","(deg)","(deg)","(deg)","(deg)",""]

    from serialtbx.detector import basis
    def get_full_basis_shift(pg):
      """Compute basis shift from pg to lab space"""
      shift = basis(panelgroup=pg)
      while True:
        parent = pg.parent()
        if parent is None:
          break
        shift = basis(panelgroup=parent) * shift
        pg = parent
      return shift

    # Iterate through the hierarchy levels
    for level in range(params.max_hierarchy_level+1):
      delta_xy = flex.double()
      r_offsets = flex.double()
      t_offsets = flex.double()
      z_offsets = flex.double()
      rot_z = flex.double()
      delta_r_norm = flex.double()
      delta_t_norm = flex.double()
      local_dnorm = flex.double()
      bc_dists = flex.double()
      weights = flex.double()

      rows = []

      for pg_id, (pg1, pg2) in enumerate(zip(iterate_detector_at_level(reference_root, 0, level),
                                             iterate_detector_at_level(moving_root, 0, level))):
        weight = 0
        for panel_id, p in enumerate(iterate_panels(pg2)):
          weight += len(reflections1.select(reflections1['panel'] == id_from_name(detector, p.get_name()))) + \
                    len(reflections2.select(reflections2['panel'] == id_from_name(detector, p.get_name())))
        weights.append(weight)

        bc = col(pg1.get_beam_centre_lab(s0))
        ori = get_center(pg1)
        bc_dist = (ori-bc).length()
        bc_dists.append(bc_dist)

        z_dists = []
        ori_xy = []
        for pg in [pg1,pg2]:
          ori = pg.get_local_origin()
          ori_xy.append(col((ori[0], ori[1])))
          z_dists.append(ori[2]*1000)
        dxy = (ori_xy[1]-ori_xy[0]).length()*1000
        delta_xy.append(dxy)

        z_off = z_dists[1]-z_dists[0]
        z_offsets.append(z_off)

        pgo1 = col(pg1.get_origin())
        ro_pgo = pgo1 - rori # vector from the detector origin to the panel group origin
        if ro_pgo.length() == 0:
          radial = col((0,0,0))
          transverse = col((0,0,0))
        else:
          radial = ((rf.dot(ro_pgo) * rf) + (rs.dot(ro_pgo) * rs)).normalize() # component of ro_pgo in rf rs plane
          transverse = r_norm.cross(radial).normalize()
        # now radial and transverse are vectors othogonal to each other and the detector normal, such that
        # radial points at the panel group origin

        # compute shift in local frame, then convert that shift to lab space, then make it relative to the reference's origin, in lab space
        lpgo1 = col(pg1.get_local_origin())
        lpgo2 = col(pg2.get_local_origin())
        delta_pgo = (get_full_basis_shift(pg1) * (lpgo2-lpgo1)) - pgo1

        # v is the component of delta_pgo along the radial vector
        v = (radial.dot(delta_pgo) * radial)
        r_offset = v.length() * 1000
        angle = r_norm.angle(v, deg=True)
        if r_norm.cross(v).dot(transverse) < 0:
          r_offset = -r_offset
        r_offsets.append(r_offset)
        # v is the component of delta_pgo along the transverse vector
        v = (transverse.dot(delta_pgo) * transverse)
        t_offset = v.length() * 1000
        angle = r_norm.angle(v, deg=True)
        if r_norm.cross(v).dot(radial) < 0:
          t_offset = -t_offset
        t_offsets.append(t_offset)

        pgn1 = col(pg1.get_normal())
        pgf1 = col(pg1.get_fast_axis())
        pgs1 = col(pg1.get_slow_axis())
        pgn2 = col(pg2.get_normal())
        pgf2 = col(pg2.get_fast_axis())

        # v1 and v2 are the component of pgf1 and pgf2 in the rf rs plane
        v1 = (rf.dot(pgf1) * rf) + (rs.dot(pgf1) * rs)
        v2 = (rf.dot(pgf2) * rf) + (rs.dot(pgf2) * rs)
        rz = v1.angle(v2, deg=True)
        rot_z.append(rz)

        # v1 and v2 are the components of pgn1 and pgn2 in the r_norm radial plane
        v1 = (r_norm.dot(pgn1) * r_norm) + (radial.dot(pgn1) * radial)
        v2 = (r_norm.dot(pgn2) * r_norm) + (radial.dot(pgn2) * radial)
        drn = v1.angle(v2, deg=True)
        if v2.cross(v1).dot(transverse) < 0:
          drn = -drn
        delta_r_norm.append(drn)

        # v1 and v2 are the components of pgn1 and pgn2 in the r_norm transverse plane
        v1 = (r_norm.dot(pgn1) * r_norm) + (transverse.dot(pgn1) * transverse)
        v2 = (r_norm.dot(pgn2) * r_norm) + (transverse.dot(pgn2) * transverse)
        dtn = v1.angle(v2, deg=True)
        if v2.cross(v1).dot(radial) < 0:
          dtn = -dtn
        delta_t_norm.append(dtn)

        # Determine angle between normals in local space
        lpgf1 = col(pg1.get_local_fast_axis())
        lpgs1 = col(pg1.get_local_slow_axis())
        lpgn1 = lpgf1.cross(lpgs1)
        lpgf2 = col(pg2.get_local_fast_axis())
        lpgs2 = col(pg2.get_local_slow_axis())
        lpgn2 = lpgf2.cross(lpgs2)
        ldn = lpgn1.angle(lpgn2, deg=True)
        local_dnorm.append(ldn)

        row = ["%3d"%pg_id, "%6.1f"%bc_dist, "%6.1f"%dxy,
               "%6.1f"%r_offset, "%6.1f"%t_offset, "%6.1f"%z_off,
               "%.4f"%drn, "%.4f"%dtn, "%.4f"%ldn, "%.4f"%rz, "%8d"%weight]
        rows.append(row)

      wm_row = ["Weighted mean", ""]
      ws_row = ["Weighted stddev", ""]
      s_row = ["%d"%level]
      iterable = zip([delta_xy, r_offsets, t_offsets, z_offsets, delta_r_norm, delta_t_norm, local_dnorm, rot_z],
                     ["%6.1f","%6.1f","%6.1f","%6.1f","%.4f","%.4f","%.4f","%.4f"])
      if len(z_offsets) == 0:
        wm_row.extend(["%6.1f"%0]*8)
        ws_row.extend(["%6.1f"%0]*8)
        s_row.extend(["%6.1f"%0]*8)
      elif len(z_offsets) == 1:
        for data, fmt in iterable:
          wm_row.append(fmt%data[0])
          ws_row.append(fmt%0)
          s_row.append(fmt%data[0])
          s_row.append(fmt%0)
      else:
        for data, fmt in iterable:
          stats = flex.mean_and_variance(data, weights)
          wm_row.append(fmt%stats.mean())
          ws_row.append(fmt%stats.gsl_stats_wsd())
          s_row.append(fmt%stats.mean())
          s_row.append(fmt%stats.gsl_stats_wsd())
      wm_row.append("")
      ws_row.append("")
      summary_table_data.append(s_row)

      table_data = [table_header, table_header2]
      table_d = {d:row for d, row in zip(bc_dists, rows)}
      table_data.extend([table_d[key] for key in sorted(table_d)])
      table_data.append(wm_row)
      table_data.append(ws_row)

      from libtbx import table_utils
      print("Hierarchy level %d Detector shifts"%level)
      print(table_utils.format(table_data,has_header=2,justify='center',delim=" "))

    print("Detector shifts summary")
    print(table_utils.format(summary_table_data,has_header=3,justify='center',delim=" "))

    print()
    print("""
For each hierarchy level, the average shifts in are computed among objects at that level, weighted by the number of reflections recorded on each object. For example, for a four quadrant detector, the average Z shift will be the average of the four quadrant Z values, each weighted by the number of reflections on that quadrant.

-------------------
Column descriptions
-------------------

Individual hierarchy level tables only:
PanelG id: ID of the panel group.
BC dist: distance of the panel group from the beam center.
N Refls: number of reflections on this panel group

All tables:
Delta XY: magnitude of the shift in the local XY frame.
R, T offsets: shifts relative to the parent object's location in the radial and transverse directions (relative to the detector center).
Z offsets: relative shifts in the local frame in the local Z direction.
R, T Norm: angle between normal vectors in lab space, projected onto the radial or transverse plane.
Local dNorm: local relative angle between normal vectors.
Rot Z: rotation around detector normal in lab space
""")

if __name__ == '__main__':
  with show_mail_on_error():
    script = Script()
    script.run()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cspad_detector_statistics.py
#!/usr/bin/env python
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# detector_congruence.py
#
#  Copyright (C) 2016 Lawrence Berkeley National Laboratory (LBNL)
#
#  Author: Aaron Brewster
#
#  This code is distributed under the X license, a copy of which is
#  included in the root directory of this package.
#
# LIBTBX_SET_DISPATCHER_NAME cspad.detector_statistics
#
from __future__ import absolute_import, division, print_function
from six.moves import range
from dials.util import show_mail_on_error
from libtbx.phil import parse
import libtbx.load_env
from libtbx.utils import Usage
from libtbx import easy_run

help_message = '''

This helper program is for looking at all hierarchy levels of the cspad detector after joint hierarchical
refinement using cspad.cbf_metrology

Example:

  %s tag=v1metrology
''' % libtbx.env.dispatcher_name

# Create the phil parameters
phil_scope = parse('''
tag = None
  .type = str
  .help = Used in the plot titles
''')

class Script(object):
  ''' Class to parse the command line options. '''

  def __init__(self):
    ''' Set the expected options. '''
    from dials.util.options import ArgumentParser

    # Create the option parser
    usage = "usage: %s tag=tagname" % libtbx.env.dispatcher_name
    self.parser = ArgumentParser(
      usage=usage,
      sort_options=True,
      phil=phil_scope,
      epilog=help_message)

  def run(self):
    ''' Parse the options. '''
    # Parse the command line arguments
    params, options = self.parser.parse_args(show_diff_phil=True)
    self.params = params

    if params.tag is None:
      raise Usage(self.parser.usage)

    level_json = "%s_%d_refined_level%d.expt"
    level_pickle = "%s_%d_refined_level%d.refl"

    command = "cspad.detector_congruence %s %s %s %s hierarchy_level=%d show_plots=False"

    help_strs = [
      "detector as a whole block",
      "quadrants",
      "sensors, I.E. 2x1s",
      "ASICs, I.E. individual tiles"]

    for i in range(3):
      c = command%(level_json%(params.tag, 1, i),
                   level_pickle%(params.tag, 1, i),
                   level_json%(params.tag, 2, i),
                   level_pickle%(params.tag, 2, i),
                   i)

      print("*"*80)
      print("Showing statistics for detector at level %d (%s)"%(i, help_strs[i]))
      print("*"*80)
      print(c)
      result = easy_run.fully_buffered(c).raise_if_errors()
      result.show_stdout()


if __name__ == '__main__':
  with show_mail_on_error():
    script = Script()
    script.run()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_apply_metrology.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# LIBTBX_SET_DISPATCHER_NAME cxi.apply_metrology
# $Id
#

import sys, os, pycbf
import libtbx.phil
from libtbx.utils import Usage, Sorry
import tempfile, shutil

master_phil = libtbx.phil.parse("""
source_cbf = None
  .type = str
  .help = cbf file to apply to destination file(s), can be just a header.
  .optional = False
dest_cbf = None
  .type = str
  .help = cbf files on which to apply the metrology measurements from the source.
  .help = Can provide multiple files as once
  .multiple = True
apply_detector_distance = False
  .type = bool
  .help = If True, copy detector distance
""")

if (__name__ == "__main__") :
  user_phil = []
  for arg in sys.argv[1:]:
    if (os.path.isfile(arg)) :
      user_phil.append(libtbx.phil.parse("""dest_cbf=\"%s\"""" % arg))
    else :
      try :
        user_phil.append(libtbx.phil.parse(arg))
      except RuntimeError as e :
        raise Sorry("Unrecognized argument '%s' (error: %s)" % (arg, str(e)))

  params = master_phil.fetch(sources=user_phil).extract()
  if (params.source_cbf is None) or not os.path.isfile(params.source_cbf):
    master_phil.show()
    raise Usage("source_cbf must be a file")
  if (params.source_cbf is None):
    master_phil.show()
    raise Usage("dest_cbf must be a file (either dest_cbf=XXX, or the file path(s) alone).")

  print("Source file:", params.source_cbf)
  print("Destination file(s):", end=' ')
  for path in params.dest_cbf:
    print(path, end=' ')
  print()

  # categories required to match between the two files.  Tuples of category names
  # and table column names which are keys, I.E., there should be only one row in the
  # category with a given value in the table
  required_categories = [("diffrn"                      , "id"),
                         ("diffrn_source"               , "diffrn_id"),
                         ("diffrn_detector"             , "id"),
                         ("diffrn_detector_axis"        , "axis_id"),
                         ("diffrn_detector_element"     , "id"),
                         ("diffrn_data_frame"           , "detector_element_id"),
                         ("array_structure_list"        , None),
                         ("array_structure_list_axis"   , "axis_set_id"),
                         ("array_structure_list_section", None)]
  optional_categories = [("diffrn_radiation"            , "diffrn_id"),
                         ("diffrn_radiation_wavelength" , "id"),
                         ("diffrn_measurement"          , "diffrn_id"),
                         ("diffrn_scan"                 , "id"),
                         ("diffrn_scan_frame"           , "frame_id"),
                         ("array_intensities"           , "array_id"),
                         ("array_structure"             , "id"),
                         ("array_data"                  , "array_id")]

  # categories whose data to copy from one cbf to another
  copy_categories =     [("axis"                     , "id"),
                         ("diffrn_scan_axis"         , "axis_id"),
                         ("diffrn_scan_frame_axis"   , "axis_id")]
  req_names = [n[0] for n in required_categories]; req_names.extend([c[0] for c in copy_categories])
  opt_names = [n[0] for n in optional_categories]
  keys      = [n[1] for n in required_categories]; keys .extend([c[1] for c in copy_categories])

  src_cbf = pycbf.cbf_handle_struct()
  src_cbf.read_widefile(params.source_cbf, pycbf.MSG_DIGEST)

  # verify all the categories are present in the source cbf
  print("Testing for required categories in source:")
  src_cbf.select_category(0)
  n_found = 0
  while True:
    if src_cbf.category_name() in req_names:
      print("Found", src_cbf.category_name())
      n_found += 1
    else:
      if src_cbf.category_name() not in opt_names:
        raise Sorry("%s not a recognized category"%src_cbf.category_name())
    try:
      src_cbf.next_category()
    except Exception as e:
      assert "CBF_NOTFOUND" in str(e)
      break
  assert n_found == len(req_names)
  print("OK")

  # iterate through the files, validate the required tables and copy the others
  for path in params.dest_cbf:
    print("Validating %s..."%os.path.basename(path), end=' ')

    dst_cbf = pycbf.cbf_handle_struct()
    dst_cbf.read_widefile(path, pycbf.MSG_DIGEST)

    # Validate
    for category, key in required_categories:
      src_cbf.find_category(category)
      dst_cbf.find_category(category)

      assert src_cbf.count_columns() == dst_cbf.count_columns()
      assert src_cbf.count_rows() == dst_cbf.count_rows()

      for j in range(src_cbf.count_rows()):
        src_cbf.rewind_column()
        dst_cbf.rewind_column()

        src_cbf.select_row(j)
        if key is None:
          # for tables with non-unique key column, compare row by row
          dst_cbf.select_row(j)
        else:
          # for tables with unique key column, don't assume the rows are in the
          # same order
          src_cbf.find_column(key)
          dst_cbf.find_column(key)
          dst_cbf.find_row(src_cbf.get_value())

        for i in range(src_cbf.count_columns()):
          src_cbf.select_column(i)
          dst_cbf.find_column(src_cbf.column_name())
          if src_cbf.get_value() != dst_cbf.get_value():
            raise Sorry("Non matching values: table %s, row %d, column %s, %s vs. %s"%(category, i, src_cbf.column_name(),src_cbf.get_value(), dst_cbf.get_value()))

    # Copy
    for category, key in copy_categories:
      src_cbf.find_category(category)
      dst_cbf.find_category(category)

      assert src_cbf.count_columns() == dst_cbf.count_columns()
      assert src_cbf.count_rows() == dst_cbf.count_rows()

      for j in range(src_cbf.count_rows()):
        src_cbf.rewind_column()
        dst_cbf.rewind_column()

        src_cbf.select_row(j)
        src_cbf.find_column(key)

        # don't overwrite detector distance
        if category == "diffrn_scan_frame_axis" and src_cbf.get_value() == "AXIS_D0_Z" and not params.apply_detector_distance:
          continue

        dst_cbf.find_column(key)
        dst_cbf.find_row(src_cbf.get_value())

        for i in range(src_cbf.count_columns()):
          src_cbf.select_column(i)
          dst_cbf.find_column(src_cbf.column_name())

          if src_cbf.get_value() == '.' or src_cbf.get_typeofvalue() == "null":
            dst_cbf.set_typeofvalue("null")
          else:
            dst_cbf.set_value(src_cbf.get_value())

    print("writing cbf...", end=' ')

    t = tempfile.NamedTemporaryFile(delete=False)
    destpath = t.name
    t.close()

    dst_cbf.write_widefile(destpath.encode(),pycbf.CBF,\
                           pycbf.MIME_HEADERS|pycbf.MSG_DIGEST|pycbf.PAD_4K,0)

    del dst_cbf

    shutil.move(destpath, path)

    print("Done")


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_calibdir2cbfheader.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# LIBTBX_SET_DISPATCHER_NAME cxi.calibdir2cbfheader
# $Id
#

import sys, os
import libtbx.phil
from libtbx.utils import Usage, Sorry
from xfel.cftbx.detector.metrology2phil import metrology2phil, sections2phil
from xfel.cftbx.detector.cspad_cbf_tbx import write_cspad_cbf
from scitbx.matrix import col

master_phil = libtbx.phil.parse("""
metrology_dir = None
  .type = str
  .help = Directory with optical metrology information posistioning quadrants and sensors, corrected for
  .help = rectangularity
  .optional = False
corrections_phil = None
  .type = str
  .help = Phil file with quad/unit pixel translations and optionally subpixel metrology
  .optional = True
round_and_orthogonalize = True
  .type = str
  .help = Use if supplying a corrections phil. Quad and unit pixel translations assume an orthogonalized \
          and rounded detector, meaning the angles are multiples of 90 and the pixel values are ints. \
          If False, the optical metrology tilts and subpixel measurements are preserved when adding quad \
          and unit pixel corrections
out = None
  .type = str
  .help = Output file name
  .optional = False
""")

def get_asics_center(asics):
  center = col((0,0))
  for asic in asics:
    x1, y1, x2, y2 = asic
    center += col(((x2-x1)/2,(y2-y1)/2)) + col((x1,y1))
  return center / len(asics)

if (__name__ == "__main__") :
  user_phil = []
  for arg in sys.argv[1:]:
    if (os.path.isdir(arg)) :
      user_phil.append(libtbx.phil.parse("""metrology_dir=\"%s\" """ % arg))
    else :
      try :
        user_phil.append(libtbx.phil.parse(arg))
      except RuntimeError as e :
        raise Sorry("Unrecognized argument '%s' (error: %s)" % (arg, str(e)))

  params = master_phil.fetch(sources=user_phil).extract()
  if (params.metrology_dir is None) :
    master_phil.show()
    raise Usage("metrology_dir must be defined (either metrology_dir=XXX, or the directory path alone).")
  assert params.out is not None

  if params.corrections_phil is None:
    metro_phil = metrology2phil(params.metrology_dir, verbose=True)
    write_cspad_cbf(None, metro_phil, 'calibdir', None, params.out, None, 0, header_only=True)
  else:
    from xfel.cxi.cspad_ana.parse_calib import calib2sections
    sections = calib2sections(params.metrology_dir)

    from spotfinder.applications.xfel import cxi_phil
    horizons_phil = cxi_phil.cxi_versioned_extract()
    horizons_phil = horizons_phil.persist.phil_scope.fetch(source=libtbx.phil.parse(file_name=params.corrections_phil))
    corrections_params = horizons_phil.extract()

    bc = [0,0]
    for q in range(len(sections)):
      corner = sections[q][1].corners(True)[0]
      bc     = [bc[0] + corner[1] / len(sections), bc[1] + corner[0] / len(sections)]
    beam = col(bc)

    for itile in range(len(corrections_params.distl.tile_translations) // 4): # 128 tile_translations/4 = 32 sensors
      # order of quads in sections (UL,UR,LR,LL) is not the same as in quad_translations (UL,UR,LL,LR)
      sections_quad = itile//8
      phil_quad = [0,1,3,2].index(sections_quad)

      s = sections[sections_quad][itile%8] # not the same as iquad!
      c = list(s.center)
      c[0] += corrections_params.distl.quad_translations[2 * phil_quad + 0] + \
              corrections_params.distl.tile_translations[4 * itile + 0]
      c[1] += corrections_params.distl.quad_translations[2 * phil_quad + 1] + \
              corrections_params.distl.tile_translations[4 * itile + 1]
      s.center = tuple(c)

    if params.round_and_orthogonalize:
      # In order to match the image pickle metrology, we have to discard the tilts and offets in the
      # section objects, and instead use the active areas returned by corners_asic as the tile
      # locations.
      from serialtbx.detector import basis
      from xfel.cftbx.detector.cspad_cbf_tbx import pixel_size, asic_dimension, asic_gap
      null_ori = col((0,0,1)).axis_and_angle_as_unit_quaternion(0, deg=True)

      # the detector is rotated 90 degrees from how corners_asic reports things
      rot_ori = col((0,0,1)).axis_and_angle_as_unit_quaternion(-90, deg=True)
      metro = { (0,): basis(rot_ori, col((0,0,0))) } # basis dictionary used to build cbf header
      for quad_id, quad in enumerate(sections):
        quad_asics = []
        for sensor in quad:
          quad_asics.extend(sensor.corners_asic())
        quad_center = col(get_asics_center(quad_asics))
        v = (quad_center-beam)*pixel_size # vector from beam center to quad center
        metro[(0,quad_id)] = basis(null_ori, col((v[0],v[1],0)))
        for sensor_id, sensor in enumerate(quad):
          sensor_center = get_asics_center(sensor.corners_asic())
          # include sensor rotation, rounded to 90 degrees
          ori = col((0,0,1)).axis_and_angle_as_unit_quaternion(90.0 * round(sensor.angle / 90.0), deg=True)
          v = (sensor_center - quad_center)*pixel_size # vector from quad center to 2x1 center
          metro[(0,quad_id,sensor_id)] = basis(ori, col((v[0],v[1],0)))
          # add the two asics
          w = pixel_size * (asic_dimension[0]/2 + asic_gap/2)
          metro[(0,quad_id,sensor_id,0)] = basis(null_ori,col((-w,0,0)))
          metro[(0,quad_id,sensor_id,1)] = basis(null_ori,col((+w,0,0)))
      write_cspad_cbf(None, metro, 'cbf', None, params.out, None, 0, header_only=True)
    else:
      metro_phil = sections2phil(sections, verbose=True)
      write_cspad_cbf(None, metro_phil, 'calibdir', None, params.out, None, 0, header_only=True)


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_cbfheader2slaccalib.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.cbfheader2slaccalib
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1
# $Id
#

import sys, os, dxtbx, getpass
import libtbx.phil
from libtbx.utils import Usage, Sorry
from PSCalib.GeometryAccess import GeometryAccess, GeometryObject
from scitbx.matrix import sqr, col
from xfel.cxi.cspad_ana.cspad_tbx import evt_timestamp

master_phil = libtbx.phil.parse("""
cbf_header = None
  .type = str
  .help = Cbf file with metrology to be converted
  .optional = False
out_metrology_file = 0-end.data
  .type = str
  .help = File with optical metrology information posistioning quadrants and sensors.
  .help = Should be placed in the calib/<Csapd version>/<detector address/geometry folder
  .help = of the experiment, in the form of N-end.data
  .optional = False
""")

class GeometryAccessFromCspadCBF(GeometryAccess):
  """ Override of GeometryAccess to read the metrology from a CSPAD CBF instead
      of a SLAC metrology file """

  def load_pars_from_file(self, path=None) :
    """ Use the dxtbx object model to build a GeometryObject hierarchy
        @param path Path to the CSPAD CBF file
    """
    if path is not None : self.path = path

    if self.pbits & 32 : print('Load file: %s' % self.path)

    img = dxtbx.load(self.path)
    cbf = img._cbf_handle
    cbf.find_category(b"diffrn_source")
    cbf.find_column(b"type")

    self.dict_of_comments = {
      "TITLE"     : "Geometry parameters of CSPAD",
      "DATE_TIME" : evt_timestamp(),
      "AUTHOR"    : getpass.getuser(),
      "EXPERIMENT": cbf.get_value(),
      "DETECTOR"  : "CSPAD",
      "CALIB_TYPE": "geometry",
      "COMMENT:01": "Table contains the list of geometry parameters for alignment of 2x1 sensors, quads, CSPAD, etc",
      "COMMENT:02": " translation and rotation pars of the object are defined w.r.t. parent object Cartesian frame",
      "PARAM:01"  : "PARENT     - name and version of the parent object",
      "PARAM:02"  : "PARENT_IND - index of the parent object",
      "PARAM:03"  : "OBJECT     - name and version of the object",
      "PARAM:04"  : "OBJECT_IND - index of the new object",
      "PARAM:05"  : "X0         - x-coordinate [um] of the object origin in the parent frame",
      "PARAM:06"  : "Y0         - y-coordinate [um] of the object origin in the parent frame",
      "PARAM:07"  : "Z0         - z-coordinate [um] of the object origin in the parent frame",
      "PARAM:08"  : "ROT_Z      - object design rotation angle [deg] around Z axis of the parent frame",
      "PARAM:09"  : "ROT_Y      - object design rotation angle [deg] around Y axis of the parent frame",
      "PARAM:10"  : "ROT_X      - object design rotation angle [deg] around X axis of the parent frame",
      "PARAM:11"  : "TILT_Z     - object tilt angle [deg] around Z axis of the parent frame",
      "PARAM:12"  : "TILT_Y     - object tilt angle [deg] around Y axis of the parent frame",
      "PARAM:13"  : "TILT_X     - object tilt angle [deg] around X axis of the parent frame"
    }

    self.list_of_geos = []

    detector = img.get_detector()
    hierarchy = detector.hierarchy()

    for q, quad in enumerate(hierarchy):
      for s, sensor in enumerate(quad):
        self.list_of_geos.append(self._load_geo(q,"QUAD:V1",s,"SENS2X1:V1",sensor))

    for q, quad in enumerate(hierarchy):
      self.list_of_geos.append(self._load_geo(0,"CSPAD:V1",q,"QUAD:V1",quad))

    # Add placeholder RAIL and IP vectors, including the XY component of the hierarchy's d0 vector
    go = self._load_geo(0,'RAIL',0,'CSPAD:V1',hierarchy)
    go.move_geo(0,0,-go.z0+1000000) # Placeholder
    self.list_of_geos.append(go)
    self.list_of_geos.append(self._null_geo(0,"IP",0,"RAIL"))

    self._set_relations()
    self.valid = True

  def _null_geo(self, pindex, pname, oindex, oname):
    """ Get a GeometryObject whose frameshift is zero
        @param pindex Index of the parent object
        @param pname  Name of the parent object
        @param oindex Index of the current object
        @param oname  Name of the current object

        @return an assembled GeometryObject
    """

    d = {
      'pname' :pname,
      'pindex':pindex,
      'oname' :oname,
      'oindex':oindex,
      'x0':    0,
      'y0':    0,
      'z0':    0,
      'rot_z': 0,
      'rot_y': 0,
      'rot_x': 0,
      'tilt_z':0,
      'tilt_y':0,
      'tilt_x':0
    }

    return GeometryObject(**d)

  def _load_geo(self, pindex, pname, oindex, oname, group):
    """ Given a dxtbx panel group, assemble the appropiate GeometryObject
        @param pindex Index of the parent object
        @param pname  Name of the parent object
        @param oindex Index of the current object
        @param oname  Name of the current object
        @param group  dxtbx panel group

        @return an assembled GeometryObject
    """
    x = col(group.get_local_fast_axis())
    y = col(group.get_local_slow_axis())
    z = x.cross(y)
    rot = sqr((x[0],y[0],z[0],
               x[1],y[1],z[1],
               x[2],y[2],z[2]))
    rotx, roty, rotz = rot.r3_rotation_matrix_as_x_y_z_angles(deg=True)

    # round to the nearest 90 degrees
    rrotx = round(rotx/90)*90
    rroty = round(roty/90)*90
    rrotz = round(rotz/90)*90

    ox, oy, oz = group.get_local_origin()

    d = {
      'pname': pname,
      'pindex':pindex,
      'oname': oname,
      'oindex':oindex,
      'x0':    ox * 1000,
      'y0':    oy * 1000,
      'z0':    oz * 1000,
      'rot_z': rrotz%360, # design
      'rot_y': rroty%360, # design
      'rot_x': rrotx%360, # design
      'tilt_z':rotz - rrotz,
      'tilt_y':roty - rroty,
      'tilt_x':rotx - rrotx
    }

    return GeometryObject(**d)

def run(args):
  if ("--help" in args or "-h" in args) :
    print("Write a SLAC metrology file from a CSPAD CBF. Parameters:")
    master_phil.show(attributes_level=2)
    return

  user_phil = []
  for arg in args:
    try :
      user_phil.append(libtbx.phil.parse(arg))
    except RuntimeError as e :
      raise Sorry("Unrecognized argument '%s' (error: %s)" % (arg, str(e)))

  params = master_phil.fetch(sources=user_phil).extract()

  if params.cbf_header is None or \
     params.out_metrology_file is None:
    raise Usage("%s cbf_header=<path> out_metrology_file=<path>"%libtbx.env.dispatcher_name)

  if not os.path.exists(params.cbf_header):
    raise Sorry("File not found: %s"%params.cbf_header)

  print("Converting", params.cbf_header, "to", params.out_metrology_file)

  geometry = GeometryAccessFromCspadCBF(params.cbf_header)

  geometry.save_pars_in_file(params.out_metrology_file)

if (__name__ == "__main__") :
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_cspad_pinwheel.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.cspad_pinwheel
#
# Removes all but the central sensors from a CSPAD CBF
#

import dxtbx, sys, os
import libtbx.option_parser
from xfel.cftbx.detector.cspad_cbf_tbx import cbf_file_to_basis_dict, write_cspad_cbf
from libtbx.utils import Usage
from six.moves import range

def run(argv=None):
  if argv is None:
    argv = sys.argv[1:]

  command_line = libtbx.option_parser.option_parser(
    usage="%s files" % libtbx.env.dispatcher_name).process(args=argv)

  paths = command_line.args
  if len(paths) <= 0:
    raise Usage("No files specified")

  for path in paths:
    # Load the metrology dictionary, containting basis shifts for each item in the hierarchy
    metro = cbf_file_to_basis_dict(path)

    # Remove from the hiearchy all but the central sensors (sensor 1 of each quadrant).
    # Need to remove the sesnor basis shifts and the corresponding asic shifts
    for quad in range(4):
      for sensor in [0,2,3,4,5,6,7]:
        metro.pop((0,quad,sensor))
        for asic in range(2):
          metro.pop((0,quad,sensor,asic))

    # Renumber the sensors to 0 instead of 1
    for key in metro:
      if len(key) == 3:
        detector, quad, sensor = key
        metro[(detector,quad,0)] = metro.pop(key)
      elif len(key) == 4:
        detector, quad, sensor, asic = key
        metro[(detector,quad,0,asic)] = metro.pop(key)

    # Build the tiles dictionary for only sensor 1 of each quadrant.  Rename that sensor to zero.
    img = dxtbx.load(path)
    tiles = {}
    for quad in range(4):
      src_sensor = 1
      dest_sensor = 0
      for asic in range(2):
        tiles[(0,quad,dest_sensor,asic)] = img.get_raw_data()[(quad*16)+(src_sensor*2)+asic] # FIXME get the panel ID from dxtbx

    destpath = os.path.splitext(path)[0] + "_pinwheel.cbf"

    hierarchy = img.get_detector().hierarchy()
    beam = img.get_beam()

    # write the result.  Have to call abs on the root distance because of a bug with the hierarchy matrix.
    write_cspad_cbf(tiles, metro, 'cbf', None, destpath, beam.get_wavelength(), hierarchy.get_distance())

if (__name__ == "__main__") :
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_detector_format_versions.py
from __future__ import absolute_import, division, print_function
#-*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.detector_format_versions
#
# Utility for printing information about cspad detector format versions
#

import sys
from iotbx.detectors.cspad_detector_formats import _detector_format_version_dict

def print_version(version):
  """
  Given a detector format version, print its vital stats
  """
  from xfel.cxi.cspad_ana.cspad_tbx import evt_timestamp
  print("%14s "%version, "%17s"%_detector_format_version_dict[version]['address'], end=' ')
  if _detector_format_version_dict[version]['start_time'] is None:
    print("None                   ", end=' ')
  else:
    print(evt_timestamp((_detector_format_version_dict[version]['start_time'], 0)), end=' ')

  if _detector_format_version_dict[version]['start_time'] is None:
    print("None")
  else:
    print(evt_timestamp((_detector_format_version_dict[version]['end_time'], 0)))

if __name__=='__main__':

  if len(sys.argv) <= 1:
    print("Listing of all known detector formats. Use cxi.detector_format_versions <version> to show the quadrant and unit pixel translations for a given format version")
    print()
    print("Format version   Det. address     Start time              End time")

    for key in sorted(_detector_format_version_dict):
      print_version(key)
  else:
    from spotfinder.applications.xfel.cxi_phil import cxi_versioned_extract
    for version in sys.argv[1:]:
      if version not in _detector_format_version_dict:
        print("Version %s not found.  Did you use quotes?"%version)
        continue
      print("Showing info for %s detector format version"%version)
      print()
      print("Format version   Det. address     Start time              End time")
      print_version(version)
      print()

      phil = cxi_versioned_extract(["distl.detector_format_version=%s"%version])

      print("Quad translations:", phil.distl.quad_translations)
      print("Tile translations:", phil.distl.tile_translations)


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_diff.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.diff
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT

import sys

from libtbx import easy_pickle

from xfel.cxi.cspad_ana import cspad_tbx

def run(args):
  assert len(args) == 3
  d1 = easy_pickle.load(args[0])
  d2 = easy_pickle.load(args[1])

  image_1 = d1["DATA"]
  image_2 = d2["DATA"]

  assert image_1.all() == image_2.all()
  diff_image = image_1 - image_2
  d = cspad_tbx.dpack(
    data=diff_image,
    timestamp=cspad_tbx.evt_timestamp(),
    distance=1,
  )
  easy_pickle.dump(args[2], d)


if __name__ == '__main__':
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_display_metrology.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# LIBTBX_SET_DISPATCHER_NAME cxi.display_metrology
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1
# $Id
#

import sys, os
import libtbx.phil
from libtbx.utils import Usage, Sorry
from matplotlib import pyplot as plt
from matplotlib.patches import Rectangle,Polygon
from xfel.cxi.cspad_ana.cspad_tbx import xpp_active_areas
from scitbx.matrix import col

master_phil = libtbx.phil.parse("""
metrology = None
  .type = str
  .help = File with metrology information or XPP active area name
  .optional = False
""")

if (__name__ == "__main__") :
  user_phil = []
  for arg in sys.argv[1:]:
    if (os.path.isfile(arg)) or arg in xpp_active_areas or os.path.isdir(arg):
      user_phil.append(libtbx.phil.parse("""metrology=\"%s\" """ % arg))
    else :
      try :
        user_phil.append(libtbx.phil.parse(arg))
      except RuntimeError as e :
        raise Sorry("Unrecognized argument '%s' (error: %s)" % (arg, str(e)))

  params = master_phil.fetch(sources=user_phil).extract()
  if (params.metrology is None) :
    master_phil.show()
    raise Usage("metrology must be defined (either metrology=XXX, or the name alone).")

  fig = plt.figure()
  ax = fig.add_subplot(111, aspect='equal')

  if os.path.isfile(params.metrology):
    # Try dxtbx first to see if this is a regular diffraction image
    import dxtbx.format.Registry
    try:
      # Read the detector object using dxtbx
      reader = dxtbx.format.Registry.get_format_class_for_file(params.metrology)
      detector = reader(params.metrology).get_detector()
    except (IOError, TypeError):
      # See if it's a json file
      from dxtbx.model.experiment_list import ExperimentListFactory
      try:
        experiments = ExperimentListFactory.from_json_file(params.metrology)
        assert len(experiments) == 1
        detector = experiments[0].detector
      except Exception as e:
        detector = None

    if detector is None:
      # see if it's a SLAC geometry file
      from scitbx import matrix
      try:
        from PSCalib.GeometryAccess import GeometryAccess
        geometry = GeometryAccess(params.metrology)
      except Exception as e:
        geometry = None

      if geometry is None:
        # see if this is a Ginn metrology file (see Helen Ginn et. al. (2015), Acta D Cryst)
        panels = []
        for line in open(params.metrology).readlines():
          if len(line) > 0 and line[0] != '#' and len(line.strip().split()) == 10:
            panels.append(line.strip())

        if len(panels) != 64:
          raise Sorry("Can't parse this metrology file")

        print("Ginn metrology file")
        for panel_id, panel in enumerate(panels):
          PANEL, x1, y1, x2, y2, shiftX, shiftY, tiltX, tiltY, _ = panel.split()
          x1 = float(x1) + float(shiftX)
          x2 = float(x2) + float(shiftX)
          y1 = float(y1) + float(shiftY)
          y2 = float(y2) + float(shiftY)

          p0 = col((x1,y1,0))
          p1 = col((x2,y1,0))
          p2 = col((x2,y2,0))
          p3 = col((x1,y2,0))

          v1 = p1-p0
          v2 = p3-p0
          vcen = ((v2/2) + (v1/2)) + p0

          ax.add_patch(Polygon((p0[0:2],p1[0:2],p2[0:2],p3[0:2]), closed=True, color='green', fill=False, hatch='/'))
          ax.annotate("%d"%(int(PANEL)), vcen[0:2])
        ax.set_xlim((0, 2000))
        ax.set_ylim((0, 2000))
        ax.set_ylim(ax.get_ylim()[::-1])
      else:
        from serialtbx.detector.xtc import basis_from_geo

        root = geometry.get_top_geo()
        root_basis = basis_from_geo(root)

        # get pixel mappings to real space.
        x, y, z = geometry.get_pixel_coords()
        assert x.shape == y.shape == z.shape
        if len(x.shape) == 6:
          x = x.reshape(x.shape[2:6])
          y = y.reshape(y.shape[2:6])
          z = z.reshape(z.shape[2:6])
        elif len(x.shape) == 5:
          x = x.reshape(x.shape[1:5])
          y = y.reshape(y.shape[1:5])
          z = z.reshape(z.shape[1:5])
        else:
          assert len(x.shape) == 4
        sensor_slow = x.shape[2]
        sensor_fast = x.shape[3]
        ori = matrix.col((0,0,0))
        while True:
          if len(root.get_list_of_children()) == 4 or len(root.get_list_of_children()) == 32:
            break
          assert len(root.get_list_of_children()) == 1, len(root.get_list_of_children())
          child = root.get_list_of_children()[0]
          child_basis = root_basis * basis_from_geo(child)

          arrow_end = child_basis * matrix.col((0,0,0))
          dx, dy, _ = arrow_end - ori
          if dx != 0 or dy != 0:
            ax.arrow(ori[0], ori[1], dx, dy, head_width=0.5, head_length=1.0, fc='k', ec='k', length_includes_head=True)
          ori = arrow_end
          root = child
          root_basis = child_basis

        if len(root.get_list_of_children()) == 4:
          for quad_id, quad in enumerate(root.get_list_of_children()):
            quad_basis = root_basis * basis_from_geo(quad)
            arrow_end = quad_basis * matrix.col((0,0,0))
            dx, dy, _ = arrow_end - ori
            if dx != 0 or dy != 0:
              ax.arrow(ori[0], ori[1], dx, dy, head_width=0.5, head_length=1.0, fc='k', ec='k', length_includes_head=True)
            arrow_start = arrow_end
            for sensor_id, sensor in enumerate(quad.get_list_of_children()):
              sensor_x, sensor_y, sensor_z = sensor.get_pixel_coords()
              transformed_x, transformed_y, transformed_z = quad.transform_geo_coord_arrays(sensor_x, sensor_y, sensor_z)

              sensor_basis = quad_basis * basis_from_geo(sensor)
              arrow_end = sensor_basis * matrix.col((0,0,0))
              dx, dy, _ = arrow_end - arrow_start
              ax.arrow(arrow_start[0], arrow_start[1], dx, dy, head_width=0.5, head_length=1.0, fc='k', ec='k', length_includes_head=True)

              p0 = col((x[quad_id,sensor_id,0,0]/1000,
                        y[quad_id,sensor_id,0,0]/1000))
              p1 = col((x[quad_id,sensor_id,sensor_slow-1,0]/1000,
                        y[quad_id,sensor_id,sensor_slow-1,0]/1000))
              p2 = col((x[quad_id,sensor_id,sensor_slow-1,sensor_fast-1]/1000,
                        y[quad_id,sensor_id,sensor_slow-1,sensor_fast-1]/1000))
              p3 = col((x[quad_id,sensor_id,0,sensor_fast-1]/1000,
                        y[quad_id,sensor_id,0,sensor_fast-1]/1000))

              v1 = p1-p0
              v2 = p3-p0
              vcen = ((v2/2) + (v1/2)) + p0

              ax.add_patch(Polygon((p0[0:2],p1[0:2],p2[0:2],p3[0:2]), closed=True, color='green', fill=False, hatch='/'))
              ax.annotate("%d,%d"%(quad_id,sensor_id), vcen[0:2])
          ax.set_xlim((-100, 100))
          ax.set_ylim((-100, 100))
        else:
          for quad_id in range(4):
            for sensor_id, sensor in enumerate(root.get_list_of_children()[quad_id*8:(quad_id+1)*8]):
              sensor_x, sensor_y, sensor_z = sensor.get_pixel_coords()
              #transformed_x, transformed_y, transformed_z = quad.transform_geo_coord_arrays(sensor_x, sensor_y, sensor_z)

              #arrow_start = matrix.col((quad.x0/1000, quad.y0/1000))
              #arrow_end = matrix.col((transformed_x[0,0]/1000, transformed_y[0,0]/1000))
              #dx, dy = arrow_end - arrow_start
              #ax.arrow(quad.x0/1000, quad.y0/1000, dx, dy, head_width=0.05, head_length=0.1, fc='k', ec='k')

              p0 = col((x[0,quad_id*8+sensor_id,0,0]/1000,
                        y[0,quad_id*8+sensor_id,0,0]/1000))
              p1 = col((x[0,quad_id*8+sensor_id,sensor_slow-1,0]/1000,
                        y[0,quad_id*8+sensor_id,sensor_slow-1,0]/1000))
              p2 = col((x[0,quad_id*8+sensor_id,sensor_slow-1,sensor_fast-1]/1000,
                        y[0,quad_id*8+sensor_id,sensor_slow-1,sensor_fast-1]/1000))
              p3 = col((x[0,quad_id*8+sensor_id,0,sensor_fast-1]/1000,
                        y[0,quad_id*8+sensor_id,0,sensor_fast-1]/1000))

              v1 = p1-p0
              v2 = p3-p0
              vcen = ((v2/2) + (v1/2)) + p0

              ax.add_patch(Polygon((p0[0:2],p1[0:2],p2[0:2],p3[0:2]), closed=True, color='green', fill=False, hatch='/'))
              ax.annotate("%d,%d"%(quad_id,sensor_id), vcen[0:2])

            ax.set_xlim((0, 200))
            ax.set_ylim((0, 200))

    else:
      for i, panel in enumerate(detector):
        size = panel.get_image_size()
        p0 = col(panel.get_pixel_lab_coord((0,0)))
        p1 = col(panel.get_pixel_lab_coord((size[0]-1,0)))
        p2 = col(panel.get_pixel_lab_coord((size[0]-1,size[1]-1)))
        p3 = col(panel.get_pixel_lab_coord((0,size[1]-1)))

        v1 = p1-p0
        v2 = p3-p0
        vcen = ((v2/2) + (v1/2)) + p0

        ax.add_patch(Polygon((p0[0:2],p1[0:2],p2[0:2],p3[0:2]), closed=True, color='green', fill=False, hatch='/'))
        ax.annotate(i, vcen[0:2])

        ax.set_xlim((-100, 100))
        ax.set_ylim((-100, 100))

      if hasattr(detector, "hierarchy"):
        def draw_arrow(pg, start):
          o = pg.get_origin()
          if o[0:2] != start[0:2]:
            delta = col(o) - col(start)
            ax.arrow(start[0], start[1], delta[0], delta[1], head_width=0.05, head_length=0.1, fc='k', ec='k', length_includes_head=True)
          if hasattr(pg, 'children'):
            for child in pg:
              draw_arrow(child, o)
        draw_arrow(detector.hierarchy(), (0.0,0.0,0.0))

  elif params.metrology in xpp_active_areas:
    # Read the metrology from the XPP dictionary
    active_areas = xpp_active_areas[params.metrology]['active_areas']

    for asic_number, (y1, x1, y2, x2) in enumerate([(active_areas[(i*4)+0]+1,
                                                     active_areas[(i*4)+1]+1,
                                                     active_areas[(i*4)+2]-1,
                                                     active_areas[(i*4)+3]-1) for i in range(len(active_areas)//4)]):
      ax.add_patch(Rectangle((x1,y1), x2-x1, y2-y1, color="grey"))
      ax.annotate(asic_number, (x1+(x2-x1)/2,y1+(y2-y1)/2))

    ax.set_xlim((0, 2000))
    ax.set_ylim((0, 2000))
    ax.set_ylim(ax.get_ylim()[::-1])
  else:
    # Read the metrology from an LCLS calibration directory
    from xfel.cxi.cspad_ana.parse_calib import calib2sections
    sections = calib2sections(params.metrology)
    for q_id, quad in enumerate(sections):
      for s_id, sensor in enumerate(quad):
        for a_id, asic in enumerate(sensor.corners_asic()):
          y1, x1, y2, x2 = asic
          p0 = col((x1,y1))
          p1 = col((x2,y1))
          p2 = col((x2,y2))
          p3 = col((x1,y2))
          v1 = p1-p0
          v2 = p3-p0
          vcen = ((v2/2) + (v1/2)) + p0

          ax.add_patch(Polygon((p0[0:2],p1[0:2],p2[0:2],p3[0:2]), closed=True, color='green', fill=False, hatch='/'))
          ax.annotate(q_id*16+s_id*2+a_id, vcen[0:2])

    ax.set_xlim((0, 2000))
    ax.set_ylim((0, 2000))
    ax.set_ylim(ax.get_ylim()[::-1])

  plt.show()


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_generate_circular_gain_mask.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# LIBTBX_SET_DISPATCHER_NAME cxi.generate_circular_gain_mask
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export BOOST_ADAPTBX_FPE_DEFAULT=1
# $Id
#

"""
This command line function generates a gain ascii file suitable for use
by CXI.  The pixels out to the specified resolution in a circular pattern
will be set to low gain.
"""

import sys, numpy, math
import libtbx.phil
from libtbx.utils import Sorry
from xfel.cxi.cspad_ana.parse_calib import calib2sections
from scitbx.array_family import flex
from iotbx.detectors.npy import NpyImage
from spotfinder.applications.xfel import cxi_phil
from xfel.command_line.make_mask import point_inside_circle

master_phil = libtbx.phil.parse("""
detector_format_version = None
  .type = str
  .help = Detector metrology on which to overlay the gain map
resolution = None
  .type = float
  .help = Low gain pixels will be set out to this resolution. If using an annulus, instead, pixels higher than this resolution will be set to low gain
annulus_inner = None
  .type = float
  .help = Use a low gain annulus instead of masking out all the pixels below the given resolution.
annulus_outer = None
  .type = float
  .help = Use a low gain annulus instead of masking out all the pixels below the given resolution.
distance = None
  .type = float
  .help = Detector distance
wavelength = None
  .type = float
  .help = Beam wavelength
out = circle.gain
  .type = str
  .help = Output file path
optical_metrology_path = None
  .type = str
  .help = Path to slac optical metrology file. If not set, use Run 4 metrology
""")

if (__name__ == "__main__") :
  user_phil = []
  for arg in sys.argv[1:]:
    try :
      user_phil.append(libtbx.phil.parse(arg))
    except RuntimeError as e :
      raise Sorry("Unrecognized argument '%s' (error: %s)" % (arg, str(e)))

  params = master_phil.fetch(sources=user_phil).extract()
  assert params.resolution is not None or (params.annulus_inner is not None and params.annulus_outer is not None)
  assert params.distance is not None
  assert params.wavelength is not None

  annulus = (params.annulus_inner is not None and params.annulus_outer is not None)

  if annulus and params.resolution is not None:
    assert params.resolution < params.annulus_outer

  if annulus:
    if params.resolution is None:
      print("Generating annular gain mask using %s metrology between %f and %f angstroms, assuming a distance %s mm and wavelength %s angstroms" % \
        (str(params.detector_format_version), params.annulus_inner, params.annulus_outer, params.distance, params.wavelength))
    else:
      print("Generating annular gain mask using %s metrology between %f and %f angstroms, assuming a distance %s mm and wavelength %s angstroms. Also, pixels higher than %f angstroms will be set to low gain." % \
        (str(params.detector_format_version), params.annulus_inner, params.annulus_outer, params.distance, params.wavelength, params.resolution))
  elif params.resolution is not None:
    print("Generating circular gain mask using %s metrology at %s angstroms, assuming a distance %s mm and wavelength %s angstroms" % \
      (str(params.detector_format_version), params.resolution, params.distance, params.wavelength))

  from xfel.cxi.cspad_ana.cspad_tbx import dpack, evt_timestamp, cbcaa, pixel_size, CsPadDetector
  from iotbx.detectors.cspad_detector_formats import address_and_timestamp_from_detector_format_version
  from xfel.command_line.convert_gain_map import fake_env, fake_config, fake_evt, fake_cspad_ElementV2
  address, timestamp = address_and_timestamp_from_detector_format_version(params.detector_format_version)
  timestamp = evt_timestamp((timestamp,0))

  raw_data = numpy.zeros((11840,194))
  if params.detector_format_version is not None and "XPP" in params.detector_format_version:
    from xfel.cxi.cspad_ana.cspad_tbx import xpp_active_areas
    active_areas = xpp_active_areas[params.detector_format_version]['active_areas']
    data = flex.int(flex.grid((1765,1765)))
    beam_center = (1765 // 2, 1765 // 2)
  else:
    if params.optical_metrology_path is None:
      calib_dir = libtbx.env.find_in_repositories("xfel/metrology/CSPad/run4/CxiDs1.0_Cspad.0")
      sections = calib2sections(calib_dir)
    else:
      sections = calib2sections(params.optical_metrology_path)
    asic_start = 0
    data3d = []
    for i_quad in range(4):
      asic_size = 185 * 194
      section_size = asic_size * 4
      quad_start = i_quad * section_size * 4
      quad_asics = []
      for i_2x2 in range(4):
        for i_asic in range(2):
          asic_end = asic_start + 185
          a = raw_data[asic_start:asic_end, :]
          asic_start = asic_end

          asic_end = asic_start + 185
          b = raw_data[asic_start:asic_end, :]
          asic_start = asic_end

          quad_asics.append(numpy.concatenate((a,b),axis=1))
      quad_data = numpy.dstack(quad_asics)
      quad_data = numpy.rollaxis(quad_data, 2,0)
      data3d.append(fake_cspad_ElementV2(quad_data, i_quad))

    env = fake_env(fake_config())
    evt = fake_evt(data3d)
    beam_center, active_areas = cbcaa(fake_config(),sections)
    data = flex.int(CsPadDetector(address, evt, env, sections).astype(numpy.float64))

  img_dict = dpack(
          active_areas=active_areas,
          address=address,
          beam_center_x=beam_center[0]*pixel_size,
          beam_center_y=beam_center[1]*pixel_size,
          data=data,
          distance=params.distance,
          pixel_size=pixel_size,
          timestamp=timestamp,
          wavelength=params.wavelength)

  img = NpyImage("", source_data=img_dict)

  args = ["distl.detector_format_version=%s"%params.detector_format_version]
  horizons_phil = cxi_phil.cxi_versioned_extract(args)

  img.readHeader(horizons_phil)
  img.translate_tiles(horizons_phil)
  tm = img.get_tile_manager(horizons_phil)
  effective_active_areas = tm.effective_tiling_as_flex_int()

  if annulus:
    inner = params.distance * math.tan(2*math.sinh(params.wavelength/(2*params.annulus_inner)))/pixel_size
    outer = params.distance * math.tan(2*math.sinh(params.wavelength/(2*params.annulus_outer)))/pixel_size
    print("Pixel inner:", inner)
    print("Pixel outer:", outer)
  if params.resolution is not None:
    radius = params.distance * math.tan(2*math.sinh(params.wavelength/(2*params.resolution)))/pixel_size
    print("Pixel radius:", radius)

  print("Percent done: 0", end=' '); sys.stdout.flush()
  next_percent = 10
  for y in range(data.focus()[1]):
    if y*100/data.focus()[1] > next_percent:
      print(next_percent, end=' '); sys.stdout.flush()
      next_percent += 10
    for x in range(data.focus()[0]):
      if annulus:
        if not point_inside_circle(x,y,beam_center[0],beam_center[1],outer) or point_inside_circle(x,y,beam_center[0],beam_center[1],inner):
          data[y,x] = 1
      if params.resolution is not None:
        if annulus:
          if not point_inside_circle(x,y,beam_center[0],beam_center[1],radius):
            data[y,x] = 0
        else:
          if not point_inside_circle(x,y,beam_center[0],beam_center[1],radius):
            data[y,x] = 1
  print(100)

  if 'XPP' in params.detector_format_version:
    rotations = xpp_active_areas[params.detector_format_version]['rotations']
    angles = [(x+1) * 90 for x in rotations]
  else:
    angles = []
    for quad in sections:
      for section in quad:
        angles.append(section.angle)
        angles.append(section.angle)
  assert len(angles) == int(len(effective_active_areas)/4)

  raw_data = flex.int(flex.grid((11840,194)))
  for i in range(int(len(effective_active_areas)/4)):
    ul_slow = effective_active_areas[4 * i + 0]
    ul_fast = effective_active_areas[4 * i + 1]
    lr_slow = effective_active_areas[4 * i + 2]
    lr_fast = effective_active_areas[4 * i + 3]

    angle = angles[i]

    block = data.matrix_copy_block(
      i_row=ul_slow,i_column=ul_fast,
      n_rows=lr_slow-ul_slow,n_columns=lr_fast-ul_fast)

    if block.focus()[0] > block.focus()[1]:
      block = block.matrix_rot90(1 + (int(round(angle / 90.0)) % 4))
    else:
      block = block.matrix_rot90(-1 + (int(round(angle / 90.0)) % 4))

    raw_data.matrix_paste_block_in_place(
      block = block,
      i_row = i*block.focus()[0],
      i_column = 0
    )

  numpy.savetxt(params.out, raw_data.as_numpy_array(), fmt="%d")


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_index.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME cxi.index
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH PHENIX_GUI_ENVIRONMENT=1
# LIBTBX_PRE_DISPATCHER_INCLUDE_SH export PHENIX_GUI_ENVIRONMENT

from xfel.cxi.display_spots import run_one_index
from libtbx.utils import Usage, Sorry
import libtbx.option_parser
import sys,os
from six.moves import range

if (__name__ == "__main__"):
  command_line = (libtbx.option_parser.option_parser(
    usage="%s [-d] [-s] [-n num_procs] [-o output_dir] [-b output_basename] [-e extension] target=targetpath files" % libtbx.env.dispatcher_name,
    more_help=["Target: the phil file containing further indexing/integration parameters"])
                  .option(None, "--no_display", "-d",
                          action="store_true",
                          default=False,
                          dest="no_display",
                          help="Do not show indexing graphics")
                  .option(None, "--skip_processed", "-s",
                          action="store_true",
                          default=False,
                          dest="skip_processed",
                          help="Skip files that have alread been processed")
                  .option(None, "--num_procs", "-n",
                          type="int",
                          default=1,
                          dest="num_procs",
                          help="Number of processors to use")
                  .option(None, "--output_dir", "-o"    ,
                          type="string",
                          default=None,
                          dest="output_dir",
                          help="Directory for integration pickles")
                  .option(None, "--output_basename", "-b",
                          type="string",
                          default="int_",
                          dest="output_basename",
                          help="String to append to the front of output integration pickles")
                  .option(None, "--extension", "-e",
                          type="string",
                          default=".pickle",
                          dest="extension",
                          help="File extension use to filter input files if a directory is given as input")
                  ).process(args=sys.argv[1:])

  files = [arg for arg in command_line.args if os.path.isfile(arg)]
  dirs = [arg for arg in command_line.args if os.path.isdir(arg)]
  for directory in dirs:
    for path in os.listdir(directory):
      if os.path.splitext(path)[1] == command_line.options.extension:
        files.append(os.path.join(directory, path))

  arguments = [arg for arg in command_line.args if not os.path.isfile(arg) and not os.path.isdir(arg)]

  found_it = False
  for arg in arguments:
    if "target=" in arg:
      found_it = True
      break
  if not found_it:
    raise Usage(command_line.parser.usage)

  if command_line.options.no_display:
    display = False
    arguments.append('--nodisplay')
  else:
    display = True

  assert command_line.options.num_procs > 0
  if command_line.options.output_dir is not None and \
    not os.path.isdir(command_line.options.output_dir):
    raise Sorry("Output dir %s doesn't exist"%command_line.options.output_dir)

  def do_work(item):
    file, arguments, kwargs = item
    try:
      run_one_index(file, *arguments, **({'display':display}))
    except Exception as e:
      if hasattr(e, "classname"):
        print(e.classname, "for %s:"%file, end=' ')
      else:
        print("Indexing error for %s:"%file, end=' ')
      print(e)

  if command_line.options.num_procs == 1:
    for file in files:
      if command_line.options.output_dir is not None:
        int_pickle_path = os.path.join(command_line.options.output_dir, \
          command_line.options.output_basename + os.path.basename(file))
        if command_line.options.skip_processed and os.path.isfile(int_pickle_path):
          print(file, "already processed, skipping")
          continue
        arguments.append("indexing.completeness_pickle=%s"%int_pickle_path)
      do_work((file, arguments, ({'display':display})))
  else:
    import multiprocessing, copy

    def worker():
      for item in iter( q.get, None ):
        do_work(item)
        q.task_done()
      q.task_done()

    q = multiprocessing.JoinableQueue()
    procs = []
    for i in range(command_line.options.num_procs):
      procs.append(multiprocessing.Process(target=worker))
      procs[-1].daemon = True
      procs[-1].start()

    for file in files:
      if command_line.options.output_dir is not None:
        int_pickle_path = os.path.join(command_line.options.output_dir, \
          command_line.options.output_basename + os.path.basename(file))
        if command_line.options.skip_processed and os.path.isfile(int_pickle_path):
          print(file, "already processed, skipping")
          continue
        args = copy.copy(arguments)
        args.append("indexing.completeness_pickle=%s"%int_pickle_path)
      else:
        args = arguments

      q.put((file, args, ({'display':display})))

    q.join()

    for p in procs:
      q.put( None )

    q.join()

    for p in procs:
      p.join()

    print("Finished everything....")
    print("num active children:", len(multiprocessing.active_children()))


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_lsf.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.lsf
#
# $Id$

from __future__ import absolute_import, division, print_function

import sys


def run(argv=None):
  import libtbx.load_env

  from os import path
  from libtbx import easy_run

  if argv is None:
    argv = sys.argv

  # Absolute path to the executable Bourne-shell script.
  lsf_sh = libtbx.env.under_dist('xfel', path.join('cxi', 'lsf.sh'))

  return easy_run.call(' '.join([lsf_sh] + argv[1:]))


if __name__ == '__main__':
  sys.exit(run())


 *******************************************************************************


 *******************************************************************************
xfel/command_line/cxi_merge.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# LIBTBX_SET_DISPATCHER_NAME cxi.merge
# LIBTBX_SET_DISPATCHER_NAME xfel.merge
#
# $Id$

from __future__ import absolute_import, division, print_function
from six.moves import range

from rstbx.dials_core.integration_core import show_observations
import iotbx.phil
from iotbx import data_plots
from cctbx.array_family import flex
from cctbx import miller
from cctbx.crystal import symmetry
from cctbx.sgtbx.bravais_types import bravais_lattice
from cctbx import uctbx
from libtbx.str_utils import format_value
from libtbx.utils import Usage, Sorry, multi_out
from libtbx import easy_pickle
from libtbx import adopt_init_args, group_args, Auto
from six.moves import cStringIO as StringIO
import os
import math
import time
import sys
import glob
from scitbx import matrix
import six
from six.moves import zip
op = os.path

from xfel.merging.database.merging_database import mysql_master_phil
master_phil="""
data = None
  .type = path
  .multiple = True
  .help = Directory containing integrated data in pickle format.  Repeat to \
    specify additional directories.
targlob = None
  .type = str
  .multiple = True
  .help = new feature, instead of data records giving directories containing integration pickles
  .help = give a single blob giving the paths of tar files where the pickles are packaged up.
  .help = This reduces the number of files to be read in.  But as currently implemented
  .help = it does not reduce the number of file opens.
hash_filenames = False
  .type = bool
  .help = For CC1/2, instead of using odd/even filenames to split images into two sets, \
          hash the filename using md5 and split the images using odd/even hashes.
predictions_to_edge {
  apply = False
    .type = bool
    .help = If True and key 'indices_to_edge' not found in integration pickles, predictions
    .help = will be made to the edge of the detector based on current unit cell, orientation,
    .help = and mosaicity.
  image = None
    .type = path
    .help = Path to an example image from which to extract active areas and pixel size.
  detector_phil = None
    .type = path
    .help = Path to the detector version phil file used to generate the selected data.
}
short_circuit = False
  .type = bool
  .help = Assess per-image resolution limits and exit early.
a_list = None
  .type = path
  .multiple = False # for now XXX possibly make it multiple later
  .help = Text file containing a list of acceptable integration pickles, that is, not ones
  .help = that are misindexed, wrong type, or otherwise rejected as determined separately by user
filename_extension = "pickle"
  .type = str
  .help = Filename extension for integration pickle files. Usually pickle but can be otherwise.
data_subset = 0
  .type = int
  .help = 0: use all data / 1: use odd-numbered frames / 2: use even-numbered frames
validation {
  exclude_CSPAD_sensor = None
    .type = int
    .help = Index in range(32) of a sensor on the CSPAD to exclude from merging, for the purposes
    .help = of testing whether an individual sensor is poorly calibrated.
}
model = None
  .type = str
  .help = PDB filename containing atomic coordinates & isomorphous cryst1 record
  .help = or MTZ filename from a previous cycle of cxi.merge (not yet tested with prime MTZ)
model_reindex_op = h,k,l
  .type = str
  .help = Kludge for cases with an indexing ambiguity, need to be able to adjust scaling model
data_reindex_op = h,k,l
  .type = str
  .help = Reindex, e.g. to change C-axis of an orthorhombic cell to align Bravais lattice from indexing with actual space group
target_unit_cell = None
  .type = unit_cell
  .help = leave as None, program uses the model PDB file cryst1 record
target_space_group = None
  .type = space_group
  .help = leave  as None, program uses the model PDB file cryst1 record
set_average_unit_cell = False
  .type = bool
rescale_with_average_cell = False
  .type = bool
d_min = None
  .type = float
  .help = limiting resolution for scaling and merging
d_max = None
  .type = float
  .help = limiting resolution for scaling and merging.  Implementation currently affects only the CCiso cal
k_sol = 0.35
  .type = float
  .help = bulk solvent scale factor - approximate mean value in PDB \
    (according to Pavel)
b_sol = 46.00
  .type = float
  .help = bulk solvent B-factor - approximate mean value in PDB \
    (according to Pavel)
merge_anomalous = False
  .type = bool
  .help = Merge anomalous contributors
include_bulk_solvent = True
  .type = bool
wavelength = None
  .type = float
pixel_size = None
  .type = float
  .help = Detector-specific parameter for pixel size in mm
elements = None
  .type = str
  .multiple = True
significance_filter {
  apply = True
    .type = bool
    .help = Apply a sigma cutoff (on unmerged data) to limit resolution from each diffraction pattern
  n_bins = 12
    .type = int (value_min=2)
    .help = Initial target number of resolution bins for sigma cutoff calculation
  min_ct = 10
    .type = int
    .help = Decrease number of resolution bins to require mean bin population >= min_ct
  max_ct = 50
    .type = int
    .help = Increase number of resolution bins to require mean bin population <= max_ct
  sigma = 0.5
    .type = float
    .help = Remove highest resolution bins such that all accepted bins have <I/sigma> >= sigma
}
min_corr = 0.1
  .type = float
  .help = Correlation cutoff for rejecting individual frames.
  .help = This filter is not applied if model==None.
unit_cell_length_tolerance = 0.1
  .type = float
  .help = Fractional change in unit cell dimensions allowed (versus target \
    cell).
unit_cell_angle_tolerance = 2.
  .type = float
nproc = None
  .type = int
raw_data {
  sdfac_auto = False
    .type = bool
    .help = apply sdfac to each-image data assuming negative intensities are normally distributed noise
  sdfac_refine = False
    .type = bool
    .help = Correct merged sigmas by refining sdfac, sdb and sdadd according to Evans 2011. Not \
            compatible with sdfac_auto.
  errors_from_sample_residuals = False
    .type = bool
    .help = Use sample residuals as error estimates. Not compatible with sdfac_auto or sdfac_refine.
  reduced_chi_squared_correction = False
    .type = bool
    .help = Multiply the merged error by the reduced chi-squared to correct for under-estimation \
            of experimental errors. Applied after other error models.
  propagate_errors = False
    .type = bool
    .help = Propagate errors from estimated parameters
  error_models {
    errors_from_sample_residuals {
      biased = False
        .type = bool
        .help = Used biased variance instead of unbiased variance when computing the sample \
                variance. Difference of N (biased) vs. N-1 (unbiased) in the denominator
    }
    sdfac_refine {
      random_seed = None
        .help = Random seed. May be int or None. Only used for the simplex minimizer
        .type = int
        .expert_level = 1
      minimizer = *simplex lbfgs LevMar
        .type = choice
        .help = Which minimizer to use while refining the Sdfac terms
      refine_propagated_errors = False
        .type = bool
        .help = If True and if propagate_errors is True, then during sdfac refinement, also \
                refine the estimated error used for error propagation.
      show_finite_differences = False
        .type = bool
        .help = If True and minimizer is lbfgs, show the finite vs. analytical differences
      plot_refinement_steps = False
        .type = bool
        .help = If True, plot refinement steps during refinement.
      apply_to_I_only = False
        .type = bool
        .expert_level = 4
        .help = If True, after sdfac refinement, apply the new errors only to the merged I \
                during the weighted sum, keeping the old errors for the sigI value.        \
                Flag for research only, not recommended for general use.
    }
  }
}
output {
  n_bins = 10
    .type = int
    .help = Number of resolution bins in statistics table
  prefix = iobs
    .type = str
    .help = Prefix for all output file names
  title = None
    .type = str
    .help = Title for run - will appear in MTZ file header
  unit_cell = None
    .type = unit_cell
    .help = override the average or reference unit cell
  space_group = None
    .type = space_group
    .help = override the identified space group
}
merging {
  refine_G_Imodel = False
    .type = bool
    .help = "Refine per-frame scaling factors and model intensities
             after initial merging"
  reverse_lookup = None
    .type = str
    .help = filename, pickle format, generated by the cxi.brehm_diederichs program.  Contains a
    .help = (key,value) dictionary where key is the filename of the integrated data pickle file (supplied
    .help = with the data phil parameter and value is the h,k,l reindexing operator that resolves the
    .help = indexing ambiguity.
  minimum_multiplicity = None
    .type = int
    .help = If defined, merged structure factors not produced for the Miller indices below this threshold.
}
scaling {
  mtz_file = None
    .type = str
    .help = for Riso/ CCiso, the reference structure factors, must have data type F
    .help = a fake file is written out to this file name if model is None
  mtz_column_F = fobs
    .type = str
    .help = for Riso/ CCiso, the column name containing reference structure factors
  log_cutoff = None
    .type = float
    .help = for CC calculation, log(intensity) cutoff, ignore values less than this
  show_plots = False
    .type = bool
  enable = True
    .type = bool
    .help = enable the mark0 algorithm, otherwise individual-image scale factors are set to 1.0
    .expert_level = 3
  algorithm = *mark0 mark1 levmar
    .type = choice
    .help = "mark0: original per-image scaling by reference to
             isomorphous PDB model"
    .help = "mark1: no scaling, just averaging (i.e. Monte Carlo
             algorithm).  Individual image scale factors are set to 1."
    .help = "Sparse-matrix Levenberg-Marquardt scaling and merging.  Under development, not available in cxi.merge"
  simulation = None
    .type = str
    .help = To test scaling, use simulated data from the model instead of the actual observations
    .help = String value governs how the sim data are calculated
  simulation_data = None
    .type = floats
    .help = Extra parameters for the simulation, exact meaning depends on calculation method
  report_ML = False
    .type = bool
    .help = Report statistics on per-frame attributes modeled by max-likelihood fit (expert only)
}
postrefinement {
  enable = False
    .type = bool
    .help = enable the preliminary postrefinement algorithm (monochromatic)
    .expert_level = 3
  algorithm = *rs rs2 rs_hybrid eta_deff
    .type = choice
    .help = rs only, eta_deff protocol 7
    .expert_level = 3
  rs2
    .help = Reimplement postrefinement with the following (Oct 2016):
    .help = Refinement engine now work on analytical derivatives instead of finite differences
    .help = Better convergence using "traditional convergence test"
    .help = Use a streamlined frame_db schema, currently only supported for FS (filesystem) backend
    {}
  rs_hybrid
    .help = More aggressive postrefinement with the following (Oct 2016):
    .help = One round of 'rs2' using LBFGS minimizer as above to refine G,B,rotx,roty
    .help = Gentle weighting rather than unit weighting for the postrefinement target
    .help = Second round of LevMar adding an Rs refinement parameter
    .help = Option of weighting the merged terms by partiality
    {
    partiality_threshold = 0.2
      .type = float ( value_min = 0.01 )
      .help = throw out observations below this value. Hard coded as 0.2 for rs2, allow value for hybrid
      .help = must enforce minimum positive value because partiality appears in the denominator
    }
  target_weighting = *unit variance gentle extreme
    .type = choice
    .help = weights for the residuals in the postrefinement target (for rs2 or rs_hybrid)
    .help = Unit: each residual weighted by 1.0
    .help = Variance: weighted by 1/sigma**2.  Doesn't seem right, constructive feedback invited
    .help = Gentle: weighted by |I|/sigma**2.  Seems like best option
    .help = Extreme: weighted by (I/sigma)**2.  Also seems right, but severely downweights weak refl
  merge_weighting = *variance
    .type = choice
    .help = assumed that individual reflections are weighted by the counting variance
  merge_partiality_exponent = 0
    .type = float
    .help = additionally weight each measurement by partiality**exp when merging
    .help = 0 is no weighting, 1 is partiality weighting, 2 is weighting by partiality-squared
  lineshape = *lorentzian gaussian
    .type = choice
    .help = Soft sphere RLP modeled with Lorentzian radial profile as in prime
    .help = or Gaussian radial profile. (for rs2 or rs_hybrid)
  show_trumpet_plot = False
    .type = bool
    .help = each-image trumpet plot showing before-after plot. Spot color warmth indicates I/sigma
    .help = Spot radius for lower plot reflects partiality. Only implemented for rs_hybrid
}
include_negatives = False
  .type = bool
  .help = Whether to include negative intensities during scaling and merging
include_negatives_fix_27May2018 = True
  .type = bool
  .help = Bugfix for include negatives. Affects cxi.xmerge.
plot_single_index_histograms = False
  .type = bool
data_subsubsets {
  subsubset = None
    .type = int
  subsubset_total = None
    .type = int
}
isoform_name = None
  .type = str
  .help = Only accept this isoform
memory {
  shared_array_allocation = None
    .type = int
    .help = Yes, it's true. Python shared arrays don't seem to be dynamic in size,
    .help = so we have to allocate them just like in Fortran.  Take your best guess
    .help = as to how many total measurements you are joining.  Only for the levmar
    .help = algorithm for now. Insufficient allocation generates
    .help = ValueError: Can only assign sequence of same size
}
levmar {
  compute_cc_half = True
    .type = bool
    .help = Double the work, but only way to get reportable results. False for debug.
  sdfac_value = 1.0
    .type = float
    .help = Multiply all input sigmas by a constant factor so as to adjust the final
    .help = values of chisq/dof to about 1.0.  Still trial and error after 30 years.
  termination
    .help = Adjust the termination criteria for LevMar non-linear least squares refinement.
    {
    step_threshold = 0.0001
      .type = float
      .help = threshold for ||step||/||parameters||.  Very sensitive, 0.0001 is comprehensive
      .help = refinement, 0.001 is short refinement, 0.01 is abortive.
    objective_decrease_threshold = None
      .type = float
      .help = threshold for the fractional decrease of the objective function. Convenient
      .help = parameter for fine tuning very large parameter optimizations.  1.E-7 is a
      .help = reasonably comprehensive refinement, 1.E-4 is a good (short) choice for >10E5 parameters.
    }
  parameter_flags = PartialityDeff PartialityEtaDeff Bfactor Deff Eta Rxy
    .help = choices for the refinement
    .help = PartialityDeff and PartialityEtaDeff are mutually exclusize mosaic models (or don't use)
    .help = Others are refineable parameters to choose from, default is only refine G and I.
    .type = choice
    .multiple = True
}
lattice_rejection {
  unit_cell = Auto
    .type = unit_cell
    .help = unit_cell for filtering crystals with the given unit cell params. If Auto will automatically choose PDB model unit cell.
  space_group = Auto
    .type = space_group
    .help = space_group for filtering crystals with the given space group params. If Auto will automatically choose PDB space group.
  d_min = None
    .type = float
    .help = minimum resolution for lattices to be merged
}
""" + mysql_master_phil

def get_observations (work_params):
  try:
    data_dirs = work_params.data
    data_subset = work_params.data_subset
    subsubset = work_params.data_subsubsets.subsubset
    subsubset_total = work_params.data_subsubsets.subsubset_total
    extension = work_params.filename_extension
  except Exception as e:
    exit("Changed the interface for get_observations, please contact authors "+str(e))

  print("Step 1.  Get a list of all files")
  if work_params.a_list is not None:
    permissible_file_names = [a.strip() for a in open(work_params.a_list,"r").readlines()]
    permissible_file_hash = dict( zip(permissible_file_names, [None]*len(permissible_file_names)))
    n_sorry = 0
  file_names = []
  if work_params.targlob:
    tar_list = [tar for tg in work_params.targlob for tar in glob.glob(tg)]
    for tarname in tar_list:
      import tarfile
      T = tarfile.open(name=tarname, mode='r')
      K = T.getmembers()
      NT = len(K)
      for nt in range(NT):
        k = os.path.basename(K[nt].path)
        file_names.append("%s;member%05d;timestamp%s"%(tarname,nt,k))
      print(tarname,NT)
  else:
    for dir_name in data_dirs :
      if not os.path.isdir(dir_name):
        if os.path.isfile(dir_name):
          #check if list-of-pickles text file is given
          pickle_list_file = open(dir_name,'r')
          pickle_list = pickle_list_file.read().split("\n")
        else:
          pickle_list = glob.glob(dir_name)
        for pickle_filename in pickle_list:
          if work_params.a_list is not None and pickle_filename not in permissible_file_hash:
            # use A_list mechanism to reject files not on the "acceptable" list
            #print "SORRY--%s FILE NOT ON THE A-List"%(pickle_filename)
            n_sorry+=1
            continue
          if os.path.isfile(pickle_filename) and pickle_filename.endswith("."+extension):
            if data_subset==0 or \
              (data_subset==1 and (int(os.path.basename(pickle_filename).split("."+extension)[0][-1])%2==1)) or \
              (data_subset==2 and (int(os.path.basename(pickle_filename).split("."+extension)[0][-1])%2==0)):
              file_names.append(pickle_filename)
        continue
      for file_name in os.listdir(dir_name):
        if work_params.a_list is not None and os.path.join(dir_name, file_name) not in permissible_file_hash:
          # use A_list mechanism to reject files not on the "acceptable" list
          print("SORRY--%s FILE NOT ON THE A-List"%(os.path.join(dir_name, file_name)))
          n_sorry+=1
          continue
        if (file_name.endswith("_00000."+extension)):
          if data_subset==0 or \
            (data_subset==1 and (int(os.path.basename(file_name).split("_00000."+extension)[0][-1])%2==1)) or \
            (data_subset==2 and (int(os.path.basename(file_name).split("_00000."+extension)[0][-1])%2==0)):
            file_names.append(os.path.join(dir_name, file_name))
        elif (file_name.endswith("."+extension)):
          if data_subset==0 or \
            (data_subset==1 and (int(os.path.basename(file_name).split("."+extension)[0][-1])%2==1)) or \
            (data_subset==2 and (int(os.path.basename(file_name).split("."+extension)[0][-1])%2==0)):
            file_names.append(os.path.join(dir_name, file_name))
    if work_params.a_list is not None:
      print ("A_LIST: %d names rejected for not being on the a_list, leaving %d accepted"%(n_sorry, len(file_names)))
    if subsubset is not None and subsubset_total is not None:
      file_names = [file_names[i] for i in range(len(file_names)) if (i+subsubset)%subsubset_total == 0]
  print("Number of pickle files found:", len(file_names))
  print()
  return file_names

class WrongBravaisError (Exception) :
  pass

class OutlierCellError (Exception) :
  pass

def get_boundaries_from_sensor_ID (sensor_ID,
                                   detector_version_phil=None,
                                   image_with_header=None) :
  if detector_version_phil is not None and image_with_header is not None:
    from xfel.command_line.cctbx_integration_pickle_viewer import get_CSPAD_active_areas
    active_areas = get_CSPAD_active_areas(image_with_header, detector_version_phil)
  else:
    from xfel.command_line.cctbx_integration_pickle_viewer import LG36_active_areas
    active_areas = LG36_active_areas
  return active_areas[8*sensor_ID:8*sensor_ID+8]

def load_result (file_name,
                 ref_bravais_type,
                 reference_cell,
                 params,
                 reindex_op,
                 out,
                 get_predictions_to_edge=False,
                 image_info=None,
                 exclude_CSPAD_sensor=None) :
  # If @p file_name cannot be read, the load_result() function returns
  # @c None.

  print("-" * 80, file=out)
  print("Step 2.  Load pickle file into dictionary obj and filter on lattice & cell with",reindex_op, file=out)
  print(file_name, file=out)
  """
  Take a pickle file, confirm that it contains the appropriate data, and
  check the lattice type and unit cell against the reference settings - if
  rejected, raises an exception (for tracking statistics).
  """
  # Ignore corrupted pickle files.
  if params.targlob:
    file_name,imember = file_name.split(";member")
    imember,timestamp = imember.split(";timestamp")
    import sys,tarfile
    T = tarfile.open(name=file_name, mode='r')
    K = T.getmembers()
    this_member = K[int(imember)]
    fileIO = T.extractfile(member=this_member)
    from six.moves import cPickle as pickle
    try:
      obj = pickle.load(fileIO)
    except Exception:
      return None
  else:
    try:
      obj = easy_pickle.load(file_name=file_name)
    except Exception:
      return None
  if ("observations" not in obj) :
    return None
  if params.isoform_name is not None:
    if not "identified_isoform" in obj:
      return None
    if obj["identified_isoform"] != params.isoform_name:
      return None
  if reindex_op == "h,k,l":
    pass
    #raise WrongBravaisError("Skipping file with h,k,l")
  else:
    for idx in range(len(obj["observations"])):
      obj["observations"][idx] = obj["observations"][idx].change_basis(reindex_op)
    from cctbx.sgtbx import change_of_basis_op
    cb_op = change_of_basis_op(reindex_op)
    ORI = obj["current_orientation"][0]
    Astar = matrix.sqr(ORI.reciprocal_matrix())
    CB_OP_sqr = matrix.sqr(cb_op.c().r().as_double()).transpose()
    from cctbx.crystal_orientation import crystal_orientation
    ORI_new = crystal_orientation((Astar*CB_OP_sqr).elems, True)
    obj["current_orientation"][0] = ORI_new
    # unexpected, first since cxi_merge had a previous postrefinement implementation that
    #  apparently failed to apply the reindex_op (XXX still must be fixed), and secondly
    #  that the crystal_orientation.change basis() implementation fails to work because
    #  it does not use the transpose:  obj["current_orientation"][0].change_basis(cb_op)
    pass
    #raise WrongBravaisError("Skipping file with alternate indexing %s"%reindex_op)

  result_array = obj["observations"][0]
  unit_cell = result_array.unit_cell()
  sg_info = result_array.space_group_info()
  print("", file=out)

  print(sg_info, file=out)
  print(unit_cell, file=out)

  if obj.get('beam_s0',None) is not None:
    # Remove the need for pixel size within cxi.merge.  Allows multipanel detector with dissimilar panels.
    # Relies on new frame extractor code called by dials.stills_process that writes s0, s1 and polarization normal
    # vectors all to the integration pickle.  Future path: use dials json and reflection file.
    s0_vec = matrix.col(obj["beam_s0"]).normalize()
    s0_polar_norm = obj["beam_polarization_normal"]
    s1_vec = obj["s1_vec"][0]
    Ns1 = len(s1_vec)
    # project the s1_vector onto the plane normal to s0.  Get result by subtracting the
    # projection of s1 onto s0, which is (s1.dot.s0_norm)s0_norm
    s0_norm = flex.vec3_double(Ns1,s0_vec)
    s1_proj = (s1_vec.dot(s0_norm))*s0_norm
    s1_in_normal_plane = s1_vec - s1_proj
    # Now want the polar angle between the projected s1 and the polarization normal
    s0_polar_norms = flex.vec3_double(Ns1,s0_polar_norm)
    dotprod = (s1_in_normal_plane.dot(s0_polar_norms))
    costheta = dotprod/(s1_in_normal_plane.norms())
    theta = flex.acos(costheta)
    prospective = flex.cos(2.0*theta)
    obj["cos_two_polar_angle"] = prospective
    # gives same as old answer to ~1% but not exact.  Not sure why, should not matter.

  else:
    #Check for pixel size (at this point we are assuming we have square pixels, all experiments described in one
    #refined.expt file use the same detector, and all panels on the detector have the same pixel size)
    if params.pixel_size is not None:
      pixel_size = params.pixel_size
    elif "pixel_size" in obj:
      pixel_size = obj["pixel_size"]
    else:
      raise Sorry("Cannot find pixel size. Specify appropriate pixel size in mm for your detector in phil file.")

    #Calculate displacements based on pixel size
    assert obj['mapped_predictions'][0].size() == obj["observations"][0].size()
    mm_predictions = pixel_size*(obj['mapped_predictions'][0])
    mm_displacements = flex.vec3_double()
    cos_two_polar_angle = flex.double()
    for pred in mm_predictions:
      mm_displacements.append((pred[0]-obj["xbeam"],pred[1]-obj["ybeam"],0.0))
      cos_two_polar_angle.append( math.cos( 2. * math.atan2(pred[1]-obj["ybeam"],pred[0]-obj["xbeam"]) ) )
    obj["cos_two_polar_angle"] = cos_two_polar_angle
    #then convert to polar angle and compute polarization correction

  if (not bravais_lattice(sg_info.type().number()) == ref_bravais_type) :
    raise WrongBravaisError("Skipping cell in different Bravais type (%s)" %
      str(sg_info))
  if (not unit_cell.is_similar_to(
      other=reference_cell,
      relative_length_tolerance=params.unit_cell_length_tolerance,
      absolute_angle_tolerance=params.unit_cell_angle_tolerance)) :
    raise OutlierCellError(
      "Skipping cell with outlier dimensions (%g %g %g %g %g %g" %
      unit_cell.parameters())
  # Illustrate how a unit cell filter would be implemented.
  #ucparams = unit_cell.parameters()
  #if not (130.21 < ucparams[2] < 130.61) or not (92.84 < ucparams[0] < 93.24):
  #  print >> out, "DOES NOT PASS ERSATZ UNIT CELL FILTER"
  #  return None
  print("Integrated data:", file=out)
  result_array.show_summary(f=out, prefix="  ")
  # XXX don't force reference setting here, it will be done later, after the
  # original unit cell is recorded

  #Remove observations on a selected sensor if requested
  if exclude_CSPAD_sensor is not None:
    print("excluding CSPAD sensor %d" % exclude_CSPAD_sensor, file=out)
    fast_min1, slow_min1, fast_max1, slow_max1, \
    fast_min2, slow_min2, fast_max2, slow_max2 = \
      get_boundaries_from_sensor_ID(exclude_CSPAD_sensor)
    fast_min = min(fast_min1, fast_min2)
    slow_min = min(slow_min1, slow_min2)
    fast_max = max(fast_max1, fast_max2)
    slow_max = max(slow_max1, slow_max2)
    accepted = flex.bool()
    px_preds = obj['mapped_predictions'][0]
    for idx in range(px_preds.size()):
      pred_fast, pred_slow = px_preds[idx]
      if (pred_fast < fast_min) or (pred_fast > fast_max) or (pred_slow < slow_min) or (pred_slow > slow_max):
        accepted.append(True)
      else:
        accepted.append(False)
    obj['mapped_predictions'][0] = obj['mapped_predictions'][0].select(accepted)
    obj['observations'][0] = obj['observations'][0].select(accepted)
    obj['cos_two_polar_angle'] = obj["cos_two_polar_angle"].select(accepted)
  if not 'indices_to_edge' in obj:
    if get_predictions_to_edge:
      from xfel.merging.predictions_to_edges import extend_predictions
      extend_predictions(obj, file_name, image_info, dmin=1.5, dump=False)
    else:
      obj['indices_to_edge'] = None
  return obj

from xfel.cxi.merging_utils import intensity_data, frame_data, null_data

class unit_cell_distribution (object) :
  """
  Container for collecting unit cell edge length statistics - both for frames
  included in the final dataset, and those rejected due to poor correlation.
  (Frames with incompatible indexing solutions will not be included.)
  """
  # TODO make this more general - currently assumes that angles are fixed,
  # which is true for the systems studied so far
  def __init__ (self) :
    self.uc_a_values = flex.double()
    self.uc_b_values = flex.double()
    self.uc_c_values = flex.double()
    self.all_uc_a_values = flex.double()
    self.all_uc_b_values = flex.double()
    self.all_uc_c_values = flex.double()

  def add_cell (self, unit_cell, rejected=False) :
    if (unit_cell is None) :
      return
    (a,b,c,alpha,beta,gamma) = unit_cell.parameters()
    if (not rejected) :
      self.uc_a_values.append(a)
      self.uc_b_values.append(b)
      self.uc_c_values.append(c)
    self.all_uc_a_values.append(a)
    self.all_uc_b_values.append(b)
    self.all_uc_c_values.append(c)

  def add_cells(self, uc) :
    """Addition operation for unit cell statistics."""
    self.uc_a_values.extend(uc.uc_a_values)
    self.uc_b_values.extend(uc.uc_b_values)
    self.uc_c_values.extend(uc.uc_c_values)
    self.all_uc_a_values.extend(uc.all_uc_a_values)
    self.all_uc_b_values.extend(uc.all_uc_b_values)
    self.all_uc_c_values.extend(uc.all_uc_c_values)

  def show_histograms (self, reference, out, n_slots=20) :
    [a0,b0,c0,alpha0,beta0,gamma0] = reference.parameters()
    print("", file=out)
    labels = ["a","b","c"]
    ref_edges = [a0,b0,c0]
    def _show_each (edges) :
      for edge, ref_edge, label in zip(edges, ref_edges, labels) :
        h = flex.histogram(edge, n_slots=n_slots)
        smin, smax = flex.min(edge), flex.max(edge)
        stats = flex.mean_and_variance(edge)
        print("  %s edge" % label, file=out)
        print("     range:     %6.2f - %.2f" % (smin, smax), file=out)
        print("     mean:      %6.2f +/- %6.2f on N = %d" % (
          stats.mean(), stats.unweighted_sample_standard_deviation(), edge.size()), file=out)
        print("     reference: %6.2f" % ref_edge, file=out)
        h.show(f=out, prefix="    ", format_cutoffs="%6.2f")
        print("", file=out)
    edges = [self.all_uc_a_values, self.all_uc_b_values, self.all_uc_c_values]
    print("Unit cell length distribution (all frames with compatible indexing):", file=out)
    _show_each(edges)
    edges = [self.uc_a_values, self.uc_b_values, self.uc_c_values]
    print("Unit cell length distribution (frames with acceptable correlation):", file=out)
    _show_each(edges)

  def get_average_cell_dimensions (self) :
    a = flex.mean(self.uc_a_values)
    b = flex.mean(self.uc_b_values)
    c = flex.mean(self.uc_c_values)
    return a,b,c

#-----------------------------------------------------------------------
class scaling_manager (intensity_data) :
  def __init__ (self, miller_set, i_model, params, log=None) :
    if (log is None) :
      log = sys.stdout
    self.log = log
    self.params = params
    self.miller_set = miller_set
    self.i_model = i_model
    self.ref_bravais_type = bravais_lattice(
      miller_set.space_group_info().type().number())
    intensity_data.__init__(self, miller_set.size())
    self.reverse_lookup = None
    if params.merging.reverse_lookup is not None:
      self.reverse_lookup = easy_pickle.load(params.merging.reverse_lookup)
    self.reset()

  def reset (self) :
    self.n_processed = 0
    self.n_accepted = 0
    self.n_file_error = 0
    self.n_low_signal = 0
    self.n_wrong_bravais = 0
    self.n_wrong_cell = 0
    self.n_low_resolution = 0
    self.n_low_corr = 0
    self.failure_modes = {}
    self.observations = flex.int()
    self.predictions_to_edge = flex.int()
    self.corr_values = flex.double()
    self.rejected_fractions = flex.double()
    self.uc_values = unit_cell_distribution()
    self.d_min_values = flex.double()
    self.wavelength = flex.double()
    self.initialize()

  @staticmethod
  def single_reflection_histograms(obs, ISIGI):
    # Per-bin sum of I and I/sig(I) for each observation.
    for i in obs.binner().array_indices(i_bin) :
      import numpy as np
      index = obs.indices()[i]
      if (index in ISIGI) :
        # Compute m, the "merged" intensity, as the average intensity
        # of all observations of the reflection with the given index.
        N = 0
        m = 0
        for t in ISIGI[index] :
          N += 1
          m += t[0]
          print("Miller %20s n-obs=%4d  sum-I=%10.0f"%(index, N, m))
          plot_n_bins = N//10
          hist,bins = np.histogram([t[0] for t in ISIGI[index]],bins=25)
          width = 0.7*(bins[1]-bins[0])
          center = (bins[:-1]+bins[1:])/2
          import matplotlib.pyplot as plt
          plt.bar(center, hist, align="center", width=width)
          plt.show()

  def scale_all (self, file_names) :
    t1 = time.time()
    if self.params.backend == 'MySQL':
      from xfel.merging.database.merging_database import manager
    elif self.params.backend == 'SQLite':
      from xfel.merging.database.merging_database_sqlite3 import manager
    else:
      from xfel.merging.database.merging_database_fs import manager

    db_mgr = manager(self.params)
    db_mgr.initialize_db(self.miller_set.indices())

    # Unless the number of requested processes is greater than one,
    # try parallel multiprocessing on a parallel host.  Block until
    # all database commands have been processed.
    nproc = self.params.nproc
    if (nproc is None) or (nproc is Auto):
      import libtbx.introspection
      nproc = libtbx.introspection.number_of_processors()
    if nproc > 1:
      try :
        import multiprocessing
        self._scale_all_parallel(file_names, db_mgr)
      except ImportError as e :
        print("multiprocessing module not available (requires Python >= 2.6)\n" \
          "will scale frames serially", file=self.log)
        self._scale_all_serial(file_names, db_mgr)
    else:
      self._scale_all_serial(file_names, db_mgr)
    db_mgr.join()

    t2 = time.time()
    print("", file=self.log)
    print("#" * 80, file=self.log)
    print("FINISHED MERGING", file=self.log)
    print("  Elapsed time: %.1fs" % (t2 - t1), file=self.log)
    print("  %d of %d integration files were accepted" % (
      self.n_accepted, len(file_names)), file=self.log)
    print("  %d rejected due to wrong Bravais group" % \
      self.n_wrong_bravais, file=self.log)
    print("  %d rejected for unit cell outliers" % \
      self.n_wrong_cell, file=self.log)
    print("  %d rejected for low resolution" % \
      self.n_low_resolution, file=self.log)
    print("  %d rejected for low signal" % \
      self.n_low_signal, file=self.log)
    print("  %d rejected due to up-front poor correlation under min_corr parameter" % \
      self.n_low_corr, file=self.log)
    print("  %d rejected for file errors or no reindex matrix" % \
      self.n_file_error, file=self.log)
    for key in self.failure_modes.keys():
      print("  %d rejected due to %s"%(self.failure_modes[key], key), file=self.log)

    checksum = self.n_accepted  + self.n_file_error \
               + self.n_low_corr + self.n_low_signal \
               + self.n_wrong_bravais + self.n_wrong_cell \
               + self.n_low_resolution \
               + sum([val for val in six.itervalues(self.failure_modes)])
    assert checksum == len(file_names)

    high_res_count = (self.d_min_values <= self.params.d_min).count(True)
    print("Of %d accepted images, %d accepted to %5.2f Angstrom resolution" % \
      (self.n_accepted, high_res_count, self.params.d_min), file=self.log)

    if self.params.raw_data.propagate_errors and not self.params.raw_data.error_models.sdfac_refine.refine_propagated_errors:
      assert self.params.postrefinement.enable
      from xfel.merging.algorithms.error_model.sdfac_propagate import sdfac_propagate
      error_modeler = sdfac_propagate(self)
      error_modeler.adjust_errors()

    if self.params.raw_data.sdfac_refine or self.params.raw_data.errors_from_sample_residuals or \
        self.params.raw_data.error_models.sdfac_refine.refine_propagated_errors:
      if self.params.raw_data.sdfac_refine:
        if self.params.raw_data.error_models.sdfac_refine.minimizer == 'simplex':
          from xfel.merging.algorithms.error_model.sdfac_refine import sdfac_refine as error_modeler
        elif self.params.raw_data.error_models.sdfac_refine.minimizer == 'lbfgs':
          if self.params.raw_data.error_models.sdfac_refine.refine_propagated_errors:
            from xfel.merging.algorithms.error_model.sdfac_propagate_and_refine import sdfac_propagate_and_refine as error_modeler
          else:
            from xfel.merging.algorithms.error_model.sdfac_refine_lbfgs import sdfac_refine_refltable_lbfgs as error_modeler
        elif self.params.raw_data.error_models.sdfac_refine.minimizer == 'LevMar':
          from xfel.merging.algorithms.error_model.sdfac_refine_levmar import sdfac_refine_refltable_levmar as error_modeler

      if self.params.raw_data.errors_from_sample_residuals:
        from xfel.merging.algorithms.error_model.errors_from_residuals import errors_from_residuals as error_modeler

      error_modeler(self).adjust_errors()

    if self.params.raw_data.reduced_chi_squared_correction:
      from xfel.merging.algorithms.error_model.reduced_chi_squared import reduced_chi_squared
      reduced_chi_squared(self).compute()

  def _scale_all_parallel (self, file_names, db_mgr) :
    import multiprocessing
    import libtbx.introspection

    nproc = self.params.nproc
    if (nproc is None) or (nproc is Auto) :
      nproc = libtbx.introspection.number_of_processors()

    # Input files are supplied to the scaling processes on demand by
    # means of a queue.
    #
    # XXX The input queue may need to either allow non-blocking
    # put():s or run in a separate process to prevent the procedure
    # from blocking here if the list of file paths does not fit into
    # the queue's buffer.
    input_queue = multiprocessing.Manager().JoinableQueue()
    for file_name in file_names:
      input_queue.put(file_name)

    pool = multiprocessing.Pool(processes=nproc)
    # Each process accumulates its own statistics in serial, and the
    # grand total is eventually collected by the main process'
    # _add_all_frames() function.
    for i in range(nproc) :
      sm = scaling_manager(self.miller_set, self.i_model, self.params)
      pool.apply_async(
        func=sm,
        args=[input_queue, db_mgr],
        callback=self._add_all_frames)
    pool.close()
    pool.join()

    # Block until the input queue has been emptied.
    input_queue.join()


  def _scale_all_serial (self, file_names, db_mgr) :
    """
    Scale frames sequentially (single-process).  The return value is
    picked up by the callback.
    """
    for file_name in file_names :
      scaled = self.scale_frame(file_name, db_mgr)
      if (scaled is not None) :
        self.add_frame(scaled)
    return (self)

  def add_frame (self, data) :
    """
    Combine the scaled data from a frame with the current overall dataset.
    Also accepts None or null_data objects, when data are unusable but we
    want to record the file as processed.
    """
    self.n_processed += 1
    if (data is None) :
      return None
    #data.show_log_out(self.log)
    #self.log.flush()
    if (isinstance(data, null_data)) :
      if (data.file_error) :
        self.n_file_error += 1
      elif (data.low_signal) :
        self.n_low_signal += 1
      elif (data.wrong_bravais) :
        self.n_wrong_bravais += 1
      elif (data.wrong_cell) :
        self.n_wrong_cell += 1
      elif (data.low_resolution) :
        self.n_low_resolution += 1
      elif (data.low_correlation) :
        self.n_low_corr += 1
      elif (getattr(data,"reason",None) is not None):
        if str(data.reason)!="":
          self.failure_modes[str(data.reason)] = self.failure_modes.get(str(data.reason),0) + 1
        elif repr(type(data.reason))!="":
          self.failure_modes[repr(type(data.reason))] = self.failure_modes.get(repr(type(data.reason)),0) + 1
        else:
          self.failure_modes["other reasons"] = self.failure_modes.get("other reasons",0) + 1
      return
    if (data.accept) :
      self.n_accepted    += 1
      self.completeness  += data.completeness
      #print "observations count increased by %d" % flex.sum(data.completeness)
      self.completeness_predictions += data.completeness_predictions
      #print "predictions count increased by %d" % flex.sum(data.completeness_predictions)
      self.summed_N      += data.summed_N
      self.summed_weight += data.summed_weight
      self.summed_wt_I   += data.summed_wt_I
      for index, isigi in six.iteritems(data.ISIGI) :
        if (index in self.ISIGI):
          self.ISIGI[index] += isigi
        else:
          self.ISIGI[index] = isigi
    else :
      self.n_low_corr += 1 # FIXME this is no longer the right default
    self.uc_values.add_cell(data.indexed_cell,
      rejected=(not data.accept))
    if not self.params.short_circuit:
      self.observations.append(data.n_obs)
    if (data.n_obs > 0) :
      frac_rejected = data.n_rejected / data.n_obs
      self.rejected_fractions.append(frac_rejected)
      self.d_min_values.append(data.d_min)
    self.corr_values.append(data.corr)
    self.wavelength.append(data.wavelength)

  def _add_all_frames (self, data) :
    """The _add_all_frames() function collects the statistics accumulated
    in @p data by the individual scaling processes in the process
    pool.  This callback function is run in serial, so it does not
    need a lock.
    """
    self.n_accepted += data.n_accepted
    self.n_file_error += data.n_file_error
    self.n_low_corr += data.n_low_corr
    self.n_low_signal += data.n_low_signal
    self.n_processed += data.n_processed
    self.n_wrong_bravais += data.n_wrong_bravais
    self.n_wrong_cell += data.n_wrong_cell
    self.n_low_resolution += data.n_low_resolution
    for key in data.failure_modes.keys():
      self.failure_modes[key] = self.failure_modes.get(key,0) + data.failure_modes[key]

    for index, isigi in six.iteritems(data.ISIGI) :
      if (index in self.ISIGI):
        self.ISIGI[index] += isigi
      else:
        self.ISIGI[index] = isigi

    self.completeness += data.completeness
    #print "observations count increased by %d" % flex.sum(data.completeness)
    self.completeness_predictions += data.completeness_predictions
    #print "predictions count increased by %d" % flex.sum(data.completeness_predictions)
    self.summed_N += data.summed_N
    self.summed_weight += data.summed_weight
    self.summed_wt_I += data.summed_wt_I

    self.corr_values.extend(data.corr_values)
    self.d_min_values.extend(data.d_min_values)
    if not self.params.short_circuit:
      self.observations.extend(data.observations)
    self.rejected_fractions.extend(data.rejected_fractions)
    self.wavelength.extend(data.wavelength)

    self.uc_values.add_cells(data.uc_values)

  def show_unit_cell_histograms (self) :
    self.uc_values.show_histograms(
      reference=self.miller_set.unit_cell(),
      out=self.log)

  def get_plot_statistics (self) :
    return plot_statistics(
      prefix=self.params.output.prefix,
      unit_cell_statistics=self.uc_values,
      reference_cell=self.miller_set.unit_cell(),
      correlations=self.corr_values,
      min_corr=self.params.min_corr,
      rejected_fractions=self.rejected_fractions,
      frame_d_min=self.d_min_values)

  def get_overall_correlation (self, sum_I) :
    """
    Correlate the averaged intensities to the intensities from the
    reference data set.  XXX The sum_I argument is really a kludge!
    """
    sum_xx = 0
    sum_xy = 0
    sum_yy = 0
    sum_x  = 0
    sum_y  = 0
    N      = 0
    for i in range(len(self.summed_N)):
      if (self.summed_N[i] <= 0):
        continue
      # skip structure factor if i_model.sigma is invalid (intentionally < 0)
      if self.i_model.sigmas()[i] < 0: continue
      I_r       = self.i_model.data()[i]
      I_o       = sum_I[i]/self.summed_N[i]
      N      += 1
      sum_xx += I_r**2
      sum_yy += I_o**2
      sum_xy += I_r * I_o
      sum_x  += I_r
      sum_y  += I_o
    slope = (N * sum_xy - sum_x * sum_y) / (N * sum_xx - sum_x**2)
    corr  = (N * sum_xy - sum_x * sum_y) / (math.sqrt(N * sum_xx - sum_x**2) *
             math.sqrt(N * sum_yy - sum_y**2))
    print("SUMMARY: For %d reflections, got slope %f, correlation %f" \
        % (N, slope, corr), file=self.log)
    return N, corr

  def finalize_and_save_data (self) :
    """
    Assemble a Miller array with the summed data, setting the unit cell to
    the consensus average if desired, and write to an MTZ file (including
    merged/non-anomalous data too).
    """
    print("", file=self.log)
    print("#" * 80, file=self.log)
    print("OUTPUT FILES", file=self.log)
    if self.params.merging.minimum_multiplicity is None:
      multiplicity_flag = flex.bool(len(self.summed_N),True)
    else:
      multiplicity_flag = (self.summed_N > self.params.merging.minimum_multiplicity)
    Iobs_all = flex.double(self.miller_set.size())
    SigI_all = flex.double(self.miller_set.size())
    for i in range(len(Iobs_all)):
      if (self.summed_weight[i] > 0.):
       if (multiplicity_flag[i]):
        Iobs_all[i] = self.summed_wt_I[i] / self.summed_weight[i]
        if hasattr(self, 'summed_weight_uncorrected'):
          summed_weight = self.summed_weight_uncorrected[i]
        else:
          summed_weight = self.summed_weight[i]
        if self.params.raw_data.reduced_chi_squared_correction:
          SigI_all[i] = math.sqrt(self.reduced_chi_squared[i] / summed_weight)
        else:
          SigI_all[i] = math.sqrt(1. / summed_weight)
    if (self.params.set_average_unit_cell) :
      # XXX since XFEL crystallography runs at room temperature, it may not
      # be appropriate to use the cell dimensions from a cryo structure.
      # also, some runs seem to have huge variance in the indexed cell
      # dimensions, so downstream programs (MR, refinement) may run better
      # with the cell set to the mean edge lengths.
      abc = self.uc_values.get_average_cell_dimensions()
      print("  (will use final unit cell edges %g %g %g)" % abc, file=self.log)
      angles = self.miller_set.unit_cell().parameters()[3:]
      unit_cell = uctbx.unit_cell(list(abc) + list(angles))
      final_symm = symmetry(
        unit_cell=unit_cell,
        space_group_info=self.miller_set.space_group_info())
    else :
      final_symm = self.miller_set
    if (self.params.output.unit_cell is not None or self.params.output.space_group is not None) :
      output_uc = self.params.output.unit_cell or final_symm.unit_cell()
      output_sg = self.params.output.space_group or final_symm.space_group_info()
      final_symm = symmetry(unit_cell=output_uc,
                            space_group_info=output_sg)
    all_obs = miller.array(
      miller_set=self.miller_set.customized_copy(
        crystal_symmetry=final_symm),
      data=Iobs_all,
      sigmas=SigI_all).resolution_filter(
        d_min=self.params.d_min).set_observation_type_xray_intensity()
    mtz_file = "%s.mtz" % self.params.output.prefix
    if not self.params.include_negatives:
      all_obs = all_obs.select(all_obs.data() > 0)

    mtz_out = all_obs.as_mtz_dataset(
      column_root_label="Iobs",
      title=self.params.output.title,
      wavelength=flex.mean(self.wavelength))
    mtz_out.add_miller_array(
      miller_array=all_obs.average_bijvoet_mates(),
      column_root_label="IMEAN")
    mtz_obj = mtz_out.mtz_object()
    mtz_obj.write(mtz_file)
    print("  Anomalous and mean data:\n    %s" % \
      os.path.abspath(mtz_file), file=self.log)
    print("", file=self.log)
    print("Final data:", file=self.log)
    all_obs.show_summary(self.log, prefix="  ")
    return mtz_file, all_obs

  def __call__ (self, input_queue, db_mgr) :
    # Scale frames sequentially within the current process.  The
    # return value is picked up by the callback.  See also
    # self.scale_all_serial()
    from Queue import Empty

    try :
      while True:
        try:
          file_name = input_queue.get_nowait()
        except Empty:
          return self

        scaled = self.scale_frame(file_name, db_mgr)
        if scaled is not None:
          self.add_frame(scaled)
        input_queue.task_done()

    except Exception as e :
      print(str(e), file=self.log)
      return None

  def scale_frame (self, file_name, db_mgr) :
    """The scale_frame() function populates a back end database with
    appropriately scaled intensities derived from a single frame.  The
    mark0 scaling algorithm determines the scale factor by correlating
    the frame's corrected intensities to those of a reference
    structure, while the mark1 algorithm applies no scaling at all.
    The scale_frame() function can be called either serially or via a
    multiprocessing map() function.

    @note This function must not modify any internal data or the
          parallelization will not yield usable results!

    @param file_name Path to integration pickle file
    @param db_mgr    Back end database manager
    @return          An intensity_data object
    """

    out = StringIO()
    wrong_cell = wrong_bravais = False
    reindex_op = self.params.data_reindex_op
    if self.reverse_lookup is not None:
      reindex_op = self.reverse_lookup.get(file_name, None)
      if reindex_op is None:
        return null_data(file_name=file_name, log_out=out.getvalue(), file_error=True)
    if (self.params.predictions_to_edge.apply) and (self.params.predictions_to_edge.image is not None):
      from xfel.merging.predictions_to_edges import ImageInfo
      image_info = ImageInfo(self.params.predictions_to_edge.image, detector_phil=self.params.predictions_to_edge.detector_phil)
    else:
      image_info = None

    if self.params.lattice_rejection.unit_cell == Auto:
      self.params.lattice_rejection.unit_cell = self.params.target_unit_cell

    try :
      result = load_result(
        file_name=file_name,
        reference_cell=self.params.lattice_rejection.unit_cell,
        ref_bravais_type=self.ref_bravais_type,
        params=self.params,
        reindex_op = reindex_op,
        out=out,
        get_predictions_to_edge=self.params.predictions_to_edge.apply,
        image_info=image_info,
        exclude_CSPAD_sensor=self.params.validation.exclude_CSPAD_sensor)
      if result is None:
        return null_data(
          file_name=file_name, log_out=out.getvalue(), file_error=True)
    except OutlierCellError as e :
      print(str(e), file=out)
      return null_data(
        file_name=file_name, log_out=out.getvalue(), wrong_cell=True)
    except WrongBravaisError as e :
      print(str(e), file=out)
      return null_data(
        file_name=file_name, log_out=out.getvalue(), wrong_bravais=True)
    return self.scale_frame_detail(result, file_name, db_mgr, out)

  def scale_frame_detail(self, result, file_name, db_mgr, out):
    # If the pickled integration file does not contain a wavelength,
    # fall back on the value given on the command line.  XXX The
    # wavelength parameter should probably be removed from master_phil
    # once all pickled integration files contain it.
    if ("wavelength" in result):
      wavelength = result["wavelength"]
    elif (self.params.wavelength is not None):
      wavelength = self.params.wavelength
    else:
      # XXX Give error, or raise exception?
      return None
    assert (wavelength > 0)

    observations = result["observations"][0]

    if len(observations.data()) == 0:
      print("skipping image: no observations", file=out)
      return null_data(
        file_name=file_name, log_out=out.getvalue(), low_signal=True)

    cos_two_polar_angle = result["cos_two_polar_angle"]
    indices_to_edge = result["indices_to_edge"]

    assert observations.size() == cos_two_polar_angle.size()
    tt_vec = observations.two_theta(wavelength)
    #print >> out, "mean tt degrees",180.*flex.mean(tt_vec.data())/math.pi
    cos_tt_vec = flex.cos( tt_vec.data() )
    sin_tt_vec = flex.sin( tt_vec.data() )
    cos_sq_tt_vec = cos_tt_vec * cos_tt_vec
    sin_sq_tt_vec = sin_tt_vec * sin_tt_vec
    P_nought_vec = 0.5 * (1. + cos_sq_tt_vec)

    F_prime = -1.0 # Hard-coded value defines the incident polarization axis
    P_prime = 0.5 * F_prime * cos_two_polar_angle * sin_sq_tt_vec
    # XXX added as a diagnostic
    prange=P_nought_vec - P_prime

    other_F_prime = 1.0
    otherP_prime = 0.5 * other_F_prime * cos_two_polar_angle * sin_sq_tt_vec
    otherprange=P_nought_vec - otherP_prime
    diff2 = flex.abs(prange - otherprange)
    print("mean diff is",flex.mean(diff2), "range",flex.min(diff2), flex.max(diff2), file=out)
    # XXX done
    observations = observations / ( P_nought_vec - P_prime )
    # This corrects observations for polarization assuming 100% polarization on
    # one axis (thus the F_prime = -1.0 rather than the perpendicular axis, 1.0)
    # Polarization model as described by Kahn, Fourme, Gadet, Janin, Dumas & Andre
    # (1982) J. Appl. Cryst. 15, 330-337, equations 13 - 15.

    print("Step 3. Correct for polarization.", file=out)
    indexed_cell = observations.unit_cell()

    observations_original_index = observations.deep_copy()
    if result.get("model_partialities",None) is not None and result["model_partialities"][0] is not None:
      # some recordkeeping useful for simulations
      partialities_original_index = observations.customized_copy(
        crystal_symmetry=self.miller_set.crystal_symmetry(),
        data = result["model_partialities"][0]["data"],
        sigmas = flex.double(result["model_partialities"][0]["data"].size()), #dummy value for sigmas
        indices = result["model_partialities"][0]["indices"],
        ).resolution_filter(d_min=self.params.d_min)

    assert len(observations_original_index.indices()) == len(observations.indices())

    # Now manipulate the data to conform to unit cell, asu, and space group
    # of reference.  The resolution will be cut later.
    # Only works if there is NOT an indexing ambiguity!
    if indices_to_edge is not None:
      predictions = self.miller_set.customized_copy(
        anomalous_flag=not self.params.merge_anomalous,
        crystal_symmetry=self.miller_set.crystal_symmetry(),
        indices=indices_to_edge
        ).map_to_asu()
    else:
      predictions = None

    observations = observations.customized_copy(
      anomalous_flag=not self.params.merge_anomalous,
      crystal_symmetry=self.miller_set.crystal_symmetry()
      ).map_to_asu()

    observations_original_index = observations_original_index.customized_copy(
      anomalous_flag=not self.params.merge_anomalous,
      crystal_symmetry=self.miller_set.crystal_symmetry()
      )
    print("Step 4. Filter on global resolution and map to asu", file=out)
    print("Data in reference setting:", file=out)
    #observations.show_summary(f=out, prefix="  ")
    show_observations(observations, out=out)

    if self.params.significance_filter.apply is True: #------------------------------------
      print("Step 5. Frame by frame resolution filter", file=out)
      # Apply an I/sigma filter ... accept resolution bins only if they
      #   have significant signal; tends to screen out higher resolution observations
      #   if the integration model doesn't quite fit
      N_obs_pre_filter = observations.size()
      N_bins_small_set = N_obs_pre_filter // self.params.significance_filter.min_ct
      N_bins_large_set = N_obs_pre_filter // self.params.significance_filter.max_ct

      # Ensure there is at least one bin.
      N_bins = max(
        [min([self.params.significance_filter.n_bins,N_bins_small_set]),
         N_bins_large_set, 1]
      )
      print("Total obs %d Choose n bins = %d"%(N_obs_pre_filter,N_bins), file=out)
      if indices_to_edge is not None:
        print("Total preds %d to edge of detector"%indices_to_edge.size(), file=out)
      bin_results = show_observations(observations, out=out, n_bins=N_bins)

      if result.get("fuller_kapton_absorption_correction", None) is not None:
        fkac = result["fuller_kapton_absorption_correction"][0]
        #assert that we have absorption corrections for all observations
        if len(fkac)!=N_obs_pre_filter: raise Sorry(
           "Fuller corrections %d don't match obs %d"%(len(fkac),N_obs_pre_filter))
        unobstructed = (fkac==1.0)

        xypred = result["mapped_predictions"][0]
        if len(xypred)!=N_obs_pre_filter: raise Sorry(
           "Mapped predictions %d don't match obs %d"%(len(xypred),N_obs_pre_filter))

        print("unobstructed",unobstructed.count(True), "obstructed", unobstructed.count(False), file=out)

        if False:
          from matplotlib import pyplot as plt
          xy_unobstructed = xypred.select(unobstructed)
          xy_obstructed = xypred.select(~unobstructed)
          plt.plot([a[0] for a in xy_unobstructed], [a[1] for a in xy_unobstructed], "r.")
          plt.plot([a[0] for a in xy_obstructed], [a[1] for a in xy_obstructed], "b.")
          #plt.plot([a[0] for a in xypred], [a[1] for a in xypred], "r.")
          plt.axes().set_aspect("equal")
          plt.show()

        from xfel.merging.absorption import show_observations as aso
        try:
          ASO = aso(observations, unobstructed, self.params, out=out, n_bins=N_bins)
        except Exception as e:
          # in development encountered:
          # RuntimeError, flex.mean() of empty array
          # ValueError, max() arg is empty sequence
          print("skipping image: could not process obstructed/unobstructed bins", file=out)
          return null_data(
            file_name=file_name, log_out=out.getvalue(), low_signal=True)
        observations = observations.select(ASO.master_selection)
        observations_original_index = observations_original_index.select(ASO.master_selection)

        if False:
          from matplotlib import pyplot as plt
          unobstructed_subset = (fkac.select(ASO.master_selection)==1.0)
          xy_unobstructed = xypred.select(ASO.master_selection).select(unobstructed_subset)
          xy_obstructed = xypred.select(ASO.master_selection).select(~unobstructed_subset)
          plt.plot([a[0] for a in xy_unobstructed], [a[1] for a in xy_unobstructed], "r.")
          plt.plot([a[0] for a in xy_obstructed], [a[1] for a in xy_obstructed], "b.")
          #plt.plot([a[0] for a in xypred], [a[1] for a in xypred], "r.")
          plt.axes().set_aspect("equal")
          plt.show()

      else:
        acceptable_resolution_bins = [
          bin.mean_I_sigI > self.params.significance_filter.sigma for bin in bin_results]
        acceptable_nested_bin_sequences = [i for i in range(len(acceptable_resolution_bins))
                                           if False not in acceptable_resolution_bins[:i+1]]
        if len(acceptable_nested_bin_sequences)==0:
          return null_data(
            file_name=file_name, log_out=out.getvalue(), low_signal=True)
        else:
          N_acceptable_bins = max(acceptable_nested_bin_sequences) + 1
          imposed_res_filter = float(bin_results[N_acceptable_bins-1].d_range.split()[2])
          imposed_res_sel = observations.resolution_filter_selection(
            d_min=imposed_res_filter)
          observations = observations.select(
            imposed_res_sel)
          observations_original_index = observations_original_index.select(
            imposed_res_sel)
          print("New resolution filter at %7.2f"%imposed_res_filter,file_name, file=out)
        print("N acceptable bins",N_acceptable_bins, file=out)
      print("Old n_obs: %d, new n_obs: %d"%(N_obs_pre_filter,observations.size()), file=out)
      if indices_to_edge is not None:
        print("Total preds %d to edge of detector"%indices_to_edge.size(), file=out)
      # Finished applying the binwise I/sigma filter---------------------------------------
    if self.params.raw_data.sdfac_auto is True:
      I_over_sig = observations.data()/observations.sigmas()
      #assert that at least a few I/sigmas are less than zero
      Nlt0 = I_over_sig.select(I_over_sig<0.).size()
      if Nlt0 > 2:
        # get a rough estimate for the SDFAC, assuming that negative measurements
        # represent false predictions and therefore normally distributed noise.
        no_signal = I_over_sig.select(I_over_sig<0.)
        for xns in range(len(no_signal)):
          no_signal.append(-no_signal[xns])
        Stats = flex.mean_and_variance(no_signal)
        SDFAC = Stats.unweighted_sample_standard_deviation()
      else: SDFAC=1.
      print("The applied SDFAC is %7.4f"%SDFAC, file=out)
      corrected_sigmas = observations.sigmas() * SDFAC
      observations = observations.customized_copy(sigmas = corrected_sigmas)
      observations_original_index = observations_original_index.customized_copy(
        sigmas = observations_original_index.sigmas() * SDFAC)

    print("Step 6.  Match to reference intensities, filter by correlation, filter out negative intensities.", file=out)
    assert len(observations_original_index.indices()) \
      ==   len(observations.indices())

    data = frame_data(self.n_refl, file_name)
    data.set_indexed_cell(indexed_cell)
    data.current_orientation = result['current_orientation'][0]
    data.d_min = observations.d_min()

    # Ensure that match_multi_indices() will return identical results
    # when a frame's observations are matched against the
    # pre-generated Miller set, self.miller_set, and the reference
    # data set, self.i_model.  The implication is that the same match
    # can be used to map Miller indices to array indices for intensity
    # accumulation, and for determination of the correlation
    # coefficient in the presence of a scaling reference.
    if self.i_model is not None:
      assert len(self.i_model.indices()) == len(self.miller_set.indices()) \
        and  (self.i_model.indices() ==
              self.miller_set.indices()).count(False) == 0

    matches = miller.match_multi_indices(
      miller_indices_unique=self.miller_set.indices(),
      miller_indices=observations.indices())

    if predictions is not None:
      matches_predictions = miller.match_multi_indices(
        miller_indices_unique=self.miller_set.indices(),
        miller_indices=predictions.indices())
    else:
      matches_predictions = None

    # matches_preds_obs = miller.match_multi_indices(
    #   miller_indices_unique=indices_to_edge,
    #   miller_indices=observations_original_index.indices())

    use_weights = False # New facility for getting variance-weighted correlation
    if self.params.scaling.algorithm in ['mark1','levmar']:
      # Because no correlation is computed, the correlation
      # coefficient is fixed at zero.  Setting slope = 1 means
      # intensities are added without applying a scale factor.
      sum_x = 0
      sum_y = 0
      for pair in matches.pairs():
        data.n_obs += 1
        if not self.params.include_negatives and observations.data()[pair[1]] <= 0:
          data.n_rejected += 1
        else:
          sum_y += observations.data()[pair[1]]
      N = data.n_obs - data.n_rejected

      slope = 1
      offset = 0
      corr = 0

    else:
      sum_xx = 0
      sum_xy = 0
      sum_yy = 0
      sum_x = 0
      sum_y = 0
      sum_w = 0.

      for pair in matches.pairs():
        if self.params.scaling.simulation is not None:
          observations.data()[pair[1]] = self.i_model.data()[pair[0]]     # SIM
          observations.sigmas()[pair[1]] = self.i_model.sigmas()[pair[0]] # SIM

        data.n_obs += 1
        if not self.params.include_negatives and observations.data()[pair[1]] <= 0:
          data.n_rejected += 1
          continue
        # Update statistics using reference intensities (I_r), and
        # observed intensities (I_o).
        if use_weights: I_w = 1./(observations.sigmas()[pair[1]])**2 #variance weighting
        else: I_w = 1.

        # skip the structure factor if i_model.sigma is invalid (intentionally < 0)
        if self.i_model.sigmas()[pair[0]] < 0:
          I_w = 0.

        I_r = self.i_model.data()[pair[0]]
        I_o = observations.data()[pair[1]]
        sum_xx += I_w * I_r**2
        sum_yy += I_w * I_o**2
        sum_xy += I_w * I_r * I_o
        sum_x += I_w * I_r
        sum_y += I_w * I_o
        sum_w += I_w
      # Linearly fit I_r to I_o, i.e. find slope and offset such that
      # I_o = slope * I_r + offset, optimal in a least-squares sense.
      # XXX This is backwards, really.
      N = data.n_obs - data.n_rejected
      DELTA = sum_w * sum_xx - sum_x**2 # see p. 105 in Bevington & Robinson
      if (DELTA) == 0:
        print("Skipping frame with",sum_w,sum_xx,sum_x**2, file=out)
        return null_data(file_name=file_name,
                         log_out=out.getvalue(),
                         low_signal=True)
      slope = (sum_w * sum_xy - sum_x * sum_y) / DELTA
      offset = (sum_xx * sum_y - sum_x * sum_xy) / DELTA
      corr = (sum_w * sum_xy - sum_x * sum_y) / (math.sqrt(sum_w * sum_xx - sum_x**2) *
                                                 math.sqrt(sum_w * sum_yy - sum_y**2))

    # Early return if there are no positive reflections on the frame.
    if data.n_obs <= data.n_rejected:
      return null_data(
        file_name=file_name, log_out=out.getvalue(), low_signal=True)

    # Update the count for each matched reflection.  This counts
    # reflections with non-positive intensities, too.
    data.completeness += matches.number_of_matches(0).as_int()
    if matches_predictions is not None:
      data.completeness_predictions += matches_predictions.number_of_matches(0).as_int()
    # print "updated observations count by %d and preds count by %d" % \
    #   (flex.sum(matches.number_of_matches(0).as_int()), flex.sum(matches_predictions.number_of_matches(0).as_int()))
    # print "preds matching indices:", len(matches_predictions.pairs())
    # print "preds total:", len(indices_to_edge)
    data.corr = corr
    data.wavelength = wavelength

    # Apply the correlation coefficient threshold, if appropriate.
    if self.params.scaling.algorithm == 'mark0' and \
       corr <= self.params.min_corr:
      print("Skipping these data - correlation too low.", file=out)
      data.set_log_out(out.getvalue())
      data.show_log_out(sys.stdout)
      return null_data(file_name=file_name, log_out=out.getvalue(), low_correlation=True)
    # Apply a resolution filter, if appropriate.
    if self.params.lattice_rejection.d_min and \
      observations.d_min() >= self.params.lattice_rejection.d_min:
      print("Skipping these data - diffraction worse than %.2f Angstrom" % self.params.lattice_rejection.d_min, file=out)
      data.set_log_out(out.getvalue())
      data.show_log_out(sys.stdout)
      return null_data(file_name=file_name, log_out=out.getvalue(), low_resolution=True)

    from xfel.cxi.postrefinement_factory import factory
    PF = factory(self.params)
    postrefinement_algorithm = PF.postrefinement_algorithm()

    if self.params.postrefinement.enable:
      # Refactorization of the Sauter(2015) code; result should be same to 5 significant figures.
      # Lack of binary identity is due to the use of Python for old-code weighted correlation,
      #   contrasted with flex.double arithmetic for new-code.
      postx=postrefinement_algorithm(observations_original_index, self.params,
           self.i_model, self.miller_set, result, out)
      try:
        postx.run_plain()
        observations_original_index,observations,matches = postx.result_for_cxi_merge(file_name)
      except (AssertionError,ValueError,RuntimeError) as e:
        return null_data(file_name=file_name, log_out=out.getvalue(), reason=e)

      self.postrefinement_params = postx.parameterization_class(postx.MINI.x)

      if self.params.postrefinement.show_trumpet_plot is True:
        from xfel.cxi.trumpet_plot import trumpet_wrapper
        trumpet_wrapper(result, postx, file_name, self.params, out)

    if not self.params.scaling.enable or self.params.postrefinement.enable: # Do not scale anything
      print("Scale factor to an isomorphous reference PDB will NOT be applied.", file=out)
      slope = 1.0
      offset = 0.0

    if db_mgr is None: return unpack(MINI.x) # special exit for two-color indexing

    frame_id_0_base = PF.insert_frame_call(locals())

    observations_original_index_indices = observations_original_index.indices()
    xypred = result["mapped_predictions"][0]
    indices = flex.size_t([pair[1] for pair in matches.pairs()])

    sel_observations = flex.intersection(
      size=observations.data().size(),
      iselections=[indices])

    if self.params.include_negatives_fix_27May2018:
      # Super-rare exception. If saved sigmas instead of I/sigmas in the ISIGI dict, this wouldn't be needed.
      sel_observations &= observations.data() != 0

    set_original_hkl = observations_original_index_indices.select(
      flex.intersection(
        size=observations_original_index_indices.size(),
        iselections=[indices]))
    set_xypred = xypred.select(
      flex.intersection(
        size=xypred.size(),
        iselections=[indices]))

    kwargs = {'hkl_id_0_base': [pair[0] for pair in matches.pairs()],
              'i': observations.data().select(sel_observations),
              'sigi': observations.sigmas().select(sel_observations),
              'detector_x': [xy[0] for xy in set_xypred],
              'detector_y': [xy[1] for xy in set_xypred],
              'frame_id_0_base': [frame_id_0_base] * len(matches.pairs()),
              'overload_flag': [0] * len(matches.pairs()),
              'original_h': [hkl[0] for hkl in set_original_hkl],
              'original_k': [hkl[1] for hkl in set_original_hkl],
              'original_l': [hkl[2] for hkl in set_original_hkl]}

    db_mgr.insert_observation(**kwargs)

    print("For %d reflections, got slope %f, correlation %f" % \
        (data.n_obs - data.n_rejected, slope, corr), file=out)
    print("average obs", sum_y / (data.n_obs - data.n_rejected), \
      "average calc", sum_x / (data.n_obs - data.n_rejected), file=out)
    print("Rejected %d reflections with negative intensities" % \
        data.n_rejected, file=out)

    data.extra_stuff = {}


    data.accept = True
    if self.params.postrefinement.enable and self.params.postrefinement.algorithm in ["rs_hybrid"]:
      assert slope == 1.0
      assert self.params.include_negatives
    for pair in matches.pairs():
      if not self.params.include_negatives and (observations.data()[pair[1]] <= 0) :
        continue
      Intensity = observations.data()[pair[1]] / slope
      # Super-rare exception. If saved sigmas instead of I/sigmas in the ISIGI dict, this wouldn't be needed.
      if Intensity == 0:
        continue

      # Add the reflection as a two-tuple of intensity and I/sig(I)
      # to the dictionary of observations.
      index = self.miller_set.indices()[pair[0]]
      isigi = (Intensity,
               observations.data()[pair[1]] / observations.sigmas()[pair[1]],
               slope)
      if index in data.ISIGI:
        data.ISIGI[index].append(isigi)
      else:
        data.ISIGI[index] = [isigi]
        data.extra_stuff[index] = flex.double(), flex.miller_index()

      data.extra_stuff[index][0].append(observations_original_index.data()[pair[1]])
      data.extra_stuff[index][1].append(observations_original_index.indices()[pair[1]])

      sigma = observations.sigmas()[pair[1]] / slope
      variance = sigma * sigma
      data.summed_N[pair[0]] += 1
      data.summed_wt_I[pair[0]] += Intensity / variance
      data.summed_weight[pair[0]] += 1 / variance
    print("Selected file %s to %5.2f Angstrom resolution limit" % (file_name, observations.d_min()), file=out)
    data.set_log_out(out.getvalue())
    data.show_log_out(sys.stdout)
    return data

  def sum_intensities(self):
    sum_I = flex.double(self.miller_set.size(), 0.)
    sum_I_SIGI = flex.double(self.miller_set.size(), 0.)
    for i in range(self.miller_set.size()) :
      index = self.miller_set.indices()[i]
      if index in self.ISIGI :
        for t in self.ISIGI[index]:
          sum_I[i] += t[0]
          sum_I_SIGI[i] += t[1]
    return sum_I, sum_I_SIGI

def consistent_set_and_model(work_params,i_model=None):
  # Adjust the minimum d-spacing of the generated Miller set to assure
  # that the desired high-resolution limit is included even if the
  # observed unit cell differs slightly from the target.  Use the same
  # expansion formula as used in merging/general_fcalc.py, to assure consistency.
  # If a reference model is present, ensure that Miller indices are ordered
  # identically.
  miller_set = symmetry(
      unit_cell=work_params.target_unit_cell,
      space_group_info=work_params.target_space_group
    ).build_miller_set(
      anomalous_flag=not work_params.merge_anomalous,
      d_max=work_params.d_max,
      d_min=work_params.d_min / math.pow(
        1 + work_params.unit_cell_length_tolerance, 1 / 3))
  miller_set = miller_set.change_basis(
    work_params.model_reindex_op).map_to_asu()

  if i_model is not None:
    # Handle the case where model is anomalous=False but the requested merging is anomalous=True
    if i_model.anomalous_flag() is False and miller_set.anomalous_flag() is True:
      i_model = i_model.generate_bijvoet_mates()
    # manage the sizes of arrays.  General_fcalc assures that
    # N(i_model) >= N(miller_set) since it fills non-matches with invalid structure factors
    # However, if N(i_model) > N(miller_set) it's because this run of cxi.merge requested
    # a smaller resolution range.  Must prune off the reference model.

    if i_model.indices().size() > miller_set.indices().size():
      matches = miller.match_indices(i_model.indices(), miller_set.indices())
      pairs = matches.pairs()
      i_model = i_model.select(pairs.column(0))

    matches = miller.match_indices(i_model.indices(), miller_set.indices())
    assert not matches.have_singles()
    miller_set = miller_set.select(matches.permutation())

  return miller_set, i_model
#-----------------------------------------------------------------------
def run(args):
  if ("--help" in args) :
    iotbx.phil.parse(master_phil).show(attributes_level=2)
    return
  processor = iotbx.phil.process_command_line(args=args, master_string=master_phil)
  if len(processor.remaining_args) > 0:
    print("The following arguments were not recognized:")
    print("\n".join(processor.remaining_args))
    iotbx.phil.parse(master_phil).show(attributes_level=2)
    return
  phil = processor.show()
  work_params = phil.work.extract()
  from xfel.merging.phil_validation import application
  application(work_params)

  if ((work_params.d_min is None) or
      (work_params.data is None) or
      ( (work_params.model is None) and work_params.scaling.algorithm != "mark1") ) :
    command_name = os.environ["LIBTBX_DISPATCHER_NAME"]
    raise Usage(command_name + " "
                "d_min=4.0 "
                "data=~/scratch/r0220/006/strong/ "
                "model=3bz1_3bz2_core.pdb")
  if ((work_params.rescale_with_average_cell) and
      (not work_params.set_average_unit_cell)) :
    raise Usage("If rescale_with_average_cell=True, you must also specify "+
      "set_average_unit_cell=True.")
  if [work_params.raw_data.sdfac_auto, work_params.raw_data.sdfac_refine, work_params.raw_data.errors_from_sample_residuals].count(True) > 1:
    raise Usage("Specify only one of sdfac_auto, sdfac_refine or errors_from_sample_residuals.")

  # Read Nat's reference model from an MTZ file.  XXX The observation
  # type is given as F, not I--should they be squared?  Check with Nat!
  log = open("%s.log" % work_params.output.prefix, "w")
  out = multi_out()
  out.register("log", log, atexit_send_to=None)
  out.register("stdout", sys.stdout)
  print("I model", file=out)
  if work_params.model is not None:
    from xfel.merging.general_fcalc import run
    i_model = run(work_params)
    work_params.target_unit_cell = i_model.unit_cell()
    work_params.target_space_group = i_model.space_group_info()
    i_model.show_summary()
  else:
    i_model = None

  print("Target unit cell and space group:", file=out)
  print("  ", work_params.target_unit_cell, file=out)
  print("  ", work_params.target_space_group, file=out)

  miller_set, i_model = consistent_set_and_model(work_params,i_model)

  frame_files = get_observations(work_params)
  scaler = scaling_manager(
    miller_set=miller_set,
    i_model=i_model,
    params=work_params,
    log=out)
  scaler.scale_all(frame_files)
  if scaler.n_accepted == 0:
    return None
  scaler.show_unit_cell_histograms()
  if (work_params.rescale_with_average_cell) :
    average_cell_abc = scaler.uc_values.get_average_cell_dimensions()
    average_cell = uctbx.unit_cell(list(average_cell_abc) +
      list(work_params.target_unit_cell.parameters()[3:]))
    work_params.target_unit_cell = average_cell
    print("", file=out)
    print("#" * 80, file=out)
    print("RESCALING WITH NEW TARGET CELL", file=out)
    print("  average cell: %g %g %g %g %g %g" % \
      work_params.target_unit_cell.parameters(), file=out)
    print("", file=out)
    scaler.reset()
    scaler.scale_all(frame_files)
    scaler.show_unit_cell_histograms()
  if False : #(work_params.output.show_plots) :
    try :
      plot_overall_completeness(completeness)
    except Exception as e :
      print("ERROR: can't show plots")
      print("  %s" % str(e))
  print("\n", file=out)

  # Sum the observations of I and I/sig(I) for each reflection.
  sum_I, sum_I_SIGI = scaler.sum_intensities()

  miller_set_avg = miller_set.customized_copy(
    unit_cell=work_params.target_unit_cell)
  table1 = show_overall_observations(
    obs=miller_set_avg,
    redundancy=scaler.completeness,
    redundancy_to_edge=scaler.completeness_predictions,
    summed_wt_I=scaler.summed_wt_I,
    summed_weight=scaler.summed_weight,
    ISIGI=scaler.ISIGI,
    n_bins=work_params.output.n_bins,
    title="Statistics for all reflections",
    out=out,
    work_params=work_params)
  print("", file=out)
  if work_params.model is not None:
    n_refl, corr = scaler.get_overall_correlation(sum_I)
  else:
    n_refl, corr = ((scaler.completeness > 0).count(True), 0)
  print("\n", file=out)
  table2 = show_overall_observations(
    obs=miller_set_avg,
    redundancy=scaler.summed_N,
    redundancy_to_edge=scaler.completeness_predictions,
    summed_wt_I=scaler.summed_wt_I,
    summed_weight=scaler.summed_weight,
    ISIGI=scaler.ISIGI,
    n_bins=work_params.output.n_bins,
    title="Statistics for reflections where I > 0",
    out=out,
    work_params=work_params)
  #from libtbx import easy_pickle
  #easy_pickle.dump(file_name="stats.pickle", obj=stats)
  #stats.report(plot=work_params.plot)
  #miller_counts = miller_set_p1.array(data=stats.counts.as_double()).select(
  #  stats.counts != 0)
  #miller_counts.as_mtz_dataset(column_root_label="NOBS").mtz_object().write(
  #  file_name="nobs.mtz")
  if work_params.data_subsubsets.subsubset is not None and work_params.data_subsubsets.subsubset_total is not None:
    easy_pickle.dump("scaler_%d.pickle"%work_params.data_subsubsets.subsubset, scaler)
  explanation = """
Explanation:
Completeness       = # unique Miller indices present in data / # Miller indices theoretical in asymmetric unit
Asu. Multiplicity  = # measurements / # Miller indices theoretical in asymmetric unit
Obs. Multiplicity  = # measurements / # unique Miller indices present in data
Pred. Multiplicity = # predictions on all accepted images / # Miller indices theoretical in asymmetric unit"""
  print(explanation, file=out)
  mtz_file, miller_array = scaler.finalize_and_save_data()
  #table_pickle_file = "%s_graphs.pkl" % work_params.output.prefix
  #easy_pickle.dump(table_pickle_file, [table1, table2])
  loggraph_file = os.path.abspath("%s_graphs.log" % work_params.output.prefix)
  f = open(loggraph_file, "w")
  f.write(table1.format_loggraph())
  f.write("\n")
  f.write(table2.format_loggraph())
  f.close()
  result = scaling_result(
    miller_array=miller_array,
    plots=scaler.get_plot_statistics(),
    mtz_file=mtz_file,
    loggraph_file=loggraph_file,
    obs_table=table1,
    all_obs_table=table2,
    n_reflections=n_refl,
    overall_correlation=corr)
  easy_pickle.dump("%s.pkl" % work_params.output.prefix, result)
  return result

def show_overall_observations(
  obs, redundancy, summed_wt_I, summed_weight, ISIGI,
  n_bins=15, out=None, title=None, work_params=None, redundancy_to_edge=None):
  if out is None:
    out = sys.stdout
  obs.setup_binner(d_max=100000, d_min=work_params.d_min, n_bins=n_bins)
  result = []

  cumulative_unique = 0
  cumulative_meas   = 0
  cumulative_n_pred = 0
  cumulative_pred   = 0
  cumulative_theor  = 0
  cumulative_In     = 0
  cumulative_I      = 0.0
  cumulative_Isigma = 0.0

  for i_bin in obs.binner().range_used():
    sel_w = obs.binner().selection(i_bin)
    sel_fo_all = obs.select(sel_w)
    obs.binner()._have_format_strings=True
    obs.binner().fmt_bin_range_used ="%6.3f -%6.3f"
    d_range = obs.binner().bin_legend(
      i_bin=i_bin, show_bin_number=False, show_counts=False)
    sel_redundancy = redundancy.select(sel_w)
    if redundancy_to_edge is not None:
      sel_redundancy_pred = redundancy_to_edge.select(sel_w)
    else:
      sel_redundancy_pred = None
    sel_absent = sel_redundancy.count(0)
    n_present = sel_redundancy.size() - sel_absent
    sel_complete_tag = "[%d/%d]" % (n_present, sel_redundancy.size())
    sel_measurements = flex.sum(sel_redundancy)

    # Alternatively, redundancy (or multiplicity) is calculated as the
    # average number of observations for the observed
    # reflections--missing reflections do not affect the redundancy
    # adversely, and the reported value becomes
    # completeness-independent.
    val_redundancy_obs = 0
    if n_present > 0:
      val_redundancy_obs = flex.sum(sel_redundancy) / n_present
    # Repeat on the full set of predictions, calculated w.r.t. the asymmetric unit
    val_redundancy_pred = 0
    if redundancy_to_edge is not None and n_present > 0:
      val_redundancy_pred = flex.sum(sel_redundancy_pred) / sel_redundancy.size()

    # Per-bin sum of I and I/sig(I).  For any reflection, the weight
    # of the merged intensity must be positive for this to make sense.
    sel_o = (sel_w & (summed_weight > 0))
    intensity = summed_wt_I.select(sel_o) / summed_weight.select(sel_o)
    I_sum = flex.sum(intensity)
    I_sigI_sum = flex.sum(intensity * flex.sqrt(summed_weight.select(sel_o)))
    I_n = sel_o.count(True)

    if work_params is not None and \
     (work_params.plot_single_index_histograms and \
      N >= 30 and \
      work_params.data_subset == 0):
      scaling_manager.single_reflection_histograms(obs, ISIGI)

    if sel_measurements > 0:
      mean_I = mean_I_sigI = 0
      if I_n > 0:
        mean_I = I_sum / I_n
        mean_I_sigI = I_sigI_sum / I_n
      bin = resolution_bin(
        i_bin=i_bin,
        d_range=d_range,
        d_min=obs.binner().bin_d_min(i_bin),
        redundancy_asu=flex.mean(sel_redundancy.as_double()),
        redundancy_obs=val_redundancy_obs,
        redundancy_to_edge=val_redundancy_pred,
        complete_tag=sel_complete_tag,
        completeness=n_present / sel_redundancy.size(),
        measurements=sel_measurements,
        predictions=sel_redundancy_pred,
        mean_I=mean_I,
        mean_I_sigI=mean_I_sigI)
      result.append(bin)
    cumulative_unique += n_present
    cumulative_meas   += sel_measurements
    if redundancy_to_edge is not None:
      cumulative_n_pred += flex.sum(sel_redundancy_pred)
      cumulative_pred   += redundancy_to_edge
    cumulative_theor  += sel_redundancy.size()
    cumulative_In     += I_n
    cumulative_I      += I_sum
    cumulative_Isigma += I_sigI_sum

  if (title is not None) :
    print(title, file=out)
  from libtbx import table_utils
  table_header = ["","","","","<asu","<obs","<pred","","","",""]
  table_header2 = ["Bin","Resolution Range","Completeness","%","multi>","multi>","multi>",
    "n_meas", "n_pred","<I>","<I/sig(I)>"]
  use_preds = (redundancy_to_edge is not None and flex.sum(redundancy_to_edge) > 0)
  include_columns = [True, True, True, True, True, True, use_preds, True, use_preds, True, True]
  table_data = []
  table_data.append(table_header)
  table_data.append(table_header2)
  for bin in result:
    table_row = []
    table_row.append("%3d" % bin.i_bin)
    table_row.append("%-13s" % bin.d_range)
    table_row.append("%13s" % bin.complete_tag)
    table_row.append("%5.2f" % (100*bin.completeness))
    table_row.append("%6.2f" % bin.redundancy_asu)
    table_row.append("%6.2f" % bin.redundancy_obs)
    table_row.append("%6.2f" % (0 if redundancy_to_edge is None else bin.redundancy_to_edge))
    table_row.append("%6d" % bin.measurements)
    table_row.append("%6d" % (0 if redundancy_to_edge is None else flex.sum(bin.predictions)))
    table_row.append("%8.0f" % bin.mean_I)
    table_row.append("%8.3f" % bin.mean_I_sigI)
    table_data.append(table_row)
  if len(table_data) <= 2:
    print("Table could not be constructed -- no bins accepted.", file=out)
    return
  table_data.append([""]*len(table_header))
  table_data.append(  [
      format_value("%3s",   "All"),
      format_value("%-13s", "                 "),
      format_value("%13s",  "[%d/%d]"%(cumulative_unique,cumulative_theor)),
      format_value("%5.2f", 100*(cumulative_unique/cumulative_theor)),
      format_value("%6.2f", cumulative_meas/cumulative_theor),
      format_value("%6.2f", cumulative_meas/cumulative_unique),
      format_value("%6.2f", (0 if redundancy_to_edge is None else cumulative_n_pred/cumulative_theor)),
      format_value("%6d",   cumulative_meas),
      format_value("%6d",   (0 if redundancy_to_edge is None else flex.sum(redundancy_to_edge))),
      format_value("%8.0f", cumulative_I/cumulative_In),
      format_value("%8.3f", cumulative_Isigma/cumulative_In),
  ])
  table_data = table_utils.manage_columns(table_data, include_columns)

  print()
  print(table_utils.format(table_data,has_header=2,justify='center',delim=" "), file=out)

  # XXX generate table object for displaying plots
  if (title is None) :
    title = "Data statistics by resolution"
  table = data_plots.table_data(
    title=title,
    x_is_inverse_d_min=True,
    force_exact_x_labels=True)
  table.add_column(
    column=[1 / bin.d_min**2 for bin in result],
    column_name="d_min",
    column_label="Resolution")
  table.add_column(
    column=[bin.redundancy_asu for bin in result],
    column_name="redundancy",
    column_label="Redundancy")
  table.add_column(
    column=[bin.completeness for bin in result],
    column_name="completeness",
    column_label="Completeness")
  table.add_column(
    column=[bin.mean_I_sigI for bin in result],
    column_name="mean_i_over_sigI",
    column_label="<I/sig(I)>")
  table.add_graph(
    name="Redundancy vs. resolution",
    type="GRAPH",
    columns=[0,1])
  table.add_graph(
    name="Completeness vs. resolution",
    type="GRAPH",
    columns=[0,2])
  table.add_graph(
    name="<I/sig(I)> vs. resolution",
    type="GRAPH",
    columns=[0,3])
  return table

class resolution_bin(object):
  def __init__(self,
               i_bin=None,
               d_range=None,
               d_min=None,
               redundancy_asu=None,
               redundancy_obs=None,
               redundancy_to_edge=None,
               absent=None,
               complete_tag=None,
               completeness=None,
               measurements=None,
               predictions=None,
               mean_I=None,
               mean_I_sigI=None,
               sigmaa=None):
    adopt_init_args(self, locals())

class scaling_result (group_args) :
  """
  Container for any objects that might need to be saved for future use (e.g.
  in a GUI).  Must be pickle-able!
  """
  pass

#-----------------------------------------------------------------------
# graphical goodies
def plot_overall_completeness(completeness):
  completeness_range = range(-1,flex.max(completeness)+1)
  completeness_counts = [completeness.count(n) for n in completeness_range]
  from matplotlib import pyplot as plt
  plt.plot(completeness_range,completeness_counts,"r+")
  plt.show()

class plot_statistics (object) :
  """
  Container for assorted histograms of frame statistics.  The resolution bin
  plots are stored separately, since they can be displayed using the loggraph
  viewer.
  """
  def __init__ (self,
                prefix,
                unit_cell_statistics,
                reference_cell,
                correlations,
                min_corr,
                rejected_fractions,
                frame_d_min) :
    adopt_init_args(self, locals())

  def show_all_pyplot (self, n_slots=20) :
    """
    Display histograms using pyplot.  For use in a wxPython GUI the figure
    should be created separately in a wx.Frame.
    """
    from matplotlib import pyplot as plt
    fig = plt.figure(figsize=(9,12))
    self.plot_unit_cell_histograms(
      figure=fig,
      a_values=self.unit_cell_statistics.all_uc_a_values,
      b_values=self.unit_cell_statistics.all_uc_b_values,
      c_values=self.unit_cell_statistics.all_uc_c_values,
      n_slots=n_slots,
      title=\
        "Unit cell length distribution (all frames with compatible indexing): %s" % self.prefix)
    plt.show()
    fig = plt.figure(figsize=(9,12))
    self.plot_unit_cell_histograms(
      figure=fig,
      a_values=self.unit_cell_statistics.uc_a_values,
      b_values=self.unit_cell_statistics.uc_b_values,
      c_values=self.unit_cell_statistics.uc_c_values,
      n_slots=n_slots,
      title=\
        "Unit cell length distribution (frames with acceptable correlation): %s" % self.prefix)
    plt.show()
    fig = plt.figure(figsize=(9,12))
    self.plot_statistics_histograms(
      figure=fig,
      n_slots=n_slots)
    plt.show()

  def plot_unit_cell_histograms (self,
      figure,
      a_values,
      b_values,
      c_values,
      n_slots=20,
      title="Distribution of unit cell edge lengths") :
    [a0,b0,c0,alpha0,beta0,gamma0] = self.reference_cell.parameters()
    ax1 = figure.add_axes([0.1, 0.1, 0.8, 0.25])
    ax2 = figure.add_axes([0.1, 0.4, 0.8, 0.25])
    ax3 = figure.add_axes([0.1, 0.7, 0.8, 0.25])
    ax1.hist(c_values, n_slots, color=[1.0,0.0,0.0])
    ax2.hist(b_values, n_slots, color=[0.0,1.0,0.0])
    ax3.hist(a_values, n_slots, color=[0.0,0.5,1.0])
    ax1.axvline(c0, linestyle='.', linewidth=2, color='k')
    ax2.axvline(b0, linestyle='.', linewidth=2, color='k')
    ax3.axvline(a0, linestyle='.', linewidth=2, color='k')
    ax1.set_xlabel("c edge")
    ax2.set_xlabel("b edge")
    ax3.set_xlabel("a edge")
    ax3.set_title("%s: %s" % (title, self.prefix))

  def plot_statistics_histograms (self,
      figure,
      n_slots=20) :
    ax1 = figure.add_axes([0.1, 0.1, 0.8, 0.25])
    ax2 = figure.add_axes([0.1, 0.4, 0.8, 0.25])
    ax3 = figure.add_axes([0.1, 0.7, 0.8, 0.25])
    ax1.hist(self.correlations, n_slots, color=[1.0,0.0,0.0])
    ax2.hist(self.rejected_fractions, n_slots, color=[0.0,1.0,0.0])
    ax3.hist(self.frame_d_min, n_slots, color=[0.0,0.5,1.0])
    ax1.axvline(self.min_corr, linestyle='.', linewidth=2, color='k')
    ax1.set_xlabel("Correlation to reference dataset")
    ax2.set_xlabel("Fraction of rejected zero or negative intensities")
    ax3.set_xlabel("Integrated resolution limit")
    ax1.set_title("Correlation by frame (%s)" % self.prefix)
    ax2.set_title("Rejected reflections by frame (%s)" % self.prefix)
    ax3.set_title("Resolution by frame (%s)" % self.prefix)

if (__name__ == "__main__"):
  show_plots = False
  if ("--plots" in sys.argv) :
    sys.argv.remove("--plots")
    show_plots = True
  result = run(args=sys.argv[1:])
  if result is None:
    sys.exit(1)
  if (show_plots) :
    try :
      result.plots.show_all_pyplot()
      from wxtbx.command_line import loggraph
      loggraph.run([result.loggraph_file])
    except Exception as e :
      print("Can't display plots")
      print("You should be able to view them by running this command:")
      print("  wxtbx.loggraph %s" % result.loggraph_file)
      raise e


 *******************************************************************************
