

 *******************************************************************************
xfel/cxi/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/cxi/average_spots.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# The average_spots jiffy calculates average pickled images of either
# raw, pickled CsPad shots, or spotfinder results on raw, pickled
# CsPad shots.
#
# $Id$

import numpy, os, sys

from libtbx import easy_pickle
from iotbx               import detectors
from iotbx.detectors.npy import NpyImage


def ImageFactory(path):
  if os.path.isfile(path):
    I = NpyImage(path)
    I.readHeader()
    return (I)

detectors.ImageFactory = ImageFactory


from spotfinder.diffraction.imagefiles import FileName
FileName.exts.append("pickle")


# The img_add() function reads the image, distance, and beam energy
# from the pickled file whose name is the string pointed to by path,
# and adds them to img_sum, dist_sum, and nrg_sum.
def img_add(path, img_sum, dist_sum, nrg_sum, nmemb):
  img_cspad = easy_pickle.load(path)

  if (img_sum is None):
    img_sum    = img_cspad["image"].astype(numpy.uint32)
    dist_sum   = img_cspad["distance"]
    nrg_sum    = img_cspad["beamEnrg"]
    nmemb      = 1
  else:
    img_sum   += img_cspad["image"].astype(numpy.uint32)
    dist_sum  += img_cspad["distance"]
    nrg_sum   += img_cspad["beamEnrg"]
    nmemb     += 1

  return (img_sum, dist_sum, nrg_sum, nmemb)


# The spot_add() function reads the image, distance, and beam energy
# from the pickled file whose name is the string pointed to by path.
# The spots found in the image are added to spot_sum, while the
# distance and energy are added to dist_sum and nrg_sum respectively.
def spot_add(path, spot_sum, dist_sum, nrg_sum, nmemb):
  limits="""1479, 1515, 1672, 1699
            1281, 1515, 1474, 1699
            1092, 1506, 1249, 1699
            1065, 1506, 1090, 1699
             853, 1505, 1037, 1698
            1650, 1081, 1672, 1487
            1479, 1303, 1604, 1487
            1281, 1303, 1474, 1487
            1466, 1082, 1650, 1274
            1065, 1308, 1249, 1501
            1253, 1080, 1437, 1273
             853, 1307, 1037, 1500
             622, 1465,  815, 1649
             424, 1465,  617, 1649
             213, 1473,  397, 1666
            1048, 1098, 1241, 1282
             850, 1098, 1043, 1282
             623, 1252,  816, 1436
             425, 1252,  618, 1436
             213, 1275,  397, 1468
            1466,  883, 1650, 1076
            1506,  664, 1699,  848
            1253,  882, 1437, 1075
            1048,  885, 1241, 1069
            1308,  664, 1501,  848
            1099,  656, 1283,  849
             850,  885, 1043, 1069
             634, 1048,  818, 1241
             421, 1048,  605, 1241
             198, 1064,  391, 1248
            1506,  451, 1699,  635
            1308,  451, 1501,  635
            1099,  458, 1283,  651
            1514,  231, 1698,  424
            1085,  262, 1278,  446
            1514,   33, 1698,  226
             885,  656, 1069,  849
             885,  458, 1069,  651
             887,  262, 1080,  446
             634,  850,  818, 1043
             421,  850,  605, 1043
             656,  626,  849,  810
             656,  414,  849,  598
             664,  198,  848,  391
             664,    0,  848,  193
             458,  626,  651,  810
             198,  851,  391, 1035
             458,  414,  651,  598
             451,  198,  635,  391
             451,    0,  635,  193
               1, 1473,  185, 1666
               1, 1275,  185, 1468
               0, 1064,  193, 1248
               0,  851,  193, 1035
             263,  617,  447,  810
              50,  616,  234,  809
             263,  419,  447,  612
              50,  418,  234,  611
             231,  213,  424,  397
              33,  213,  226,  397"""

  args = [path,
          "distl.bins.verbose=True",
          "distl.minimum_spot_area=3",
          "distl.detector_tiling=%s" % limits.replace(" ", "").replace("\n", ","), # XXX should not need to reproduce the tiling on every single use
          "distl.peripheral_margin=1",
          "distl.res.outer=2.1"
          ]
  from spotfinder.command_line.signal_strength import run
  try:
    info = run(args)
  except e:
    return (spot_sum, dist_sum, nrg_sum, nmemb)

  img_cspad = easy_pickle.load(path)

  if (spot_sum is None):
    spot_sum   = numpy.zeros((img_cspad["image"].shape[0],
                              img_cspad["image"].shape[1]), dtype="float")
    dist_sum   = img_cspad["distance"]
    nrg_sum    = img_cspad["beamEnrg"]
    nmemb      = 0
  else:
    dist_sum  += img_cspad["distance"]
    nrg_sum   += img_cspad["beamEnrg"]
    nmemb     += 1

  # XXX Note that x and y are flipped in the summation.
  for spot in info.S.images[info.frames[0]]["spots_inlier"]:
    for i in range(len(spot.bodypixels)):
      spot_sum[spot.bodypixels[i].x, spot.bodypixels[i].y] += spot.wts[i]

  return (spot_sum, dist_sum, nrg_sum, nmemb)


# http://www.artima.com/weblogs/viewpost.jsp?thread=4829
def main(argv = None):
  if (argv is None):
    argv = sys.argv

  outpath = "average_prutt_00001.pickle" # XXX Should be argument!

  img_sum  = None
  dist_sum = 0
  nrg_sum  = 0
  nmemb    = 0
  for arg in argv[1:]: # XXX ugly hack!
    if (False): # XXX Should be argument?
      img_sum, dist_sum, nrg_sum, nmemb = img_add(
        arg, img_sum, dist_sum, nrg_sum, nmemb)
    else:
      img_sum, dist_sum, nrg_sum, nmemb = spot_add(
        arg, img_sum, dist_sum, nrg_sum, nmemb)
  if (nmemb == 0):
    return (0)

  # XXX Post-mortem--avoid overflows!  But breaks distance and energy!
  #nmemb = 1.0 * img_sum.max() / (2**14 - 16)

  easy_pickle.dump(outpath,
                   dict(beamEnrg = 1.0 / nmemb * nrg_sum,
                        distance = 1.0 / nmemb * dist_sum,
                        image    = 1.0 / nmemb * img_sum), # XXX implicit cast?
                   )
  print("Wrote average of %d images to '%s'" % (nmemb, outpath))
  return (0)


# Run with "phenix.python average_spots.py `awk '{print $2;}'
# section01.out`".
if (__name__ == "__main__"):
  sys.exit(main())


 *******************************************************************************


 *******************************************************************************
xfel/cxi/clustering/Cluster.py
from __future__ import absolute_import, division, print_function
import os
import math
from cctbx.uctbx.determine_unit_cell import NCDist
import scipy.cluster.hierarchy as hcluster
import numpy as np
import json
from .SingleFrame import SingleFrame
import logging
from six.moves import range


class Clu_BELIEVE_THIS_WHOLE_DIRECTORY_IS_DEAD_CODE_20171120_ster:
  def __init__(self, data, cname, info, log_level='INFO'):
    """ Contains a list of SingFrame objects, as well as information about these
    as a cluster (e.g. mean unit cell)."""
    self.cname = cname
    self.members = data
    self.info = info

    # Calculate medians and stdevs
    unit_cells = np.zeros([len(self.members), 6])
    self.pg_composition = {}
    for i, member in enumerate(self.members):
      unit_cells[i, :] = member.uc
      # Calculate point group composition
      if member.pg in self.pg_composition:
        self.pg_composition[member.pg] += 1
      else:
        self.pg_composition[member.pg] = 1

    self.medians = np.median(unit_cells, 0).tolist()
    self.stdevs = np.std(unit_cells, 0).tolist()
    #ToDo
    self.res = None


  @classmethod
  def from_directories(cls, path_to_integration_dir,
                       _prefix='cluster_from_file'):
    """Constructor to get a cluster from pickle files, from the recursively
    walked paths. Can take more than one argument for multiple folders.
    usage: Cluster.from_directories(..)"""
    data = []
    for arg in path_to_integration_dir:
      for (dirpath, dirnames, filenames) in os.walk(arg):
        for filename in filenames:
          path = os.path.join(dirpath, filename)
          data.append(SingleFrame(path, filename))
    return cls(data, _prefix,
               'Made from files in {}'.format(path_to_integration_dir[:]))

  @classmethod
  def from_json(cls, json_file, _prefix='cluster_from_json'):
    """ Does not work!! Do not use! """
    with open(json_file, 'rb') as inFile:
      data = json.load(inFile)
    return cls(data, _prefix,
               'Made from {}'.format(json_file))

  def make_sub_cluster(self, new_members, new_prefix, new_info):
    """ Make a sub-cluster from a list of cluster indicies from the old
    SingleFrame array.
    """
    return Cluster(new_members, new_prefix,
                   '{}\n{} Next filter {}\n{}\n{} of {} images passed on to this cluster'.format(
                     self.info, '#' * 30, '#' * 30, new_info, len(new_members), len(self.members)))

  def print_ucs(self):
    outfile = "{}_niggli_ucs".format(self.cname)
    out_str = ["File name, Point group, a, b, c, alpha, beta, gamma"]
    for image in self.members:
      out_str.append("{}, {}, {}, {}, {}, {}, {}, {}".format(
        image.name, image.pg,
        image.uc[0], image.uc[1],
        image.uc[2], image.uc[3],
        image.uc[4], image.uc[5]))
    with open("{}.csv".format(outfile), 'w') as _outfile:
      _outfile.write("\n".join(out_str))

  def point_group_filer(self, point_group):
    """ Return all the SingleFrames that have a given pointgroup. """
    new_prefix = '{}_only'.format(point_group)
    new_info = 'Cluster filtered by for point group {}.'.format(point_group)
    return self.make_sub_cluster([image for image in self.members if image.pg == point_group],
                                 new_prefix,
                                 new_info)

  def total_intensity_filter(self, res='', completeness_threshold=0.95, plot=False):
    """ Creates a sub-cluster using the highest total intensity images that yield a dataset specified by:
          res -- desired resolution. Defaults to that of the dataset.
          completeness -- the desired completeness of the subset
          multiplicity -- the desired multiplicity of the subset
    """
    logging.info(("Performing intensity filtering, aiming for {}% overall completenes"
                  " at {} A resolution").format(completeness_threshold * 100, res))

    # 0. Check that the cluster has consistent point_group (completness doesn't mean much otherwise...
    assert all(i.pg == self.members[0].pg for i in self.members)

    # 1. Sort SingleFrames by total intensity
    sorted_cluster = sorted(self.members, key=lambda y: -1 * y.total_i)

    if plot:
      from matplotlib import pyplot as plt
      plt.plot([x.total_i for x in sorted_cluster])
      plt.show()

    if res == '':
      res = sorted_cluster[0].d_min()  # Use the high-res limit from the brightest image. ToDo: make this better.
      logging.warning("no resolution limit specified, using the res limit of the top-rankeed image: {} A".format(res))

    # 2. Incrementally merge frames until criterion are matched

    temp_miller_indicies = sorted_cluster[0].miller_array
    for idx, image in enumerate([x.miller_array for x in sorted_cluster[1:]]):
      temp_miller_indicies = temp_miller_indicies.concatenate(image, assert_is_similar_symmetry=False)
      current_completeness = temp_miller_indicies.merge_equivalents().array().completeness()
      logging.debug("{} images: {:.2f}% complete".format(idx, current_completeness * 100))
      if current_completeness <= completeness_threshold:
        temp_miller_indicies.concatenate(image, assert_is_similar_symmetry=False)
        if idx + 1 == len(sorted_cluster[1:]):
          logging.warning("Desired completeness could not be achieved, sorry.")
          file_threshold = idx
          break
      else:
        file_threshold = idx
        break

    return self.make_sub_cluster(sorted_cluster[:file_threshold],
                                 'I_threshold_d{}_{}comp'.format(res, completeness_threshold),
                                 ('Subset cluster made using total_intensity_filter() with'
                                  '\nRes={}\ncompleteness_threshold={}').format(res,
                                                                                completeness_threshold))

  def ab_cluster(self, threshold=10000, method='distance', linkage_method='single', log=False, plot=False):
    """ Do basic hierarchical clustering using the Andrews-Berstein distance
    on the Niggli cells """
    print("Hierarchical clustering of unit cells:")
    import scipy.spatial.distance as dist

    print("Using Andrews-Bernstein Distance from Andrews & Bernstein J Appl Cryst 47:346 (2014).")

    def make_g6(uc):
      """ Take a reduced Niggli Cell, and turn it into the G6 representation """
      a = uc[0] ** 2
      b = uc[1] ** 2
      c = uc[2] ** 2
      d = 2 * uc[1] * uc[2] * math.cos(uc[3])
      e = 2 * uc[0] * uc[2] * math.cos(uc[4])
      f = 2 * uc[0] * uc[1] * math.cos(uc[5])
      return [a, b, c, d, e, f]

    # 1. Create a numpy array of G6 cells
    g6_cells = np.array([make_g6(image.uc)
                         for image in self.members])

    # 2. Do hierarchichal clustering, using the find_distance method above.
    pair_distances = dist.pdist(g6_cells,
                                metric=lambda a, b: NCDist(a, b))
    logging.debug("Distances have been calculated")
    this_linkage = hcluster.linkage(pair_distances,
                                    method=linkage_method,
                                    metric=lambda a, b: NCDist(a, b))
    cluster_ids = hcluster.fcluster(this_linkage,
                                    threshold,
                                    criterion=method)
    logging.debug("Clusters have been calculated")
    # Create an array of sub-cluster objects from the clustering
    sub_clusters = []
    for cluster in range(max(cluster_ids)):
      info_string = ('Made using ab_cluster with t={},'
                     ' {} method, and {} linkage').format(threshold,
                                                          method,
                                                          linkage_method)
      sub_clusters.append(self.make_sub_cluster([self.members[i]
                                                 for i in
                                                 range(len(self.members))
                                                 if
                                                 cluster_ids[i] == cluster + 1],
                                                'cluster_{}'.format(
                                                  cluster + 1),
                                                info_string))

    # 3. print out some information that is useful.
    out_str = "{} clusters have been identified.".format(max(cluster_ids))
    out_str += "\n{:^5} {:^14} {:<11} {:<11} {:<11} {:<12} {:<12} {:<12}".format(
      "C_id",
      "Num in cluster",
      "Med_a", "Med_b", "Med_c",
      "Med_alpha", "Med_beta", "Med_gamma")
    singletons = []
    for cluster in sub_clusters:
      if len(cluster.members) != 1:

        sorted_pg_comp = sorted(list(cluster.pg_composition.items()), key=lambda x: -1 * x[1])
        pg_strings = ["{} in {}".format(pg[1], pg[0])
                      for pg in sorted_pg_comp]
        point_group_string = ", ".join(pg_strings) + "."
        out_str += ("\n{:^5} {:^14} {:<5.1f}({:<4.1f}) {:<5.1f}({:<4.1f})"
                    " {:<5.1f}({:<4.1f}) {:<6.2f}({:<4.2f}) {:<6.2f}"
                    "({:<4.2f}) {:<6.2f}({:<4.2f})").format(
          cluster.cname,
          len(cluster.members),
          cluster.medians[0], cluster.stdevs[0],
          cluster.medians[1], cluster.stdevs[1],
          cluster.medians[2], cluster.stdevs[2],
          cluster.medians[3], cluster.stdevs[3],
          cluster.medians[4], cluster.stdevs[4],
          cluster.medians[5], cluster.stdevs[5])
        out_str += "\n" + point_group_string
      else:
        singletons.append("".join([("{:<14} {:<11.1f} {:<11.1f} {:<11.1f}"
                                    "{:<12.1f} {:<12.1f} {:<12.1f}").format(
          list(cluster.pg_composition.keys())[0],
          cluster.members[0].uc[0], cluster.members[0].uc[1],
          cluster.members[0].uc[2], cluster.members[0].uc[3],
          cluster.members[0].uc[4], cluster.members[0].uc[5]),
                                   '\n']))
    out_str += "\nStandard deviations are in brackets."
    out_str += "\n" + str(len(singletons)) + " singletons:"
    out_str += "\n{:^14} {:<11} {:<11} {:<11} {:<12} {:<12} {:<12}".format(
      "Point group",
      "a", "b", "c",
      "alpha", "beta", "gamma")
    out_str += "".join(singletons)
    print(out_str)

    if plot:
      import matplotlib.pyplot as plt

      fig = plt.figure("Distance Dendogram")
      hcluster.dendrogram(this_linkage,
                          labels=[image.name for image in self.members],
                          leaf_font_size=8,
                          color_threshold=threshold)
      ax = fig.gca()
      if log:
        ax.set_yscale("log")
      else:
        ax.set_ylim(-ax.get_ylim()[1] / 100, ax.get_ylim()[1])
      fig.savefig("{}_dendogram.pdf".format(self.cname))
      plt.show()

    return sub_clusters

  def dump_file_list(self, out_file_name=None):
    if out_file_name is None:
      out_file_name = self.cname

    with open("{}.members".format(out_file_name), 'wb') as outfile:
      for i in self.members:
        outfile.write(i.path + "\n")

def cluster_pca(self):
  """ Should use BLEND clustering protocols in python (Foaldi et al. Acta D.
  2013). i.e. filter for parameters that vary, do PCA, then ward linkage
  clustering on this. """
  # Will come back to this soon <-- Oli
  #columns_to_use = [True if np.std(self.niggli_ucs[:, i])
  #                  else False for i in range(6)]


 *******************************************************************************


 *******************************************************************************
xfel/cxi/clustering/SingleFrame.py
from __future__ import absolute_import, division, print_function
from libtbx import easy_pickle
import logging

class SingleFrame:
  """ Class that creates single-image agregate metrics/scoring that can then be
  used in downstream clustering or filtering procedures.
  """
  def __init__(self, path, filename, crystal_num=0):
    try:
      # Warn on error, but continue directory traversal.
      d = easy_pickle.load(path)
      self.miller_array = d['observations'][crystal_num]
      self.path = path
      self.name = filename
      self.pg = d['pointgroup']
      self.uc = d['current_orientation'][crystal_num].unit_cell() \
                                                     .niggli_cell() \
                                                     .parameters()
      self.total_i = d['observations'][crystal_num].sum()
      logging.debug("Extracted image {}".format(filename))
    except KeyError:
      logging.warning("Could not extract point group and unit cell from %s\n" % path)
    except IOError:
      logging.warning("Could not read %s. It may not be a pickle file.\n" % path)


 *******************************************************************************


 *******************************************************************************
xfel/cxi/clustering/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/cxi/clustering/test_cluster.py

from __future__ import absolute_import, division, print_function
from .Cluster import Cluster

import logging
FORMAT = '%(levelname)s %(module)s.%(funcName)s: %(message)s'
logging.basicConfig(level=logging.DEBUG, format=FORMAT)
""" A test cluster script for playing with the class """
path_to_some_integration_pickles = r"/usr/local/Dropbox/Stanford_Postdoc/CODING/Fraser_test_data"

test_cluster = Cluster.from_directories([path_to_some_integration_pickles],
                                        'test_script')
logging.info("data imported")
clust = test_cluster.point_group_filer('P222')
clust = clust.ab_cluster(80, plot=False)
big_cluster = max(clust, key=lambda x: len(x.members))

best_data = big_cluster.total_intensity_filter(res=8, completeness_threshold=0.5, plot=False)

print(best_data.info)
best_data.dump_file_list()


 *******************************************************************************


 *******************************************************************************
xfel/cxi/command_line/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/cxi/command_line/einsle.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# LIBTBX_SET_DISPATCHER_NAME LM14.einsle
import sys,os
import iotbx.pdb
import string
from scitbx.array_family import flex
import mmtbx.programs.fmodel
import mmtbx.f_model

"""Fit the anomalous scattering parameters f' and f", given the pdb model (for phases) and the
   intensities with Friedel mates separated, as in Oliver Einsle, Susana L. A. Andrade, Holger Dobbek,
   Jacques Meyer, and Douglas C. Rees (2007). Assignment of Individual Metal Redox States in a
   Metalloprotein by Crystallographic Refinement at Multiple X-ray Wavelengths. Journal of the
   American Chemical Society 129, 2210-2211."""

class Model:
  def __init__(self,file_name, d_min, algorithm = "direct", use_solvent=False, plot=True):
    """Can we compare Fmodel calculated from Phenix GUI, vs. from a script?
       Answer: they are in perfect agreement.
       if False: script calculated f_calc without solvent
       if True:  script calculated f_model with solvent. OK"""
    self.d_min = d_min
    self.pdb_inp = iotbx.pdb.input(file_name)
    self.xray_structure = self.pdb_inp.xray_structure_simple()
    self.xray_structure.show_summary()
    phil2 = mmtbx.programs.fmodel.master_phil
    params2 = phil2.extract()
    # adjust the cutoff of the generated intensities to assure that
    # statistics will be reported to the desired high-resolution limit
    # even if the observed unit cell differs slightly from the reference.
    params2.output.type = "complex"
    params2.high_resolution = d_min
    if use_solvent :
      params2.fmodel.k_sol = 0.35
      params2.fmodel.b_sol = 46.
    self.f_model_complex = mmtbx.utils.fmodel_from_xray_structure(
      xray_structure = self.xray_structure,
      f_obs          = None,
      add_sigmas     = False,
      params         = params2).f_model

    self.f_model_real = abs(self.f_model_complex)
    self.f_model_real.set_observation_type_xray_amplitude()
    self.f_model_real.show_summary(prefix="FMODEL ")

  def wavelength_independent_phases(self,group_sulfurs):
    complex = self.f_model_complex
    self.phases=complex.phases(deg=False)

    #now loop through the heavy atom scatterers
    pdb_hierarchy = self.pdb_inp.construct_hierarchy()
    asc = pdb_hierarchy.atom_selection_cache()
    selection = asc.selection("(element Zn or element Ca or element S or element Fe or element Yb)")
    #selection = asc.selection("element Zn or element Fe")
    #selection = asc.selection("chain A and resseq 201")
    print("%d atoms selected out of total %d"%(
    selection.count(True), selection.size()))
    self.N_anom_scatterers = selection.count(True)
    xrs_ions    = self.xray_structure.select(selection)
    xrs_ions.show_scatterers()
    self.scatterer_model_idx = self.get_scatterer_model_idx(xrs_ions,group_sulfurs)
    self.scatterer_idx = selection.iselection(True)
    self.f_calc = self.xray_structure.structure_factors(d_min=self.d_min, algorithm=algorithm).f_calc()
    self.metals = xrs_ions

  def get_scatterer_model_idx(self,ions,group_sulfurs):
    if not group_sulfurs:
      return flex.size_t(range(len(ions.scatterers())))
    # if it is desired to group all the sulfurs together
    sulfur_idx = None
    parameter_no = 0
    indirection = flex.size_t()
    for j,sc in enumerate(ions.scatterers()):
      if sc.scattering_type=="S":
        if sulfur_idx is None:
          sulfur_idx = parameter_no
          parameter_no += 1
        indirection.append(sulfur_idx)
      else:
        indirection.append(parameter_no)
        parameter_no+=1
    return indirection

  def parts(self,fpp):
    dano_summation = None
    Ddanocalc_Dp = []
    for j,idx in enumerate(self.scatterer_idx):
      bool_array = self.xray_structure.by_index_selection([idx])
      xrs_atom = self.xray_structure.select(bool_array)
      #xrs_atom.show_scatterers()
      f_calc_atom = self.f_calc.structure_factors_from_scatterers(
        xray_structure=xrs_atom, algorithm=algorithm).f_calc()
      adata = f_calc_atom.data().parts()[0]
      bdata = f_calc_atom.data().parts()[1]

      product_factor = bdata * flex.cos(self.phases.data()) - adata * flex.sin(self.phases.data())
      # correspnds to -b cos(alpha) + a sin(alpha)
      #print list(xrs_atom.scattering_types())
      #xrs_atom.scattering_type_registry().show_summary()
      #xrs_atom.scattering_type_registry().show()
      f_zero = xrs_atom.scattering_type_registry().sum_of_scattering_factors_at_diffraction_angle_0()
      #print f_zero
      Ddanocalc_Dp.append((-2./f_zero)*product_factor)
      term = (-2.*fpp[j]/f_zero)*product_factor
      if dano_summation is None:
        dano_summation = term
      else:
        dano_summation += term
    return dano_summation,Ddanocalc_Dp


    """
Next things to do:
DONE Evaluate the f" summation, looping over all the heavy atom scatterers
DONE Calculate functional and gradient
DONE minimize and inspect results for thermolysin
DONE do the same for the ferredoxin
do the same for the high-energy remote of ferredoxin
DONE Refine the LD91 lysozyme model
DONE try f" refinement on lysozyme

debugs:
DONE Report variance-weighted C.C.
DONE Wrap all Sulfurs into a single parameter
Fit the f' as well
---->Fit Obs to the Fcalc with a B-factor or bin scaling
DONE Try to sort out f" for a nested subset of Yb-lyso datasets.
DONE Compare my f" refined values with tabular values (in fact, look them up with a script)
-->make sure Mona's program outputs the wavelength
---->Sort out the geometry in AxFd monomer A
-->Look at the high-energy remote of ferredoxin
Phil-out a few cases so we can get the pdb files out of the code
"""

  def scale(self,other):
    from cctbx import miller
    matches = miller.match_indices(self.f_model_real.indices(),other.indices())
    sel0 = flex.size_t([p[0] for p in matches.pairs()])
    sel1 = flex.size_t([p[1] for p in matches.pairs()])

    val0 = self.f_model_real.data().select(sel0)
    val1 = other.data().select(sel1)
    plot=False
    if plot:
      from matplotlib import pyplot as plt
      plt.plot([-1,4],[-1,4],"g-")
      plt.plot(flex.log10(val0),flex.log10(val1),"r.")
      plt.show()

    from xfel.cxi.cxi_cc import correlation
    slope,offset,corr,N = correlation(
      self = self.f_model_real.select(sel0),
      other = other.select(sel1))
    print(slope,offset,corr,N)
    if plot:
      from matplotlib import pyplot as plt
      plt.plot([-1,4],[-1,4],"g-")
      plt.plot(flex.log10(val0),flex.log10(slope * val1),"r,")
      plt.show()
    return slope

  def scaling_metrics(self,other):
    # Read reflections
    # some requirements. 1) Fobs scaled to Fcalc, not the other way around.
    # 2) ability to make a plot of the two scaled sets
    # 3) set the number of bins
    # 4) understand and print out the per-bin scaling factor
    # 5) print an overall stats line at the end of the table
    # 6) choose one or the other binnings
    """1) scaling and analysis are separate functions"""

    #f_obs, r_free_flags = f_obs.common_sets(r_free_flags)
    f_obs = other
    #r_free_flags = r_free_flags.array(data=r_free_flags.data()==1)
    # Read model

    # Get Fmodel
    fmodel = mmtbx.f_model.manager(
      f_obs          = f_obs,
      #r_free_flags   = r_free_flags,
      xray_structure = self.xray_structure)
    # Do anisotropic overall scaling, bulk-solvent modeling, outlier rejection
    #fmodel.update_all_scales()
    print("r_work, r_free: %6.4f, %6.4f"%(fmodel.r_work(), fmodel.r_free()))
    # Print statistics in resolution bins
    f_model = fmodel.f_model_scaled_with_k1()
    bin_selections = fmodel.f_obs().log_binning()
    dsd = fmodel.f_obs().d_spacings().data()
    print("Bin# Resolution    Nref Cmpl  Rw     CC")
    fmt="%2d: %6.3f-%-6.3f %5d %5.3f %6.4f %6.4f"
    for i_bin, sel in enumerate(bin_selections):
      d           = dsd.select(sel)
      d_min       = flex.min(d)
      d_max       = flex.max(d)
      fmodel_sel  = fmodel.select(sel)
      n           = d.size()
      f_obs_sel   = fmodel.f_obs().select(sel)
      f_model_sel = abs(f_model.select(sel)).data()
      cmpl        = f_obs_sel.completeness(d_max=d_max)
      r_work      = fmodel_sel.r_work()
      cc          = flex.linear_correlation(x=f_obs_sel.data(),
                    y=f_model_sel).coefficient()
      print(fmt%(i_bin, d_max, d_min, n, cmpl, r_work, cc))
    # Alternative binning
    print()
    print("Bin# Resolution    Nref Cmpl  Rw     CC")
    fmodel.f_obs().setup_binner(reflections_per_bin = 2500)
    f_model.use_binning_of(fmodel.f_obs())
    for i_bin in fmodel.f_obs().binner().range_used():
      sel = fmodel.f_obs().binner().selection(i_bin)
      d           = dsd.select(sel)
      d_min       = flex.min(d)
      d_max       = flex.max(d)
      fmodel_sel  = fmodel.select(sel)
      n           = d.size()
      f_obs_sel   = fmodel.f_obs().select(sel)
      f_model_sel = abs(f_model.select(sel)).data()
      cmpl        = f_obs_sel.completeness(d_max=d_max)
      r_work      = fmodel_sel.r_work()
      cc          = flex.linear_correlation(x=f_obs_sel.data(),
                    y=f_model_sel).coefficient()
      print(fmt%(i_bin, d_max, d_min, n, cmpl, r_work, cc))


def get_obs(file_name,tag):
  """Can we scale one amplitude array to another?"""
  Possible=["i(+)","iobs(+)"]
  if tag is not None:  Possible.append(tag.lower())
  from iotbx import mtz
  data_SR = mtz.object(file_name)
  for array in data_SR.as_miller_arrays():
    this_label = array.info().label_string().lower()
    array.show_summary(prefix="OBS ")
    if True in [this_label.find(tag)>=0 for tag in Possible]: break
  assert True in [this_label.find(tag)>=0 for tag in Possible], \
         "Cannot find i(+); use phenix.mtz.dump and give iobs_tag in phil string"
  wavelength = get_wavelength(data_SR,Possible)
  f_ampl = array.as_amplitude_array()
  merged = array.average_bijvoet_mates()
  f_ampl_merged = merged.as_amplitude_array()

  return f_ampl,f_ampl_merged,wavelength

def get_wavelength(self, Possible):#mtz_obj
    # code is adapted from iotbx/mtx/__init__.py show_summary()
    wavelength = None
    for i_crystal,crystal in enumerate(self.crystals()):
      for i_dataset,dataset in enumerate(crystal.datasets()):
        if (dataset.n_columns() > 0):
          fields_list = [[
            "label", "#valid", "%valid", "min", "max", "type", ""]]
          max_field_lengths = [len(field) for field in fields_list[0]]
          max_field_lengths[-2] = 0
          for i_column,column in enumerate(dataset.columns()):
            fields = column.format_fields_for_mtz_dump(
              n_refl=self.n_reflections())
            fields_list.append(fields)
            for i,field in enumerate(fields):
              max_field_lengths[i] = max(max_field_lengths[i], len(field))
          format = "    %%-%ds %%%ds %%%ds %%%ds %%%ds %%%ds %%s" % tuple(
            max_field_lengths[:6])
          for fields in fields_list:
            if fields[0].lower().strip() in Possible:
              wavelength = dataset.wavelength()
    return wavelength


import scitbx.lbfgs
class FPP_optimizer:
  def __init__(self, model,diffs,params):
    self.params = params
    self.model = model
    if params.group_sulfurs:
      self.n = 1 + flex.max(self.model.scatterer_model_idx)
    else:
      self.n = self.model.N_anom_scatterers
    self.x = flex.double(self.n,0.)

    from cctbx import miller
    matches = miller.match_indices(self.model.f_model_real.indices(),diffs.indices())
    self.sel0 = flex.size_t([p[0] for p in matches.pairs()])
    self.sel1 = flex.size_t([p[1] for p in matches.pairs()])

    self.diffs = diffs.select(self.sel1)

    print("SELECTED %d diffs out of %d"%(len(self.diffs.data()), len(diffs.data())))

    self.minimizer = scitbx.lbfgs.run(target_evaluator=self,
        termination_params=scitbx.lbfgs.termination_parameters(
        traditional_convergence_test=True,
        traditional_convergence_test_eps=1.e-4,
        max_iterations=20))

  def compute_functional_and_gradients(self,plot=False):
    dano_summation,Ddanocalc_Dp = self.model.parts(self.get_fpp())
    if plot: self.plot(dano_summation)

    #calculate the functional
    residual = self.diffs.data() - dano_summation.select(self.sel0)
    if self.params.use_weights:  residual /= self.diffs.sigmas()
    F = 0.5 * flex.sum(residual * residual)
    print(("LBFGS stp",F))

    g = flex.double(self.n, 0.)
    for j,item in enumerate(Ddanocalc_Dp):
      deriv = item.select(self.sel0)
      vector=-deriv*residual
      if self.params.use_weights:  vector /= self.diffs.sigmas()

      g[self.model.scatterer_model_idx[j]] += flex.sum(vector)
    return F, g

  def correlation(self):
    dano_summation,Ddanocalc_Dp = self.model.parts(self.get_fpp())

    #calculate the correlation
    self.diffs.data(),dano_summation.select(self.sel0)
    if self.params.use_weights:
      wt = 1./(self.diffs.sigmas()*self.diffs.sigmas())
    else: wt = flex.double(len(self.sel0), 1.)

    from scitbx.math.tests.tst_weighted_correlation import weighted_correlation
    return weighted_correlation(wt, self.diffs.data(), dano_summation.select(self.sel0))

  def get_fpp(self):
    if self.params.group_sulfurs:
      return self.x.select(self.model.scatterer_model_idx)
    else:
      return self.x

  def plot(self,dano_summation):
    from matplotlib import pyplot as plt

    if self.params.use_weights:
      wt = 1./(self.diffs.sigmas()*self.diffs.sigmas())
      order = flex.sort_permutation(wt)
      wt = wt.select(order)
      df = self.diffs.data().select(order)
      dano = dano_summation.select(self.sel0).select(order)
      from matplotlib.colors import Normalize
      dnorm = Normalize()
      dnorm.autoscale(wt.as_numpy_array())
      CMAP = plt.get_cmap("rainbow")
      for ij in range(len(self.diffs.data())):
        #blue represents zero weight:  red, large weight
        plt.plot([df[ij]],[dano[ij]],color=CMAP(dnorm(wt[ij])),marker=".", markersize=4)

    else:
      plt.plot(self.diffs.data(),dano_summation.select(self.sel0),"r,")
    plt.axes().set_aspect("equal")
    plt.axes().set_xlabel("Observed Dano")
    plt.axes().set_ylabel("Model Dano")
    plt.show()


def show_scatterers(self,fpp,wave):
    print ("""Label            f"      Coordinates        Occupancy """
                 "Uiso, Ustar as Uiso")
    scatterers = self.scatterers()
    types_used = {}
    for j,sc in enumerate(scatterers):
      sc.fdp = fpp[j]
      show_scatterer(sc, unit_cell=self.unit_cell())
      types_used[sc.scattering_type]=None
    print("Tabular values for %7.1f eV (%8.5f Angstrom):"%(12398/wave,wave))
    from cctbx.eltbx import sasaki, henke
    for tag in types_used.keys():
      fpp_expected_sasaki = sasaki.table(tag).at_angstrom(
          wave).fdp()
      fpp_expected_henke = henke.table(tag).at_angstrom(
          wave).fdp()
      print("           %-4s"%tag, end=' ')
      print("%6.3f" % (max(fpp_expected_sasaki,fpp_expected_henke)))

def show_scatterer(self, f=None, unit_cell=None):

  if (f is None): f = sys.stdout
  from cctbx import adptbx
  print("%-4s" % self.label[5:15], end=' ', file=f)
  print("%-4s" % self.scattering_type, end=' ', file=f)
  #print >> f, "%3d" % self.multiplicity(),
  print("%6.3f" % (self.fdp), end=' ', file=f)
  print("(%7.4f %7.4f %7.4f)" % self.site, end=' ', file=f)
  print("%4.2f" % self.occupancy, end=' ', file=f)
  if self.flags.use_u_iso():
    print("%6.4f" % self.u_iso, end=' ', file=f)
  else:
    print('[ - ]', end=' ', file=f)
  if self.flags.use_u_aniso():
    assert unit_cell is not None
    u_cart = adptbx.u_star_as_u_cart(unit_cell, self.u_star)
    print("%6.4f" % adptbx.u_cart_as_u_iso(u_cart), file=f)
    print("     u_cart =", ("%6.3f " * 5 + "%6.3f") % u_cart, end=' ', file=f)
  else:
    print('[ - ]', end=' ', file=f)
  if False and (self.fp != 0 or self.fdp != 0):
    print("\n     fp,fdp = %6.4f,%6.4f" % (
      self.fp,
      self.fdp), end=' ', file=f)
  print(file=f)

from libtbx.phil import parse
phil_scope = parse("""
  pdb = None
    .type = str
  mtz = None
    .type = str
  d_min = 2.0
    .type = float
  use_weights = True
    .type = bool
    .help = Use variance weighting for LSQ fitting and plotting
  make_plot = True
    .type = bool
    .help = Plot everything at the end
  group_sulfurs = True
    .type = bool
    .help = Treat all sulfur atoms as having the same f' and f"
  iobs_tag = None
    .type = str
    .help = lower-case string identifying the anomalous iobs(+)
    .help = need not give i(+) or iobs(+) as these are already hard coded
  wavelength_overrride = None
    .type = float
  """, process_includes = True)

def get_params(args):
  user_phil = []
  for arg in args:
    if os.path.isfile(arg):
      try:
        user_phil.append(parse(file_name=arg))
      except Exception as e:
        print(str(e))
        raise Sorry("Couldn't parse phil file %s"%arg)
    else:
      try:
        user_phil.append(parse(arg))
      except Exception as e:
        print(str(e))
        raise Sorry("Couldn't parse argument %s"%arg)
  params = phil_scope.fetch(sources=user_phil).extract()
  return params


if __name__ == "__main__":
  params = get_params(sys.argv[1:])
  #params.mtz = "/Users/nksauter/xtalwork/LM14/Yb-lysozyme/merge118/Yblyso118_post_anom_4etc_s0_mark0.mtz"
  params.wavelength_overrride=1.3853 # for Yb-lysozyme
  obs_ampl,obs_ampl_merged,wave = get_obs(params.mtz, params.iobs_tag)
  if wave==None or wave==1.:  wave = params.wavelength_overrride
  if params.d_min == None:
    params.d_min = obs_ampl_merged.d_min()
  print("DMIN = %7.2f"%params.d_min)

  #params.pdb = "/Users/nksauter/xtalwork/LM14/Yb-lysozyme/Refine_4/LM14_1colorYblyso_refine_4.pdb"
  algorithm = "direct"
  algorithm = "fft"
  M = Model(params.pdb,params.d_min)
  slope = M.scale(other=obs_ampl_merged)

  #Note the observations are scaled to the Fmodel, not the other way around
  scaled_obs_ampl = obs_ampl.customized_copy(data = slope*obs_ampl.data(), sigmas=slope*obs_ampl.sigmas())
  scaled_obs_ampl_merged = obs_ampl_merged.customized_copy(
    data = slope*obs_ampl_merged.data(), sigmas=slope*obs_ampl_merged.sigmas())

  M.scaling_metrics(other = scaled_obs_ampl)

  anomalous_diffs = scaled_obs_ampl.anomalous_differences()

  for x in range(5):
    print(anomalous_diffs.indices()[x], anomalous_diffs.data()[x],anomalous_diffs.sigmas()[x])

  M.wavelength_independent_phases(params.group_sulfurs)
  print(M.N_anom_scatterers ,"anomalous_scatterers")
  M.parts(flex.double(M.N_anom_scatterers))
  FPPO = FPP_optimizer(M, anomalous_diffs, params)

  show_scatterers(M.metals, FPPO.get_fpp(), wave)
  print("C.C. = %7.2f%%"%(100. * FPPO.correlation()))
  if params.make_plot:
    FPPO.compute_functional_and_gradients(plot=True)


 *******************************************************************************


 *******************************************************************************
xfel/cxi/completeness_plot.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import os,math
from cctbx.array_family import flex
from cctbx.crystal import symmetry
from rstbx.apps.stills.simple_integration import show_observations
op = os.path

def get_observations(set,file_names,params):
  from libtbx import easy_pickle

  print("Number of pickle files found:", len(file_names))
  print()
  returned=0
  from cctbx.sgtbx.bravais_types import bravais_lattice
  ref_bravais_type = bravais_lattice(set.space_group_info().type().number())

  for name in file_names:
    if name=="stats.pickle":continue

    full_path = file_name = op.abspath(name)
    obj = easy_pickle.load(file_name=full_path)
    if "observations" not in obj: continue
    unit_cell = obj["observations"][0].unit_cell()
    result_array = obj["observations"][0]
    #unit_cell, img_miller_indices = obj

    print(file_name,unit_cell, obj["observations"][0].space_group_info())

    if not bravais_lattice(
      obj["observations"][0].space_group_info().type().number()) == ref_bravais_type:
      print("Skipping cell in different Bravais type")
      continue

    if not unit_cell.is_similar_to(
      other=set.unit_cell(),
      relative_length_tolerance=0.1,
      absolute_angle_tolerance=2):
      print("Skipping cell with outlier dimensions")
      continue

    obj["observations"][0].show_summary()
    # Now do manipulate the data to conform to unit cell, asu, and space group of reference
    # Only works if there is NOT an indexing ambiguity!
    ref_setting_obs = obj["observations"][0].customized_copy(crystal_symmetry=set.crystal_symmetry()
      ).resolution_filter(d_min=params.d_min).map_to_asu()
    returned+=1
    yield ref_setting_obs,name

  print("Only %d of %d obs arrays had the correct cell"%(returned,len(file_names)))

def plot_overall_completeness(completeness):
  completeness_range = range(-1,flex.max(completeness)+1)
  completeness_counts = [completeness.count(n) for n in completeness_range]
  from matplotlib import pyplot as plt
  plt.plot(completeness_range,completeness_counts,"r+")
  plt.show()


def run(args):
  import iotbx.phil
  phil = iotbx.phil.process_command_line(args=args, master_string="""
target_unit_cell = 78,78,37,90,90,90
  .type = unit_cell
target_space_group = P43212
  .type = space_group
d_min = 2.1
  .type = float
plot = False
  .type = str
cut_short_at = None
  .type = int
""").show()
  print()
  work_params = phil.work.extract()
  assert work_params.d_min is not None

  print(work_params.target_unit_cell)
  print(work_params.target_space_group)

  from cctbx import miller
  miller_set = symmetry(
      unit_cell=work_params.target_unit_cell,
      space_group_info=work_params.target_space_group
    ).build_miller_set(
      anomalous_flag=True,
      d_min=work_params.d_min
    )

  miller_set.show_summary()

  # reality check
  #recip_cell_volume = work_params.target_unit_cell.reciprocal().volume()
  #recip_sphere_volume = (4/3)*math.pi*math.pow(1./work_params.d_min,3)
  #resolution_cells = recip_sphere_volume/recip_cell_volume
  #print "Number of asu's in sphere=",resolution_cells/miller_set.size()

  results = get_observations(miller_set,phil.remaining_args,work_params)

  # Create (and initialise?) arrays for statistics on the set of the
  # observed reflections which are present in the reference data set.
  completeness = flex.int(miller_set.size())
  sum_I        = flex.double(miller_set.size())
  sum_I_SIGI   = flex.double(miller_set.size())
  #last = completeness.deep_copy()

  for result,filename in results:
    result.show_summary()
    show_observations(result)

    if work_params.plot==True:
      #private interface to get the very strong diffraction images
      from six.moves import StringIO
      G = StringIO()
      show_observations(result,out=G)
      for line in G.getvalue().split("\n"):
        tokens = line.split()
        if len(tokens)>6:
          try:
            if float(tokens[3]) < 2.6 and float(tokens[-1]) > 10:
              print("Strong signal",filename,line)
          except ValueError: pass
    print()

    # Match up the observed intensities against the reference data
    # set, i_model, instead of the pre-generated miller set,
    # miller_set.
    matches = miller.match_indices(
      miller_set.indices(),
      result.indices())

    #for ih,hkl in enumerate(result.indices()):
    #  print hkl, result.data()[ih]
    print()

    # Update the count for each matched reflection.
    completeness +=  (~matches.single_selection(0)).as_int()
    for pair in matches.pairs():
      sum_I[pair[0]] += result.data()[pair[1]]
      sum_I_SIGI[pair[0]] += (result.data()[pair[1]]/result.sigmas()[pair[1]])
    #for ih,hkl in enumerate(miller_set.indices()):
    #  print "%15s"%str(hkl),"%4d"%last[ih],"%4d"%completeness[ih], sum_I[ih]

    #print matches
    #help(matches)
    #print matches.pair_selection(0)
    #for item in matches.pairs(): print item
    #print list(miller_set.indices().select(matches.pairs().column(1)))
    #print list(~matches.single_selection(0))
    #print list(~matches.single_selection(1))
    #last  = completeness.deep_copy()

  #plot_overall_completeness(completeness)

  show_overall_observations(miller_set,completeness,sum_I,sum_I_SIGI)

  #from libtbx import easy_pickle
  #easy_pickle.dump(file_name="stats.pickle", obj=stats)
  #stats.report(plot=work_params.plot)
  #miller_counts = miller_set_p1.array(data=stats.counts.as_double()).select(
  #  stats.counts != 0)
  #miller_counts.as_mtz_dataset(column_root_label="NOBS").mtz_object().write(
  #  file_name="nobs.mtz")

def show_overall_observations(obs,redundancy,I,I_SIGI,out=None):
  if out==None:
    import sys
    out = sys.stdout
  from libtbx.str_utils import format_value

  obs.setup_binner(n_bins = 15)
  result = []
  for i_bin in obs.binner().range_used():
    sel_w = obs.binner().selection(i_bin)
    sel_fo_all = obs.select(sel_w)
    d_max_,d_min_ = sel_fo_all.d_max_min()
    d_range = obs.binner().bin_legend(
      i_bin=i_bin, show_bin_number=False, show_counts=False)
    sel_redundancy = redundancy.select(sel_w)
    sel_absent = sel_redundancy.count(0)
    sel_complete_tag = "[%d/%d]"%(sel_redundancy.size()-sel_absent,sel_redundancy.size())
    sel_measurements = flex.sum(sel_redundancy)
    sel_data = I.select(sel_w)
    sel_sig = I_SIGI.select(sel_w)
    if (sel_data.size() > 0 and sel_measurements>0):
      bin = resolution_bin(
        i_bin        = i_bin,
        d_range      = d_range,
        redundancy   = flex.mean(sel_redundancy.as_double()),
        complete_tag = sel_complete_tag,
        measurements = sel_measurements,
        mean_I       = flex.sum(sel_data)/sel_measurements,
        mean_I_sigI  = flex.sum(sel_sig)/sel_measurements,
        )
      result.append(bin)
  print("\n Bin  Resolution Range      Compl. <Redundancy>  #Measurements  <I>     <I/sig(I)>", file=out)
  for bin in result:
    fmt = " %s %s %s %s       %s      %s   %s"
    print(fmt%(
      format_value("%3d",   bin.i_bin),
      format_value("%-13s", bin.d_range),
      format_value("%13s",  bin.complete_tag),
      format_value("%4.0f", bin.redundancy),
      format_value("%8d",   bin.measurements),
      format_value("%8.1f", bin.mean_I),
      format_value("%8.1f", bin.mean_I_sigI),
    ), file=out)
class resolution_bin(object):
  def __init__(self,
               i_bin         = None,
               d_range       = None,
               redundancy    = None,
               absent        = None,
               complete_tag  = None,
               measurements  = None,
               mean_I        = None,
               mean_I_sigI   = None,
               sigmaa        = None):
    from libtbx import adopt_init_args
    adopt_init_args(self, locals())


if (__name__ == "__main__"):
  import sys
  run(args=sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/cxi/correction_vector_plot.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import os,math
from scitbx import matrix
from cctbx.array_family import flex
from xfel import correction_vector_store,get_correction_vector_xy
from xfel import get_radial_tangential_vectors

class manage_sql:
  def __init__(self,params):
    self.params = params
    self.have_db = False
    if self.params.mysql.runtag is not None:
      self.have_db = True
      from xfel.merging.database.merging_database import manager
      self.manager = manager

  def get_cursor(self):
    CART = self.manager(self.params)
    db = CART.connection()
    cursor = db.cursor()
    return cursor

  def initialize_tables_and_insert_command(self):
    if self.have_db is not True: return
    CART = self.manager(self.params)
    db = CART.connection()
    cursor = db.cursor()
    new_tables = CART.positional_refinement_schema_tables(self.params.mysql.runtag)
    for table in new_tables:
      cursor.execute("DROP TABLE IF EXISTS %s;"%table[0])
      cursor.execute("CREATE TABLE %s "%table[0]+table[1].replace("\n"," ")+" ;")
    from six.moves import cStringIO as StringIO
    self.query = StringIO()
    self.query.write("INSERT INTO %s_spotfinder VALUES "%self.params.mysql.runtag)
    self.firstcomma = ""

  def get_frame_dictionary(self):
    if self.have_db is not True: return dict()
    CART = self.manager(self.params)
    db = CART.connection()
    cursor = db.cursor()
    cursor.execute("SELECT unique_file_name,frame_id_1_base FROM %s_frame"%(self.params.mysql.runtag))
    del CART
    frame_dict = {}
    for ftuple in cursor.fetchall():
      frame_dict[ftuple[0]] = ftuple[1]
    return frame_dict

  def insert(self,run,itile,tokens):
    self.query.write(self.firstcomma); self.firstcomma=","
    self.query.write("('%d','%d','%10.2f','%10.2f','%10.2f','%10.2f','%10.2f','%10.2f','%10.2f','%10.2f',"%(
          run,itile,float(tokens[2]),float(tokens[3]),float(tokens[5]),float(tokens[6]),
          float(tokens[8]),float(tokens[9]),float(tokens[11]),float(tokens[12]) ))
    self.query.write("'%4d','%4d','%4d','%6.3f','%6.3f')"%(
      int(tokens[14]),int(tokens[15]),int(tokens[16]),float(tokens[19]),float(tokens[21])))


  def send_insert_command(self):
    if self.have_db is not True: return
    cursor = self.get_cursor()
    cursor.execute( self.query.getvalue() )

class lines(correction_vector_store):
  def __init__(self,params):
    correction_vector_store.__init__(self)
    self.params = params
    self.database = manage_sql(self.params)

  def literals(self):
    frame_dict = self.database.get_frame_dictionary()

    if self.params.run_numbers is None:
      path = self.params.outdir_template
      stream = open(path,"r")
      print(path)
      for line in stream:
        if line.find("XFEL processing:") == 0:
           tokens = line.strip().split("/")
           picklefile = line.strip().split()[2]
           frame_id = frame_dict.get(picklefile, None)
           if frame_id is not None:
             print("FETCHED",frame_id)
        if line.find("CV OBSCENTER")==0 and line.find("Traceback")==-1:
          potential_tokens = line.strip().split()
          if len(potential_tokens)==22 and \
                 potential_tokens[17].isdigit() and \
                 int(potential_tokens[17])==self.params.bravais_setting_id:
            yield frame_id,potential_tokens
        if len(self.tiles)==0 and line.find("EFFEC")==0:
          self.tiles = flex.int([int(a) for a in line.strip().split()[2:]])
          assert len(self.tiles)==256
          print(list(self.tiles))
          self.initialize_per_tile_sums()
      return
    for run in self.params.run_numbers:
        templ = self.params.outdir_template%run
        items = os.listdir(templ)
        for item in items:
          path = os.path.join(templ,item)
          stream = open(path,"r")
          print(path)
          for line in stream:
            if line.find("CV OBSCENTER")==0:
              potential_tokens = line.strip().split()
              if len(potential_tokens)==22 and \
                 potential_tokens[17].isdigit() and \
                 int(potential_tokens[17])==self.params.bravais_setting_id:
                yield None,potential_tokens
            if len(self.tiles)==0 and line.find("EFFEC")==0:
              self.tiles = flex.int([int(a) for a in line.strip().split()[2:]])
              assert len(self.tiles)==256
              print(list(self.tiles))
              self.initialize_per_tile_sums()

  def vectors(self):
    self.database.initialize_tables_and_insert_command()

    self.tile_rmsd = [0.]*64

    for run,tokens in self.literals():
     try:
      itile = self.register_line( float(tokens[2]),float(tokens[3]),
                       float(tokens[5]),float(tokens[6]),
                       float(tokens[8]),float(tokens[9]),
                       float(tokens[11]),float(tokens[12]) )
      if run is not None:
        self.database.insert(run,itile,tokens)
      yield "OK"
     except ValueError:
       print("Valueerror")

    self.database.send_insert_command()
    for x in range(64):
      if self.tilecounts[x]==0: continue
      self.radii[x]/=self.tilecounts[x]
      sum_cv = matrix.col(self.mean_cv[x])
      self.mean_cv[x] = sum_cv/self.tilecounts[x]
      mean_cv = matrix.col(self.mean_cv[x])
      selection = (self.master_tiles == x)
      selected_cv = self.master_cv.select(selection)
      if len(selected_cv)>0:
        self.tile_rmsd[x] = math.sqrt(
      flex.mean(flex.double([ (matrix.col(cv) - mean_cv).length_sq() for cv in selected_cv ]))
      )
      else: self.tile_rmsd[x]=0.
    self.overall_N = flex.sum(flex.int( [int(t) for t in self.tilecounts] ))
    self.overall_cv = matrix.col(self.overall_cv)/self.overall_N
    self.overall_rmsd = math.sqrt( self.sum_sq_cv / self.overall_N )

def run_correction_vector_plot(working_phil):

  L = lines(working_phil)
  for line in L.vectors():
    pass # pull out the information, lines class does all the work

  close_x = flex.double()
  close_y = flex.double()
  far_x = flex.double()
  far_y = flex.double()
  master_coords = L.master_coords
  master_cv = L.master_cv
  master_tiles = L.master_tiles
  for idx in range(0,len(master_coords),10):
    if matrix.col(master_cv[idx]).length() < L.tile_rmsd[ master_tiles[idx] ]:
      pass
      #close_x.append(master_coords[idx][0])
      #close_y.append(master_coords[idx][1])
    else:
      far_x.append(master_coords[idx][0])
      far_y.append(master_coords[idx][1])
      close_x.append(master_coords[idx][0]+master_cv[idx][0])
      close_y.append(master_coords[idx][1]+master_cv[idx][1])
  if working_phil.show_plots is True:
    from matplotlib import pyplot as plt
    plt.plot(close_x,close_y,"r.")
    plt.plot(far_x,far_y,"g.")
    plt.axes().set_aspect("equal")
    plt.show()


  sort_radii = flex.sort_permutation(flex.double(L.radii))
  tile_rmsds = flex.double()
  radial_sigmas = flex.double(64)
  tangen_sigmas = flex.double(64)
  for idx in range(64):
    x = sort_radii[idx]
    print("Tile %2d: radius %7.2f, %6d observations, delx %5.2f  dely %5.2f, rmsd = %5.2f"%(
      x, L.radii[x], L.tilecounts[x], L.mean_cv[x][0], L.mean_cv[x][1],
      L.tile_rmsd[x]
        ), end=' ')
    if L.tilecounts[x] < 3:
      print()
      radial = (0,0)
      tangential = (0,0)
      rmean,tmean,rsigma,tsigma=(0,0,1,1)
    else:
      wtaveg = L.weighted_average_angle_deg_from_tile(x)
      print("Tile rotation %6.2f deg"%wtaveg, end=' ')
      radial,tangential,rmean,tmean,rsigma,tsigma = get_radial_tangential_vectors(L,x)
      print("%6.2f %6.2f"%(rsigma,tsigma))
    radial_sigmas[x]=rsigma
    tangen_sigmas[x]=tsigma
  rstats = flex.mean_and_variance(radial_sigmas,L.tilecounts.as_double())
  tstats = flex.mean_and_variance(tangen_sigmas,L.tilecounts.as_double())

  print("\nOverall                 %8d observations, delx %5.2f  dely %5.2f, rmsd = %5.2f"%(
      L.overall_N, L.overall_cv[0], L.overall_cv[1], L.overall_rmsd))
  print("Average tile rmsd %5.2f"%flex.mean(flex.double(L.tile_rmsd)))
  print("Average tile displacement %5.2f"%(flex.mean(
    flex.double([matrix.col(cv).length() for cv in L.mean_cv]))))
  print("Weighted average radial sigma %6.2f"%rstats.mean())
  print("Weighted average tangential sigma %6.2f"%tstats.mean())

  if working_phil.show_plots is True:
    plt.plot([(L.tiles[4*x+0]+L.tiles[4*x+2])/2. for x in range(64)],[(L.tiles[4*x+1]+L.tiles[4*x+3])/2. for x in range(64)],"go")
    for x in range(64):
      plt.text(10+(L.tiles[4*x+0]+L.tiles[4*x+2])/2.,10+(L.tiles[4*x+1]+L.tiles[4*x+3])/2.,"%d"%x)
    plt.show()

    for idx in range(64):
      x = sort_radii[idx]
      print("Tile %2d: radius %7.2f, %6d observations, delx %5.2f  dely %5.2f, rmsd = %5.2f"%(
        x, L.radii[x], L.tilecounts[x], L.mean_cv[x][0], L.mean_cv[x][1],
        L.tile_rmsd[x]
        ), end=' ')
      if L.tilecounts[x] < 3:
        print()
        radial = (0,0)
        tangential = (0,0)
        rmean,tmean,rsigma,tsigma=(0,0,1,1)
      else:
        wtaveg = L.weighted_average_angle_deg_from_tile(x)
        print("Tile rotation %6.2f deg"%wtaveg, end=' ')
        radial,tangential,rmean,tmean,rsigma,tsigma = get_radial_tangential_vectors(L,x)
        print("%6.2f %6.2f"%(rsigma,tsigma))

      if working_phil.colormap:
        from pylab import imshow, axes, colorbar, show
        import numpy

        xcv,ycv = get_correction_vector_xy(L,x)
        _min = min(min(xcv),min(ycv))
        _max = max(max(xcv),max(ycv))

        hist,xedges,yedges = numpy.histogram2d(xcv,ycv,bins=40,range=[[_min,_max],[_min,_max]])
        extent = [xedges[0], xedges[-1], yedges[0], yedges[-1] ]

        imshow(hist.T,extent=extent,interpolation='nearest',origin='lower')
        from matplotlib.patches import Ellipse
        ell = Ellipse(xy=(L.mean_cv[x][0],L.mean_cv[x][1]),
                      width=2.*rsigma, height=2.*tsigma,
                      angle=math.atan2(-(radial[1]),-(radial[0]))*180./math.pi,
                      edgecolor="y", linewidth=2, fill=False, zorder=100)
        axes().add_artist(ell)
        colorbar()
        show()

      else:
        from matplotlib import pyplot as plt
        xcv,ycv = get_correction_vector_xy(L,x)
        if len(xcv)==0 or len(ycv)==0: continue
        plt.plot(xcv,ycv,"r.")
        plt.plot([L.mean_cv[x][0]],[L.mean_cv[x][1]],"go")
        plt.plot([L.mean_cv[x][0]+radial[0]],[L.mean_cv[x][1]+radial[1]],"yo")
        plt.plot([L.mean_cv[x][0]+tangential[0]],[L.mean_cv[x][1]+tangential[1]],"bo")
        from matplotlib.patches import Ellipse
        ell = Ellipse(xy=(L.mean_cv[x][0],L.mean_cv[x][1]),
                      width=2.*rsigma, height=2.*tsigma,
                      angle=math.atan2(-(radial[1]),-(radial[0]))*180./math.pi,
                      edgecolor="y", linewidth=2, fill=False, zorder=100)
        plt.axes().add_artist(ell)
        plt.axes().set_aspect("equal")
        _min = min(min(xcv),min(ycv))
        _max = max(max(xcv),max(ycv))
        plt.axes().set_xlim(_min,_max)
        plt.axes().set_ylim(_min,_max)
        plt.show()


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/__init__.py
from __future__ import absolute_import, division, print_function

def skip_event_flag():
  from numpy import ndarray
  return ndarray([0])


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/average_tbx.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# $Id$

from __future__ import absolute_import, division, print_function

import math
import multiprocessing
import numpy

from libtbx import easy_pickle
from scitbx.array_family import flex
import scitbx.math
from xfel.cxi.cspad_ana import common_mode
from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import skip_event_flag
from six.moves import zip

class average_mixin(common_mode.common_mode_correction):
  def __init__(self,
               address,
               avg_dirname=None,
               avg_basename=None,
               stddev_dirname=None,
               stddev_basename=None,
               max_dirname=None,
               max_basename=None,
               background_path=None,
               flags=None,
               hot_threshold=None,
               gain_threshold=None,
               noise_threshold=7,
               elastic_threshold=9,
               symnoise_threshold=4,
               **kwds):
    """
    @param address         Full data source address of the DAQ device
    @param avg_dirname     Directory portion of output average image
                           XXX mean
    @param avg_basename    Filename prefix of output average image XXX
                           mean
    @param flags inactive:  Eliminate the inactive pixels
                 noelastic: Eliminate elastic scattering
                 nohot:     Eliminate the hot pixels
                 nonoise:   Eliminate noisy pixels
                 symnoise:  Symmetrically eliminate noisy pixels
    @param stddev_dirname  Directory portion of output standard
                           deviation image XXX std
    @param stddev_basename Filename prefix of output standard
                           deviation image XXX std
    @param max_dirname     Directory portion of output maximum
                           projection image
    @param max_basename    Filename prefix of output maximum
                           projection image
     """

    super(average_mixin, self).__init__(
      address=address,
      **kwds
    )
    self.roi = None
    self.avg_basename = cspad_tbx.getOptString(avg_basename)
    self.avg_dirname = cspad_tbx.getOptString(avg_dirname)
    self.detector = cspad_tbx.address_split(address)[0]
    self.flags = cspad_tbx.getOptStrings(flags, default = [])
    self.stddev_basename = cspad_tbx.getOptString(stddev_basename)
    self.stddev_dirname = cspad_tbx.getOptString(stddev_dirname)
    self.max_basename = cspad_tbx.getOptString(max_basename)
    self.max_dirname = cspad_tbx.getOptString(max_dirname)
    self.background_path = cspad_tbx.getOptString(background_path)
    self.hot_threshold = cspad_tbx.getOptFloat(hot_threshold)
    self.gain_threshold = cspad_tbx.getOptFloat(gain_threshold)
    self.noise_threshold = cspad_tbx.getOptFloat(noise_threshold)
    self.elastic_threshold = cspad_tbx.getOptFloat(elastic_threshold)
    self.symnoise_threshold = cspad_tbx.getOptFloat(symnoise_threshold)

    if background_path is not None:
      background_dict = easy_pickle.load(background_path)
      self.background_img = background_dict['DATA']

    self._have_max = self.max_basename is not None or \
                     self.max_dirname is not None
    self._have_mean = self.avg_basename is not None or \
                      self.avg_dirname is not None
    self._have_std = self.stddev_basename is not None or \
                     self.stddev_dirname is not None

    # Start a server process which holds a set of Python objects that
    # other processes can manipulate using proxies.  The queues will
    # be used in endjob() to pass images between the worker processes,
    # and the lock will ensure the transfer is treated as a critical
    # section.  There is therefore the risk of a hang if the queues
    # cannot hold all the data one process will supply before another
    # empties it.
    #
    # In an attempt to alleviate this issue, separate queues are used
    # for the potentially big images.  The hope is to prevent
    # producers from blocking while consumers are locked out by using
    # more buffers.
    mgr = multiprocessing.Manager()
    self._lock = mgr.Lock()
    self._metadata = mgr.dict()
    self._queue_max = mgr.Queue()
    self._queue_sum = mgr.Queue()
    self._queue_ssq = mgr.Queue()


  def beginjob(self, evt, env):
    """The beginjob() function does one-time initialisation from
    event- or environment data.  It is called at an XTC configure
    transition.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    super(average_mixin, self).beginjob(evt, env)

    if self.dark_img is not None and self.hot_threshold is not None:
      self.hot_threshold *= flex.median(self.dark_img.as_1d())
      self.logger.info("HOT THRESHOLD: %.2f" % self.hot_threshold)
      self.logger.info("Number of pixels above hot threshold: %i" % \
                       (self.dark_img > self.hot_threshold).count(True))

    self._nfail = 0
    self._nmemb = 0

    # The time_base metadata item is a two-long array of seconds and
    # milliseconds, and it must be recorded only once.  It indicates
    # the base time, which is subtracted from all per-shot times.
    # Assuming an average run length of ten minutes, five minutes past
    # the start of a run is a good base time.
    self._lock.acquire()
    if 'time_base' not in self._metadata:
      self._metadata['time_base'] = (cspad_tbx.evt_time(evt)[0] + 5 * 60, 500)
    self._lock.release()


  def event(self, evt, env):
    """The event() function is called for every L1Accept transition.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    super(average_mixin, self).event(evt, env)
    if evt.get('skip_event'):
      return

    # Get the distance for the detectors that should have it, and set
    # it to NaN for those that should not.
    if self.detector == 'CxiDs1' or \
       self.detector == 'CxiDs2' or \
       self.detector == 'CxiDsd' or \
       self.detector == 'XppGon':
      distance = cspad_tbx.env_distance(self.address, env, self._detz_offset)
      if distance is None:
        self._nfail += 1
        self.logger.warning("event(): no distance, shot skipped")
        evt.put(skip_event_flag(), 'skip_event')
        return
    else:
      distance = float('nan')

    if ("skew" in self.flags):
      # Take out inactive pixels
      if self.roi is not None:
        pixels = self.cspad_img[self.roi[2]:self.roi[3], self.roi[0]:self.roi[1]]
        dark_mask = self.dark_mask[self.roi[2]:self.roi[3], self.roi[0]:self.roi[1]]
        pixels = pixels.as_1d().select(dark_mask.as_1d())
      else:
        pixels = self.cspad_img.as_1d().select(self.dark_mask.as_1d()).as_double()
      stats = scitbx.math.basic_statistics(pixels.as_double())
      #stats.show()
      self.logger.info("skew: %.3f" %stats.skew)
      self.logger.info("kurtosis: %.3f" %stats.kurtosis)
      if 0:
        from matplotlib import pyplot
        hist_min, hist_max = flex.min(flex_cspad_img.as_double()), flex.max(flex_cspad_img.as_double())
        print(hist_min, hist_max)
        n_slots = 100
        n, bins, patches = pyplot.hist(flex_cspad_img.as_1d().as_numpy_array(), bins=n_slots, range=(hist_min, hist_max))
        pyplot.show()

      # XXX This skew threshold probably needs fine-tuning
      skew_threshold = 0.35
      if stats.skew < skew_threshold:
        self._nfail += 1
        self.logger.warning("event(): skew < %f, shot skipped" % skew_threshold)
        evt.put(skip_event_flag(), 'skip_event')
        return
      #self.cspad_img *= stats.skew

    if ("inactive" in self.flags):
      self.cspad_img.set_selected(self.dark_stddev <= 0, 0)

    if ("noelastic" in self.flags):
      ELASTIC_THRESHOLD = self.elastic_threshold
      self.cspad_img.set_selected(self.cspad_img > ELASTIC_THRESHOLD, 0)

    if self.hot_threshold is not None:
      HOT_THRESHOLD = self.hot_threshold
      self.cspad_img.set_selected(self.dark_img > HOT_THRESHOLD, 0)

    if self.gain_map is not None and self.gain_threshold is not None:
      # XXX comparing each pixel to a moving average would probably be better
      # since the gain should vary approximately smoothly over different areas
      # of the detector
      GAIN_THRESHOLD = self.gain_threshold
      #self.logger.debug(
        #"rejecting: %i" %(self.gain_map > GAIN_THRESHOLD).count(True))
      self.cspad_img.set_selected(self.gain_map > GAIN_THRESHOLD, 0)

    if ("nonoise" in self.flags):
      NOISE_THRESHOLD = self.noise_threshold
      self.cspad_img.set_selected(self.cspad_img < NOISE_THRESHOLD, 0)

    if ("sigma_scaling" in self.flags):
      self.do_sigma_scaling()

    if ("symnoise" in self.flags):
      SYMNOISE_THRESHOLD = self.symnoise_threshold
      self.cspad_img.set_selected((-SYMNOISE_THRESHOLD < self.cspad_img) &
                                  ( self.cspad_img  < SYMNOISE_THRESHOLD), 0)

    if ("output" in self.flags):
      try:
        from six.moves import cPickle as pickle
      except ImportError:
        import pickle
      import os
      if (not os.path.isdir(self.pickle_dirname)):
        os.makedirs(self.pickle_dirname)
      flexdata = flex.int(self.cspad_img.astype(numpy.int32))
      d = cspad_tbx.dpack(
        address=self.address,
        data=flexdata,
        timestamp=cspad_tbx.evt_timestamp(cspad_tbx.evt_time(evt))
      )
      G = open(os.path.join(".",self.pickle_dirname)+"/"+self.pickle_basename,
               "ab")
      pickle.dump(d,G,pickle.HIGHEST_PROTOCOL)
      G.close()

    if self.photon_threshold is not None and self.two_photon_threshold is not None:
      self.do_photon_counting()

    if self.background_path is not None:
      self.cspad_img -= self.background_img


    # t and self._sum_time are a two-long arrays of seconds and
    # milliseconds which hold time with respect to the base time.
    t = [t1 - t2 for (t1, t2) in zip(cspad_tbx.evt_time(evt),
                                     self._metadata['time_base'])]
    if self._nmemb == 0:
      # The peers metadata item is a bit field where a bit is set if
      # the partial sum from the corresponding worker process is
      # pending.  If this is the first frame a worker process sees,
      # set its corresponding bit in the bit field since it will
      # contribute a partial sum.
      if env.subprocess() >= 0:
        self._lock.acquire()
        if 'peers' in self._metadata:
          self._metadata['peers'] |= (1 << env.subprocess())
        else:
          self._metadata['peers'] = (1 << env.subprocess())
        self._lock.release()

      self._sum_distance = distance
      self._sum_time = (t[0], t[1])
      self._sum_wavelength = self.wavelength

      if self._have_max:
        self._max_img = self.cspad_img.deep_copy()
      if self._have_mean:
        self._sum_img = self.cspad_img.deep_copy()
      if self._have_std:
        self._ssq_img = flex.pow2(self.cspad_img)

    else:
      self._sum_distance += distance
      self._sum_time = (self._sum_time[0] + t[0], self._sum_time[1] + t[1])
      self._sum_wavelength += self.wavelength

      if self._have_max:
        sel = (self.cspad_img > self._max_img).as_1d()
        self._max_img.as_1d().set_selected(
          sel, self.cspad_img.as_1d().select(sel))
      if self._have_mean:
        self._sum_img += self.cspad_img
      if self._have_std:
        self._ssq_img += flex.pow2(self.cspad_img)

    self._nmemb += 1


  #signature for pyana:
  #def endjob(self, env):

  #signature for psana:
  #def endjob(self, evt, env):

  def endjob(self, obj1, obj2=None):
    """The endjob() function finalises the mean and standard deviation
    images.  The distance and wavelength in all images is actually the
    mean distance and wavelength, since standard deviations or maximum
    values of those quantities do not make much sense in
    visualisation.

    @param evt Event object (psana only)
    @param env Environment object
    @return    A dictionary object with accumulated statistics or @c
               none if the contribution from the worker process is
               accounted for elsewhere
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2
    from Queue import Empty

    super(average_mixin, self).endjob(env)

    # This entire function is protected by self._lock to guard against
    # race conditions.
    self._lock.acquire()

    # Attempt to get all the information from the shared objects,
    # without blocking.
    try:
      queue_max = self._queue_max.get_nowait()
      queue_sum = self._queue_sum.get_nowait()
      queue_ssq = self._queue_ssq.get_nowait()

      queue_distance = self._metadata['distance']
      queue_nfail = self._metadata['nfail']
      queue_nmemb = self._metadata['nmemb']
      queue_time = self._metadata['time']
      queue_wavelength = self._metadata['wavelength']
    except (Empty, KeyError):
      pass

    # If a complete set of items could be retrieved from the shared
    # objects, add them to this process's partial sums.  If only a
    # subset of the expected items could be retrieved from the shared
    # objects, log an error and proceed.
    items = [not self._have_max or 'queue_max' in locals(),
             not self._have_mean or 'queue_sum' in locals(),
             not self._have_std or 'queue_ssq' in locals(),
             'queue_distance' in locals(),
             'queue_nfail' in locals(),
             'queue_nmemb' in locals(),
             'queue_time' in locals(),
             'queue_wavelength' in locals()]
    if items.count(False) == 0:
      if self._have_max:
        if hasattr(self, '_max_img'):
          sel = (queue_max > self._max_img).as_1d()
          self._max_img.as_1d().set_selected(sel, queue_max.as_1d().select(sel))
        else:
          self._max_img = queue_max
      if self._have_mean:
        self._sum_img = getattr(self, '_sum_img', 0) + queue_sum
      if self._have_std:
        self._ssq_img = getattr(self, '_ssq_img', 0) + queue_ssq

      self._sum_distance = getattr(self, '_sum_distance', 0) + queue_distance
      self._nfail = getattr(self, '_nfail', 0) + queue_nfail
      self._nmemb = getattr(self, '_nmemb', 0) +  queue_nmemb
      self._sum_time = (getattr(self, '_sum_time', (0, 0))[0] + queue_time[0],
                        getattr(self, '_sum_time', (0, 0))[1] + queue_time[1])
      self._sum_wavelength = getattr(self, '_sum_wavelength', 0) + \
                             queue_wavelength

    elif items.count(True) > 0:
      self.logger.error("Queue holds incomplete set of data items")

    # Clear the bit field for the worker process.
    if env.subprocess() >= 0:
      self._metadata['peers'] &= ~(1 << env.subprocess())

    if self._metadata.get('peers', -1) > 0:
      # There are other processes left.  Place the accumulated sums
      # back in the shared objects.  If this ever blocks, the buffer
      # size of the queues will probably have to be increased.
      if self._nmemb > 0:
        self._metadata['distance'] = self._sum_distance
        self._metadata['nfail'] = self._nfail
        self._metadata['nmemb'] = self._nmemb
        self._metadata['time'] = self._sum_time
        self._metadata['wavelength'] = self._sum_wavelength

        if self._have_max:
          self._queue_max.put(self._max_img)
        if self._have_mean:
          self._queue_sum.put(self._sum_img)
        if self._have_std:
          self._queue_ssq.put(self._ssq_img)

      self._lock.release()
      return None

    # This is the last worker process, all others must have
    # contributed their partial sums.  Finalise the max, mean, and
    # standard deviation images if requested.
    d = {'nfail': self._nfail,
         'nmemb': self._nmemb}
    if self._nmemb > 0:
      d['distance'] = self._sum_distance / self._nmemb
      d['time'] = (
        self._metadata['time_base'][0] +
        int(round(self._sum_time[0] / self._nmemb)),
        self._metadata['time_base'][1] +
        int(round(self._sum_time[1] / self._nmemb)))
      d['wavelength'] = self._sum_wavelength / self._nmemb

      if self._have_max:
        d['max_img'] = self._max_img
      if self._have_mean or self._have_std:
        mean_img = self._sum_img / self._nmemb
      if self._have_mean:
        d['mean_img'] = mean_img
      if self._have_std:
        # Accumulating floating-point numbers introduces errors,
        # which may cause negative variances.  Since a two-pass
        # approach is unacceptable, the standard deviation is
        # clamped at zero.
        d['std_img'] = self._ssq_img - self._sum_img * mean_img
        d['std_img'].set_selected(d['std_img'] < 0, 0)
        if self._nmemb == 1:
          d['std_img'] = flex.sqrt(d['std_img'])
        else:
          d['std_img'] = flex.sqrt(d['std_img'] / (self._nmemb - 1))

    self._lock.release()
    return d


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/common_mode.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# $Id$

"""Base class for dark subtraction and common-mode correction

XXX Better named cspad_base?
"""
from __future__ import absolute_import, division, print_function
from six.moves import range
from six.moves import zip

__version__ = "$Revision$"

import numpy
from .parse_calib         import Section
from .parse_calib         import calib2sections

from libtbx import easy_pickle
from scitbx.array_family import flex
from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import skip_event_flag
from xfel.cxi.cspad_ana.mod_event_info import mod_event_info
from serialtbx.detector.xtc import old_address_to_new_address


class common_mode_correction(mod_event_info):
  """Dark subtraction and alternate implementation of common mode
  substituting for cspad_tbx.

  Known problems: the algorithm relies on a substantial part of the
  sensor having no signal, which is not the case if a water ring
  crosses the sensor.
  """


  def __init__(self,
               address,
               calib_dir=None,
               common_mode_correction="none",
               photon_threshold=None,
               two_photon_threshold=None,
               dark_path=None,
               dark_stddev=None,
               mask_path=None,
               gain_map_path=None,
               gain_map_level=None,
               cache_image=True,
               roi=None,
               laser_1_status=None,
               laser_4_status=None,
               laser_wait_time=None,
               override_beam_x=None,
               override_beam_y=None,
               bin_size=None,
               crop_rayonix=False,
               **kwds):
    """The common_mode_correction class constructor stores the
    parameters passed from the pyana configuration file in instance
    variables.

    @param address         Full data source address of the DAQ device
    @param calib_dir       Directory with calibration information
    @param common_mode_correction The type of common mode correction to apply
    @param dark_path       Path to input average dark image
    @param dark_stddev     Path to input standard deviation dark
                           image, required if @p dark_path is given
    @param mask_path       Path to input mask.  Pixels to mask out should be set to -2
    @param gain_map_path   Path to input gain map.  Multiplied times the image.
    @param gain_map_level  If set, all the '1' pixels in the gain_map are set to this multiplier
                           and all the '0' pixels in the gain_map are set to '1'.  If not set,
                           use the values in the gain_map directly
    @param laser_1_status  0 or 1 to indicate that the laser should be off or on respectively
    @param laser_4_status  0 or 1 to indicate that the laser should be off or on respectively
    @param laser_wait_time Length of time in milliseconds to wait after a laser
                           change of status to begin accepting images again.
                           (rejection of images occurs immediately after status
                           change).
    @param override_beam_x override value for x coordinate of beam center in pixels
    @param override_beam_y override value for y coordinate of beam center in pixels
    @param bin_size bin size for rayonix detector used to determin pixel size
    @param crop_rayonix    whether to crop rayonix images such that image center is the beam center
    """

    # Cannot use the super().__init__() construct here, because
    # common_mode_correction refers to the argument, and not the
    # class.
    mod_event_info.__init__(self, address=address, **kwds)

    # The paths will be substituted in beginjob(), where evt and env
    # are available.
    self._dark_path = cspad_tbx.getOptString(dark_path)
    self._dark_stddev_path = cspad_tbx.getOptString(dark_stddev)
    self._gain_map_path = cspad_tbx.getOptString(gain_map_path)
    self._mask_path = cspad_tbx.getOptString(mask_path)

    self.gain_map_level = cspad_tbx.getOptFloat(gain_map_level)
    self.common_mode_correction = cspad_tbx.getOptString(common_mode_correction)
    self.photon_threshold = cspad_tbx.getOptFloat(photon_threshold)
    self.two_photon_threshold = cspad_tbx.getOptFloat(two_photon_threshold)
    self.cache_image = cspad_tbx.getOptBool(cache_image)
    self.filter_laser_1_status = cspad_tbx.getOptInteger(laser_1_status)
    self.filter_laser_4_status = cspad_tbx.getOptInteger(laser_4_status)
    if self.filter_laser_1_status is not None:
      self.filter_laser_1_status = bool(self.filter_laser_1_status)
    if self.filter_laser_4_status is not None:
      self.filter_laser_4_status = bool(self.filter_laser_4_status)
    self.filter_laser_wait_time = cspad_tbx.getOptInteger(laser_wait_time)
    self.override_beam_x = cspad_tbx.getOptFloat(override_beam_x)
    self.override_beam_y = cspad_tbx.getOptFloat(override_beam_y)
    self.bin_size = cspad_tbx.getOptInteger(bin_size)
    self.crop_rayonix = cspad_tbx.getOptBool(crop_rayonix)

    self.cspad_img = None # The current image - set by self.event()
    self.sum_common_mode = 0
    self.sumsq_common_mode = 0
    self.roi = cspad_tbx.getOptROI(roi) # used to ignore the signal region in chebyshev fit

    assert self.common_mode_correction in \
        ("gaussian", "mean", "median", "mode", "none", "chebyshev")

    # Get and parse metrology.
    self.sections = None
    device = cspad_tbx.address_split(self.address)[2]
    if device == 'Andor':
      self.sections = [] # XXX FICTION
    elif device == 'Cspad':
      if self.address == 'XppGon-0|Cspad-0':
        self.sections = [] # Not used for XPP
      else:
        self.sections = calib2sections(cspad_tbx.getOptString(calib_dir))
    elif device == 'Cspad2x2':
      # There is no metrology information for the Sc1 detector, so
      # make it up.  The sections are rotated by 90 degrees with
      # respect to the "standing up" convention.
      self.sections = [[Section(90, (185 / 2 + 0,   (2 * 194 + 3) / 2)),
                        Section(90, (185 / 2 + 185, (2 * 194 + 3) / 2))]]
    elif device == 'marccd':
      self.sections = [] # XXX FICTION
    elif device == 'pnCCD':
      self.sections = [] # XXX FICTION
    elif device == 'Rayonix':
      self.sections = [] # XXX FICTION
    elif device == 'Opal1000':
      self.sections = [] # XXX FICTION
    if self.sections is None:
      raise RuntimeError("Failed to load metrology")


  def beginjob(self, evt, env):
    """The beginjob() function does one-time initialisation from
    event- or environment data.  It is called at an XTC configure
    transition.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    super(common_mode_correction, self).beginjob(evt, env)
    # Load the dark image and ensure it is signed and at least 32 bits
    # wide, since it will be used for differencing.  If a dark image
    # is provided, a standard deviation image is required, and all the
    # ADU scales must match up.
    #
    # XXX Can we zap all the ADU_SCALE stuff?
    #
    # XXX Do we really need to store the substituted values in the
    # instance here?  At least self._dark_path is referenced later on.
    #
    # Note that this will load the dark, standard deviation and gain
    # once (SEVERAL TIMES) for each process, but the gain is that we
    # can do substitutions.  But it is only done at the beginning of
    # the job.
    self.dark_img = None
    if self._dark_path is not None:
      self._dark_path = cspad_tbx.getOptEvalOrString(
        cspad_tbx.pathsubst(self._dark_path, evt, env))

      assert self._dark_stddev_path is not None
      dark_dict = easy_pickle.load(self._dark_path)
      #assert "ADU_SCALE" not in dark_dict # force use of recalculated dark
      self.dark_img = dark_dict['DATA']
      assert isinstance(self.dark_img, flex.double)

      self._dark_stddev_path = cspad_tbx.getOptEvalOrString(
        cspad_tbx.pathsubst(self._dark_stddev_path, evt, env))

      self.dark_stddev = easy_pickle.load(self._dark_stddev_path)['DATA']
      assert isinstance(self.dark_stddev, flex.double)
      self.dark_mask = (self.dark_stddev > 0)

    # Load the mask image and ensure it is signed and at least 32 bits
    # wide, since it will be used for differencing.
    self.gain_map = None
    if self._gain_map_path is not None:
      self._gain_map_path = cspad_tbx.getOptEvalOrString(
        cspad_tbx.pathsubst(self._gain_map_path, evt, env))
      self.gain_map = easy_pickle.load(self._gain_map_path)['DATA']
      if self.gain_map_level is not None:
        sel = flex.bool([bool(f) for f in self.gain_map])
        sel.reshape(flex.grid(self.gain_map.focus()))
        self.gain_map = self.gain_map.set_selected(~sel, self.gain_map_level)
        self.gain_map = self.gain_map.set_selected(sel, 1)
      assert isinstance(self.gain_map, flex.double)

    self.mask_img = None
    if self._mask_path is not None:
      self._mask_path = cspad_tbx.getOptEvalOrString(
        cspad_tbx.pathsubst(self._mask_path, evt, env))

      self.mask_img = easy_pickle.load(self._mask_path)['DATA']
      assert isinstance(self.mask_img, flex.double) \
        or   isinstance(self.mask_img, flex.int)

    if self.address == 'XppGon-0|marccd-0':
      #mod_mar.py will set these during its event function
      self.active_areas = None
      self.beam_center = None
    elif self.address == 'XppEndstation-0|Rayonix-0' or \
         self.address == 'MfxEndstation-0|Rayonix-0':
      assert self.override_beam_x is not None
      assert self.override_beam_y is not None
      from xfel.cxi.cspad_ana import rayonix_tbx
      maxx, maxy = rayonix_tbx.get_rayonix_detector_dimensions(env)
      if self.crop_rayonix:
        bx = int(round(self.override_beam_x))
        by = int(round(self.override_beam_y))
        minsize = min([bx,by,maxx-bx,maxy-by])
        self.beam_center = minsize,minsize
        self.active_areas = flex.int([0,0,2*minsize,2*minsize])
        self.rayonix_crop_slice = slice(by-minsize,by+minsize), slice(bx-minsize,bx+minsize)
      else:
        self.beam_center = self.override_beam_x,self.override_beam_y
        self.active_areas = flex.int([0,0,maxx,maxy])
    elif self.address == 'XppGon-0|Cspad-0':
      evt_time = cspad_tbx.evt_time(evt) # tuple of seconds, milliseconds
      timestamp = cspad_tbx.evt_timestamp(evt_time) # human readable format
      from iotbx.detectors.cspad_detector_formats import detector_format_version, reverse_timestamp
      from xfel.cxi.cspad_ana.cspad_tbx import xpp_active_areas
      version_lookup = detector_format_version(self.address, reverse_timestamp(timestamp)[0])
      assert version_lookup is not None
      self.active_areas = xpp_active_areas[version_lookup]['active_areas']
      self.beam_center = [1765 // 2, 1765 // 2]
    else:
      (self.beam_center, self.active_areas) = cspad_tbx.cbcaa(
        cspad_tbx.getConfig(self.address, env), self.sections)

  def common_mode(self, img, stddev, mask):
    """The common_mode() function returns the mode of image stored in
    the array pointed to by @p img.  @p mask must be such that the @p
    stddev at the selected pixels is greater than zero.

    @param img    2D integer array of the image
    @param stddev 2D integer array of the standard deviation of each
                  pixel in @p img
    @param mask   2D Boolean array, @c True if the pixel is to be
                  included, @c False otherwise
    @return       Mode of the image, as a real number
    """

    # Flatten the image and take out inactive pixels XXX because we
    # cannot take means and medians of 2D arrays?
    img_1d = img.as_1d().select(mask.as_1d()).as_double()
    assert img_1d.size() > 0

    if (self.common_mode_correction == "mean"):
      # The common mode is approximated by the mean of the pixels with
      # signal-to-noise ratio less than a given threshold.  XXX Breaks
      # if the selection is empty!
      THRESHOLD_SNR = 2
      img_snr = img_1d / stddev.as_double().as_1d().select(mask.as_1d())
      return (flex.mean(img_1d.select(img_snr < THRESHOLD_SNR)))

    elif (self.common_mode_correction == "median"):
      return (flex.median(img_1d))

    # Identify the common-mode correction as the peak histogram of the
    # histogram of pixel values (the "standard" common-mode correction, as
    # previously implemented in this class).
    hist_min = -40
    hist_max = 40
    n_slots = 100

    hist = flex.histogram(img_1d, hist_min, hist_max, n_slots=n_slots)
    slots = hist.slots()
    i = flex.max_index(slots)
    common_mode = list(hist.slot_infos())[i].center()

    if (self.common_mode_correction == "mode"):
      return (common_mode)

    # Determine the common-mode correction from the peak of a single
    # Gaussian function fitted to the histogram.
    from scitbx.math.curve_fitting import single_gaussian_fit
    x = hist.slot_centers()
    y = slots.as_double()
    fit = single_gaussian_fit(x, y)
    scale, mu, sigma = fit.a, fit.b, fit.c
    self.logger.debug("fitted gaussian: mu=%.3f, sigma=%.3f" %(mu, sigma))
    mode = common_mode
    common_mode = mu
    if abs(mode-common_mode) > 1000: common_mode = mode # XXX
    self.logger.debug("delta common mode corrections: %.3f" %(mode-common_mode))

    if 0 and abs(mode-common_mode) > 0:
      #if 0 and skew > 0.5:
      # view histogram and fitted gaussian
      from numpy import exp
      from matplotlib import pyplot
      x_all = x
      n, bins, patches = pyplot.hist(section_img.as_1d().as_numpy_array(), bins=n_slots, range=(hist_min, hist_max))
      y_all = scale * flex.exp(-flex.pow2(x_all-mu) / (2 * sigma**2))
      scale = slots[flex.max_index(slots)]
      y_all *= scale/flex.max(y_all)
      pyplot.plot(x_all, y_all)
      pyplot.show()

    return (common_mode)


  def event(self, evt, env):
    """The event() function is called for every L1Accept transition.

    @param evt Event data object, a configure object
    @param env Environment object
    """
    super(common_mode_correction, self).event(evt, env)
    if (evt.get("skip_event")):
      return

    if not hasattr(self, 'active_areas') or self.active_areas is None or \
       not hasattr(self, 'beam_center')  or self.beam_center  is None:
      if self.address == 'XppGon-0|marccd-0':
        # The mod_mar module needs to have been called before this one
        # to set this up.  The MAR does not have a configure object.
        self.beam_center = evt.get("marccd_beam_center")
        self.active_areas = evt.get("marccd_active_areas")
      elif self.address == 'XppEndstation-0|Rayonix-0' or \
           self.address == 'MfxEndstation-0|Rayonix-0':
        pass # bc and aa set in the beginjob function
      elif self.address == 'XppGon-0|Cspad-0':
        # Load the active areas as determined from the optical metrology
        from iotbx.detectors.cspad_detector_formats import detector_format_version, reverse_timestamp
        from xfel.cxi.cspad_ana.cspad_tbx import xpp_active_areas
        version_lookup = detector_format_version(self.address, reverse_timestamp(self.timestamp)[0])
        assert version_lookup is not None
        self.active_areas = xpp_active_areas[version_lookup]['active_areas']
        self.beam_center = [1765 // 2, 1765 // 2]
      else:
        (self.beam_center, self.active_areas) = \
          cspad_tbx.cbcaa(cspad_tbx.getConfig(self.address, env), self.sections)

    if self.filter_laser_1_status is not None:
      if (self.laser_1_status.status != self.filter_laser_1_status or
          (self.laser_1_ms_since_change is not None and
           self.laser_1_ms_since_change < self.filter_laser_wait_time)):
        evt.put(skip_event_flag(), "skip_event")
        return
    if self.filter_laser_4_status is not None:
      if (self.laser_4_status.status != self.filter_laser_4_status or
          (self.laser_4_ms_since_change is not None and
           self.laser_4_ms_since_change < self.filter_laser_wait_time)):
        evt.put(skip_event_flag(), "skip_event")
        return

    # Early return if the full detector image is already stored in the
    # event.  Otherwise, get it from the stream as a double-precision
    # floating-point flex array.  XXX It is probably not safe to key
    # the image on self.address, so we should come up with our own
    # namespace.  XXX Misnomer--could be CAMP, too

    self.cspad_img = evt.get(self.address)
    if self.cspad_img is not None:
      return
    if self.address == 'XppGon-0|Cspad-0':
      # Kludge until cspad_tbx.image() can be rewritten to handle the
      # XPP metrology.
      self.cspad_img = cspad_tbx.image_xpp(
        self.address, evt, env, self.active_areas)
    elif self.address == 'XppEndstation-0|Rayonix-0' or \
         self.address == 'MfxEndstation-0|Rayonix-0':
      from psana import Source, Camera
      import numpy as np
      address = old_address_to_new_address(self.address)
      src=Source('DetInfo(%s)'%address)
      self.cspad_img = evt.get(Camera.FrameV1,src)
      if self.cspad_img is not None:
        self.cspad_img = self.cspad_img.data16().astype(np.float64)
    elif self.address=='CxiDg3-0|Opal1000-0':
      if evt.getFrameValue(self.address) is not None:
        self.cspad_img = evt.getFrameValue(self.address).data()
    elif self.address=='CxiEndstation-0|Opal1000-2':
      if evt.getFrameValue(self.address) is not None:
        self.cspad_img = evt.getFrameValue(self.address).data()
    elif self.address=='FeeHxSpectrometer-0|Opal1000-1':
      if evt.getFrameValue(self.address) is not None:
        self.cspad_img = evt.getFrameValue(self.address).data()
    elif self.address=='NoDetector-0|Cspad2x2-0':
        import numpy as np
        from pypdsdata import xtc
        test=[]
        self.cspad_img = evt.get(xtc.TypeId.Type.Id_Cspad2x2Element,self.address).data()
        self.cspad_img=np.reshape(self.cspad_img,(370, 388))
    else:
      try:
        self.cspad_img = cspad_tbx.image(
          self.address, cspad_tbx.getConfig(self.address, env),
          evt, env, self.sections)
      except Exception as e:
        self.logger.error("Error reading image data: " + str(e))
        evt.put(skip_event_flag(), "skip_event")
        return

    if self.cspad_img is None:
      if cspad_tbx.address_split(self.address)[2] != 'Andor':
        self.nfail += 1
        self.logger.warning("event(): no image, shot skipped")
        evt.put(skip_event_flag(), "skip_event")
      return
    self.cspad_img = flex.double(self.cspad_img.astype(numpy.float64))
    # If a dark image was provided, subtract it from the image.  There
    # is no point in doing common-mode correction unless the dark
    # image was subtracted.
    if (self.dark_img is not None):
      self.cspad_img -= self.dark_img

      if (self.common_mode_correction != "none"):
        # Mask out inactive pixels prior to common mode correction.
        # Pixels are marked as inactive either due to low ADU values
        # or non-positive standard deviations in dark image.  XXX Make
        # the threshold tunable?
        cspad_mask = self.dark_mask.deep_copy()

        if self.roi is not None and self.common_mode_correction == "chebyshev":
          roi_mask = cspad_mask[self.roi[2]:self.roi[3], :]
          roi_mask = flex.bool(roi_mask.accessor(), False)
          cspad_mask.matrix_paste_block_in_place(
            block=roi_mask,
            i_row=self.roi[2],
            i_column=0)

        # Extract each active section from the assembled detector
        # image and apply the common mode correction.  XXX Make up a
        # quadrant mask for the emission detector.  Needs to be
        # checked!
        config = cspad_tbx.getConfig(self.address, env)
        if len(self.sections) == 1:
          q_mask = 1
        else:
          q_mask = config.quadMask()
        for q in range(len(self.sections)):
          if (not((1 << q) & q_mask)):
            continue

          # XXX Make up section mask for the emission detector.  Needs
          # to be checked!
          import _pdsdata
          if len(self.sections) == 1 and type(config) in (
            _pdsdata.cspad2x2.ConfigV1, _pdsdata.cspad2x2.ConfigV2):
            s_mask = config.roiMask()
          else:
            s_mask = config.roiMask(q)
          for s in range(len(self.sections[q])):
            # XXX DAQ misconfiguration?  This mask appears not to work
            # reliably for the Sc1 detector.
#            if (not((1 << s) & s_mask)):
#              continue
            corners   = self.sections[q][s].corners()
            i_row     = int(round(min(c[0] for c in corners)))
            i_column  = int(round(min(c[1] for c in corners)))
            n_rows    = int(round(max(c[0] for c in corners))) - i_row
            n_columns = int(round(max(c[1] for c in corners))) - i_column

            section_img    = self.cspad_img.matrix_copy_block(
              i_row  = i_row,  i_column  = i_column,
              n_rows = n_rows, n_columns = n_columns)
            section_mask   = cspad_mask.matrix_copy_block(
              i_row  = i_row,  i_column  = i_column,
              n_rows = n_rows, n_columns = n_columns)
            section_stddev = self.dark_stddev.matrix_copy_block(
              i_row  = i_row,  i_column  = i_column,
              n_rows = n_rows, n_columns = n_columns)

            if section_mask.count(True) == 0: continue

            if self.common_mode_correction == "chebyshev":
              assert len(self.sections[q]) == 2
              if s == 0:
                section_imgs = [section_img]
                section_masks = [section_mask]
                i_rows = [i_row]
                i_columns = [i_column]
                continue
              else:
                section_imgs.append(section_img)
                section_masks.append(section_mask)
                i_rows.append(i_row)
                i_columns.append(i_column)

                chebyshev_corrected_imgs = self.chebyshev_common_mode(
                  section_imgs, section_masks)
                for i in range(2):
                  section_imgs[i].as_1d().copy_selected(
                    section_masks[i].as_1d().iselection(),
                    chebyshev_corrected_imgs[i].as_1d())
                  self.cspad_img.matrix_paste_block_in_place(
                    block=section_imgs[i],
                    i_row=i_rows[i],
                    i_column=i_columns[i])

            else:
              common_mode = self.common_mode(
                section_img, section_stddev, section_mask)
              self.sum_common_mode += common_mode
              self.sumsq_common_mode += common_mode**2

              # Apply the common mode correction to the
              # section, and paste it back into the image.
              self.cspad_img.matrix_paste_block_in_place(
                block    = section_img - common_mode,
                i_row    = i_row,
                i_column = i_column)

    if self.gain_map is not None:
      self.cspad_img *= self.gain_map

    if (self.mask_img is not None):
      sel = (self.mask_img == -2 )|(self.mask_img == cspad_tbx.cspad_mask_value)
      self.cspad_img.set_selected(sel, cspad_tbx.cspad_mask_value)

    if (self.address == 'XppEndstation-0|Rayonix-0' or \
        self.address == 'MfxEndstation-0|Rayonix-0') and \
        self.crop_rayonix:
      # Crop the masked data so that the beam center is in the center of the image
      self.cspad_img = self.cspad_img[self.rayonix_crop_slice[0], self.rayonix_crop_slice[1]]

    if self.cache_image:
      # Store the image in the event.
      evt.put(self.cspad_img, self.address)


  #signature for pyana:
  #def endjob(self, env):

  #signature for psana:
  #def endjob(self, evt, env):

  def endjob(self, obj1, obj2=None):
    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    if 0 and self._dark_path is not None and self.nmemb > 1:
      print(self.sum_common_mode, self.sumsq_common_mode)
      self.mean_common_mode = self.sum_common_mode / self.nmemb
      print(self.mean_common_mode)
      self.stddev_commond_mode = math.sqrt((self.sumsq_common_mode
        - self.sum_common_mode * self.mean_common_mode) / (self.nmemb - 1))

      self.logger.info("mean common mode: %.3f" %self.mean_common_mode)
      self.logger.info("std. dev. common mode: %.3f" %self.stddev_commond_mode)


  def do_sigma_scaling(self):
    # Divide each pixel value by it's dark standard deviation.  Since we are led
    # to believe that the standard deviation of a pixel is proportional to the
    # gain of said pixel, this approximates a gain correction.
    assert self.dark_img is not None
    assert self.gain_map is None # not appropriate to do sigma scaling and gain correction at the same time!
    flex_cspad_img = self.cspad_img.as_double()
    flex_cspad_img_sel = flex_cspad_img.as_1d().select(self.dark_mask.as_1d())
    flex_dark_stddev = self.dark_stddev.select(self.dark_mask.as_1d()).as_double()
    assert flex_dark_stddev.count(0) == 0
    flex_dark_stddev /= flex.mean(flex_dark_stddev)
    flex_cspad_img_sel /= flex_dark_stddev
    flex_cspad_img.as_1d().set_selected(self.dark_mask.as_1d().iselection(), flex_cspad_img_sel)
    self.cspad_img = flex_cspad_img
    if 0: # for debugging
      from matplotlib import pyplot
      hist_min, hist_max = flex.min(flex_cspad_img_sel.as_double()), flex.max(flex_cspad_img_sel.as_double())
      print(hist_min, hist_max)
      n_slots = 100
      n, bins, patches = pyplot.hist(flex_cspad_img_sel.as_1d().as_numpy_array(), bins=n_slots, range=(hist_min, hist_max))
      pyplot.show()


  def chebyshev_common_mode(self, imgs, masks):
    assert len(imgs) == 2
    assert len(masks) == 2
    corrected_imgs = []
    # first fit the variation along the columns of the detector
    sum_y = flex.double()
    for i, (img, mask) in enumerate(zip(imgs, masks)):
      img -= flex.mean(img)
      masked_img = img.deep_copy()
      masked_img.set_selected(~mask, 0.)
      rows, columns = masked_img.all()
      sum_y.extend(flex.sum(masked_img, axis=1))
    # if the region of interest is across a row, use the noise from the
    # same row on the other section
    midpoint = sum_y.size()//2
    for i in range(0, midpoint):
      if sum_y[i] == 0:
        sum_y[i] == sum_y[i + midpoint]
    for i in range(midpoint, sum_y.size()):
      if sum_y[i] == 0:
        sum_y[i] == sum_y[midpoint - i]
    # we assume that both sections have the same variation
    y_obs = sum_y[:midpoint] + sum_y[midpoint:]
    y_obs /= (2 * columns)
    x_obs = flex.double(range(y_obs.size()))
    w_obs = flex.double(x_obs.size(), 1)
    # don't let the edge pixels influence the fit
    w_obs[0] = 1e16
    w_obs[-1] = 1e16
    y_fitted = self.chebyshev_fit(x_obs, y_obs, w_obs, n_terms=5)

    y_correction = flex.double()
    for i in range(columns):
      y_correction.extend(y_fitted)
    y_correction.reshape(flex.grid(columns, rows))
    y_correction.matrix_transpose_in_place()
    if 0:
      from matplotlib import pyplot
      pyplot.imshow(y_correction.as_numpy_array())
      pyplot.show()

    # now fit the variation along the rows
    n_terms = 10
    for img, mask in zip(imgs, masks):
      img -= y_correction
      masked_img = img.deep_copy()
      masked_img.set_selected(~mask, 0.)
      n_masked_rows = flex.sum(masked_img, axis=1).count(0)
      rows, columns = masked_img.all()
      sum_x = flex.sum(masked_img, axis=0)
      sum_x /= (rows - n_masked_rows)
      assert img.all() == (185, 391) # XXX
      # fit one polynome for both asics
      x_obs = sum_x[:194] + sum_x[197:]
      x_obs /= 2
      y_obs = flex.double(range(x_obs.size()))
      w_obs = flex.double(y_obs.size(), 1)
      # mask out the edges and the gap down the middle from the fit
      w_obs.set_selected(y_obs == 0, 1e16)
      w_obs[0] = 1e16
      w_obs[-1] = 1e16
      x_fitted = self.chebyshev_fit(y_obs, x_obs, w_obs, n_terms=10)
      x_calc = x_fitted.deep_copy()
      x_calc.extend(flex.double([0,0,0])) # The 3 pixel gap between asics
      x_calc.extend(x_fitted)

      correction = flex.double()
      for i in range(rows):
        correction.extend(x_calc)
      correction.reshape(img.accessor())
      zero_pixels_sel = (img == 0)
      img -= correction
      if 0:
        from matplotlib import pyplot
        pyplot.imshow(correction.as_numpy_array())
        pyplot.show()

      img.set_selected(zero_pixels_sel, 0)
      corrected_imgs.append(img)

    return corrected_imgs


  def chebyshev_fit(self, x_obs, y_obs, w_obs, n_terms=None):
    from scitbx.math import chebyshev_polynome
    from scitbx.math import chebyshev_lsq_fit
    if n_terms is None:
      # determining the number of terms takes much, much longer than the fit
      n_terms = chebyshev_lsq_fit.cross_validate_to_determine_number_of_terms(
        x_obs, y_obs, w_obs,
        min_terms=5, max_terms=20,
        n_goes=20, n_free=20)
    self.logger.info("Fitting with %i terms" %n_terms)
    fit = chebyshev_lsq_fit.chebyshev_lsq_fit(n_terms, x_obs, y_obs, w_obs)
    self.logger.info("Least Squares residual: %7.6f" %(fit.f))
    fit_funct = chebyshev_polynome(
      n_terms, fit.low_limit, fit.high_limit, fit.coefs)
    y_fitted = fit_funct.f(x_obs)
    if 0:
      # debugging plots
      from matplotlib import pyplot
      pyplot.clf()
      pyplot.plot(x_obs, y_obs)
      pyplot.plot(x_obs, y_fitted)
      pyplot.draw()
      pyplot.show()
    return y_fitted


  def do_photon_counting(self):
    # This only makes sense in combination with some sort of gain correction
    # XXX TODO: count 2, 3, 4, ..., photons
    PHOTON_THRESHOLD = self.photon_threshold
    TWO_PHOTON_THRESHOLD = self.two_photon_threshold
    if [PHOTON_THRESHOLD, TWO_PHOTON_THRESHOLD].count(None) > 0:
      self.logger.info("Skipping photon counting: photon_threshold is not defined")
      return
    self.cspad_img.set_selected(self.cspad_img<PHOTON_THRESHOLD, 0)
    self.cspad_img.set_selected(self.cspad_img>=TWO_PHOTON_THRESHOLD, 2)
    self.cspad_img.set_selected(self.cspad_img>=PHOTON_THRESHOLD, 1)
    self.logger.debug("zero photon counts: %i" %self.cspad_img.count(0))
    self.logger.debug("one photon counts: %i" %self.cspad_img.count(1))
    self.logger.debug("two photon counts: %i" %self.cspad_img.count(2))
    self.logger.info("No. photons: %i" %flex.sum(self.cspad_img))
    s, ms = self.evt_time
    evt_time = s + ms/1000
    self.stats_logger.info("N_PHOTONS %.3f %s" %(evt_time, flex.sum(self.cspad_img)))


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/cspad_tbx.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# $Id$

"""Toolbox for images from the Cornell SLAC Pixel Array Detector
(CSpad).

XXX Better named cspad_common?

XXX Read out detector temperature (see Hart et al., 2012)?
"""
from __future__ import absolute_import, division, print_function
from six.moves import range

import math
import numpy
import os
import time

from libtbx import easy_pickle
from scitbx.array_family import flex
from xfel.cxi.cspad_ana.parse_calib import Section
import six
from six.moves import zip

import serialtbx.util.time
import serialtbx.detector.cspad
from serialtbx.detector.cspad import pixel_size
from serialtbx.detector.xtc import old_address_to_new_address, get_ebeam, env_detz, address_split # import dependency

__version__ = "$Revision$"


# As long as the mask value is outside of the trusted range, the pixel should
# be ignored by any downstream software.
cspad_mask_value = -100000

# The side length of a square quadrant from the old XtcExplorer code.
# XXX This should be obsoleted!
npix_quad = 850

# origin of section in quad coordinate system.  x-position
# correspond to column number.  XXX Note/reference the source!
# XXX This should be obsoleted!
xpos_sec2x1 = [[ 414,  626,    0,    0,  213,    1,  418,  419],  # 2:5 were not measured
               [ 421,  634,    0,    0,  213,    1,  424,  425],
               [ 417,  630,    0,    1,  212,    0,  425,  426],
               [ 416,  630,    0,    0,  213,    1,  420,  421]] # 2:5 were not measured
# y-position correspond to maxrows - row number
ypos_sec2x1 = [[   0,    0,  214,    1,  425,  425,  615,  402],  # 2:5 were not measured
               [   0,    0,  214,    1,  425,  425,  615,  402],
               [   0,    0,  215,    3,  431,  431,  616,  403],
               [   0,    0,  214,    1,  425,  425,  615,  403]] # 2:5 were not measured


def cbcaa(config, sections):
  """The cbcaa() function uses on-disk calibration data to estimate
  the beam centre and the active detector areas.  The beam centre is
  approximated as the average of the four ASIC corners closest to the
  detector centre.  That is the first corner of the section 1 in every
  quadrant.  Note that first corner index is vertical coordinate,
  second index is the horizontal coordinate.  XXX Construct the active
  areas in "spotfinder format", i.e. opposing corners.  XXX This is a
  really bad function name!  XXX The beam centre may be extracted from
  the ebeam object?

  @param config   XXX
  @param sections XXX Directory with calibration information
  @return         Tuple of 2D beam centre, and active areas in
                  "spotfinder format"
  """

  aa = flex.int()
  if (sections is None):
    # The active areas of the detector, (UL_slow, UL_fast, LR_slow,
    # LR_fast) A two-by-one is 185-by-392 pixels with a 4-pixel gap.
    # An ASIC is 185-by-194 pixels.  XXX Still need to sort out the
    # inclusive/exclusive detail.  Upper-left corner is inclusive,
    # lower-right corner is exclusive.  XXX Should subtract one from
    # x, y on lower-right corner and verify with zoom.  XXX All this
    # should probably go by now
    for q in range(4): # loop over quadrants
      for i in range(8): # loop over two-by-one:s XXX variable name!

        # Skip this two-by-one if it is missing.
        if not (config.roiMask(q) & 0x1 << i):
          continue

        # XXX Note the different coordinate systems in use here!
        xpos =       xpos_sec2x1[q][i] # x-value of lower, left corner
        ypos = 850 - ypos_sec2x1[q][i] # y-value of lower, left corner

        if (i == 0 or i == 1 or i == 4 or i == 5):
          UL1_x = xpos
          UL2_x = xpos
          UL1_y = ypos - 194 - 4 - 194
          UL2_y = ypos - 194

          LR1_x = UL1_x + 185
          LR2_x = UL2_x + 185
          LR1_y = UL1_y + 194
          LR2_y = UL2_y + 194

        elif (i == 2 or i == 3 or i == 6 or i == 7):
          UL1_x = xpos
          UL2_x = xpos + 194 + 4
          UL1_y = ypos - 185
          UL2_y = ypos - 185

          LR1_x = UL1_x + 194
          LR2_x = UL2_x + 194
          LR1_y = UL1_y + 185
          LR2_y = UL2_y + 185

        # Quadrant rotations, counter-clockwise.  Zeroth quadrant
        # needs no special action.
        if (q == 0):
          pass

        elif (q == 1):
          UL1_x, UL1_y               = 850 + 850 - UL1_y, UL1_x
          LR1_x, LR1_y               = 850 + 850 - LR1_y, LR1_x

          UL2_x, UL2_y               = 850 + 850 - UL2_y, UL2_x
          LR2_x, LR2_y               = 850 + 850 - LR2_y, LR2_x

          UL1_x, LR1_x               = LR1_x, UL1_x
          UL2_x, LR2_x               = LR2_x, UL2_x

        elif (q == 2):
          UL1_x, UL1_y               = 850 + 850 - UL1_x, 850 + 850 - UL1_y
          LR1_x, LR1_y               = 850 + 850 - LR1_x, 850 + 850 - LR1_y

          UL2_x, UL2_y               = 850 + 850 - UL2_x, 850 + 850 - UL2_y
          LR2_x, LR2_y               = 850 + 850 - LR2_x, 850 + 850 - LR2_y

          UL1_x, UL1_y, LR1_x, LR1_y = LR1_x, LR1_y, UL1_x, UL1_y
          UL2_x, UL2_y, LR2_x, LR2_y = LR2_x, LR2_y, UL2_x, UL2_y

        elif (q == 3):
          UL1_x, UL1_y               = UL1_y, 850 + 850 - UL1_x
          LR1_x, LR1_y               = LR1_y, 850 + 850 - LR1_x

          UL2_x, UL2_y               = UL2_y, 850 + 850 - UL2_x
          LR2_x, LR2_y               = LR2_y, 850 + 850 - LR2_x

          UL1_y, LR1_y               = LR1_y, UL1_y
          UL2_y, LR2_y               = LR2_y, UL2_y

        # This is row-major matrix layout; FAST <=> x, SLOW <=> y.
        aa.extend(flex.int([UL1_y, UL1_x, LR1_y, LR1_x]))
        aa.extend(flex.int([UL2_y, UL2_x, LR2_y, LR2_x]))

    # The beam centre is estimated as the centre of the image.
    return ([npix_quad, npix_quad], aa)

  # Old way of computing beam center, phased out 05/19/15
  #bc     = [0, 0]

  # XXX Make up a quadrant mask for the emission detector.  Needs to
  # be checked!
  if len(sections) <= 1:
    q_mask = 1
  else:
    q_mask = config.quadMask()

  for q in range(len(sections)):
    if (not((1 << q) & q_mask)):
      continue

    # Old way of computing beam center, phased out 05/19/15
    #corner = sections[q][1].corners(True)[0]
    #bc     = [bc[0] + corner[1] / len(sections),
    #          bc[1] + corner[0] / len(sections)]

    # XXX Make up section mask for the emission detector.  Needs to be
    # checked!
    try:
      import _pdsdata
      types = _pdsdata.cspad2x2.ConfigV1, _pdsdata.cspad2x2.ConfigV2
    except ImportError:
      import psana
      types = psana.CsPad2x2.ConfigV1, psana.CsPad2x2.ConfigV2
    if len(sections) == 1 and type(config) in types:
      s_mask = config.roiMask()
    else:
      s_mask = config.roiMask(q)
    for s in range(len(sections[q])):
      if (not((1 << s) & s_mask)):
        continue
      c = sections[q][s].corners_asic()
      aa.extend(flex.int(c[0]))
      aa.extend(flex.int(c[1]))

  # The beam center was defined above as the center of the innermost 4 sensors. Recently,
  # that center has drifted too much from the true image center (Spring 2015). So, here we
  # use the true image center instead.
  return [882.5,882.5], aa


def CsPad2x2Image(data, config, sections):
  """The CsPad2x2Image() function assembles a two-dimensional image
  from the Sc1 detector readout in @p data.

  @param data     Detector readout from XTC stream
  @param config   XXX
  @param sections XXX Directory with calibration information
  @return         Assembled detector image
  """

  assert (data.shape[2] == 2)

  det  = numpy.zeros((2 * 185, 2 * 194 + 3))

  # XXX config.sections is now a function returning a list?  Since the
  # masking was disabled in December commenting out this bit does not
  # cause any further breakage XXX Does this still work for runs 4 and
  # 5?
#  s = config.sections
#  mask = map(s, range(2))

  # For this detector, the quadrant index is always zero.
  q_idx = 0
  for s in range(2):
    # XXX DAQ misconfiguration?  This mask appears not to work
    # reliably for the Sc1 detector.
#    if (s not in mask[q_idx]):
#      continue
    asics  = numpy.vsplit(numpy.rot90(data[:, :, s], -1), 2)
    gap    = numpy.zeros((3, 185), dtype = data.dtype)
    s_data = numpy.vstack((asics[0], gap, asics[1]))

    angle  = sections[q_idx][s].angle
    center = sections[q_idx][s].center
    rplace(det, s_data, angle, center)
  return (det)

def evt_get_quads(address, evt, env):
  try:
    # pyana
    quads = evt.getCsPadQuads(address, env)
  except AttributeError:
    # psana
    from psana import Source, CsPad
    src = Source(address)
    cspad = evt.get(CsPad.DataV2, src)
    if cspad is None:
      return None
    quads = [cspad.quads(i) for i in range(cspad.quads_shape()[0])]
  return quads

def CsPadDetector(address, evt, env, sections, right=True, quads=None):
  """The CsPadDetector() function assembles a two-dimensional image
  from the Ds1 detector readout in @p data3d and the calibration
  information in @p sections.  XXX General question: do
  variable/function names make sense?

  @param address  Full data source address of the DAQ device
  @param evt      Event data object, a configure object
  @param env      Environment object
  @param sections XXX Directory with calibration information
  @param right    @c True to restrict rotations to right angles
  @return         Assembled detector image
  """

  device = address_split(address)[2]
  if device is None or device != 'Cspad':
    return None

  # Get a current configure object for the detector
  config = getConfig(address, env)
  if config is None:
    return None

  # For consistency, one could/should verify that len(quads) is equal
  # to len(sections).
  if quads is None:
    quads = evt_get_quads(address, evt, env)

  if quads is None or len(quads) != len(sections):
    return None

  # This is from Mikhail S. Dubrovin's
  # HDF5Explorer/src/ConfigCSpad.py, which uses a detector size of
  # 1765-by-1765 pixels.
  extra_space = (1765 - 2 * Section.q_size[0],
                 1765 - 2 * Section.q_size[1])

  # Start out with a blank image of the detector.  This assumes that
  # the type of the first section in the first quadrant is identical
  # to the type of all the other sections.
  det = numpy.zeros((2 * Section.q_size[0] + extra_space[0],
                     2 * Section.q_size[1] + extra_space[1]),
                    dtype=quads[0].data()[0].dtype)

  ### need to swap the quadrants for data collected mid October, 2013
  evttime = time.gmtime(evt_time(evt)[0])
  swap = evttime.tm_year == 2013 and evttime.tm_mon == 10 and evttime.tm_mday >= 20 and evttime.tm_mday <= 25

  for quad in quads:
    q_data = quad.data()
    q_idx = quad.quad()
    if swap:
      q_idx = [0,1,3,2][q_idx]
    try:
      # pyana
      # example: if the third sensor (2x1) is disabled, q_mask = [0,1,3,4,5,6,7]
      q_mask = config.sections(q_idx)
    except AttributeError:
      # psana
      # as above, using config.roiMask, a bitstring where the ith bit is true if the ith sensor is active. x << y means bitwise shift
      # x, y times, and & is the bitwise AND operator
      q_mask = [i for i in range(len(sections[q_idx])) if 1 << i & config.roiMask(q_idx)]

    # For consistency, assert that there is data for each unmasked
    # section.
    assert len(q_data) == len(q_mask)
    for (s_data, s_idx) in zip(q_data, q_mask):
      # Rotate the section from the XTC-stream by -90 degrees to
      # conform to the "standing up" convention used by the
      # calibration data, and insert a 3-pixel gap between the ASIC:s.
      # This requires the horizontal dimension of the unrotated
      # section to be even.
      assert s_data.shape[1] % 2 == 0
      asics = numpy.vsplit(numpy.rot90(s_data, -1), 2)
      gap = numpy.zeros((3, s_data.shape[0]), dtype=s_data.dtype)
      s_data = numpy.vstack((asics[0], gap, asics[1]))

      # Place the section in the detector image, either by forcing
      # rotation to right angles or by interpolating.
      angle = sections[q_idx][s_idx].angle
      center = sections[q_idx][s_idx].center
      if right:
        rplace(det, s_data, angle, center)
      else:
        iplace(det, s_data, angle, center)
  return det


def CsPadElement(data3d, qn, config):
  """Construct one image for each quadrant, each with 8 sections from
  a data3d = 3 x 2*194 x 185 data array.  This function was originally
  written by Ingrid Ofte for pyana's XtcExplorer module.  XXX
  Documentation!
  """

  # If any sections are missing, insert zeros.
  mask = [config.sections(i) for i in range(4)]
  if (len(data3d) < 8):
    zsec = numpy.zeros((185, 388), dtype = data3d.dtype)
    for i in range(8) :
      if (i not in mask[qn]):
        data3d = numpy.insert(data3d, i, zsec, axis = 0)

  pairs = []
  for i in range(8) :
    # Insert gap between ASIC:s in the 2x1.
    asics = numpy.hsplit(data3d[i], 2)
    gap   = numpy.zeros((185, 4), dtype = data3d.dtype)
    pair  = numpy.hstack((asics[0], gap, asics[1]))

    # Sections 2,3 and 6,7 are as is.  The others need some rotation,
    # implemented as a matrix transposition here.
    if (i == 0 or i == 1):
      pair = pair[:, ::-1].T
    if (i == 4 or i == 5):
      pair = pair[::-1, :].T
    pairs.append(pair)

  # Make the array for this quadrant, and insert the 2x1 sections.
  quadrant = numpy.zeros((npix_quad, npix_quad), dtype = data3d.dtype)
  for sec in range(8):
    nrows, ncols = pairs[sec].shape

    # x,y  in quadrant coordinate system
    xpos = xpos_sec2x1[qn][sec]
    ypos = ypos_sec2x1[qn][sec]
    colp = xpos
    rowp = npix_quad - ypos
    quadrant[(rowp - nrows):rowp, colp:(colp + ncols)] = \
        pairs[sec][0:nrows, 0:ncols]

  # Finally, rotate the quadrant as needed.
  if (qn > 0):
    quadrant = numpy.rot90(quadrant, 4 - qn)
  return quadrant

def dpack(*kwargs):
  """ thin wrapper """
  return serialtbx.detector.cspad.dpack(*kwargs)

def hdf5pack(hdf5_file,
             active_areas=None,
             address=None,
             attenuation=None,
             beam_center_x=None,
             beam_center_y=None,
             ccd_image_saturation=None,
             data=None,
             distance=None,
             pixel_size=None,
             pulse_length=None,
             saturated_value=None,
             timestamp=None,
             wavelength=None,
             xtal_target=None):
  """Similar but far from identical to the HDF5 output from CASS.  XXX
  Poor diagnostics--we don't know if it failed or not.

  @note Does not include the deprecated SEQUENCE_NUMBER attribute.
        While some redundant items are written in order to keep the
        HDF5 synchronised to the pickle format, neither SIZE1 nor
        SIZE2 are included.
  """

  # Need this because we cannot write None values to the HDF5 file.
  if address is None:
    address = repr(None)
  if attenuation is None:
    attenuation = 0
  if xtal_target is None:
    xtal_target = repr(None)
  if pixel_size is None:
    pixel_size = globals()['pixel_size'] # XXX CSpad-specific!
  if pulse_length is None:
    pulse_length = 0

  d = dpack(address=address,
            active_areas=active_areas,
            beam_center_x=beam_center_x,
            beam_center_y=beam_center_y,
            ccd_image_saturation=ccd_image_saturation,
            data=data,
            distance=distance,
            pixel_size=pixel_size,
            saturated_value=saturated_value,
            timestamp=timestamp,
            wavelength=wavelength,
            xtal_target=xtal_target)
  if d is None:
    return

  grp_event = hdf5_file.create_group(d['TIMESTAMP'])
  grp_detector = grp_event.create_group(address)
  for (key, value) in six.iteritems(d):
    if key == 'ACTIVE_AREAS':
      grp_detector.create_dataset(key, data=value.as_numpy_array())
    elif key == 'DATA':
      # Compress the image data with gzip at the default level (4).
      # CASS seems to use maximum compression level (9), which gives a
      # moderate decrease in file size at the price of much longer
      # running time.
      grp_detector.create_dataset(
        key, compression='gzip', data=value.as_numpy_array())
    else:
      grp_event.create_dataset(key, data=[value])
  grp_event.create_dataset('ATTENUATION', data=[attenuation])
  grp_event.create_dataset('PULSE_LENGTH', data=[pulse_length])

def write_tiff(d, dirname=None, basename=None):
  """The write an image tiff.  Basic implementation no frills, no metadata
  """

  if basename is None:
    basename = ""
  if dirname is None:
    dirname = "."
  if not os.path.isdir(dirname):
    os.makedirs(dirname)

  # The output path should not contain any funny characters which may
  # not work in all environments.  This constructs a sequence number à
  # la evt_seqno() from the dictionary's timestamp.
  t = d['TIMESTAMP']
  s = t[0:4] + t[5:7] + t[8:10] + t[11:13] + t[14:16] + t[17:19] + t[20:23]

  path = os.path.join(dirname, basename + s + '.tiff')

  #assure that the 2-byte data are within the unsigned limits
  selecthi = d["DATA"]>65535
  d["DATA"].set_selected(selecthi,0)
  selectlo = d["DATA"]<0
  d["DATA"].set_selected(selectlo,0)

  idata = d["DATA"].as_numpy_array()
  idata =  idata.astype("uint16")
  import cv2 # psdm install should have this extension
  cv2.imwrite(path,idata)
  return path

def dwritef(d, dirname=None, basename=None):
  """The dwritef() function pickles the dictionary pointed to by @p d
  to the file whose directory and filename portions are pointed to by
  @p dirname and @p basename, respectively.  The directory at @p
  dirname, as well as any intermediate directories, are recursively
  created if they do not already exist.  The name of the written file
  is the concatenation of the @p basename parameter and a sequence
  number derived from the timestamp in the dictionary, @p d.

  @param d        Dictionary, as created by e.g. dpack()
  @param dirname  Directory portion of output file
  @param basename Filename prefix of output file
  @return         Path of output file
  """

  if basename is None:
    basename = ""
  if dirname is None:
    dirname = "."
  if not os.path.isdir(dirname):
    os.makedirs(dirname)

  # The output path should not contain any funny characters which may
  # not work in all environments.  This constructs a sequence number à
  # la evt_seqno() from the dictionary's timestamp.
  t = d['TIMESTAMP']
  s = t[0:4] + t[5:7] + t[8:10] + t[11:13] + t[14:16] + t[17:19] + t[20:23]

  # XXX Several non-pyana tools rely on the .pickle extension.  Fix
  # those before migrating to .pkl.
  path = os.path.join(dirname, basename + s + '.pickle')
  easy_pickle.dump(path, d)
  return path


def dwritef2(obj, path):
  """The dwritef2() function writes the object @p obj to the Python
  pickle file whose path is pointed to by @p path.  Non-existent
  directories of @p path are created as necessary.

  @param obj  Object to write, as created by e.g. dpack()
  @param path Path of output file
  @return     Path of output file
  """

  dirname = os.path.dirname(path)
  if dirname != "" and not os.path.isdir(dirname):
    os.makedirs(dirname)

  easy_pickle.dump(path, obj)
  return path


def pathsubst(format_string, evt, env, **kwargs):
  """The pathsubst() function provides variable substitution and value
  formatting as described in PEP 3101.  The function returns a copy of
  the input string, @p format_string, with field names replaced by
  their appropriate values as determined by either @p evt, @p env, or
  the user-supplied keyworded arguments, @p kwargs.

  chunk:      Chunk number or -1 if unknown.

  epoch:      Time of the event, in number of seconds since midnight,
              1 January 1970 UTC (Unix time), to millisecond
              precision.

  experiment: Experiment name, or empty string if unknown.

  expNum:     Experiment number or -1 if unknown.

  instrument: Instrument name, or empty string if unknown.

  iso8601:    The time of the event as an extended human-readable ISO
              8601 timestamp, to millisecond precision, or the empty
              string if unknown.  Not suitable for file names, because
              it contains characters that do not play well with
              certain file systems (e.g. NTFS).

  jobName:    Job name.

  jobNameSub: Combination of job name and subprocess index as a string
              which is unique for all subprocesses in a job.

  run:        Run number or -1 if unknown.

  seqno:      Sequence number or -1 if unknown.

  stream:     Stream number or -1 if unknown.

  subprocess: Subprocess number.  This is a non-negative integer in
              the range [0, nproc) when multiprocessing, or -1 for a
              single-process job.

  user:       The "login name" of the user.

  In addition to the standard conversion flags, the pathsubst()
  function implements the <code>!u</code> and <code>!l</code> flags
  for conversion to upper- and lower-case strings, respectively.

  Literal braces can be escaped by doubling, i.e. <code>{</code> is
  written <code>{{</code>, and <code>}</code> as <code>}}</code>.

  @note Chunk number, expNum, run number, and stream number are
        determined from the input XTC file name.  If a file does not
        adhere to the standard format, it may not be possible to
        determine these quantities.

  @note String substitution requires PSDM pyana version 0.10.3 or
        greater.

  @param format_string String containing replacement fields
  @param evt           Event data object, a configure object
  @param env           Environment object
  @param kwargs        User-supplied replacements, on the form
                       <code>field_name=value</code>
  @return               Copy of @p format_string, with replacement
                       fields substituted by their appropriate values
  """

  from getpass import getuser
  from string import Formatter

  class CaseFormatter(Formatter):
    def convert_field(self, value, conversion):
      # Extends the stock Formatter class with lower() and upper()
      # conversion types.

      if conversion == 'l':
        return str(value).lower()
      elif conversion == 'u':
        return str(value).upper()
      return super(CaseFormatter, self).convert_field(value, conversion)

    def get_value(self, key, args, kwargs_local):
      # The get_value() function sequentially applies user-supplied
      # and standard substitutions, and implements suitable defaults
      # in case a field name evaluates to None.  XXX Emit a warning
      # when this happens?
      if key in kwargs:
        return kwargs[key]

      value = super(CaseFormatter, self).get_value(key, args, kwargs_local)
      if value is None:
        if key == 'chunk':
          return -1
        elif key == 'expNum':
          return -1
        elif key == 'iso8601':
          return ''
        elif key == 'run':
          return -1
        elif key == 'seqno':
          return -1
        elif key == 'stream':
          return -1
      return value

  t = evt_time(evt)
  if t is not None:
    epoch = t[0] + t[1] / 1000
  else:
    epoch = None
  fmt = CaseFormatter()

  try:
    # psana
    expNum = env.expNum()
  except AttributeError:
    # pyana
    expNum = evt.expNum()

  try:
    # pyana
    chunk = evt.chunk()
  except AttributeError:
    # not supported in psana
    chunk = None

  try:
    # pyana
    stream = evt.stream()
  except AttributeError:
    # not supported in psana
    stream = None

  # If chunk or stream numbers cannot be determined, which may happen
  # if the XTC file has a non-standard name, evt.chunk() and
  # evt.stream() will return None.
  return fmt.format(format_string,
                    chunk=chunk,
                    epoch=epoch,
                    experiment=env.experiment(),
                    expNum=expNum,
                    instrument=env.instrument(),
                    iso8601=evt_timestamp(t),
                    jobName=env.jobName(),
                    jobNameSub=env.jobNameSub(),
                    run=evt.run(),
                    seqno=int(evt_seqno(evt)),
                    stream=stream,
                    subprocess=env.subprocess(),
                    user=getuser())

def env_laser_status(env, laser_id):
  """The return value is a bool that indicates whether the laser in
  question was on for that particular shot.  Bear in mind that sample
  hit by the laser will only encounter the X-rays some time after,
  depending on the flow rate.
  """

  if env is not None:
    pv_in = env.epicsStore().value('CXI:LAS:SHT:%02i:IN' % laser_id)
    pv_out = env.epicsStore().value('CXI:LAS:SHT:%02i:OUT' % laser_id)

    if pv_in is None or pv_out is None:
      return

    if hasattr(pv_in, "values"):
      if len(pv_in.values) != 1:
        return
      laser_off = pv_in.values[0]
    else:
      laser_off = pv_in

    if hasattr(pv_out, "values"):
      if len(pv_out.values) != 1:
        return
      laser_on = pv_out.values[0]
    else:
      laser_on = pv_out

    if laser_on and laser_off:
      # According to LCLS staff, this means the laser is not plugged in
      return False

    return bool(laser_on)


def env_injector_xyz(env):
  """Returns the coordinates of the sample injector.  XXX units unknown?"""
  if env is not None:
    return tuple([
      env.epicsStore().value("CXI:USR:MZM:0%i:ENCPOSITIONGET" %(i+1))
                             for i in range(3)])


def env_distance(*kwargs):
  """ thin wrapper """
  return serialtbx.detector.xtc.env_distance(*kwargs)


def env_sifoil(env):
  """The env_sifoil() function returns the total thickness of Si-foil,
  in um, that attenuates the beam.  According to an e-mail from Garth
  Williams, the centres of the attenuators are in the beam at around 0
  mm, and leave the beam at something like -7 mm.  The "out" position
  is at approximately -15 mm.

  @param env Environment object
  @return    Total thickness of attenuating Si-foil
  """

  if (env is None):
    return (None)

  # the pv name (? XXX) and the length of Si-foil it corresponds to
  # XXX static?
  dia = { "XRT:DIA:MMS:02.RBV":    20,
          "XRT:DIA:MMS:03.RBV":    40,
          "XRT:DIA:MMS:04.RBV":    80,
          "XRT:DIA:MMS:05.RBV":   160,
          "XRT:DIA:MMS:06.RBV":   320,
          "XRT:DIA:MMS:07.RBV":   640,
          "XRT:DIA:MMS:08.RBV":  1280,
          "XRT:DIA:MMS:09.RBV":  2560,
          "XRT:DIA:MMS:10.RBV":  5120,
          "XRT:DIA:MMS:11.RBV": 10240 }

  si_tot = 0
  for pvname, si_len in six.iteritems(dia):
    pv = env.epicsStore().value(pvname)

    # XXX Why is this an EpicsPvTime object?  The absorption
    # coefficient of Si is E-18 * n_{0} * lambda^2, (for lambda >= 5
    # um, Schroder, D. K., R. N. Thomos, and J. C. Swartz, IEEE
    # Trans. Electron. Dev. ED-25, 2(1978) 254-261).  See also
    # http://henke.lbl.gov/optical_constants/filter2.html

    #print "For ", pvname, " got ", pv, " and ", pv.values[0]
    if pv is not None: # and pv.units          == "mm"
      if hasattr(pv, "values"):
        # pyana
        if len(pv.values) == 1 and abs(pv.values[0]) <  7:
          si_tot += si_len
      else:
        # psana
        if abs(pv) < 7:
          si_tot += si_len

  return (si_tot)


def env_wavelength_sxr(evt, env):
  """The env_wavelength_sxr() function returns the wavelength in
  Ångström of the environment pointed to by @p env at the time of the
  event @p evt.  The function returns a positive value or @c None if
  no wavelength is available for the event.  See Heimann et al. (2011)
  Rev. Sci. Instrum. 82, 093104.

  @note The wavelength in eV is 12398.4187 divided by the value
        returned from env_wavelength_sxr().

  @param evt Event data object, a configure object
  @param env Environment object
  @return    Wavelength, in Ångström
  """

  from calendar import timegm
  from time import strptime

  if evt is None or env is None:
    return None

  t = evt.getTime()
  if t is None:
    return None

  es = env.epicsStore()
  if es is None:
    return None

  # Note that the monochromator coefficients could change from day to
  # day.  Unless specific values for the requested time are available,
  # attempt to retrieve them from EPICS.
  #
  # The compiler could recognize that strptime() and timegm() are pure
  # and reduce the test expression to an integer comparison.
  f = '%Y-%m-%d, %H:%M %Z'
  s = t.seconds()
  if s is None:
    return None
  elif s < timegm(strptime('2012-11-12, 17:00 UTC', f)):
    return None
  elif s < timegm(strptime('2012-11-17, 17:00 UTC', f)):
    abc = [+3.65920, -0.76851, +0.02105]
  elif s < timegm(strptime('2012-11-20, 17:00 UTC', f)):
    abc = [+4.18190, -0.77650, +0.01020]

  if 'abc' not in locals():
    pv = []
    for name in ['SXR:IOC:POLY:POLY:Lambda:O1:G3:A',
                 'SXR:IOC:POLY:POLY:Lambda:O1:G3:B',
                 'SXR:IOC:POLY:POLY:Lambda:O1:G3:C']:
      pv.append(es.value(name))
      if pv[-1] is None or len(pv[-1].values) != 1:
        return None
      pv[-1] = pv[-1].values[0]
      if pv[-1] is None:
        return None
    abc = [pv[i] for i in range(3)]

  # Get the grating motor position from EPICS.
  pv = es.value('SXR:MON:MMS:06.RBV')
  if pv is not None and len(pv.values) == 1:
    x = pv.values[0]
    e = 10 * (abc[0] + abc[1] * x + abc[2] * x**2)
    if e > 0:
      return e
  return None


def evt_pulse_energy(evt):
  """The evt_pulse_energy() function returns the energy, or the
  intensity, of the pulse in arbitrary units.  The returned value
  should be proportional to the number of photons in the pulse, and
  may be negative due to noise.

  @note An absolute, but less accurate, estimate of the number of
        photons in the pulse may be obtained from the gas monitor
        detector's fMilliJoulesPerPulse value.

  @param evt Event data object, a configure object
  @return    Pulse intensity, in arbitrary units
  """

  from pypdsdata.xtc import TypeId

  if evt is None:
    return None

  gmd = evt.get(key=TypeId.Type.Id_GMD)
  if hasattr(gmd, 'fRelativeEnergyPerPulse') and evt.expNum() == 208:
    # Note that for L632 (experiment number 208)
    # fRelativeEnergyPerPulse actually gives the negated value
    # sought.  Details are given in Moeller, S. (2012) "GMD Look
    # up Sheet for variable names in the DAQ (BLD) versus the C++
    # code".
    return -gmd.fRelativeEnergyPerPulse

  elif hasattr(gmd, 'fCorrectedSumPerPulse'):
    # This relatively pressure-independent quantity in arbitrary
    # units is preferable.  It is also known as
    # SXR:GMD:BLD:CumSumAllPeaks.
    return gmd.fCorrectedSumPerPulse
  return None


def evt_pulse_length(evt):
  """The evt_pulse_length() function returns the pulse length in fs.
  It is calculated as the ratio of the charge (in nC) and the peak
  current (in A).

  @param evt Event data object, a configure object
  @return    Pulse length, in fs
  """

  if (evt is not None):
    ebeam = get_ebeam(evt)

    if ebeam is None:
      return

    try:
      if ebeam.fEbeamPkCurrBC2 > 0:
        return 1e6 * ebeam.fEbeamCharge / ebeam.fEbeamPkCurrBC2
    except AttributeError:
      if ebeam.ebeamPkCurrBC2() > 0:
        return 1e6 * ebeam.ebeamCharge() / ebeam.ebeamPkCurrBC2()
  return None


def evt_repetition_rate(evt, address='*'):
  """The evt_repetition_rate() function returns the repetition rate of
  the instrument in Hz.  See
  https://confluence.slac.stanford.edu/display/PCDS/EVR+Event+Codes

  @param evt     Event data object, a configure object
  @param address Data source address of the DAQ device
  @return        Integer repetition rate, in Hz
  """

  evr = evt.getEvrData(address)
  if evr is not None:
    event_code_map = [120, 60, 30, 10, 5, 1]
    for i in range(evr.numFifoEvents() - 1, -1, -1):
      # Search for the last repetition rate event code.
      j = evr.fifoEvent(i).EventCode
      if j >= 40 and j <= 45:
        # These are the NO BEAM event codes.
        return event_code_map[j - 40]
      if j >= 140 and j <= 145:
        # These are the undocumented BEAM event codes.
        return event_code_map[j - 140]
  return None


def evt_beam_charge(evt):
  """The evt_beam_charge() function returns the charge of the pulse in
  nC.

  @param evt Event data object, a configure object
  @return    Pulse charge, in nC
  """

  if evt is not None:
    ebeam = get_ebeam(evt)

    if ebeam is None:
      return
    try:
      ebeam = evt.getEBeam()
      return ebeam.fEbeamCharge
    except AttributeError:
      return ebeam.ebeamCharge()
  return None


def evt_seqno(evt=None):
  """The evt_seqno() function returns string representation of a
  sequence number.  If @p evt is not @c None the return value reflects
  the time at which @p evt occurred, otherwise the current time is
  used.  If @p evt does not contain a time, evt_seqno() returns @c
  None.  XXX Should probably return an integer type instead?

  @param evt Event data object, a configure object
  @return    String representation of sequence number
  """

  t = evt_time(evt=evt)
  if t is None:
    return None
  return time.strftime("%Y%m%d%H%M%S", time.gmtime(t[0])) + ("%03d" % t[1])


def evt_time(evt=None):
  """The evt_time() function returns the time of the event @p evt since
  midnight, 1 January 1970 UTC (Unix time) to millisecond precision.
  If @p evt does not contain a time, evt_time() returns @c None.  If
  @p evt is @c None the return value reflects current time is used.

  @note Millisecond precision is sufficient, because at 120 Hz, shots
        are taken at 8.3 ms intervals.

  @param evt Event data object, a configure object
  @return    Unix time as a tuple of seconds and milliseconds
  """

  if evt is None:
    return serialtbx.util.time.now_s_ms()

  if hasattr(evt, "getTime"):
    t = evt.getTime()
    if t is None:
      return None
    return (t.seconds(), t.nanoseconds() // 1000000)
  else:
    from psana import EventId
    id = evt.get(EventId)
    return (id.time()[0], id.time()[1] // 1000000)


def evt_timestamp(t=None):
  """The evt_timestamp() function returns a string representation of
  an extended human-readable ISO 8601 timestamp.  If @p t is @c None
  the current time is used.  The function returns @c None on failure.

  @param t Tuple of the time in seconds and milliseconds
  @return  Human-readable ISO 8601 timestamp in string representation
  """

  return serialtbx.util.time.timestamp(t)

def evt_wavelength(*kwargs):
  """ thin wrapper """
  return serialtbx.detector.xtc.evt_wavelength(*kwargs)

def getConfig(address, env):
  """ Given a detector address, find the config object in an env object
  that goes with it.
  @param address detector address
  @param env environment object to search"""

  if hasattr(env, 'configStore'):
    good_key = None
    address = old_address_to_new_address(address)
    for key in env.configStore().keys():
      if address in str(key.src()) and key.type() is not None:
        good_key = key
        break
    if good_key is None:
      return None
    return env.configStore().get(good_key.type(),good_key.src())
  else:
    # Try the pyana method for older data
    from pypdsdata.xtc import TypeId
    return env.getConfig(TypeId.Type.Id_CspadConfig, address)

def getOptBool(s):
  if s is None or s == "None": return False
  elif isinstance(s, bool):
    return s
  s = s.strip().lower()
  return s == "true"

def getOptEvalOrString(s) :
  """Allow python code macros in the pyana configuration file, e.g.
  dark_path   = "/location_of_darks/r%%04d/Ds1-avg.pickle"%%(max([{True:dark,False:0}[3 > dark] for dark in [1,2,6,9,12,14,17,19]]))
  """
  possible_string = getOptString(s)
  try:
    eval_string = eval(possible_string,{},{})
    return eval_string
  except (SyntaxError, TypeError):
    return possible_string

def getOptString(s) :
  """XXX Return the string, strip of any white space (make sure there
  are no newline characters here).  This function was originally
  written by Ingrid Ofte for pyana's XtcExplorer module.
  """

  if (s is None):
    return (None)

  s = s.strip()
  if (s == "" or s == "No" or s == "None"):
    return (None)
  return (s)


def getOptStrings(s, default=None) :
  """XXX Return a list of strings.  This function was originally
  written by Ingrid Ofte for pyana's XtcExplorer module.
  """
  if (s is None):
    return default

  # strip off any leading or trailing whitespace
  s = s.strip()

  # make sure there are no newline characters here
  s = s.split("\n")
  s = " ".join(s)

  # make a list
  l = s.split()

  if (len(l) == 0 or (len(l) == 1 and (s == "" or s == "No" or s == "None"))):
    return ([])

  # all other cases:
  return (l)


def getOptInteger(s):
  """XXX Return a single integer.  This function was originally
  written by Ingrid Ofte for pyana's XtcExplorer module.  XXX What if
  conversion fails?
  """

  if (s is None or s == "" or s == "None"):
    return None
  return (int(s))

def getOptFloat(s):
  """Return a single float.
  """

  if (s is None or s == "" or s == "None"):
    return None
  return (float(s))

def getOptROI(s):
  """Return a tuple of the region of interest.
     Format: roi = fast_low:fast_high,slow_low:slow_high
  """
  roi_str    = getOptString(s)
  if (roi_str is not None and roi_str != ""):
    ivl        = roi_str.strip("()").split(",")
    ivl_x      = ivl[0].split(":")
    ivl_y      = ivl[1].split(":")
    roi = [ivl_x[0], ivl_x[1], ivl_y[0], ivl_y[1]]
    for i in range(4):
      if roi[i] == "": roi[i] = None
      else: roi[i] = int(roi[i])
    return tuple(roi)


def image(address, config, evt, env, sections=None):
  """Assemble the uint16 detector image, and sum it up as int32.  Sum
  the image of squared intensities as uint64.  XXX Documentation! XXX
  Would be nice to get rid of the constant string names.  XXX Better
  named evt_image()?

  @param address  Full data source address of the DAQ device
  @param config   XXX This should go--get up-to-date object on the fly!
  @param evt      Event data object, a configure object
  @param env      Environment object
  @param sections XXX
  @return         XXX
  """

  device = address_split(address)[2]
  if device is None:
    return None

  elif device == 'Andor':
    # XXX There is no proper getter for Andor frames yet, and
    # evt.getFrameValue(address) does not appear to work.
    from pypdsdata.xtc import TypeId
    value = evt.get(TypeId.Type.Id_AndorFrame, address)
    if value is not None:
      img = value.data()
      return img

  elif device == 'Cspad':
    if sections is not None:
      return CsPadDetector(address, evt, env, sections)
    else:
      # XXX This is obsolete code, provided for backwards
      # compatibility with the days before detector metrology was
      # used.
      assert False # sections always required now as of Sep 1 2014
      quads = evt.getCsPadQuads(address, env)
      qimages = numpy.empty((4, npix_quad, npix_quad), dtype='uint16')
      for q in quads:
        qimages[q.quad()] = CsPadElement(q.data(), q.quad(), config)
      return numpy.vstack((numpy.hstack((qimages[0], qimages[1])),
                           numpy.hstack((qimages[3], qimages[2]))))

  elif device == 'Cspad2x2':
    from pypdsdata.xtc import TypeId
    quads = evt.get(TypeId.Type.Id_Cspad2x2Element, address)
    if quads is not None:
      return CsPad2x2Image(quads.data(), config, sections)

  elif device == 'pnCCD':
    value = evt.getPnCcdValue(address, env)
    if value is not None:
      # Returns the image data as a numpy 1024-by-1024 uint16 array
      # XXX Should be split up into tiles (halves) to allow metrology
      # to be adjusted?  Will require a sections parameter!
      img = value.data()

      # Deal with overflows.  XXX This might be dependent on the
      # particular version of pyana.  CASS ignores the two most
      # significant bits, which is different from what is done below,
      # but Lutz Foucar says they do contain data which could be used.
      img[img > 2**14 - 1] = 2**14 - 1
      return img
  return None

def image_xpp(address, evt, env, aa, quads = None):
  """Assemble the uint16 detector image, see also
  cspad_tbx.CsPadDetector().  XXX Documentation! XXX Would be nice to
  get rid of the constant string names.  XXX Better named evt_image()?

  @param address Full data source address of the DAQ device
  @param evt     Event data object, a configure object
  @param env     Environment object
  @param aa      Active areas, in lieu of full metrology object
  @param quads   Data, if None get it from the event
  @return        XXX
  """

  if address != 'XppGon-0|Cspad-0':
    return None

  # Get a current configure object for the detector
  config = getConfig(address, env)
  if config is None:
    return None

  if quads is None:
    # For consistency, one could/should verify that len(quads) is equal
    # to len(sections).
    quads = evt_get_quads(address, evt, env)
    if quads is None or len(quads) != len(aa) // (8 * 2 * 4):
      return None

  # Start out with a blank image of the detector.  Mikhail
  # S. Dubrovin's HDF5Explorer/src/ConfigCSpad.py uses a detector
  # size of 1765-by-1765 pixels.  This assumes that the type of the
  # first section in the first quadrant is identical to the type of
  # all the other sections.
  det = numpy.zeros((1765, 1765), dtype=quads[0].data()[0].dtype)

  for quad in quads:
    q_data = quad.data()
    q_idx = quad.quad()
    try:
      # pyana
      # example: if the third sensor (2x1) is disabled, q_mask = [0,1,3,4,5,6,7]
      q_mask = config.sections(q_idx)
    except AttributeError:
      # psana
      # as above, using config.roiMask, a bitstring where the ith bit is true if the ith sensor is active. x << y means bitwise shift
      # x, y times, and & is the bitwise AND operator
      q_mask = [i for i in range(config.numSect()//config.numQuads()) if 1 << i & config.roiMask(q_idx)]

    # For consistency, one could/should verify that len(q_data) is
    # equal to len(sections[q_idx]).
    assert len(q_data) == len(q_mask)
    for (s_data, s_idx) in zip(q_data, q_mask):
      # Rotate the "lying down" sensor readout from the XTC stream by
      # an integer multiple of 90 degrees to match the orientation on
      # the detector.  This assumes that the horizontal dimension of
      # the unrotated sensor is even.  Note that the XPP CSPAD is
      # rotated by 180 degrees with respect to the optical metrology
      # measurements.
      assert s_data.shape[1] % 2 == 0
      if   q_idx == 0 and s_idx in [2, 3, 6, 7] or \
           q_idx == 1 and s_idx in [0, 1]       or \
           q_idx == 3 and s_idx in [4, 5]:
        asics = numpy.hsplit(numpy.rot90(s_data, 0 + 2), 2)
        asics.reverse()
      elif q_idx == 0 and s_idx in [0, 1]       or \
           q_idx == 2 and s_idx in [4, 5]       or \
           q_idx == 3 and s_idx in [2, 3, 6, 7]:
        asics = numpy.vsplit(numpy.rot90(s_data, 1 + 2), 2)
      elif q_idx == 1 and s_idx in [4, 5]       or \
           q_idx == 2 and s_idx in [2, 3, 6, 7] or \
           q_idx == 3 and s_idx in [0, 1]:
        asics = numpy.hsplit(numpy.rot90(s_data, 2 + 2), 2)
      elif q_idx == 0 and s_idx in [4, 5]       or \
           q_idx == 1 and s_idx in [2, 3, 6, 7] or \
           q_idx == 2 and s_idx in [0, 1]:
        asics = numpy.vsplit(numpy.rot90(s_data, 3 + 2), 2)
        asics.reverse()
      else:
        # NOTREACHED
        return None

      # Use the active areas to place the two ASICS on the
      # destination detector image.
      for a_idx in range(len(asics)):
        aa_idx = q_idx * (8 * 2 * 4) + s_idx * (2 * 4) + a_idx * 4
        det[aa[aa_idx + 0]:aa[aa_idx + 2],
            aa[aa_idx + 1]:aa[aa_idx + 3]] = asics[a_idx]

  return det


def iplace(dst, src, angle, center):
    """The iplace() function places @p src in @p dst centred on @p
    center after rotating it by @p angle degrees counter-clockwise.
    The source image is mapped onto the destination image by bilinear
    interpolation.  While this may introduce interpolation artifacts
    it is significantly simpler than many other interpolation
    methods--and bog slow.

    @p dst    Destination image
    @p src    Source image
    @p angle  Rotation angle, in degrees
    @p center Centre of @p src in @p dst, after rotation
    """

    a = math.radians(angle)
    c = math.cos(a)
    s = math.sin(a)

    # Find the origin-centred bounding box of the rotated source
    # image.  Due to the symmetry of a rectangle, the extrema can be
    # determined by the transformed coordinates of two adjacent
    # corners.
    hsize = [0.5 * max(abs(c * src.shape[0] - s * src.shape[1]),
                       abs(c * src.shape[0] + s * src.shape[1])),
             0.5 * max(abs(s * src.shape[0] + c * src.shape[1]),
                       abs(s * src.shape[0] - c * src.shape[1]))]
    xlim  = [int(math.floor(-hsize[0])),
             int(math.ceil( +hsize[0])) + 1]
    ylim  = [int(math.floor(-hsize[1])),
             int(math.ceil( +hsize[1])) + 1]

    # For each pixel in the bounding box, determine the real-valued
    # components in coordinate system of the untransformed source
    # image, (xp, yp).  Then do bilinear interpolation based on the
    # four pixels with integer coordinates around (xp, yp).
    for x in range(xlim[0], xlim[1]):
        for y in range(ylim[0], ylim[1]):
            xp =  c * x + s * y + 0.5 * src.shape[0]
            yp = -s * x + c * y + 0.5 * src.shape[1]
            if (xp >= 0 and math.ceil(xp) < src.shape[0] and
                yp >= 0 and math.ceil(yp) < src.shape[1]):

                xi =[int(math.floor(xp)), int(math.ceil(xp))]
                yi =[int(math.floor(yp)), int(math.ceil(yp))]

                xf = xp - xi[0]
                yf = yp - yi[0]

                dst[int(round(x + center[0])),
                    int(round(y + center[1]))] =              \
                    src[xi[0], yi[0]] * (1 - xf) * (1 - yf) + \
                    src[xi[1], yi[0]] * xf       * (1 - yf) + \
                    src[xi[0], yi[1]] * (1 - xf) * yf       + \
                    src[xi[1], yi[1]] * xf       * yf


def rplace(dst, src, angle, center):
    """The rplace() function places @p src in @p dst centred on @p
    centre after rotating it by @p angle degrees counter-clockwise.
    The rotation angle is rounded to the nearest integer multiple of
    90 degrees before transformation.

    @p dst    Destination image
    @p src    Source image
    @p angle  Rotation angle, in degrees
    @p center Centre of @p src in @p dst, after rotation
    """

    # Rotate the source image, and determine the upper, left corner of
    # its location in the destination image.
    rot = numpy.rot90(src, int(round(angle / 90.0)) % 4)
    ulc = [int(round(center[0] - 0.5 * rot.shape[0])),
           int(round(center[1] - 0.5 * rot.shape[1]))]

    dst[ulc[0]:(ulc[0] + rot.shape[0]),
        ulc[1]:(ulc[1] + rot.shape[1])] = rot

# For the moment, the XPP CSPAD detector's metrology is stored here
# as a series of active areas
_xpp_active_areas = {
  'XPP 7.1': { # metrology recorded 1/24/13 and processed by flatfile.py
    'active_areas': flex.int([
           865, 1121, 1059, 1306, 1062, 1121, 1256, 1306,
           864,  909, 1058, 1094, 1061,  909, 1255, 1094,
          1083, 1534, 1268, 1728, 1083, 1337, 1268, 1531,
           871, 1538, 1056, 1732,  871, 1341, 1056, 1535,
          1495, 1326, 1689, 1511, 1298, 1326, 1492, 1511,
          1496, 1539, 1690, 1724, 1299, 1539, 1493, 1724,
          1482, 1105, 1667, 1299, 1482,  908, 1667, 1102,
          1270, 1107, 1455, 1301, 1270,  910, 1455, 1104,
          1123,  706, 1308,  900, 1123,  509, 1308,  703,
           910,  706, 1095,  900,  910,  509, 1095,  703,
          1535,  498, 1729,  683, 1338,  498, 1532,  683,
          1534,  711, 1728,  896, 1337,  711, 1531,  896,
          1324,   77, 1509,  271, 1324,  274, 1509,  468,
          1537,   75, 1722,  269, 1537,  272, 1722,  466,
          1104,   97, 1298,  282,  907,   97, 1101,  282,
          1105,  310, 1299,  495,  908,  310, 1102,  495,
           706,  457,  900,  642,  509,  457,  703,  642,
           705,  669,  899,  854,  508,  669,  702,  854,
           496,   36,  681,  230,  496,  233,  681,  427,
           709,   38,  894,  232,  709,  235,  894,  429,
            77,  256,  271,  441,  274,  256,  468,  441,
            77,   44,  271,  229,  274,   44,  468,  229,
            98,  467,  283,  661,   98,  664,  283,  858,
           311,  467,  496,  661,  311,  664,  496,  858,
           457,  867,  642, 1061,  457, 1064,  642, 1258,
           670,  865,  855, 1059,  670, 1062,  855, 1256,
            37, 1084,  231, 1269,  234, 1084,  428, 1269,
            37,  871,  231, 1056,  234,  871,  428, 1056,
           256, 1495,  441, 1689,  256, 1298,  441, 1492,
            43, 1497,  228, 1691,   43, 1300,  228, 1494,
           469, 1481,  663, 1666,  666, 1481,  860, 1666,
           467, 1269,  661, 1454,  664, 1269,  858, 1454]),
    'rotations' : flex.int([
                   3,3,3,3,2,2,2,2,1,1,1,1,2,2,2,2,
                   2,2,2,2,1,1,1,1,0,0,0,0,1,1,1,1,
                   1,1,1,1,0,0,0,0,3,3,3,3,0,0,0,0,
                   0,0,0,0,3,3,3,3,2,2,2,2,3,3,3,3
                  ])
  },
  'XPP 7.2': { # metrology recorded 1/29/13 and processed by flatfile.py
    'active_areas': flex.int([
           868, 1122, 1062, 1307, 1065, 1122, 1259, 1307,
           868,  910, 1062, 1095, 1065,  910, 1259, 1095,
          1087, 1534, 1272, 1728, 1087, 1337, 1272, 1531,
           874, 1536, 1059, 1730,  874, 1339, 1059, 1533,
          1497, 1328, 1691, 1513, 1300, 1328, 1494, 1513,
          1499, 1541, 1693, 1726, 1302, 1541, 1496, 1726,
          1483, 1105, 1668, 1299, 1483,  908, 1668, 1102,
          1271, 1106, 1456, 1300, 1271,  909, 1456, 1103,
          1122,  705, 1307,  899, 1122,  508, 1307,  702,
           909,  705, 1094,  899,  909,  508, 1094,  702,
          1534,  497, 1728,  682, 1337,  497, 1531,  682,
          1533,  710, 1727,  895, 1336,  710, 1530,  895,
          1323,   76, 1508,  270, 1323,  273, 1508,  467,
          1536,   75, 1721,  269, 1536,  272, 1721,  466,
          1103,   97, 1297,  282,  906,   97, 1100,  282,
          1103,  310, 1297,  495,  906,  310, 1100,  495,
           705,  456,  899,  641,  508,  456,  702,  641,
           704,  669,  898,  854,  507,  669,  701,  854,
           495,   35,  680,  229,  495,  232,  680,  426,
           707,   38,  892,  232,  707,  235,  892,  429,
            75,  256,  269,  441,  272,  256,  466,  441,
            75,   43,  269,  228,  272,   43,  466,  228,
            97,  467,  282,  661,   97,  664,  282,  858,
           310,  466,  495,  660,  310,  663,  495,  857,
           456,  866,  641, 1060,  456, 1063,  641, 1257,
           669,  865,  854, 1059,  669, 1062,  854, 1256,
            36, 1084,  230, 1269,  233, 1084,  427, 1269,
            35,  870,  229, 1055,  232,  870,  426, 1055,
           254, 1494,  439, 1688,  254, 1297,  439, 1491,
            42, 1496,  227, 1690,   42, 1299,  227, 1493,
           468, 1481,  662, 1666,  665, 1481,  859, 1666,
           465, 1268,  659, 1453,  662, 1268,  856, 1453]),
    'rotations' : flex.int([
                   3,3,3,3,2,2,2,2,1,1,1,1,2,2,2,2,
                   2,2,2,2,1,1,1,1,0,0,0,0,1,1,1,1,
                   1,1,1,1,0,0,0,0,3,3,3,3,0,0,0,0,
                   0,0,0,0,3,3,3,3,2,2,2,2,3,3,3,3
                  ])
  },
  'XPP 8.1': { # metrology recorded 10/09/13 and processed by flatfile.py
    'active_areas': flex.int([
           863, 1118, 1057, 1303, 1060, 1118, 1254, 1303,
           865,  913, 1059, 1098, 1062,  913, 1256, 1098,
          1070, 1532, 1255, 1726, 1070, 1335, 1255, 1529,
           863, 1532, 1048, 1726,  863, 1335, 1048, 1529,
          1484, 1335, 1678, 1520, 1287, 1335, 1481, 1520,
          1484, 1543, 1678, 1728, 1287, 1543, 1481, 1728,
          1475, 1110, 1660, 1304, 1475,  913, 1660, 1107,
          1268, 1109, 1453, 1303, 1268,  912, 1453, 1106,
          1119,  707, 1304,  901, 1119,  510, 1304,  704,
           912,  707, 1097,  901,  912,  510, 1097,  704,
          1533,  506, 1727,  691, 1336,  506, 1530,  691,
          1533,  715, 1727,  900, 1336,  715, 1530,  900,
          1334,   84, 1519,  278, 1334,  281, 1519,  475,
          1541,   85, 1726,  279, 1541,  282, 1726,  476,
          1108,  103, 1302,  288,  911,  103, 1105,  288,
          1108,  311, 1302,  496,  911,  311, 1105,  496,
           706,  460,  900,  645,  509,  460,  703,  645,
           706,  666,  900,  851,  509,  666,  703,  851,
           507,   38,  692,  232,  507,  235,  692,  429,
           713,   38,  898,  232,  713,  235,  898,  429,
            82,  241,  276,  426,  279,  241,  473,  426,
            82,   37,  276,  222,  279,   37,  473,  222,
           103,  459,  288,  653,  103,  656,  288,  850,
           310,  460,  495,  654,  310,  657,  495,  851,
           460,  862,  645, 1056,  460, 1059,  645, 1253,
           666,  863,  851, 1057,  666, 1060,  851, 1254,
            38, 1070,  232, 1255,  235, 1070,  429, 1255,
            38,  864,  232, 1049,  235,  864,  429, 1049,
           242, 1484,  427, 1678,  242, 1287,  427, 1481,
            37, 1484,  222, 1678,   37, 1287,  222, 1481,
           458, 1475,  652, 1660,  655, 1475,  849, 1660,
           459, 1267,  653, 1452,  656, 1267,  850, 1452]),
    'rotations' : flex.int([
                   3,3,3,3,2,2,2,2,1,1,1,1,2,2,2,2,
                   2,2,2,2,1,1,1,1,0,0,0,0,1,1,1,1,
                   1,1,1,1,0,0,0,0,3,3,3,3,0,0,0,0,
                   0,0,0,0,3,3,3,3,2,2,2,2,3,3,3,3
                  ])
  },

  #SOME BIG ISSUES REMAIN WITH Sacla.MPCCD.8tile format
  # Evidently the data from Takanori 22 Sep 2015 already has slight rotation
  # applied to the MPCCD modules, as the data rectangles displayed in cctbx.image_viewer are tilted
  # This is inconsistent with the expectation that npy.py should get the raw data, not preprocessed.

  'Sacla.MPCCD.8tile': { # as given by Takanori 22 Sep 2015
    'active_areas': flex.int([
           112,  189,  622, 1212,  647,  188, 1156, 1212,
          1180,  140, 1691, 1163, 1714,  140, 2226, 1163,
           159, 1231,  671, 2254,  694, 1230, 1206, 2253,
          1229, 1180, 1740, 2203, 1762, 1180, 2274, 2202,
     ]),
    'rotations' : flex.int([
                   0,0,0,0,0,0,0,0,
                  ])
  },

}
_xpp_active_areas['XPP 11.1'] = _xpp_active_areas['XPP 9.1'] = _xpp_active_areas['XPP 8.1']
xpp_active_areas = _xpp_active_areas


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/display_calib.py
from __future__ import absolute_import, division, print_function
from six.moves import range
#! /usr/bin/python
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# XXX Jiffy summary here
#
# $Id$

import numpy
import sys

import matplotlib
from   matplotlib.collections import PatchCollection
import matplotlib.patches     as     mpatches
import matplotlib.pyplot      as     plt

from optparse import OptionParser

from xfel.cxi.cspad_ana.parse_calib import calib2sections


# XXX http://www.artima.com/weblogs/viewpost.jsp?thread=4829
def display_calib(dirname, right, verbose):
  """XXX Docstring, in fact revise all the documentation

  @param dirname Directory with calibration information
  @param right   @c True to restrict rotations to right angles
  @param verbose @c True to print ASIC coordinates
  """

  fig     = plt.figure(figsize = (10, 10))
  ax      = plt.axes([0, 0, 1, 1])
  plt.axis([0, 1765, 1765, 0])

  colours  = []
  patches  = []
  sections = calib2sections(dirname)
  for q in range(len(sections)):
    for s in range(len(sections[q])):

      # Get the vertices of the section s in quadrant q, and round
      # rotation angles to integer multiples of 90 degrees by default.
      # Change from matrix-coordinate system to screen coordinate
      # system, where origin is in the top left corner, the first
      # coordinate increases to the right, and the second coordinate
      # increases downwards.  Ensure that the eight sections within
      # the quadrants are coloured consistently.
      vertices = sections[q][s].corners(right)
      for i in range(len(vertices)):
        vertices[i] = [vertices[i][1], vertices[i][0]]

      art = mpatches.Circle(vertices[0], 10)
      patches.append(art)
      colours.append(s)

      polygon = mpatches.Polygon(vertices)
      patches.append(polygon)
      colours.append(s)

      plt.text(sections[q][s].center[1], sections[q][s].center[0],
               "(%1d, %1d)" % (q, s),
               family = "sans-serif",
               size   = 14,
               ha     = "center",
               va     = "center")

      # Assuming that rotations are integer multiples of 90 degrees,
      # print the ASIC coordinates in "spotfinder" format, ordered by
      # quadrant, section, and ASIC.  XXX This only makes sense for
      # right = True.
      if (verbose):
        vertices = sections[q][s].corners_asic()
        print("(%4d, %4d, %4d, %4d)" % \
            (vertices[0][0], vertices[0][1], vertices[0][2], vertices[0][3]))
        print("(%4d, %4d, %4d, %4d)" % \
            (vertices[1][0], vertices[1][1], vertices[1][2], vertices[1][3]))

  collection = PatchCollection(patches, cmap = matplotlib.cm.jet, alpha = 0.4)
  collection.set_array(numpy.array(colours))
  ax.add_collection(collection)
  ax.set_xticks([])
  ax.set_yticks([])
  plt.show()

  return (0)


# Run with "display_calib.py
# /reg/d/ana11/cxi/data/CSPAD-metrology/run4/CxiDs1.0:Cspad.0".  XXX
# http://docs.python.org/library/optparse.html
if (__name__ == "__main__"):
  parser = OptionParser()
  parser.add_option("-r", "--rotate",
                    action  = "store_false",
                    default = True,
                    dest    = "right",
                    help    = "Allow sections to rotate by arbitrary angles")
  parser.add_option("-v", "--verbose",
                    action  = "store_true",
                    default = False,
                    dest    = "verbose",
                    help    = "Print ordered list of diagonal ASIC corners")

  (options, args) = parser.parse_args()
  for arg in args:
    ret = display_calib(arg, options.right, options.verbose)
    if (ret != 0):
      sys.exit(ret)


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/histogram_finalise.py
from __future__ import absolute_import, division, print_function
import os
import glob
from scitbx.array_family import flex # import dependency

from libtbx import easy_pickle
import six

class histogram_finalise(object):

  def __init__(self,
               output_dirname,
               runs,
               pickle_pattern=None):
    avg_basename="avg_"
    stddev_basename="stddev"
    self.adu_offset = 0
    self.histogram = None
    self.nmemb = 0
    for i_run, run in enumerate(runs):
      run_scratch_dir = run
      result = finalise_one_run(run_scratch_dir, pickle_pattern=pickle_pattern)
      if result.histogram is None: continue
      if self.histogram is None:
        self.histogram = result.histogram
      else:
        self.histogram = update_histograms(self.histogram, result.histogram)
      self.nmemb += result.nmemb

    if (output_dirname  is not None and
        avg_basename is not None):
      if (not os.path.isdir(output_dirname)):
        os.makedirs(output_dirname)

    pickle_path = os.path.join(output_dirname, "hist.pickle")
    easy_pickle.dump(pickle_path, self.histogram)

    print("Total number of images used from %i runs: %i" %(i_run+1, self.nmemb))

class finalise_one_run(object):

  def __init__(self, scratch_dir, pickle_pattern=None):
    pickle_dirname = "pickle"
    pickle_basename = "pkl_"
    self.nmemb = 0
    self.histogram = None
    if pickle_pattern is not None:
      path_pattern = "%s/%s/%s" %(scratch_dir, pickle_dirname, pickle_pattern)
    else:
      path_pattern = "%s/%s/%ss[0-9][0-9]-*.pickle" %(
        scratch_dir, pickle_dirname, pickle_basename)
    print(path_pattern)
    g = glob.glob(path_pattern)
    if len(g) == 0:
      print("No matches found for pattern: %s" %path_pattern)
      return
    for path in g:
      try:
        d = easy_pickle.load(file_name=path)
      except EOFError:
        print("EOFError: skipping %s:" %path)
        continue
      if len(d["histogram"]) == 0: continue
      if self.histogram is None:
        self.histogram = d["histogram"]
      else:
        self.histogram = update_histograms(self.histogram, d["histogram"])
      self.nmemb += d["nmemb"]
      print("Read %d images from %s" % (d["nmemb"], path))

    print("Number of images used: %i" %self.nmemb)
    #assert self.nmemb > 0

def update_histograms(hist_dict1, hist_dict2):
  for key, value in six.iteritems(hist_dict1):
    value.update(hist_dict2[key])
  return hist_dict1


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/hitfinder_tbx.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# $Id: common_mode.py 17569 2013-06-11 07:58:18Z phyy-nx $

from __future__ import absolute_import, division, print_function
from six.moves import range
import numpy
import math

from scitbx.array_family import flex
from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import rayonix_tbx
from iotbx.detectors.cspad_detector_formats import detector_format_version as detector_format_function
from iotbx.detectors.cspad_detector_formats import reverse_timestamp

# alternate implementation of hitfinder, use the idea of running spotfinder
#   on the data from the innermost four sensors.  Once this is done, a hit is
#   defined as an image where there are >=16 spots whose peak values
#   exceed the defined threshold.

class distl_hitfinder(object):

  def distl_filter(self,
                   address,
                   cspad_img,
                   distance,
                   timestamp,
                   wavelength):
    self.hitfinder_d["DATA"] = cspad_img
    self.hitfinder_d["DISTANCE"] = distance
    self.hitfinder_d["TIMESTAMP"] = timestamp
    self.hitfinder_d["WAVELENGTH"] = wavelength
    self.hitfinder_d["DETECTOR_ADDRESS"] = address

    args = ["indexing.data=dummy",
            "distl.bins.verbose=False",
            self.asic_filter,
            ]

    detector_format_version = detector_format_function(
      address, reverse_timestamp(timestamp)[0])
    args += ["distl.detector_format_version=%s" % detector_format_version]

    from xfel.phil_preferences import load_cxi_phil
    horizons_phil = load_cxi_phil(self.m_xtal_target, args)
    horizons_phil.indexing.data = self.hitfinder_d

    from xfel.cxi import display_spots
    display_spots.parameters.horizons_phil = horizons_phil

    from rstbx.new_horizons.index import pre_indexing_validation,pack_names
    pre_indexing_validation(horizons_phil)
    imagefile_arguments = pack_names(horizons_phil)

    from spotfinder.applications import signal_strength
    info = signal_strength.run_signal_strength_core(horizons_phil,imagefile_arguments)

    imgdata = info.Files.images[0].linearintdata

    active_data = self.get_active_data(info.Files.images[0],horizons_phil)

    peak_heights = flex.int( [
      imgdata[ spot.max_pxl_x(), spot.max_pxl_y() ]
      for spot in info.S.images[info.frames[0]]["spots_total"]
    ])

    outscale = 256
    corrected = peak_heights.as_double() * self.correction
    outvalue = outscale *(1.0-corrected)
    outvalue.set_selected(outvalue<0.0,0.)
    outvalue.set_selected(outvalue>=outscale,int(outscale)-1)
    outvalue = flex.int(outvalue.as_numpy_array().astype(numpy.int32))
    # essentially, select a peak if the peak's ADU value is > 2.5 * the 90-percentile pixel value

    #work = display_spots.wrapper_of_callback(info)
    #work.display_with_callback(horizons_phil.indexing.data)
    return peak_heights,outvalue

  def get_active_data(self,imgobj,phil):
    active_areas = imgobj.get_tile_manager(phil).effective_tiling_as_flex_int()
    data = imgobj.linearintdata

    active_data = flex.double()
    for tile in range(len(active_areas)//4):
      block = data.matrix_copy_block(
          i_row=active_areas[4*tile+0],i_column=active_areas[4*tile+1],
          n_rows=active_areas[4*tile+2]-active_areas[4*tile+0],
          n_columns=active_areas[4*tile+3]-active_areas[4*tile+1]).as_1d().as_double()
      active_data = active_data.concatenate(block)

    #print "The mean is ",flex.mean(active_data),"on %d pixels"%len(active_data)
    order = flex.sort_permutation(active_data)
    #print "The 90-percentile pixel is ",active_data[order[int(0.9*len(active_data))]]
    #print "The 99-percentile pixel is ",active_data[order[int(0.99*len(active_data))]]

    adjlevel = 0.4
    brightness = 1.0
    percentile90 = active_data[order[int(0.9*len(active_data))]]
    if percentile90 > 0.:
      self.correction = brightness * adjlevel / percentile90
    else: self.correction = 1.0
    return active_data

  def set_up_hitfinder(self, env):
    # See r17537 of mod_average.py.
    device = cspad_tbx.address_split(self.address)[2]
    if device == 'Cspad':
      img_dim = (1765, 1765)
      pixel_size = cspad_tbx.pixel_size
    elif device == 'marccd':
      img_dim = (4500, 4500)
      pixel_size = 0.079346
    elif device == 'Rayonix':
      img_dim = rayonix_tbx.get_rayonix_detector_dimensions(env)
      pixel_size = rayonix_tbx.get_rayonix_pixel_size(self.bin_size)
    else:
      raise RuntimeError("Unsupported device %s" % self.address)

    if self.beam_center is None:
      self.beam_center = [0,0]

    self.hitfinder_d = cspad_tbx.dpack(
      active_areas=self.active_areas,
      beam_center_x=pixel_size * self.beam_center[0],
      beam_center_y=pixel_size * self.beam_center[1],
      data=flex.int(flex.grid(img_dim[0], img_dim[1]), 0),
      xtal_target=self.m_xtal_target)

    if device == 'Cspad':
      # Figure out which ASIC:s are on the central four sensors.  This
      # only applies to the CSPAD.
      assert len(self.active_areas) % 4 == 0
      distances = flex.double()
      for i in range(0, len(self.active_areas), 4):
        cenasic = ((self.active_areas[i + 0] + self.active_areas[i + 2]) / 2,
                   (self.active_areas[i + 1] + self.active_areas[i + 3]) / 2)
        distances.append(math.hypot(cenasic[0] - self.beam_center[0],
                                    cenasic[1] - self.beam_center[1]))
      orders = flex.sort_permutation(distances)

      # Use the central 8 ASIC:s (central 4 sensors).
      flags = flex.int(len(self.active_areas) // 4, 0)
      for i in range(8):
        flags[orders[i]] = 1
      self.asic_filter = "distl.tile_flags=" + ",".join(
        ["%1d" % b for b in flags])

    elif device == 'marccd':
      # There is only one active area for the MAR CCD, so use it.
      self.asic_filter = "distl.tile_flags=1"
    elif device == 'Rayonix':
      # There is only one active area for the Rayonix, so use it.
      self.asic_filter = "distl.tile_flags=1"


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_average.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# $Id$
"""First- and second-order statistics for CS-PAD images

The mod_average user analysis module computes the (arithmetic) mean and the
standard deviation from the images in an XTC stream.  On successful
completion, the mean and standard deviation images are written to disk
as pickled dictionaries.
"""
from __future__ import absolute_import, division, print_function

__version__ = "$Revision$"

from xfel.cxi.cspad_ana import average_tbx
from xfel.cxi.cspad_ana import cspad_tbx


class mod_average(average_tbx.average_mixin):
  """Class for generating first- and second-order statistics within
  the pyana framework

  XXX Maybe this module should be renamed to mod_stat12, mod_sstat or
  some such?
  """


  def __init__(self,
               address,
               max_out=None,
               mean_out=None,
               std_out=None,
               **kwds):
    """The mod_average class constructor stores the parameters passed
    from the pyana configuration file in instance variables.  All
    parameters, except @p address are optional, and hence need not be
    defined in pyana.cfg.

    @param address Full data source address of the DAQ device
    """

    super(mod_average, self).__init__(address=address, **kwds)

    self._max_out = cspad_tbx.getOptString(max_out)
    self._mean_out = cspad_tbx.getOptString(mean_out)
    self._std_out = cspad_tbx.getOptString(std_out)

    # XXX Ugly hack here instead of modifying avg_tbx.py
    if max_out is not None:
      self._have_max = True
    if mean_out is not None:
      self._have_mean = True
    if std_out is not None:
      self._have_std = True


  def beginjob(self, evt, env):
    """The beginjob() function does one-time initialisation from
    event- or environment data.  It is called at an XTC configure
    transition.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    super(mod_average, self).beginjob(evt, env)

    # XXX Potential problem if one uses stream substitution, which may
    # change between the time of substitution, and the time the stream
    # is written.  Since files are written in endjob() (which doesn't
    # trigger on an event) substitutions cannot be made there.  Just
    # note the caveats here, perhaps?

#    print "*** P _mean_out : ", self._mean_out
#    print "*** P _max_out  : ", self._max_out
#    print "*** P _std_out  : ", self._std_out

    if self._mean_out is not None:
      self._mean_out = cspad_tbx.pathsubst(self._mean_out, evt, env)

    if self._max_out is not None:
      self._max_out = cspad_tbx.pathsubst(self._max_out, evt, env)

    if self._std_out is not None:
      self._std_out = cspad_tbx.pathsubst(self._std_out, evt, env)

#    print "*** S _mean_out : ", self._mean_out
#    print "*** S _max_out  : ", self._max_out
#    print "*** S _std_out  : ", self._std_out


  def event(self, evt, env):
    """The event() function is called for every L1Accept transition.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    super(mod_average, self).event(evt, env)
    if evt.get('skip_event'):
      return
    self.logger.info("Shot number %i"  % self._nmemb)

  #signature for pyana:
  #def endjob(self, env):

  #signature for psana:
  #def endjob(self, evt, env):

  def endjob(self, obj1, obj2=None):
    """The endjob() function writes the mean and standard deviation images
    to disk.

    @param evt Event object (psana only)
    @param env Environment object
    """
    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    stats = super(mod_average, self).endjob(env)
    if stats is None:
      return

    device = cspad_tbx.address_split(self.address)[2]
    if device == 'Andor':
      beam_center = (0, 0) # XXX Fiction!
      pixel_size = 13.5e-3 # XXX Should not be hardcoded here!
      saturated_value = 10000
    elif device == 'Cspad' or device == 'Cspad2x2':
      beam_center = self.beam_center
      pixel_size = cspad_tbx.pixel_size
      saturated_value = cspad_tbx.cspad_saturated_value
    elif device == 'marccd':
      beam_center = tuple(t // 2 for t in d['mean_img'].focus())
      pixel_size = 0.079346
      saturated_value = 2**16 - 1

    if stats['nmemb'] > 0:
      if self.avg_dirname  is not None or \
         self.avg_basename is not None or \
         self._mean_out    is not None:
        d = cspad_tbx.dpack(
          active_areas=self.active_areas,
          address=self.address,
          beam_center_x=pixel_size * beam_center[0],
          beam_center_y=pixel_size * beam_center[1],
          data=stats['mean_img'],
          distance=stats['distance'],
          pixel_size=pixel_size,
          saturated_value=saturated_value,
          timestamp=cspad_tbx.evt_timestamp(stats['time']),
          wavelength=stats['wavelength'])
        if self._mean_out is not None:
          p = cspad_tbx.dwritef2(d, self._mean_out)
        else:
          p = cspad_tbx.dwritef(d, self.avg_dirname, self.avg_basename)
        self.logger.info("Average written to %s" % p)

      if self.stddev_dirname  is not None or \
         self.stddev_basename is not None or \
         self._std_out    is not None:
        d = cspad_tbx.dpack(
          active_areas=self.active_areas,
          address=self.address,
          beam_center_x=pixel_size * beam_center[0],
          beam_center_y=pixel_size * beam_center[1],
          data=stats['std_img'],
          distance=stats['distance'],
          pixel_size=pixel_size,
          saturated_value=saturated_value,
          timestamp=cspad_tbx.evt_timestamp(stats['time']),
          wavelength=stats['wavelength'])
        if self._std_out is not None:
          p = cspad_tbx.dwritef2(d, self._std_out)
        else:
          p = cspad_tbx.dwritef(d, self.stddev_dirname, self.stddev_basename)
        self.logger.info("Standard deviation written to %s" % p)

      if self.max_dirname  is not None or \
         self.max_basename is not None or \
         self._max_out    is not None:
        d = cspad_tbx.dpack(
          active_areas=self.active_areas,
          address=self.address,
          beam_center_x=pixel_size * beam_center[0],
          beam_center_y=pixel_size * beam_center[1],
          data=stats['max_img'],
          distance=stats['distance'],
          pixel_size=pixel_size,
          saturated_value=saturated_value,
          timestamp=cspad_tbx.evt_timestamp(stats['time']),
          wavelength=stats['wavelength'])
        if self._max_out is not None:
          p = cspad_tbx.dwritef2(d, self._max_out)
        else:
          p = cspad_tbx.dwritef(d, self.max_dirname, self.max_basename)
        self.logger.info("Max written to %s" % p)

    if stats['nfail'] == 0:
      self.logger.info("%d images processed" % stats['nmemb'])
    else:
      self.logger.warning(
        "%d images processed, %d failed" % (stats['nmemb'], stats['nfail']))


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_daq_status.py
from __future__ import absolute_import, division, print_function
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# XXX Could include injector positions.  What about laser intensities
# as read out from the diodes?
#
# $Id$

import logging
import threading
import wx

from xfel.cxi.gfx import status_plot
from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import skip_event_flag

class StatusFrame_thread(threading.Thread):
  """The XrayFrame_thread class allows Run MainLoop() to be run as a
  thread, which is necessary because all calls to wxPython must be
  made from the same thread that originally imported wxPython.

  This is all based on "Running MainLoop in a separate thread",
  http://wiki.wxpython.org/MainLoopAsThread.
  """
  def __init__(self):
    threading.Thread.__init__(self) # XXX super()?
    self.setDaemon(1)
    self.start_orig = self.start
    self.start      = self.start_local
    self.frame      = None
    self.lock       = threading.Lock()
    self.lock.acquire()
    self.start()

  def run(self):
    import wx
    app   = wx.App(0)
    frame = status_plot.StatusFrame(None, -1, "CXI experiment status")
    frame.Show()
    self.frame = frame
    self.lock.release()
    app.MainLoop()

  def start_local(self):
    """The start_local() function calls the run() function through
    self.start_orig, and exists only after self.lock has been
    released.  This eliminates a race condition which could cause
    updates to be sent to non-existent frame."""
    self.start_orig()
    self.lock.acquire()

class mod_daq_status (object) :
  # XXX Could inherit from mod_event?
  def __init__ (self) :
    self.initialize()
    self.logger = logging.getLogger(__name__)
    self.logger.setLevel(logging.INFO)
    self.display_thread = StatusFrame_thread()
    self.window = self.display_thread.frame
    self.run_id = None

  def __del__ (self):
    logging.shutdown()

  def initialize (self) :
    self.nfail = 0
    self.nshots = 0
    self._t = []
    self._wavelength = []
    self._det_z = []
    self._laser01 = []
    self._laser04 = []
    self._laser04_power = []
    self._si_foil = []

  def beginjob(self, evt, env):
    env.update(evt)
    self.initialize()
    self.run_id = evt.run()
    event = status_plot.RunNumberEvent(self.run_id)
    wx.PostEvent(self.window, event)

  def event (self, evt, env) :
    if (evt.get("skip_event")) :
      return
    self.nshots += 1

    s = None
    t = evt.getTime()
    if (t is not None):
      s = t.seconds() + (t.nanoseconds() / 1000000000)
    else :
      self.nfail += 1
      self.logger.warning("event(): no timestamp, shot skipped")
      evt.put(skip_event_flag(), "skip_event")
      return
    if (not isinstance(s, float)) :
      raise RuntimeError("Wrong type for 's': %s" % type(s).__name__)

    # XXX This hardcodes the address for the front detector!
    det_z = cspad_tbx.env_detz('CxiDs1-0|Cspad-0', env)
    if (det_z is None):
      self.nfail += 1
      self.logger.warning("event(): no distance, shot skipped")
      evt.put(skip_event_flag(), "skip_event")
      return

    laser01 = cspad_tbx.env_laser_status(env, 1)
    if laser01 is None:
      self.nfail += 1
      self.logger.warning("event(): no status for laser 1, shot skipped")
      evt.put(skip_event_flag(), 'skip_event')
      return

    laser04 = cspad_tbx.env_laser_status(env, 4)
    if laser04 is None:
      self.nfail += 1
      self.logger.warning("event(): no status for laser 4, shot skipped")
      evt.put(skip_event_flag(), 'skip_event')
      return

    # Laser power for fourth laser.  The control name was provided by
    # Jan Kern.  XXX Move to its own function in cspad_tbx?
    laser04_power = None
    if env is not None:
      pv = env.epicsStore().value('CXI:LAS:MMN:02:ROT.RBV')
      if pv is not None and len(pv.values) == 1:
        laser04_power = pv.values[0]
    if laser04_power is None:
      self.nfail += 1
      self.logger.warning("event(): no power for laser 4, shot skipped")
      evt.put(skip_event_flag(), 'skip_event')
      return

    si_foil = cspad_tbx.env_sifoil(env)
    if (si_foil is None):
      self.nfail += 1
      self.logger.warning("event(): no Si-foil thickness, shot skipped")
      evt.put(skip_event_flag(), "skip_event")
      return
    if (not (isinstance(si_foil, float) or isinstance(si_foil, int))) :
      raise RuntimeError("Wrong type for 'si_foil': %s"% type(si_foil).__name__)

    wavelength = cspad_tbx.evt_wavelength(evt)
    if (wavelength is None):
      self.nfail += 1
      self.logger.warning("event(): no wavelength, shot skipped")
      evt.put(skip_event_flag(), "skip_event")
      return

    # In order to keep all arrays the same length, only append once
    # all values have been successfully obtained.  XXX Still bugs: see
    # June run 119.
    self._t.append(s)
    self._si_foil.append(si_foil)
    self._wavelength.append(wavelength)
    self._det_z.append(det_z)
    self._laser01.append(laser01)
    self._laser04.append(laser04)
    self._laser04_power.append(laser04_power)
    if (self.nshots % 120 == 0) :
      self.update_plot()

  def update_plot (self) :
    """
    Post an update event with current plot values to redraw the window.
    """
    event = status_plot.UpdateEvent(
      self._t, self._det_z, self._laser01, self._laser04, self._laser04_power,
      self._si_foil, self._wavelength)
    wx.PostEvent(self.window, event)

  #signature for pyana:
  #def endjob(self, env):

  #signature for psana:
  #def endjob(self, evt, env):

  def endjob(self, obj1, obj2=None):
    """
    @param evt Event object (psana only)
    @param env Environment object
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    # Make sure any remaining shots are taken into account.  XXX
    # Hardcoded update frequency.
    if (self.nshots % 120 != 0) :
      self.update_plot()
    print("END OF RUN")
    wx.PostEvent(self.window, status_plot.SaveImageEvent())

    # Uncomment to close the frame immediately.  Otherwise, wouldn't
    # it be nice if the window's title bar indicated that the run has
    # ended, so that one wouldn't have to watch the controlling
    # terminal?  XXX It may be safer to post an event than to call
    # Close() directly.
#    self.window.Close()

    self.display_thread.join()


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_dump.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# $Id$

"""Output image to the file system.
"""


from __future__ import absolute_import, division, print_function

__version__ = "$Revision$"

from xfel.cxi.cspad_ana import common_mode
from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import rayonix_tbx


class mod_dump(common_mode.common_mode_correction):
  """Class for outputting images to the file system within the pyana
  analysis framework.  XXX This should eventually deprecate the
  'write_dict' dispatch from mod_hitfind.
  """

  def __init__(self, address, out_dirname, out_basename, out_format="pickle", **kwds):
    """The mod_dump class constructor stores the parameters passed from
    the pyana configuration file in instance variables.

    @param address      Full data source address of the DAQ device
    @param out_dirname  Directory portion of output image pathname
    @param out_basename Filename prefix of output image pathname
    @param out_format   Output the data as pickle or TIFF
    """

    super(mod_dump, self).__init__(address=address, **kwds)

    self._basename = cspad_tbx.getOptString(out_basename)
    self._dirname = cspad_tbx.getOptString(out_dirname)
    self._format = cspad_tbx.getOptString(out_format)


  def event(self, evt, env):
    """The event() function is called for every L1Accept transition.  It
    outputs the detector image associated with the event @p evt to the
    file system.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    super(mod_dump, self).event(evt, env)
    if (evt.get('skip_event')):
      return

    if self.cspad_img is None:
      print("No image to save for %s"%self.timestamp)
      return

    # Where the sample-detector distance is not available, set it to
    # zero.
    distance = cspad_tbx.env_distance(self.address, env, self._detz_offset)
    if distance is None:
      distance = 0

    # See r17537 of mod_average.py.
    device = cspad_tbx.address_split(self.address)[2]
    if device == 'Cspad':
      pixel_size = cspad_tbx.pixel_size
      saturated_value = cspad_tbx.cspad_saturated_value
      output_filename = self._basename
    elif device == 'Rayonix':
      pixel_size = rayonix_tbx.get_rayonix_pixel_size(self.bin_size)
      saturated_value = rayonix_tbx.rayonix_saturated_value
      output_filename = self._basename
    elif device == 'marccd':
      if distance == 0:
        distance = evt.get('marccd_distance')
      pixel_size = 0.079346
      saturated_value = 2**16 - 1
      output_filename = self._basename + evt.get(str, 'mccd_name') + "_"

    d = cspad_tbx.dpack(
      active_areas=self.active_areas,
      address=self.address,
      beam_center_x=pixel_size * self.beam_center[0],
      beam_center_y=pixel_size * self.beam_center[1],
      data=self.cspad_img.iround(), # XXX ouch!
      distance=distance,
      pixel_size=pixel_size,
      saturated_value=saturated_value,
      timestamp=self.timestamp,
      wavelength=self.wavelength)
    if self._format == "pickle":
      cspad_tbx.dwritef(d, self._dirname, output_filename)
    elif self._format == "tiff":
      cspad_tbx.write_tiff(d, self._dirname, output_filename)
    output_filename = None


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_dump_bitmap.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# $Id$

"""Output image to the file system.
"""


from __future__ import absolute_import, division, print_function

__version__ = "$Revision$"

import logging
import os
from xfel.cxi.cspad_ana import common_mode
from xfel.cxi.cspad_ana import cspad_tbx


class mod_dump_bitmap(common_mode.common_mode_correction):
  """Class for outputting images to the file system within the pyana
  analysis framework.  XXX This should eventually deprecate the
  'write_dict' dispatch from mod_hitfind.
  """

  def __init__(self, address, out_dirname, out_basename,
               binning=1, brightness=1.0, color_scheme=0,
               format='png', **kwds):
    """The mod_dump_bitmap class constructor stores the parameters passed from
    the pyana configuration file in instance variables.

    @param address      Full data source address of the DAQ device
    @param out_dirname  Directory portion of output image pathname
    @param out_basename Filename prefix of output image pathname

    """

    #define COLOR_GRAY 0
    #define COLOR_RAINBOW 1
    #define COLOR_HEAT 2
    #define COLOR_INVERT 3

    super(mod_dump_bitmap, self).__init__(address=address, **kwds)

    self._basename = cspad_tbx.getOptString(out_basename)
    self._dirname = cspad_tbx.getOptString(out_dirname)
    self._binning = cspad_tbx.getOptInteger(binning)
    self._brightness = cspad_tbx.getOptFloat(brightness)
    self._color_scheme = cspad_tbx.getOptInteger(color_scheme)
    self._format = cspad_tbx.getOptString(format)
    self._ext = self._format.lower()
    self._logger = logging.getLogger(self.__class__.__name__)
    if (not os.path.isdir(self._dirname)):
      os.makedirs(self._dirname)


  def event(self, evt, env):
    """The event() function is called for every L1Accept transition.  It
    outputs the detector image associated with the event @p evt to the
    file system.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    super(mod_dump_bitmap, self).event(evt, env)
    if (evt.get('skip_event')):
      return

    # Where the sample-detector distance is not available, set it to
    # zero.
    distance = cspad_tbx.env_distance(self.address, env, self._detz_offset)
    if distance is None:
      distance = 0

    # See r17537 of mod_average.py.
    device = cspad_tbx.address_split(self.address)[2]
    if device == 'Cspad':
      pixel_size = cspad_tbx.pixel_size
      saturated_value = cspad_tbx.cspad_saturated_value
    elif device == 'marccd':
      pixel_size = 0.079346
      saturated_value = 2**16 - 1

    from iotbx.detectors import FlexImage_d as FlexImage
    vendortype = device
    saturation = 65535
    flex_img = FlexImage(
      rawdata=self.cspad_img,
      binning=self._binning,
      vendortype=vendortype,
      brightness=self._brightness,
      saturation=saturated_value)

    flex_img.setWindow(0, 0, 1)
    flex_img.adjust(color_scheme=self._color_scheme)
    flex_img.prep_string()
    try:
      import PIL.Image as Image
    except ImportError:
      import Image
    # XXX is size//self._binning safe here?
    try:
      pil_img = Image.fromstring(
        'RGB', (flex_img.size2()//self._binning,
                flex_img.size1()//self._binning),
        flex_img.export_string)
    except NotImplementedError:
      pil_img = Image.frombytes(
        'RGB', (flex_img.size2()//self._binning,
                flex_img.size1()//self._binning),
        flex_img.export_string)

    # The output path should not contain any funny characters which may
    # not work in all environments.  This constructs a sequence number a
    # la evt_seqno() from the dictionary's timestamp.
    t = self.timestamp
    s = t[0:4] + t[5:7] + t[8:10] + t[11:13] + t[14:16] + t[17:19] + t[20:23]

    path = os.path.join(
      self._dirname, self._basename + s + '.' + self._ext)

    self._logger.info("Exporting %s" %path)
    tmp_stream = open(path, 'wb')
    pil_img.save(tmp_stream, format=self._format)
    tmp_stream.close()


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_event_code.py
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#

"""The mod_event_code module creates an "alist" for cxi.merge based on what events integrated
(using DIALS or LABELIT backends), given a set of psana evr event codes.
"""
from __future__ import absolute_import, division, print_function

import logging, os

from xfel.cxi.cspad_ana import cspad_tbx
import psana
from six.moves import zip

class mod_event_code(object):
  def __init__(
    self,
    integration_dirname,
    out_dirname,
    event_codes,
    alist_names):
    """
    @param integration_dirname directory with integration pickle files
    @param out_dirname directory for alists
    @event_codes space-seperated list of evr event codes
    @alist_names corresponding list of names to give the alists
    """

    self.logger = logging.getLogger(self.__class__.__name__)
    self.logger.setLevel(logging.INFO)

    self.integration_dirname  = cspad_tbx.getOptString(integration_dirname)
    self.out_dirname          = cspad_tbx.getOptString(out_dirname)
    self.event_codes          = cspad_tbx.getOptStrings(event_codes)
    self.alist_names          = cspad_tbx.getOptStrings(alist_names)

    self.event_codes = [int(s) for s in self.event_codes]

    # Try to guess multiprocessing method
    if 'SGE_TASK_ID'    in os.environ and \
       'SGE_TASK_FIRST' in os.environ and \
       'SGE_TASK_LAST'  in os.environ:
      if 'SGE_STEP_SIZE' in os.environ:
        assert int(os.environ['SGE_STEP_SIZE']) == 1
      if os.environ['SGE_TASK_ID'] == 'undefined' or os.environ['SGE_TASK_ID'] == 'undefined' or os.environ['SGE_TASK_ID'] == 'undefined':
        self.rank = 0
        self.size = 1
      else:
        self.rank = int(os.environ['SGE_TASK_ID']) - int(os.environ['SGE_TASK_FIRST'])
        self.size = int(os.environ['SGE_TASK_LAST']) - int(os.environ['SGE_TASK_FIRST']) + 1
    else:
      try:
        from libtbx.mpi4py import MPI
      except ImportError:
        self.rank = 0
        self.size = 1
      else:
        comm = MPI.COMM_WORLD
        self.rank = comm.Get_rank() # each process in MPI has a unique id, 0-indexed
        self.size = comm.Get_size() # size: number of processes running in this job

    # Save a dicitonary of timestamps that satisfy any of the event codes given
    self.timestamps_d = {}
    for alist in self.alist_names:
      self.timestamps_d[alist] = []

  def __del__(self):
    logging.shutdown()

  def beginjob(self, evt, env):
    pass

  def event(self, evt, env):
    """The event() function puts a "skip_event" object with value @c
    True into the event if the shot is to be skipped.

    Read the evr codes for this event and save the timestamp if it has an evr code we are
    looking for.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    if (evt.get("skip_event")):
      return

    evr = psana.Detector('evr0')
    codes = evr.eventCodes(evt)
    timestamp = cspad_tbx.evt_timestamp(cspad_tbx.evt_time(evt)) # human readable format
    logging.info("mod_event_code, rank: %d, timestamp: %s, code_list: %s"%(self.rank, timestamp, ",".join(["%d"%c for c in codes])))

    # Save this timestamp if it has an event_code w are looking for
    for alist, code in zip(self.alist_names, self.event_codes):
      if code in codes:
        self.timestamps_d[alist].append(timestamp)

  #signature for pyana:
  #def endjob(self, env):

  #signature for psana:
  #def endjob(self, evt, env):

  def endjob(self, obj1, obj2=None):
    """
    Write the alist files, seperated by rank. We do it in the endjob method so this module can be called
    before or after indexing and integration occurs during processing.

    @param evt Event object (psana only)
    @param env Environment object
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    for alist in self.alist_names:
      alist_f = open(os.path.join(self.out_dirname, "alist_%s_rank_%d.txt"%(alist, self.rank)), 'w')
      for t in self.timestamps_d[alist]:
        s = t[0:4] + t[5:7] + t[8:10] + t[11:13] + t[14:16] + t[17:19] + t[20:23]

        if os.path.exists(os.path.join(self.integration_dirname, "int-0-%s.pickle"%s)): # DIALS output
          alist_f.write(os.path.join(self.integration_dirname, "int-0-%s.pickle\n"%s))
        elif os.path.exists(os.path.join(self.integration_dirname, "int-%s_00000.pickle"%t)): # LABELIT output
          alist_f.write(os.path.join(self.integration_dirname, "int-%s_00000.pickle\n"%t))
      alist_f.close()


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_event_info.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# $Id$

from __future__ import absolute_import, division, print_function
import logging

from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import skip_event_flag

class mod_event_info(object):
  """Extract basic information from the evt and env objects for each event.
  """


  def __init__(self, address, detz_offset=575, check_beam_status=True, verbose=False, delta_k=0.0, override_energy=None, **kwds):
    """The mod_event_info class constructor stores the
    parameters passed from the pyana configuration file in instance
    variables.

    @param address           Full data source address of the DAQ
                             device
    @param detz_offset       The distance from the interaction region
                             to the back of the detector stage, in mm
    @param check_beam_status Flag used to skip checking the beam
                             parameters
    @param delta_k           Correction to the K value used when calculating
                             wavelength
    @param override_energy   Use this energy instead of what is in the
                             XTC stream
    """
    logging.basicConfig()
    self.logger = logging.getLogger(self.__class__.__name__)
    self.logger.setLevel(logging.INFO)

    # The subclasses accept keyword arguments; warn about any
    # unhandled arguments at the end of the inheritance chain.
    if len(kwds) > 0:
      self.logger.warning("Ignored unknown arguments: " +
                          ", ".join(kwds))

    # This is for messages that are picked up by Nat's monitoring program
    self.stats_logger = logging.getLogger("stats logger")
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(message)s')
    handler.setFormatter(formatter)
    self.stats_logger.addHandler(handler)
    self.stats_logger.removeHandler(self.stats_logger.handlers[0])
    self.stats_logger.setLevel(logging.INFO)

    self._detz_offset = cspad_tbx.getOptFloat(detz_offset)
    self.delta_k = cspad_tbx.getOptFloat(delta_k)
    self.override_energy = cspad_tbx.getOptFloat(override_energy)

    self.address = cspad_tbx.getOptString(address)
    self.verbose = cspad_tbx.getOptBool(verbose)
    self.check_beam_status = cspad_tbx.getOptBool(check_beam_status)
    self.distance = None
    self.sifoil = None
    self.wavelength = None # The current wavelength - set by self.event()
    self.laser_1_status = laser_status(laser_id=1)
    self.laser_4_status = laser_status(laser_id=4)


  def __del__(self):
    logging.shutdown()


  def beginjob(self, evt, env):
    """The beginjob() function does one-time initialisation from
    event- or environment data.  It is called at an XTC configure
    transition.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    # XXX Not needed now that the distance is read in the event?
    if hasattr(env, "update"):
      env.update(evt)

    self.nfail  = 0
    self.nshots = 0
    self.nmemb = 0


  def event(self, evt, env):
    """The event() function is called for every L1Accept transition.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    # Increase the event counter, even if this event is to be skipped.
    self.nshots += 1
    if (evt.get("skip_event")):
      return

    sifoil = cspad_tbx.env_sifoil(env)
    if self.check_beam_status and sifoil is None:
      self.nfail += 1
      self.logger.warning("event(): no Si-foil thickness, shot skipped")
      evt.put(skip_event_flag(), "skip_event")
      return
    if (self.sifoil is not None and self.sifoil != sifoil):
      self.logger.warning("event(): Si-foil changed mid-run: % 8i -> % 8d" %
        (self.sifoil, sifoil))
    self.sifoil = sifoil
    if self.verbose: self.logger.info("Si-foil thickness: %i" %sifoil)

    self.evt_time = cspad_tbx.evt_time(evt) # tuple of seconds, milliseconds
    self.timestamp = cspad_tbx.evt_timestamp(self.evt_time) # human readable format
    if (self.timestamp is None):
      self.nfail += 1
      self.logger.warning("event(): no timestamp, shot skipped")
      evt.put(skip_event_flag(), "skip_event")
      return
    if self.verbose: self.logger.info(self.timestamp)

    if self.override_energy is None:
      self.wavelength = cspad_tbx.evt_wavelength(evt, self.delta_k)
      if self.wavelength is None:
        if self.check_beam_status:
          self.nfail += 1
          self.logger.warning("event(): no wavelength, shot skipped")
          evt.put(skip_event_flag(), "skip_event")
          return
        else:
          self.wavelength = 0
    else:
      self.wavelength = 12398.4187/self.override_energy
    if self.verbose: self.logger.info("Wavelength: %.4f" %self.wavelength)

    self.pulse_length = cspad_tbx.evt_pulse_length(evt)
    if self.pulse_length is None:
      if self.check_beam_status:
        self.nfail += 1
        self.logger.warning("event(): no pulse length, shot skipped")
        evt.put(skip_event_flag(), "skip_event")
        return
      else:
        self.pulse_length = 0
    if self.verbose: self.logger.info("Pulse length: %s" %self.pulse_length)

    self.beam_charge = cspad_tbx.evt_beam_charge(evt)
    if self.beam_charge is None:
      if self.check_beam_status:
        self.nfail += 1
        self.logger.warning("event(): no beam charge, shot skipped")
        evt.put(skip_event_flag(), "skip_event")
        return
      else:
        self.beam_charge = 0
    if self.verbose: self.logger.info("Beam charge: %s" %self.beam_charge)

    self.injector_xyz = cspad_tbx.env_injector_xyz(env)
    #if self.injector_xyz is not None:
      #self.logger.info("injector_z: %i" %self.injector_xyz[2].value)

    self.laser_1_status.set_status(cspad_tbx.env_laser_status(env, laser_id=1), self.evt_time)
    self.laser_4_status.set_status(cspad_tbx.env_laser_status(env, laser_id=4), self.evt_time)
    self.laser_1_ms_since_change = self.laser_1_status.ms_since_last_status_change(self.evt_time)
    self.laser_4_ms_since_change = self.laser_4_status.ms_since_last_status_change(self.evt_time)
    if self.verbose:
      if self.laser_1_ms_since_change is not None:
        self.logger.info("ms since laser 1 status change: %i" %self.laser_1_ms_since_change)
      if self.laser_4_ms_since_change is not None:
        self.logger.info("ms since laser 4 status change: %i" %self.laser_4_ms_since_change)
      if self.laser_1_status is not None and self.laser_1_status.status is not None:
        self.logger.info("Laser 1 status: %i" %int(self.laser_1_status.status))
      if self.laser_4_status is not None and self.laser_4_status.status is not None:
        self.logger.info("Laser 4 status: %i" %int(self.laser_4_status.status))


  #signature for pyana:
  #def endjob(self, env):

  #signature for psana:
  #def endjob(self, evt, env):

  def endjob(self, obj1, obj2=None):
    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2


class laser_status(object):

  _status = None
  status_change_timestamp = None

  def __init__(self, laser_id, status=None):
    self._status = status

  @property
  def status(self):
    return self._status

  def set_status(self, status, evt_time):
    if self._status is not None and self._status != status:
      self.status_change_timestamp = evt_time
    self._status = status

  def ms_since_last_status_change(self, evt_time):
    if self.status_change_timestamp is not None:
      return (1000 * (evt_time[0] - self.status_change_timestamp[0])
              + (evt_time[1] - self.status_change_timestamp[1]))


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_filter.py
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# $Id$

"""The mod_filter module extracts timestamps from a file and uses them
to filter events.  A single valid timestamp can be embedded anywhere
in a word, where individual words must be separated by any amount of
white space.  Lines where the first non-white space character is a
hash mark are ignored.

By default, only events whose timestamps match are passed through to
downwind modules.  If @c negate is @c True, mismatching events are
selected instead.  Events with missing timestamps are always skipped.
"""
from __future__ import absolute_import, division, print_function
from six.moves import range

__version__ = "$Revision$"

import logging
import re

from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import skip_event_flag

class mod_filter(object):
  def __init__(
    self, timestamps_path=None, timestamps_interval=None, negate="False"):
    """Extracts timestamps from the file whose name is the string
    pointed to by @p timestamps_path.

    @param timestamps_path     Path to file containing timestamps
    @param timestamps_interval Comma-separated inclusive endpoints of
                               a timestamp interval.  The lower or the
                               upper endpoint may be omitted, in which
                               case it will be treated as -infinity or
                               +infinity.
    @param negate              Select shots not matching any of the
                               timestamps
    """

    self.logger = logging.getLogger(self.__class__.__name__)
    self.logger.setLevel(logging.INFO)

    if (not((timestamps_path is None) ^ (timestamps_interval is None))):
      raise RuntimeError(
        "Must specify either timestamps_path or timestamps_interval")

    self.negate = cspad_tbx.getOptBool(negate)

    if (timestamps_path is not None):
      p_old = re.compile(r"\d{4}-\d{2}-\d{2}T\d{2}:\d{2}Z\d{2}\.\d{3}")
      p_new = re.compile(r"\d{17}")
      f = open(timestamps_path, "r")
      self.timestamps_list = []
      for line in f.readlines():
        s = line.strip()
        if (len(s) == 0 or s[0] == "#"):
          continue
        for t in s.split():
          m = p_old.findall(t)
          if len(m) == 0:
            m = p_new.findall(t)
          if (len(m) == 1):
            self.timestamps_list.append(self._ts2sms(m[0]))
      f.close()
      self.timestamps_interval = None
    else:
      try:
        s = timestamps_interval.split(",")
        if (len(s) == 1 or len(s) == 2 and len(s[1]) == 0):
          self.timestamps_interval = (self._ts2sms(s[0]), None)
        elif (len(s) == 2 and len(s[0]) == 0):
          self.timestamps_interval = (None, self._ts2sms(s[1]))
        elif (len(s) == 2):
          self.timestamps_interval = (self._ts2sms(s[0]), self._ts2sms(s[1]))
        else:
          raise ValueError()

        # Ensure lower endpoint is earlier than upper endpoint.
        if (self.timestamps_interval[0] is not None and
            self.timestamps_interval[1] is not None and
            self._timestamp_compar(self.timestamps_interval[0],
                                   self.timestamps_interval[1]) > 0):
          raise ValueError()

      except ValueError:
        raise RuntimeError(
          "Failed to parse timestamp interval %s" % timestamps_interval)
      self.timestamps_list = None

    self.naccepted = 0
    self.nshots = 0


  def __del__(self):
    logging.shutdown()


  def _timestamp_compar(self, l, r):
    """The _timestamp_compar() function returns an integer less than,
    equal to, or greater than zero if @p l is considered to be
    respectively less than, equal to or greater than @p r.

    @param l First timestamp, two-tuple of s and ms
    @param r Second timestamp, two-tuple of s and ms
    @return  Negative if @p l < @p r, zero if @p l == @p r, or
             positive if @p l > @p r
    """

    for i in range(2):
      if (l[i] < r[i]):
        return -1
      if (l[i] > r[i]):
        return +1
    return 0


  def _tir(self, t, i):
    """The _tir() function returns @c True if the timestamp @p t lies
    in the inclusive interval given by @p i, and @c False otherwise.
    """

    if (i[0] is not None and self._timestamp_compar(t, i[0]) < 0 or
        i[1] is not None and self._timestamp_compar(t, i[1]) > 0):
      return False
    return True


  def _ts2sms(self, timestamp):
    """The _ts2sms() function converts a string representation of a
    timestamp to a two-tuple of seconds and milliseconds since the
    epoch.  The function raises @c ValueError if @p timestamp cannot
    be interpreted.  The _ts2sms() function is the inverse of
    cspad_tbx.evt_timestamp().

    @param timestamp String representation of human-readable ISO 8601
                     timestamp
    @return          Tuple of the time in seconds and milliseconds
                     since the epoch
    """

    from calendar import timegm
    from time import strptime

    if (len(timestamp) != 17 and (len(timestamp) != 23 or timestamp[19] != ".")):
      raise ValueError()

    if len(timestamp) == 17:
      s = timegm(strptime(timestamp[0:14], "%Y%m%d%H%M%S"))
      ms = int(timestamp[14:])

    else:
      # int() and strptime() both raise ValueError on error.
      s = timegm(strptime(timestamp[0:19], "%Y-%m-%dT%H:%MZ%S"))
      ms = int(timestamp[20:23])

    return (s, ms)


  def beginjob(self, evt, env):
    pass


  def event(self, evt, env):
    """The event() function puts a "skip_event" object with value @c
    True into the event if the shot is to be skipped.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    self.nshots += 1
    if (evt.get("skip_event")):
      return

    if (self.timestamps_list is not None):
      t = cspad_tbx.evt_time(evt)
      if (t is None):
        self.logger.warning("event(): no timestamp, shot skipped. Shot: %s"%self.nshots)
        evt.put(skip_event_flag(), "skip_event")
        return
      elif (self.negate and t in self.timestamps_list):
        evt.put(skip_event_flag(), "skip_event")
        return
      elif (not self.negate and t not in self.timestamps_list):
        evt.put(skip_event_flag(), "skip_event")
        return

    else:
      t = cspad_tbx.evt_time(evt)
      if (t is None):
        self.logger.warning("event(): no timestamp, shot skipped. Shot: %s"%self.nshots)
        evt.put(skip_event_flag(), "skip_event")
        return
      if (self.negate and self._tir(t, self.timestamps_interval)):
        evt.put(skip_event_flag(), "skip_event")
        return
      elif (not self.negate and not self._tir(t, self.timestamps_interval)):
        evt.put(skip_event_flag(), "skip_event")
        return

    self.logger.info("event(): event accepted. Shot: %s, TS: %s"%(self.nshots,cspad_tbx.evt_timestamp(t)))
    self.naccepted += 1

  #signature for pyana:
  #def endjob(self, env):

  #signature for psana:
  #def endjob(self, evt, env):

  def endjob(self, obj1, obj2=None):
    """
    @param evt Event object (psana only)
    @param env Environment object
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    self.logger.info(
      "Saw %d shots, accepted %d, skipped %d" %
      (self.nshots, self.naccepted, self.nshots - self.naccepted))


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_hdf5.py
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# $Id$

"""The mod_hdf5 module writes all events to a single HDF5
(hierarchical data format, version 5) file.  XXX Not sure how this
module will behave under multiprocessing.
"""
from __future__ import absolute_import, division, print_function

__version__ = "$Revision$"

import h5py

from xfel.cxi.cspad_ana import common_mode
from xfel.cxi.cspad_ana import cspad_tbx


class mod_hdf5(common_mode.common_mode_correction):
  def __init__(self, address, path, **kwds):
    """The constructor stores the path the output HDF5 file.  All
    intermediate directories must exist.

    @param address Address string XXX Que?!
    @param path    Path to output HDF5 file
    """

    super(mod_hdf5, self).__init__(address=address, **kwds)
    self._file = h5py.File(cspad_tbx.getOptString(path), 'w')


  def __del__(self):
    """The destructor closes the HDF5 file opened by __init__().
    """

    self._file.close()


  def event(self, evt, env):
    """The event() function creates a HDF5 group for the event, unless
    it contains a "skip_event" object with value @c True.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    super(mod_hdf5, self).event(evt, env)
    if (evt.get('skip_event')):
      return

    # If no detector distance is available set it to NaN, since
    # Python's None is not permitted in HDF5
    distance = cspad_tbx.env_distance(self.address, env, self._detz_offset)
    if distance is None:
      distance = float('nan')

    cspad_tbx.hdf5pack(
      hdf5_file=self._file,
      active_areas=self.active_areas,
      address=self.address,
      attenuation=self.sifoil,
      beam_center_x=cspad_tbx.pixel_size * self.beam_center[0],
      beam_center_y=cspad_tbx.pixel_size * self.beam_center[1],
      ccd_image_saturation=cspad_tbx.cspad_saturated_value,
      data=self.cspad_img,
      distance=distance,
      pixel_size=cspad_tbx.pixel_size,
      pulse_length=self.pulse_length,
      saturated_value=cspad_tbx.cspad_saturated_value,
      timestamp=self.timestamp,
      wavelength=self.wavelength,
      xtal_target=repr(None))


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_hitfind.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# $Id$

"""Hitfinding for CSPad images

XXX
"""
from __future__ import absolute_import, division, print_function
from six.moves import zip

__version__ = "$Revision$"

from scitbx.array_family import flex
from xfel.cxi.cspad_ana.hitfinder_tbx import distl_hitfinder
from xfel.cxi.cspad_ana import common_mode
from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import rayonix_tbx
from xfel.cxi.cspad_ana import skip_event_flag
from iotbx.detectors.cspad_detector_formats import detector_format_version as detector_format_function
#import getpass

# import matplotlib
# matplotlib.use("PDF")

class mod_hitfind(common_mode.common_mode_correction, distl_hitfinder):
  """Class for hitfinding within the pyana framework
  """

  def __init__(self,
               address,
               dispatch               = None,
               integration_dirname    = None,
               integration_basename   = None,
               out_dirname            = None,
               out_basename           = None,
               roi                    = None,
               distl_min_peaks        = None,
               distl_flags            = None,
               threshold              = None,
               xtal_target            = None,
               negate_hits            = False,
               trial_id               = None,
               db_logging             = False,
               progress_logging       = False,
               sql_buffer_size        = 1,
               db_host                = None,
               db_name                = None,
               db_table_name          = None,
               db_experiment_tag      = None,
               db_user                = None,
               db_password            = None,
               db_tags                = None,
               trial                  = None,
               rungroup_id            = None,
               db_version             = 'v1',
               **kwds):
    """The mod_hitfind class constructor stores the parameters passed
    from the pyana configuration file in instance variables.  All
    parameters, except @p address are optional, and hence need not be
    defined in pyana.cfg.

    @param address      Full data source address of the DAQ device
    @param dispatch     Function to call
    @param integration_dirname
                        Directory portion of output integration file
                        pathname
    @param integration_basename
                        Filename prefix of output integration file
                        pathname
    @param out_dirname  Directory portion of output image pathname
    @param out_basename Filename prefix of output image pathname
    @param roi          Region of interest for thresholding, on the
                        form fast_low:fast_high,slow_low:slow_high
    @param threshold    Minimum value in region of interest to pass
    """

    super(mod_hitfind, self).__init__(address=address, **kwds)

    self.m_dispatch             = cspad_tbx.getOptString(dispatch)
    self.m_integration_basename = cspad_tbx.getOptString(integration_basename)
    self.m_integration_dirname  = cspad_tbx.getOptString(integration_dirname)
    self.m_out_basename         = cspad_tbx.getOptString(out_basename)
    self.m_out_dirname          = cspad_tbx.getOptString(out_dirname)
    self.m_distl_min_peaks      = cspad_tbx.getOptInteger(distl_min_peaks)
    self.m_distl_flags          = cspad_tbx.getOptStrings(distl_flags)
    self.m_threshold            = cspad_tbx.getOptInteger(threshold)
    self.m_xtal_target          = cspad_tbx.getOptString(xtal_target)
    self.m_negate_hits          = cspad_tbx.getOptBool(negate_hits)
    self.m_trial_id             = cspad_tbx.getOptInteger(trial_id)
    self.m_db_logging           = cspad_tbx.getOptBool(db_logging)
    self.m_progress_logging     = cspad_tbx.getOptBool(progress_logging)
    self.m_sql_buffer_size      = cspad_tbx.getOptInteger(sql_buffer_size)
    self.m_db_host              = cspad_tbx.getOptString(db_host)
    self.m_db_name              = cspad_tbx.getOptString(db_name)
    self.m_db_table_name        = cspad_tbx.getOptString(db_table_name)
    self.m_db_experiment_tag    = cspad_tbx.getOptString(db_experiment_tag)
    self.m_db_user              = cspad_tbx.getOptString(db_user)
    self.m_db_password          = cspad_tbx.getOptString(db_password)
    self.m_db_tags              = cspad_tbx.getOptString(db_tags)
    self.m_trial                = cspad_tbx.getOptInteger(trial)
    self.m_rungroup_id          = cspad_tbx.getOptInteger(rungroup_id)
    self.m_db_version           = cspad_tbx.getOptString(db_version)
    # A ROI should not contain any ASIC boundaries, as these are
    # noisy.  Hence circular ROI:s around the beam centre are probably
    # not such a grand idea.
    self.m_roi = cspad_tbx.getOptROI(roi)

    # Verify that dist_min_peaks is either "restrictive" or
    # "permissive", but not both.  ^ is the logical xor operator
    if self.m_distl_min_peaks is not None:
      if (not (('permissive'  in self.m_distl_flags) ^
               ('restrictive' in self.m_distl_flags))):
        raise RuntimeError("""Sorry, with the distl_min_peaks option,
          distl_flags must be set to 'permissive' or 'restrictive'.""")
      if (self.m_roi is not None):
        raise RuntimeError("""Sorry, either specify region of interest
          (roi) or distl_min_peaks, but not both.""")

    if self.m_db_logging:
      self.buffered_sql_entries = []
      assert self.m_sql_buffer_size >= 1

    if self.m_progress_logging:
      if self.m_db_version == 'v1':
        self.buffered_progress_entries = []
        assert self.m_sql_buffer_size >= 1
        self.isoforms = {}
      elif self.m_db_version == 'v2':
        from xfel.ui import db_phil_str
        from libtbx.phil import parse
        extra_phil = """
        input {
          trial = None
            .type = int
          trial_id = None
            .type = int
          rungroup = None
            .type = int
        }
        """
        self.db_params = parse(db_phil_str + extra_phil).extract()
        self.db_params.experiment_tag = self.m_db_experiment_tag
        self.db_params.db.host = self.m_db_host
        self.db_params.db.name = self.m_db_name
        self.db_params.db.user = self.m_db_user
        self.db_params.db.password = self.m_db_password
        self.db_params.input.trial = self.m_trial
        self.db_params.input.rungroup = self.m_rungroup_id

    if self.m_db_tags is None:
      self.m_db_tags = ""


  def beginjob(self, evt, env):
    """The beginjob() function does one-time initialisation from
    event- or environment data.  It is called at an XTC configure
    transition.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    super(mod_hitfind, self).beginjob(evt, env)
    self.set_up_hitfinder(env)

    if self.m_db_logging:
      from cxi_xdr_xes.cftbx.cspad_ana import db
      self.logger.info("Connecting to db...")
      dbobj = db.dbconnect(self.m_db_host, self.m_db_name, self.m_db_user, self.m_db_password)
      assert dbobj.open
      self.logger.info("Connected.")

      try:
        self.trial = self.m_trial_id # TODO: beat the race condition and use db.get_next_trial_id if
                                      # this is not set or is zero or less
        db.create_tables(dbobj, self.m_db_table_name)

      except Exception as e:
        self.logger.info("Couldn't create root tables: %s"%(e))
      dbobj.close()

  def event(self, evt, env):
    """The event() function is called for every L1Accept transition.
    XXX more?

    Previously, common-mode correction was applied only after initial
    threshold filtering.  Since the common_mode class applies the
    (lengthy) common-mode correction immediately after reading the
    image from the stream, this optimisation is currently not
    (elegantly) doable.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    super(mod_hitfind, self).event(evt, env)
    if (evt.get("skip_event")):
      return

    # This module only applies to detectors for which a distance is
    # available.
    distance = cspad_tbx.env_distance(self.address, env, self._detz_offset)
    if distance is None:
      self.nfail += 1
      self.logger.warning("event(): no distance, shot skipped")
      evt.put(skip_event_flag(), "skip_event")
      return

    device = cspad_tbx.address_split(self.address)[2]

    # ***** HITFINDING ***** XXX For hitfinding it may be interesting
    # to look at the fraction of subzero pixels in the dark-corrected
    # image.
    if (self.m_threshold is not None):
      # If a threshold value is given it can be applied in one of three ways:
      #    1.  Apply it over the whole image
      if (self.m_roi is None and self.m_distl_min_peaks is None):
        vmax = flex.max(self.cspad_img)
        if (vmax < self.m_threshold):
          if not self.m_negate_hits:
            # Tell downstream modules to skip this event if the threshold was not met.
            evt.put(skip_event_flag(), "skip_event")
            return
        elif self.m_negate_hits:
          evt.put(skip_event_flag(), "skip_event")
          return

      #    2. Apply threshold over a rectangular region of interest.
      elif (self.m_roi is not None):
        vmax = flex.max(self.cspad_img[self.m_roi[2]:self.m_roi[3],
                                       self.m_roi[0]:self.m_roi[1]])
        if (vmax < self.m_threshold):
          if not self.m_negate_hits:
            evt.put(skip_event_flag(), "skip_event")
            return
        elif self.m_negate_hits:
          evt.put(skip_event_flag(), "skip_event")
          return

      #    3. Determine the spotfinder spots within the central ASICS, and accept the
      #       image as a hit if there are m_distl_min_peaks exceeding m_threshold.
      #       As a further requirement, the peaks must exceed 2.5 * the 90-percentile
      #       pixel value of the central ASICS.  This filter was added to avoid high-background
      #       false positives.
      elif (self.m_distl_min_peaks is not None):
        if device == 'marccd':
          self.hitfinder_d['BEAM_CENTER_X'] = self.beam_center[0]
          self.hitfinder_d['BEAM_CENTER_Y'] = self.beam_center[1]
        elif device == 'Rayonix':
          self.hitfinder_d['BEAM_CENTER_X'] = self.beam_center[0]
          self.hitfinder_d['BEAM_CENTER_Y'] = self.beam_center[1]

        peak_heights,outvalue = self.distl_filter(
          self.address,
          self.cspad_img.iround(), # XXX correct?
          distance,
          self.timestamp,
          self.wavelength)
        if ('permissive' in self.m_distl_flags):
          number_of_accepted_peaks = (peak_heights > self.m_threshold).count(True)
        else:
          number_of_accepted_peaks = ((peak_heights > self.m_threshold).__and__(outvalue==0)).count(True)

        sec,ms = cspad_tbx.evt_time(evt)
        evt_time = sec + ms/1000
        self.stats_logger.info("BRAGG %.3f %d" %(evt_time, number_of_accepted_peaks))

        skip_event = False
        if number_of_accepted_peaks < self.m_distl_min_peaks:
          self.logger.info("Subprocess %02d: Spotfinder NO  HIT image #%05d @ %s; %d spots > %d" %(
            env.subprocess(), self.nshots, self.timestamp, number_of_accepted_peaks, self.m_threshold))

          if not self.m_negate_hits:
            skip_event = True
        else:
          self.logger.info("Subprocess %02d: Spotfinder YES HIT image #%05d @ %s; %d spots > %d" %(
            env.subprocess(), self.nshots, self.timestamp, number_of_accepted_peaks, self.m_threshold))

          if self.m_negate_hits:
            skip_event = True

        if skip_event:
          if self.m_db_logging:
            # log misses to the database
            self.queue_entry((self.trial, evt.run(), "%.3f"%evt_time, number_of_accepted_peaks, distance,
                              self.sifoil, self.wavelength, False, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, self.m_db_tags))
          evt.put(skip_event_flag(), "skip_event")
          return
        # the indexer will log this hit when it is ran. Bug: if the spotfinder is ran by itself, this
        # hit will not be logged in the db.
        evt.put(number_of_accepted_peaks, 'sfspots')

    self.logger.info("Subprocess %02d: process image #%05d @ %s" %
                     (env.subprocess(), self.nshots, self.timestamp))

    # See r17537 of mod_average.py.
    if device == 'Cspad':
      pixel_size = cspad_tbx.pixel_size
      saturated_value = cspad_tbx.cspad_saturated_value
    elif device == 'marccd':
      pixel_size = evt.get("marccd_pixel_size")
      saturated_value = evt.get("marccd_saturated_value")
    elif device == 'Rayonix':
      pixel_size = rayonix_tbx.get_rayonix_pixel_size(self.bin_size)
      saturated_value = rayonix_tbx.rayonix_saturated_value

    d = cspad_tbx.dpack(
      active_areas=self.active_areas,
      address=self.address,
      beam_center_x=pixel_size * self.beam_center[0],
      beam_center_y=pixel_size * self.beam_center[1],
      data=self.cspad_img.iround(), # XXX ouch!
      distance=distance,
      pixel_size=pixel_size,
      saturated_value=saturated_value,
      timestamp=self.timestamp,
      wavelength=self.wavelength,
      xtal_target=self.m_xtal_target)

    if (self.m_dispatch == "index"):
      import sys
      from xfel.cxi.integrate_image_api import integrate_one_image
      info = integrate_one_image(d,
                                 integration_dirname  = self.m_integration_dirname,
                                 integration_basename = self.m_integration_basename)
      sys.stdout = sys.__stdout__
      sys.stderr = sys.__stderr__

      indexed = info is not None and hasattr(info, 'spotfinder_results')
      if self.m_progress_logging:
        if self.m_db_version == 'v1':
          if indexed:
            # integration pickle dictionary is available here as info.last_saved_best
            if info.last_saved_best["identified_isoform"] is not None:
              #print info.last_saved_best.keys()
              from cxi_xdr_xes.cftbx.cspad_ana import db
              dbobj = db.dbconnect(self.m_db_host, self.m_db_name, self.m_db_user, self.m_db_password)
              cursor = dbobj.cursor()
              if info.last_saved_best["identified_isoform"] in self.isoforms:
                PM, indices, miller_id = self.isoforms[info.last_saved_best["identified_isoform"]]
              else:
                from xfel.xpp.progress_support import progress_manager
                PM = progress_manager(info.last_saved_best,self.m_db_experiment_tag, self.m_trial_id, self.m_rungroup_id, evt.run())
                indices, miller_id = PM.get_HKL(cursor)
                # cache these as they don't change for a given isoform
                self.isoforms[info.last_saved_best["identified_isoform"]] = PM, indices, miller_id
              if self.m_sql_buffer_size > 1:
                self.queue_progress_entry(PM.scale_frame_detail(self.timestamp,cursor,do_inserts=False))
              else:
                PM.scale_frame_detail(self.timestamp,cursor,do_inserts=True)
                dbobj.commit()
                cursor.close()
                dbobj.close()
        elif self.m_db_version == 'v2':
          key_low = 'cctbx.xfel.radial_average.two_theta_low'
          key_high = 'cctbx.xfel.radial_average.two_theta_high'
          tt_low = evt.get(key_low)
          tt_high = evt.get(key_high)

          from xfel.ui.db.dxtbx_db import log_frame
          if indexed:
            n_spots = len(info.spotfinder_results.images[info.frames[0]]['spots_total'])
          else:
            sfspots = evt.get('sfspots')
            if sfspots is None:
              if info is None or not isinstance(info, int):
                n_spots = 0
              else:
                n_spots = info
            else:
              n_spots = sfspots

          if indexed:
            known_setting = info.horizons_phil.known_setting
            indexed_setting = info.organizer.info['best_integration']['counter']
            if known_setting is None or known_setting == indexed_setting:
              from xfel.command_line.frame_unpickler import construct_reflection_table_and_experiment_list
              c = construct_reflection_table_and_experiment_list(info.last_saved_best, None, pixel_size, proceed_without_image=True)
              c.assemble_experiments()
              c.assemble_reflections()
              log_frame(c.experiment_list, c.reflections, self.db_params, evt.run(), n_spots, self.timestamp, tt_low, tt_high)
            else:
              print("Not logging %s, wrong bravais setting (expecting %d, got %d)" % (
                self.timestamp, known_setting, indexed_setting))
          else:
            log_frame(None, None, self.db_params, evt.run(), n_spots, self.timestamp, tt_low, tt_high)

      if self.m_db_logging:
        sec,ms = cspad_tbx.evt_time(evt)
        evt_time = sec + ms/1000
        sfspots = evt.get('sfspots')
        if sfspots is None:
          if indexed:
            n_spots = len(info.spotfinder_results.images[info.frames[0]]['spots_total'])
          else:
            n_spots = 0
        else:
          n_spots = sfspots

        if indexed:
          mosaic_bloc_rotation = info.last_saved_best.get('ML_half_mosaicity_deg', [0])[0]
          mosaic_block_size = info.last_saved_best.get('ML_domain_size_ang', [0])[0]
          ewald_proximal_volume = info.last_saved_best.get('ewald_proximal_volume', [0])[0]

          obs = info.last_saved_best['observations'][0]
          cell_a, cell_b, cell_c, cell_alpha, cell_beta, cell_gamma = obs.unit_cell().parameters()
          pointgroup = info.last_saved_best['pointgroup']
          resolution = obs.d_min()
        else:
          mosaic_bloc_rotation = mosaic_block_size = ewald_proximal_volume = cell_a = cell_b = cell_c = \
            cell_alpha = cell_beta = cell_gamma = spacegroup = resolution = 0

        self.queue_entry((self.trial, evt.run(), "%.3f"%evt_time, n_spots, distance,
                          self.sifoil, self.wavelength, indexed, mosaic_bloc_rotation,
                          mosaic_block_size, ewald_proximal_volume, pointgroup, cell_a,
                          cell_b, cell_c, cell_alpha, cell_beta, cell_gamma, resolution,
                          self.m_db_tags))

      if (not indexed):
        evt.put(skip_event_flag(), "skip_event")
        return

    elif (self.m_dispatch == "nop"):
      pass

    elif (self.m_dispatch == "view"): #interactive image viewer

      args = ["indexing.data=dummy"]
      detector_format_version = detector_format_function(
        self.address, evt.GetTime())
      if detector_format_version is not None:
        args += ["distl.detector_format_version=%" % detector_format_version]

      from xfel.phil_preferences import load_cxi_phil
      horizons_phil = load_cxi_phil(self.m_xtal_target, args)
      horizons_phil.indexing.data = d

      from xfel.cxi import display_spots
      display_spots.parameters.horizons_phil = horizons_phil
      display_spots.wrapper_of_callback().display(horizons_phil.indexing.data)

    elif (self.m_dispatch == "spots"): #interactive spotfinder viewer

      args = ["indexing.data=dummy"]
      detector_format_version = detector_format_function(
        self.address, evt.GetTime())
      if detector_format_version is not None:
        args += ["distl.detector_format_version=%s" % detector_format_version]

      from xfel.phil_preferences import load_cxi_phil
      horizons_phil = load_cxi_phil(self.m_xtal_target, args)
      horizons_phil.indexing.data = d

      from xfel.cxi import display_spots
      display_spots.parameters.horizons_phil = horizons_phil

      from rstbx.new_horizons.index import pre_indexing_validation,pack_names
      pre_indexing_validation(horizons_phil)
      imagefile_arguments = pack_names(horizons_phil)
      horizons_phil.persist.show()
      from spotfinder.applications import signal_strength
      info = signal_strength.run_signal_strength_core(horizons_phil,imagefile_arguments)

      work = display_spots.wrapper_of_callback(info)
      work.display_with_callback(horizons_phil.indexing.data)

    elif (self.m_dispatch == "write_dict"):
      self.logger.warning(
        "event(): deprecated dispatch 'write_dict', use mod_dump instead")
      if (self.m_out_dirname  is not None or
          self.m_out_basename is not None):
        cspad_tbx.dwritef(d, self.m_out_dirname, self.m_out_basename)

    # Diagnostic message emitted only when all the processing is done.
    if (env.subprocess() >= 0):
      self.logger.info("Subprocess %02d: accepted #%05d @ %s" %
                       (env.subprocess(), self.nshots, self.timestamp))
    else:
      self.logger.info("Accepted #%05d @ %s" %
                       (self.nshots, self.timestamp))

  #signature for pyana:
  #def endjob(self, env):

  #signature for psana:
  #def endjob(self, evt, env):

  def endjob(self, obj1, obj2=None):
    """The endjob() function logs the number of processed shots.

    @param evt Event object (psana only)
    @param env Environment object
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    super(mod_hitfind, self).endjob(env)
    if (env.subprocess() >= 0):
      self.logger.info("Subprocess %02d: processed %d shots" %
                       (env.subprocess(), self.nshots))
    else:
      self.logger.info("Processed %d shots" % self.nshots)

    if self.m_db_logging:
      self.commit_entries()

    if self.m_progress_logging and self.m_db_version == 'v1':
      self.commit_progress_entries()

  def queue_entry(self, entry):
    self.buffered_sql_entries.append(entry)
    if len(self.buffered_sql_entries) >= self.m_sql_buffer_size:
      self.commit_entries()

  def commit_entries(self):
    if len(self.buffered_sql_entries) > 0:
      from cxi_xdr_xes.cftbx.cspad_ana import db
      dbobj = db.dbconnect(self.m_db_host, self.m_db_name, self.m_db_user, self.m_db_password)
      cursor = dbobj.cursor()
      cmd = "INSERT INTO %s (trial,run,eventstamp,hitcount,distance,sifoil,wavelength,indexed,\
mosaic_block_rotation,mosaic_block_size,ewald_proximal_volume,spacegroup,\
cell_a,cell_b,cell_c,cell_alpha,cell_beta,cell_gamma,resolution,tags) VALUES "%(self.m_db_table_name)
      comma = ""
      for entry in self.buffered_sql_entries:
        cmd += comma + "(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,'%s',%s,%s,%s,%s,%s,%s,%s,'%s')"%entry
        comma = ", "
      cursor.execute(cmd)
      dbobj.commit()
      dbobj.close()
      self.buffered_sql_entries = []

  def queue_progress_entry(self, entry):
    self.buffered_progress_entries.append(entry)
    if len(self.buffered_progress_entries) >= self.m_sql_buffer_size:
      self.commit_progress_entries()

  def commit_progress_entries(self):
    if len(self.buffered_progress_entries) > 0:
      print("Commiting %d entries to the db"%len(self.buffered_progress_entries))

      from cxi_xdr_xes.cftbx.cspad_ana import db
      dbobj = db.dbconnect(self.m_db_host, self.m_db_name, self.m_db_user, self.m_db_password)
      cursor = dbobj.cursor()

      for entry in self.buffered_progress_entries:
        frame_sql, parameters, kwargs = entry['frame']

        cursor.execute(frame_sql, parameters[0])
        frame_id = cursor.lastrowid

        _, _, kwargs = entry['observations']

        kwargs['frames_id'] = [frame_id] * len(kwargs['frames_id'])

        query = ("INSERT INTO `%s_observations` (" % self.m_db_experiment_tag) \
                + ", ".join(kwargs) + ") values (" \
                + ", ".join(["%s"] * len(kwargs)) + ")"
        try:
          parameters = list(zip(*list(kwargs.values())))
        except TypeError:
          parameters = [list(kwargs.values())]
        cursor.executemany(query, parameters)

      dbobj.commit()
      cursor.close()
      dbobj.close()
      self.buffered_progress_entries = []


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_illumination_filter.py
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# $Id$

"""The mod_illumination_filter module filter events by their
illumination condition.  When illumination conditions matter, laser 4
should always be off, and laser 1 must have had its state changed for
least laser_wait_time ms before the event.  Events with missing
timestamps are always skipped.
"""
from __future__ import absolute_import, division, print_function

__version__ = "$Revision$"

import logging

from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana.mod_event_info import laser_status
from xfel.cxi.cspad_ana import skip_event_flag

class mod_illumination_filter(object):
  def __init__(self, illumination, laser_wait_time="2000"):
    """Initialise laser status, and validate input.  The @p
    illumination parameter is mandatory.

    @param illumination    If @c dark or @c light, only pass dark or
                           light shots, respectively.  If @c other,
                           pass events that are neither light or dark,
                           due to recent state changes to the lasers.
    @param laser_wait_time Number of ms the lasers have to be stable
                           before classifying events as light or dark.
                           Jan F. Kern recommends a value between 1000
                           and 2000 ms.
    """

    self.logger = logging.getLogger(self.__class__.__name__)
    self.logger.setLevel(logging.INFO)

    self._filter = cspad_tbx.getOptString(illumination)
    if (self._filter != "dark" and
        self._filter != "light" and
        self._filter != "other"):
      raise RuntimeError(
        "Parameter illumination must be either "
        "\"light\", \"dark\", or \"other\"")

    self._wait = cspad_tbx.getOptFloat(laser_wait_time)
    if (self._wait is None or self._wait < 0):
      raise RuntimeError(
        "Parameter laser_wait_time must be number >= 0")

    self.laser_1 = laser_status(laser_id=1)
    self.laser_4 = laser_status(laser_id=4)

    self.naccepted = 0
    self.nshots = 0


  def __del__(self):
    logging.shutdown()


  def beginjob(self, evt, env):
    pass


  def event(self, evt, env):
    """The event() function puts a "skip_event" object with value @c
    True into the event if the shot is to be skipped.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    self.nshots += 1
    if (evt.get("skip_event")):
      return

    # Get time as a (seconds, milliseconds) tuple.
    t = cspad_tbx.evt_time(evt)
    if (t is None):
      self.logger.warning("event(): no timestamp, shot skipped")
      evt.put(skip_event_flag(), "skip_event")
      return

    # Update laser status.
    self.laser_1.set_status(cspad_tbx.env_laser_status(env, laser_id=1), t)
    self.laser_4.set_status(cspad_tbx.env_laser_status(env, laser_id=4), t)

    t1 = self.laser_1.ms_since_last_status_change(t)
    t4 = self.laser_4.ms_since_last_status_change(t)
    if (self.laser_4.status or
        (t4 is not None and t4 < self._wait) or
        (t1 is not None and t1 < self._wait)):
      # If laser 4 is on or was switched off less than self._wait ms
      # ago, the shot falls in the "other" category.  If laser 1
      # changed its state less than self._wait ms ago the shot falls
      # in the "other" category.

      if (self._filter != "other"):
        evt.put(skip_event_flag(), "skip_event")
        return

    elif (self.laser_1.status):
      # If laser 1 is on the shot falls in the "light" category.
      if (self._filter != "light"):
        evt.put(skip_event_flag(), "skip_event")
        return

    elif (not self.laser_1.status):
      # If laser 1 is off the shot falls in the "dark" category.
      if (self._filter != "dark"):
        evt.put(skip_event_flag(), "skip_event")
        return

    else:
      # NOTREACHED
      self.logger.error("Could not determine shot category")
      raise RuntimeError("XXX")

    self.naccepted += 1

  #signature for pyana:
  #def endjob(self, env):

  #signature for psana:
  #def endjob(self, evt, env):

  def endjob(self, obj1, obj2=None):
    """
    @param evt Event object (psana only)
    @param env Environment object
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    self.logger.info(
      "Saw %d shots, accepted %d, skipped %d" %
      (self.nshots, self.naccepted, self.nshots - self.naccepted))


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_image_dict.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#

"""Create the CSPAD image dict and put it in the event
"""
from __future__ import absolute_import, division, print_function

from xfel.cxi.cspad_ana import common_mode
from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import rayonix_tbx
from xfel.cxi.cspad_ana import skip_event_flag

class mod_image_dict(common_mode.common_mode_correction):
  """Class for saving the CSPAD image dict in the psana event
  """

  def __init__(self,
               address,
               out_key  = "cctbx.xfel.image_dict",
               **kwds):
    """The mod_image_dict class constructor stores the parameters passed
    from the psana configuration file in instance variables.  All
    parameters, except @p address are optional, and hence need not be
    defined in psana.cfg.

    @param address      Full data source address of the DAQ device
    @param out_key      The image dict will be saved in the event with this
                        name
    """

    super(mod_image_dict, self).__init__(address=address, **kwds)

    self.m_out_key = cspad_tbx.getOptString(out_key)


  def beginjob(self, evt, env):
    """The beginjob() function does one-time initialisation from
    event- or environment data.  It is called at an XTC configure
    transition.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    super(mod_image_dict, self).beginjob(evt, env)

  def event(self, evt, env):
    """The event() function is called for every L1Accept transition.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    super(mod_image_dict, self).event(evt, env)
    if (evt.get("skip_event")):
      return

    if self.cspad_img is None:
      return

    # This module only applies to detectors for which a distance is
    # available.
    distance = cspad_tbx.env_distance(self.address, env, self._detz_offset)
    if distance is None:
      self.nfail += 1
      self.logger.warning("event(): no distance, shot skipped")
      evt.put(skip_event_flag(), "skip_event")
      return

    device = cspad_tbx.address_split(self.address)[2]

    self.logger.info("Subprocess %02d: process image #%05d @ %s" %
                     (env.subprocess(), self.nshots, self.timestamp))

    # See r17537 of mod_average.py.
    if device == 'Cspad':
      pixel_size = cspad_tbx.pixel_size
      saturated_value = cspad_tbx.cspad_saturated_value
    elif device == 'Rayonix':
      pixel_size = rayonix_tbx.get_rayonix_pixel_size(self.bin_size)
      saturated_value = rayonix_tbx.rayonix_saturated_value
    elif device == 'marccd':
      pixel_size = evt.get("marccd_pixel_size")
      saturated_value = evt.get("marccd_saturated_value")
      if distance == 0:
        distance = evt.get("marccd_distance")

    d = cspad_tbx.dpack(
      active_areas=self.active_areas,
      address=self.address,
      beam_center_x=pixel_size * self.beam_center[0],
      beam_center_y=pixel_size * self.beam_center[1],
      data=self.cspad_img.iround(), # XXX ouch!
      distance=distance,
      pixel_size=pixel_size,
      saturated_value=saturated_value,
      timestamp=self.timestamp,
      wavelength=self.wavelength)

    evt.put(d, self.m_out_key)

    # Diagnostic message emitted only when all the processing is done.
    if (env.subprocess() >= 0):
      self.logger.info("Subprocess %02d: accepted #%05d @ %s" %
                       (env.subprocess(), self.nshots, self.timestamp))
    else:
      self.logger.info("Accepted #%05d @ %s" %
                       (self.nshots, self.timestamp))

  #signature for pyana:
  #def endjob(self, env):

  #signature for psana:
  #def endjob(self, evt, env):

  def endjob(self, obj1, obj2=None):
    """The endjob() function logs the number of processed shots.

    @param evt Event object (psana only)
    @param env Environment object
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    super(mod_image_dict, self).endjob(env)
    if (env.subprocess() >= 0):
      self.logger.info("Subprocess %02d: processed %d shots" %
                       (env.subprocess(), self.nshots))
    else:
      self.logger.info("Processed %d shots" % self.nshots)


 *******************************************************************************
