

 *******************************************************************************
scitbx/lstbx/__init__.py
"""
lstbx
"""

from __future__ import division


 *******************************************************************************


 *******************************************************************************
scitbx/lstbx/benchmarks/__init__.py
"""
lstbx/benchmarks/__init__
"""

from __future__ import division


 *******************************************************************************


 *******************************************************************************
scitbx/lstbx/boost_python/__init__.py
"""
lstbx/boost_python/__init__
"""

from __future__ import division


 *******************************************************************************


 *******************************************************************************
scitbx/lstbx/normal_eqns.py
from __future__ import absolute_import, division, print_function
import libtbx.load_env
import boost_adaptbx.boost.python as bp
bp.import_ext("scitbx_lstbx_normal_equations_ext")
import scitbx_lstbx_normal_equations_ext as ext
from scitbx_lstbx_normal_equations_ext import *

class non_linear_ls_mixin(object):
  """ Synopsis:

  Either of

  class non_linear_ls_xxx(core_non_linear_ls_xxx,
                          non_linear_ls_mixin):
    pass

  class non_linear_ls_xxx(non_linear_ls_mixin):

    # define required methods

  This class is a mixin to be inherited as shown. It requires
  core_non_linear_ls_xxx to provide some methods in the first case,
  or those methods to be defined directly in the heir in the second case:
  those that this mixin marks as not-implemented.
  """

  def parameter_vector_norm(self):
    raise NotImplementedError()

  def build_up(self, objective_only=False):
    raise NotImplementedError()

  def step_equations(self):
    raise NotImplementedError()

  def objective(self):
    raise NotImplementedError()

  def step_forward(self):
    raise NotImplementedError()

  def opposite_of_gradient(self):
    return self.step_equations().right_hand_side()

  def normal_matrix_packed_u(self):
    return self.step_equations().normal_matrix_packed_u()

  def solve(self):
    self.step_equations().solve()

  def solve_and_step_forward(self):
    self.solve()
    self.step_forward()

  def step(self):
    return self.step_equations().solution()

@bp.inject_into(linear_ls)
class _():

  def __iter__(self):
    yield self.normal_matrix_packed_u()
    yield self.right_hand_side()


non_linear_ls_with_separable_scale_factor_description = """\
* non-linear L.S. by optimising the overall scale factor alone first
  and then the other parameters alone
"""

class non_linear_ls_with_separable_scale_factor_BLAS_2(
  ext.non_linear_ls_with_separable_scale_factor__level_2_blas_impl,
  non_linear_ls_mixin):

  @property
  def description(self):
    return (non_linear_ls_with_separable_scale_factor_description +
            "* slow normal matrix computation\n")

  @property
  def debug_info(self):
    return ""


if libtbx.env.has_module('fast_linalg'):
  class non_linear_ls_with_separable_scale_factor_BLAS_3(
    ext.non_linear_ls_with_separable_scale_factor__level_3_blas_impl,
    non_linear_ls_mixin):

    @property
    def description(self):
      return (non_linear_ls_with_separable_scale_factor_description +
              "* fast normal matrix computation")

    @property
    def debug_info(self):
      import fast_linalg
      e = fast_linalg.env
      return '\n'.join((
        "\n", "*"*80,
        "*** Using OpenBLAS with %i threads on a machine with %i %s cores" %
        (e.threads, e.physical_cores, e.cpu_family),
        "*** OpenBLAS was built with the following options:",
        "*** %s" % e.build_config,
        "*"*80, "\n",
      ))


non_linear_ls_with_separable_scale_factor = \
  non_linear_ls_with_separable_scale_factor_BLAS_2


 *******************************************************************************


 *******************************************************************************
scitbx/lstbx/normal_eqns_solving.py
""" Tools to solve non-linear L.S. problems formulated with normal-equations.
"""
from __future__ import absolute_import, division, print_function

import libtbx
from scitbx.array_family import flex
from timeit import default_timer as current_time


class journaled_non_linear_ls(object):
  """ A decorator that keeps the history of the objective, gradient,
  step, etc of an underlying normal equations object. An instance of this
  class is a drop-in replacement of that underlying object, with the
  journaling just mentioned automatically happening behind the scene.
  """

  def __init__(self, non_linear_ls, journal, track_gradient, track_step):
    """ Decorate the given normal equations. The history will be accumulated
    in relevant attributes of journal. The flags track_xxx specify whether
    to journal the gradient and/or the step, a potentially memory-hungry
    operation.
    """
    self.actual = non_linear_ls
    self.journal = journal
    self.journal.objective_history = flex.double()
    if track_gradient:
      self.journal.gradient_history = []
    else:
      self.journal.gradient_history = None
    self.journal.gradient_norm_history = flex.double()
    if track_step:
      self.journal.step_history = []
    else:
      self.journal.step_history = None
    self.journal.step_norm_history = flex.double()
    self.journal.parameter_vector_norm_history = flex.double()
    if hasattr(non_linear_ls, "scale_factor"):
      self.journal.scale_factor_history = flex.double()
    else:
      self.journal.scale_factor_history = None
    self.normal_equations_building_time = 0
    self.normal_equations_solving_time = 0

  def __getattr__(self, name):
    return getattr(self.actual, name)

  def build_up(self, objective_only=False):
    t0 = current_time()
    self.actual.build_up(objective_only)
    t1 = current_time()
    self.normal_equations_building_time += t1 - t0
    if objective_only: return
    self.journal.parameter_vector_norm_history.append(
      self.actual.parameter_vector_norm())
    self.journal.objective_history.append(self.actual.objective())
    self.journal.gradient_norm_history.append(
      self.actual.opposite_of_gradient().norm_inf())
    if self.journal.gradient_history is not None:
      self.journal.gradient_history.append(-self.actual.opposite_of_gradient())
    if self.journal.scale_factor_history is not None:
      self.journal.scale_factor_history.append(self.actual.scale_factor())

  def solve(self):
    t0 = current_time()
    self.actual.solve()
    t1 = current_time()
    self.normal_equations_solving_time += t1 - t0
    self.journal.step_norm_history.append(self.actual.step().norm())
    if self.journal.step_history is not None:
      self.journal.step_history.append(self.actual.step().deep_copy())

  def step_forward(self):
    self.actual.step_forward()

  def solve_and_step_forward(self):
    self.solve()
    self.step_forward()


class iterations(object):
  """ Iterations to solve a non-linear L.S. minimisation problem.

  It is assumed that the objective function is properly scaled to be
  between 0 and 1 (or approximately so), so that this class can
  meaningfully thresholds the norm of the gradients to watch for
  convergence.

  The interface expected from the normal equations object passed to __init__
  is that of lstbx.non_linear_normal_equations_mixin

  Use classic stopping criteria: c.f. e.g.
  Methods for non-linear least-squares problems,
  K. Madsen, H.B. Nielsen, O. Tingleff,
  http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3215/pdf/imm3215.pdf

  The do_damping and do_scale_shifts function implements the shelxl damping as
  described here:
  http://shelx.uni-ac.gwdg.de/SHELX/shelx.pdf
  do_scale_shifts aslo compares the maximum shift/esd with
  convergence_as_shift_over_esd and returns boolean to indicate if
  the refinement converged according to this criterion
  These functions do nothing unless called by a derived class
  """

  track_step = False
  track_gradient = False
  track_all = False
  n_max_iterations = 100
  gradient_threshold = None
  step_threshold = None
  verbose_iterations = False

  def __init__(self, non_linear_ls, **kwds):
    """
    """
    libtbx.adopt_optional_init_args(self, kwds)
    if self.track_all: self.track_step = self.track_gradient = True
    self.non_linear_ls = journaled_non_linear_ls(non_linear_ls, self,
                                                 self.track_gradient,
                                                 self.track_step)
    self.do()

  def has_gradient_converged_to_zero(self):
    eps_1 = self.gradient_threshold
    g = self.gradient_norm_history[-1]
    return eps_1 is not None and g <= eps_1

  def had_too_small_a_step(self):
    eps_2 = self.step_threshold
    if eps_2 is None: return False
    x = self.parameter_vector_norm_history[-1]
    h = self.step_norm_history[-1]
    return h <= eps_2*(x + eps_2)

  def do_damping(self, value):
    a = self.non_linear_ls.normal_matrix_packed_u()
    a.matrix_packed_u_diagonal_add_in_place(value*a.matrix_packed_u_diagonal())

  def do_scale_shifts(self, limit_shift_over_su):
    x = self.non_linear_ls.step()
    esd = self.non_linear_ls.covariance_matrix().matrix_packed_u_diagonal()
    ls_shifts_over_su = flex.abs(x/flex.sqrt(esd))
    #max shift for the LS
    self.max_ls_shift_over_su = flex.max(ls_shifts_over_su)
    jac_tr = self.non_linear_ls.actual.\
      reparametrisation.jacobian_transpose_matching_grad_fc()
    self.shifts_over_su = jac_tr.transpose() * ls_shifts_over_su
    self.max_shift_over_su = flex.max(self.shifts_over_su)
    if self.max_shift_over_su < self.convergence_as_shift_over_esd:
      return True
    if self.max_ls_shift_over_su > limit_shift_over_su:
      shift_scale = limit_shift_over_su/self.max_ls_shift_over_su
      x *= shift_scale
    return False

  def do(self):
    raise NotImplementedError


class naive_iterations(iterations):

  def do(self):
    self.n_iterations = 0
    while self.n_iterations < self.n_max_iterations:
      self.non_linear_ls.build_up()
      if self.has_gradient_converged_to_zero(): break
      self.non_linear_ls.solve()
      if self.had_too_small_a_step(): break
      self.non_linear_ls.step_forward()
      self.n_iterations += 1

  def __str__(self):
    return "pure Gauss-Newton"


class naive_iterations_with_damping(iterations):

  damping_value = 0.0007

  def do(self):
    self.n_iterations = 0
    do_last = False
    while self.n_iterations < self.n_max_iterations:
      self.non_linear_ls.build_up()
      if self.has_gradient_converged_to_zero():
        do_last = True
      self.do_damping(self.damping_value)
      self.non_linear_ls.solve()
      step_too_small = self.had_too_small_a_step()
      self.non_linear_ls.step_forward()
      self.n_iterations += 1
      if do_last or step_too_small: break

  def __str__(self):
    return "pure Gauss-Newton with damping"


class naive_iterations_with_damping_and_shift_limit(
  naive_iterations_with_damping):

  max_shift_over_esd = 15
  convergence_as_shift_over_esd = 1e-5

  def do(self):
    self.n_iterations = 0
    do_last = False
    while self.n_iterations < self.n_max_iterations:
      self.non_linear_ls.build_up()
      if self.has_gradient_converged_to_zero():
        do_last = True
      self.do_damping(self.damping_value)
      self.non_linear_ls.solve()
      step_too_small = self.had_too_small_a_step()
      if not step_too_small:
        step_too_small = self.do_scale_shifts(self.max_shift_over_esd)
      self.non_linear_ls.step_forward()
      self.n_iterations += 1
      if do_last or step_too_small: break

  def __str__(self):
    return "pure Gauss-Newton with damping and shift scaling"


class levenberg_marquardt_iterations(iterations):

  tau = 1e-3

  @property
  def mu(self):
    return self._mu

  @mu.setter
  def mu(self, value):
    self.mu_history.append(value)
    self._mu = value

  def do(self):
    self.mu_history = flex.double()
    self.n_iterations = 0
    nu = 2
    self.non_linear_ls.build_up()
    if self.has_gradient_converged_to_zero(): return
    a = self.non_linear_ls.normal_matrix_packed_u()
    self.mu = self.tau*flex.max(a.matrix_packed_u_diagonal())
    while self.n_iterations < self.n_max_iterations:
      a.matrix_packed_u_diagonal_add_in_place(self.mu)
      objective = self.non_linear_ls.objective()
      g = -self.non_linear_ls.opposite_of_gradient()
      self.non_linear_ls.solve()
      if self.had_too_small_a_step(): break
      self.n_iterations += 1
      h = self.non_linear_ls.step()
      expected_decrease = 0.5*h.dot(self.mu*h - g)
      self.non_linear_ls.step_forward()
      self.non_linear_ls.build_up(objective_only=True)
      objective_new = self.non_linear_ls.objective()
      actual_decrease = objective - objective_new
      rho = actual_decrease/expected_decrease
      if rho > 0:
        if self.has_gradient_converged_to_zero(): break
        self.mu *= max(1/3, 1 - (2*rho - 1)**3)
        nu = 2
      else:
        self.non_linear_ls.step_backward()
        self.mu *= nu
        nu *= 2
      self.non_linear_ls.build_up()

  def __str__(self):
    return "Levenberg-Marquardt"

class levenberg_marquardt_iterations_encapsulated_eqns(
      levenberg_marquardt_iterations):

  objective_decrease_threshold = None

  def __init__(self, non_linear_ls, **kwds):
    iterations.__init__(self, non_linear_ls, **kwds)
    """NKS 7/17/2015 Differs from Luc's original code two ways:
      1) unbreak the encapsulation of the normal matrix; enforce access
         to the normal matrix object through the non_linear_ls interface.
         Luc's original version assumes foreknowledge of the normal matrix
         data structure that will change in future sparse matrix implementation.
      2) avoid a memory leak by deleting the following circular reference to self:
    """
    if self.verbose_iterations:
      print("Iteration      Objective        Mu     ||Gradient||       Step")
    del self.non_linear_ls.journal

  def had_too_small_a_step(self):
    if self.verbose_iterations:
      print("%5d %18.4f"%(self.n_iterations,self.objective_history[-1]), end=' ')
      print("%12.7f"%(self.mu),"%12.3f"%(self.gradient_norm_history[-1]), end=' ')
      x = self.parameter_vector_norm_history[-1]
      h = self.step_norm_history[-1]
      import math
      root = (-x + math.sqrt(x*x+4.*h))/2.
      form_p = int(-math.log10(self.step_threshold))
      format = "%%12.%df"%(form_p+1)
      print(format%(root))
    return iterations.had_too_small_a_step(self)

  def do(self):
    self.mu_history = flex.double()
    self.n_iterations = 0
    nu = 2
    self.non_linear_ls.build_up()
    if self.has_gradient_converged_to_zero(): return
    a_diag = self.non_linear_ls.get_normal_matrix_diagonal()

    self.mu = self.tau*flex.max(a_diag)
    while self.n_iterations < self.n_max_iterations:
      self.non_linear_ls.add_constant_to_diagonal(self.mu)
      objective = self.non_linear_ls.objective()
      g = -self.non_linear_ls.opposite_of_gradient()
      self.non_linear_ls.solve()
      if self.had_too_small_a_step(): break
      self.n_iterations += 1
      h = self.non_linear_ls.step()
      expected_decrease = 0.5*h.dot(self.mu*h - g)
      self.non_linear_ls.step_forward()
      self.non_linear_ls.build_up(objective_only=True)
      objective_new = self.non_linear_ls.objective()
      actual_decrease = objective - objective_new
      rho = actual_decrease/expected_decrease
      if self.objective_decrease_threshold is not None:
        if actual_decrease/objective < self.objective_decrease_threshold: break
      if rho > 0:
        if self.has_gradient_converged_to_zero(): break
        self.mu *= max(1/3, 1 - (2*rho - 1)**3)
        nu = 2
      else:
        self.non_linear_ls.step_backward()
        self.mu *= nu
        nu *= 2
      self.non_linear_ls.build_up()

  def __str__(self):
    return "Levenberg-Marquardt, with Eigen-based sparse matrix algebra"


 *******************************************************************************


 *******************************************************************************
scitbx/lstbx/tests/__init__.py
"""
lstbx.tests
"""

from __future__ import division


 *******************************************************************************


 *******************************************************************************
scitbx/lstbx/tests/test_problems.py
""" A collection of L.S. test problems """
from __future__ import absolute_import, division, print_function

from scitbx.array_family import flex
from scitbx.lstbx import normal_eqns
import libtbx
from six.moves import range


def polynomial_fit(non_linear_ls_with_separable_scale_factor_impl):
  class klass(non_linear_ls_with_separable_scale_factor_impl,
              normal_eqns.non_linear_ls_mixin):
    """ Fit yc(t) = -t^3 + a t^2 + b t + c to synthetic data produced by
        yo(t) = 2 t^2 (1-t) + noise, with an overall scale factor on yc(t)
        whose optimal value is therefore 2.
    """

    noise = 1e-5
    n_data = 10
    x_0 = flex.double((0.5, 0.3, 0.2))

    def __init__(self, normalised, **kwds):
      super(klass, self).__init__(n_parameters=3,
                                  normalised=normalised)
      libtbx.adopt_optional_init_args(self, kwds)
      self.t = t = flex.double_range(self.n_data)/self.n_data
      noise = self.noise*flex.double([ (-1)**i for i in range(self.n_data) ])
      self.yo = 2.*t**2.*(1-t) + noise
      self.one = flex.double(self.n_data, 1)
      self.t2 = t**2
      self.t3 = t**3
      self.restart()

    def restart(self):
      self.x = self.x_0.deep_copy()
      self.old_x = None

    def step_forward(self):
      self.old_x = self.x.deep_copy()
      self.x += self.step()

    def step_backward(self):
      assert self.old_x is not None
      self.x, self.old_x = self.old_x, None

    def parameter_vector_norm(self):
      return self.x.norm()

    def build_up(self, objective_only=False):
      self.reset()
      a, b, c = self.x
      one, t, t2, t3 = self.one, self.t, self.t2, self.t3
      yc = -t3 + a*t2 + b*t + c
      grad_yc = (t2, t, one)
      jacobian_yc = flex.double(flex.grid(self.n_data, self.n_parameters))
      for j, der_r in enumerate(grad_yc):
        jacobian_yc.matrix_paste_column_in_place(der_r, j)
      self.add_equations(yc, jacobian_yc, self.yo, weights=None)
      self.finalise()
  return klass


def polynomial_fit_with_penalty(non_linear_ls_with_separable_scale_factor_impl):
  class klass(polynomial_fit(non_linear_ls_with_separable_scale_factor_impl)):

    def build_up(self, objective_only=False):
      super(klass, self).build_up(objective_only)
      a, b, c = self.x
      reduced = self.reduced_problem()
      reduced.add_equation(residual=(a - b + c - 2),
                           grad_residual=flex.double((1, -1, 1)),
                           weight=1)
  return klass


class exponential_fit(
  normal_eqns.non_linear_ls,
  normal_eqns.non_linear_ls_mixin):

  """ Model M(x, t) = x3 e^{x1 t} + x4 e^{x2 t}

      Problem 18 from

      UCTP Test Problems for Unconstrained Optimization
      Hans Bruun Nielsen
      TECHNICAL REPORT IMM-REP-2000-17
  """

  n_data = 45
  arg_min = flex.double((-4, -5, 4, -4))
  x_0     = flex.double((-1, -2, 1, -1))

  def __init__(self):
    super(exponential_fit, self).__init__(n_parameters=4)
    self.t = 0.02*flex.double_range(1, self.n_data + 1)
    self.y = flex.double((
      0.090542, 0.124569, 0.179367, 0.195654, 0.269707, 0.286027, 0.289892,
      0.317475, 0.308191, 0.336995, 0.348371, 0.321337, 0.299423, 0.338972,
      0.304763, 0.288903, 0.300820, 0.303974, 0.283987, 0.262078, 0.281593,
      0.267531, 0.218926, 0.225572, 0.200594, 0.197375, 0.182440, 0.183892,
      0.152285, 0.174028, 0.150874, 0.126220, 0.126266, 0.106384, 0.118923,
      0.091868, 0.128926, 0.119273, 0.115997, 0.105831, 0.075261, 0.068387,
      0.090823, 0.085205, 0.067203
      ))
    assert len(self.y) == len(self.t)
    self.restart()

  def restart(self):
    self.x = self.x_0.deep_copy()
    self.old_x = None

  def parameter_vector_norm(self):
    return self.x.norm()

  def build_up(self, objective_only=False):
    x1, x2, x3, x4 = self.x
    exp_x1_t = flex.exp(x1*self.t)
    exp_x2_t = flex.exp(x2*self.t)
    residuals = x3*exp_x1_t + x4*exp_x2_t
    residuals -= self.y

    self.reset()
    if objective_only:
      self.add_residuals(residuals, weights=None)
    else:
      grad_r = (self.t*x3*exp_x1_t,
                self.t*x4*exp_x2_t,
                exp_x1_t,
                exp_x2_t)
      jacobian = flex.double(flex.grid(self.n_data, self.n_parameters))
      for j, der_r in enumerate(grad_r):
        jacobian.matrix_paste_column_in_place(der_r, j)
      self.add_equations(residuals, jacobian, weights=None)

  def step_forward(self):
    self.old_x = self.x.deep_copy()
    self.x += self.step()

  def step_backward(self):
    assert self.old_x is not None
    self.x, self.old_x = self.old_x, None


 *******************************************************************************


 *******************************************************************************
scitbx/lstbx/tests/tst_normal_equations.py
from __future__ import absolute_import, division, print_function
import libtbx.load_env
from scitbx.array_family import flex
from scitbx import sparse
from scitbx.lstbx import normal_eqns, normal_eqns_solving
from libtbx.test_utils import approx_equal, Exception_expected

from scitbx.lstbx.tests import test_problems
from six.moves import range

def exercise_linear_normal_equations():
  py_eqs = [ ( 1, (-1,  0,  0),  1),
             ( 2, ( 2, -1,  0),  3),
             (-1, ( 0,  2,  1),  2),
             (-2, ( 0,  1,  0), -2),
             ]

  eqs_0 = normal_eqns.linear_ls(3)
  for b, a, w in py_eqs:
    eqs_0.add_equation(right_hand_side=b,
                       design_matrix_row=flex.double(a),
                       weight=w)

  eqs_1 = normal_eqns.linear_ls(3)
  b = flex.double()
  w = flex.double()
  a = sparse.matrix(len(py_eqs), 3)
  for i, (b_, a_, w_) in enumerate(py_eqs):
    b.append(b_)
    w.append(w_)
    for j in range(3):
      if a_[j]: a[i, j] = a_[j]
  eqs_1.add_equations(right_hand_side=b, design_matrix=a, weights=w)

  assert approx_equal(
    eqs_0.normal_matrix_packed_u(), eqs_1.normal_matrix_packed_u(), eps=1e-15)
  assert approx_equal(
    eqs_0.right_hand_side(), eqs_1.right_hand_side(), eps=1e-15)
  assert approx_equal(
    list(eqs_0.normal_matrix_packed_u()), [ 13, -6, 0, 9, 4, 2 ], eps=1e-15)
  assert approx_equal(
    list(eqs_0.right_hand_side()), [ 11, -6, -2 ], eps=1e-15)


non_linear_ls_with_separable_scale_factor__impls = (
  normal_eqns.non_linear_ls_with_separable_scale_factor__level_2_blas_impl,
)

try:
  from fast_linalg import env
  if env.initialised:
    non_linear_ls_with_separable_scale_factor__impls += (
      normal_eqns.non_linear_ls_with_separable_scale_factor__level_3_blas_impl,
    )
except ImportError:
  print('Skipping fast_linalg checks')

def exercise_non_linear_ls_with_separable_scale_factor():
  for impl in non_linear_ls_with_separable_scale_factor__impls:
    test = test_problems.polynomial_fit(impl)(normalised=False)
    test.build_up()

    assert test.n_equations == test.n_data;

    # Reference values computed in tst_normal_equations.nb
    eps = 5e-14
    assert approx_equal(test.optimal_scale_factor(), 0.6148971786833856, eps)
    assert approx_equal(test.objective(), 0.039642707534326034, eps)
    assert approx_equal(test.chi_sq(), 0.011326487866950296, eps)


    assert not test.step_equations().solved
    try:
      test.step_equations().cholesky_factor_packed_u()
      raise Exception_expected
    except RuntimeError:
      pass
    try:
      test.step_equations().solution()
      raise Exception_expected
    except RuntimeError:
      pass

    assert approx_equal(
      list(test.step_equations().normal_matrix_packed_u()),
      [ 0.371944193675858, 0.39066546997866547  , 0.10797294655500618,
                             0.41859250354804045, 0.08077629438075473,
                                                  0.19767268057900367 ],
      eps)
    assert approx_equal(
      list(test.step_equations().right_hand_side()),
      [ 0.12149917297914861, 0.13803759252793774, -0.025190641142579157 ],
      eps)

    test.step_equations().solve()

    assert test.step_equations().solved
    try:
      test.step_equations().normal_matrix_packed_u()
      raise Exception_expected
    except RuntimeError:
      pass
    try:
      test.step_equations().right_hand_side()
      raise Exception_expected
    except RuntimeError:
      pass

    assert approx_equal(
      list(test.step_equations().cholesky_factor_packed_u()),
      [ 0.6098722765266986, 0.6405693208478925   ,  0.1770418999366983 ,
                              0.09090351333425013, -0.3589664912436558 ,
                                                    0.19357661121640218 ],
      eps)
    assert approx_equal(
      list(test.step_equations().solution()),
      [ 1.2878697604109028, -0.7727798877778043, -0.5151113342942297 ],
      eps=1e-12)

    test_bis = test_problems.polynomial_fit(impl)(normalised=True)
    test_bis.build_up()
    assert approx_equal(test_bis.objective(),
                        test.objective()/test.sum_w_yo_sq(),
                        eps=1e-15)
    assert approx_equal(test_bis.chi_sq(), test.chi_sq(), eps=1e-15)


def exercise_non_linear_ls_with_separable_scale_factor_plus_penalty():
  for impl in non_linear_ls_with_separable_scale_factor__impls:
    test = test_problems.polynomial_fit_with_penalty(impl)(normalised=False)
    test.build_up()
    assert test.n_equations == test.n_data + 1

    eps = 5e-14
    # reference values from tst_normal_equations.nb again

    assert approx_equal(test.optimal_scale_factor(), 0.6148971786833856, eps)
    redu = test.reduced_problem()
    assert test.objective() == redu.objective()
    assert test.step_equations().right_hand_side()\
           .all_eq(redu.step_equations().right_hand_side())
    assert test.step_equations().normal_matrix_packed_u()\
           .all_eq(redu.step_equations().normal_matrix_packed_u())

    assert approx_equal(test.objective(), 1.3196427075343262, eps)
    assert approx_equal(test.chi_sq(), 0.32991067688358156, eps)
    assert approx_equal(
      test.step_equations().right_hand_side(),
      (1.7214991729791487, -1.4619624074720623, 1.5748093588574208),
      eps)
    assert approx_equal(
      test.step_equations().normal_matrix_packed_u(),
      (1.371944193675858, -0.6093345300213344,  1.107972946555006,
                           1.4185925035480405, -0.9192237056192452,
                                                1.1976726805790037),
      eps)

    test_bis = test_problems.polynomial_fit_with_penalty(impl)(normalised=True)
    test_bis.build_up()
    assert approx_equal(test_bis.chi_sq(), test.chi_sq(), eps=1e-15)

    n_equations = test.n_equations
    test.build_up()
    assert test.n_equations == n_equations

def exercise_levenberg_marquardt(non_linear_ls, plot=False):
  non_linear_ls.restart()
  iterations = normal_eqns_solving.levenberg_marquardt_iterations(
    non_linear_ls,
    track_all=True,
    gradient_threshold=1e-8,
    step_threshold=1e-8,
    tau=1e-4,
    n_max_iterations=200)
  assert non_linear_ls.n_equations == non_linear_ls.n_data
  assert approx_equal(non_linear_ls.x, non_linear_ls.arg_min, eps=5e-4)
  print("L-M: %i iterations" % iterations.n_iterations)
  if plot:
    f = open('plot.nb', 'w')
    print("g=%s;" % iterations.gradient_norm_history.mathematica_form(), file=f)
    print("\\[Mu]=%s;" % iterations.mu_history.mathematica_form(), file=f)
    print("ListLogPlot[{g,\\[Mu]},Joined->True]", file=f)
    f.close()

def run():
  import sys
  plot = '--plot' in sys.argv[1:]
  t = test_problems.exponential_fit()
  exercise_levenberg_marquardt(t, plot)
  exercise_linear_normal_equations()
  exercise_non_linear_ls_with_separable_scale_factor()
  exercise_non_linear_ls_with_separable_scale_factor_plus_penalty()
  print('OK')

if __name__ == '__main__':
  run()


 *******************************************************************************
