

 *******************************************************************************
mmtbx/pdb_symmetry.py

"""
Create a simple table of crystal symmetry in structures deposited in the PDB.
This can be used for comparison with "novel" datasets, possibly indicating
whether a crystallized is actually a previously characterized contaminant.
"""

from __future__ import absolute_import, division, print_function
import libtbx.load_env
from libtbx import easy_pickle
from libtbx import group_args
from math import sqrt
import operator
import os
import sys
from six.moves import zip

def parse_database(file_name):
  from cctbx import crystal
  from cctbx import sgtbx
  from cctbx import uctbx
  db = []
  lines = open(file_name).readlines()
  process = False
  for line in lines :
    line = line.strip()
    if (process):
      fields = line.split()
      pdb_id = fields[0]
      try :
        uc = uctbx.unit_cell([ float(x.replace(",","")) for x in fields[2:8] ])
      except RuntimeError as e :
        print("Unit cell error:")
        print(line)
        continue
      try :
        sg = sgtbx.space_group_info(" ".join(fields[8:-1]))
      except RuntimeError as e :
        print("Unrecognized space group:")
        print(line)
        continue
      try :
        symm = crystal.symmetry(unit_cell=uc, space_group_info=sg)
      except AssertionError :
        print("Incompatible unit cell parameters:")
        print(line)
        continue
      niggli_symm = symm.niggli_cell()
      db.append(group_args(
        pdb_id=pdb_id,
        crystal_symmetry=symm,
        niggli_cell=niggli_symm))
    elif (line.startswith("-----")):
      process = True
  return db

db_ = None
def load_db(save_pickle=False):
  global db_
  if (db_ is not None):
    return db_
  pkl_file_name = libtbx.env.find_in_repositories(
    relative_path="chem_data/pdb/crystal.pkl",
    test=os.path.isfile)
  txt_file_name = libtbx.env.find_in_repositories(
    relative_path="chem_data/pdb/crystal.idx",
    test=os.path.isfile)
  if (pkl_file_name is None) or (save_pickle):
    assert (txt_file_name is not None)
    db_ = parse_database(txt_file_name)
    if (save_pickle):
      easy_pickle.dump(txt_file_name[:-4] + ".pkl", db_)
  else :
    db_ = easy_pickle.load(pkl_file_name)
  return db_

def rms_difference(params1, params2):
  assert (len(params1) == len(params2) != 0)
  sum = 0
  n = 0
  for x1, x2 in zip(params1, params2):
    sum += (x1-x2)**2
  return sqrt(sum / len(params1))

def symmetry_search(
    symmetry_db,
    crystal_symmetry,
    max_rmsd=None):
  scores = []
  niggli_cell = crystal_symmetry.niggli_cell()
  input_volume = niggli_cell.unit_cell().volume()
  niggli_uc = niggli_cell.unit_cell().parameters()
  for entry in symmetry_db :
    entry_volume = entry.niggli_cell.unit_cell().volume()
    entry_uc = entry.niggli_cell.unit_cell().parameters()
    # XXX suggested by James Holton - sum separate RMSDs for edge lengths and
    # angles
    rmsd = rms_difference(niggli_uc[0:3], entry_uc[0:3]) + \
           rms_difference(niggli_uc[3:6], entry_uc[3:6])
    if (max_rmsd is not None) and (rmsd > max_rmsd):
      continue
    scores.append(group_args(entry=entry,
      rmsd=rmsd,
      volume_ratio=entry_volume/input_volume))
  scores.sort(key=operator.attrgetter("rmsd"))
  return scores

def download_crystal_db():
  from six.moves import urllib
  host = "files.wwpdb.org"
  file = "pub/pdb/derived_data/index/crystal.idx"
  url = "https://%s/%s" % (host, file)
  print("Retrieving %s" % url)
  data = urllib.request.urlopen(url)
  dest_dir = libtbx.env.find_in_repositories(
    relative_path="chem_data/pdb",
    test=os.path.isdir)
  dest_file = os.path.join(dest_dir, "crystal.idx")
  f = open(dest_file, "w")
  f.write(data.read())
  f.close()
  print("Wrote %s" % dest_file)

if (__name__ == "__main__"):
  if ("--update" in sys.argv):
    download_crystal_db()
  load_db(save_pickle=True)


 *******************************************************************************


 *******************************************************************************
mmtbx/pdbtools.py
from __future__ import absolute_import, division, print_function
import iotbx.phil
from cctbx import crystal
from iotbx.pdb import resseq_encode, resseq_decode
import iotbx.pdb
from cctbx.array_family import flex
from libtbx.utils import Sorry
import random
import scitbx.matrix
import sys
import scitbx.rigid_body
from libtbx import group_args
import mmtbx.utils


master_params_str = """\
modify
  .short_caption = Modify starting model
  .style = menu_item scrolled auto_align
{
remove = None
  .type = atom_selection
  .help = Selection for the atoms to be removed
  .short_caption=Remove atom selection
  .input_size=400
  .style = bold noauto
keep = None
  .type = atom_selection
  .help = Select atoms to keep
  .short_caption=Keep only atom selection
  .input_size=400
  .style = bold noauto
put_into_box_with_buffer = None
  .type = float
  .help = Move molecule into center of box
selection = None
  .type = atom_selection
  .help = Selection for atoms to be modified
  .short_caption = Modify atom selection
  .input_size=400
  .style = bold noauto
flip_symmetric_amino_acids = False
  .type = bool
  .short_caption = Flip symmetric amino acid side chains
  .help = Flip symmetric amino acid side chains
adp
  .help = Scope of options to modify ADP of selected atoms
  .multiple = True
  .short_caption=Modify ADPs
  .style = auto_align menu_item parent_submenu:model_modifications noauto
{
  atom_selection = None
    .type = atom_selection
    .help = Selection for atoms to be modified. \\
            Overrides parent-level selection.
    .short_caption = Modify ADPs for selection
    .input_size=400
    .style  = bold
  randomize = False
    .type = bool
    .help = Randomize ADP within a certain range
    .short_caption=Randomize ADPs
  set_b_iso = None
    .type = float
    .help = Set ADP of atoms to set_b_iso
    .short_caption=Set isotropic B to
    .input_size = 64
  convert_to_isotropic = False
    .type = bool
    .help = Convert atoms to isotropic
  convert_to_anisotropic = False
    .type = bool
    .help = Convert atoms to anisotropic
  shift_b_iso = None
    .type = float
    .help = Add shift_b_iso value to ADP
    .short_caption=Increase B_iso by
  scale_adp = None
    .type = float
    .help = Multiply ADP by scale_adp
    .short_caption=ADP scale factor
}
sites
  .help = Scope of options to modify coordinates of selected atoms
  .multiple = True
  .short_caption=Modify coordinates
  .style = auto_align noauto menu_item parent_submenu:model_modifications
{
  atom_selection = None
    .type = atom_selection
    .help = Selection for atoms to be modified. \\
            Overrides parent-level selection.
    .input_size=400
    .short_caption = Modify sites for selection
    .style = bold
  shake = None
    .type = float
    .help = Randomize coordinates with mean error value equal to shake
    .short_caption = Randomize coordinates (mean value)
  switch_rotamers = max_distant min_distant exact_match fix_outliers
    .type=choice(multi=False)
  translate = 0 0 0
    .type = floats(size=3)
    .optional = False
    .help = Translational shift (x,y,z)
  rotate = 0 0 0
    .type = floats(size=3)
    .optional = False
    .help = Rotational shift (x,y,z)
  euler_angle_convention = *xyz zyz
    .type = choice
    .help = Euler angles convention to be used for rotation
}
occupancies
  .help = Scope of options to modify occupancies of selected atoms
  .multiple = True
  .short_caption=Modify occupancies
  .style  = noauto menu_item parent_submenu:model_modifications
{
  atom_selection = None
    .type = atom_selection
    .help = Selection for atoms to be modified. \\
            Overrides parent-level selection.
    .input_size=400
    .short_caption = Modify sites for selection
    .style = bold
  randomize = False
    .type = bool
    .help = Randomize occupancies within a certain range
    .short_caption = Randomize occupancies
  set = None
    .type = float
    .help = Set all or selected occupancies to given value
    .short_caption=Set occupancies to
    .input_size = 64
}
rotate_about_axis
  .style = box auto_align
{
  axis = None
    .type = str
  angle = None
    .type = float
  atom_selection = None
    .type = str
}
change_of_basis = None
  .type = str
  .short_caption = Change of basis operator
  .help = Apply change-of-basis operator (e.g. reindexing operator) to \
    the coordinates and symmetry.  Example: 'a,c,b'.
renumber_residues = False
  .type = bool
  .help = Re-number residues
increment_resseq = None
  .type = int
  .help = Increment residue number
  .short_caption = Increment residue numbers by
truncate_to_polyala = False
  .type = bool
  .help = Truncate a model to poly-Ala.
  .short_caption = Truncate to poly-Ala
  .style = noauto
truncate_to_polygly  = False
  .type = bool
  .help = Truncate a model to poly-Gly.
  .short_caption = Truncate to poly-Gly
  .style = noauto
remove_alt_confs = False
  .type = bool
  .help = Deletes atoms whose altloc identifier is not blank or 'A', and \
    resets the occupancies of the remaining atoms to 1.0.
  .short_caption = Remove alternate conformers
  .style = noauto
always_keep_one_conformer = False
  .type = bool
  .help = Modifies behavior of remove_alt_confs so that residues with no \
    conformer labeled blank or A are not deleted.  Silent if remove_alt_confs \
    is False.
keep_occupancy = False
  .type = bool
  .help = Do not reset occupancy to 1 after removing altlocs
altloc_to_keep = None
  .type = str
  .help = Modifies behavior of remove_alt_confs so that the altloc identifier \
            to keep in addition to blank is this one (instead of 'A'). \
            Silent if remove_alt_confs is False.
  .short_caption = Conformer to keep
average_alt_confs = False
  .type = bool
  .help = Averages the coordinates of altloc atoms with 1.0 angstrom.
  .short_caption = Average alternate conformers
  .style = noauto
set_chemical_element_simple_if_necessary = None
  .type = bool
  .short_caption = Guess element field if necessary
  .help = Make a simple guess about what the chemical element is (based on \
          atom name and the way how it is formatted) and write it into output file.
set_seg_id_to_chain_id = False
  .type = bool
  .short_caption = Set segID to chain ID
  .help = Sets the segID field to the chain ID (padded with spaces).
  .style = noauto
clear_seg_id = False
  .type = bool
  .short_caption = Clear segID field
  .help = Erases the segID field.
  .style = noauto
convert_semet_to_met = False
  .type = bool
  .short_caption = Convert SeMet residues to Met
  .style = noauto
convert_met_to_semet = False
  .type = bool
  .short_caption = Convert Met residues to SeMet
  .style = noauto
rename_chain_id
  .help = Rename chains
  .short_caption = Rename chain ID
  .style = box
{
  old_id = None
    .type = str
    .input_size = 50
    .short_caption = Old ID
  new_id = None
    .type = str
    .input_size = 50
    .short_caption = New ID
}
set_charge
  .short_caption = Set atomic charge
  .style = box auto_align
{
  charge_selection = None
    .type = atom_selection
    .short_caption = Atom selection
  charge = None
    .type = int(value_max=7,value_min=-3)
}
neutralize_scatterers = False
  .type = bool
  .short_caption = Neutralize all atoms in the model
remove_fraction = None
  .short_caption = Remove atoms randomly (fraction)
  .type = float
random_seed = None
  .type = int
  .help = Random seed
move_waters_last = False
  .type = bool
  .short_caption = Move waters to end of model
  .help = Transfer waters to the end of the model.  Addresses some \
    limitations of water picking in phenix.refine.
renumber_and_move_waters = False
  .type = bool
  .short_caption = Renumber and move waters to end of chains
  .help = Transfer waters to the end of respective chains. Renumber them \
    so that numbering starts at the nearest 100 after the last non-water \
    in the chain.
}
"""

def master_params():
  return iotbx.phil.parse(master_params_str, process_includes=False)

class modify(object):
  def __init__(self, model, params, log = None):
    self.log = log
    self.params = params
    self.model = model
    self._neutralize_scatterers()
    if not model.crystal_symmetry() or not model.crystal_symmetry().unit_cell():
      # Make it up
      from cctbx.maptbx.box import shift_and_box_model
      model = shift_and_box_model(model, shift_model=False)

    self.pdb_hierarchy = model.get_hierarchy()
    self.crystal_symmetry = model.crystal_symmetry()
    if(self.log is None): self.log = sys.stdout
    self.xray_structure = model.get_xray_structure()
    asc = self.pdb_hierarchy.atom_selection_cache(
        special_position_settings=crystal.special_position_settings(
            crystal_symmetry = self.crystal_symmetry))
    if(self.params.random_seed is not None):
      random.seed(self.params.random_seed)
      flex.set_random_seed(self.params.random_seed)
    self.top_selection = flex.smart_selection(
        flags=flex.bool(self.xray_structure.scatterers().size(), True))
    if(self.params.selection is not None):
      self.top_selection = flex.smart_selection(
        flags=asc.selection(self.params.selection))
    self._rotate_about_axis()
    self._process_adp()
    self._process_sites()
    self._process_occupancies()
    self._put_in_box()
    self._change_of_basis()
    # Up to this point we are done with self.xray_structure
    self.model.set_xray_structure(self.xray_structure)
    self.pdb_hierarchy = self.model.get_hierarchy()
    # Now only manipulations that use self.pdb_hierarchy are done
### segID manipulations
    if (params.set_seg_id_to_chain_id):
      if (params.clear_seg_id):
        raise Sorry("Parameter conflict - set_seg_id_to_chain_id=True and "+
          "clear_seg_id=True.  Please choose only one of these options.")
      for atom in self.pdb_hierarchy.atoms():
        labels = atom.fetch_labels()
        atom.segid = "%-4s" % labels.chain_id
    elif (params.clear_seg_id):
      for atom in self.pdb_hierarchy.atoms():
        atom.segid = "    "
    if(self.params.set_chemical_element_simple_if_necessary or
       self.params.rename_chain_id.old_id or
       self.params.renumber_residues or self.params.increment_resseq or
       self.params.convert_semet_to_met or
       self.params.convert_met_to_semet or
       self.params.set_charge.charge or
       self.params.truncate_to_polyala or
       self.params.truncate_to_polygly or
       self.params.remove_alt_confs or
       self.params.average_alt_confs or
       self.params.move_waters_last or
       self.params.renumber_and_move_waters or
       self.params.remove_fraction or
       self.params.keep or
       self.params.remove):
      # del self.xray_structure # it is invalide below this point
      self._set_chemical_element_simple_if_necessary()
      self._rename_chain_id()
      self._renumber_residues()
      self._convert_semet_to_met()
      self._convert_met_to_semet()
      self._set_atomic_charge()
      self._truncate_to_poly_ala()
      self._truncate_to_poly_gly()
      self._remove_alt_confs()
      self._average_alt_confs()
      self._move_waters()
      self._renumber_and_move_waters()
      self._remove_atoms()
      self._apply_keep_remove()
      # Here goes really nasty hack. Never repeat it.
      # It is here because I don't have clear idea about how to handle
      # such dramatic changes in number of atoms etc that just was performed
      # for hierarchy.
      self.pdb_hierarchy.reset_atom_i_seqs()
      self.pdb_hierarchy.atoms_reset_serial()
      self.model._pdb_hierarchy = self.pdb_hierarchy
      self.model._xray_structure = self.pdb_hierarchy.extract_xray_structure(
          crystal_symmetry=self.model.crystal_symmetry())
      self.model._update_atom_selection_cache()
      self.model._update_has_hd()
      self.model.get_hierarchy().atoms().reset_i_seq()



  def _apply_keep_remove(self):
    cn = [self.params.remove, self.params.keep].count(None)
    if(not cn in [1,2]):
      raise Sorry("'keep' and 'remove' keywords cannot be used simultaneously.")
    s1 = self.pdb_hierarchy.atoms_size()
    if(self.params.remove is not None):
      asc = self.pdb_hierarchy.atom_selection_cache(
        special_position_settings=crystal.special_position_settings(
            crystal_symmetry = self.crystal_symmetry))
      sel = ~asc.selection(self.params.remove)
      self.pdb_hierarchy = self.pdb_hierarchy.select(sel)
      s2 = self.pdb_hierarchy.atoms_size()
      print("Size before:", s1, "size after:", s2, file=self.log)
    if(self.params.keep is not None):
      asc = self.pdb_hierarchy.atom_selection_cache(
        special_position_settings=crystal.special_position_settings(
            crystal_symmetry = self.crystal_symmetry))
      sel = asc.selection(self.params.keep)
      self.pdb_hierarchy = self.pdb_hierarchy.select(sel)
      s2 = self.pdb_hierarchy.atoms_size()
      print("Size before:", s1, "size after:", s2, file=self.log)

  def _change_of_basis(self):
    if(self.params.change_of_basis is not None):
      print("Applying change-of-basis operator '%s'" % \
        self.params.change_of_basis, file=self.log)
      from cctbx import sgtbx
      cb_op = sgtbx.change_of_basis_op(self.params.change_of_basis)
      self.xray_structure = self.xray_structure.change_basis(cb_op)
      self.pdb_hierarchy.atoms().set_xyz(self.xray_structure.sites_cart())
      print("New symmetry:", file=self.log)
      self.xray_structure.crystal_symmetry().show_summary(f=self.log, prefix="  ")
      self.crystal_symmetry = self.xray_structure.crystal_symmetry()

  def _renumber_and_move_waters(self):
    def next_hundred(num):
      if num % 100 == 0:
        return num + 100
      return ((num // 100) + 1) * 100
    if not self.params.renumber_and_move_waters:
      return
    if (len(self.pdb_hierarchy.models()) > 1):
      raise Sorry("Renumbering and reorganizing water molecules is "+
        "not supported for multi-MODEL structures.")
    print("Renumbering and reorganizing waters", file=self.log)
    new_h = iotbx.pdb.hierarchy.root()
    mm = iotbx.pdb.hierarchy.model()
    new_h.append_model(mm)
    # make chain dict 'chain id': [chains]:
    chain_dict = {}
    for c in self.pdb_hierarchy.only_model().chains():
      if c.id not in chain_dict:
        chain_dict[c.id] = [c]
      else:
        chain_dict[c.id].append(c)
    for c_id, chain_list in chain_dict.items():
      largest_nonwater_resseq = max(
          [resseq_decode(list(c.residue_groups())[-1].resseq) for c in chain_list if not c.is_water()],
          default=1)
      # now merge all water chains to a new one, while adding non-water to new h
      cc = iotbx.pdb.hierarchy.chain()
      cc.id = c_id
      for c in chain_list:
        if not c.is_water():
          # adding to new h
          mm.append_chain(c.detached_copy())
        else:
          for rg in c.residue_groups():
            cc.append_residue_group(rg.detached_copy())
      # renumber waters in cc:
      curent_resseq = next_hundred(largest_nonwater_resseq)
      for rg in cc.residue_groups():
        rg.resseq = resseq_encode(curent_resseq)
        curent_resseq += 1
      if cc.residue_groups_size() > 0:
        mm.append_chain(cc)
    new_h.reset_atom_i_seqs()
    new_h.atoms_reset_serial()
    self.pdb_hierarchy = new_h

  def _move_waters(self):
    if(self.params.move_waters_last):
      print("Moving waters to end of model", file=self.log)
      if (len(self.pdb_hierarchy.models()) > 1):
        raise Sorry("Rearranging water molecules is not supported for "+
          "multi-MODEL structures.")
      sel_cache = self.pdb_hierarchy.atom_selection_cache(
        special_position_settings=crystal.special_position_settings(
            crystal_symmetry = self.crystal_symmetry))
      water_sel = sel_cache.selection("resname HOH or resname WAT") # BAD XXX
      n_waters = water_sel.count(True)
      if (n_waters == 0):
        print("No waters found, skipping", file=self.log)
      else :
        print("%d atoms will be moved." % n_waters, file=self.log)
        hierarchy_water = self.pdb_hierarchy.select(water_sel)
        hierarchy_non_water = self.pdb_hierarchy.select(~water_sel)
        for chain in hierarchy_water.only_model().chains():
          hierarchy_non_water.only_model().append_chain(chain.detached_copy())
        self.pdb_hierarchy = hierarchy_non_water # does this work?

  def _remove_alt_confs(self):
    if(self.params.remove_alt_confs):
      print("Remove altlocs", file=self.log)
      self.pdb_hierarchy.remove_alt_confs(
        always_keep_one_conformer = self.params.always_keep_one_conformer,
        altloc_to_keep = self.params.altloc_to_keep,
        keep_occupancy = self.params.keep_occupancy)

  def _average_alt_confs(self):
    if(self.params.average_alt_confs):
      print("Average altlocs", file=self.log)
      self.pdb_hierarchy.average_alt_confs()

  def _truncate_to_poly_gly(self):
    if(self.params.truncate_to_polygly):
      print("Truncate to poly-gly", file=self.log)
      self.pdb_hierarchy.truncate_to_poly_gly()

  def _truncate_to_poly_ala(self):
    if(self.params.truncate_to_polyala):
      print("Truncate to poly-ala", file=self.log)
      self.pdb_hierarchy.truncate_to_poly_ala()

  def _set_atomic_charge(self):
    if(self.params.set_charge.charge_selection is not None):
      print("Setting atomic charge", file=self.log)
      selection = self.params.set_charge.charge_selection
      charge    = self.params.set_charge.charge
      sel_cache = self.pdb_hierarchy.atom_selection_cache(
        special_position_settings=crystal.special_position_settings(
            crystal_symmetry = self.crystal_symmetry))
      isel = sel_cache.selection(selection).iselection()
      self.pdb_hierarchy.set_atomic_charge(iselection=isel, charge=charge)

  def _convert_met_to_semet(self):
    if(self.params.convert_met_to_semet):
      print("Convert MET->MSE", file=self.log)
      self.pdb_hierarchy.convert_met_to_semet()

  def _convert_semet_to_met(self):
    if(self.params.convert_semet_to_met):
      print("Convert MSE->MET", file=self.log)
      self.pdb_hierarchy.convert_semet_to_met()

  def _renumber_residues(self):
    if((self.params.increment_resseq) or
       (self.params.renumber_residues)):
      print("Re-numbering residues", file=self.log)
      renumber_from  = self.params.increment_resseq
      atom_selection = self.params.selection
      pdb_hierarchy  = self.pdb_hierarchy
      selected_i_seqs = None
      if (atom_selection is not None):
        sel_cache = pdb_hierarchy.atom_selection_cache(
        special_position_settings=crystal.special_position_settings(
            crystal_symmetry = self.crystal_symmetry))
        selected_i_seqs = sel_cache.selection(atom_selection).iselection()
      for model in pdb_hierarchy.models():
        for chain in model.chains():
          if (selected_i_seqs is not None):
            chain_i_seqs = chain.atoms().extract_i_seq()
            intersection = selected_i_seqs.intersection(chain_i_seqs)
            if (len(intersection) == 0):
              continue
            elif (len(intersection) != len(chain_i_seqs)):
              print("Warning: chain '%s' is only partially selected (%d out of %d) - will not renumber." % (chain.id, len(intersection), len(chain_i_seqs)), file=self.log)
              continue
          if (renumber_from is None):
            counter = 1
            for rg in chain.residue_groups():
              rg.resseq=resseq_encode(counter)
              counter += 1
          else :
            for rg in chain.residue_groups():
              resseq = rg.resseq_as_int()
              resseq += renumber_from
              rg.resseq = resseq_encode(resseq)

  def _rename_chain_id(self):
    if([self.params.rename_chain_id.old_id,
       self.params.rename_chain_id.new_id].count(None)==0):
      print("Rename chain id", file=self.log)
      print("old_id= '%s'"%self.params.rename_chain_id.old_id, file=self.log)
      print("new_id= '%s'"%self.params.rename_chain_id.new_id, file=self.log)
      self.pdb_hierarchy.rename_chain_id(
        old_id = self.params.rename_chain_id.old_id,
        new_id = self.params.rename_chain_id.new_id)

  def _set_chemical_element_simple_if_necessary(self):
    if(self.params.set_chemical_element_simple_if_necessary):
      print("Set chemical element", file=self.log)
      self.pdb_hierarchy.atoms().set_chemical_element_simple_if_necessary()

  def _remove_atoms(self):
    if(self.params.remove_fraction is not None):
      self.pdb_hierarchy = \
        self.pdb_hierarchy.remove_atoms(fraction=self.params.remove_fraction)

  def _put_in_box(self):
    if(self.params.put_into_box_with_buffer is not None):
      result = \
        self.xray_structure.orthorhombic_unit_cell_around_centered_scatterers(
          buffer_size = self.params.put_into_box_with_buffer)
      self.xray_structure.replace_scatterers(result.scatterers())

  def _print_action(self, text, selection):
    print("%s: selected atoms: %s" % (
      text, selection.format_summary()), file=self.log)

  def _process_adp(self):
    for adp in self.params.adp:
      if (adp.atom_selection is None):
        selection = self.top_selection
      else:
        asc = self.pdb_hierarchy.atom_selection_cache(
        special_position_settings=crystal.special_position_settings(
            crystal_symmetry = self.crystal_symmetry))
        sel = asc.selection(adp.atom_selection)
        selection = flex.smart_selection(flags=sel)
      if (adp.convert_to_isotropic):
        self._convert_to_isotropic(selection=selection)
      if (adp.convert_to_anisotropic):
        self._convert_to_anisotropic(selection=selection)
      self._set_b_iso(selection=selection, b_iso=adp.set_b_iso)
      self._scale_adp(selection=selection, factor=adp.scale_adp)
      self._shift_b_iso(selection=selection, shift=adp.shift_b_iso)
      if (adp.randomize):
        self._randomize_adp(selection=selection)

  def _convert_to_isotropic(self, selection):
    self._print_action(
      text = "Converting to isotropic ADP",
      selection = selection)
    self.xray_structure.convert_to_isotropic(selection=selection.indices)

  def _convert_to_anisotropic(self, selection):
    self._print_action(
      text = "Converting to anisotropic ADP",
      selection = selection)
    self.xray_structure.convert_to_anisotropic(selection=selection.flags)

  def _set_b_iso(self, selection, b_iso):
    if (b_iso is not None):
      self._print_action(
        text = "Setting all isotropic ADP = %.3f" % b_iso,
        selection = selection)
      self.xray_structure.set_b_iso(value=b_iso, selection=selection.flags)

  def _scale_adp(self, selection, factor):
    if (factor is not None):
      self._print_action(
        text = "Multiplying all ADP with factor = %.6g" % factor,
        selection = selection)
      self.xray_structure.scale_adp(factor=factor, selection=selection.flags)

  def _shift_b_iso(self, selection, shift):
    if (shift is not None):
      self._print_action(
        text = "Adding shift = %.2f to all ADP" % shift,
        selection = selection)
      self.xray_structure.shift_us(b_shift=shift, selection=selection.indices)

  def _randomize_adp(self, selection):
    self._print_action(
      text = "Randomizing ADP",
      selection = selection)
    self.xray_structure.shake_adp(selection=selection.flags)

  def _process_sites(self):
    if(self.params.flip_symmetric_amino_acids):
      self.pdb_hierarchy.flip_symmetric_amino_acids()
      self.xray_structure.set_sites_cart(
        sites_cart = self.pdb_hierarchy.atoms().extract_xyz())
    for sites in self.params.sites:
      if (sites.atom_selection is None):
        selection = self.top_selection
      else:
        asc = self.pdb_hierarchy.atom_selection_cache(
        special_position_settings=crystal.special_position_settings(
            crystal_symmetry = self.crystal_symmetry))
        sel = asc.selection(sites.atom_selection)
        selection = flex.smart_selection(flags=sel)
      self._shake_sites(selection=selection, rms_difference=sites.shake)
      self._switch_rotamers(selection=selection, mode=sites.switch_rotamers)
      self._rb_shift(
        selection=selection,
        translate=sites.translate,
        rotate=sites.rotate,
        euler_angle_convention=sites.euler_angle_convention)

  def _switch_rotamers(self, selection, mode):
    if(mode is None): return
    self._print_action(
      text = "Switching rotamers; mode = %s"%mode,
      selection = selection)
    self.pdb_hierarchy.atoms().set_xyz(self.xray_structure.sites_cart())
    self.pdb_hierarchy = mmtbx.utils.switch_rotamers(
      pdb_hierarchy=self.pdb_hierarchy,
      mode=mode,
      selection=selection.flags)
    self.xray_structure.set_sites_cart(
      sites_cart = self.pdb_hierarchy.atoms().extract_xyz())

  def _shake_sites(self, selection, rms_difference):
    if (rms_difference is not None):
      self._print_action(
        text = "Shaking sites (RMS = %.3f)" % rms_difference,
        selection = selection)
      self.xray_structure.shake_sites_in_place(
        rms_difference=rms_difference,
        selection=selection.flags)

  def _rb_shift(self, selection, translate, rotate, euler_angle_convention):
    trans = [float(i) for i in translate]
    rot   = [float(i) for i in rotate]
    if(len(trans) != 3): raise Sorry("Wrong value: translate= " + translate)
    if(len(rot) != 3): raise Sorry("Wrong value: translate= " + rotate)
    if (   trans[0] != 0 or trans[1] != 0 or trans[2] != 0
        or rot[0] != 0 or rot[1] != 0 or rot[2] != 0):
      self._print_action(
        text = "Rigid body shift",
        selection = selection)
      if (euler_angle_convention == "zyz"):
        rot_obj = scitbx.rigid_body.rb_mat_zyz(
          phi = rot[0],
          psi = rot[1],
          the = rot[2])
      else:
        rot_obj = scitbx.rigid_body.rb_mat_xyz(
          phi = rot[0],
          psi = rot[1],
          the = rot[2])
      self.xray_structure.apply_rigid_body_shift(
        rot       = rot_obj.rot_mat().as_mat3(),
        trans     = trans,
        selection = selection.indices)

  def _process_occupancies(self):
    def check_if_already_modified():
      if(self.top_selection): return
      if (self._occupancies_modified):
        raise Sorry("Can't modify occupancies (already modified).")
      else:
        self._occupancies_modified = True
    for occ in self.params.occupancies:
      if(occ.atom_selection is None):
        selection = self.top_selection
      else:
        asc = self.pdb_hierarchy.atom_selection_cache(
        special_position_settings=crystal.special_position_settings(
            crystal_symmetry = self.crystal_symmetry))
        sel = asc.selection(occ.atom_selection)
        selection = flex.smart_selection(flags=sel)
      if(occ.randomize):
        self._print_action(
          text = "Randomizing occupancies",
          selection = selection)
        check_if_already_modified()
        self.xray_structure.shake_occupancies(selection=selection.flags)
      if(occ.set is not None):
        self._print_action(
          text = "Setting occupancies to: %8.3f"%occ.set, selection = selection)
        check_if_already_modified()
        self.xray_structure.set_occupancies(
            value = occ.set,
            selection = selection.flags)

  def _rotate_about_axis(self):
    raap = self.params.rotate_about_axis
    sites_cart = self.xray_structure.sites_cart()
    if([raap.axis, raap.atom_selection, raap.angle].count(None)==0):
      axis = []
      try:
        for a in raap.axis.split():
          axis.append(float(a))
      except Exception:
        asc = self.pdb_hierarchy.atom_selection_cache()
        sel = asc.selection(raap.axis)
        axis = [i for i in sites_cart.select(sel).as_double()]
      if(len(axis)!=6):
        raise Sorry("Bad selection rotate_about_axis.axis: %s"%str(raap.axis))
      p1 = scitbx.matrix.col(axis[:3])
      p2 = scitbx.matrix.col(axis[3:])
      raa = p1.rt_for_rotation_around_axis_through(
        point=p2, angle=raap.angle, deg=True)
      asc = self.pdb_hierarchy.atom_selection_cache()
      sel = asc.selection(raap.atom_selection)
      if(sel.count(True)==0):
        raise Sorry(
          "Empty selection rotate_about_axis.selection: %s"%str(raap.atom_selection))
      sites_cart_rotated = raa * sites_cart.select(sel)
      self.xray_structure.set_sites_cart(
        sites_cart.set_selected(sel, sites_cart_rotated))

  def _neutralize_scatterers(self):
    if self.params.neutralize_scatterers:
      self.model.neutralize_scatterers()

  def get_results(self):
    return group_args(
      model            = self.model,
      # pdb_hierarchy    = self.pdb_hierarchy,
      # crystal_symmetry = self.crystal_symmetry,
      )


 *******************************************************************************


 *******************************************************************************
mmtbx/process_predicted_model.py
from __future__ import division, print_function
import sys

################################################################################
#################### TOOLS FOR PROCESSING A PREDICTED MODEL ####################
################################################################################

"""
   process_predicted_model: tools to update B-values used in some
    model predictions as an error estimate indicator and to split up model
    into domains that contain the more confident predictions
"""

from scitbx.array_family import flex
from libtbx.utils import Sorry
from scitbx.matrix import col
from libtbx import group_args
from cctbx.maptbx.segment_and_split_map import get_co
import iotbx.phil

################################################################################
####################   process_predicted_model  ################################
################################################################################


master_phil_str = """
  process_predicted_model{

    remove_low_confidence_residues = True
      .type = bool
      .help = Remove low-confidence residues (based on minimum plddt or \
             maximum_rmsd, whichever is specified)
      .short_caption = Remove low-confidence residues
      .expert_level = 3

    continuous_chain = False
      .type = bool
      .help = When removing low-confidence residues, only trim from ends
      .short_caption = Maintain continuous chain
      .expert_level = 3

    split_model_by_compact_regions = True
      .type = bool
      .help = Split model into compact regions after removing \
           low-confidence residues.
      .short_caption = Split model into compact regions
      .expert_level = 3

    maximum_domains = 3
      .type = int
      .help = Maximum domains to obtain.  You can use this to merge \
                the closest domains at the end of splitting the model. Make\
                it bigger (and optionally make domain_size smaller) to \
                get more domains.  If model is processed in chunks, \
                maximum_domains will apply to each chunk.
      .short_caption = Maximum domains

    domain_size = 15
      .type = float
      .help = Approximate size of domains to be found (A units).  This is the \
               resolution that \
              will be used to make a domain map.  If you are getting too many \
              domains, try making domain_size bigger (maximum is 70 A).
      .short_caption = Domain size (A)

    adjust_domain_size = True
      .type = bool
      .help = If more that maximum_domains are initially found, increase \
               domain_size in increments of 5 A and take the value that \
               gives the smallest number of domains, but at \
               least maximum_domains.
      .short_caption = Adjust domain size

    minimum_domain_length = None
      .type = float
      .help = Minimum length of a domain to keep (reject at end if smaller).\
              Default is 10 if no pae matrix or alt_pae_params=False and \
                20 for pae_matrix with alt_pae_params=True
      .short_caption = Minimum domain length (residues)

    maximum_fraction_close = 0.3
      .type = float
      .help = Maximum fraction of CA in one domain close to one in another \
              before merging them
      .short_caption = Maximum fraction close

    minimum_sequential_residues = None
      .type = int
      .help = Minimum length of a short segment to keep (reject at end ). \
              Default is 5 if no pae matrix or alt_pae_params=False and \
                4 for pae_matrix with alt_pae_params=True
      .short_caption = Minimum sequential_residues

    minimum_remainder_sequence_length = 15
      .type = int
      .help = used to choose whether the sequence of a removed \
               segment is written to the remainder sequence file.
      .short_caption = Minimum remainder sequence length

    b_value_field_is = *plddt rmsd b_value
      .type = choice
      .help = The B-factor field in predicted models can be pLDDT \
             (confidence, 0-1 or 0-100) or rmsd (A) or a B-factor
      .short_caption = Contents of B-value field for input models
      .expert_level = 3

    input_plddt_is_fractional = None
      .type = bool
      .help = You can specify if the input plddt values (in B-factor field) \
                are fractional (0-1) or not (0-100). By default if all  \
               values are between 0 and 1 it is fractional.
      .short_caption = Input plddt is fractional

    minimum_plddt = None
      .type = float
      .help = If low-confidence residues are removed, the cutoff is defined by \
          minimum_plddt or maximum_rmsd, whichever is defined (you cannot \
          define both).  A minimum plddt of 0.70 corresponds to a maximum rmsd \
          of 1.5.  Minimum plddt values are fractional or not depending on \
          the value of input_plddt_is_fractional.
      .short_caption = Minimum plddt


    maximum_rmsd = 1.5
      .type = float
      .help = If low-confidence residues are removed, the cutoff is defined by \
          minimum_plddt or maximum_rmsd, whichever is defined (you cannot \
          define both).  A minimum plddt of 0.70 corresponds to a maximum rmsd \
          of 1.5.  Minimum plddt values are fractional or not depending on \
          the value of input_plddt_is_fractional.
      .short_caption = Maximum rmsd

    default_maximum_rmsd = 1.5
      .type = float
      .help = Default value of maximum_rmsd, used if maximum_rmsd is not set
      .short_caption = default_maximum_rmsd

    subtract_minimum_b = False
      .type = bool
      .help = If set, subtract the lowest B-value from all B-values \
          just before writing \
          out the final files.  Does not affect the cutoff for removing low-\
           confidence residues.

     pae_power = None
       .type = float
       .help = If PAE matrix (predicted alignment error matrix) is supplied,\
            each edge in the graph will be weighted proportional to \
              (1/pae**pae_power). Use this to try and get the number of domains\
              that you want (try 1, 0.5, 1.5, 2). \
             Default is 1 alt_pae_params=False and 2 for alt_pae_params=True
       .short_caption = PAE power (if PAE matrix supplied)

     pae_cutoff = None
       .type = float
       .help = If PAE matrix (predicted alignment error matrix) is supplied,\
            graph edges will only be created for residue pairs with \
            pae<pae_cutoff. \
          Default is 5 alt_pae_params=False and 4 for alt_pae_params=True
       .short_caption = PAE cutoff (if PAE matrix supplied)

     pae_graph_resolution = None
       .type = float
       .help = If PAE matrix (predicted alignment error matrix) is supplied,\
            pae_graph_resolution regulates how aggressively the clustering \
            algorithm is. Smaller values lead to larger clusters. Value \
            should be larger than zero, and values larger than 5 are \
            unlikely to be useful, \
          Default is 0.5 alt_pae_params=False and 4 for alt_pae_params=True
       .short_caption = PAE graph resolution (if PAE matrix supplied)

     alt_pae_params = True
       .type = bool
       .help = If PAE matrix is supplied, use alternative set of defaults \
           (minimum_domain_length=20 minimum_sequential_residues=10 \
            pae_power=2 pae_cutoff=4 pae_graph_resolution=4). \
           Standard parameters (alt_pae_params=False) are: \
           (minimum_domain_length=10 minimum_sequential_residues=5 \
            pae_power=1 pae_cutoff=5 pae_graph_resolution=0.5).
       .short_caption = Use PAE defaults for pae_power=2

     weight_by_ca_ca_distance = False
       .type = bool
       .help = Adjust the edge weighting for each residue pair according  \
             to the distance between CA residues. If this is True, \
             then distance_model can be provided, otherwise supplied model \
             will be used. See also distance_power
       .short_caption = Weight by CA-CA distance

     distance_power = 1
       .type = float
       .help = If weight_by_ca_ca_distance is True, then edge weights will \
          be multiplied by 1/distance**distance_power.
       .short_caption = Distance power (for weighting by CA-CA distance)

     stop_if_no_residues_obtained = True
      .type = bool
      .help = Raise Sorry and stop if processing yields no residues
      .short_caption = Stop if no result

     keep_all_if_no_residues_obtained = False
      .type = bool
      .help = Keep everything if processing yields no residues
      .short_caption = Keep all if no result

     vrms_from_rmsd_intercept = 0.25
       .type = float
       .help = Estimate of vrms (error in model) from pLDDT will be based on\
           vrms_from_rmsd_intercept + vrms_from_rmsd_slope * pLDDT \
           where mean pLDDT of non-low_confidence_residues is used.
       .short_caption = vRMS intercept

     vrms_from_rmsd_slope = 1.0
       .type = float
       .help = Estimate of vrms (error in model) from pLDDT will be based on\
           vrms_from_rmsd_intercept + vrms_from_rmsd_slope * pLDDT \
           where mean pLDDT of non-low_confidence_residues is used.
       .short_caption = vRMS slope

     break_into_chunks_if_length_is = 1500
       .type = int
       .help = If a sequence is at least \
                break_into_chunks_if_length_is, break it into chunks \
                of length chunk_size with overlap of overlap_size for \
                domain identification using split_model_by_compact_regions \
                without a pae matrix
       .short_caption = Threshold for domain identification in chunks

     chunk_size = 600
       .type = int
       .help = If a sequence is at least \
                break_into_chunks_if_length_is, break it into chunks \
                of length chunk_size with overlap of overlap_size for \
                domain identification using split_model_by_compact_regions \
                without a pae_matrix
       .short_caption = Chunk size

     overlap_size = 200
       .type = int
       .help = If a sequence is at least \
                break_into_chunks_if_length_is, break it into chunks \
                of length chunk_size with overlap of overlap_size for \
                domain identification using split_model_by_compact_regions \
                without a pae_matrix
       .short_caption = Overlap length



    }

    """


def process_predicted_model(
    model,
    params,
    pae_matrix = None,
    distance_model = None,
    mark_atoms_to_keep_with_occ_one = False,
    log = sys.stdout):


  """
  process_predicted_model:
  Purpose:  Convert values in B-value field to pseudo-B-values, remove
    low_confidence residues, optionally split into compact regions.
  Rationale: predicted models may have regions of low and high confidence.
    This routine uses values in the B-value field to identify confidence,
    removes low-confidence regions, and then examines the remaining model to
    find regions that are compact (residues have high contact with neighbors)
    and that are separate from other regions (low contact with neigbors).

  Inputs (supplied as model and a params object):
    model:  iotbx.model.model object containing model information.
           Normally contains a single chain.   If multiple chains, process
           each separately.

    b_value_field_is:  'plddt' or 'rmsd' or 'b_value'.  For AlphaFold models
                        the b-value field is a value of pLDDT (confidence)
                        on scale of 0-1 or 0-100
                        For RoseTTAFold, the B-value field is rmsd (A)
                        If b_value... it is left as is.

    input_plddt_is_fractional:  if True, input plddt is scale of 0 to 1,
        otherwise 0 - 100
       If None, set to True if all plddt are from 0 to 1
    remove_low_confidence_residues: remove residues with low confidence
        (plddt or rmsd as set below)
    continuous_chain: if removing low-confidence residues, trim ends only. Note
         that if this is set, only the pae_matrix method of finding domains
         will work; the standard method will give a single domain.
    minimum_plddt: minimum plddt to keep residues (on same scale as b_value_field,
      if not set, calculated from maximum_rmsd).
    maximum_rmsd: alternative specification of minimum confidence based on rmsd.
        If not set, calculated from minimum_plddt.
    default_maximum_rmsd:  used as default if nothing specified for
         maximum_rmsd or minimum_plddt .Default is 1.5 A,
    split_model_by_compact_regions: split resulting model into compact regions
      and return a list of models in the group_arg return object
    pae_matrix:  matrix of predicted aligned errors (e.g., from AlphaFold2), NxN
      matrix of RMSD values, N = number of residues in model.
      Alternative to splitting by compact regions. Split to minimize predicted
          aligned errors in each grouping.
        pae_power (default=1): each edge in the graph will be weighted
           proportional to (1/pae**pae_power)
        pae_cutoff (optional, default=5): graph edges will only be created for
         residue pairs with pae<pae_cutoff
    weight_by_ca_ca_distance: (optional, default=False): adjust the edge
        weighting for each residue pair according to the distance between
        CA residues. If this is True, then distance_model must be provided.
    distance_power (optional, default=1): If weight_by_ca_ca_distance` is True,
         then edge weights will be multiplied by 1/distance**distance_power.
    distance_model ((optional, default=None): A model corresponding to the
        PAE matrix. Only needed if weight_by_ca_ca_distances is True.

    domain_size: typical size of domains (resolution used for filtering is
       the domain size)
    adjust_domain_size: increase domain_size if more than maximum domains found
    minimum_domain_length: minimum length (residues) of a domain to keep
    maximum_fraction_close: Merge domains with more than this fraction of close
                           CA atoms
    maximum_domains: if more than this many domains, merge close ones to reduce
       number
    chain_id: if model contains more than one chain, split this chain only.
              NOTE: only one chain can be processed at a time.
    if subtract_minimum_b is set, subtract minimum(B values) from all B values
       after applying any B value cutoffs

    If mark_atoms_to_keep_with_occ_one is set, return list of models, each
      of which is complete, but in which occupancy = 1 marks atoms to include
      and occupancy=0 marks those to exclude

    If stop_if_no_residues_obtained (default), stop with Sorry if no residues
      are obtained after processing, except if
        keep_all_if_no_residues_obtained (not default), then take everything.

    break_into_chunks_if_length_is, chunk_size, overlap_size:
       If a sequence is at least break_into_chunks_if_length_is, break
         it into chunks of length chunk_size with overlap of overlap_size for
         domain identification using split_model_by_compact_regions without
         pae_matrix

    alt_pae_params: Use alternative set of defaults for pae params if pae_matrix
        is present. Two possibilities are:
          Alternative if pae_matrix:
           (minimum_domain_length=20 minimum_sequential_residues=10 \
            pae_power=2 pae_cutoff=4 pae_graph_resolution=4).
           Standard parameters (alt_pae_params=False or no pae_matrix) are:
           (minimum_domain_length=10 minimum_sequential_residues=5
            pae_power=1 pae_cutoff=5 pae_graph_resolution=0.5).


  Output:
    processed_model_info: group_args object containing:
      processed_model:  single model with regions identified in chainid field
      model_list:  list of models representing domains
      plddt_list: one plddt on scale of 0 to 1 for each residue in input model.
      vrms_list: one vrms estimate (rms model error in A for each model)

  How to get the parameters object set up:

    You can set up a parameters object like this (see example at end of this
    file as well:

    master_phil = iotbx.phil.parse(master_phil_str)
    params = master_phil.extract()
    from mmtbx.process_predicted_model import set_defaults
    set_defaults(params, pae_matrix = pae_matrix)

    The default values are set in the master_phil_str string above.
    You can then set values of params:

    params.process_predicted_model.split_model_by_compact_regions = True


  """

  # Make sure we have what we expect:
  import mmtbx.model
  assert isinstance(model, mmtbx.model.manager)

  # Make sure we have just 1 chain or a chain ID supplied
  chain_ids = model.chain_ids()
  if len(chain_ids) != 1:
    chain_id = model.first_chain_id()
    model.add_crystal_symmetry_if_necessary()
    model = model.apply_selection_string('chain %s' %(chain_id))

  # Decide what to do
  p = params.process_predicted_model
  set_defaults(p, pae_matrix = pae_matrix)


  # Determine if input plddt is fractional and get b values

  b_value_field = model.get_hierarchy().atoms().extract_b()
  if p.b_value_field_is == 'plddt':
    if p.input_plddt_is_fractional is None:
      sel = (b_value_field < 0) | (b_value_field > 1)
      p.input_plddt_is_fractional = (sel.count(True) == 0)

    b_values = get_b_values_from_plddt(b_value_field,
       input_plddt_is_fractional = p.input_plddt_is_fractional)

    if p.input_plddt_is_fractional:
      print("B-value field interpreted as pLDDT %s" %("(0 - 1)"), file = log)
    else:
      print("B-value field interpreted as pLDDT %s" %("(0 - 100)"), file = log)

  elif p.b_value_field_is == 'rmsd':
    b_values = get_b_values_rmsd(b_value_field)
    print("B-value field interpreted as rmsd %s" %("(0 - 1)"), file = log)

  elif p.b_value_field_is == 'b_value':
    b_values = b_value_field
    print("B-value field interpreted as b_values", file = log)
  else:
    raise Sorry("Please set b_value_field_is to b_value, plddt or rmsd")

  if (not p.input_plddt_is_fractional):
    if p.minimum_plddt is not None: # convert to fractional
      p.minimum_plddt = p.minimum_plddt * 0.01
      print("Minimum pLDDT converted to %.2f" %(p.minimum_plddt), file = log)

  # From here on we work only with fractional plddt

  # Get confidence cutoff if needed
  if p.remove_low_confidence_residues:
    maximum_b_value = get_cutoff_b_value(
      p.maximum_rmsd,
      p.minimum_plddt,
      default_maximum_rmsd = p.default_maximum_rmsd,
      log = log)
  else:
    maximum_b_value = None


  # Offset b-values and cutoff if requested
  if p.subtract_minimum_b:
    minimum_b = b_values.min_max_mean().min
    b_values -= minimum_b
    assert b_values.min_max_mean().min == 0
    if maximum_b_value is not None:
      maximum_b_value -= minimum_b  # offset this too
    print("Subtracting minimum B of " +
      "%.2f from values and from cutoff (now %s)" %(
      minimum_b, " %.2f" %maximum_b_value if maximum_b_value is not None else "None"), file = log)

  # Make a new model with new B-values

  ph  = model.get_hierarchy().deep_copy()
  ph.atoms().set_b(b_values)
  full_model_with_new_b_values = \
     model.as_map_model_manager().model_from_hierarchy(
     ph, return_as_model = True)

  # Remove low_confidence regions if desired
  if p.remove_low_confidence_residues:
    n_before = ph.overall_counts().n_residues
    selection_string = " (bfactor < %s)" %maximum_b_value

    # Get selection based on CA/P atoms
    asc1 = ph.atom_selection_cache()
    sel1 = asc1.selection('(name ca or name P) and (%s) ' %selection_string)
    if p.continuous_chain:  # trim ends only
       restore_true_except_at_ends(sel1)
    ca_ph = ph.select(sel1)
    selection_string_2 = get_selection_for_short_segments(ca_ph,None)

    # Apply this selection to full hierarchy
    asc1 = ph.atom_selection_cache()
    sel = asc1.selection(selection_string_2)
    working_ph = ph.select(sel).deep_copy() # XXX for double selection

    if p.minimum_sequential_residues:  #
      # Remove any very short segments
      asc1 = working_ph.atom_selection_cache()
      sel1 = asc1.selection('name ca or name P')
      ca_ph = working_ph.select(sel1)
      selection_to_remove = get_selection_for_short_segments(ca_ph,
         p.minimum_sequential_residues)
      if selection_to_remove:
        print("Removing short segments: %s" %(selection_to_remove), file = log)
        asc1 = ph.atom_selection_cache() # original ph
        sel2 = asc1.selection(selection_to_remove)
        sel = ~ (~sel | sel2)

    new_ph = ph.select(sel).deep_copy()
    n_after = new_ph.overall_counts().n_residues
    print("Total of %s of %s residues kept after B-factor filtering" %(
       n_after, n_before), file = log)
    from mmtbx.secondary_structure.find_ss_from_ca import \
     get_selection_string_from_model
    selection_for_b_factor_filtering = get_selection_string_from_model(
      hierarchy = new_ph)
    if mark_atoms_to_keep_with_occ_one:
      selection_for_b_factor_filtering_no_chain_id = \
        get_selection_string_from_model(
      hierarchy = new_ph, skip_chain_id = True)
    else:
      selection_for_b_factor_filtering_no_chain_id = None
    print("\nSelection string for B-factor filtering: %s" %(
      selection_for_b_factor_filtering), file = log)
    keep_all = False
    remainder_sequence_str = None
    if n_after == 0:
      if p.stop_if_no_residues_obtained:
        raise Sorry("No residues remaining after filtering...please check if "+
         "B-value field is really '%s'. Adjust maximum_rmsd if necessary." %(
           p.b_value_field_is))
      elif p.keep_all_if_no_residues_obtained:
        keep_all = True
        print("Keeping everything as no residues obtained after filtering",
           file = log)
      else:
        return group_args(
         group_args_type = 'processed predicted model',
         model = None,
         model_list = [],
         chainid_list = [],
         remainder_sequence_str = "",
         b_values = [],
         )

    if not keep_all:
      removed_ph = ph.select(~sel).deep_copy()
      from mmtbx.secondary_structure.find_ss_from_ca import model_info, \
         split_model
      remainder_sequence_str = ""
      for m in split_model(model_info(removed_ph)):
        seq = m.hierarchy.as_sequence(as_string = True)
        if len(seq) >= p.minimum_remainder_sequence_length:
          remainder_sequence_str += "\n> fragment sequence "
          remainder_sequence_str += "\n%s\n" %(
            m.hierarchy.as_sequence(as_string = True))
      ph = new_ph
  else:
    remainder_sequence_str = None
  # Get a new model
  new_model = model.as_map_model_manager().model_from_hierarchy(
     ph, return_as_model = True)

  # Get high-confidence regions as domains if desired:
  if p.split_model_by_compact_regions:

    if pae_matrix is not None: # use pae matrix method
      info = split_model_with_pae(model, new_model, pae_matrix,
        maximum_domains = p.maximum_domains,
        pae_power = p.pae_power,
        pae_cutoff = p.pae_cutoff,
        pae_graph_resolution = p.pae_graph_resolution,
        minimum_domain_length = p.minimum_domain_length,
        weight_by_ca_ca_distance = p.weight_by_ca_ca_distance,
        distance_power = p.distance_power,
        distance_model = distance_model,
        log = log)
    else: # usual
      info = split_model_into_compact_units(new_model,
        d_min = p.domain_size,
        adjust_domain_size = p.adjust_domain_size,
        maximum_domains = p.maximum_domains,
        minimum_domain_length = p.minimum_domain_length,
        maximum_fraction_close = p.maximum_fraction_close,
        break_into_chunks_if_length_is = p.break_into_chunks_if_length_is,
        chunk_size = p.chunk_size,
        overlap_size = p.overlap_size,
        log = log)
    if info is None:
      print("No compact regions identified", file = log)
      chainid_list = []
      model_list = []
    else:
      new_model = info.model
      chainid_list = info.chainid_list
      print("Total of %s regions identified" %(
        len(chainid_list)), file = log)
      model_list = split_model_by_chainid(new_model, chainid_list,
        mark_atoms_to_keep_with_occ_one = mark_atoms_to_keep_with_occ_one,
        full_model = full_model_with_new_b_values,
        selection_for_b_factor_filtering_no_chain_id =
          selection_for_b_factor_filtering_no_chain_id
       )
  else:
    model_list = []
    chainid_list = []

  # Estimate vrms (model error) for each domain
  vrms_list = get_vrms_list(p, model_list)
  return group_args(
    group_args_type = 'processed predicted model',
    model = new_model,
    model_list = model_list,
    chainid_list = chainid_list,
    remainder_sequence_str = remainder_sequence_str,
    b_values = b_values,
    vrms_list = vrms_list,
    )

def set_defaults(p, pae_matrix = None, log = sys.stdout):
  # Set defaults for some parameters that depend on inputs
  if p.minimum_domain_length is None:
    if (pae_matrix is not None) and p.alt_pae_params:
      p.minimum_domain_length = 20
    else:
      p.minimum_domain_length = 10
    print("Minimum_domain_length=%s" %(p.minimum_domain_length), file = log)
  if p.minimum_sequential_residues is None:
    if (pae_matrix is not None) and p.alt_pae_params:
      p.minimum_sequential_residues = 10
    else:
      p.minimum_sequential_residues = 5
    print("Minimum_sequential_residues=%s" %(p.minimum_sequential_residues),
      file = log)
  if p.pae_power is None:
    if (pae_matrix is not None) and p.alt_pae_params:
      p.pae_power = 2
    else:
      p.pae_power = 1
    print("pae_power=%s" %(p.pae_power), file = log)
  if p.pae_cutoff is None:
    if (pae_matrix is not None) and p.alt_pae_params:
      p.pae_cutoff = 4
    else:
      p.pae_cutoff = 5
    print("pae_cutoff=%s" %(p.pae_cutoff), file = log)
  if p.pae_graph_resolution is None:
    if (pae_matrix is not None) and p.alt_pae_params:
      p.pae_graph_resolution = 4
    else:
      p.pae_graph_resolution = 0.5
    print("pae_graph_resolution=%s" %(p.pae_graph_resolution), file = log)



def restore_true_except_at_ends(sel1):
  ''' Set all values that are not at ends of sel1 to False (any number
    at ends may be False)'''

  values = list(sel1)
  if not True in values:
    return  # nothing to do
  first_true = values.index(True)
  values.reverse()
  last_true_from_end = values.index(True)
  last_true = len(values) - last_true_from_end
  for i in range(first_true,last_true):
    sel1[i] = True

def get_vrms_list(p, model_list):
  vrms_list = []
  for m in model_list:
    s = m.as_sequence(as_string = True)
    b_values = m.apply_selection_string(
       '(name ca or name P) and not element ca').get_b_iso()
    rmsd = get_rmsd_from_plddt(get_plddt_from_b(b_values)).min_max_mean().mean
    vrms = rmsd * p.vrms_from_rmsd_slope + p.vrms_from_rmsd_intercept
    vrms_list.append(vrms)
  return vrms_list

def get_selection_for_short_segments(ph, minimum_sequential_residues):
  chain_dict = {}
  for model in ph.models():
    for chain in model.chains():
      residue_list = []
      for rg in chain.residue_groups():
        resseq_int = rg.resseq_as_int()
        residue_list.append(resseq_int)
      residue_list = sorted(residue_list)
      chain_dict[chain.id] = residue_list
  selections = []
  for chain_id in chain_dict.keys():
    residue_list = chain_dict[chain_id]
    for r in get_indices_as_ranges(residue_list):
      if (minimum_sequential_residues is None) or (
          r.end - r.start + 1 < minimum_sequential_residues):
        selections.append("(chain '%s' and resseq %s:%s)" %(
          chain_id, r.start, r.end))
  selection_string = " or ".join(selections)
  return selection_string




def split_model_by_chainid(m, chainid_list,
    mark_atoms_to_keep_with_occ_one = False, full_model = None,
    selection_for_b_factor_filtering_no_chain_id = None):

  """
   Split a model into pieces based on chainid
   Optionally write out everything for each model, using
      occupancy=0 to mark everything that is not select3ed
  """
  split_model_list = []
  for chainid in chainid_list:
    selection_string = "chain %s" %(chainid)
    ph = m.get_hierarchy()
    asc1 = ph.atom_selection_cache()
    sel = asc1.selection(selection_string)
    if (not mark_atoms_to_keep_with_occ_one): # usual
      m1 = m.select(sel)
    else:  # for Voyager, mark unused with zero occupancies
      # Start with full model
      m1a = m.select(sel)
      from mmtbx.secondary_structure.find_ss_from_ca import \
         get_selection_string_from_model
      selection_for_working = get_selection_string_from_model(model = m1a,
        skip_chain_id = True)
      full_selection = "(%s) and (%s)" %(
        selection_for_b_factor_filtering_no_chain_id,
         selection_for_working)

      m1 = full_model.deep_copy()
      ph1 = m1.get_hierarchy()
      asc1 = ph1.atom_selection_cache()
      sel1 = asc1.selection(full_selection)

      atoms = ph1.atoms()
      occupancies = atoms.extract_occ()
      occupancies.set_selected(sel1, 1)
      occupancies.set_selected(~sel1, 0)
      atoms.set_occ(occupancies)
    split_model_list.append(m1)
  return split_model_list

def get_cutoff_b_value(
    maximum_rmsd,
    minimum_plddt,
    default_maximum_rmsd = None,
    log = sys.stdout):

  # Get B-value cutoff

  if maximum_rmsd is None and minimum_plddt is None:
    maximum_rmsd = default_maximum_rmsd
    assert maximum_rmsd is not None

  if maximum_rmsd is not None:
    print("Maximum rmsd of %.2f A used" %(maximum_rmsd), file = log)
  elif minimum_plddt:
    print("Minimum confidence level is %.2f" %(
      minimum_plddt), file = log)
    if minimum_plddt< 0 or \
      minimum_plddt> 1:
      raise Sorry("minimum_plddt must "+
         "be between 0 and 1")
    maximum_rmsd = get_rmsd_from_plddt(
           flex.double(1,minimum_plddt),
           is_fractional = True)[0]
    print("Maximum rmsd set to %.2f A based on confidence cutoff of %.2f" %(
       maximum_rmsd, minimum_plddt), file = log)
  else:
     raise Sorry( "Need to set either maximum_rmsd or " +
          "minimum_plddt")

  maximum_b_value = get_b_values_rmsd(
     flex.double(1,maximum_rmsd))[0]

  print("Maximum B-value to be included: %.2f A**2" %(maximum_b_value),
    file = log)
  return maximum_b_value




################################################################################
####################   get_b_values_from_plddt  ################################
################################################################################

def get_b_values_from_plddt(plddt_values,
    input_plddt_is_fractional = True):
  """
  get_b_values_from_plddt:
  Purpose:  AlphaFold models are supplied with values of pLDDT (predicted
   local-distance difference test) in the B-value field.  This routine
   uses the formula from:

     Hiranuma, N., Park, H., Baek, M. et al. Improved protein structure
       refinement guided by deep learning based accuracy estimation.
       Nat Commun 12, 1340 (2021).
       https://doi.org/10.1038/s41467-021-21511-x

   to convert these values to error estimates,
   and then uses the relation between B-values and coordinate rms to
   generate pseudo-B-factors

  NOTE: formulas taken from phaser_voyager implementation by
  Claudia Millan and Massimo Sammito at
  phaser_voyager/src/Voyager/MDSLibraries/pdb_structure.py


  Inputs:
    plddt_values: flex array of plddt values
    input_plddt_is_fractional: if False, convert by multiplying * 0.01
  Outputs:
    flex array of B-values
  """

  if input_plddt_is_fractional:
    rmsd = get_rmsd_from_plddt(plddt_values) # usual
  else:
    rmsd = get_rmsd_from_plddt(0.01 * plddt_values)
  b_values = get_b_values_rmsd(rmsd)

  return b_values

################################################################################
####################   get_plddt_from_b          ################################
################################################################################

def get_plddt_from_b(b_values, input_plddt_is_fractional = True):
  """  Inverse of get_b_values_from_plddt
  Inputs:
    flex array of B-values
    input_plddt_is_fractional: if False, convert by multiplying * 100 at end
  Outputs:
    plddt_values: flex array of plddt values
  """
  if not b_values:
    return None

  # b_values = flex.pow2(rmsd) * ((8 * (3.14159 ** 2)) / 3.0)
  if b_values.min_max_mean().min < 0:
    b_values = b_values.deep_copy()
    b_values.set_selected(b_values < 0, 0)
  rmsd = flex.sqrt( b_values/ ((8 * (3.14159 ** 2)) / 3.0))

  # rmsd  = 1.5 * flex.exp(4*(0.7-plddt))
  plddt = 0.7 - 0.25 * flex.log(rmsd/1.5)
  if plddt.min_max_mean().min < 0 or plddt.min_max_mean().max > 1:
    plddt.set_selected(plddt < 0, 0)
    plddt.set_selected(plddt > 1, 1)

  if not input_plddt_is_fractional:
    plddt = plddt * 100

  return plddt


################################################################################
####################   get_rmsd_from_plddt  ################################
################################################################################

def get_rmsd_from_plddt(plddt_values, is_fractional = None):
  """
  get_rmsd_from_plddt:
  Purpose:  AlphaFold models come with predicted pLDDT values in the B-value
   field to indicate confidence.  This routine uses a formula provided in the
   supplementary material of the RoseTTAFold paper to convert these values
   to error estimates.
  NOTE: plddt_values can be fractional (0 to 1) or percentage (0 to 100)
  If is_fractional is not set, assume fractional if all between 0 and 1
  rmsd_est = 1.5 * flex.exp(4*(0.7-fractional_values))

  NOTE: formulas taken from phaser_voyager implementation by
  Claudia Millan and Massimo Sammito at
  phaser_voyager/src/Voyager/MDSLibraries/pdb_structure.py


  Inputs:
    plddt_values: flex array of plddt values
  Outputs:
    flex array of error estimates (A)
  """
  if is_fractional is None:
    is_fractional = ( plddt_values.min_max_mean().min >= 0  and
      plddt_values.min_max_mean().max <= 1 )

  if is_fractional:
    fractional_values = plddt_values.deep_copy()
  else:
    fractional_values = plddt_values * 0.01

  fractional_values.set_selected((fractional_values < 0), 0)
  fractional_values.set_selected((fractional_values > 1), 1)

  rmsd_est = 1.5 * flex.exp(4*(0.7-fractional_values))
  return rmsd_est

################################################################################
####################   get_b_values_rmsd #######################
################################################################################

def get_b_values_rmsd(rmsd):
  """
  get_b_values_rmsd:
  Purpose:  TTAFold models are supplied with values of rmsd (A)
   in the B-value field.  This routine converts error estimates into
   b-values

  NOTE: formulas taken from phaser_voyager implementation by
  Claudia Millan and Massimo Sammito at
  phaser_voyager/src/Voyager/MDSLibraries/pdb_structure.py


  Inputs:
    rmsd: flex array of error estimates (A)
  Outputs:
    flex array of B-values (A**2)
  """

  rmsd = rmsd.deep_copy() # do not change original

  # Make sure error estimates are in reasonable range
  rmsd.set_selected((rmsd < 0), 0)
  rmsd.set_selected((rmsd > 20), 20)

  b_values = flex.pow2(rmsd) * ((8 * (3.14159 ** 2)) / 3.0)
  return b_values

################################################################################
####################   split_model_with_pae  ###################################
################################################################################

def split_model_with_pae(
     model,
     m,
     pae_matrix,
     maximum_domains = None,
     pae_power = 1.,
     pae_cutoff = 5.,
     pae_graph_resolution = 0.5,
     minimum_domain_length = 10,
     weight_by_ca_ca_distance = False,
     distance_power = 1,
     distance_model = None,
     log = sys.stdout):

  """
   Function to identify groups of atoms in a model that form compact units
   using a predicted alignment error matrix (pae_matrix).
   Normally used after trimming low-confidence regions in
   predicted models to isolate domains that are likely to have indeterminate
   relationships.

   m:  cctbx.model.model object containing information about the input model
     after trimming
   model: model before trimming
   pae_matrix:  matrix of predicted aligned errors (e.g., from AlphaFold2), NxN
       matrix of RMSD values, N = number of residues in model.
   maximum_domains:  If more than this many domains, merge closest ones until
     reaching this number
   pae_power (default=1): each edge in the graph will be weighted
       proportional to (1/pae**pae_power)
   pae_cutoff (optional, default=5): graph edges will only be created for
       residue pairs with pae<pae_cutoff
   pae_graph_resolution (optional, default = 0.5): regulates how aggressively
       the clustering algorithm is. Smaller values lead to larger clusters.
       Value should be larger than zero, and values larger than 5 are
        unlikely to be useful
   weight_by_ca_ca_distance: (optional, default=False): adjust the edge
        weighting for each residue pair according to the distance between
        CA residues. If this is True, then distance_model must be provided.
   distance_power (optional, default=1): If weight_by_ca_ca_distance` is True,
         then edge weights will be multiplied by 1/distance**distance_power.
   distance_model ((optional, default=None): A model corresponding to the
        PAE matrix. Only needed if weight_by_ca_ca_distances is True.
   minimum_domain_length:  if a region is smaller than this, skip completely

   Output:
   group_args object with members:
    m:  new model with chainid values from 0 to N where there are N domains
      chainid 1 to N are the N domains, roughly in order along the chain.
    chainid_list:  list of all the chainid values

   On failure:  returns None
  """

  print("\nSelecting domains with predicted alignment uncertainty estimates",
     file = log)
  # Select CA and P atoms with B-values in range
  selection_string = '(name ca or name p)'
  m_ca_or_p = m.apply_selection_string(selection_string)
  n = model.apply_selection_string(selection_string
       ).get_hierarchy().overall_counts().n_residues

  # Make sure matrix matches
  if tuple(pae_matrix.shape) != (n, n):
     raise Sorry("The pae matrix has a size of (%s,%s) " %(
      tuple(pae_matrix.shape)) +
      "but the number of residues in the model is %s" %(n))
  first_resno = model.first_resseq_as_int()

  #  Assign all CA in model to a region
  from mmtbx.domains_from_pae import get_domain_selections_from_pae_matrix
  selection_list = get_domain_selections_from_pae_matrix(
    pae_matrix = pae_matrix,
     pae_power = pae_power,
     pae_cutoff = pae_cutoff,
     graph_resolution = pae_graph_resolution,
     first_resno = first_resno,
     weight_by_ca_ca_distance = weight_by_ca_ca_distance,
     distance_power = distance_power,
     distance_model = distance_model,
    )

  # And apply to full model

  unique_regions = list(range(len(selection_list)))

  keep_list = []
  good_selections = []
  ph = m.get_hierarchy()
  ca_or_p_string =  '(name ca or name p)'

  for selection_string, region_number in zip(selection_list,unique_regions):
    asc1 = ph.atom_selection_cache()
    sel = asc1.selection("(%s) and %s" %(selection_string,ca_or_p_string))
    if sel.count(True) >= minimum_domain_length:
      keep_list.append(True)
      good_selections.append(selection_string)
    else:
      keep_list.append(False)
      print("Skipping region with selection '%s' that contains %s residues" %(
         selection_string,sel.count(True)),
        file = log)

  region_name_dict, chainid_list = get_region_name_dict(m, unique_regions,
    keep_list = keep_list)
  print("\nSelection list based on PAE values:", file =log)

  # Now create new model with chains based on region list
  full_new_model = None
  for keep, selection_string, region_number in zip(
     keep_list, selection_list,unique_regions):
    if not keep: continue
    new_m = m.apply_selection_string(selection_string)
    print("%s (%s residues)  "%(selection_string,
      new_m.get_hierarchy().overall_counts().n_residues), file = log)
    # Now put all of new_m in a chain with chain.id = str(region_number)
    for model in new_m.get_hierarchy().models()[:1]: # only one model
      for chain in model.chains()[:1]: # only allowing one chain
        chain.id = region_name_dict[region_number]
    new_m._update_atom_selection_cache() # changed chain IDs...
    if full_new_model:
      full_new_model = add_model(full_new_model, new_m)
    else:
      full_new_model = new_m
  m = full_new_model

  from mmtbx.secondary_structure.find_ss_from_ca import \
     get_selection_string_from_model
  if m:
    selection_for_full_model = get_selection_string_from_model(model = m)
    print("\nSelection string for final model: %s" %(selection_for_full_model),
      file = log)
    print("Total residues in final model: %s" %(
     m.overall_counts().n_residues), file = log)
  else:
    print("No model obtained...", file = log)


  # All done
  return group_args(
    group_args_type = 'model_info',
    model = m,
    chainid_list = chainid_list)

################################################################################
####################   split_model_into_compact_units_by_chunks   ##############
################################################################################

def split_model_into_compact_units_by_chunks(
     m,
     d_min = 15,
     grid_resolution = 6,
     close_distance = 15,
     minimum_domain_length = 10,
     maximum_fraction_close = 0.3,
     maximum_domains = None,
     adjust_domain_size = None,
     break_into_chunks_if_length_is = None,
     chunk_size = None,
     overlap_size = None,
     log = sys.stdout):

  """
   Function to sequentially run split_model_into_compact_units on
    chunks of a large model, breaking into overlapping chunks of size
    chunk_size, get domains from each, putting them together.
   The value of maximum_domains will apply only within a chunk.
   See split_model_into_compact_units for details

  """
  first_resno = m.first_resseq_as_int()
  last_resno = m.last_resseq_as_int()
  # Select CA and P atoms with B-values in range
  selection_string = '(name ca or name p)'
  m_ca_or_p = m.apply_selection_string(selection_string)
  print("Residue range: (%s - %s) " %(first_resno,last_resno), file = log)
  chunks = get_residue_ranges_for_chunks(model = m,
     first_resno = first_resno,
     last_resno = last_resno,
     chunk_size = chunk_size, overlap_size = overlap_size)
  print("Residue ranges for chunks:", file = log)
  for c in chunks:
    print(c, file = log)

  info_list = []
  for c in chunks:
    selection_text = "resseq %s:%s" %(c[0], c[1])
    working_m = m.apply_selection_string(selection_text)
    print("\nGetting compact domains in residue range %s" %(selection_text),
      file = log)
    info = split_model_into_compact_units(
     working_m,
     d_min = d_min,
     grid_resolution = grid_resolution,
     close_distance = close_distance,
     minimum_domain_length = minimum_domain_length,
     maximum_fraction_close = maximum_fraction_close,
     maximum_domains = maximum_domains,
     adjust_domain_size = adjust_domain_size,
     break_into_chunks_if_length_is = working_m.overall_counts().n_residues + 1,
     log = log)
    info.first_resno = c[0]
    info.last_resno = c[1]
    info.residues_present = get_residues_present_list(working_m)
    info_list.append(info)

  # Find crossover points between each block that minimizes
  #   crossing over between parts of a domain
  crossover_points_list = find_crossover_points(info_list)

  # Assemble into full list. Crossover is residue number of last residue
  new_regions_list = []
  numbers_list = []
  for info, start_crossover, end_crossover in zip(
     info_list, [None] + crossover_points_list , crossover_points_list +[None]):
    if start_crossover in info.residues_present:
      working_start_crossover = info.residues_present.index(start_crossover)
    else:
      working_start_crossover = None
    if end_crossover in info.residues_present:
      working_end_crossover = info.residues_present.index(end_crossover)
    else:
      working_end_crossover = None
    r1 = list(info.residues_present)
    if (working_start_crossover is not None) and (
       working_end_crossover is not None):
      section = info.regions_list[
           working_start_crossover:working_end_crossover]
      numbers_section = r1[working_start_crossover:working_end_crossover]
    elif working_start_crossover is not None:
      section = info.regions_list[working_start_crossover:]
      numbers_section = r1[working_start_crossover:]
    else:
      numbers_section = r1[:working_end_crossover]
      section = info.regions_list[:working_end_crossover]
    unique_numbers = get_unique_values(section)
    used_numbers = get_unique_values(new_regions_list)
    new_numbers = get_new_numbers(unique_numbers, used_numbers)
    new_section = replace_numbers(section, unique_numbers, new_numbers)
    new_regions_list += new_section


  new_regions_list = flex.int(new_regions_list)
  # Update region list
  new_regions_list = recalculate_regions_list(new_regions_list,
     minimum_domain_length = minimum_domain_length,
     model = m_ca_or_p, close_distance = close_distance,
     maximum_domains = maximum_domains * len(chunks) if \
       maximum_domains is not None else None,
     maximum_fraction_close = maximum_fraction_close,
     log = log)

  # Get unique domain ids for each chunk and make globally unique
  info = set_chain_id_by_region(m, m_ca_or_p, new_regions_list, log = log)

  return info

def get_residues_present_list(m):
  hierarchy = m.get_hierarchy()
  residue_list = []
  for model in hierarchy.models()[:1]:
    for chain in model.chains()[:1]:
       for rg in chain.residue_groups():
          residue_list.append(rg.resseq_as_int())
  return residue_list

def replace_numbers(section, unique_numbers, new_numbers):
  s = flex.int(section)
  for old,new in zip(unique_numbers, new_numbers):
    sel = (s == old)
    s.set_selected(sel, new)
  return list(s)

def get_new_numbers(unique_numbers, used_numbers, keep_zeros = True):
  if used_numbers:
    max_n = max(used_numbers)
  else:
    max_n = 0
  new_numbers = []
  for i in unique_numbers:
    if keep_zeros and i == 0:
      if not 0 in used_numbers:
        used_numbers.append(i)
      new_numbers.append(i)
    if not i in used_numbers:
      used_numbers.append(i)
      new_numbers.append(i)
    else:
      max_n = max_n + 1
      used_numbers.append(max_n)
      new_numbers.append(max_n)
  return new_numbers

def find_crossover_points(info_list):
  # Try to find crossover points between each block that minimizes
  #   crossing over between parts of a domain
  crossover_points_list = []
  for info_1, info_2 in zip(info_list, info_list[1:]):
    crossover_points_list.append(find_one_crossover_point(info_1, info_2))
  return crossover_points_list

def find_one_crossover_point(info_1, info_2):
  # Try to find crossover points between blocks info_1 and info_2 that
  #  minimizes crossing over between parts of a domain at least on one of
  # the two chains
  unique_regions_1 = get_unique_values(info_1.regions_list)
  unique_regions_2 = get_unique_values(info_2.regions_list)
  # Find places in overlap where all values in info_1 after overlap are
  #   different from all values before, and same for info_2
  first_crossover = info_2.first_resno
  last_crossover = info_1.last_resno
  best_crossover = None
  best_crossover_score = None
  for crossover in range(first_crossover, last_crossover + 1):
    crossover_info = get_crossover_info(info_1, info_2, crossover)
    if not crossover_info:
      continue
    unique_regions_1a = crossover_info.unique_regions_1a
    unique_regions_1b = crossover_info.unique_regions_1b
    unique_regions_2a = crossover_info.unique_regions_2a
    unique_regions_2b = crossover_info.unique_regions_2b
    values_distinct_1 = not values_overlap(unique_regions_1a, unique_regions_1b)
    if values_distinct_1:
      overlap_1 = 0
    else:
      overlap_1 = count_overlap(unique_regions_1a, unique_regions_1b)
    values_distinct_2 = not values_overlap(unique_regions_2a, unique_regions_2b)
    if values_distinct_2:
      overlap_2 = 0
    else:
      overlap_2 = count_overlap(unique_regions_2a, unique_regions_2b)

    if values_distinct_1 and values_distinct_2: # perfect
      if (best_crossover_score is None) or (best_crossover_score < 3):
        best_crossover_score = 3
        best_crossover = crossover
    elif unique_regions_1a and unique_regions_1b and values_distinct_1:
      if (best_crossover_score is None) or (best_crossover_score < 2):
        best_crossover_score = 2
        best_crossover = crossover
    elif unique_regions_2a and unique_regions_2b and values_distinct_2:
      if (best_crossover_score is None) or (best_crossover_score < 2):
        best_crossover_score = 2
        best_crossover = crossover
    else:
      score = -1 * min(overlap_1, overlap_2)
      if (best_crossover_score is None) or (best_crossover_score < score):
        best_crossover_score = score
        best_crossover = crossover

  # Here everything is distinct
  print("Found overlap at %s" %(best_crossover))
  print("Found overlap at %s" %(best_crossover - info_1.first_resno ))
  crossover_info = get_crossover_info(info_1, info_2, best_crossover)
  unique_regions_1a = crossover_info.unique_regions_1a
  unique_regions_1b = crossover_info.unique_regions_1b
  unique_regions_2a = crossover_info.unique_regions_2a
  unique_regions_2b = crossover_info.unique_regions_2b

  print("Unique values in info 1: (%s), (%s)  info 2: (%s), (%s)" %(
    str(unique_regions_1a),str(unique_regions_1b),
    str(unique_regions_2a),str(unique_regions_2b),))
  return best_crossover

def get_crossover_info(info_1, info_2, crossover):
    # crossover is residue number to cross over. crossover_1 and _2 are
    #   indices in the full_regions_list that correspond to this residue
    #   number
    if info_1.residues_present and (not crossover in info_1.residues_present):
      return None
    if info_2.residues_present and (not crossover in info_2.residues_present):
      return None
    crossover_1 = info_1.residues_present.index(crossover)
    crossover_2 = info_2.residues_present.index(crossover)
    unique_regions_1a = get_unique_values(
        info_1.regions_list[:crossover_1])
    unique_regions_1b = get_unique_values(
        info_1.regions_list[crossover_1:])
    unique_regions_2a = get_unique_values(
        info_2.regions_list[:crossover_2])
    unique_regions_2b = get_unique_values(
        info_2.regions_list[crossover_2:])
    return group_args(group_args_type = 'overlap info',
      unique_regions_1a = unique_regions_1a,
      unique_regions_1b = unique_regions_1b,
      unique_regions_2a = unique_regions_2a,
      unique_regions_2b = unique_regions_2b,
      )
def count_overlap(v1,v2):
  count = 0
  for v in v1:
    if v in v2:
      count += 1
  return count

def values_overlap(v1,v2):
  for v in v1:
    if v in v2:
      return True
  return False

def get_residue_ranges_for_chunks(model = None,
     first_resno = None, last_resno = None,
     chunk_size = None, overlap_size = None):
  resno_list = list(range(first_resno, last_resno+1))

  chunks = []
  while resno_list:
    if len(resno_list) < 2*chunk_size and len(resno_list) > chunk_size:
      c = resno_list[:chunk_size]
      resno_list = resno_list[-chunk_size:] # take last chunk_size
      print(len(resno_list))
    else: # usual
      c = resno_list[:chunk_size]
      resno_list = resno_list[chunk_size-overlap_size:]
      if len(resno_list) < chunk_size: # we are done
        resno_list = ""
      print(len(resno_list))
    chunks.append([c[0], c[-1]])
  return chunks

################################################################################
####################   split_model_into_compact_units   ########################
################################################################################

def split_model_into_compact_units(
     m,
     d_min = 15,
     grid_resolution = 6,
     close_distance = 15,
     minimum_domain_length = 10,
     maximum_fraction_close = 0.3,
     maximum_domains = None,
     adjust_domain_size = None,
     break_into_chunks_if_length_is = None,
     chunk_size = None,
     overlap_size = None,
     log = sys.stdout):

  """
   Function to identify groups of atoms in a model that form compact units
   (domains).  Normally used after trimming low-confidence regions in
   predicted models to isolate domains that are likely to have indeterminate
   relationships.

   Method: calculate a low-resolution map based on the input model; identify
    large blobs corresponding to domains.  Assign all atoms in structure to
    a domain.  Then regroup in order to have few cases where small parts of
    model are part of one domain but neighboring parts are part of another.

   For very long chains (longer than break_into_chunks_if_length_is), break
   into overlapping chunks of size chunk_size, get domains from each, put
   them together.

   Inputs:
   m:  cctbx.model.model object containing information about the input model
   d_min:  resolution used for low-res map.  Corresponds roughly to domain size.
   adjust_domain_size: Vary d_min to try and obtain maximum_domains in initial
                       domain identification
   grid_resolution:  resolution of map used to define the gridding
   close_distance:  distance between two CA (or P) atoms considered close
                    NOTE: may be useful to double default for P compared to CA
   minimum_domain_length: typical size (CA or P) of smallest segments to keep
   minimum_remainder_sequence_length: minimum length of a removed sequence
      segment to write out to a new sequence file
   bfactor_min: smallest bfactor for atoms to include in calculations
   bfactor_max: largest bfactor for atoms to include in calculations
   maximum_domains:  If more than this many domains, merge closest ones until
     reaching this number
   break_into_chunks_if_length_is, chunk_size, overlap_size:
       If a sequence is at least break_into_chunks_if_length_is, break
         it into chunks of length chunk_size with overlap of overlap_size.
         Run on each chunk, then apply results to whole.
   Output:
   group_args object with members:
    m:  new model with chainid values from 0 to N where there are N domains
      chainid 1 to N are the N domains, roughly in order along the chain.
    chainid_list:  list of all the chainid values

   On failure:  returns None
  """
  if break_into_chunks_if_length_is is not None and \
       m.overall_counts().n_residues >= break_into_chunks_if_length_is:
    return split_model_into_compact_units_by_chunks(
     m,
     d_min = d_min,
     grid_resolution = grid_resolution,
     close_distance = close_distance,
     minimum_domain_length = minimum_domain_length,
     maximum_fraction_close = maximum_fraction_close,
     maximum_domains = maximum_domains,
     adjust_domain_size = adjust_domain_size,
     break_into_chunks_if_length_is = break_into_chunks_if_length_is,
     chunk_size = chunk_size,
     overlap_size = overlap_size,
     log = log)


  print("\nSelecting domains as compact chains",
     file = log)
  d_min = min(50, d_min) # limitation in fmodel

  # Make sure the model has P1 crystal_symmetry.  Put a box around it that is
  #  big (do not use original crystal symmetry because it might  be too big
  #  or too small)

  m = m.deep_copy()  # don't modify original

  box_cushion = 0.5 * d_min  # big box
  original_crystal_symmetry = m.crystal_symmetry()
  original_uc_crystal_symmetry = m.unit_cell_crystal_symmetry()
  m.add_crystal_symmetry_if_necessary(box_cushion = box_cushion, force = True)
  m.set_shift_cart(None)
  m.set_unit_cell_crystal_symmetry(m.crystal_symmetry())

  # Select CA and P atoms with B-values in range
  selection_string = '(name ca or name p)'
  m_ca_or_p = m.apply_selection_string(selection_string)


  # Put the model inside a box and get a map_model_manager
  put_model_inside_cell(m_ca_or_p, grid_resolution)

  # Generate map at medium_res for this model and use it to get domains
  info = get_map_and_d_min(
    m_ca_or_p, d_min = d_min, target_regions = maximum_domains,
    adjust_domain_size = adjust_domain_size,
    grid_resolution = grid_resolution, log = log)
  if not info:
    print("Failed to find domains", file = log)
    return # Nothing to do
  map_data = info.map_data
  co_info = info.co_info


  # Assign all points in box to a grouping
  co_info = assign_all_points(co_info, map_data, log = log)

  #  Assign all CA in model to a region
  regions_list = assign_ca_to_region(co_info, m_ca_or_p, minimum_domain_length,
     close_distance,
     maximum_domains = maximum_domains,
     maximum_fraction_close = maximum_fraction_close,
     log = log)

  info = set_chain_id_by_region(m, m_ca_or_p, regions_list, log = log)
  if original_crystal_symmetry and info and info.model:
    info.model.set_crystal_symmetry(original_crystal_symmetry)
  return info

def get_map_and_d_min(m_ca_or_p, d_min = None,
      adjust_domain_size = None, target_regions = None,
      grid_resolution = None,
      log = sys.stdout):
  # Choose d_min that give about target_regions clusters
  best_co_info = None
  best_map_data = None
  best_n = None
  best_d_min = None
  if adjust_domain_size:
     n_offset = 10
  else:
     n_offset = 1
  for offset in range(n_offset):
    mmm = m_ca_or_p.as_map_model_manager()
    d_min_use = d_min + 5 * offset
    if d_min_use > 70:
      continue # max is about 70
    mmm.set_resolution(d_min_use)
    try:
      mmm.generate_map(d_min_use,
        resolution_factor = 0.25 * grid_resolution/d_min )
    except Exception as e: # too low resolution
      continue

    # Box the map and set SD to 1 mean to 0
    box_mmm = mmm.extract_all_maps_around_model()
    box_mmm.map_manager().set_mean_zero_sd_one()

    # Now get regions where there is model
    map_data = box_mmm.map_manager().map_data()

    #  Get a connectivity analysis of this map data
    co_info = get_best_co(map_data)
    if not co_info:
      continue
    n = len(co_info.sorted_by_volume)
    if (best_n is None) or (target_regions is None) or (
        (n >= target_regions) and (n < best_n)):
      best_n = n
      best_co_info = co_info
      best_map_data = map_data
      best_d_min = d_min_use
  if co_info is None:
    return # Nothing to do
  n = best_n
  d_min = best_d_min
  co_info = best_co_info
  map_data = best_map_data
  print("Best resolution for domains is %.1f A giving %s regions" %(
        d_min, n), file = log)
  return group_args(group_args_type = 'co_info and map_data',
    d_min = d_min,
    co_info = co_info,
    map_data = map_data,)


def get_region_name_dict(m, unique_regions, keep_list = None):
  region_name_dict = {}
  chainid_list = []
  chainid = m.first_chain_id().strip()
  if not keep_list:
    keep_list = len(unique_regions) * [True]
  assert len(keep_list) == len(unique_regions)

  i = 0
  for region_number, keep in zip(unique_regions, keep_list):
      if not keep: continue
      i += 1
      if len(chainid) == 1 and i < 10:
          region_name = "%s%s" %(chainid,i)
      else:
          region_name = str(i)
      region_name_dict[region_number] = region_name
      chainid_list.append(region_name)
  return region_name_dict, chainid_list

def set_chain_id_by_region(m, m_ca_or_p, regions_list, log = sys.stdout):
  # Set chainid based on regions_list

  atoms = m_ca_or_p.get_hierarchy().atoms()  # new
  unique_regions = get_unique_values(regions_list)

  region_name_dict, chainid_list = get_region_name_dict(m, unique_regions)

  region_dict = {}
  for at, region_number in zip(atoms, regions_list):
    resseq_int = at.parent().parent().resseq_as_int()
    region_dict[resseq_int] = region_number

  # And apply to full model
  full_regions_list = flex.int()
  for at in m.get_hierarchy().atoms():
    resseq_int = at.parent().parent().resseq_as_int()
    region = region_dict.get(resseq_int,0)
    full_regions_list.append(region)

  # Now create new model with chains based on region list
  full_new_model = None
  print("\nSelection list based on domains:", file =log)
  for region_number in unique_regions:
    sel = (full_regions_list == region_number)
    new_m = m.select(sel)
    selection_string = selection_string_from_model(
       new_m.apply_selection_string("name ca or name P"))

    print("%s (%s residues)  "%(selection_string,
      new_m.get_hierarchy().overall_counts().n_residues), file = log)
    # Now put all of new_m in a chain with chain.id = str(region_number)
    for model in new_m.get_hierarchy().models()[:1]: # only one model
      for chain in model.chains()[:1]: # only allowing one chain
        chain.id = region_name_dict[region_number]
    if full_new_model:
      full_new_model = add_model(full_new_model, new_m)
    else:
      full_new_model = new_m
  full_new_model.reset_after_changing_hierarchy()
  m = full_new_model

  # All done
  return group_args(
    group_args_type = 'model_info',
    model = m,
    chainid_list = chainid_list,
    regions_list = regions_list,
    full_regions_list = full_regions_list)

def selection_string_from_model(model):
    resno_list = get_residue_numbers_in_model(model)
    from mmtbx.domains_from_pae import cluster_as_selection
    selection_string = cluster_as_selection(resno_list)
    return selection_string


def assign_ca_to_region(co_info,
    m,
    minimum_domain_length,
    close_distance,
    maximum_domains = None,
    maximum_fraction_close = None,
    log = sys.stdout):
  region_id_map = co_info.region_id_map
  id_list = co_info.id_list
  regions_list = flex.int()
  sites_frac = m.crystal_symmetry().unit_cell().fractionalize(m.get_sites_cart())
  for sf in sites_frac:
    regions_list.append(int(region_id_map.value_at_closest_grid_point(sf)))
  # Now remove occasional ones out of place

  regions_list = recalculate_regions_list(regions_list,
     minimum_domain_length = minimum_domain_length,
     model = m, close_distance = close_distance,
     maximum_domains = maximum_domains,
     maximum_fraction_close = maximum_fraction_close,
     log = log)

  return regions_list

def recalculate_regions_list(regions_list,
     minimum_domain_length = None,
     model = None,
     close_distance = None,
     maximum_domains = None,
     maximum_fraction_close = None,
     n_cycles = 10,
     log = sys.stdout):

  for cycle in range(n_cycles):
    regions_list = replace_lone_sites(regions_list)
    regions_list = replace_short_segments(regions_list, minimum_domain_length)
    for i in range(len(get_unique_values(regions_list))):
      new_regions_list = swap_close_regions(
        model.get_sites_cart(),
       regions_list, minimum_domain_length, close_distance)
      if new_regions_list:
         regions_list = new_regions_list
      else:
       break
  # Merge close regions if there are too many
  for k in range(len(get_unique_values(regions_list))):
    if maximum_domains and \
         len((get_unique_values(regions_list))) > maximum_domains:
      regions_list = merge_closest_regions(model.get_sites_cart(), regions_list,
        close_distance, log = log)
    else:
      break

  # Merge close regions if they are really close
  for k in range(len(get_unique_values(regions_list))):
    regions_list = merge_very_close_regions(model.get_sites_cart(),
        regions_list,
        close_distance,
        minimum_domain_length =  minimum_domain_length,
        maximum_fraction_close = maximum_fraction_close,
        log = log)

  # Finally check for any short fragments not attached to neighbors
  regions_list = remove_short_fragments_obscured_by_gap(regions_list,
    model, minimum_domain_length)
  return regions_list

def remove_short_fragments_obscured_by_gap(regions_list,
    m, minimum_domain_length):
  # Find any regions that are very short (<minimum_domain_length) and merge
  # with adjacent sequence if available
  # This catches cases where there was a gap in sequence.

  region_dict = {}
  for at, region_number in zip(m.get_hierarchy().atoms(), regions_list):
    resseq_int = at.parent().parent().resseq_as_int()
    region_dict[resseq_int] = region_number

  # Find all cases where regions go like:
  #  1 1 1 2 2 (gap)   -> 1 1 1 1 1 (gap)
  #  1 1 1 2 2 3 3 3 3  -> 1 1 1 1 1 3 3 3 3 or 1 1 1 3 3 3 3 3 3
  #  (gap) 2 2 3 3 3 3 -> (gap) 3 3 3 3 3 3
  # First split up resseq_int into ranges..
  residues_as_groups = get_indices_as_ranges(list(region_dict.keys()))
  for r in residues_as_groups:
    # Find all the places where region_number changes
    working_regions = []
    working_region = None
    for i in range(r.start, r.end+1):
      region_number = region_dict[i]
      if not working_region or region_number !=working_region.region_number:
        working_region = group_args(
          group_args_type = 'working region',
          region_number = region_number,
          start = i,
          end = i,
          )
        working_regions.append(working_region)
      else:
        working_region.end = i
    for previous_region,working_region,next_region in zip(
      [None]+working_regions[:-1], working_regions, working_regions[1:]+[None]):
      if (working_region.end - working_region.start + 1) < \
           minimum_domain_length:
        if previous_region:
          working_region.region_number = previous_region.region_number
        elif next_region:
          working_region.region_number = next_region.region_number
        else:  # skip as nothing to do
          pass
    # And update dictionary
    for working_region in working_regions:
      for i in range(working_region.start, working_region.end+1):
        region_dict[i] = working_region.region_number
  # And use dictionary to update regions_list

  new_regions_list = regions_list.deep_copy()
  i = -1
  for at in m.get_hierarchy().atoms():
    i += 1
    resseq_int = at.parent().parent().resseq_as_int()
    new_regions_list[i] = region_dict[resseq_int]

  return new_regions_list

def merge_very_close_regions(sites_cart, regions_list, close_distance,
     minimum_domain_length= None,
     maximum_fraction_close = None,
     log = sys.stdout):
  unique_values = get_unique_values(regions_list)
  n = len(unique_values)
  close_to_other_info  = get_close_to_other_list(sites_cart, regions_list,
     close_distance)
  if close_to_other_info.n_close_list:  # there are close pairs
    # Get best pair
    n_close_list = sorted(close_to_other_info.n_close_list,
      key = lambda c: c.n_close, reverse = True)
    best_pair = n_close_list[0]
    n_close = best_pair.n_close
    n_possible = best_pair.n_possible
    if n_possible >=  minimum_domain_length and \
        n_close >= n_possible * maximum_fraction_close and \
        best_pair.i is not None:
      best_i = best_pair.i
      best_j = best_pair.j
      print("Merging groups with %s sets of close residues" %(
        best_pair.n_close), file = log)

      sel = (regions_list == best_j)
      regions_list.set_selected(sel, best_i)
      update_regions_list(regions_list)

  return regions_list
def merge_closest_regions(sites_cart, regions_list, close_distance,
     log = sys.stdout):
  unique_values = get_unique_values(regions_list)
  n = len(unique_values)
  close_to_other_info  = get_close_to_other_list(sites_cart, regions_list,
     close_distance)
  if close_to_other_info.n_close_list:  # there are close pairs
    # Get best pair
    n_close_list = sorted(close_to_other_info.n_close_list,
      key = lambda c: c.n_close, reverse = True)
    best_pair = n_close_list[0]
    best_i = best_pair.i
    best_j = best_pair.j
    if best_i is not None:
      print("Merging groups with %s sets of close residues" %(
        best_pair.n_close), file = log)
  else:  # just take the pair that comes closest
    best_i = None
    best_j = None
    best_dist = None
    for k in range(len(unique_values)):
      i = unique_values[k]
      sc1 = sites_cart.select(regions_list == i)
      for l in range(k+1, len(unique_values)):
        j = unique_values[l]
        sc2 = sites_cart.select(regions_list == j)
        dist, i1, i2 = sc1.min_distance_between_any_pair_with_id(sc2)
        if dist and (best_dist is None or (dist < best_dist)):
          best_i = i
          best_j = j
          best_dist = dist
    if best_i is not None:
      print("Merging groups with distance of %.2f A" %(best_dist), file = log)

  if best_i is not None:
    sel = (regions_list == best_j)
    regions_list.set_selected(sel, best_i)
    update_regions_list(regions_list)

  return regions_list

def get_unique_values(regions_list):
  unique_values = []
  for x in regions_list:
    if not x in unique_values:
      unique_values.append(x)
  return unique_values

def swap_close_regions(sites_cart, regions_list, minimum_domain_length,
    close_distance = None):

  # Count number of residues in each pair that are close to the other
  # Split a group if some residues are close to other and not to self

  close_to_other_info  = get_close_to_other_list(sites_cart, regions_list,
     close_distance)
  close_to_other_list = close_to_other_info.close_to_other_list

  closer_to_other_swaps = get_closer_to_other(close_to_other_list,
      minimum_domain_length)
  found_something = False
  # Apply close swaps
  for s in closer_to_other_swaps:
    c = s.k_list[0]
    for k in range(c.start, c.end+1):
      regions_list[k] = s.j
      found_something = True

  update_regions_list(regions_list)

  if found_something:
    return regions_list

def get_close_to_other_list(sites_cart, regions_list, close_distance):

  sites_dict = {}
  index_dict = {}
  id_list = get_unique_values(regions_list)

  for co_id in id_list:
    sel = (regions_list == co_id)
    sites_dict[co_id] = sites_cart.select(sel)
    index_dict[co_id] = sel.iselection()

  n_close_list = []
  typical_n_close = 0
  typical_n_close_n = 0
  close_to_other_list = []
  for i in id_list:
    for j in id_list:
      if i==j: continue
      n_close = 0 # number in i close to j
      n_possible = 0

      for k in range(sites_dict[i].size()):
         index = index_dict[i][k]
         distances = (sites_dict[j] - col(sites_dict[i][k])).norms()
         local_n_close = (distances < close_distance).count(True)
         if local_n_close > 0:
           n_close += 1
         n_possible += 1
         distances_self = (sites_dict[i] - col(sites_dict[i][k])).norms()
         self_local_n_close = (distances_self < close_distance).count(True) - 1
         if local_n_close > self_local_n_close:
           close_to_other_list.append(
             group_args(group_args_type = 'closer to other',
             excess = local_n_close - self_local_n_close,
             index = index,
             i = i,
             k = k,
             j = j))


      n_close_list.append(group_args(  # how many in i close to j
        group_args_type = 'n close',
        n_close = n_close,
        n_possible = n_possible,
        i = i,
        j = j,))
  return group_args(
    group_args_type = 'close to other and close list',
      n_close_list = n_close_list,
      close_to_other_list = close_to_other_list)

def get_closer_to_other(close_to_other_list, minimum_domain_length):
  close_dict = {}
  for c in close_to_other_list:
    i,j = c.i,c.j
    if not i in close_dict.keys():
       close_dict[i] = {}
    if not j in close_dict[i].keys():
      close_dict[i][j] = 0
    close_dict[i][j] += 1
  for i in close_dict.keys():
    for j in close_dict[i].keys():
      if close_dict[i][j] >= 0: # minimum_domain_length//2:
        pass
      else:
        del close_dict[i][j]
        if not close_dict[i]:
          del close_dict[i]
  all_k_list = []
  for i in close_dict.keys():
    for j in close_dict[i].keys():
      k_list = get_k_list(i,j,close_to_other_list)
      k_list = merge_k_list(k_list, minimum_domain_length)
      if not k_list:continue
      all_k_list.append(group_args(
        group_args_type = 'k list',
        i = i,
        j = j,
        k_list = k_list,
       ))
  return all_k_list


def merge_k_list(k_list, minimum_domain_length):
  n = len(k_list)
  for i in range(n):
    last_n = len(k_list)
    k_list = merge_k_list_once(k_list)
    if len(k_list) == last_n:
      break

  new_k_list = []
  for k1 in k_list:
    n1 = k1.end - k1.start + 1
    if n1 >= minimum_domain_length//3:
      new_k_list.append(k1)
  return new_k_list

def merge_k_list_once(k_list):
  new_k_list = []
  for k1,k2 in zip(k_list,k_list[1:]):
    n1 = k1.end - k1.start + 1
    n2 = k2.end - k2.start + 1
    n_between = k2.start - k1.end - 1
    if n_between < min(n1,n2):
      k1.end = k2.end
      k2.start  = None
      k2.end = None
      break
  for k1 in k_list:
    if k1.start is not None:
      new_k_list.append(k1)
  return new_k_list


def get_k_list(i,j,close_to_other_list):
  k_list = []
  for c in close_to_other_list:
    if c.i==i and c.j==j:
      k_list.append(c.index)
  k_list.sort()
  k_list_as_groups = get_indices_as_ranges(k_list)
  return k_list_as_groups

def replace_short_segments(regions_list, minimum_domain_length):
  id_list = get_unique_values(regions_list)
  new_regions_list = regions_list.deep_copy()
  for co_id in id_list:
    indices = (regions_list == co_id).iselection()
    indices_as_ranges = get_indices_as_ranges(indices)
    for r in indices_as_ranges:
      if r.end - r.start + 1 < minimum_domain_length:
        value = regions_list[r.start - 1] if r.start > 0 else \
           regions_list[min(regions_list.size() - 1, r.end + 1)]
        for i in range(r.start,r.end+1):
          new_regions_list[i] = value

  regions_list = new_regions_list
  update_regions_list(regions_list)
  return regions_list

def update_regions_list(regions_list):
  id_list = get_unique_values(regions_list)
  new_id_dict = {}
  i = 0
  for id_value in id_list:
    i += 1
    new_id_dict[id_value] = i
  new_id_list = list(new_id_dict.keys())

  for i in range(regions_list.size()):
    regions_list[i] = new_id_dict[regions_list[i]]

def get_indices_as_ranges(indices):
  indices = sorted(indices)
  ranges = []
  grouping = None
  for index in indices:
    if not grouping or index != grouping.end + 1: # new grouping
      grouping = group_args(
        group_args_type = 'grouping',
        start = index,
        end = index)
      ranges.append(grouping)
    else:
      grouping.end = index
  return ranges

def replace_lone_sites(regions_list):
  regions_list[0] = regions_list[1]
  regions_list[-1] = regions_list[-2]
  o0 = regions_list[:-2]
  o1 = regions_list[1:-1]
  o2 = regions_list[2:]
  # find o1 is different than 0 or 2 and 0 and 2 are the same
  same_02 = (o2 == o0)
  different_01 = (o1 != 00)
  lone = (same_02 & different_01)
  for i in lone.iselection():
    index = i + 1
    regions_list[i + 1] =  regions_list[i]
  update_regions_list(regions_list)
  return regions_list

def put_model_inside_cell(m, grid_resolution):
  # Put model inside cell
  sc = m.get_sites_cart()
  sc -= col(sc.min())
  sc += col((grid_resolution,grid_resolution,grid_resolution))  # inside box
  m.set_sites_cart(sc)
  return m

def assign_all_points(co_info, map_data, log = sys.stdout):
  # add shells around all co until everything is covered
  co = co_info.co
  id_list = []
  for i in range(1,len(co_info.sorted_by_volume)):
    id_list.append(co_info.sorted_by_volume[i][1])


  # Set starting points
  region_id_map = co.result()

  done = False
  for i in range(1,map_data.all()[0]):  # max possible
    if done: continue
    for co_id in id_list:
      if done: continue
      available = (region_id_map == 0)
      if available.count(True) == 0:
        done = True
        break

      bool_region_mask = co.expand_mask(
        id_to_expand = co_id, expand_size = i)
      new = (bool_region_mask & available)
      region_id_map.set_selected(new, co_id)

  co_info.region_id_map = region_id_map
  co_info.id_list = id_list
  return co_info

def get_best_co(map_data, min_cutoff = 0.5):
  max_value = map_data.as_1d().min_max_mean().max
  avg_value = map_data.as_1d().min_max_mean().mean

  # Find max number of clusters in range of 0.5 to 1.0 * max
  n = 100
  max_clusters = None
  cutoff= None
  for t in range(int(min_cutoff*n),n+1):
    threshold = avg_value + t * (max_value-avg_value)/n
    co, sorted_by_volume, min_b, max_b  = get_co(
      map_data, threshold = threshold, wrapping = False)
    if ((not max_clusters) or (len(sorted_by_volume) > max_clusters)) and (
        len(sorted_by_volume) > 1 ):
     max_clusters = len(sorted_by_volume)
     cutoff = threshold
  if max_clusters is None:
    return None
  print("Clusters: %s   Threshold: %.2f " %(max_clusters, cutoff))
  co, sorted_by_volume, min_b, max_b  = get_co(
      map_data, threshold = cutoff , wrapping = False)
  return group_args(
    group_args_type = 'co info',
    co = co,
    sorted_by_volume = sorted_by_volume)

################################################################################
####################   end of split_model_into_compact_units   #################
################################################################################

################################################################################
####################   Convenience functions          ##########################
################################################################################
def set_high_pae_for_missing(pae_matrix, pae_cutoff,
      residues_remaining):
   n,n = tuple(pae_matrix.shape)
   pae_1d = pae_matrix.flatten().tolist()
   skip_this_one = []
   for i in range(n):
     if i in residues_remaining:
        skip_this_one.append(False)
     else:
        skip_this_one.append(True)
   n_skipped = 0
   ii = -1
   for i in range(n):
     for j in range(n):
       ii += 1
       if skip_this_one[i] or skip_this_one[j]:
         pae_1d[ii] = pae_cutoff + 10
         n_skipped += 1
   import numpy

   matrix = numpy.empty((n, n))

   matrix.ravel()[:] = pae_1d
   return matrix


def get_residue_numbers_in_model(m_ca_or_p, remove_offset_of = None):
  residue_numbers = []
  for at in m_ca_or_p.get_hierarchy().atoms():
    resseq_int = at.parent().parent().resseq_as_int()
    if remove_offset_of is not None:
      resseq_int = resseq_int - remove_offset_of
    residue_numbers.append(resseq_int)
  return residue_numbers

def add_model(s1, s2):
  ''' add chains from s2 to existing s1'''
  s1 = s1.deep_copy()
  s1_ph = s1.get_hierarchy() # working hierarchy
  existing_chain_ids = s1_ph.chain_ids()
  for model_mm_2 in s2.get_hierarchy().models()[:1]:
    for chain in model_mm_2.chains():
      assert chain.id not in existing_chain_ids # duplicate chains in add_model
      new_chain = chain.detached_copy()
      for model_mm in s1_ph.models()[:1]:
        model_mm.append_chain(new_chain)

  s1.reset_after_changing_hierarchy()
  return s1
################################################################################
####################   END Convenience functions          ######################
################################################################################

if __name__ == "__main__":
  # run a simple version by default to demo usage
  args = sys.argv[1:]

  master_phil = iotbx.phil.parse(master_phil_str)
  params = master_phil.extract()
  master_phil.format(python_object=params).show(out=sys.stdout)
  p = params.process_predicted_model

  if len(args) < 2:
    print("libtbx.python process_predicted_model.py input.pdb output.pdb")
  else:
    input_file_name = args[0]
    output_file_name = args[1]
    if len(args) > 2:
       p.b_value_field_is = args[2]
    else:
       p.b_value_field_is = 'plddt'
    if len(args) > 3:
       p.domain_size = float(args[3])
    else:
       p.domain_size = 15
    from iotbx.data_manager import DataManager
    dm = DataManager()
    dm.set_overwrite(True)
    m = dm.get_model(input_file_name)

    p.remove_low_confidence_residues = True
    p.maximum_rmsd = 1.5
    p.split_model_by_compact_regions = True

    print("\nProcessing and splitting model into domains")
    model_info = process_predicted_model(m,  params)

    chainid_list = model_info.chainid_list
    print("Segments found: %s" %(" ".join(chainid_list)))

    mmm = model_info.model.as_map_model_manager()
    mmm.write_model(output_file_name)
    for chainid in chainid_list:
      selection_string = "chain %s" %(chainid)
      ph = model_info.model.get_hierarchy()
      asc1 = ph.atom_selection_cache()
      sel = asc1.selection(selection_string)
      m1 = model_info.model.select(sel)
      dm.write_model_file(m1, '%s_%s.pdb' %(output_file_name[:-4],chainid))


 *******************************************************************************


 *******************************************************************************
mmtbx/proq2.py
from __future__ import absolute_import, division, print_function

from HTMLParser import HTMLParser


class ProQError(Exception):
  """
  Module exception
  """


class JobFolderParseError(ProQError):
  """
  Could not parse out job folder
  """


class JobFolderParser(HTMLParser):

  JOB_FOLDER_TEXT = "The job folder is located here:"

  def __init__(self):

    HTMLParser.__init__( self )

    self.job_folder = None
    self.capture_next_anchor = False


  def handle_starttag(self, tag, attrs):

    if tag == "a" and self.capture_next_anchor:
      self.job_folder = None

      for ( key, value ) in attrs:
        if key == "href":
          self.job_folder = value

      self.capture_next_anchor = False


  def handle_data(self, data):

    if data.find( self.JOB_FOLDER_TEXT ) != -1:
      self.capture_next_anchor = True


def parse_submission_page(stream):

  parser = JobFolderParser()

  for line in stream:
    parser.feed( line )

    if parser.job_folder is not None:
      return parser.job_folder

  raise JobFolderParseError("Could not find job folder text")


class Job(object):
  """
  Interface to ProQ2 server
  """

  SERVERURL = "http://duffman.it.liu.se/ProQ2/index.php"
  OUTPUTFILE = "input.pdb.orig.B"

  def __init__(self, pdbstr, name):

    from iotbx.pdb import download
    from six.moves import urllib

    data_for = {
      "pdb": pdbstr,
      "bfactorPDB": "Yes",
      "name": name,
      "nomail": "Yes",
      "do": "Submit",
      }

    stream = download.openurl(
      url = self.SERVERURL,
      data = urllib.parse.urlencode( list(data_for.items()) ),
      )
    self.job_folder = parse_submission_page( stream = stream )
    stream.close()


  def __call__(self):

    from iotbx.pdb import download
    return download.openurl(
      url = "%s/%s" % ( self.job_folder, self.OUTPUTFILE ),
      )


if __name__ == "__main__":
  import sys

  if len( sys.argv ) == 2:
    timeout = 600

  elif len( sys.argv ) == 3:
    try:
      timeout = float( sys.argv[2] )

    except ValueError as e:
      print("Cannot interpret as number: %s (%s)" % ( sys.argv[2], e ))

  else:
    print("Usage: %s PDBFILE <timeout = 600>" % sys.argv[0])
    sys.exit( 1 )

  pdbstr = open( sys.argv[1] ).read()
  sys.stdout.write( "Submitting job..." )
  sys.stdout.flush()

  try:
    myjob = Job( pdbstr = pdbstr, name = "phenix.proq2" )

  except JobFolderParseError as e:
    sys.stdout.write( "failed\n" )
    print("Unexpected response: cannot find job folder")
    sys.exit( 1 )

  sys.stdout.write( "done\n" )
  print("Job folder:", myjob.job_folder)

  sys.stdout.write( "Waiting for results" )
  sys.stdout.flush()

  from libtbx import progress
  from iotbx.pdb import download
  waiter = progress.complete_on_success( func = myjob, excspec = download.NotFound )

  try:
    progress.wait(
      condition = waiter,
      waittime = 2,
      timeout = timeout,
      callback = progress.streamprint( stream = sys.stdout, character = "." ),
      )

  except progress.TimeoutError as e:
    sys.stdout.write( "%s\n" % e )
    sys.exit( 1 )

  assert hasattr( waiter, "result" )
  result = waiter.result.read()
  waiter.result.close()

  sys.stdout.write( "done\n" )

  import os.path
  output = "proq2-%s" % os.path.basename( sys.argv[1] )
  print("Output:", output)

  with open( output, "w" ) as ofile:
    ofile.write( result )
    ofile.write( "\n" )

  print("Done!")


 *******************************************************************************


 *******************************************************************************
mmtbx/real_space_correlation.py

from __future__ import absolute_import, division, print_function
import mmtbx.utils
from iotbx import reflection_file_reader
from iotbx import reflection_file_utils
from iotbx.file_reader import any_file
import iotbx.phil
import iotbx.pdb
from cctbx.array_family import flex
from cctbx import miller
from cctbx import maptbx
from libtbx.utils import Sorry, null_out
from libtbx import group_args
from mmtbx.command_line.map_comparison import get_mtz_labels, get_d_min,\
  get_crystal_symmetry
from cctbx.sgtbx import space_group_info
import os
import sys
from six.moves import range
from iotbx import extract_xtal_data

core_params_str = """\
atom_radius = None
  .type = float
  .help = Atomic radius for map CC calculation. Determined automatically if \
          if None is given
  .expert_level = 2
hydrogen_atom_radius = None
  .type = float
  .help = Atomic radius for map CC calculation for H or D.
  .expert_level = 2
resolution_factor = 1./4
  .type = float
use_hydrogens = None
  .type = bool
"""

map_files_params_str = """\
map_file_name = None
  .type = path
  .help = A CCP4-formatted map
  .style = file_type:ccp4_map input_file
d_min = None
  .type = float
  .short_caption = Resolution
  .help = Resolution of map
map_coefficients_file_name = None
  .type = path
  .help = MTZ file containing map
  .style = file_type:ccp4_map input_file process_hkl child:map_labels:map_coefficients_label
map_coefficients_label = None
  .type = str
  .short_caption = Data label
  .help = Data label for complex map coefficients in MTZ file
  .style = renderer:draw_map_arrays_widget
"""

master_params_str = """\
%s
scattering_table = *n_gaussian wk1995 it1992 neutron
  .type = choice
  .help = Scattering table for structure factors calculations
detail = atom residue *automatic
  .type = choice(multi=False)
  .help = Level of details to show CC for
map_1
  .help = First map to use in map CC calculation
{
 type = Fc
   .type = str
   .help = Electron density map type. Example xmFobs-yDFcalc (for \
           maximum-likelihood weighted map) or xFobs-yFcalc (for simple \
           unweighted map), x and y are any real numbers.
 fill_missing_reflections = False
   .type = bool
 isotropize = False
   .type = bool
}
map_2
  .help = Second map to use in map CC calculation
{
 type = 2mFo-DFc
   .type = str
   .help = Electron density map type. Example xmFobs-yDFcalc (for \
           maximum-likelihood weighted map) or xFobs-yFcalc (for simple \
           unweighted map), x and y are any real numbers.
 fill_missing_reflections = True
   .type = bool
 isotropize = True
   .type = bool
}
pdb_file_name = None
  .type = str
  .help = PDB/mmCIF file name.
reflection_file_name = None
  .type = str
  .help = File with experimental data (most of formats: CNS, SHELX, MTZ, etc).
data_labels = None
  .type = str
  .help = Labels for experimental data.
high_resolution = None
  .type=float
low_resolution = None
  .type=float
%s
"""%(core_params_str, map_files_params_str)

def master_params():
  return iotbx.phil.parse(master_params_str, process_includes=False)

def pdb_to_xrs(pdb_file_name, scattering_table):
  pdb_inp = iotbx.pdb.input(file_name = pdb_file_name)
  xray_structure = pdb_inp.xray_structure_simple()
  pdb_hierarchy = pdb_inp.construct_hierarchy()
  pdb_hierarchy.atoms().reset_i_seq() # VERY important to do.
  mmtbx.utils.setup_scattering_dictionaries(
    scattering_table = scattering_table,
    xray_structure = xray_structure,
    d_min = None)
  return group_args(
    xray_structure = xray_structure,
    pdb_hierarchy  = pdb_hierarchy)

def extract_data_and_flags(params, crystal_symmetry=None):
  data_and_flags = None
  if(params.reflection_file_name is not None):
    reflection_file = reflection_file_reader.any_reflection_file(
      file_name = params.reflection_file_name)
    reflection_file_server = reflection_file_utils.reflection_file_server(
      crystal_symmetry = crystal_symmetry,
      force_symmetry   = True,
      reflection_files = [reflection_file])
    parameters = extract_xtal_data.data_and_flags_master_params().extract()
    parameters.force_anomalous_flag_to_be_equal_to = False
    if(params.data_labels is not None):
      parameters.labels = [params.data_labels]
    if(params.high_resolution is not None):
      parameters.high_resolution = params.high_resolution
    if(params.low_resolution is not None):
      parameters.low_resolution = params.low_resolution
    data_and_flags = extract_xtal_data.run(
      reflection_file_server = reflection_file_server,
      parameters             = parameters,
      extract_r_free_flags   = False) #XXX
  return data_and_flags

def compute_map_from_model(high_resolution, low_resolution, xray_structure,
                           grid_resolution_factor=None,
                           crystal_gridding = None):
  f_calc = xray_structure.structure_factors(d_min = high_resolution).f_calc()
  if (low_resolution is not None):
    f_calc = f_calc.resolution_filter(d_max = low_resolution)
  if (crystal_gridding is None):
    return f_calc.fft_map(
      resolution_factor = min(0.5,grid_resolution_factor),
      symmetry_flags    = None)
  return miller.fft_map(
    crystal_gridding     = crystal_gridding,
    fourier_coefficients = f_calc)

def extract_input_pdb(pdb_file, params):
  fn1, fn2 = None,None
  if(pdb_file is not None and (
      iotbx.pdb.is_pdb_file(pdb_file.file_name) or
      iotbx.pdb.is_pdb_mmcif_file(pdb_file.file_name))):
    fn1 = pdb_file.file_name
  if(params.pdb_file_name is not None and (
      iotbx.pdb.is_pdb_file(params.pdb_file_name) or
      iotbx.pdb.is_pdb_mmcif_file(params.pdb_file_name))):
    fn2 = params.pdb_file_name
  if([fn1, fn2].count(None)!=1):
    raise Sorry("PDB/mmCIF file must be provided.")
  result = None
  if(fn1 is not None): result = fn1
  else: result = fn2
  params.pdb_file_name = result

def extract_input_data(hkl_file, params):
  fn1, fn2 = None, None
  if (hkl_file is not None):
    miller_arrays = hkl_file.file_object.as_miller_arrays()
    for miller_array in miller_arrays:
      if (miller_array.is_xray_data_array()):
        fn1 = hkl_file.file_name
      elif (miller_array.is_complex_array()):
        fn2 = hkl_file.file_name

  # check if defined reflection file exists
  if (params.reflection_file_name is not None):
    if (os.path.isfile(params.reflection_file_name) == False):
      raise Sorry('%s cannot be found.' % params.reflection_file_name)
  elif (fn1 is not None):
    # set reflections_file_name only if no map files are defined (default)
    if ( (params.map_file_name is None) and
         (params.map_coefficients_file_name is None) and
         (params.map_coefficients_label is None) ):
      params.reflection_file_name = fn1

  # check if map coefficients exist
  if (params.map_coefficients_file_name is not None):
    if (os.path.isfile(params.map_coefficients_file_name) == False):
      raise Sorry('%s cannot be found.' % params.map_coefficients_file_name)
  elif (fn2 is not None):
    # set map_coefficients file_name only if no other map file is defined
    if ( (params.reflection_file_name is None) and
         (params.map_file_name is None) ):
      params.map_coefficients_file_name = fn2
  if ( (params.map_coefficients_file_name is not None) and
       (params.map_coefficients_label is None) ):
    raise Sorry('Please specify map coefficient labels for %s.' %
                params.map_coefficients_file_name)

def check_map_file(map_file, params):
  if ( (params.map_coefficients_file_name is not None) and
       (params.map_file_name is not None) ):
    raise Sorry('Please use map coefficients or the map, not both.')
  elif (params.map_coefficients_file_name is not None):
    file_handle = any_file(params.map_coefficients_file_name)
    if (file_handle.file_type != 'hkl'):
      raise Sorry('%s is not in MTZ format.' % file_handle.file_name)
    labels = get_mtz_labels(file_handle)
    if (params.map_coefficients_label is None):
      raise Sorry('Data labels for map coefficients are not specified for %s' %
                  params.map_coefficients_file_name)
    elif (params.map_coefficients_label not in labels):
      raise Sorry('%s labels for map_coefficients were not found in %s' %
                  (params.map_coefficients_label,
                   params.map_coefficients_file_name))
  elif (params.map_file_name is not None):
    file_handle = any_file(params.map_file_name)
    if (file_handle.file_type != 'ccp4_map'):
      raise Sorry('%s is not a CCP4-formatted map file.' %
                  file_handle.file_name)
  elif (map_file is not None):
    params.map_file_name = map_file.file_name

def broadcast(m, log):
  print("-"*79, file=log)
  print(m, file=log)
  print("*"*len(m), file=log)

def cmd_run(args, command_name, log=None):
  if(log is None): log = sys.stdout
  args = list(args)
  msg = """\

Compute map correlation coefficient given input PDB model and reflection data.

Examples:

  phenix.real_space_correlation m.pdb d.mtz
  phenix.real_space_correlation m.pdb d.mtz detail=atom
  phenix.real_space_correlation m.pdb d.mtz detail=residue
  phenix.real_space_correlation m.pdb d.mtz data_labels=FOBS
  phenix.real_space_correlation m.pdb d.mtz scattering_table=neutron
  phenix.real_space_correlation m.pdb d.mtz detail=atom use_hydrogens=true
  phenix.real_space_correlation m.pdb d.mtz map_1.type=Fc map_2.type="2mFo-DFc"

  phenix.real_space_correlation m.pdb d.mtz map_coefficients_label="2FOFCWT,PH2FOFCWT"
  phenix.real_space_correlation m.pdb d.ccp4
"""
  if(len(args) == 0) or (args == ["--help"]) or (args == ["--options"]):
    print(msg, file=log)
    broadcast(m="Default parameters:", log = log)
    master_params().show(out = log, prefix="  ")
    return
  else :
    pdb_file = None
    reflection_file = None
    map_file = None
    phil_objects = []
    n_files = 0
    for arg in args :
      if(os.path.isfile(arg)):
        inp = any_file(arg)
        if(  inp.file_type == "phil"): phil_objects.append(inp.file_object)
        elif(inp.file_type == "pdb"):  pdb_file = inp
        elif(inp.file_type == "hkl"):  reflection_file = inp
        elif(inp.file_type == "ccp4_map"): map_file = inp
        else:
          raise Sorry(("Don't know how to deal with the file %s - unrecognized "+
            "format '%s'.  Please verify that the syntax is correct.") % (arg,
              str(inp.file_type)))
        n_files += 1
      else:
        try:
          phil_objects.append(iotbx.phil.parse(arg))
        except RuntimeError as e:
          raise Sorry("Unrecognized parameter or command-line argument '%s'." %
            arg)
    if (n_files > 2):
      raise Sorry('Only 2 files are needed, a structure and the data')
    working_phil, unused = master_params().fetch(sources=phil_objects,
      track_unused_definitions=True)
    if(len(unused)>0):
      for u in unused:
        print(str(u))
      raise Sorry("Unused parameters: see above.")
    params = working_phil.extract()

    # PDB file
    extract_input_pdb(pdb_file=pdb_file, params=params)
    broadcast(m="Input PDB file name: %s"%params.pdb_file_name, log=log)
    pdbo = pdb_to_xrs(pdb_file_name=params.pdb_file_name,
      scattering_table=params.scattering_table)
    pdbo.xray_structure.show_summary(f=log, prefix="  ")

    # data file
    # set params.reflection_file_name and params.map_coefficients_file_name
    extract_input_data(hkl_file=reflection_file, params=params)
    data_and_flags = None
    if (params.reflection_file_name is not None):
      broadcast(
        m="Input reflection file name: %s"%params.reflection_file_name, log=log)
      data_and_flags = extract_data_and_flags(params = params)
      data_and_flags.f_obs.show_comprehensive_summary(f=log, prefix="  ")

    # map file (if available)
    # set params.map_file_name
    check_map_file(map_file, params)
    map_name = ( (params.map_coefficients_file_name) or
                 (params.map_file_name) )
    if (map_name is not None):
      map_handle = any_file(map_name)
      broadcast(m='Input map file name: %s' % map_name, log=log)
      print('  Map type: ', end=' ', file=log)
      if (map_handle.file_type == 'hkl'):
        print('map coefficients', file=log)
        print('  Map labels:', params.map_coefficients_label, file=log)
      else:
        print('CCP4-format', file=log)

      # check crystal symmetry
      cs1 = pdbo.xray_structure.crystal_symmetry()
      cs2 = get_crystal_symmetry(map_handle)
      if (cs1.is_similar_symmetry(cs2) is False):
        raise Sorry('The symmetry of the two files, %s and %s, is not similar' %
                    (fobs_handle.file_name, map_handle.file_name))

    # check that only one data file is defined
    if ( (map_name is not None) and
         (params.reflection_file_name is not None) ):
      raise Sorry('Please use F_obs or a map, not both.')
    if ( (params.reflection_file_name is None) and
         (map_name is None) ):
      raise Sorry('A data file is required.')

    # create fmodel with f_obs (if available)
    fmodel = None
    if (params.reflection_file_name is not None):
      r_free_flags = data_and_flags.f_obs.array(
        data = flex.bool(data_and_flags.f_obs.size(), False))
      fmodel = mmtbx.utils.fmodel_simple(
        xray_structures     = [pdbo.xray_structure],
        scattering_table    = params.scattering_table,
        f_obs               = data_and_flags.f_obs,
        r_free_flags        = r_free_flags)
      broadcast(m="R-factors, reflection counts and scales", log=log)
      fmodel.show(log=log, show_header=False)

    # compute cc
    results = simple(
      fmodel        = fmodel,
      pdb_hierarchy = pdbo.pdb_hierarchy,
      params        = params,
      show_results  = True,
      log           = log)

def simple(fmodel, pdb_hierarchy, params=None, log=None, show_results=False):
  if(params is None): params = master_params().extract()
  if(log is None): log = sys.stdout

  crystal_gridding = None
  unit_cell = None
  d_min = 1.0
  map_1 = None
  map_2 = None

  # compute map_1 and map_2 if given F_obs (fmodel exists)
  if ( (params.map_file_name is None) and
       (params.map_coefficients_file_name is None) and
       (fmodel is not None) ):
    e_map_obj = fmodel.electron_density_map()
    coeffs_2 = e_map_obj.map_coefficients(
      map_type     = params.map_2.type,
      fill_missing = params.map_2.fill_missing_reflections,
      isotropize   = params.map_2.isotropize)
    fft_map_2 = coeffs_2.fft_map(resolution_factor = params.resolution_factor)
    crystal_gridding = fft_map_2
    fft_map_2.apply_sigma_scaling()
    map_2 = fft_map_2.real_map_unpadded()

    coeffs_1 = e_map_obj.map_coefficients(
      map_type     = params.map_1.type,
      fill_missing = params.map_1.fill_missing_reflections,
      isotropize   = params.map_1.isotropize)
    fft_map_1 = miller.fft_map(crystal_gridding = crystal_gridding,
                               fourier_coefficients = coeffs_1)
    fft_map_1.apply_sigma_scaling()
    map_1 = fft_map_1.real_map_unpadded()

    unit_cell = fmodel.xray_structure.unit_cell()
    d_min = fmodel.f_obs().d_min()

  # or read map coefficents
  elif (params.map_coefficients_file_name is not None):
    map_handle = any_file(params.map_coefficients_file_name)
    crystal_symmetry = get_crystal_symmetry(map_handle)
    unit_cell = crystal_symmetry.unit_cell()
    d_min = get_d_min(map_handle)
    crystal_gridding = maptbx.crystal_gridding(
      crystal_symmetry.unit_cell(), d_min=d_min,
      resolution_factor=params.resolution_factor,
      space_group_info=crystal_symmetry.space_group_info())
    coeffs_2 = map_handle.file_server.get_miller_array(
      params.map_coefficients_label)
    fft_map_2 = miller.fft_map(crystal_gridding=crystal_gridding,
                               fourier_coefficients=coeffs_2)
    fft_map_2.apply_sigma_scaling()
    map_2 = fft_map_2.real_map_unpadded()

  # or read CCP4 map
  else:
    map_handle = any_file(params.map_file_name)
    unit_cell = map_handle.file_object.unit_cell()
    sg_info = space_group_info(map_handle.file_object.space_group_number)
    n_real = map_handle.file_object.unit_cell_grid
    crystal_gridding = maptbx.crystal_gridding(
      unit_cell, space_group_info=sg_info, pre_determined_n_real=n_real)
    map_2 = map_handle.file_object.map_data()

    # check for origin shift
    # modified from phenix.command_line.real_space_refine
    # plan to centralize functionality in another location
    # -------------------------------------------------------------------------
    shift_manager = mmtbx.utils.shift_origin(
      map_data=map_2, pdb_hierarchy=pdb_hierarchy,
      crystal_symmetry=map_handle.crystal_symmetry())
    if (shift_manager.shift_cart is not None):
      print("Map origin is not at (0,0,0): shifting the map and model.", file=log)
    pdb_hierarchy = shift_manager.pdb_hierarchy
    map_2 = shift_manager.map_data
    # -------------------------------------------------------------------------

  # compute map_1 (Fc) if given a map (fmodel does not exist)
  if (map_1 is None):
    xray_structure = pdb_hierarchy.extract_xray_structure(
      crystal_symmetry=crystal_gridding.crystal_symmetry())
    fft_map_1 = compute_map_from_model(d_min, None, xray_structure,
                                       crystal_gridding=crystal_gridding)
    fft_map_1.apply_sigma_scaling()
    map_1 = fft_map_1.real_map_unpadded()

  # compute cc
  assert ( (map_1 is not None) and (map_2 is not None) )
  broadcast(m="Map correlation and map values", log=log)
  overall_cc = flex.linear_correlation(x = map_1.as_1d(),
    y = map_2.as_1d()).coefficient()
  print("  Overall map cc(%s,%s): %6.4f"%(params.map_1.type,
    params.map_2.type, overall_cc), file=log)
  detail, atom_radius = params.detail, params.atom_radius
  detail, atom_radius = set_detail_level_and_radius(
    detail=detail, atom_radius=atom_radius, d_min=d_min)
  use_hydrogens = params.use_hydrogens
  if(use_hydrogens is None):
    if(params.scattering_table == "neutron" or d_min <= 1.2):
      use_hydrogens = True
    else:
      use_hydrogens = False
  hydrogen_atom_radius = params.hydrogen_atom_radius
  if(hydrogen_atom_radius is None):
    if(params.scattering_table == "neutron"):
      hydrogen_atom_radius = atom_radius
    else:
      hydrogen_atom_radius = 1
  results = compute(
    pdb_hierarchy        = pdb_hierarchy,
    unit_cell            = unit_cell,
    fft_n_real           = map_1.focus(),
    fft_m_real           = map_1.all(),
    map_1                = map_1,
    map_2                = map_2,
    detail               = detail,
    atom_radius          = atom_radius,
    use_hydrogens        = use_hydrogens,
    hydrogen_atom_radius = hydrogen_atom_radius)
  if(show_results):
    show(log=log, results=results, params=params, detail=detail)
  return overall_cc, results

def show(log, results, detail, params=None, map_1_name=None, map_2_name=None):
  assert params is not None or [map_1_name,map_2_name].count(None)==0
  if([map_1_name,map_2_name].count(None)==2):
    map_1_name,map_2_name = params.map_1.type, params.map_2.type
  print(file=log)
  print("Rho1 = %s, Rho2 = %s"%(map_1_name, map_2_name), file=log)
  print(file=log)
  if(detail == "atom"):
    print(" <----id string---->  occ     ADP      CC   Rho1   Rho2", file=log)
  else:
    print("  <id string>    occ     ADP      CC   Rho1   Rho2", file=log)
  fmt = "%s %4.2f %7.2f %7.4f %6.2f %6.2f"
  for r in results:
    print(fmt%(r.id_str, r.occupancy, r.b, r.cc, r.map_value_1,
      r.map_value_2), file=log)

def compute(pdb_hierarchy,
            unit_cell,
            fft_n_real,
            fft_m_real,
            map_1,
            map_2,
            detail,
            atom_radius,
            use_hydrogens,
            hydrogen_atom_radius):
  assert detail in ["atom", "residue"]
  results = []
  for chain in pdb_hierarchy.chains():
    for residue_group in chain.residue_groups():
      for conformer in residue_group.conformers():
        for residue in conformer.residues():
          r_id_str = "%2s %1s %3s %4s %1s"%(chain.id, conformer.altloc,
            residue.resname, residue.resseq, residue.icode)
          r_sites_cart = flex.vec3_double()
          r_b          = flex.double()
          r_occ        = flex.double()
          r_mv1        = flex.double()
          r_mv2        = flex.double()
          r_rad        = flex.double()
          for atom in residue.atoms():
            a_id_str = "%s %4s"%(r_id_str, atom.name)
            if(atom.element_is_hydrogen()): rad = hydrogen_atom_radius
            else: rad = atom_radius
            if(not (atom.element_is_hydrogen() and not use_hydrogens)):
              map_value_1 = map_1.eight_point_interpolation(
                unit_cell.fractionalize(atom.xyz))
              map_value_2 = map_2.eight_point_interpolation(
                unit_cell.fractionalize(atom.xyz))
              r_sites_cart.append(atom.xyz)
              r_b         .append(atom.b)
              r_occ       .append(atom.occ)
              r_mv1       .append(map_value_1)
              r_mv2       .append(map_value_2)
              r_rad       .append(rad)
              if(detail == "atom"):
                sel = maptbx.grid_indices_around_sites(
                  unit_cell  = unit_cell,
                  fft_n_real = fft_n_real,
                  fft_m_real = fft_m_real,
                  sites_cart = flex.vec3_double([atom.xyz]),
                  site_radii = flex.double([rad]))
                cc = flex.linear_correlation(x=map_1.select(sel),
                  y=map_2.select(sel)).coefficient()
                result = group_args(
                  chain_id    = chain.id,
                  atom        = atom,
                  id_str      = a_id_str,
                  cc          = cc,
                  map_value_1 = map_value_1,
                  map_value_2 = map_value_2,
                  b           = atom.b,
                  occupancy   = atom.occ,
                  n_atoms     = 1)
                results.append(result)
          if(detail == "residue") and (len(r_mv1) > 0):
            sel = maptbx.grid_indices_around_sites(
              unit_cell  = unit_cell,
              fft_n_real = fft_n_real,
              fft_m_real = fft_m_real,
              sites_cart = r_sites_cart,
              site_radii = r_rad)
            cc = flex.linear_correlation(x=map_1.select(sel),
              y=map_2.select(sel)).coefficient()
            result = group_args(
              residue     = residue,
              chain_id    = chain.id,
              id_str      = r_id_str,
              cc          = cc,
              map_value_1 = flex.mean(r_mv1),
              map_value_2 = flex.mean(r_mv2),
              b           = flex.mean(r_b),
              occupancy   = flex.mean(r_occ),
              n_atoms     = r_sites_cart.size())
            results.append(result)
  return results

def set_detail_level_and_radius(detail, atom_radius, d_min):
  assert detail in ["atom","residue","automatic"]
  if(detail == "automatic"):
    if(d_min < 2.0): detail = "atom"
    else:            detail = "residue"
  if(atom_radius is None):
    if(d_min < 1.0):                    atom_radius = 1.0
    elif(d_min >= 1.0 and d_min<2.0):   atom_radius = 1.5
    elif(d_min >= 2.0 and d_min < 4.0): atom_radius = 2.0
    else:                               atom_radius = 2.5
  return detail, atom_radius

class selection_map_statistics_manager(object):
  """
  Utility class for performing repeated calculations on multiple maps.  Useful
  in post-refinement validation, ligand fitting, etc. where we want to collect
  both CC and values for 2mFo-DFc and mFo-DFc maps.
  """
  __slots__ = ["fft_m_real", "fft_n_real", "atom_selection", "sites",
               "sites_frac", "atom_radii", "map_sel"]

  def __init__(self,
      atom_selection,
      xray_structure,
      fft_m_real,
      fft_n_real,
      atom_radius=1.5,
      exclude_hydrogens=False):
    self.fft_m_real = fft_m_real
    self.fft_n_real = fft_n_real
    if (isinstance(atom_selection, flex.bool)):
      atom_selection = atom_selection.iselection()
    assert (len(atom_selection) == 1) or (not atom_selection.all_eq(0))
    if (exclude_hydrogens):
      not_hd_selection = (~(xray_structure.hd_selection())).iselection()
      atom_selection = atom_selection.intersection(not_hd_selection)
    assert (len(atom_selection) != 0)
    self.atom_selection = atom_selection
    self.sites = xray_structure.sites_cart().select(atom_selection)
    self.sites_frac = xray_structure.sites_frac().select(atom_selection)
    scatterers = xray_structure.scatterers().select(atom_selection)
    self.atom_radii = flex.double(self.sites.size(), atom_radius)
    for i_seq, sc in enumerate(scatterers):
      if (sc.element_symbol().strip().lower() in ["h","d"]):
        assert (not exclude_hydrogens)
        self.atom_radii[i_seq] = 1.0
    self.map_sel = maptbx.grid_indices_around_sites(
      unit_cell  = xray_structure.unit_cell(),
      fft_n_real = fft_n_real,
      fft_m_real = fft_m_real,
      sites_cart = self.sites,
      site_radii = self.atom_radii)

  def analyze_map(self, map, model_map=None, min=None, max=None,
      compare_at_sites_only=False):
    """
    Extract statistics for the given map, plus statistics for the model map
    if given.  The CC can either be calculated across grid points within the
    given radius of the sites, or at the sites directly.
    """
    assert (map.focus() == self.fft_n_real) and (map.all() == self.fft_m_real)
    map_sel = map.select(self.map_sel)
    map_values = flex.double()
    model_map_sel = model_map_mean = model_map_values = None
    if (model_map is not None):
      assert ((model_map.focus() == self.fft_n_real) and
              (model_map.all() == self.fft_m_real))
      model_map_sel = model_map.select(self.map_sel)
      model_map_values = flex.double()
    for site_frac in self.sites_frac:
      map_values.append(map.eight_point_interpolation(site_frac))
      if (model_map is not None):
        model_map_values.append(model_map.eight_point_interpolation(site_frac))
    cc = None
    if (model_map is not None):
      if (compare_at_sites_only):
        cc = flex.linear_correlation(x=map_values,
          y=model_map_values).coefficient()
      else :
        cc = flex.linear_correlation(x=map_sel, y=model_map_sel).coefficient()
      model_map_mean = flex.mean(model_map_values)
    n_above_max = n_below_min = None
    if (min is not None):
      n_below_min = (map_values < min).count(True)
    if (max is not None):
      n_above_max = (map_values > max).count(True)
    return group_args(
      cc=cc,
      min=flex.min(map_values),
      max=flex.max(map_values),
      mean=flex.mean(map_values),
      n_below_min=n_below_min,
      n_above_max=n_above_max,
      model_mean=model_map_mean)

def map_statistics_for_atom_selection(
    atom_selection,
    fmodel=None,
    resolution_factor=0.25,
    map1=None,
    map2=None,
    xray_structure=None,
    map1_type="2mFo-DFc",
    map2_type="Fmodel",
    atom_radius=1.5,
    exclude_hydrogens=False):
  """
  Simple-but-flexible function to give the model-to-map CC and mean density
  values (sigma-scaled, unless pre-calculated maps are provided) for any
  arbitrary atom selection.
  """
  assert (atom_selection is not None) and (len(atom_selection) > 0)
  if (fmodel is not None):
    assert (map1 is None) and (map2 is None) and (xray_structure is None)
    edm = fmodel.electron_density_map()
    map1_coeffs = edm.map_coefficients(map1_type)
    map1 = map1_coeffs.fft_map(
      resolution_factor=resolution_factor).apply_sigma_scaling().real_map()
    map2_coeffs = edm.map_coefficients(map2_type)
    map2 = map2_coeffs.fft_map(
      resolution_factor=resolution_factor).apply_sigma_scaling().real_map()
    xray_structure = fmodel.xray_structure
  else :
    assert (not None in [map1, map2, xray_structure])
    assert isinstance(map1, flex.double) and isinstance(map2, flex.double)
  if (exclude_hydrogens):
    hd_selection = xray_structure.hd_selection()
    if (type(atom_selection).__name__ == "size_t"):
      atom_selection_new = flex.size_t()
      for i_seq in atom_selection :
        if (not hd_selection[i_seq]):
          atom_selection_new.append(i_seq)
      atom_selection = atom_selection_new
      assert (len(atom_selection) > 0)
    else :
      assert (type(atom_selection).__name__ == "bool")
      atom_selection &= ~hd_selection
  manager = selection_map_statistics_manager(
    atom_selection=atom_selection,
    xray_structure=xray_structure,
    fft_n_real = map1.focus(),
    fft_m_real = map1.all(),
    exclude_hydrogens=exclude_hydrogens)
  stats = manager.analyze_map(
    map=map1,
    model_map=map2)
  return group_args(
    cc=stats.cc,
    map1_mean=stats.mean,
    map2_mean=stats.model_mean)

def map_statistics_for_fragment(fragment, **kwds):
  """
  Shortcut to map_statistics_for_atom_selection using a PDB hierarchy object
  to define the atom selection.
  """
  atoms = fragment.atoms()
  i_seqs = atoms.extract_i_seq()
  assert (not i_seqs.all_eq(0))
  return map_statistics_for_atom_selection(i_seqs, **kwds)

def find_suspicious_residues(
    fmodel,
    pdb_hierarchy,
    hetatms_only=True,
    skip_single_atoms=True,
    skip_alt_confs=True,
    min_acceptable_cc=0.8,
    min_acceptable_2fofc=1.0,
    max_frac_atoms_below_min=0.5,
    ignore_resnames=(),
    log=None):
  if (log is None) : log = null_out()
  xray_structure = fmodel.xray_structure
  assert (len(pdb_hierarchy.atoms()) == xray_structure.scatterers().size())
  edm = fmodel.electron_density_map()
  map_coeffs1 = edm.map_coefficients(
    map_type="2mFo-DFc",
    fill_missing=False)
  map1 = map_coeffs1.fft_map(
    resolution_factor=0.25).apply_sigma_scaling().real_map_unpadded()
  map_coeffs2 = edm.map_coefficients(
    map_type="Fc",
    fill_missing=False)
  map2 = map_coeffs2.fft_map(
    resolution_factor=0.25).apply_sigma_scaling().real_map_unpadded()
  unit_cell = xray_structure.unit_cell()
  hd_selection = xray_structure.hd_selection()
  outliers = []
  for chain in pdb_hierarchy.models()[0].chains():
    for residue_group in chain.residue_groups():
      atom_groups = residue_group.atom_groups()
      if (len(atom_groups) > 1) and (skip_alt_confs):
        continue
      for atom_group in residue_group.atom_groups():
        if (atom_group.resname in ignore_resnames):
          continue
        atoms = atom_group.atoms()
        assert (len(atoms) > 0)
        if (len(atoms) == 1) and (skip_single_atoms):
          continue
        if (hetatms_only):
          if (not atoms[0].hetero):
            continue
        map_stats = map_statistics_for_fragment(
          fragment=atom_group,
          map1=map1,
          map2=map2,
          xray_structure=fmodel.xray_structure,
          exclude_hydrogens=True)
        n_below_min = n_heavy = sum = 0
        for atom in atoms :
          if (hd_selection[atom.i_seq]):
            continue
          n_heavy += 1
          site = atom.xyz
          site_frac = unit_cell.fractionalize(site)
          map_value = map1.tricubic_interpolation(site_frac)
          if (map_value < min_acceptable_2fofc):
            n_below_min += 1
          sum += map_value
        map_mean = sum / n_heavy
        frac_below_min = n_below_min / n_heavy
        if ((map_stats.cc < min_acceptable_cc) or
            (frac_below_min > max_frac_atoms_below_min) or
            (map_mean < min_acceptable_2fofc)):
          residue_info = "%1s%3s%2s%5s" % (atom_group.altloc,
            atom_group.resname, chain.id, residue_group.resid())
          xyz_mean = atoms.extract_xyz().mean()
          outliers.append((residue_info, xyz_mean))
          print("Suspicious residue: %s" % residue_info, file=log)
          print("  Overall CC to 2mFo-DFc map = %.2f" % map_stats.cc, file=log)
          print("  Fraction of atoms where 2mFo-DFc < %.2f = %.2f" % \
            (min_acceptable_2fofc, frac_below_min), file=log)
          print("  Mean 2mFo-DFc value = %.2f" % map_mean, file=log)
  return outliers

def extract_map_stats_for_single_atoms(xray_structure, pdb_atoms, fmodel,
                                        selection=None, fc_map=None,
                                        two_fofc_map=None):
  """
  Memory-efficient routine for harvesting map values for individual atoms
  (e.g. waters).  Only one FFT'd map at a time is in memory.

  :param selection: optional atom selection (flex array)
  :returns: group_args object with various simple metrics
  """
  if (selection is None):
    selection = ~(xray_structure.hd_selection())
  sites_cart = xray_structure.sites_cart()
  sites_frac = xray_structure.sites_frac()
  unit_cell = xray_structure.unit_cell()
  def collect_map_values(map, get_selections=False):
    values = []
    selections = []
    if (map is None):
      assert (not get_selections)
      return [ None ] * len(pdb_atoms)
    for i_seq, atom in enumerate(pdb_atoms):
      if (selection[i_seq]):
        site_frac = sites_frac[i_seq]
        values.append(map.eight_point_interpolation(site_frac))
        if (get_selections):
          sel = maptbx.grid_indices_around_sites(
            unit_cell  = unit_cell,
            fft_n_real = map.focus(),
            fft_m_real = map.all(),
            sites_cart = flex.vec3_double([sites_cart[i_seq]]),
            site_radii = flex.double([1.5]))
          selections.append(map.select(sel))
      else :
        values.append(None)
        selections.append(None)
    if (get_selections):
      return values, selections
    else :
      return values
  def get_map(map_type):
    map_coeffs = fmodel.map_coefficients(map_type=map_type)
    if (map_coeffs is not None):
      return map_coeffs.fft_map(
        resolution_factor=0.25).apply_sigma_scaling().real_map_unpadded()

  # use maps
  if ( (fmodel is None) and (fc_map is not None) and
       (two_fofc_map is not None) ):
    fofc = [ 0.0 for i in range(len(pdb_atoms)) ]
    anom = [ None for i in range(len(pdb_atoms)) ]
    two_fofc, two_fofc_sel = collect_map_values(
      two_fofc_map, get_selections=True)
    f_model_val, f_model_sel = collect_map_values(
      fc_map, get_selections=True)

  # otherwise, use data to calculate maps
  else:
    two_fofc_map = get_map("2mFo-DFc")
    two_fofc, two_fofc_sel = collect_map_values(
      two_fofc_map, get_selections=True)
    del two_fofc_map
    fofc_map = get_map("mFo-DFc")
    fofc = collect_map_values(fofc_map)
    del fofc_map
    anom_map = get_map("anomalous")
    anom = collect_map_values(anom_map)
    del anom_map
    fmodel_map = get_map("Fmodel")
    f_model_val, f_model_sel = collect_map_values(
      fmodel_map, get_selections=True)
    del fmodel_map

  two_fofc_ccs = []
  for i_seq, atom in enumerate(pdb_atoms):
    if (selection[i_seq]):
      cc = flex.linear_correlation(x=two_fofc_sel[i_seq],
        y=f_model_sel[i_seq]).coefficient()
      two_fofc_ccs.append(cc)
    else :
      two_fofc_ccs.append(None)
  return group_args(
    two_fofc_values=two_fofc,
    fofc_values=fofc,
    anom_values=anom,
    fmodel_values=f_model_val,
    two_fofc_ccs=two_fofc_ccs)


 *******************************************************************************


 *******************************************************************************
mmtbx/resolve_resources.py
from __future__ import absolute_import, division, print_function
import libtbx.phil

phenix_masks_master_params = """\
solvent_content = 0.5
  .type = float
map_type = *2mFo-DFc
  .type = choice(multi=False)
resolution_factor = 0.25
  .type = float
probability_mask = True
  .type = bool
diff_map_cutoff = 1.5
  .type = float
output_all_masks = False
  .type = bool
use_dm_map = False
  .type = bool
"""

def get_phenix_masks_master_params():
  return libtbx.phil.parse(phenix_masks_master_params, process_includes=False)


 *******************************************************************************


 *******************************************************************************
mmtbx/restraints.py

"""
Wrapper module for computing targets and gradients for restraints (or other
energy functions) on coordinates and B-factors; used in phenix.refine.
"""

from __future__ import absolute_import, division, print_function
import cctbx.adp_restraints
import math
from cctbx.array_family import flex
import scitbx.restraints
from cctbx import adptbx
from libtbx.utils import Sorry
import sys
from libtbx.str_utils import make_header

class manager(object):
  """
  Central management of restraints on molecular geometry and ADPs.  This
  includes the standard stereochemistry restraints (i.e. Engh & Huber),
  non-crystallographic symmetry restraints, reference model restraints, and
  optionally.
  """

  def __init__(self,
        geometry=None,
        cartesian_ncs_manager=None,
        normalization=False,
        use_afitt=False, #afitt
        afitt_object=None):
    self.geometry = geometry
    self.cartesian_ncs_manager = cartesian_ncs_manager
    self.normalization = normalization
    # amber moved to a Base_geometry class
    # afitt
    self.use_afitt = use_afitt
    self.afitt_object = afitt_object

  def init_afitt(self, params, pdb_hierarchy, log):
    if hasattr(params, "afitt"):
      use_afitt = params.afitt.use_afitt
      if (use_afitt):
        from mmtbx.geometry_restraints import afitt
        # this only seems to work for a single ligand
        # multiple ligands are using the monomers input
        if params.afitt.ligand_file_name is None:
          ligand_paths = params.input.monomers.file_name
        else:
          ligand_paths = [params.afitt.ligand_file_name]
        afitt.validate_afitt_params(params.afitt)
        ligand_names=params.afitt.ligand_names.split(',')
        if len(ligand_names)!=len(ligand_paths) and len(ligand_names)==1:
          # get restraints library instance of ligand
          from mmtbx.monomer_library import server
          for ligand_name in ligand_names:
            result = server.server().get_comp_comp_id_direct(ligand_name)
            if result is not None:
              so = result.source_info # not the smartest way
              if so.find("file:")==0:
                ligand_paths=[so.split(":")[1].strip()]
        if len(ligand_names)!=len(ligand_paths):
          raise Sorry("need restraint CIF files for each ligand")
        make_header("Initializing AFITT", out=log)
        #print >> log, "  ligands: %s" % params.afitt.ligand_file_name
        afitt_object = afitt.afitt_object(
            ligand_paths,
            ligand_names,
            pdb_hierarchy,
            params.afitt.ff,
            params.afitt.scale)
        print(afitt_object, file=log)
        afitt_object.check_covalent(self.geometry)
        # afitt log output
        afitt_object.initial_energies = afitt.get_afitt_energy(
            ligand_paths,
            ligand_names,
            pdb_hierarchy,
            params.afitt.ff,
            pdb_hierarchy.atoms().extract_xyz(),
            self.geometry)
        self.afitt_object = afitt_object

  def select(self, selection):
    if (self.geometry is None):
      geometry = None
    else:
      if(isinstance(selection, flex.size_t)):
        geometry = self.geometry.select(iselection=selection)
      else:
        geometry = self.geometry.select(selection=selection)
    if (self.cartesian_ncs_manager is None):
      cartesian_ncs_manager = None
    else:
      cartesian_ncs_manager = self.cartesian_ncs_manager.select(selection=selection)

    return manager(
      geometry=geometry,
      cartesian_ncs_manager=cartesian_ncs_manager,
      normalization=self.normalization,
      )

  def energies_sites(self,
        sites_cart,
        geometry_flags=None,
        external_energy_function=None,
        custom_nonbonded_function=None,
        compute_gradients=False,
        gradients=None,
        force_restraints_model=False,
        disable_asu_cache=False,
        hd_selection=None,
        ):
    """
    Compute energies for coordinates.  Originally this just used the standard
    geometry restraints from the monomer library, but it has since been
    extended to optionally incorporate a variety of external energy functions.

    :returns: scitbx.restraints.energies object
    """
    result = scitbx.restraints.energies(
      compute_gradients=compute_gradients,
      gradients=gradients,
      gradients_size=sites_cart.size(),
      gradients_factory=flex.vec3_double,
      normalization=self.normalization)
    if (self.geometry is None):
      result.geometry = None
    else:
      if (self.use_afitt and
          len(sites_cart)==self.afitt_object.total_model_atoms
          ):
        ##################################################################
        #                                                                #
        # AFITT CALL - OpenEye AFITT gradients and target                #
        #                                                                #
        ##################################################################
        from mmtbx.geometry_restraints import afitt
        result.geometry = self.geometry.energies_sites(
          sites_cart=sites_cart,
          flags=geometry_flags,
          external_energy_function=external_energy_function,
          custom_nonbonded_function=custom_nonbonded_function,
          compute_gradients=compute_gradients,
          gradients=result.gradients,
          disable_asu_cache=disable_asu_cache,
          normalization=False,
          )
        result = afitt.apply(result, self.afitt_object, sites_cart)
        result = afitt.adjust_energy_and_gradients(
          result,
          self,
          sites_cart,
          hd_selection,
          self.afitt_object,
        )
        result.target = result.residual_sum
        result.afitt_energy=result.residual_sum
      else :
        result.geometry = self.geometry.energies_sites(
          sites_cart=sites_cart,
          flags=geometry_flags,
          external_energy_function=external_energy_function,
          custom_nonbonded_function=custom_nonbonded_function,
          compute_gradients=compute_gradients,
          gradients=result.gradients,
          disable_asu_cache=disable_asu_cache,
          normalization=False)
      result += result.geometry
    if (self.cartesian_ncs_manager is None):
      result.cartesian_ncs_manager = None
    else:
      result.cartesian_ncs_manager = self.cartesian_ncs_manager.energies_sites(
        sites_cart=sites_cart,
        compute_gradients=compute_gradients,
        gradients=result.gradients,
        normalization=False)
      result += result.cartesian_ncs_manager
    result.finalize_target_and_gradients()
    return result

  def energies_adp_iso(self,
        xray_structure,
        parameters,
        use_u_local_only,
        use_hd,
        wilson_b=None,
        compute_gradients=False,
        tan_b_iso_max=None,
        u_iso_refinable_params=None,
        gradients=None):
    """
    Compute target and gradients for isotropic ADPs/B-factors relative to
    restraints.

    :returns: scitbx.restraints.energies object
    """
    result = scitbx.restraints.energies(
      compute_gradients=compute_gradients,
      gradients=gradients,
      gradients_size=xray_structure.scatterers().size(),
      gradients_factory=flex.double,
      normalization=self.normalization)
    if (self.geometry is None):
      result.geometry = None
    else:
      result.geometry = cctbx.adp_restraints.energies_iso(
        plain_pair_sym_table=self.geometry.plain_pair_sym_table,
        xray_structure=xray_structure,
        parameters=parameters,
        wilson_b=wilson_b,
        use_hd = use_hd,
        use_u_local_only=use_u_local_only,
        compute_gradients=compute_gradients,
        gradients=result.gradients)
      result += result.geometry
    if (self.cartesian_ncs_manager is None):
      result.cartesian_ncs_manager = None
    else:
      result.cartesian_ncs_manager = self.cartesian_ncs_manager.energies_adp_iso(
        u_isos=xray_structure.extract_u_iso_or_u_equiv(),
        average_power=parameters.average_power,
        compute_gradients=compute_gradients,
        gradients=result.gradients)
      result += result.cartesian_ncs_manager
    result.finalize_target_and_gradients()
    if(compute_gradients):
       #XXX highly inefficient code: do something asap by adopting new scatters flags
       if(tan_b_iso_max is not None and tan_b_iso_max != 0):
          u_iso_max = adptbx.b_as_u(tan_b_iso_max)
          if(u_iso_refinable_params is not None):
             chain_rule_scale = u_iso_max / math.pi / (flex.pow2(u_iso_refinable_params)+1.0)
          else:
             u_iso_refinable_params = flex.tan(math.pi*(xray_structure.scatterers().extract_u_iso()/u_iso_max-1./2.))
             chain_rule_scale = u_iso_max / math.pi / (flex.pow2(u_iso_refinable_params)+1.0)
       else:
          chain_rule_scale = 1.0
       result.gradients = result.gradients * chain_rule_scale
    return result

  def energies_adp_aniso(self,
        xray_structure,
        use_hd = None,
        selection = None,
        compute_gradients=False,
        gradients=None):
    """
    Compute target and gradients for isotropic ADPs/B-factors relative to
    restraints.

    :returns: scitbx.restraints.energies object
    """
    result = cctbx.adp_restraints.adp_aniso_restraints(
        restraints_manager = self.geometry,
        xray_structure = xray_structure,
        selection = selection,
        use_hd = use_hd
        #compute_gradients=compute_gradients,
        #gradients=result.gradients
        )
    if(self.normalization):
       normalization_scale = 1.0
       if(result.number_of_restraints > 0):
          normalization_scale /= result.number_of_restraints
       result.target *= normalization_scale
       result.gradients_aniso_cart *= normalization_scale
       result.gradients_aniso_star *= normalization_scale
       if(result.gradients_iso is not None):
          result.gradients_iso *= normalization_scale
    return result

  def write_geo_file(self,
      hierarchy = None,
      sites_cart=None,
      site_labels=None,
      file_name=None,
      file_descriptor=sys.stdout,
      header="# Geometry restraints\n",
      # Stuff for outputting ncs_groups
      excessive_distance_limit = 1.5,
      xray_structure=None,
      # processed_pdb_file=None,
      ):
    """
    This should make complete .geo file with geometry and NCS if present.
    Instead of sites_cart and site_labels one may pass xray_structure and they
    will be extracted from it.
    Content of header will be outputted before restraints in the file.
    If file_name is not specified, everything will be outputted to
    file_descriptor which is stdout by default. Instead of file_name one may
    directly pass file_descriptor and it will be used for output. The caller
    will have take care of saving, closing or flushing it separately.
    """

    def show_selected_atoms(
          selection,
          hierarchy,
          header_lines=None,
          out=None,
          prefix=""):
      if (out is None): out = sys.stdout
      if (header_lines is not None):
        for line in header_lines:
          print(prefix+line, file=out)
      sub_hierarchy = hierarchy.select(atom_selection=selection)
      s = sub_hierarchy.as_pdb_or_mmcif_string()
      if (len(s) == 0 and header_lines is not None):
        s = "  None\n"
      if (prefix == ""):
        out.write(s)
      else:
        for line in s.splitlines():
          print(prefix+line, file=out)

    outf_descriptor = None
    if file_name is None:
      outf_descriptor = file_descriptor
    else:
      outf_descriptor = open(file_name, "w")
    if xray_structure is not None:
      if sites_cart is None:
        sites_cart = xray_structure.sites_cart()
      if site_labels is None:
        site_labels = xray_structure.scatterers().extract_labels()
    self.geometry.write_geo_file(
      sites_cart=sites_cart,
      site_labels=site_labels,
      header=header,
      file_descriptor=outf_descriptor)
    if [self.cartesian_ncs_manager, xray_structure].count(None) == 0:
      self.cartesian_ncs_manager.show_sites_distances_to_average(
          sites_cart=sites_cart,
          site_labels=site_labels,
          excessive_distance_limit=excessive_distance_limit,
          out=outf_descriptor)
      print(file=outf_descriptor)
      self.cartesian_ncs_manager.show_adp_iso_differences_to_average(
          u_isos=xray_structure.extract_u_iso_or_u_equiv(),
          site_labels=site_labels,
          out=outf_descriptor)
      print(file=outf_descriptor)
      # show_atoms_without_ncs_restraints
      show_selected_atoms(
          selection = ~self.cartesian_ncs_manager.selection_restrained(
              n_seq=hierarchy.atoms_size()),
          hierarchy = hierarchy,
          header_lines = ["Atoms without NCS restraints:"],
          out = outf_descriptor)
      print(file=outf_descriptor)
    if file_name is not None:
      outf_descriptor.close()


 *******************************************************************************


 *******************************************************************************
mmtbx/run_tests.py
from __future__ import absolute_import, division, print_function
from libtbx import test_utils
import libtbx.load_env
from libtbx import easy_run
import libtbx.load_env

general_tests = [
  "$D/suitename/unit-test/FullTest.py",
  #
  "$D/regression/tst_sampled_model_density.py",
  # pair interaction
  "$D/pair_interaction/tst_00.py",
  "$D/pair_interaction/tst_01.py",
  "$D/pair_interaction/tst_02.py",
  "$D/pair_interaction/tst_03.py",
  "$D/pair_interaction/tst_04.py",
  "$D/pair_interaction/tst_05.py",
  # ions SVM
  "$D/ions/svm/tst_classifier_01.py",
  "$D/ions/svm/tst_classifier_02.py",
  "$D/ions/tst_pick_ca_svm.py",
  "$D/ions/tst_pickle.py",
  # ion picking
  "$D/ions/tst_parameters.py",
  "$D/ions/tst_pick_ca.py",
  "$D/ions/tst_pick_mg.py",
  "$D/ions/tst_pick_approx_zn.py",
  "$D/ions/tst_validate_ca.py",
  "$D/ions/tst_validate_mg.py",
  "$D/ions/tst_symmetry_axis.py",
  "$D/ions/tst_utils.py",
  # TLS
  "$D/regression/tls/tst_tls.py",
  "$D/regression/tls/tst_tls_analysis.py",
  "$D/regression/tls/tst_tls_utils.py",
  "$D/regression/tls/tst_tls_optimise_amplitudes.py",
  "$D/regression/tls/tst_get_t_scheme.py",
  ["$D/regression/tls/tst_tls_refinement_fft.py", "--comprehensive", "--random_seed=2679941"],
  "$D/regression/tls/tst_u_tls_vs_u_ens_00.py",
  "$D/regression/tls/tst_u_tls_vs_u_ens_01.py",
  "$D/regression/tls/tst_u_tls_vs_u_ens_02.py",
  "$D/regression/tls/tst_u_tls_vs_u_ens_03.py",
  "$D/regression/tls/tst_u_tls_vs_u_ens_04.py",
  "$D/regression/tls/tst_tls_as_xyz.py",
  "$D/regression/tls/tst_formula_t_S_10_vs_11_2igd.py",
  "$D/regression/tls/tst_formula_t_S_10_vs_11_4muy.py",
  #
  "$D/regression/tst_mmtbx_refinement_wrappers.py",
  "$D/regression/tst_mmtbx_refinement_wrappers_1.py",
  #
  "$D/regression/tst_angle.py",
  "$D/regression/tst_holton_geometry_validation.py",
  "$D/rotamer/tst_rotamer_eval.py",
  "$D/rotamer/tst_geostd_vs_sidechain_angles_props.py",
  "$D/monomer_library/tst_idealized_aa.py",
  "$D/regression/tst_ml_estimate.py",
  "$D/density_modification/tst_density_modification.py",
  "$D/geometry_restraints/tst_ramachandran.py",
  "$D/geometry_restraints/tst_manager.py",
  "$D/geometry_restraints/external.py",
  "$D/regression/tst_map_type_parser.py",
  "$D/rsr/tst.py",
  "$D/polygon/tst.py",
  "$D/polygon/tst_gui.py",
  "$D/polygon/tst_polygon_data.py",
  "$D/chemical_components/tst.py",
  "$D/regression/tst_add_h_to_water.py",
  "$D/rotamer/rotamer_eval.py",
  "$D/regression/tst_pdbtools.py",
  "$D/real_space/tst.py",
  "$D/ias/tst_ias.py",
  "$D/refinement/tst_fit_rotamers.py",
  "$D/regression/tst_process_predicted_model.py",
  "$D/regression/tst_process_predicted_model_cif.py",
  "$D/regression/tst_domains_from_pae.py",
  "$D/regression/tst_domains_from_pae_cif.py",
  ["$D/refinement/tst_anomalous_scatterer_groups.py", "P3"],
  "$D/refinement/tst_rigid_body.py",
  "$D/refinement/tst_rigid_body_groups_from_pdb_chains.py",
  "$D/refinement/tst_refinement_flags.py",
  "$D/geometry_restraints/torsion_restraints/tst_reference_model.py",
  "$D/geometry_restraints/torsion_restraints/tst_reference_model_ligands.py",
  # model tests, some more elsewhere in this file
  "$D/regression/model/tst_model_mtrix.py",
  "$D/regression/model/tst_model_ncs.py",
  "$D/regression/model/tst_model_2.py",
  "$D/regression/model/tst_model_biomt_mtrix.py",
  "$D/regression/model/tst_model_biomt_mtrix_2.py",
  "$D/regression/model/tst_model_neutralize_scatterers.py",
  "$D/regression/model/tst_model_set_hydrogen_bond_length.py",
  "$D/regression/model/tst_model_cart_ref_restraints.py",
  "$D/regression/model/tst_model_tors_ref_restraints.py",
  "$D/regression/model/tst_model_remove_alternative_conformations.py",
  "$D/regression/model/tst_model_get_vdw_radii.py",
  "$D/regression/model/tst_model_dedeuterate.py",
  "$D/regression/model/tst_model_utils_merge_models.py",
  #
  "$D/regression/tst_fmodel.py",
  "$D/regression/tst_utils.py",
  "$D/regression/tst_alignment.py",
  ["$D/regression/tst_fmodel_fd.py", "P31"],
  "$D/regression/discamb/tst_fmodel_fd_discamb.py",
  "$D/maps/tst_composite_omit_map.py",
  "$D/maps/tst_composite_omit_map_2.py",
  "$D/maps/tst_composite_omit_map_3.py",
  "$D/maps/tst_correlation.py",
  "$D/maps/tst_map_model_cc_and_vals_per_atom_xtal.py",
  "$D/regression/ncs/tst_ncs_utils.py",
  "$D/regression/ncs/tst_ncs_restraints_group_list.py",
  "$D/regression/ncs/tst_ncs_search.py",
  "$D/regression/ncs/tst_ncs_search_flips.py",
  "$D/regression/ncs/tst_restraints.py",
  "$D/regression/tst_geometry_minimization.py",
  "$D/regression/tst_special_term_at_end.py",
  ["$D/ncs/ncs.py", "exercise"],
  "$D/regression/tst_adp_restraints.py",
  "$D/regression/tst_metal_link_1.py",
  # Xtriage
  "$D/scaling/tst_scaling.py",
  "$D/scaling/tst_outlier.py",
  "$D/scaling/tst_absences.py",
  "$D/scaling/tst_xtriage.py",
  "$D/scaling/tst_xtriage_twin_analyses.py",
  "$D/scaling/matthews.py",
  "$D/scaling/absence_likelihood.py",
  ["$D/scaling/thorough_outlier_test.py", "P21"],
  "$D/twinning/probabalistic_detwinning.py",
  #"$D/scaling/tst_xtriage_massage_data.py",
  # monomer library
  "$D/monomer_library/tst_rna_sugar_pucker_analysis.py",
  "$D/monomer_library/tst_cif_types.py",
  "$D/monomer_library/tst_motif.py",
  "$D/monomer_library/tst_cif_triage.py",
  "$D/monomer_library/tst_rotamer_utils.py",
  "$D/monomer_library/tst_selection.py",
  "$D/monomer_library/tst_tyr_from_gly_and_bnz.py",
  "$D/monomer_library/tst_pdb_interpretation.py",
  "$D/monomer_library/tst_pdb_interpretation_3.py",
  "$D/monomer_library/tst_rna_dna_interpretation.py",
  "$D/monomer_library/tst_protein_interpretation.py",
  "$D/monomer_library/tst_pdb_interpretation_ncs_processing.py",
  "$D/monomer_library/tst_geo_reduce_for_tardy.py",
  "$D/monomer_library/tst_chg.py",
  "$D/monomer_library/tst_neutron_distance.py",
  "$D/monomer_library/tst_carbo_linking.py",
  "$D/monomer_library/tst_glyco_hand.py",
  "$D/monomer_library/tst_glyco_coordination.py",
  "$D/monomer_library/tst_paral_geo.py",
  #
  "$D/regression/tst_altloc_remediate.py",
  "$D/hydrogens/build_hydrogens.py",
  "$D/hydrogens/tst.py",
  # Reduce 2 - hydrogenate
  "$D/hydrogens/tst_add_hydrogen_1.py",
  "$D/hydrogens/tst_add_hydrogen_2.py",
  "$D/hydrogens/tst_add_hydrogen_3.py",
  "$D/hydrogens/tst_add_hydrogen_4.py",
  "$D/hydrogens/tst_add_hydrogen_5.py",
  "$D/hydrogens/tst_add_hydrogen_6.py",
  "$D/hydrogens/tst_add_hydrogen_7.py",
  "$D/hydrogens/tst_add_hydrogen_8.py",
  #"$D/hydrogens/tst_add_hydrogen_time.py",
  "$D/hydrogens/tst_validate_H.py",
  "$D/hydrogens/tst_connectivity.py",
  "$D/hydrogens/tst_riding_coefficients.py",
  "$D/hydrogens/tst_riding_manager.py",
  "$D/hydrogens/tst_parameterization_1.py",
  "$D/hydrogens/tst_parameterization_2.py",
  "$D/hydrogens/tst_parameterization_3.py",
  "$D/hydrogens/tst_parameterization_4.py",
  "$D/hydrogens/tst_parameterization_5.py",
  "$D/hydrogens/tst_parameterization_6.py",
  "$D/hydrogens/tst_parameterization_7.py",
  "$D/hydrogens/tst_parameterization_8.py",
  "$D/hydrogens/tst_parameterization_9.py",
  "$D/hydrogens/tst_riding_fd_1.py",
  "$D/hydrogens/tst_riding_fd_2.py",
  "$D/hydrogens/tst_riding_fd_3.py",
  "$D/hydrogens/tst_riding_fd_4.py",
  "$D/hydrogens/tst_riding_fd_5.py",
  "$D/hydrogens/tst_riding_minimize.py",
  "$D/max_lik/tst_maxlik.py",
  "$D/masks/tst_masks.py",
  "$D/masks/tst_asu_mask.py",
  "$D/max_lik/tst_max_lik.py",
  "$D/dynamics/tst_cartesian_dynamics.py",
  "$D/dynamics/tst_sa.py",
  #
  "$D/examples/f_model_manager.py",
  # Bulk solvent
  "$D/bulk_solvent/tst_bulk_solvent_and_scaling.py",
  "$D/bulk_solvent/tst_fit_kexpb_to_ktotal.py",
  "$D/bulk_solvent/tst_scaler.py",
  "$D/bulk_solvent/tst_flat_nonuniform.py",
  "$D/bulk_solvent/tst_aniso_scaler.py",
  "$D/bulk_solvent/tst_kb_sol_u_star_fd.py",
  "$D/bulk_solvent/tst_mosaic.py",
  #
  "$D/invariant_domain.py",
  # restraints
  "$D/regression/tst_psi_phi_extraction.py",
  "$D/secondary_structure/tst.py",
  "$D/secondary_structure/tst_insertion_codes.py",
  "$D/secondary_structure/tst_segid.py",
  "$D/geometry_restraints/tst_reference_coordinate.py",
  "$D/geometry_restraints/tst_reference_coordinate2.py",
  "$D/geometry_restraints/tst_c_beta_restraints.py",
  "$D/geometry_restraints/torsion_restraints/tst_torsion_ncs.py",
  "$D/conformation_dependent_library/tst_cdl.py",
  "$D/conformation_dependent_library/tst_cdl_esd.py",
  "$D/conformation_dependent_library/tst_rdl.py",
  "$D/conformation_dependent_library/tst_hpdl.py",
  "$D/conformation_dependent_library/tst_cis_c_n_ca.py",
  "$D/conformation_dependent_library/cdl_svl_database.py",
  "$D/conformation_dependent_library/tst_pH_mechanism.py",
  "$D/conformation_dependent_library/tst_mcl_01.py",
  "$D/conformation_dependent_library/tst_mcl_02.py",
  "$D/conformation_dependent_library/tst_mcl_03.py",
  "$D/regression/tst_find_ss_structure.py",
  "$D/regression/tst_find_ss_structure_cif.py",
  "$D/regression/tst_chain_comparison.py",
  "$D/regression/tst_chain_comparison_cif.py",
  "$D/regression/tst_regularize_from_pdb.py",
  "$D/regression/tst_regularize_from_pdb_cif.py",
  "$D/regression/tst_find_ncs.py",
  "$D/regression/tst_minimize_chain.py",
  "$D/regression/tst_sequence_validation.py",
  "$D/regression/tst_prune_model.py",
  "$D/regression/tst_real_space_correlation.py",
  "$D/regression/tst_examples.py",
  "$D/regression/tst_sort_hetatms.py",
  "$D/regression/tst_schrodinger_interface.py",
  # real-space tools
  "$D/refinement/real_space/tst_aa_residue_axes_and_clusters.py",
  "$D/refinement/real_space/tst_fit_residue_full_sampling.py",
  "$D/refinement/real_space/tst_fit_residue_0.py",
  "$D/refinement/real_space/tst_fit_residue_0H.py",
  "$D/refinement/real_space/tst_fit_residue_1.py",
  "$D/refinement/real_space/tst_fit_residue_1H.py",
  "$D/refinement/real_space/tst_fit_residue_2.py",
  "$D/refinement/real_space/tst_fit_residue_3.py",
  "$D/refinement/real_space/tst_fit_residue_4.py",
  "$D/refinement/real_space/tst_fit_residue_5.py",
  "$D/refinement/real_space/tst_fit_residue_6.py",
  "$D/refinement/real_space/tst_fit_residue_7.py",
  "$D/refinement/real_space/tst_fit_residues_1.py",
  "$D/refinement/real_space/tst_fit_residues_1H.py",
  "$D/refinement/real_space/tst_fit_residues_2.py",
  "$D/refinement/real_space/tst_fit_residues_3.py",
  "$D/refinement/real_space/tst_fit_residues_4.py",
  "$D/refinement/real_space/tst_fit_residues_5.py",
  "$D/refinement/real_space/tst_fit_residues_6.py",
  "$D/refinement/real_space/tst_fit_residues_selection.py",
  "$D/refinement/real_space/tst_fit_water.py",
  "$D/refinement/real_space/tst_individual_sites_1.py",
  "$D/refinement/real_space/tst_individual_sites_2.py",
  "$D/refinement/real_space/tst_individual_sites_3.py",
  "$D/refinement/real_space/tst_monitor.py",
  "$D/refinement/real_space/tst_rigid_body.py",
  "$D/refinement/real_space/tst_weight.py",
  #
  "$D/idealized_aa_residues/tst.py",
  #
  "$D/regression/tst_validation_summary.py",
  "$D/regression/tst_maps_misc.py",
  "$D/regression/tst_anomalous_substructure.py",
  #
  "$D/regression/tst_fmodel_no_cryst1.py",
  "$D/regression/tst_fmodel_and_dm.py",
  "$D/regression/tst_fmodel_misc.py",
  "$D/regression/tst_isomorphous_difference_misc.py",
  "$D/regression/tst_dynamics_cli.py",
  "$D/ligands/tst_xtal_screens.py",
  "$D/ligands/tst_ready_set_utils.py",
  "$D/regression/tst_mtz2map.py",
  # ringer
  "$D/regression/tst_ringer.py",
  "$D/ringer/tst_emringer.py",
  "$D/ringer/tst_em_rscc.py",
  # validation/molprobity
  "$D/regression/tst_probe.py",
  "$D/regression/tst_reduce.py",
  "$D/regression/tst_ribbons.py",
  "$D/validation/regression/tst_molprobity_arguments.py",
  "$D/validation/regression/tst_chiral_validation.py",
  "$D/validation/regression/tst_waters.py",
  "$D/validation/regression/tst_keep_hydrogens.py",
  "$D/validation/regression/tst_mp_geo.py",
  "$D/validation/regression/tst_rotalyze.py",
  "$D/validation/regression/tst_ramalyze.py",
  "$D/validation/regression/tst_ramalyze2.py",
  "$D/validation/regression/tst_ramalyze_rare.py",
  "$D/validation/regression/tst_clashscore.py",
  "$D/validation/regression/tst_clashscore_2.py",
  "$D/validation/regression/tst_restraints.py",
  "$D/validation/regression/tst_omegalyze.py",
  "$D/validation/regression/tst_rna_validate.py",
  "$D/validation/regression/tst_suitename.py",
  "$D/validation/regression/tst_model_properties.py",
  "$D/validation/regression/tst_molprobity_2.py",
  "$D/validation/regression/tst_molprobity_3.py",
  "$D/validation/regression/tst_mp_validate_bonds.py",
  "$D/validation/regression/tst_hydrogen_addition_clashscore.py",
  "$D/validation/regression/tst_symmetry_SS.py",
  "$D/validation/regression/tst_do_flips_clashscore.py",
  "$D/validation/regression/tst_find_region_max_value.py",
  "$D/validation/regression/tst_undowser2.py",
  #
  "$D/refinement/tst_select_best_starting_model.py",
  "$D/regression/tst_refine_anomalous_substructure.py",
  "$D/regression/tst_helix_sheet_recs_as_pdb_files.py",
  "$D/regression/tst_command_line_input.py",
  "$D/regression/tst_cif_as_mtz_wavelengths.py",
  "$D/building/tst_extend_sidechains.py",
  "$D/building/tst_extend_sidechains_2.py",
  # alt confs
  "$D/disorder/tst.py",
  "$D/disorder/tst_backbone.py",
  "$D/disorder/tst_analyze_model.py",
  #
  "$D/refinement/tst_group.py",
  "$D/refinement/tst_group_2.py",
  "$D/secondary_structure/build/tst_1.py",
  "$D/secondary_structure/build/tst_2.py",
  "$D/secondary_structure/build/tst_3.py",
  "$D/secondary_structure/build/tst_io.py",
  "$D/secondary_structure/build/tst_ss_id_ncs.py",
  "$D/regression/tst_loop_closure.py",
  "$D/regression/model_idealization/tst_nomap_01.py",
  "$D/regression/model_idealization/tst_nomap_02.py",
  "$D/regression/model_idealization/tst_nomap_03.py",
  "$D/regression/model_idealization/tst_nomap_04.py",
  "$D/regression/model_idealization/tst_withmap_01.py",
  "$D/regression/model_idealization/tst_withmap_02.py",
  "$D/regression/model_idealization/tst_withmap_03.py",
  "$D/regression/model_idealization/tst_withmap_04.py",
  "$D/regression/model_idealization/tst_withmap_05.py",
  "$D/regression/model_idealization/tst_withmap_06.py",
  "$D/regression/model_idealization/tst_ligands.py",
  "$D/regression/model_idealization/tst_with_mtz.py",
  "$D/utils/tst_switch_rotamers.py",
  "$D/refinement/tst_occupancy_selections.py",
  "$D/regression/ncs/tst_minimization_ncs_constraints.py",
  "$D/regression/ncs/tst_minimization_ncs_constraints2.py",
  "$D/regression/ncs/tst_minimization_ncs_constraints3.py",
  "$D/regression/ncs/tst_minimization_ncs_constraints_real_space.py",
  "$D/monomer_library/tst_correct_hydrogens.py",
  "$D/monomer_library/tst_deuterium_terminii.py",
  # automatic linking
  "$D/monomer_library/tst_superpose_ideal.py",
  "$D/monomer_library/tst_iron_sulfur_clusters.py",
  #
  "$D/scaling/tst_plan_sad_experiment.py",
  #
  "$D/regression/tst_models_to_from_chains.py",
  # tNCS
  "$D/regression/tncs/tst_pair.py",
  "$D/regression/tncs/tst_epsfac_and_radius.py",
  "$D/regression/tncs/tst_fd.py",
  "$D/regression/tncs/tst_moments.py",
  #
  "$D/regression/tst_rank_scale_map.py",
  "$D/regression/tst_polder.py",
  "$D/regression/tst_polder_1.py",
  "$D/regression/tst_polder_2.py",
  "$D/regression/tst_polder_3.py",
  "$D/regression/tst_polder_4.py",
  "$D/regression/tst_polder_box.py",
  "$D/regression/tst_polder_ccs.py",
  "$D/regression/tst_map_model_cc.py",
  "$D/regression/tst_model_map.py",
  #
  "$D/regression/tst_validate_ligands.py",
  #
  "$D/regression/tst_cis_trans_peptide_link.py",
  "$D/regression/tst_apply_cif_restraints.py",
  "$D/regression/tst_multi_residue_class.py",
  "$D/regression/tst_superpose.py",
  "$D/monomer_library/tst_server.py",
  #
  "$D/geometry/tests/tst_altloc.py",
  "$D/geometry/tests/tst_asa.py",
  "$D/geometry/tests/tst_clash.py",
  "$D/geometry/tests/tst_indexing.py",
  "$D/geometry/tests/tst_shared_types.py",
  "$D/geometry/tests/tst_sphere_surface_sampling.py",
  "$D/geometry/tests/tst_topology.py",
  #
  "$D/nci/tst_hbond.py",
  "$D/wwpdb/tst_rcsb_web_services.py",
  "$D/wwpdb/tst_rcsb_entry_request.py",
  "$D/regression/command_line/tst_find_residue_in_pdb.py",
  "$D/monomer_library/tst_all_cif_files.py",
  "$D/refinement/tst_misc.py",
  "$D/refinement/tst_real_space_simple.py",
  "$D/scaling/tst_bayesian_estimator.py",
  "$D/scaling/tst_mean_f_rms_f.py",
  "$D/scaling/tst_sigmaa.py",
  "$D/scaling/tst_massage_data.py",
  "$D/ions/tst_geometries.py",
  "$D/ions/tst_pick_k.py",
  "$D/ions/tst_environment.py",
  "$D/regression/tst_table_one.py",
  "$D/regression/tst_msa.py",
  "$D/regression/tst_cc_star.py",
  "$D/regression/tst_fmodel_2.py",
  "$D/regression/tst_rigid_bond_test.py",
  "$D/validation/regression/tst_cablam.py",
  "$D/regression/real_space_refine_chain/tst_00.py",
  "$D/regression/real_space_refine_chain/tst_01.py",
  "$D/conformation_dependent_library/tst_omega_cdl.py",
  "$D/regression/fix_cablam/tst_basic_cl_operations.py",
  "$D/regression/fix_cablam/tst_single_outliers_surroundings.py",
  "$D/regression/fix_cablam/tst_one_resid_rotation.py",
  "$D/regression/fix_cablam/tst_insertion_codes.py",
  # Tests involving scattering table
  "$D/regression/tst_scattering_type_registry.py",
  "$D/regression/tst_scattering_table.py",
  #
  "$D/atomic_environment_vectors/tst.py",
  #
  "$D/building/ligands/tst_00.py",
  # Tests that were not in the list and caused us to fail as of 10/6/2021
  "$D/regression/tst_ligand_ncs.py",
  "$D/regression/tst_origin_ids.py",
  "$D/regression/model/tst_model_ramachandran.py",
  "$D/regression/model_idealization/tst_ext_map_01.py",
  "$D/validation/regression/tst_mp_geo_endtoend.py",
  "$D/validation/regression/tst_mp_geo_chiral_volume_cases.py",
  "$D/conformation_dependent_library/tst_multi_residue_rna.py",
  "$D/conformation_dependent_library/tst_cdl_nucleotides.py",
  "$D/conformation_dependent_library/tst_rna_dna_fragments.py",
  "$D/conformation_dependent_library/testing_utils.py",
  "$D/conformation_dependent_library/tst_poly_ca_multi_residue.py",
  "$D/conformation_dependent_library/tst_mutli_residue.py",
  ]

molprobity_tests = [
  "$D/validation/regression/tst_rama_z_01.py",
  "$D/validation/regression/tst_rama_z_02.py",
  "$D/regression/pdb_interpretation/tst_edits.py",
  "$D/regression/pdb_interpretation/tst_edits_actions.py",
  "$D/regression/pdb_interpretation/tst_tardy_geo.py",
  "$D/regression/pdb_interpretation/tst_custom_nb_sym_excl.py",
  "$D/regression/pdb_interpretation/tst_using_ncs_1.py",
  "$D/regression/pdb_interpretation/tst_using_ncs_2.py",
  "$D/regression/tst_add_arrows_on_plot.py",
  "$D/regression/model/tst_model.py",
  "$D/regression/tst_reduce_timeout.py",
  "$D/regression/tst_altloc_chain_break.py",
  "$D/regression/ncs/tst_geometry_minimization_ncs_constraints.py",
  "$D/regression/tst_clashes.py",
  "$D/validation/regression/tst_cbetadev.py",
  "$D/validation/regression/tst_cbetadev_02.py",
  "$D/validation/regression/tst_undowser.py",
  "$D/regression/tst_statistics_output.py",
  "$D/regression/tst_geo_min_restraints_phil.py",
  "$D/regression/tst_model_vs_map.py",
  # validation/molprobity
  "$D/validation/regression/tst_molprobity_1.py",
  "$D/validation/regression/tst_molprobity_4.py",
  "$D/validation/regression/tst_molprobity_5.py",
  "$D/validation/regression/tst_experimental.py",
  "$D/validation/regression/tst_cablam_2.py",
  # automatic linking
  ["$D/monomer_library/tst_metal_coordination.py", "1"],
  ["$D/monomer_library/tst_metal_coordination.py", "2"],
  ["$D/monomer_library/tst_metal_coordination.py", "3"],
  ["$D/monomer_library/tst_linking.py", "1"],
  ["$D/monomer_library/tst_linking.py", "2"],
  ["$D/monomer_library/tst_linking.py", "3"],
  ["$D/monomer_library/tst_linking.py", "4"],
  ["$D/monomer_library/tst_linking.py", "5"],
  ["$D/monomer_library/tst_linking.py", "6"],
  ["$D/monomer_library/tst_linking.py", "7"],
  ["$D/monomer_library/tst_linking.py", "8"],
  ["$D/monomer_library/tst_linking.py", "9"],
  ["$D/monomer_library/tst_linking.py", "10"],
  ["$D/monomer_library/tst_linking.py", "11"],
  ["$D/monomer_library/tst_linking.py", "12"],
  ["$D/monomer_library/tst_linking.py", "13"],
  ["$D/monomer_library/tst_linking.py", "14"],
  ["$D/monomer_library/tst_linking.py", "15"],
  ["$D/monomer_library/tst_linking.py", "16"],
  ["$D/monomer_library/tst_linking.py", "17"],
  ["$D/monomer_library/tst_linking.py", "18"],
  ["$D/monomer_library/tst_linking.py", "19"],
  ["$D/monomer_library/tst_linking.py", "20"],
  ["$D/monomer_library/tst_linking.py", "21"],
  ["$D/monomer_library/tst_linking.py", "22"],
  ["$D/monomer_library/tst_linking.py", "23"],
  ["$D/monomer_library/tst_linking.py", "24"],
  ["$D/monomer_library/tst_linking.py", "25"],
  ["$D/monomer_library/tst_linking.py", "26"],
  ["$D/monomer_library/tst_linking.py", "27"],
  ["$D/monomer_library/tst_linking.py", "28"],
  ["$D/monomer_library/tst_linking.py", "29"],
  ["$D/monomer_library/tst_linking.py", "30"],
  ["$D/monomer_library/tst_linking.py", "31"],
  ["$D/monomer_library/tst_linking.py", "32"],
  ["$D/monomer_library/tst_linking.py", "33"],
  ["$D/monomer_library/tst_linking.py", "34"],
  ["$D/monomer_library/tst_linking.py", "35"],
  ["$D/monomer_library/tst_linking.py", "36"],
  ["$D/monomer_library/tst_linking.py", "37"],
  ["$D/monomer_library/tst_linking.py", "38"],
  ["$D/monomer_library/tst_linking.py", "39"],
  ["$D/monomer_library/tst_linking.py", "40"],
]

if libtbx.env.has_module(name="probe"):
  tst_list = tuple(general_tests + molprobity_tests)
else:
  print("Skipping %d MolProbity tests: probe not configured"%len(molprobity_tests))
  tst_list = tuple(general_tests)

# generally failing tests
tst_list_expected_failures = [
  "$D/monomer_library/tst_pdb_interpretation_2.py",
  ]

def run():
  assert easy_run.call("libtbx.find_untested mmtbx True")==0
  build_dir = libtbx.env.under_build("mmtbx")
  dist_dir = libtbx.env.dist_path("mmtbx")
  test_utils.run_tests(build_dir, dist_dir, tst_list)

if (__name__ == "__main__"):
  run()


 *******************************************************************************


 *******************************************************************************
mmtbx/superpose.py
from __future__ import absolute_import, division, print_function
import sys
import os
import itertools

import cctbx.array_family.flex
import iotbx.pdb
import iotbx.pdb.amino_acid_codes
import iotbx.phil
import libtbx
import libtbx.math_utils
import libtbx.phil
import libtbx.runtime_utils
import mmtbx.alignment
import scitbx.math
import scitbx.math.superpose

from libtbx.utils import Sorry
from six.moves import zip
from six.moves import map

# Ideas:
#   preset options: Ca, backbone, all atom
#   sieve fit -- scitbx.math.superpose
#   SCEDS fit -- http://www.ncbi.nlm.nih.gov/pubmed/24189233
#   re-alignment of models using a different selection
#   automatic determination of subunit equivalences for multimers
#   chain independent superposition
# TODO: Merge back into a single pdb.
# TODO: Provide different alignment methods? Create subclasses override lsq?
# TODO: Port GUI

##############################################
# PHIL and Phenix Launcher infrastructure
##############################################

# PHIL Arguments.
PHIL_PARAMS = """
input {
  pdb_file_name_fixed = None
    .type = path
    .short_caption = Fixed model
    .help = Name of PDB file with model to fit to
    .style = bold file_type:pdb input_file
  pdb_file_name_moving = None
    .type = path
    .short_caption = Moving model
    .help = Name of PDB file with model that will be fit to pdb_file_name_fixed
    .style = bold file_type:pdb input_file
}

output {
  file_name = None
    .help = Name of PDB file with model that best fits to pdb_file_name_fixed
    .short_caption = Output model
    .type = path
    .style = bold new_file file_type:pdb
  include scope libtbx.phil.interface.tracking_params
}

selection_fixed = None
  .type = str
  .short_caption = Selection for fixed model
  .input_size = 400
  .help = Selection of the target atoms to fit to (optional)
selection_moving = None
  .type = str
  .short_caption = Selection for moving model
  .input_size = 400
  .help = Selection of the atoms that will be fit to selection_fixed (optional)
selection_fixed_preset = * ca backbone all
  .type = choice
  .help = Selection preset for fixed model.
selection_moving_preset = * ca backbone all
  .type = choice
  .help = Selection preset for moving model.
selection_fixed_chain = None
  .type = str
  .help = Selection chain for fixed model.
selection_moving_chain = None
  .type = str
  .help = Selection chain for moving model.

alignment
  .help = Set of parameters for sequence alignment. Defaults are good for most \
          of cases
  .short_caption = Sequence alignment
  .style = box auto_align
{
  alignment_style = local *global
    .type = choice
  gap_opening_penalty = 1
    .type = float
  gap_extension_penalty = 1
    .type = float
  similarity_matrix = blosum50  dayhoff *identity
    .type = choice
}"""

# Preset selections.
PRESETS = {
    'backbone': """pepnames and (name ca or name n or name c) and altloc \" \" chain %(chain)s""",
    'ca': """pepnames and (name ca) and altloc \" \" chain %(chain)s""",
    'all': """all chain %(chain)s""",
}
PRESET_ORDER = ['backbone', 'ca', 'all']

class SuperposePDB(object):
  """Superimpose PDB files.

  Instantiate with a filename or pdb instance. You may also provide a default
  selection string, selection preset, or chain to use for atom selection.
  Otherwise, the select method will attempt to find a reasonable default.

  Use the superpose(target) method to perform superposition. This will return
  the RMSD, and the transformation used. To write output, use output(lsq,
  filename).

  The selectomatic(target) method performs a pairwise comparison of chain
  sequence alignments, and updates the selections to use the chains with the
  highest similarity.

  You can also update the selection using the select_update() method, or return
  a given selection using select().

  Multiple models are supported by creating multiple instances. The class method
  open_models() helper can be used to open a file with multiple models.

  To create a subclass with a modified fitting routine, override fit().
  """

  # Alignment cache.
  _seqalign_cache = {}

  def __init__(self, filename=None, pdb=None, selection=None, preset=None,
               chain=None, quiet=False, log=None, desc=None):
    """A filename or pdb instance is required. You may also provide arguments
    for the atom selection. The quiet option will suppress informational
    output. The desc option provides a label for the log output.
    """
    self._quiet = quiet
    self._log = log
    self.desc = desc or os.path.basename(filename)

    # Validate input.
    if (filename is None) and (pdb is None):
      raise Sorry("Filename or PDB instance required.")

    # Read PDB or used specified pdb_hierarchy.
    self.pdb_input = pdb or iotbx.pdb.input(filename)
    self.pdb = self.pdb_input.construct_hierarchy()

    if len(self.pdb.models()) > 1:
      raise Sorry("Only one model supported; use SuperposePDB.open_models() to open multiple models.")

    # Select atoms.
    self.select_update(selection=selection, chain=chain, preset=preset)

  @classmethod
  def open_models(cls, filename=None, pdb=None, **kwargs):
    """Class method to return instances for each model in a file."""
    # pdb = pdb or iotbx.pdb.input(filename).construct_hierarchy()
    pdb = iotbx.pdb.input(filename)
    models = iotbx.pdb.input(filename).construct_hierarchy().models()
    desc = kwargs.pop('desc', 'moving')
    # print "Model check:"
    # for count, i in enumerate(models):
    #   print "\tmodel: %s, atoms: %s, chains: %s"%(count, len(i.atoms()), len(i.chains()))
    ret = []
    if len(models) == 1:
      ret.append(cls(pdb=pdb, desc=desc, **kwargs))
    elif len(models) > 1:
      for count, model in enumerate(models):
        pdb_new = iotbx.pdb.hierarchy.root()
        pdb_new.append_model(model.detached_copy())
        ret.append(cls(pdb=pdb_new, desc='%s-%s'%(desc, count), **kwargs))
    else:
      raise Sorry("No models found!")
    return ret

  def get_transformed(self):
    """Return a transformed model."""
    raise NotImplemented

  def selectomatic(self, target):
    """Perform pairwise sequence alignments and find the best aligned chain pair.

    This method performs pairwise sequence alignments between all chains in
    itself and the target. The selections are updated to the pair with the
    highest sequence similarity, with chain IDs in the return value.
    """
    alignments = []
    for moving_chain in self.pdb.chains():
      _, moving_selection, _ = self.select(selection="all chain %s"%moving_chain.id)
      for target_chain in target.pdb.chains():
        # For each pair,
        #   ... get the selection array
        _, target_selection, _ = target.select(selection="all chain %s"%target_chain.id)
        #   ... extract sequence from selection
        moving_seq, _ = self._extract_sequence_chain(moving_chain, moving_selection)
        target_seq, _ = target._extract_sequence_chain(target_chain, target_selection)
        #   ... perform alignment
        alignment = self._seqalign(moving_seq, target_seq)
        #   ... calculate score
        score = self._seqalign_score(alignment)
        alignments.append((score, moving_chain.id, target_chain.id, alignment))

    if not alignments:
      raise Sorry("No alignments?")

    best = sorted(alignments)[-1]
    self.log("Select-o-matic: Aligning chain %s to target chain %s"%(best[1], best[2]))
    self._print_seqalign(best[3])
    self.select_update(chain=best[1])
    target.select_update(chain=best[2])
    return best[1], best[2]

  def select(self, selection=None, chain=None, preset=None):
    """Update the selection.

    Specify either an explicit selection, or one of the handy preset selections
    (ca, backbone, all). Presets will use the longest chain by default, but
    this can also be specified using chain. If no selection or preset is
    specified, the various presets will be tested before falling back to all
    atom selection.
    """
    # Check for various Nones
    if selection in [None, "None", "none"]:
      selection = None

    # Use the specified chain, or find the longest chain.
    chains = sorted(self.pdb.chains(), key=lambda x:x.atoms_size())
    chain = chain or chains[-1].id

    if selection:
      # If a selection was specified, use it directly.
      selected_atoms = self.pdb.atom_selection_cache().selection(string=selection)
    elif PRESETS.get(preset):
      # Use a preset if it was specified.
      selection = PRESETS.get(preset)
      selection = selection%{'chain':chain}
      selected_atoms = self.pdb.atom_selection_cache().selection(string=selection)
    else:
      # Automagical selection; try the various presets,
      #   eventually falling back to 'all'
      for preset in PRESET_ORDER:
        selection = PRESETS[preset]
        selection = selection%{'chain':chain}
        selected_atoms = self.pdb.atom_selection_cache().selection(string=selection)
        if selected_atoms.count(True):
          break

    # Resequence the atoms and get the xyz coords.
    atoms = self.pdb.atoms()
    atoms.reset_i_seq()
    selected_xyz = atoms.extract_xyz().select(selected_atoms)
    return selection, selected_atoms, selected_xyz

  def select_update(self, selection=None, chain=None, preset=None):
    """select() and update state."""
    self.selection, self.selected_atoms, self.selected_xyz = self.select(selection, chain, preset)
    self.log("Using selection: %s, atoms total: %s, selected: %s"%(self.selection, len(self.selected_atoms), len(self.selected_xyz)))

  def log(self, *msg):
    """Log."""
    if self._quiet:
      return
    print("%s: "%self.desc, " ".join(map(str, msg)), file=self._log)

  def _print_rmsd(self, sites_moving, sites_fixed, desc=''):
    """Print statistics on the RMSD between sites_moving and sites_fixed."""
    rmsd = sites_fixed.rms_difference(sites_moving)
    deltas = (sites_fixed - sites_moving).norms()
    try:
      pbs = libtbx.math_utils.percentile_based_spread(deltas)
      self.log("Percentile-based spread (%s): %-.3f"%(desc, pbs))
    except Exception:
      self.log("Could not calculate percentile-based spread... Please fix!")
    self.log("RMSD between fixed and moving atoms (%s): %-.3f"%(desc, rmsd))

  def _print_lsq(self, lsq=None):
    """Print the transformation described by a lsq."""
    self.log("Rotation:")
    self.log("\n"+lsq.r.mathematica_form(label="r", one_row_per_line=True, format="%8.5f"))
    self.log("Translation:")
    self.log(lsq.t.mathematica_form(label="t", format="%8.5f"))

  def _print_seqalign(self, alignment, quiet=False):
    """Print a sequence alignment details."""
    matches = alignment.matches()
    self.log("Alignment details:")
    self.log("\tmatches after alignment: %s"%(matches.count("|") + matches.count("*")))
    self.log("\tsequence alignment:")
    # Since this prints directly, check if quiet.
    if not (quiet or self._quiet):
      # Change the labels to target.desc
      alignment.pretty_print(
        matches   = matches,
        block_size  = 50,
        n_block   = 1,
        top_name  = "moving",
        bottom_name = "fixed")

  def fit(self, sites_moving, sites_fixed):
    """Perform a least squares fit between sites_moving and sites_fixed. Return
    the rmsd, lsq, and updated sites_moving, sites_fixed."""
    # Check moving and fixed are the same size.
    if len(sites_moving) != len(sites_fixed):
      raise Sorry("Cannot align different number of atoms: %s and %s"%(len(sites_moving), len(sites_fixed)))
    if len(sites_moving) == 0:
      raise Sorry("Cannot align an empty set of atoms.")
    if len(sites_moving) < 2:
      raise Sorry("Cannot align a single atom.")

    # Perform the least squares fit
    # print "len:", len(sites_moving), len(sites_fixed)
    # for i,j in zip(sites_moving, sites_fixed):
    #    print "moving/fixed:", i, j
    lsq = scitbx.math.superpose.least_squares_fit(reference_sites=sites_fixed, other_sites=sites_moving)
    sites_moving2 = lsq.other_sites_best_fit()
    rmsd = sites_fixed.rms_difference(sites_moving2)
    return rmsd, lsq, sites_moving, sites_fixed

  def superpose(self, target):
    """Superpose to target. Return rmsd and lsq.

    This method will perform a sequence alignment first if the target is not
    sequence identical.
    """
    seq_moving, str_moving = self._extract_sequence_rgs()
    seq_fixed, str_fixed = target._extract_sequence_rgs()
    self.log("Let's go!")

    # Perform the sequence alignment
    alignment = self._seqalign(seq_moving, seq_fixed)
    self._print_seqalign(alignment)

    # Too many arguments; should it just extract seq/str in _align?
    rmsds = self._superpose_align(
      alignment,
      seq_moving,
      str_moving,
      seq_fixed,
      str_fixed,
      self.selected_atoms,
      target.selected_atoms
      )
    rmsds = sorted(rmsds, key=lambda x:x[0], reverse=True)
    if not rmsds:
      raise Sorry("Failed to get a good sequence alignment, or failed to fit models.")
    rmsd, lsq, xyz_moving, xyz_fixed = rmsds.pop()

    self.log("Initial stats:")
    self._print_rmsd(xyz_moving, xyz_fixed, desc='start')
    self.log("Best fit:")
    self.log("Number of atoms for LS fitting: %s"%xyz_moving.size())
    self._print_rmsd(lsq.other_sites_best_fit(), xyz_fixed, desc='final')
    self._print_lsq(lsq)
    return rmsd, lsq

  def _superpose_align(self, alignment, seq_moving, str_moving, seq_fixed, str_fixed, sel_moving, sel_fixed, windows=None):
    """Perform a sequence alignment, try to find a sequence alignment window
    that produces the lowest lsq fit. Note that this is a generator."""
    # Find the alignment window with the lowest RMSD.
    rmsds = []
    windows = (2,1,0) # windows or (0, 1)
    for count, window in enumerate(windows):
      # Create new coordinates based on the alignment.
      xyz_moving = cctbx.array_family.flex.vec3_double()
      xyz_fixed  = cctbx.array_family.flex.vec3_double()
      aligned = self._seqalign_pickmatches(alignment, window=window)
      for i,j,k in aligned:
        # alignment index, fixed index, moving index
        # assert seq_moving[j] == seq_fixed[k]
        # Add the atoms for this residue.
        for atom1 in str_moving[j].rg.atoms():
          for atom2 in str_fixed[k].rg.atoms():
            if (atom1.name == atom2.name) and sel_moving[atom1.i_seq] and sel_fixed[atom2.i_seq]:
              # print "checked:", atom1.name, atom2.name, atom1.i_seq, atom2.i_seq
              xyz_moving.append(atom1.xyz)
              xyz_fixed.append(atom2.xyz)

      # Yield the RMSD for this window.
      try:
        yield self.fit(xyz_moving, xyz_fixed)
      except Exception as e:
        pass
        # self.log("Fitting error: %s"%e)

  def _seqalign(self, seq_moving, seq_fixed, quiet=False, cache=True):
    """Perform sequence alignment using mmtbx.alignment. Return alignment.

    Sequence alignments are cached; use cache=False to disable.
    """
    key = (seq_moving, seq_fixed)
    if self._seqalign_cache.get(key) and cache:
      return self._seqalign_cache[key]

    opts = {
      'gap_opening_penalty': 12,
      'gap_extension_penalty': 1,
      'similarity_function': 'blosum50', # identity
      'style': 'global'
    }
    opts['seq_a'] = seq_moving
    opts['seq_b'] = seq_fixed
    align_obj = mmtbx.alignment.align(**opts)
    alignment = align_obj.extract_alignment()
    if cache:
      self._seqalign_cache[key] = alignment
    return alignment

  def _seqalign_pickmatches(self, alignment, window=0):
    """Find the indexes of aligned residues.

    Returns a list of lists with the indexes:
      [match index, sequence a index, sequence b index]
    """
    # Find sequence alignment matches -- "|" or "*"
    # Return the alignment index, the source index, and target index
    #   of each match.
    matches = alignment.matches()
    aligned = []
    for i, ia, ib, im, a, b in zip(
        itertools.count(0),
        alignment.i_seqs_a,
        alignment.i_seqs_b,
        matches,
        alignment.a,
        alignment.b):
      # print i, ia, ib, im, a, b
      left = i-window
      if left < 0:
        left = 0
      right = i+window+1
      if right > len(matches):
        right = len(matches)
      w = matches[left:right]
      append = all((i == '*' or i == '|') for i in w)
      if append:
        # print "window: %s -- left, right: %s %s -- i: %s -- append? %s"%(w, left, right, i, append)
        aligned.append((i, ia, ib))
    return aligned

  def _seqalign_score(self, alignment):
    matches = alignment.matches()
    total = len(alignment.a) - alignment.a.count("-")
    equal = matches.count("|")
    similar = matches.count("*")
    score = 100.*(equal+similar) / max(1,total)
    return score

  def _extract_sequence_rgs(self):
    """Extract the sequence and residue groups of the selected atoms."""
    sequence = []
    rgs = []
    for chain in self.pdb.chains():
      a, b = self._extract_sequence_chain(chain, self.selected_atoms)
      sequence.append(a)
      rgs.extend(b)
    return "".join(sequence), rgs

  def _extract_sequence_chain(self, chain, selection):
    """Extract the sequence and residue groups of a chain."""
    sequence = []
    rgs = []
    counter = 0
    for rg in chain.residue_groups():
      good = False
      for ai in rg.atoms().extract_i_seq():
        # print ai, selection[ai]
        if selection[ai]:
          good = True
          break
      if good:
        if len(rg.unique_resnames()) == 1:
          resname = rg.unique_resnames()[0]
          olc = iotbx.pdb.amino_acid_codes.one_letter_given_three_letter.get(resname, "X")
          if olc != "X":
            # proteins only
            sequence.append(olc)
            rgs.append(libtbx.group_args(i_seq=counter, rg=rg))
            counter += 1
          elif iotbx.pdb.common_residue_names_get_class(name=resname.strip()) == "common_rna_dna":
            # rna/dna
            sequence.append(resname.strip())
            rgs.append(libtbx.group_args(i_seq=counter, rg=rg))
            counter += 1
    return "".join(sequence), rgs

  def output(self, lsq, filename):
    """Output PDB model to filename given transformation lsq."""
    output = self.pdb.atoms()
    output.set_xyz(lsq.r.elems * output.extract_xyz() + lsq.t.elems)
    output.set_uij(cctbx.array_family.flex.sym_mat3_double(output.size(), [-1,-1,-1,-1,-1,-1]))
    self.pdb.write_pdb_file(
      file_name=filename,
      crystal_symmetry=self.pdb_input.crystal_symmetry()
    )
    return 0.0 # Return RMSD to output # TODO

  @classmethod
  def output_merge(cls, instances, filename):
    """Merge multiple instances back into a single PDB file."""
    pdb = iotbx.pdb.hierarchy.root()
    for i in instances:
      pdb.append_model(i.get_transformed().detached_copy())
    pdb.write_pdb_file(file_name=filename)

class SuperposePDBSieve(SuperposePDB):
  def fit(self, sites_moving, sites_fixed):
    lsq = scitbx.math.superpose.sieve_fit(sites_fixed=sites_fixed, sites_moving=sites_moving)
    sites_moving2 = lsq.other_sites_best_fit()
    rmsd = sites_fixed.rms_difference(sites_moving2)
    return rmsd, lsq, sites_moving, sites_fixed

class SuperposePDBSCEDS(SuperposePDB):
  def fit(self, sites_moving, sites_fixed):
    raise NotImplemented

def run(args, command_name="phenix.superpose_pdbs", log=None):
  import mmtbx.utils
  quiet = False
  #if log is None:
  #  log = mmtbx.utils.set_log(args)
  # Process the parameters.
  params = iotbx.phil.parse(PHIL_PARAMS, process_includes=True)
  args = mmtbx.utils.process_command_line_args(args=args, master_params=params, suppress_symmetry_related_errors=True)
  params = args.params

  # Copy command line args back into PHIL.
  fixed, moving = None, None
  if len(args.pdb_file_names) >= 2:
    fixed, moving = args.pdb_file_names[0], args.pdb_file_names[1]
    sources = []
    parameter_interpreter = libtbx.phil.command_line.argument_interpreter(master_phil=params, home_scope=None)
    sources.append(parameter_interpreter.process(arg="pdb_file_name_fixed=%s"%fixed))
    sources.append(parameter_interpreter.process(arg="pdb_file_name_moving=%s"%moving))
    params, _ = params.fetch(sources=sources, track_unused_definitions=True)

  # Get input and output filenames.
  pe = params.extract()
  fixed, moving, output = pe.input.pdb_file_name_fixed, pe.input.pdb_file_name_moving, pe.output.file_name
  if fixed is None and moving is None:
    raise Sorry("Need two input PDB files.")

  # Information.
  params.show(out=log)

  print("\n===== Init =====")
  # The fixed model can only contain a single model.
  # It will raise an Exception if there is more than one!
  fixed = SuperposePDB(
    fixed,
    selection=pe.selection_fixed,
    preset=pe.selection_fixed_preset,
    log=log,
    quiet=quiet,
    desc=fixed
  )

  # The moving pdb can contain many models. These will each be aligned to the
  # fixed model and output as a separate file...
  moving_args = dict(
      selection=pe.selection_moving,
      preset=pe.selection_moving_preset,
      desc=moving,
      log=log,
      quiet=quiet
  )
  for count, moving in enumerate(SuperposePDB.open_models(moving, **moving_args)):
    print("\n===== Aligning %s to %s ====="%(moving.desc, fixed.desc))
    if not pe.selection_moving:
      moving.selectomatic(fixed)
    rmsd, lsq = moving.superpose(fixed)
    if output:
      filename = '%s-%s.pdb'%(output, count)
      print("\n===== Writing %s output to %s ====="%(moving.desc, filename))
      moving.output(lsq, filename=filename)

class launcher(libtbx.runtime_utils.target_with_save_result):
  def run(self):
    return run(args=list(self.args), log=sys.stdout)

def finish_job(result):
  if (result is not None):
    (output_file, rmsd) = result
    output_files = [(output_file, "Superposed model")]
    statistics = [("RMSD", format_value("%.3f", rmsd))]
    return (output_files, statistics)
  return ([], [])

def validate_params(params):
  if (params.input.pdb_file_name_fixed is None or
      params.input.pdb_file_name_moving is None):
    raise Sorry("One or both PDB files missing.")
  if params.output.file_name is None :
    raise Sorry("Please specify a name for the output file.")

if __name__ == "__main__":
  run(args=sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
mmtbx/xmanip.py
from __future__ import absolute_import, division, print_function
from mmtbx import xmanip_tasks
from iotbx import reflection_file_reader
from iotbx import reflection_file_utils
from iotbx import crystal_symmetry_from_any
import iotbx.phil
import iotbx.pdb
from cctbx import crystal
from cctbx import sgtbx
from cctbx import adptbx
from cctbx.array_family import flex
from scitbx.math import matrix
from libtbx.utils import Sorry, multi_out
from six.moves import cStringIO as StringIO
import sys, os
from six.moves import zip
from six.moves import range


class quick_rt_mx(object):
  from scitbx import matrix
  def __init__(self,
               r = [1,0,0,0,1,0,0,0,1] ,
               t = (0,0,0) ):
    self.rm = matrix.sqr(r)
    self.tv = flex.double(t)
  def r(self):
    return self.rm
  def t(self):
    return self.tv
  def inverse(self):
    tmp = quick_rt_mx(r=self.rm.inverse(), t=(-self.tv[0], -self.tv[1], -self.tv[2]) )
    return tmp
  def show(self,out=None):
    if out is None:
      out=sys.stdout
    print(file=out)
    print("R :", file=out)
    print(self.r().mathematica_form( one_row_per_line=True, format="%6.3f"), file=out)
    print("T :", file=out)
    for item in self.t():
      print("%6.3f"%(item), end=' ', file=out)
    print(file=out)
    print(file=out)

def construct_output_labels(labels, label_appendix, out=None ):
  if out is None:
    out = sys.stdout

  new_label_root = []
  standard_prefix = ["F","I","SIG", "HLA", "HL"]
  standard_postfix = ["(+)","(-)","PLUS", "MINUS","+","-","MINU" ]

  for label, app in zip(labels,label_appendix):
    if app is None:
      app=""
    #for each label, check if the prefix is present
    tmp_root = str(label[0])
    for post in standard_postfix:
      if tmp_root.endswith( post ):
        tmp_root = tmp_root[:len(post)-1]
        break

    for pre in standard_prefix:
      if tmp_root.startswith( pre ):
        tmp_root = tmp_root[len(pre)-1:]
        break

    if len(tmp_root)==0:
      tmp_root = label[0]

    if tmp_root[ len(tmp_root)-1 ]=="_":
      tmp_root = tmp_root[: len(tmp_root)-1 ]

    new_label_root.append( tmp_root+app )

  return new_label_root


def read_data(file_name, labels, xs, log=None):
  if log is None:
    log=sys.stdout
  if not os.path.isfile(file_name):
    raise Sorry("No such file: >%s<"%(file_name) )
  reflection_file = reflection_file_reader.any_reflection_file(
    file_name= file_name)
  miller_arrays = reflection_file.as_miller_arrays(crystal_symmetry=xs)
  label_table = reflection_file_utils.label_table(miller_arrays)
  return label_table.select_array(
    label=labels,
    command_line_switch="xray_data.labels",
    f=log)

def chain_name_modifier(chain_id, increment):
  ids=["A","B","C","D","E","F","G","H","I","J","K",
       "L","M","N","O","P","Q","R","S","T","U","V",
       "W","X","Y","Z","0","1","2","3","4","5","6",
       "7","8","9"]
  result=chain_id
  if chain_id in ids:
    new_index = (ids.index( chain_id ) + increment)%len(ids)
    result = ids[new_index]
  return result

def write_as_pdb_file(input_xray_structure = None,
                      input_crystal_symmetry = None,
                      input_pdb = None,
                      out = None,
                      chain_id_increment=5,
                      additional_remark=None,
                      print_cryst_and_scale=True):
  assert chain_id_increment is not None
  if out is None: out = sys.stdout

  xs = input_crystal_symmetry
  if xs is None: xs = input_xray_structure

  if additional_remark is not None:
    print("REMARK %s" % additional_remark, file=out)
  if print_cryst_and_scale:
    print("REMARK Space group: %s" % str(xs.space_group_info()), file=out)
  else:
    xs = None

  pdb_atoms = input_pdb.atoms()
  pdb_atoms.set_xyz(
    new_xyz=input_xray_structure.sites_cart())
  pdb_atoms.set_b(
    new_b=input_xray_structure.scatterers().extract_u_iso()
         *adptbx.u_as_b_factor)

  hierarchy = input_pdb.construct_hierarchy()
  for chain in hierarchy.chains():
    chain.id = chain_name_modifier(chain.id, chain_id_increment)

  print(hierarchy.as_pdb_string(crystal_symmetry=xs), file=out)

master_params = iotbx.phil.parse("""\
xmanip{
  input{
    unit_cell=None
    .type=unit_cell
    .help="Unit cell parameters"
    space_group=None
    .type=space_group
    .help="space group"
    xray_data
    .multiple=True
    .help="Scope defining xray data. Multiple scopes are allowed"
    {
      file_name=None
      .type=path
      .help="file name"
      labels=None
      .type=str
      .help="A unique label or unique substring of a label"
      label_appendix=None
      .type=str
      .help="Label appendix for output mtz file"
      name = None
      .type = str
      .help="An identifier of this particular miller array"
      write_out=None
      .type=bool
      .help="Determines if this data is written to the output file"

    }
    model
    .help="A model associated with the miller arrays. Only one model can be defined."
    {
      file_name=None
      .type=path
      .help="A model file"
    }
  }
  parameters{
    action = *reindex manipulate_pdb manipulate_miller
    .type=choice
    .help="Defines which action will be carried out."
    reindex
    .help="Reindexing parameters. Acts on coordinates and miller arrays."
    {
      standard_laws = niggli *reference_setting primitive_setting invert user_supplied
      .type=choice
      .help="Choices of reindexing operators. Will be applied on structure and miller arrays."
      user_supplied_law='h,k,l'
      .type=str
      .help="User supplied operator."
    }
    manipulate_miller
    .help="Acts on a single miller array or a set of miller arrays."
    {
      include scope mmtbx.xmanip_tasks.master_params
    }
    manipulate_pdb
    .help="Manipulate elements of a pdb file"
    {
      task = set_b apply_operator *None
      .type=choice
      .help="How to manipulate a pdb file"
      set_b{
        b_iso = 30
        .type=float
        .help="new B value for all atoms"
      }
      apply_operator{
        standard_operators = *user_supplied_operator user_supplied_cartesian_rotation_matrix
        .type=choice
        .help="Possible operators"
        user_supplied_operator = "x,y,z"
        .type=str
        .help="Actualy operator in x,y,z notation"
        user_supplied_cartesian_rotation_matrix
        .help="Rotation,translation matrix in cartesian frame"
        {
          r = None
          .type=str
          .help="Rotational part of operator"
          t = None
          .type = str
          .help="Translational part of operator"
        }
        invert = False
        .type = bool
        .help = "Invert operator given above before applying on coordinates"
        concatenate_model=False
        .type=bool
        .help="Determines if new chain is concatenated to old model"
        chain_id_increment=1
        .type=int
        .help="Cain id increment"
      }
    }
  }
  output
  .help="Output files"
  {
    logfile=xmanip.log
    .type=str
    .help="Logfile"
    hklout=xmanip.mtz
    .type=str
    .help="Ouptut miller indices and data"
    xyzout=xmanip.pdb
    .type=str
    .help="output PDB file"
  }
}
""", process_includes=True)

def print_help(command_name):
  print("""
#phil __OFF__
\t\t%s

A program for the manipulation of xray data objects (coordinates and reflection files).

The keywords are sumarized below:

#phil __ON__
xmanip {
  input {
    unit_cell = None
    space_group = None
    xray_data {
      file_name = None
      labels = None
      label_appendix = None
      name = None
      write_out = None
    }
    model {
      file_name = None
    }
  }
  parameters {
    action = reindex manipulate_pdb *manipulate_miller
    reindex {
      standard_laws = niggli *reference_setting invert user_supplied
      user_supplied_law = "h,k,l"
    }
    manipulate_miller {
      task = get_dano get_diso lsq_scale sfcalc *custom None
      output_label_root = "FMODEL"
      get_dano {
        input_data = None
      }
      get_diso {
        native = None
        derivative = None
        use_intensities = True
        use_weights = True
        scale_weight = True
      }
      lsq_scale {
        input_data_1 = None
        input_data_2 = None
        use_intensities = True
        use_weights = True
        scale_weight = True
      }
      sfcalc {
        fobs = None
        output = *2mFo-DFc mFo-DFc complex_fcalc abs_fcalc intensities
        use_bulk_and_scale = *as_estimated user_upplied
        bulk_and_scale_parameters {
          d_min = 2
          overall {
            b_cart {
              b_11 = 0
              b_22 = 0
              b_33 = 0
              b_12 = 0
              b_13 = 0
              b_23 = 0
            }
            k_overall = 0.1
          }
          solvent {
            k_sol = 0.3
            b_sol = 56
          }
        }
      }
      custom{
        code = print("hello world", file=out)
      }
    }
    manipulate_pdb{
      task = apply_operator *set_b
      apply_operator{
        operator = "x,y,z"
        invert=False
        concatenate_model=False
        chain_id_increment=1
      }
      set_b{
        b_iso = 30
      }
    }
  }
  output {
    logfile = "xmanip.log"
    hklout = "xmanip.mtz"
    xyzout = "xmanip.pdb"
  }
}
#phil __END__


Further details can be found in the documentation.

  """%(command_name))

def run(args, command_name="phenix.xmanip"):
  if (len(args) == 0 or "--help" in args or "--h" in args or "-h" in args):
    print_help(command_name=command_name)
  else:
    log = multi_out()
    if (not "--quiet" in args):
      log.register(label="stdout", file_object=sys.stdout)
    string_buffer = StringIO()
    string_buffer_plots = StringIO()
    log.register(label="log_buffer", file_object=string_buffer)

    phil_objects = []
    argument_interpreter = master_params.command_line_argument_interpreter(
      home_scope="map_coefs")

    print("#phil __OFF__", file=log)
    print("==========================", file=log)
    print("          XMANIP          ", file=log)
    print("reindexing and other tasks", file=log)
    print("==========================", file=log)
    print(file=log)


    for arg in args:
      command_line_params = None
      arg_is_processed = False
      # is it a file?
      if (os.path.isfile(arg)): ## is this a file name?
        # check if it is a phil file
        try:
          command_line_params = iotbx.phil.parse(file_name=arg)
          if command_line_params is not None:
            phil_objects.append(command_line_params)
            arg_is_processed = True
        except KeyboardInterrupt: raise
        except Exception : pass
      else:
        try:
          command_line_params = argument_interpreter.process(arg=arg)
          if command_line_params is not None:
            phil_objects.append(command_line_params)
            arg_is_processed = True
        except KeyboardInterrupt: raise
        except Exception : pass

      if not arg_is_processed:
        print("##----------------------------------------------##", file=log)
        print("## Unknown file or keyword:", arg, file=log)
        print("##----------------------------------------------##", file=log)
        print(file=log)
        raise Sorry("Unknown file or keyword: %s" % arg)

    effective_params = master_params.fetch(sources=phil_objects)
    params = effective_params.extract()

    # now get the unit cell from the files
    hkl_xs = []
    pdb_xs = None

    #multiple file names are allowed
    for xray_data in params.xmanip.input.xray_data:
      if xray_data.file_name is not None:
        hkl_xs.append( crystal_symmetry_from_any.extract_from(
           file_name=xray_data.file_name) )

    if params.xmanip.input.model.file_name is not None:
      pdb_xs = crystal_symmetry_from_any.extract_from(
        file_name=params.xmanip.input.model.file_name)

    phil_xs = crystal.symmetry(
      unit_cell=params.xmanip.input.unit_cell,
      space_group_info=params.xmanip.input.space_group  )

    combined_xs = crystal.select_crystal_symmetry(
      None,phil_xs, [pdb_xs],hkl_xs)
    if combined_xs is not None:
      # inject the unit cell and symmetry in the phil scope please
      params.xmanip.input.unit_cell = combined_xs.unit_cell()
      params.xmanip.input.space_group = \
        sgtbx.space_group_info( group = combined_xs.space_group() )

    print("#phil __ON__", file=log)
    new_params =  master_params.format(python_object=params)
    new_params.show(out=log)
    print("#phil __END__", file=log)

    if params.xmanip.input.unit_cell is None:
      raise Sorry("unit cell not specified")
    if params.xmanip.input.space_group is None:
      raise Sorry("space group not specified")

    #-----------------------------------------------------------
    #
    # step 1: read in the reflection file
    #

    miller_arrays = []
    labels = []
    label_appendix = []
    write_it = []
    names = {}

    if len(params.xmanip.input.xray_data)>0:

      phil_xs = crystal.symmetry(
        unit_cell=params.xmanip.input.unit_cell,
        space_group_info=params.xmanip.input.space_group  )

      xray_data_server =  reflection_file_utils.reflection_file_server(
        crystal_symmetry = phil_xs,
        force_symmetry = True,
        reflection_files=[])

      count=0
      for xray_data in params.xmanip.input.xray_data:
        if xray_data.file_name is not None:
          miller_array = None
          miller_array = read_data(xray_data.file_name,
                                   xray_data.labels,
                                   phil_xs)
          print(file=log)
          print("Summary info of observed data", file=log)
          print("=============================", file=log)
          if miller_array is None:
            raise Sorry("Failed to read data. see errors above" )
          miller_array.show_summary(f=log)
          print(file=log)

          miller_arrays.append( miller_array )
          labels.append( miller_array.info().labels )
          label_appendix.append( xray_data.label_appendix )

          this_name = "COL_"+str(count)
          if xray_data.name is not None:
            this_name = xray_data.name
          #check if this name is allready used
          if this_name in names:
            raise Sorry( "Non unique dataset name. Please change the input script" )
          names.update( {this_name:count} )
          count += 1

          write_it.append( xray_data.write_out)

      output_label_root = construct_output_labels( labels, label_appendix )
      for ii in range(len(labels)):
        test=0
        for jj in range( ii+1,len(labels) ):
          for lab_name1, lab_name2 in zip(labels[ii],labels[jj]):
            if lab_name1==lab_name2:
              test+=1
          if test == 2:
            print("\n***** You are trying to import the data with label(s) %s more then one time. ***** \n"%(str(labels[ii])), file=log)
      for ii in range(len(output_label_root)):
        for jj in range(ii+1,len(output_label_root)):
          if output_label_root[ii]==output_label_root[jj]:
            if write_it[ii]:
              if write_it[jj]:
                print("Output label roots:", file=log)
                print(output_label_root, file=log)
                raise Sorry( "Output labels are not unique. Modify input." )



    #----------------------------------------------------------------
    # Step 2: get an xray structure from the PDB file
    #
    pdb_model = None
    model = None
    if params.xmanip.input.model.file_name is not None:
      pdb_model = iotbx.pdb.input(
        file_name=params.xmanip.input.model.file_name)
      model = pdb_model.xray_structure_simple(crystal_symmetry=phil_xs)
      print("Atomic model summary", file=log)
      print("====================", file=log)
      model.show_summary(f=log)
      print(file=log)


    write_miller_array = False
    write_pdb_file = False
    # define some output holder thingamebobs
    new_miller_arrays = []
    new_model = None

    #manipulate miller arrays
    if params.xmanip.parameters.action == "manipulate_miller":
      write_miller_array = True
      new_miller = xmanip_tasks.manipulate_miller(names,
                                                  miller_arrays,
                                                  model,
                                                  params.xmanip.parameters.manipulate_miller,
                                                  log )
      miller_arrays.append( new_miller )
      # not very smart to rely here on a phil defintion defined in another file
      tmp_root = params.xmanip.parameters.manipulate_miller.output_label_root
      if tmp_root is None:
        tmp_root = "UNSPECIFIED"
      output_label_root.append( tmp_root )
      write_it.append(True)




    if params.xmanip.parameters.action=="reindex":
      write_miller_array = True
      #----------------------------------------------------------------
      # step 3: get the reindex laws
      phil_xs.show_summary()
      to_niggli    = phil_xs.change_of_basis_op_to_niggli_cell()
      to_reference = phil_xs.change_of_basis_op_to_reference_setting()
      to_inverse   = phil_xs.change_of_basis_op_to_inverse_hand()
      to_primitive = phil_xs.change_of_basis_op_to_primitive_setting()
      cb_op = None
      if (params.xmanip.parameters.reindex.standard_laws == "niggli"):
        cb_op = to_niggli
      if (params.xmanip.parameters.reindex.standard_laws == "reference_setting"):
        cb_op = to_reference
      if (params.xmanip.parameters.reindex.standard_laws == "invert"):
        cb_op = to_inverse
      if (params.xmanip.parameters.reindex.standard_laws == "user_supplied"):
        cb_op = sgtbx.change_of_basis_op( params.xmanip.parameters.reindex.user_supplied_law )
      if (params.xmanip.parameters.reindex.standard_laws == "primitive_setting"):
        cb_op = to_primitive


      if cb_op is None:
        raise Sorry("No change of basis operation is supplied.")

      print("Supplied reindexing law:", file=log)
      print("========================", file=log)
      print("hkl notation: ", cb_op.as_hkl(), file=log)
      print("xyz notation: ", cb_op.as_xyz(), file=log)
      print("abc notation: ", cb_op.as_abc(), file=log)
      #----------------------------------------------------------------
      # step 4: do the reindexing
      #
      # step 4a: first do the miller array object
      #new_miller_arrays = []
      for miller_array in miller_arrays:
        new_miller_array = None
        if miller_array is not None:
          new_miller_array = miller_array.change_basis( cb_op )
          new_miller_arrays.append( new_miller_array )
      #
      # step 4b: the xray structure
      if pdb_model is not None:
        write_pdb_file=True
        new_model = model.change_basis( cb_op )


    if write_miller_array:
      if len(new_miller_arrays)==0:
        new_miller_arrays = miller_arrays
      #----------------------------------------------------------------
      print(file=log)
      print("The data has been reindexed/manipulated", file=log)
      print("--------------------------------------", file=log)
      print(file=log)
      print("Writing output files....", file=log)

      mtz_dataset=None
      if len(new_miller_arrays)>0:
        first=0
        for item in range(len(write_it)):
          if write_it[item]:
            first=item
            if new_miller_arrays[ first ] is not None:
              break

        if new_miller_arrays[first] is not None:
          tmp = new_miller_arrays[first].map_to_asu()
          mtz_dataset = tmp.as_mtz_dataset(
            column_root_label=output_label_root[first])

      if mtz_dataset is not None:
        for miller_array, new_root in zip(new_miller_arrays[first+1:],
                                          output_label_root[first+1:]):
          if miller_array is not None:
            mtz_dataset = mtz_dataset.add_miller_array(
              miller_array = miller_array,
              column_root_label = new_root)

        print("Writing mtz file with name %s"%(params.xmanip.output.hklout), file=log)
        mtz_dataset.mtz_object().write(
          file_name=params.xmanip.output.hklout)

      #step 5b: write the new pdb file
      if new_model is not None:
        pdb_file = open( params.xmanip.output.xyzout, 'w')
        print("Wring pdb file to: %s"%(params.xmanip.output.xyzout), file=log)
        write_as_pdb_file(
          input_pdb = pdb_model,
          input_xray_structure = new_model,
          out = pdb_file,
          chain_id_increment= 0,
          additional_remark = "Generated by %s" % command_name)

        pdb_file.close()
      if ( [miller_array,new_model]).count(None)==2:
        print("No input reflection of coordinate files have been given", file=log)

    if params.xmanip.parameters.action=="manipulate_pdb":
      if params.xmanip.parameters.manipulate_pdb.task == "apply_operator":
        rt_mx = None
        if params.xmanip.parameters.manipulate_pdb.apply_operator.standard_operators == "user_supplied_operator":
          rt_mx = sgtbx.rt_mx(
            params.xmanip.parameters.manipulate_pdb.apply_operator.user_supplied_operator,t_den=12*8 )
          print("Applied operator : ", rt_mx.as_xyz(), file=log)
        if params.xmanip.parameters.manipulate_pdb.apply_operator.standard_operators == \
             "user_supplied_cartesian_rotation_matrix":
          rt = params.xmanip.parameters.manipulate_pdb.apply_operator.user_supplied_cartesian_rotation_matrix
          tmp_r=None
          tmp_t=None
          if "," in rt.r:
            tmp_r = rt.r.split(',')
          else:
            tmp_r = rt.r.split(' ')
          if "," in rt.r:
            tmp_t = rt.t.split(',')
          else:
            tmp_t = rt.t.split(' ')
          tmp_tmp_r=[]
          tmp_tmp_t=[]
          for item in tmp_r:
            tmp_tmp_r.append( float(item) )
          if len(tmp_tmp_r)!=9:
            raise Sorry("Invalid rotation matrix. Please check input: %s"%(rt.r) )
          for item in tmp_t:
            tmp_tmp_t.append( float(item) )
          if len(tmp_tmp_t)!=3:
            raise Sorry("Invalid translational vector. Please check input: %s"%(rt.t) )
          tmp_tmp_t = (tmp_tmp_t)
          rt_mx = quick_rt_mx(tmp_tmp_r, tmp_tmp_t)
          print("User supplied cartesian matrix and vector: ", file=log)
          rt_mx.show()
          o = matrix.sqr(model.unit_cell().orthogonalization_matrix())
          tmp_r = o.inverse()*rt_mx.r()*o
          tmp_t = o.inverse()*matrix.col(list(rt_mx.t()))
          print(file=log)
          print("Operator in fractional coordinates: ", file=log)
          rt_mx = quick_rt_mx(r=tmp_r.as_float(), t=list(tmp_t))
          rt_mx.show(out=log)
          print(file=log)


        if params.xmanip.parameters.manipulate_pdb.apply_operator.invert:
          rt_mx = rt_mx.inverse()
          print(file=log)
          print("Taking inverse of given operator", file=log)
          print(file=log)

        sites = model.sites_frac()
        new_sites = flex.vec3_double()
        for site in sites:
          new_site = rt_mx.r()*matrix.col(site)
          new_site = flex.double(new_site)+flex.double( rt_mx.t().as_double() )
          new_sites.append( tuple(new_site) )
        new_model = model.deep_copy_scatterers()

        new_model.set_sites_frac( new_sites )
        # write the new [pdb file please
        pdb_file = open( params.xmanip.output.xyzout, 'w')
        print("Wring pdb file to: %s"%(params.xmanip.output.xyzout), file=log)
        if params.xmanip.parameters.manipulate_pdb.apply_operator.concatenate_model:
          write_as_pdb_file( input_pdb = pdb_model,
                             input_xray_structure = model,
                             out = pdb_file,
                             chain_id_increment = 0,
                             additional_remark = None,
                             print_cryst_and_scale=True )

        write_as_pdb_file( input_pdb = pdb_model,
                           input_xray_structure = new_model,
                           out = pdb_file,
                           chain_id_increment = params.xmanip.parameters.manipulate_pdb.apply_operator.chain_id_increment,
                           additional_remark = None,
                           print_cryst_and_scale=False )

        pdb_file.close()

      if params.xmanip.parameters.manipulate_pdb.task =="set_b":
        #rest all the b values
        if params.xmanip.parameters.manipulate_pdb.set_b:
          b_iso = params.xmanip.parameters.manipulate_pdb.b_iso
          new_model = model.set_b_iso( value = b_iso )
          print(file=log)
          print("All B-values have been set to %5.3f"%(b_iso), file=log)
          print("Writing PDB file %s"%(params.xmanip.output.xyzout), file=log)
          print(file=log)

        pdb_file = open( params.xmanip.output.xyzout, 'w')
        write_as_pdb_file( input_pdb = pdb_model,
                           input_xray_structure = new_model,
                           out = pdb_file,
                           chain_id_increment = 0,
                           additional_remark = None,
                           print_cryst_and_scale=True)
        pdb_file.close()

    #write the logfile
    logger = open( params.xmanip.output.logfile, 'w')
    print("Writing log file with name %s  "%(params.xmanip.output.logfile), file=log)
    print(string_buffer.getvalue()[0:len(string_buffer.getvalue())-1], file=logger) #avoid a newline at the end ...
    logger.close()


 *******************************************************************************


 *******************************************************************************
mmtbx/xmanip_tasks.py
from __future__ import absolute_import, division, print_function
from cctbx import miller
import cctbx.xray.structure_factors
from libtbx.utils import Sorry
import iotbx.phil
from iotbx.pdb import xray_structure
from mmtbx.scaling import fa_estimation, pair_analyses, relative_scaling
import sys
from six.moves import zip
from six.moves import range

master_params = iotbx.phil.parse("""
      task = *get_dano get_diso lsq_scale sfcalc custom None
      .type=choice
      .help="Possible tasks"
      output_label_root=None
      .type=str
      .help="Output label root"
      get_dano
      .help="Get ||F+| - |F-|| from input data."
      {
        input_data = None
        .type=str
      }

      get_diso
      .help="Get |Fder|-|Fnat|"
      {
        native = None
        .type=str
        .help="Name of native data"
        derivative = None
        .type=str
        .help="Name of derivative data"
        use_intensities=True
        .type=bool
        .help="Scale on intensities"
        use_weights=True
        .type=bool
        .help="Use experimental sigmas as weights in scaling"
        scale_weight=True
        .type=bool
        .help="Whether or not to scale the sigmas during scaling"
      }

      lsq_scale{
        input_data_1 = None
        .type=str
        .help="Reference data"
        input_data_2 = None
        .type=str
        .help="Data to be scaled"
        use_intensities=True
        .type=bool
        .help="Scale on intensities"
        use_weights=True
        .type=bool
        .help="Use experimental sigmas as weights in scaling"
        scale_weight=True
        .type=bool
        .help="Whether or not to scale the sigmas during scaling"
      }

      sfcalc{
        fobs = None
        .type=str
        .help = "Data name of observed data"
        output = 2mFo-DFc mFo-DFc *complex_fcalc abs_fcalc intensities
        .type=choice
        .help="Output coefficients"
        use_bulk_and_scale = *as_estimated user_upplied
        .type=choice
        .help = "estimate or use parameters given by user"
        bulk_and_scale_parameters
        .help = "Parameters used in the structure factor calculation. Ignored if experimental data is given"
        {
          d_min = 2.0
          .type=float
          .help = "resolution of the data to be calculated."
          overall
          .help = "Bulk solvent and scaling parameters"
          {
            b_cart
            .help = "Anisotropic B values"
            {
              b_11 = 0
              .type=float
              b_22 = 0
              .type=float
              b_33 = 0
              .type=float
              b_12 = 0
              .type=float
              b_13 = 0
              .type=float
              b_23 = 0
              .type=float
            }
            k_overall = 0.1
            .type=float
            .help = "Overall scalar"
          }
          solvent
          .help = "Solvent parameters"
          {
            k_sol = 0.3
            .type=float
            .help="Solvent scale"
            b_sol = 56.0
            .type=float
            .help="Solvent B"
          }
        }
      }

     custom
     .help = "A custom script that uses miller_array data names as variables."
     {
       code = None
       .help = "A piece of python code"
       .type=str
       show_instructions = True
       .help = "Some instructions"
       .type = bool
     }


      """)


def patch_miller_arrays_as_names( names ):
  result = []
  for name, number in zip(names, range(len(names))):
    tmp_result = "%s =  miller_arrays[ %i ].deep_copy()"%(name,number)
    result.append( compile( tmp_result, '<string>', 'exec' )  )

  return result



def get_dano(names, miller_arrays, xray_structure, parameters, out ):
  miller_array = None
  if parameters.input_data is None:
    if len(miller_arrays)==1:
      miller_array = miller_arrays[0]
  else:
    if parameters.input_data in names:
      miller_array = miller_arrays[ names[ parameters.input_data ] ]
    else:
      raise Sorry("Unknown data name.")

  if miller_array.is_xray_intensity_array():
    miller_array = miller_array.f_sq_as_f()
  assert miller_array.is_xray_amplitude_array()



  pair_generator = fa_estimation.ano_scaling( miller_array )
  plus  = pair_generator.x1p.deep_copy()
  minus = pair_generator.x1n.deep_copy()
  delta_gen = pair_analyses.delta_generator( plus,
                                             minus )
  deltas = delta_gen.abs_delta_f.deep_copy()
  return deltas

def get_diso(names, miller_arrays, xray_structure, parameters, out):
  #first scale please
  if parameters.native is None:
    raise Sorry("Please define native data name")
  if parameters.derivative is None:
    raise Sorry("Please define derivative data name")

  native=None
  derivative=None

  if parameters.native in names:
    native = miller_arrays[ names[parameters.native] ].deep_copy()
  else:
    raise Sorry("Unknown data name: >>%s<<"%(parameters.native) )

  if parameters.derivative in names:
    derivative = miller_arrays[ names[parameters.derivative] ].deep_copy()
  else:
    raise Sorry("Unknown data name: >>%s<<"%(parameters.derivative) )

  scaler = relative_scaling.ls_rel_scale_driver(
    miller_native     = native,
    miller_derivative = derivative,
    use_intensities   = parameters.use_intensities,
    scale_weight      = parameters.scale_weight,
    use_weights       = parameters.use_weights)
  #
  scaler.show(out=out)

  if native.is_xray_intensity_array():
    native = native.f_sq_as_f()
  if derivative.is_xray_intensity_array():
    derivative = derivative.f_sq_as_f()

  delta_gen = pair_analyses.delta_generator( derivative,
                                             native )
  deltas = delta_gen.delta_f.deep_copy()
  return deltas

def lsq_scale(names, miller_arrays, xray_structure, parameters, out):
  if parameters.input_data_1 is None:
    raise Sorry("Please define input_data_1")
  if parameters.input_data_2 is None:
    raise Sorry("Please define input_data_2")

  input_data_1 = None
  input_data_2 = None

  if parameters.input_data_1 in names:
    input_data_1 = miller_arrays[ names[parameters.input_data_1] ].deep_copy()
  else:
    raise Sorry("Unknown data name: >>%s<<"%(parameters.input_data_1) )

  if parameters.input_data_2 in names:
    input_data_2 = miller_arrays[ names[parameters.input_data_2] ].deep_copy()
  else:
    raise Sorry("Unknown data name: >>%s<<"%(parameters.input_data_2) )

  scaler = relative_scaling.ls_rel_scale_driver(
    miller_native     = input_data_1,
    miller_derivative = input_data_2,
    use_intensities   = parameters.use_intensities,
    scale_weight      = parameters.scale_weight,
    use_weights       = parameters.use_weights)
  #
  scaler.show(out=out)
  return scaler.scaled_original_derivative.deep_copy()


def sfcalc(names, miller_arrays, xray_structure, parameters, out):
  from mmtbx import f_model
  f_obs = None
  if parameters.fobs is None:
    if parameters.output not in ["complex_fcalc", "abs_fcalc", "intensities" ]:
      raise Sorry("Experimental data is needed for %s coefficients.\n Please supply Fobs")
    else:
      f_obs = abs(xray_structure.structure_factors(
        d_min          = parameters.bulk_and_scale_parameters.d_min,
        anomalous_flag = False).f_calc())
  else:
    f_obs = miller_arrays[ names[parameters.fobs] ].deep_copy()

  if f_obs.is_xray_intensity_array():
    f_obs = f_obs.f_sq_as_f()

  flags = f_obs.generate_r_free_flags(fraction = 0.1,
                                      max_free = 99999999)
  b_cart = [parameters.bulk_and_scale_parameters.overall.b_cart.b_11,
            parameters.bulk_and_scale_parameters.overall.b_cart.b_22,
            parameters.bulk_and_scale_parameters.overall.b_cart.b_33,
            parameters.bulk_and_scale_parameters.overall.b_cart.b_12,
            parameters.bulk_and_scale_parameters.overall.b_cart.b_13,
            parameters.bulk_and_scale_parameters.overall.b_cart.b_23 ]

  fmodel = f_model.manager( xray_structure   = xray_structure,
                            r_free_flags     = flags,
                            target_name      = "ls_wunit_k1",
                            f_obs            = f_obs,
                            b_cart           = b_cart,
                            k_sol            = parameters.bulk_and_scale_parameters.solvent.k_sol,
                            b_sol            = parameters.bulk_and_scale_parameters.solvent.b_sol)

  if parameters.use_bulk_and_scale == "as_estimated":
    if parameters.fobs is not None:
      fmodel.update_all_scales(log=out)

  result = None
  if parameters.output in  ["complex_fcalc", "abs_fcalc", "intensities" ]:
    result = fmodel.f_model()
    if parameters.output == "complex_fcalc":
      result = result
    if parameters.output == "abs_fcalc":
      result = abs( result )
    if parameters.output == "intensities":
      result = abs(result).f_as_f_sq()
  else:
    if parameters.output == "2mFo-DFc":
      result = fmodel.electron_density_map().map_coefficients(map_type = "2m*Fobs-D*Fmodel")
    if parameters.output == "mFo-DFc":
      # XXX BUG ?
      result = fmodel.electron_density_map().map_coefficients(map_type = "2m*Fobs-D*Fmodel")

  assert result is not None
  return result



def show_restricted_custom_names(restricted_names, out):
  print("Restricted data set names are:", file=out)
  for rn in restricted_names:
    print("    -   %s"%(rn), file=out)

def print_custom_instructions(out):
  print("The custom function in the manipulate miller task of xmanip allows one to submit a small (or large)", file=out)
  print("snippet of python code, have it executed and have a single miller array returned and written to file.", file=out)
  print("If one is familiar with python and the cctbx in general, this function allows one to quickly perform", file=out)
  print("complex tasks relating reflection files without having the overhead of writing a user interface.", file=out)
  print("Data set names given to the miller arrays in the main (user specified) input file, are actual variable names", file=out)
  print("and are stored as a cctbx.miller.array object. A pdb file that was read in, is stored in the object named ", file=out)
  print("xray_structure. Note that not many safeguards are in place: make sure your code snippet is proper python!", file=out)
  print("Please note that there are some restriction on variable names: the should not contains spaces or have the name", file=out)
  print("of local variables or functions. By default, a variable named 'result' is returned", file=out)



def custom(names, miller_arrays, xray_structure, params, out):

  restricted_names = [ "restricted_names", "names", "miller_arrays", "params", "out",
                       "get_dano", "get_diso", "custom", "sfalc", "patch_miller_arrays_as_names",
                       "lsq_scale", "manipulate_miller", "show_restricted_custom_names", "print_custom_instructions" ]

  if params.show_instructions:
    print_custom_instructions(out)
    show_restricted_custom_names(restricted_names, out)


  #check if all variable names are legal
  for name in names:
    if " " in name:
      raise Sorry("Sorry, no spaces allowed in data set name >%s< to avoid compilation problems."%(name) )
    if name in restricted_names:
      show_restricted_custom_names( restricted_names )
      raise Sorry("The data set name >%s< is restricted to avoid compilation problems." %(name) )

  #first make variables from the names please
  tmp_names = patch_miller_arrays_as_names(names)
  for instruction in tmp_names:
    exec(instruction)
  result = None
  # now we have to evaulate the code
  print("Trying to evaluate the code as shown below", file=out)
  print("------------------------------------------", file=out)
  print(params.code, file=out)
  print("------------------------------------------", file=out)
  user_code = compile( params.code, '<string>', 'exec' )
  exec(user_code)

  return result


def manipulate_miller(names, miller_arrays, xray_structure, params, out=None):
  if out is None:
    out = sys.stdout
  #define a number of function pointers
  function_pointer = {
                       "get_dano" : get_dano,
                       "get_diso" : get_diso,
                       "lsq_scale": lsq_scale,
                       "sfcalc"   : sfcalc,
                       "custom"   : custom,
                     }

  #Now pay attention please
  function_arguments = None
  #these two lines allow me quickly lift the appropriate set of
  #parameters from the file scope without a lengthy set of if statements
  # patch = compile("function_arguments = params.%s"%(params.task),'<string>','exec' )
  # exec(patch)

  # This is how it can be done, compatible between Py2/Py3
  function_arguments = getattr(params, params.task)
  result = function_pointer[ params.task ]( names,
                                            miller_arrays,
                                            xray_structure,
                                            function_arguments,
                                            out)
  return result


 *******************************************************************************
