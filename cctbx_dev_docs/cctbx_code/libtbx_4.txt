

 *******************************************************************************
libtbx/phil/tst_experimental.py
from __future__ import absolute_import, division, print_function

import libtbx.phil
from libtbx.phil import experimental

def exercise():
  master_phil = libtbx.phil.parse("""
first_prop = 0
  .type = int
second_prop = "default text"
  .type = str
first_scope
  .multiple = True
  .optional = True
{
  key = None
    .type = int
    .optional = False
  number = None
    .type = int
    .optional = False
  text = "Default first scope text"
    .type = str
    .optional = False
  second_scope
    .multiple = True
    .optional = True
  {
    key = None
      .type = int
      .optional = False
    flag = None
      .type = bool
    list = None
      .type = floats(size=2)
  }
}
""")

  default_str = """
first_scope {
  key = 1
  number = 1.0
  text = "First key 1 text"
  second_scope {
    key = 0
    flag = False
  }
  second_scope {
    key = 1
    list = [0, 0]
  }
  second_scope {
    key = 2
  }
}
first_scope {
  key = 2
  text = "First key 2 text"
  second_scope {
    key = 0
    flag = True
    list = [0, 1]
  }
}
"""

  overlay_str = """
first_prop = 1
first_scope {
  key = 1
  number = 2
  second_scope {
    key = 0
    list = [1, 0]
  }
  second_scope {
    key = 1
    flag = False
    list = [1, 1]
  }
  second_scope {
    key = 4
    list = [0, 2]
  }
}
"""

  # XXX Correct use of _phil and _params?
  default_phil = master_phil.fetch(sources=[libtbx.phil.parse(default_str)])
  default_params = default_phil.extract()

  overlay_phil = master_phil.fetch(sources=[libtbx.phil.parse(overlay_str)])
  overlay_params = overlay_phil.extract()

  experimental.merge_params_by_key(default_params, overlay_params, "key")

  assert default_params.first_prop == 1
  assert default_params.second_prop == "default text"
  assert len(default_params.first_scope) == 2

  for fs in default_params.first_scope:
    if (fs.key == 1):
      assert fs.number == 2 and fs.text == "Default first scope text"
      assert len(fs.second_scope) == 4
      for ss in fs.second_scope:
        if (ss.key == 0):
          assert ss.flag == False and ss.list == [1, 0]
        elif (ss.key == 1):
          assert ss.flag == False and ss.list == [1, 1]
        elif (ss.key == 2):
          assert ss.flag is None and ss.list is None
        elif (ss.key == 4):
          assert ss.flag is None and ss.list == [0, 2]

    elif (fs.key == 2):
      assert fs.number is None and fs.text == "First key 2 text"
      assert len(fs.second_scope) == 1
      assert fs.second_scope[0].key == 0 and \
          fs.second_scope[0].flag == True and \
          fs.second_scope[0].list == [0, 1]


if (__name__ == "__main__"):
  exercise()
  print("OK")


 *******************************************************************************


 *******************************************************************************
libtbx/phil/tst_interface.py

from __future__ import absolute_import, division, print_function
from libtbx.test_utils import show_diff, Exception_expected
from libtbx.phil import interface
import libtbx.load_env
import libtbx.phil
from six.moves import cStringIO as StringIO
import sys

def exercise():
  master_phil = libtbx.phil.parse("""
refinement {
  input {
    pdb {
      file_name = None
        .type = path
        .multiple = True
    }
    sequence = None
      .type = path
      .style = seq_file
  }
  refine {
    strategy = *individual_sites *individual_adp *occupancies tls rigid_body
      .type = choice(multi=True)
    adp {
      tls = None
        .type = str
        .multiple = True
        .help = Selection for TLS group
    }
  }
  main {
    ncs = False
      .type = bool
      .help = This turns on NCS restraints
    ordered_solvent = False
      .type = bool
    number_of_macro_cycles = 3
      .type = int
    ias = False
      .type = bool
  }
  developer
    .expert_level = 3
  {
    place_elemental_ions = False
      .type = bool
  }
  gui {
    include scope libtbx.phil.interface.tracking_params
    output_dir = None
      .type = path
      .style = output_dir
  }
}
""", process_includes=True)
  refine_phil1 = libtbx.phil.parse("""
refinement {
  input {
    pdb {
      file_name = protein.pdb
      file_name = ligand.pdb
    }
  }
  refine {
    adp {
      tls = "chain A"
      tls = "chain B"
    }
  }
  main {
    ncs = True
    ordered_solvent = True
  }
}
""")
  refine_phil2_str = """
refinement {
  input {
    pdb {
      file_name = model1.pdb
    }
  }
  main {
    ncs = True
    ordered_solvent = False
  }
}"""
  refine_phil3 = libtbx.phil.parse("refinement.main.number_of_macro_cycles=5")
  refine_phil4_str = """
refinement.refine.adp.tls = None
"""
  i = libtbx.phil.interface.index(master_phil=master_phil,
    working_phil=refine_phil1,
    fetch_new=True)
  params = i.get_python_object()
  i.update(refine_phil2_str)
  # object retrieval
  pdb_phil = i.get_scope_by_name("refinement.input.pdb.file_name")
  assert len(pdb_phil) == 1
  os_phil = i.get_scope_by_name("refinement.main.ordered_solvent")
  assert os_phil.full_path() == "refinement.main.ordered_solvent"
  os_param = os_phil.extract()
  assert os_param == False
  params = i.get_python_object()
  assert len(params.refinement.refine.adp.tls) == 2
  # more updating, object extraction
  i.merge_phil(phil_object=refine_phil3)
  params = i.get_python_object()
  assert params.refinement.main.ncs == True
  assert params.refinement.main.ordered_solvent == False
  assert params.refinement.main.number_of_macro_cycles == 5
  assert params.refinement.input.pdb.file_name == ["model1.pdb"]
  i.merge_phil(phil_string=refine_phil4_str)
  params = i.get_python_object()
  assert len(params.refinement.refine.adp.tls) == 0
  phil1 = libtbx.phil.parse("""refinement.refine.strategy = *tls""")
  phil2 = libtbx.phil.parse("""refinement.input.pdb.file_name = ligand2.pdb""")
  i.save_param_file(
    file_name="tst_params.eff",
    sources=[phil1, phil2],
    extra_phil="refinement.main.ias = True",
    diff_only=True)
  params = i.get_python_from_file("tst_params.eff")
  assert params.refinement.refine.strategy == ["tls"]
  assert params.refinement.input.pdb.file_name == ["model1.pdb","ligand2.pdb"]
  assert params.refinement.main.ias == True
  i2 = i.copy(preserve_changes=False)
  params2 = i2.get_python_object()
  assert not params2.refinement.main.ncs
  i3 = i.copy(preserve_changes=True)
  params3 = i3.get_python_object()
  assert params3.refinement.main.ncs == True
  seq_file_def = i.get_seq_file_def_name()
  assert (seq_file_def == "refinement.input.sequence")

  # text searching (we can assume this will break quickly, but easily checked
  # by uncommenting the print statements)
  names = i.search_phil_text("macro_cycles", phil_name_only=True)
  assert len(names) == 1
  names = i.search_phil_text("elemental")
  assert (len(names) == 1)
  i.update("refinement.gui.output_dir=/var/tmp")
  i.update("refinement.gui.job_title=\"Hello, world!\"")
  assert (i.get_output_dir() == "/var/tmp")
  assert (i.get_job_title() == "Hello, world!")

  assert (libtbx.phil.interface.get_adjoining_phil_path(
    "refinement.input.xray_data.file_name", "labels") ==
    "refinement.input.xray_data.labels")

  master_phil = libtbx.phil.parse("""
pdb_in = None
  .type = path
  .short_caption = Input model
  .style = file_type:pdb input_file
pdb_out = None
  .type = path
  .style = file_type:pdb new_file
""")
  working_phil = master_phil.fetch(source=libtbx.phil.parse("""
pdb_in = foo.pdb
pdb_out = foo.modified.pdb
"""))
  i = libtbx.phil.interface.index(master_phil=master_phil,
    working_phil=working_phil,
    fetch_new=False)
  pdb_map = i.get_file_type_map("pdb")
  assert (pdb_map.get_param_names() == ['pdb_in'])
  assert (i.get_input_files() == [('foo.pdb', 'Input model', 'pdb_in')])
  # test captions
  master_phil = libtbx.phil.parse("""
my_options {
opt1 = *foo bar
  .type = choice
  .caption = Foo Bar
opt2 = *two_fofc fofc
  .type = choice(multi=True)
  .caption = 2mFo-DFc
}
""")
  try :
    libtbx.phil.interface.validate_choice_captions(master_phil)
  except AssertionError as e :
    assert (str(e) == "my_options.opt2")
  else :
    raise Exception_expected

# XXX sorry about the cross-import here, but I really need to test this on
# something large and complex
def exercise_2(verbose=False):
  if (not libtbx.env.has_module(name="phenix")):
    print("phenix module not available: skipping advanced tests")
    return
  from phenix.refinement import runtime
  import iotbx.phil
  from time import time
  phil_str = """
refinement.pdb_interpretation.secondary_structure.protein {
  helix {
    selection = "chain A and resseq 10:20"
  }
  helix {
    selection = "chain A and resseq 30:40"
  }
  helix {
    selection = "chain A and resseq 50:60"
  }
}
"""

  phil_str_2 = """
refinement.pdb_interpretation.secondary_structure.protein {
  helix {
    selection = "chain B and resseq 10:20"
  }
  helix {
    selection = "chain B and resseq 30:40"
  }
  helix {
    selection = "chain B and resseq 50:60"
  }
}
"""

  master_phil = runtime.master_phil()
  for phil_object in master_phil.objects:
    if phil_object.name == 'data_manager':
      master_phil.objects.remove(phil_object)
  i = interface.index(master_phil=master_phil,
    parse=iotbx.phil.parse)
  t1 = time()
  i.merge_phil(phil_string=phil_str)
  t2 = time()
  params = i.get_python_object()
  assert (params.refinement.pdb_interpretation.secondary_structure.\
      protein.helix[0].selection == "chain A and resseq 10:20")
  t3 = time()
  i.merge_phil(phil_string=phil_str_2,
    only_scope="refinement.pdb_interpretation.secondary_structure")
  t4 = time()
  params = i.get_python_object()
  assert (params.refinement.pdb_interpretation.secondary_structure.\
      protein.helix[0].selection == "chain B and resseq 10:20")
  scope = i.get_scope_by_name("refinement.pdb_interpretation.secondary_structure")
  params2 = scope.extract()
  assert (params2.protein.helix[0].selection == "chain B and resseq 10:20")
  if verbose :
    print("Merge with global fetch: %6.1fms" % ((t2-t1) * 1000))
    print("Merge with local fetch:  %6.1fms" % ((t4-t3) * 1000))
  i.merge_phil(phil_string="""
refinement.gui.migration.refinement.input.pdb.file_name = protein.pdb
refinement.gui.migration.refinement.input.pdb.file_name = ligand.pdb
refinement.gui.migration.refinement.input.monomers.file_name = ligand.cif
refinement.output.job_title = Test refinement run
""")
  names = i.search_phil_text("CIF")
  assert (set(names) == {
    'refinement.output.write_model_cif_file',
    'refinement.output.write_reflection_cif_file',
    'refinement.gui.migration.refinement.input.monomers.file_name',}
)

  expected_result = [
    ('protein.pdb', 'Input model (X-ray)', 'refinement.gui.migration.refinement.input.pdb.file_name'),
    ('ligand.pdb', 'Input model (X-ray)', 'refinement.gui.migration.refinement.input.pdb.file_name'),
    ('ligand.cif', 'Restraints (CIF)', 'refinement.gui.migration.refinement.input.monomers.file_name')]
  for f in i.get_input_files():
    assert f in expected_result
  assert len(i.get_input_files()) == len(expected_result)
  assert (i.get_job_title() == "Test refinement run")
  #
  # .style processing
  style = i.get_scope_style("refinement.refine.strategy")
  #assert (style.auto_launch_dialog == [
  #  'refinement.refine.sites.individual', 'refinement.refine.sites.individual',
  #  'refinement.refine.sites.rigid_body', 'refinement.refine.adp.individual',
  #  'refinement.refine.adp.group', 'refinement.refine.adp.tls',
  #  'refinement.refine.occupancies', 'refinement.refine.anomalous_scatterers'])
  assert (style.file_type is None)
  style = i.get_scope_style("refinement.gui.migration.refinement.input.xray_data.file_name")
  assert (style.get_list("file_type") == ["hkl"])
  assert (style.get_child_params() == {'fobs': 'labels',
    'd_max': 'low_resolution', 'd_min': 'high_resolution',
    'rfree_file': 'r_free_flags.file_name'})
  assert i.is_list_type("refinement.gui.migration.refinement.input.xray_data.labels")
  style = i.get_scope_style("refinement.gui.migration.refinement.input.xray_data.labels")
  assert (style.get_parent_params() == {"file_name" : "file_name"})
  file_map = i.get_file_type_map("pdb")
  expected_result = ['refinement.gui.migration.refinement.input.pdb.file_name',
                     'refinement.gui.migration.refinement.input.pdb.electron_file_name',
                     'refinement.gui.migration.refinement.input.pdb.neutron_file_name',
                     'refinement.reference_model.file']
  for p in file_map.get_multiple_params():
    assert p in expected_result, (p, expected_result)
  assert len(file_map.get_multiple_params()) == len(expected_result)
  assert (file_map.get_default_param() == "refinement.gui.migration.refinement.input.pdb.file_name")
  file_map = i.get_file_type_map("hkl")
  assert (file_map.get_overall_max_count() == 7)
  assert (len(file_map.get_multiple_params()) == 0)
  assert (file_map.get_max_count("refinement.gui.migration.refinement.input.xray_data.file_name") == 1)
  menu = i.get_menu_db()
  assert (len(menu.get_items()) > 15) # XXX ballpark (currently 17)
  submenu = menu.get_submenu("Atom_selections")
  assert (str(submenu.get_items()[0]) == "refinement.refine.sites")

def exercise_3():
  if (not libtbx.env.has_module(name="phaser")):
    print("phaser module not available: skipping advanced tests")
    return
  import iotbx.phil
  import phaser.phenix_interface
  master_phil = phaser.phenix_interface.master_phil()
  i = interface.index(master_phil=master_phil,
    parse=iotbx.phil.parse)
  i.merge_phil(phil_string="""\
phaser {
  hklin = "/Users/nat/Documents/beta-blip/beta_blip_P3221.mtz"
  labin = Fobs,Sigma
  composition {
    chain {
      sequence_file = "/Users/nat/Documents/beta-blip/beta.seq"
    }
    chain {
      sequence_file = "/Users/nat/Documents/beta-blip/blip.seq"
    }
  }
  ensemble {
    model_id = "beta"
    coordinates {
      pdb = "/Users/nat/Documents/beta-blip/beta.pdb"
    }
  }
  ensemble {
    model_id = "blip"
    coordinates {
      pdb = "/Users/nat/Documents/beta-blip/blip.pdb"
    }
  }
}
""")
  files_in = [
    ('/Users/nat/Documents/beta-blip/beta_blip_P3221.mtz', 'Data file',
      'phaser.hklin'),
    ('/Users/nat/Documents/beta-blip/blip.seq', 'Sequence file',
      'phaser.composition.chain.sequence_file'),
    ('/Users/nat/Documents/beta-blip/blip.pdb', 'Ensemble model',
      'phaser.ensemble.coordinates.pdb')
  ]
  assert (i.get_input_files() == files_in)
  i.save_param_file(
    file_name="phaser.eff",
    extra_phil="""
phaser.search {
  ensembles = beta
  copies = 1
}
phaser.search {
  ensembles = blip
  copies = 1
}""",
    replace_path="/Users/nat/Documents/beta-blip")
  i = interface.index(master_phil=master_phil,
    parse=iotbx.phil.parse)
  i.merge_phil(phil_file="phaser.eff")
  p = i.get_python_object().phaser
  assert (i.get_input_files() == files_in)
  assert (p.hklin == "/Users/nat/Documents/beta-blip/beta_blip_P3221.mtz")
  assert (len(p.search) == 2)
  assert (p.search[0].ensembles == ["beta"])
  # update file in-place (with variable substitution)
  interface.update_phil_file_paths(
    master_phil=master_phil,
    file_name="phaser.eff",
    old_path="/Users/nat/Documents/beta-blip",
    new_path="/Users/nat/Documents/projects/beta-blip",
    use_iotbx_parser=True)
  i = interface.index(master_phil=master_phil,
    parse=iotbx.phil.parse)
  i.merge_phil(phil_file="phaser.eff")
  p = i.get_python_object().phaser
  assert (p.hklin ==
    "/Users/nat/Documents/projects/beta-blip/beta_blip_P3221.mtz")
  # update file in-place, by modifying phil objects directly
  i.save_param_file(file_name="phaser2.eff")
  interface.update_phil_file_paths(
    master_phil=master_phil,
    file_name="phaser2.eff",
    old_path="/Users/nat/Documents/projects/beta-blip",
    new_path="/home/nat/projects/beta-blip",
    use_iotbx_parser=True)
  i = interface.index(master_phil=master_phil,
    parse=iotbx.phil.parse)
  i.merge_phil(phil_file="phaser2.eff")
  p = i.get_python_object().phaser
  assert (p.hklin == "/home/nat/projects/beta-blip/beta_blip_P3221.mtz")
  i.set_prefix("phaser")
  assert (i.get_full_path(".hklin") == "phaser.hklin")
  assert (i.get_scope_by_name(".keywords") is not None)
  # and now with Windows-style paths
  interface.update_phil_file_paths(
    master_phil=master_phil,
    file_name="phaser2.eff",
    old_path="/home/nat/projects/beta-blip",
    new_path="C:\\projects\\xtal\\beta-blip", # \x and \b are key here
    use_iotbx_parser=True)
  i = interface.index(master_phil=master_phil,
    parse=iotbx.phil.parse)
  i.merge_phil(phil_file="phaser2.eff")
  p = i.get_python_object().phaser
  # XXX obviously these are not entirely transferrable between Unix and
  # Windows - we need to caution users against this
  assert (p.hklin == "C:\\projects\\xtal\\beta-blip/beta_blip_P3221.mtz")

def exercise_adopt_phil():
  master_phil = libtbx.phil.parse("""\
scope1 {
  a = 1
    .type = int
  b = 2
    .type = int
}
""")
  working_phil = libtbx.phil.parse("""\
scope1.a = 3
scope1.b = 4
""")
  i = libtbx.phil.interface.index(master_phil=master_phil,
                                  working_phil=working_phil,
                                  fetch_new=True)
  params = i.get_python_object()
  other_master_phil = libtbx.phil.parse("""\
scope1 {
  c = 3
    .type = int
}
scope2 {
  subscope2 {
    d = 4
      .type = int
  }
  e = 5
    .type = int
}
""")
  i.adopt_phil(phil_object=other_master_phil)
  scope1 = i.get_scope_by_name("scope1")
  s = StringIO()
  scope1.show(out=s)
  assert not show_diff(s.getvalue(), """\
scope1 {
  a = 3
  b = 4
  c = 3
}
""")
  s = StringIO()
  i.working_phil.show(out=s)
  assert not show_diff(s.getvalue(), """\
scope1 {
  a = 3
  b = 4
  c = 3
}
scope2 {
  subscope2 {
    d = 4
  }
  e = 5
}
""")


if __name__ == "__main__" :
  exercise()
  exercise_2(verbose=("-v" in sys.argv[1:] or "--verbose" in sys.argv[1:]))
  exercise_3()
  exercise_adopt_phil()
  print("OK")

#---end


 *******************************************************************************


 *******************************************************************************
libtbx/phil/tst_tokenizer.py
from __future__ import absolute_import, division, print_function
from libtbx.phil import tokenizer

def exercise_basic(verbose):
  tests = [
  ["",
    []],
  ["resname=a and chain=b",
    ['resname', '=', 'a', 'and', 'chain', '=', 'b']],
  ["resname a and chain b",
    ['resname', 'a', 'and', 'chain', 'b']],
  ["resname resname and chain chain",
    ['resname', 'resname', 'and', 'chain', 'chain']],
  ["resname \"a b\"",
    ['resname', 'a b']],
  ["resname a",
    ['resname', 'a']],
  ["resname ala and backbone",
    ['resname', 'ala', 'and', 'backbone']],
  ["resname ala or backbone",
    ['resname', 'ala', 'or', 'backbone']],
  ["name x and x > 10",
    ['name', 'x', 'and', 'x', '>', '10']],
  ["((expr or expr) and expr)",
    ['(', '(', 'expr', 'or', 'expr', ')', 'and', 'expr', ')']],
  ["resname and and chain b",
    ['resname', 'and', 'and', 'chain', 'b']],
  ["resname ( and chain b",
    ['resname', '(', 'and', 'chain', 'b']],
  ["resname \"(\" and chain b",
    ['resname', '(', 'and', 'chain', 'b']],
  ["all_hydrophobic_within(5) and resname ALA",
    ['all_hydrophobic_within', '(', '5', ')', 'and', 'resname', 'ALA']],
  ["something(a, b)",
    ['something', '(', 'a', ',', 'b', ')']],
  ["something(a b)",
    ['something', '(', 'a', 'b', ')']],
  ["something(\"a\"\"b\")",
    ['something', '(', 'a', 'b', ')']],
  ["resname 'a \\\\'",
    ['resname', 'a \\']],
  ["resname 'a'",
    ['resname', 'a']],
  ["resname '\"'",
    ['resname', '"']],
  ["resname '\"\\''",
    ['resname', '"\'']],
  ["resname \"'\\\"\"",
    ['resname', '\'"']],
  ["name o1'",
    ['name', 'o1\'']],
  ['name """o1\'"""',
    ['name', 'o1\'']],
  ['name """o1\n  o2\'"""',
    ['name', "o1\n  o2'"]],
  ['name """o1\\\n  o2\'"""',
    ['name', "o1  o2'"]],
  ]
  for input_string,expected_result in tests:
    show = verbose or expected_result is None
    if (show): print(input_string)
    result = [word.value
      for word in tokenizer.word_iterator(input_string=input_string)]
    if (show): print(result)
    if (expected_result is not None):
      assert result == expected_result
    if (show): print()

def exercise_pickle():
  # TODO: verify this is intended change for py2/3 compat
  from six.moves import cPickle as pickle
  for p in [pickle]:
    o = tokenizer.word(value="hello")
    l = p.loads(p.dumps(o))
    assert l.value == "hello"
    o = tokenizer.settings(meta_comment="%")
    l = p.loads(p.dumps(o))
    assert l.meta_comment == "%"
    o = tokenizer.word_iterator(input_string="all")
    l = p.loads(p.dumps(o))
    assert l.char_iter.input_string == "all"

def run(args):
  assert args in [[], ["--verbose"]]
  verbose = len(args) != 0
  exercise_basic(verbose=verbose)
  exercise_pickle()
  print("OK")

if (__name__ == "__main__"):
  import sys
  run(args=sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
libtbx/pkg_utils.py
# Python package related functions
#
# To let libtbx modules specify and satisfy their requirements and capabilities.

from __future__ import absolute_import, division, print_function

import contextlib
import itertools
import os
import sys
import subprocess

import libtbx.load_env

try:
  pip_version_cmd = [sys.executable, '-c', 'import pip; print(pip.__version__)']
  pip_version_str = subprocess.check_output(pip_version_cmd).decode().strip()
  import pkg_resources

  # Don't run if pip version < 9.0.0.
  # There is no technical reason for this, the code should work still.
  # But in the interest of not upsetting legacy build systems let's be cautious.
  if not all(symbol in dir(pkg_resources) for symbol in
             ('parse_version', 'require', 'DistributionNotFound', 'VersionConflict')) \
     or pkg_resources.parse_version(pip_version_str) < pkg_resources.parse_version('9.0.0'):
    use_pip = False
    pkg_resources = None
except ImportError:
  use_pip = False
  pkg_resources = None

# Try to find packaging. As of Oct 2022 this should be provided explicitly when
# the conda env is created. Not safe to import the vendored version from pip;
# see https://github.com/pypa/setuptools/issues/3297
try:
  import packaging
  from packaging.requirements import Requirement
except ImportError:
  Requirement = None
  packaging = None

try:
  import setuptools
except ImportError:
  setuptools = None

def _notice(*lines, **context):
  print(os.linesep + "=" * 80 + os.linesep + os.linesep +
        os.linesep.join(l.format(**context) for l in lines) + os.linesep +
        os.linesep + "=" * 80 + os.linesep)

_defined_entrypoints = set()

def require(pkgname, version=None):
  '''Ensure a package requirement is met. Install or update package as required
     and print a warning message if this can't be done due to the local
     environment, or when automatic package management is disabled by setting
     the environment variable 'LIBTBX_DISABLE_UPDATES'.
     :param pkgname: A string describing the package requirement. This will
                     generally just be a package name, but package features
                     can be specified in square brackets. Features are not
                     enforced, but will be requested during installation and
                     update.
     :param version: An optional string describing version constraints. This
                     can be a minimum version, eg. '>=1.0', a maximum version,
                     eg. '<2', or both, eg. '>=4.5,<4.6'.
     :return: True when the requirement is met, False otherwise.'''

  if not use_pip:
    _notice("  WARNING: Can not verify python package requirements - pip/setuptools out of date",
            "  Please update pip and setuptools by running:", "",
            "    libtbx.python -m pip install pip setuptools --upgrade", "",
            "  or following the instructions at https://pip.pypa.io/en/stable/installing/")
    return False

  if not version:
    version = ''

  requirement = Requirement(pkgname+version)

  # Check if we have an environment marker in the request
  if requirement.marker and not requirement.marker.evaluate():
    # We skip dependencies that don't match our current environment
    return True
  # Erase the marker from any further output
  requirement.marker = None

  # package name without feature specification
  basepkgname = requirement.name

  requirestring = str(requirement)
  baserequirestring = requirement.name + str(requirement.specifier)
  try:
    try:
      print("requires %s, has %s" % (requirestring, pkg_resources.require(requirestring)[0].version))
      return True
    except pkg_resources.UnknownExtra:
      print("requires %s, has %s, but without features" % (requirestring, pkg_resources.require(baserequirestring)[0].version))
      return True

  except pkg_resources.DistributionNotFound:
    currentversion = '(not determined)'
    project_name = pkgname
    action = 'install'
    print("requirement %s is not currently met, package not installed" % (requirestring))

  except pkg_resources.VersionConflict:
    currentversion = pkg_resources.require(basepkgname)[0].version
    project_name = pkg_resources.require(basepkgname)[0].project_name
    action = 'update'
    print("requirement %s is not currently met, current version %s" % (requirestring, currentversion))

  # Check if package can be updated
  for path_item in sys.path:
    egg_link = os.path.join(path_item, project_name + '.egg-link')
    if os.path.isfile(egg_link):
      with open(egg_link, 'r') as fh:
        location = fh.readline().strip()
      _notice("    WARNING: Can not update package {package} automatically.", "",
              "It is installed as editable package for development purposes. The currently",
              "installed version, {currentversion}, is too old. The required version is {requirement}.",
              "Please update the package manually in its installed location:", "",
              "    {location}",
              package=pkgname, currentversion=currentversion, requirement=version, location=location)
      return False

  if not os.path.isdir(libtbx.env.under_base('.')):
    _notice("    WARNING: Can not {action} package {package} automatically.", "",
            "You are running in a base-less installation, which disables automatic package",
            "management by convention, see https://github.com/cctbx/cctbx_project/issues/151", "",
            "Please {action} the package manually.",
            package=pkgname, currentversion=currentversion, requirement=version, action=action)
    return False

  if os.getenv('LIBTBX_DISABLE_UPDATES') and os.getenv('LIBTBX_DISABLE_UPDATES').strip() not in ('0', ''):
    _notice("    WARNING: Can not {action} package {package} automatically.", "",
            "Environment variable LIBTBX_DISABLE_UPDATES is set. Please {action} manually.",
            package=pkgname, currentversion=currentversion, requirement=version, action=action)
    return False

  if libtbx.env.build_options.use_conda:
    _notice("    WARNING: Can not {action} package {package} automatically.", "",
            "You are in a conda environment. Please {action} manually.",
            package=pkgname, currentversion=currentversion, requirement=version, action=action)
    return False

  print("attempting {action} of {package}...".format(action=action, package=pkgname))
  has_req_tracker = os.environ.get('PIP_REQ_TRACKER')
  pip_install_cmd = [sys.executable, '-m', 'pip', 'install', requirestring]
  pip_install_result = subprocess.run(pip_install_cmd)
  exit_code = pip_install_result.returncode
  if not has_req_tracker:
    # clean up environment after pip call for next invocation
    os.environ.pop('PIP_REQ_TRACKER', None)
  if exit_code == 0:
    print("{action} successful".format(action=action))
    return True
  else:
    print("{action} failed. please check manually".format(action=action))
    return False

@contextlib.contextmanager
def _silence():
  '''Helper context which shuts up stdout.'''
  if os.name == "nt":
    # Can't silence using this method on Windows. Just leave it.
    # new windows console in python >= 3.6 does not work with os.dup2
    # https://bugs.python.org/issue30555
    yield
    return
  sys.stdout.flush()
  try:
    oldstdout = os.dup(sys.stdout.fileno())
    dest_file = open(os.devnull, 'w')
    os.dup2(dest_file.fileno(), sys.stdout.fileno())
    yield
  finally:
    if oldstdout is not None:
      os.dup2(oldstdout, sys.stdout.fileno())
    if dest_file is not None:
      dest_file.close()

def define_entry_points(epdict, **kwargs):
  '''A function to allow non-setuptools packages (ie. libtbx modules) to use
     the setuptools entry_points mechanism. Call this function from
     libtbx_refresh.py and pass a dictionary of entry points.'''
  # Determine the name of the calling module, and thus the internal module name
  # of the run_tests file. Use exception trick to pick up the current frame.
  try:
    raise Exception()
  except Exception:
    frame = sys.exc_info()[2].tb_frame.f_back
  # Extract the caller name
  caller = frame.f_globals['__name__']
  if caller == '__main__':
    # well that is not very informative, is it.
    caller = os.path.abspath(frame.f_code.co_filename)  # Get the full path of the libtbx_refresh.py file.
    refresh_file, _ = os.path.splitext(caller)
    if not refresh_file.endswith('libtbx_refresh'):
      raise RuntimeError('Entry points can only be defined from within libtbx_refresh.py')
    # the name of the parent directory of libtbx_refresh.py is the caller name
    caller = os.path.basename(os.path.dirname(refresh_file))
  else:
    if not caller.endswith('.libtbx_refresh'):
      raise RuntimeError('Entry points can only be defined from within libtbx_refresh.py')
    caller = caller[:-15]

  # No setuptools mechanism without setuptools.
  if not setuptools:
    raise ImportError("You must install setuptools to configure package {}. Run\n  libtbx.pip install setuptools".format(caller))

  if caller in _defined_entrypoints:
    raise RuntimeError("Entry points have already been defined for package %s. There must only be a single call to "
                       "define_entry_points() per calling package" % caller)
  _defined_entrypoints.add(caller)

  print("Updating entry points for {caller}".format(caller=caller))
  for ep in epdict:
    print("  {n} entries for entry point {ep}".format(ep=ep, n=len(epdict[ep])))

  # Temporarily change to {libtbx.env.build_path}/lib directory. This
  # is where a directory named libtbx.{caller}.egg-info will be
  # created containing the entry point info.
  try:
    curdir = os.getcwd()
    os.chdir(os.path.join(abs(libtbx.env.build_path), 'lib'))
    # Now trick setuptools into thinking it is in control here.
    try:
      argv_orig = sys.argv
      sys.argv = ['setup.py', 'egg_info']
      # And make it run quietly
      with _silence():
        setuptools.setup(
          name='libtbx.{}'.format(caller),
          description='libtbx entry point manager for {}'.format(caller),
          entry_points=epdict,
          **kwargs
        )
    finally:
      sys.argv = argv_orig
  finally:
    os.chdir(curdir)


def _merge_requirements(requirements, new_req):
  # type: (List[packaging.requirements.Requirement], packaging.requirements.Requirement) -> None
  """Merge a new requirement with a list.

  If it exists in an identical form (name, marker) then the
  specifiers and extras will be merged, otherwise it will be added.

  If the environment markers are different it will assume that they
  are mutually exclusive - entries will only be merged if identical, which
  could cause problems with duplicate requirement entries if not filtered
  by pass status later.

  URL field is also not handled, as unsure how to merge these if they differ.

  Args:
    requirements (List[packaging.requirements.Requirement]): Existing.
    new_req (packaging.requirements.Requirement): New requirement
  """
  assert new_req.url is None, "URL requirement fields not handled/tested"
  matches = [
    x
    for x in requirements
    if x.name == new_req.name
    and x.marker == new_req.marker
  ]
  if len(matches) > 0:
    if len(matches) > 1:
      print("Warning: More than one match for requirement", new_req, ": ", matches)
    match = matches[0]
  else:
    match = None

  if match:
    match.specifier = match.specifier & new_req.specifier
    match.modules = match.modules | new_req.modules
    match.extras = match.extras | new_req.extras
  else:
    requirements.append(new_req)


def collate_python_requirements(modules):
  # type: (List[libtbx.env_config.module]) -> List[packaging.requirements.Requirement]
  """Combine python requirements from a module list.

  An attempt will be made to merge any joint requirements. The requirement
  objects will have an added property 'modules', which is a set of module
  names that formed the requirement.

  Attr:
      modules (Iterable[libtbx.env_config.module]): The module list

  Returns:
      List[packaging.requirements.Requirement]: The merged requirements
  """
  requirements = []
  for module, spec in itertools.chain(*[[(x.name, y) for y in x.python_required] for x in modules if hasattr(x, "python_required")]):
    requirement = Requirement(spec)
    # Track where dependencies came from
    requirement.modules = {module}
    # Attempt to merge this with any other requirements to avoid double-specifying
    _merge_requirements(requirements, requirement)
  return requirements


 *******************************************************************************


 *******************************************************************************
libtbx/program_template.py
'''
Standard Program Template for CCTBX Programs

The "program" is the actual task to be performed without any user interfaces.
The user interfaces (command-line and graphical) build the data_manager and
params objects for the program. The "data_manager"" handles all file input
and "params" handles all the program settings. These two objects should have
all relevant information for the program to run.

The required functions break up the calling order into discrete phases

- constructor: minimal set up
- validate: check that the inputs (files and parameters) are valid and consistent
- run: run the actual task
- get_results: return the desired output from the program

The optional functions provide some extra tweaking

- custom_init: called at the end of the constructor, additional initialization
- clean_up: if temporary files are written in the course of running the program,
            this step should remove those files.

Additional functions and class attributes can be defined for doing the actual
task, but the above functions define a consistent interface.

More documentation to come
'''
from __future__ import absolute_import, division, print_function

import libtbx.phil

from libtbx import Auto, citations
from libtbx.utils import multi_out, Sorry
from libtbx.version import get_version

# =============================================================================
class ProgramTemplate(object):
  # Class variables for customizing program

  # name of the program, this overrides the LIBTBX_DISPATCHER_NAME
  # environment variable
  program_name = None

  # custom version, this overrides the default version from
  # libtbx.version.get_version
  version = None

  # description of the program
  description = '''
Program Description
'''

  # list of keywords for categorizing the program (optional)
  keywords = None

  # list of maintainer(s) (GitHub names) for the program (optional)
  maintainers = None

  # datatypes for program
  # see iotbx/data_manager/<datatype>.py for list of supported datatypes
  # default datatypes are set in iotbx/data_manager/__init__.py (default_datatypes)
  datatypes = None

  # DataManager options
  # customization for how the DataManager processes files
  # available options are set in iotbx/data_manager/__init__.py (data_manager_options)
  data_manager_options = None

  # customization of master PHIL defined by string
  # this is useful for setting different defaults
  data_manager_custom_master_phil_str = None

  # master PHIL string for the program (required)
  master_phil_str = '''
# example
program {
  parameter = None
    .type = bool
}
'''

  # define the PHIL parameter (e.g. use "parent_scope.scattering_table" if
  # self.params.parent_scope.scattering_table is a valid paramter)
  # that should be checked for determining the data type of all the data
  # stored in the DataManager.
  use_scattering_table_for_default_type = None

  # the DataManager scope includes some shared PHIL parameters
  # set this to true if the DataManager scope should be shown by default
  show_data_manager_scope_by_default = False

  # unique citations for the program. list of citation phil extract objects
  # see libtbx/citations.py for the PHIL format.
  citations = None

  # common citations used by the program that exist in libtbx/citations.params
  # list of article_id strings, e.g. ["polder", "elbow"]).
  known_article_ids = []

  # text shown at the end of the command-line program
  epilog = '''
For additional help, you can contact the developers at cctbxbb@phenix-online.org
or https://github.com/cctbx/cctbx_project

'''

  # ---------------------------------------------------------------------------
  # Reserved phil scope for output
  # This will be automatically added to the master_phil_str.
  # You should add your own output phil scope, but these parameters will be
  # automatically added, so no need to redefine.
  # The filename and file_name parameters refer to the same thing.
  # Changing one will change the other. If multiple values are specified,
  # the last one to be processed is kept.
  output_phil_str = '''
output {
  filename = None
    .alias = file_name
    .type = str
    .help = Manually set filename, overrides filename automatically \
            generated by prefix/suffix/serial
  file_name = None
    .alias = filename
    .type = str
    .help = Same as output.filename
  prefix = None
    .type = str
    .help = Prefix string added to automatically generated output filenames
  suffix = None
    .type = str
    .help = Suffix string added to automatically generated output filenames
  serial = 0
    .type = int
    .help = Serial number added to automatically generated output filenames
  serial_format = "%03d"
    .type = str
    .help = Format for serial number
 target_output_format = *None pdb mmcif
   .type = choice
   .help = Desired output format (if possible). Choices are None (\
            try to use input format), pdb, mmcif.  If output model\
             does not fit in pdb format, mmcif will be used. \
             Default is pdb.
   .short_caption = Desired output format

  overwrite = False
    .type = bool
    .help = Overwrite files when set to True
}
'''

  # ---------------------------------------------------------------------------
  # Advanced features

  # PHIL converters (in a list) for additional PHIL types
  phil_converters = list()

  # ---------------------------------------------------------------------------
  # Convenience features
  def _print(self, text):
    '''
    Print function that just replaces print(text, file=self.logger)
    '''
    print(text, file=self.logger)

  def header(self, text):
    self._print("-"*79)
    self._print(text)
    self._print("*"*len(text))

  # ---------------------------------------------------------------------------
  # Function for showing default citation for template
  @staticmethod
  def show_template_citation(text_width=80, logger=None,
                             citation_format='default'):
    assert logger is not None

    print('\nGeneral citation for CCTBX:', file=logger)
    print('-'*text_width, file=logger)
    print('', file=logger)
    citations.show_citation(citations.citations_db['cctbx'], out=logger,
                            format=citation_format)

  # ---------------------------------------------------------------------------
  def __init__(self, data_manager, params, master_phil=None, logger=None):
    '''
    Common constructor for all programs

    This is supposed to be lightweight. Custom initialization, if necessary,
    should be handled by the custom_init function. Developers should not need to
    override this function.

    Parameters
    ----------
    data_manager :
      An instance of the DataManager (libtbx/data_manager.py) class containing
      data structures from file input
    params :
      An instance of PHIL
    logger :
      Standard Python logger (from logging module), optional. A logger will be
      created if it is not provided.

    '''

    self.data_manager = data_manager
    self.master_phil = master_phil
    self.params = params
    self.logger = logger

    if self.logger is None:
      self.logger = multi_out()

    # master_phil should be provided by CCTBXParser or GUI because of
    # potential PHIL extensions
    if self.master_phil is None:
      self.master_phil = libtbx.phil.parse(
        self.master_phil_str, process_includes=True)

    # set DataManager defaults
    if self.data_manager is not None:
      self.data_manager.set_default_output_filename(
        self.get_default_output_filename())
      self.set_target_output_format()
      try:
        self.data_manager.set_overwrite(self.params.output.overwrite)
      except AttributeError:
        pass
      self.data_manager.set_program(self)

    # optional initialization
    self.custom_init()

    # set default data type, if applicable
    if self.use_scattering_table_for_default_type is not None:
      if not (self.data_manager.supports('model') and self.data_manager.supports('miller_array')):
        raise Sorry('''\
The "use_scattering_table_for_default_type" requires that the DataManager
support both "model" and "miller_array" data.
''')
      scattering_table = self.params
      for phil_name in self.use_scattering_table_for_default_type.split('.'):
        scattering_table = getattr(scattering_table, phil_name)
      self.data_manager.update_all_defaults(scattering_table)

  # ---------------------------------------------------------------------------
  def custom_init(self):
    '''
    Optional initialization step

    Developers should override this function if additional initialization is
    needed. There should be no arguments because all necessary information
    should be in self.data_manager (file input) and self.params (phil parameters)

    Parameters
    ----------
    None
    '''
    pass

  # ---------------------------------------------------------------------------
  def validate(self):
    '''

    '''
    raise NotImplementedError('The "validate" function is required.')

  # ---------------------------------------------------------------------------
  def run(self):
    '''

    '''
    raise NotImplementedError('The "run" function is required.')

  # ---------------------------------------------------------------------------
  def clean_up(self):
    '''

    '''
    pass

  # ---------------------------------------------------------------------------
  def get_results(self):
    '''

    '''
    return None

  # ---------------------------------------------------------------------------
  def get_results_as_JSON(self):
    '''

    '''
    return None

  # ---------------------------------------------------------------------------
  def get_program_phil(self, diff=False):
    '''
    Function for getting the PHIL extract of the Program

    Parameters
    ----------
    diff: bool
      When set to True, only the differences from the master PHIL are returned

    Returns
    -------
    params: libtbx.phil.scope
    '''
    working_phil = self.master_phil.format(python_object=self.params)
    if diff:
      working_phil = self.master_phil.fetch_diff(working_phil)
    return working_phil

  # ---------------------------------------------------------------------------
  def get_data_phil(self, diff=False):
    '''
    Function for getting the PHIL scope from the DataManager

    Parameters
    ----------
    diff: bool
      When set to True, only the differences from the master PHIL are returned

    Returns
    -------
    params: libtbx.phil.scope
    '''
    if self.data_manager is None:
      return libtbx.phil.parse('')
    working_phil = self.data_manager.export_phil_scope()
    if diff:
      working_phil = self.data_manager.master_phil.fetch_diff(working_phil)
    return working_phil

  # ---------------------------------------------------------------------------
  def get_program_extract(self, diff=False):
    '''
    Function for getting the PHIL extract of the Program

    Parameters
    ----------
    diff: bool
      When set to True, only the differences from the master PHIL are returned

    Returns
    -------
    params: libtbx.phil.scope_extract
    '''
    return self.get_program_phil(diff=diff).extract()

  # ---------------------------------------------------------------------------
  def get_data_extract(self, diff=False):
    '''
    Function for getting the PHIL extract from the DataManager

    Parameters
    ----------
    diff: bool
      When set to True, only the differences from the master PHIL are returned

    Returns
    -------
    params: libtbx.phil.scope_extract
    '''
    return self.get_data_phil(diff=diff).extract()

  # ---------------------------------------------------------------------------
  def get_program_phil_str(self, diff=False):
    '''
    Function for getting the PHIL string of the Program

    Parameters
    ----------
    diff: bool
      When set to True, only the differences from the master PHIL are returned

    Returns
    -------
    params: str
    '''
    return self.get_program_phil(diff=diff).as_str()

  # ---------------------------------------------------------------------------
  def get_data_phil_str(self, diff=False):
    '''
    Function for getting the PHIL string from the DataManager

    Parameters
    ----------
    diff: bool
      When set to True, only the differences from the master PHIL are returned

    Returns
    -------
    params: str
    '''
    return self.get_data_phil(diff=diff).as_str()

  # ---------------------------------------------------------------------------
  def get_full_phil_str(self, diff=False):
    '''
    Function for getting the full PHIL string of the DataManager and Program

    Parameters
    ----------
    diff: bool
      When set to True, only the differences from the master PHIL are returned

    Returns
    -------
    params: str
    '''
    return self.get_data_phil_str(diff=diff) + self.get_program_phil_str(diff=diff)

  # ---------------------------------------------------------------------------
  def set_target_output_format(self):
    """ Try to set the desired output format if not set by user (pdb or mmcif)
    """
    assert self.data_manager is not None
    if not hasattr(self.data_manager,'set_target_output_format'):
      return # No models in this data_manager

    from iotbx.pdb.utils import set_target_output_format_in_params
    if hasattr(self.data_manager, 'get_default_model_name'):
      file_name = self.data_manager.get_default_model_name()
    else:
      file_name = None
    target_output_format = set_target_output_format_in_params(self.params,
      file_name = file_name,
      out = self.logger)
    self.data_manager.set_target_output_format(target_output_format)


  # ---------------------------------------------------------------------------
  def _params_as_dict(self, params = None, base_name_list = None):
    """ Split up a params object into a dict of each individual parameter name
       as key and value as value. Add base name on to attribute name,
       Recursively traverse the params object."""
    if not params: params = self.params
    if not base_name_list: base_name_list = []
    params_dict = {}
    for x in dir(params):
      if x.startswith("__"): continue
      v = getattr(params,x)
      b = base_name_list + [x]
      if isinstance(v,libtbx.phil.scope_extract):
        self._update_params_dict(params_dict,
             self._params_as_dict(v, base_name_list = b))
      elif isinstance(v, libtbx.phil.scope_extract_list):
        for vv in v:
          if hasattr(vv,'__phil_name__'):
            self._update_params_dict(params_dict,
             self._params_as_dict(vv, base_name_list = b))
          else:
            params_dict[".".join(b)] = vv
      else:
        params_dict[".".join(b)] = v
    return params_dict

  def _update_params_dict(self, params_dict, other_params_dict):
    for key in other_params_dict.keys():
      if not key in params_dict:
        params_dict[key] = other_params_dict[key]
      else:
        if not other_params_dict[key]:
          pass
        elif not params_dict[key]:
          params_dict[key] = other_params_dict[key]
        else:
          if not isinstance(params_dict[key], list):
            params_dict[key] = [ params_dict[key]]
          if not isinstance(other_params_dict[key], list):
            other_params_dict[key] = [ other_params_dict[key]]
          params_dict[key] += other_params_dict[key]
    return params_dict
  def _fn_is_assigned(self, fn = None):
    """ Determine if fn is assigned to some parameter"""
    for x in list(self._params_as_dict().values()):
      if fn == x:
        return True
      if isinstance(x, list) and fn in x:
        return True
    else:
      return False


  def get_parameter_value(self, parameter_name, base = None):
    """ Get the full scope and the parameter from a parameter name.
     Then get value of this parameter
     For example:  autobuild.data -> (self.params.autobuild, 'data')
     returns value of self.params.autobuild.data

     parameter: parameter_name:  text parameter name in context of self.params
     parameter: base: base scope path to add before parameter name
     returns: value of self.params.parameter_name
    """

    scope, par = self._get_scope_and_parameter(parameter_name, base = base)
    if not hasattr(scope, par):
      print("The parameter %s does not exist" %(parameter_name),
         file = self.logger)
    return getattr(scope, par)

  def _get_scope_and_parameter(self, parameter_name = None, base = None):
    """ Get the full scope and the parameter from a parameter name.
     For example:  autobuild.data -> (self.params.autobuild, 'data')
    """
    if base is None:
      base = self.params
    assert parameter_name is not None, "Missing parameter name"
    spl = parameter_name.split(".")
    name = spl[-1]
    path = spl[:-1]
    for p in path:
      assert hasattr(base, p), "Missing scope: %s" %(p)
      base = getattr(base, p)
    return base, name

  def assign_if_value_is_unique_and_unassigned(self,
      parameter_name = None,
      possible_values = None):
    """ Method to assign a value to a parameter that has no value so far,
      choosing value from a list of possible values, eliminating all values
      that have been assigned to another parameter already.
      Normally used like this in a Program template:

      self.assign_if_value_is_unique_and_unassigned(
        parameter_name = 'autobuild.data',
        possible_values = self.data_manager.get_miller_array_names())

      Raises Sorry if there are multiple possibilities.

     parameter: parameter_name:  The name of the parameter in the context
                                 of self.params (self.params.autobuild.data is
                                 autobuild.data)
     parameter: possible_values: Possible values of this parameter, usually from
                                 the data_manager

     sets: value of full parameter to a unique value if present
     returns: None
    """
    v = self.get_parameter_value(parameter_name)

    has_value = not (v in ['Auto',Auto, 'None',None])
    if has_value:
      return # nothing to do, already assigned value to this parameter

    possibilities = []
    for p in possible_values:
      if p in ['Auto',Auto, 'None',None]:
        continue  # not relevant
      elif (not self._fn_is_assigned(p)): # not already assigned
        possibilities.append(p)
    if len(possibilities) == 1:
      scope, par = self._get_scope_and_parameter(parameter_name)
      setattr(scope, par, possibilities[0])
    elif len(possibilities) < 1:
      return # No unused possibilities for this parameter
    else:
      raise Sorry("Please set these parameters with keywords: (%s), " %(
        " ".join(possibilities)) + "\nFor example, '%s=%s'" %(
        parameter_name, possibilities[0]))
  # ---------------------------------------------------------------------------

  def get_default_output_filename(self, prefix=Auto, suffix=Auto, serial=Auto,
    filename=Auto):
    '''
    Given the output.prefix, output.suffix, and output.serial PHIL parameters,
    return the default output filename. The filename is constructed as

      {prefix}{suffix}_{serial:03d}

    However, if output.filename (or output.file_name), that value takes
    precedence.

    Parameters
    ----------
    prefix: str
      The prefix for the name, if set to Auto, the value from output.serial
      is used.
    suffix: str
      The suffix for the name, if set to Auto, the value from output.suffix
      is used.
    serial: int
      The serial number for the name, if set to Auto, the value from
      output.serial is used. Leading zeroes will be added so that the
      number uses 3 spaces.
    filename: str
      A name that overrides the automatically generated name. If set to
      Auto, the value from output.filename is used.

    Returns
    -------
    filename: str
      The default output filename without a file extension
    '''

    # set defaults
    output = None
    if hasattr(self.params, 'output'):
      output = self.params.output

    if prefix is Auto:
      prefix = 'cctbx_program'
      if output and getattr(output, 'prefix', None) is not None:
        prefix = output.prefix
    if suffix is Auto:
      suffix = None
      if output and getattr(output, 'suffix', None) is not None:
        suffix = output.suffix
    if serial is Auto:
      serial = None
      if output and getattr(output, 'serial', None) is not None:
        serial = self.params.output.serial
    else:
      if not isinstance(serial, int):
        raise ValueError('The serial argument should be an integer.')

    # create filename
    if filename is Auto:
      # override if necessary
      if output and getattr(output, 'filename', None) is not None:
        filename = output.filename
      else:
        filename = prefix
        if suffix is not None:
          filename += suffix
        if serial is not None:
          filename += '_{serial:03d}'.format(serial=serial)

    return filename

  # ---------------------------------------------------------------------------
  @classmethod
  def get_version(cls):
    '''
    Function for returning the version

    Parameters
    ----------
    None

    Returns
    -------
    version: str
    '''
    # try the class first
    if cls.version is not None:
      return cls.version

    # the default version
    return get_version()

# =============================================================================

import iotbx.phil
output_phil = iotbx.phil.parse(ProgramTemplate.output_phil_str)


 *******************************************************************************


 *******************************************************************************
libtbx/program_utils/__init__.py


 *******************************************************************************


 *******************************************************************************
libtbx/program_utils/result.py
from __future__ import absolute_import, division, print_function

from libtbx.program_utils import statistics_info
from libtbx import adopt_init_args
import os

class program_result(object):
  def __init__(self,
                program_name,
                job_title,
                directory=None,
                log_file=None,
                input_files=(),
                pdb_files=(),
                map_file=None,
                data_file=None,
                cif_files=(),
                phil_files=(),
                other_files=(),
                statistics={},
                other_result=None):
    adopt_init_args(self, locals())

  def get_output_dir(self):
    return self.directory

  def get_statistic(self, name):
    return self.statistics.get(name, None)

  def get_pdb_files(self):
    return [ self.get_file_path(fn) for fn in self.pdb_files ]

  @property
  def n_pdb_files(self):
    return len(self.pdb_files)

  def get_pdb_file(self):
    assert (len(self.pdb_files) <= 1)
    if (len(self.pdb_files) == 0):
      return None
    return self.get_file_path(self.pdb_files[0])

  def show_summary(self, out):
    pass

  def get_file_path(self, file_name):
    if (file_name is None):
      return None
    elif (os.path.isabs(file_name)):
      return file_name
    else :
      assert (self.directory is not None)
      return os.path.join(self.directory, file_name)

  def get_map_file(self):
    return self.get_file_path(self.map_file)

  def get_data_file(self):
    return self.get_file_path(self.data_file)

  def get_phil_files(self):
    return [ self.get_file_path(fn) for fn in self.phil_files ]

  def r_free(self):
    return self.get_statistic("r_free")

  def format_statistics(self, stat_keys):
    formatted = []
    for stat_name in stat_keys :
      stat_value = self.get_statistic(stat_name)
      if (stat_value is not None):
        if isinstance(stat_value, str):
          formatted.append((stat_label, stat_value))
        else :
          stat_label = statistics_info.keys_and_labels.get(stat_name, stat_name)
          format = statistics_info.get_format(stat_label)
          formatted.append((stat_label, format % stat_value))
    return formatted

  def get_pdb_file_caption(self):
    return "Model"

  def get_map_file_caption(self):
    return "Map coefficients"

  def finish_job(self):
    output_files = []
    for file_name in self.pdb_files :
      output_files.append((file_name, self.get_pdb_file_caption()))
    if (self.map_file is not None):
      output_files.append((self.map_file, self.get_map_file_caption()))
    output_files.extend(self.get_additional_output_files())
    stats = self.get_final_stats()
    return output_files, stats

  def get_final_stats(self):
    return []

  def get_additional_output_files(self):
    return []


 *******************************************************************************


 *******************************************************************************
libtbx/program_utils/statistics_info.py
from __future__ import absolute_import, division, print_function

# Rules of thumb for interpreting program statistics (used in Phenix GUI)

# format:
#   label min max quality strict_cutoff
threshold_values = [
  ("R-work", 0.4, 0.45, "fair", True),
  ("R-work", 0.55, 1, "bad", True),
  ("R-work", 0.0, 0.4, "good", True),
  ("R-free", 0.4, 0.45, "fair", True),
  ("R-free", 0.55, 1, "bad", True),
  ("R-free", 0.0, 0.4, "good", True),
  ("Skew", 0.15, 1, "good", True),
  ("RMS(bonds)", 0.0, 0.016, "good", True),
  ("RMS(bonds)", 0.016, 0.02, "fair", False),
  ("RMS(bonds)", 0.02, 999, "poor", False),
  ("RMS(angles)", 0.0, 1.6, "good", True),
  ("RMS(angles)", 1.6, 2.0, "fair", False),
  ("RMS(angles)", 2.0, 180, "poor", False),
  ("Ramachandran outliers", 0.0, 0.2, "good", False),
  ("Ramachandran outliers", 0.2, 0.5, "fair", False),
  ("Ramachandran outliers", 0.5, 1.0, "poor", False),
  ("Ramachandran outliers", 1.0, 100.0, "bad", False),
  ("Ramachandran favored", 98.0, 100.0, "good", False),
  ("Ramachandran favored", 95.0, 97.999, "fair", False),
  ("Ramachandran favored", 90.0, 94.999, "poor", False),
  ("Ramachandran favored", 0.0, 89.9999, "bad", False),
  ("Clashscore", 0.0, 19.999, "good", False),
  ("Clashscore", 20.0, 39.999, "fair", False),
  ("Clashscore", 40.0, 59.999, "poor", False),
  ("Clashscore", 60.0, 999, "bad", False),
  ("Rotamer outliers", 0.0, 1.999, "good", False),
  ("Rotamer outliers", 2.0, 5.0, "fair", False),
  ("Rotamer outliers", 5.0, 10.0, "poor", False),
  ("Rotamer outliers", 10.0, 100.0, "bad", False),
  # XXX backwards compatibility
  ("RMSbonds", 0.0, 0.016, "good", True),
  ("RMSbonds", 0.016, 0.02, "fair", False),
  ("RMSbonds", 0.02, 999, "poor", False),
  ("RMSangles", 0.0, 1.6, "good", True),
  ("RMSangles", 1.6, 2.0, "fair", False),
  ("RMSangles", 2.0, 180, "poor", False),
  ("CC", 0.0, 0.3, "bad", False),
  ("CC", 0.3, 0.5, "poor", False),
  ("CC", 0.5, 0.7, "fair", False),
  ("CC", 0.7, 1.0, "good", False),
  ("Model-map CC", 0.0, 0.3, "bad", False),
  ("Model-map CC", 0.3, 0.5, "poor", False),
  ("Model-map CC", 0.5, 0.7, "fair", False),
  ("Model-map CC", 0.7, 1.0, "good", False),
  ("Residues", 0, 0, "bad", False),
]

precisions = [
  ("R-work", 4),
  ("R-free", 4),
  ("RMS(bonds)", 4),
  ("RMS(angles)", 3),
  ("FOM", 3),
  ("Figure of Merit", 3),
  ("Skew", 2),
  ("Skewness", 2),
  ("Avg. B-factor", 2),
  ("Ramachandran favored", 2),
  ("Ramachandran outliers", 2),
  ("Rotamer outliers", 2),
  ("Clashscore", 1),
  # XXX backwards compatibility
  ("RMSbonds", 4),
  ("RMSangles", 3),
  ("CC", 2),
  ("Model-map CC", 2),
]

keys_and_labels = {
  "r_free" : "R-free",
  "r_work" : "R-work",
  "rms_bonds" : "RMS(bonds)",
  "rms_angles" : "RMS(angles)",
  "map_cc" : "Model-map CC",
  "cc" : "CC",
  "n_res" : "Residues",
  "rama_out" : "Ramachandran outliers",
  "rama_fav" : "Ramachandran favored",
  "rota_out" : "Rotamer outliers",
  "clashscore" : "Clashscore",
  "skew" : "Skew",
  "fom" : "Figure of merit",
  "rna_puckers" : "RNA pucker outliers",
  "rna_suites" : "RNA suite outliers",
  "rna_bonds" : "RNA bonds outliers",
  "rna_angles" : "RNA angles outliers",
}

precisions_dict_ = None
def get_format(stat_name, default="%g"):
  global precisions_dict_
  if (precisions_dict_ is None):
    precisions_dict_ = dict(precisions)
  if (stat_name in precisions_dict_):
    return "%%.%df" % precisions_dict_[stat_name]
  elif (stat_name in keys_and_labels):
    stat_label = keys_and_labels[stat_name]
    if (stat_label in precisions_dict_):
      return "%%.%df" % precisions_dict_[stat_label]
  return default


 *******************************************************************************


 *******************************************************************************
libtbx/progress.py
from __future__ import absolute_import, division, print_function


class ProgressError(Exception):
  """
  Module exception
  """


class TimeoutError(ProgressError):
  """
  Timeout exceeded in wait
  """


class streamprint(object):
  """
  Prints a character into a stream to show progress
  """

  def __init__(self, stream, character = "."):

    self.stream = stream
    self.character = character


  def __call__(self):

    self.stream.write( self.character )
    self.stream.flush()


class complete_on_success(object):
  """
  A condition that is complete when no expected errors occur. The result is
  stored to avoid problems with concurrency, i.e. that a subsequent call to the
  function returns no result
  """

  def __init__(self, func, excspec):

    self.func = func
    self.excspec = excspec


  def __call__(self):

    try:
      self.result = self.func()

    except self.excspec:
      return False

    return True


def wait(condition, waittime = 2, timeout = 600, callback = lambda: None):
  """
  Waits for a condition to become True
  """

  import time
  start = time.time()

  while not condition():
    if timeout < time.time() - start:
      raise TimeoutError("Timeout (%s s) exceeded" % timeout)

    time.sleep( 2 )
    callback()


 *******************************************************************************


 *******************************************************************************
libtbx/pyframe.py

from __future__ import absolute_import, division, print_function
import inspect
from six.moves import range

class error(Exception):
  """ libtbx.python_frame error """


def named(name):
  """ Returns an object representing the frame of the function of the given name.
      This frame will be searched upward from the current frame

      Synopsis:

      from libtbx import pyframe

      def f(i):
        for j in range(i):
          g(j)

      def g(j):
        return k(j)

      def k(j):
        f = pyframe.named('f')
        if f.j == 3: ....

      This is particularly useful for conditional breakpoints in the WingIDE debugger.
      Let's say we have a function 'test' which tests a function 'clever_algorithm' and that
      one wants to put a breakpoint in the latter that only triggers
      for one particular test (that we know is going to fail e.g.). Let's say tests are
      identified by a local variable 'idx' in function 'test'. Then we can put a conditional
      breakpoint somewhere in function 'clever_algorithm' like so:

        pyframe.named('test').idx == 5

      The advantage of conditional tests is that they don't require modifying code for
      debugging purposes, which is always a dangerous thing to do as one may inadvertantly
      leave debugging code in.

  """
  f = inspect.currentframe()
  while f.f_code.co_name != name:
    if f.f_back is None:
      raise error("frame '%s' cannot be found among the frames calling the current frame.")
    f = f.f_back
  return frame_locals(f)


def up(n):
  """ Returns an object representing the frame of the function that is n steps up in
      the calling stack, starting from the function that called this function 'up'.
      Thus n=0 corresponds to the function F that called this, n=1 to the function
      that called F, etc.
  """
  f = inspect.currentframe()
  f = f.f_back
  caller_name = f.f_code.co_name
  for i in range(n):
    if f is None:
      raise error("asked for the frame %i steps up the function '%s'"
                  " but the calling stack is not that tall.")
    f = f.f_back
  return frame_locals(f)


class frame_locals(object):

  def __init__(self, f):
    self.__dict__.update(f.f_locals)


 *******************************************************************************


 *******************************************************************************
libtbx/pyplot.py
from __future__ import absolute_import, division, print_function
from matplotlib.pyplot import *
from six.moves import zip

def plot_pairs(xy, *a, **k):
  x, y = zip(*xy)
  plot(x, y, *a, **k)


 *******************************************************************************


 *******************************************************************************
libtbx/python_code_parsing.py
from __future__ import absolute_import, division, print_function

from builtins import object

import ast

class imported_name(object):

  __slots__ = ('name', 'lineno')

  def __init__(self, name, lineno):
    self.name = tuple(name.split('.'))
    self.lineno = lineno

  def __eq__(self, other):
    return (self.name == other.name
            and self.lineno == other.lineno)

  def __ne__(self, other):
    return not self.__eq__(other)

  def __hash__(self):
    return hash((self.name, self.lineno))

  def __repr__(self):
    return '%s imported at line %i' % (self.name_as_str(), self.lineno)

  def name_as_str(self):
    return '.'.join(self.name)


class unused_imports(ast.NodeVisitor, object):
  """ Unused import's in a module.
  This finds more of them than the algorithm in
  libtbx.find_unused_imports_crude
  Work in progress: careful checking of the outcome mandatory
  """

  @classmethod
  def is_subpath_of(cls, a, b):
    """ Whether a is a subpath of b """
    if len(a) > len(b): return False
    return a == b[:len(a)]

  def __init__(self, python_source_code=None, python_source_filename=None,
               ignored_imports=(), ignored_imports_from=(),
               ignore_imports_flagged_by_comments=()):
    assert (python_source_code, python_source_filename).count(None) == 1
    if python_source_code is None:
      python_source_code = open(python_source_filename).read()
    super(unused_imports, self).__init__()
    self.comment_flags = ignore_imports_flagged_by_comments
    self.python_source_line = python_source_code.splitlines()
    self.ignored_imports = ignored_imports
    self.ignored_imports_from = ignored_imports_from
    self.current_context = () # start at module level
    self.imported_in_context = {}
    self.imported_from_full_name_in_context = {}
    self.used_in_context = {}
    tree = ast.parse(python_source_code)
    self._used = set()
    self.visit(tree)
    self._unused = set()
    for imported in self.imported_in_context.values():
      self._unused.update(imported)
    self._unused -= self._used

  def __repr__(self):
    return '\n'.join( str(imp) for imp in self )

  def __iter__(self):
    return iter(self._unused)

  def __bool__(self):
    return bool(self._unused)

  @property
  def names(self):
    return set( imp.name_as_str() for imp in self )

  def visit_Import(self, imp):
    for comment in self.comment_flags:
      if self.python_source_line[imp.lineno - 1].endswith(comment): return
    imported = self.imported_in_context.setdefault(self.current_context, set())
    imported.update(
      imported_name(name.name if name.asname is None else name.asname,
                    imp.lineno)
      for name in imp.names if name.name not in self.ignored_imports)
    self.consolidate_imports_info()

  def visit_ImportFrom(self, imp):
    for comment in self.comment_flags:
      if self.python_source_line[imp.lineno - 1].endswith(comment): return
    imported = self.imported_in_context.setdefault(self.current_context, set())
    imported.update(
      imported_name(name.name if name.asname is None else name.asname,
                    imp.lineno)
      for name in imp.names
      if name.name != '*' and imp.module not in self.ignored_imports_from)
    imported = self.imported_from_full_name_in_context.setdefault(
      self.current_context, set())
    imported.update( tuple(imp.module.split('.')) + (name.name,)
                     for name in imp.names if name.name != '*')
    self.consolidate_imports_info()

  def consolidate_imports_info(self):
    for ctx, imported in self.imported_in_context.items():
      discarded = set()
      for imp1 in self.imported_from_full_name_in_context.get(ctx, set()):
        for imp in imported:
          if self.is_subpath_of(imp1, imp.name):
            discarded.add(imp)
      imported -= discarded

  def _process_namespace(self, namespace, lineno):
    for import_ctx, imported in self.imported_in_context.items():
      if self.is_subpath_of(import_ctx, self.current_context):
        for imp in imported:
          if lineno < imp.lineno: continue
          if not self.is_subpath_of(imp.name, namespace): continue
          self._used.add(imp)

  def visit_Name(self, name):
    self._process_namespace((name.id,), name.lineno)

  def visit_Attribute(self, attr):
    namespace = []
    x = attr.value
    while not isinstance(x, ast.Name):
      if isinstance(x, ast.Attribute):
        namespace.append(x.attr)
        x = x.value
      elif isinstance(x, ast.Call):
        for arg in x.args:
          self.visit(arg)
        for keyword in x.keywords:
          self.visit(keyword)
        namespace = []
        x = x.func
      elif isinstance(x, ast.Subscript):
        self.visit(x.slice)
        namespace = []
        x = x.value
      else:
        return
    namespace.append(x.id) # got an instance of Name now
    namespace = tuple(reversed(namespace))
    self._process_namespace(namespace, attr.lineno)
    self._process_namespace(namespace + (attr.attr,), attr.lineno)

  def visit_FunctionDef(self, func):
    self.current_context += (func.name,)
    for default in func.args.defaults:
      self.visit(default)
    for stmt in func.body: self.visit(stmt)
    self.current_context = self.current_context[:-1]



class old_style_class(object):

  __slots__ = ('name', 'lineno')

  def __init__(self, name, lineno):
    self.name = tuple(name.split('.'))
    self.lineno = lineno

  def __eq__(self, other):
    return (self.name == other.name
            and self.lineno == other.lineno)

  def __ne__(self, other):
    return not self.__eq__(other)

  def __hash__(self):
    return hash((self.name, self.lineno))

  def __repr__(self):
    return 'class %s defined at line %i' % (self.name_as_str(), self.lineno)

  def name_as_str(self):
    return '.'.join(self.name)


class find_old_style_classes(ast.NodeVisitor, object):
  """ Finds old-style classes (i.e. ones that don't inherit from object)
  """

  @classmethod
  def is_subpath_of(cls, a, b):
    """ Whether a is a subpath of b """
    if len(a) > len(b): return False
    return a == b[:len(a)]

  def __init__(self, python_source_code=None, python_source_filename=None):
    assert (python_source_code, python_source_filename).count(None) == 1
    if python_source_code is None:
      python_source_code = file(python_source_filename).read()
    super(find_old_style_classes, self).__init__()
    self.python_source_line = python_source_code.splitlines()
    self.current_context = () # start at module level
    tree = ast.parse(python_source_code)
    self._old_style_classes = set()
    self.visit(tree)

  @property
  def names(self):
    return set( imp.name_as_str() for imp in self )

  def __repr__(self):
    return '\n'.join( str(imp) for imp in self )

  def __iter__(self):
    return iter(self._old_style_classes)

  def __bool__(self):
    return bool(self._old_style_classes)

  def visit_ClassDef(self, node):
    #print node.name, [n.id for n in node.bases]
    if len(node.bases) == 0:
      self._old_style_classes.add(old_style_class(node.name, node.lineno))
      #print node.name


 *******************************************************************************


 *******************************************************************************
libtbx/queuing_system_utils/__init__.py
from __future__ import absolute_import, division, print_function

import sys

from libtbx import Auto

class chunk_manager(object):

  def __init__(self, n, i):
    assert n > 0
    assert i >= 0
    assert i < n
    self.n = n
    self.i = i
    self.queuing_system_info = None

  def easy_all(self, log_format=Auto, out=Auto):
    self.queuing_system_overrides_chunk()
    self.redirect_chunk_stdout_and_stderr(log_format=log_format, out=out)
    return self

  def skip_iteration(self, i):
    return (i % self.n != self.i)

  def queuing_system_overrides_chunk(self):
    from libtbx.queuing_system_utils import pbs_utils, sge_utils
    pbs_info = pbs_utils.chunk_info()
    sge_info = sge_utils.info()
    assert [pbs_info, sge_info].count(None) <= 1
    if pbs_info.have_array():
      self.queuing_system_info = pbs_info
      n, i = pbs_info.as_n_i_pair()
      self.n = max(self.n, n)
      self.i = i
    elif sge_info.have_array():
      self.queuing_system_info = sge_info
      self.n = max(self.n, sge_info.last)
      self.i = sge_info.id - 1
    return self

  def redirect_chunk_stdout_and_stderr(self,
        log_format=Auto,
        out=Auto,
        have_array=False):
    if self.n == 1: return
    log_name = None
    if not have_array:
      i = self.queuing_system_info
      if i is not None and i.have_array():
        have_array = True
    if have_array:
      if log_format is Auto: log_format="log%%0%dd"
      fmt = log_format % max(3, len("%d" % (self.n-1)))
      log_name = fmt % self.i
      log = open(log_name, "w")
      sys.stdout = log
      sys.stderr = log
    from libtbx.utils import host_and_user
    if out is Auto: out = sys.stdout
    if out is not None:
      host_and_user().show(out=out)
      print("chunk.n:", self.n, file=out)
      print("chunk.i:", self.i, file=out)
      if log_name:
        print("log_name:", log_name, file=out)
      print(file=out)
    return self

# XXX tested on SGE only so far (2012-12-19)
def qdel(job_id, platform):
  """
  Stop a queue job.  Supports the same platforms as 'processing' sub-module,
  but primarily used by the Phenix GUI.
  """
  from libtbx import easy_run
  assert platform in ("sge", "lsf", "pbs", "condor", "pbspro", "slurm")
  cmd = None
  if platform in ("sge", "pbs", "pbspro", "slurm"):
    cmd = "qdel %s" % job_id
  elif platform == "lsf":
    cmd = "bkill %s" % job_id
  elif platform == "condor":
    cmd = "condor_rm %s" % job_id
  assert cmd
  qdel_out = easy_run.fully_buffered(
    command=cmd).raise_if_errors().stdout_lines
  print("\n".join(qdel_out))
  # XXX this is specific to SGE - need error handling for other systems too
  for line in qdel_out:
    if "denied" in line:
      if "does not exist" in line:  # SGE job does not exist
        pass
      else:
        raise RuntimeError("\n".join(qdel_out))
  return True


 *******************************************************************************


 *******************************************************************************
libtbx/queuing_system_utils/communication.py
from __future__ import absolute_import, division, print_function

from libtbx.scheduling import result

from six.moves import cPickle as pickle

class Server(object):
  """
  Communication server
  """

  def __init__(self, instream, outstream, environment):

    self.instream = instream
    self.outstream = outstream
    self.environment = environment
    self.active = True


  def serve(self):

    while self.active:
      command = pickle.load( self.instream )

      try:
        response = command( server = self )

      except Exception as e:
        pickle.dump( result.Error( exception = e ), self.outstream, 0 )

      else:
        pickle.dump( result.Success( value = response ), self.outstream, 0 )

      self.outstream.flush()


class Command(object):
  """
  Command that operates on the environment
  """

  def __call__(self, server):

    return self.process( environment = server.environment )


def ShutDown(server):

  server.environment.shutdown()
  server.active = False
  return True


class Client(object):
  """
  Communication client
  """

  def __init__(self, instream, outstream):

    self.instream = instream
    self.outstream = outstream


  def send(self, command):

    pickle.dump( command, self.outstream, 0 )
    self.outstream.flush()
    return pickle.load( self.instream )


  def close(self):

    return self.send( command = ShutDown )


 *******************************************************************************


 *******************************************************************************
libtbx/queuing_system_utils/generic.py
"""
Generic module to provide parallel job execution on queuing systems

Provides drop-in replacement classes to those defined in the multiprocessing
module (Queue and Job), with certain restrictions placed by the pickle module
"""
from __future__ import absolute_import, division, print_function

from six.moves import cPickle as pickle
from six.moves.queue import Empty as QueueEmptyException
import subprocess
import os
import time
import itertools
import glob
import re

import libtbx.load_env


class InstantTimeout(object):
  """
  Timeout immediately
  """

  def delay(self, waittime):

    raise QueueEmptyException("No data found in queue")


class TimedTimeout(object):
  """
  Timeout after given time
  """

  def __init__(self, max_delay):

      self.max_delay = max_delay


  def delay(self, waittime):

    if waittime <= self.max_delay:
      self.max_delay -= waittime
      time.sleep( waittime )

    else:
      raise QueueEmptyException("No data found in queue within timeout")


class NoTimeout(object):
  """
  No timeout
  """

  def delay(self, waittime):

    time.sleep( waittime )


class Queue(object):
  """
  Queue object to receive data from jobs running on remote machines

  Data transfer is achieved via files. It is safe to use any number of
  Queue objects in the same directory, even with a matching identifier
  """

  WAITTIME = 0.1

  def __init__(self, identifier):

    self.root = "%s_%d_%d" % ( identifier, os.getpid(), id( self ) )
    self.count = itertools.count()
    self.waiting = []


  def put(self, obj):

    index = next(self.count)
    # Writing a tempfile and renaming it may prevent reading incomplete files
    tmp_name = "%s.%d.tmp" % ( self.root, index )
    assert not os.path.exists( tmp_name )
    ofile = open( tmp_name, "wb" )
    pickle.dump( obj, ofile )
    ofile.close()
    target_name = "%s.%d" % ( self.root, index )
    assert not os.path.exists( target_name )
    os.rename( tmp_name, target_name )


  def get(self, block = True, timeout = None):

    if not block:
      predicate = InstantTimeout()

    else:
      if timeout is not None:
        predicate = TimedTimeout( max_delay = timeout )

      else:
        predicate = NoTimeout()

    while True:
      fname = next(self)

      if fname is not None:
        break

      predicate.delay( waittime = self.WAITTIME )

    assert fname is not None
    data = pickle.load( open( fname, "rb" ) )
    os.remove( fname )
    return data


  def next(self):

    if not self.waiting:
      self.read_waiting()

    try:
      return self.waiting.pop( 0 )

    except IndexError:
      return None


  def read_waiting(self):

    waiting = []

    for fname in glob.glob( "%s.*" % self.root ):
      if fname[-4] == ".tmp":
        continue

      ( root, ext ) = os.path.splitext( fname )

      try:
        number = int( ext[1:] )

      except ValueError:
        continue

      waiting.append( ( number, fname ) )

    waiting.sort( key = lambda p: p[0] )
    self.waiting = [ p[1] for p in waiting ]


class Job(object):
  """
  Job object to execute function calls on remote machines accessible via
  queuing systems

  Data transfer is achieved via files. It is safe to use any number of
  Job objects in the same directory, even with a matching identifier

  Restrictions: target has to be pickleable
  """

  SCRIPT = \
"""\
source %s
libtbx.python << EOF
from six.moves import cPickle as pickle
( target, args, kwargs ) = pickle.load( open( "%s.target" ) )
target( *args, **kwargs )
EOF
"""
  SETPATHS = libtbx.env.under_build("setpaths.sh")

  def __init__(self, name, target, qinterface, args = (), kwargs = {}):

    self.name = "%s_%d_%d" % ( name, os.getpid(), id( self ) )
    self.target = target
    self.args = args
    self.kwargs = kwargs
    self.qinterface = qinterface
    self.process = None
    self.jobid = None

  def start(self):

    if self.process is not None:
      raise RuntimeError("start called second time")

    self.write_input_data()

    cmd = self.qinterface(
      name = self.name,
      out = self.out_file(),
      err = self.err_file()
      )

    try:
        self.process = subprocess.Popen(
          cmd,
          stdin = subprocess.PIPE,
          stdout = subprocess.PIPE,
          stderr = subprocess.STDOUT
          )

    except OSError as e:
        raise RuntimeError("Error while executing: '%s': %s" % (
            " ".join( cmd ),
            e,
            ))

    self.process.stdin.write( self.SCRIPT % ( self.SETPATHS, self.name ) )
    self.process.stdin.close()
    self.parse_process_stdout()

  # grab job ID, etc. from stdout - subclass as necessary (PBSJob already
  # handles this separately)
  def parse_process_stdout(self):
    pass

  def is_alive(self):

    if self.process is None:
      raise RuntimeError("job has not been submitted yet")

    return self.process.poll() is None


  def join(self):

    while self.is_alive():
      time.sleep( 0.1 )

    if os.path.exists( self.err_file() ):
      error = open( self.err_file() ).read()

      if error:
        raise RuntimeError(error)

    for fname in [ self.target_file(), self.out_file(), self.err_file() ]:
      if os.path.exists( fname ):
        os.remove( fname )


  def write_input_data(self):

    ifile = open( self.target_file(), "wb" )
    pickle.dump( ( self.target, self.args, self.kwargs ), ifile )
    ifile.close()


  def target_file(self):

    return "%s.target" % self.name


  def out_file(self):

    return "%s.out" % self.name


  def err_file(self):

    return "%s.err" % self.name


class PBSJob(Job):
  """
  Job object to execute function calls on remote machines accessible via
  Portable Batch System (PBS)

  Data transfer is achieved via files. It is safe to use any number of
  Job objects in the same directory, even with a matching identifier

  If PBS support for 'wait for job to finish' is available, this class can be
  merged into Job

  Restrictions: target has to be picklable
  """

  REGEX = re.compile( r"job_state\s*=\s*(\w+)" )

  def __init__(self, name, target, qinterface, args = (), kwargs = {}):

    self.name = "%s_%d_%d" % ( name, os.getpid(), id( self ) )
    self.target = target
    self.args = args
    self.kwargs = kwargs
    self.qinterface = qinterface
    self.jobid = None


  def start(self):

    if self.jobid is not None:
      raise RuntimeError("start called second time")

    self.write_input_data()

    cmd = self.qinterface(
      name = self.name,
      out = self.out_file(),
      err = self.err_file()
      )

    try:
        process = subprocess.Popen(
          cmd,
          stdin = subprocess.PIPE,
          stdout = subprocess.PIPE,
          stderr = subprocess.PIPE
          )

    except OSError as e:
        raise RuntimeError("Error while executing: '%s': %s" % (
            " ".join( cmd ),
            e,
            ))

    ( out, err ) = process.communicate(
      input = self.SCRIPT % ( self.SETPATHS, self.name )
      )

    if err:
      raise RuntimeError(err)

    assert out is not None
    self.jobid = out.strip()


  def is_alive(self):

    return self.job_status() != "C"


  def job_status(self):

    if self.jobid is None:
      raise RuntimeError("job has not been submitted yet")

    process = subprocess.Popen(
      ( "qstat", "-f", self.jobid ),
      stdout = subprocess.PIPE,
      stderr = subprocess.PIPE
      )
    ( out, err ) = process.communicate()

    if err:
      # may be better to indicate finish, in case job has already been deleted
      raise RuntimeError("Jobid: %s\nPBS error: %s" % ( self.jobid, err ))

    m = self.REGEX.search( out )

    if not m:
      raise RuntimeError("Incorrect qstat output: %s" % out)

    return m.group( 1 )

class SGEJob(Job):
  def parse_process_stdout(self):
    qsub_out = self.process.stdout.readlines()
    for line in qsub_out :
      if line.startswith("Your job"):
        self.jobid = int(line.split()[2])
        break

class queue_interface(object):
  COMMAND = None
  def __init__(self, command, asynchronous=False):
    self.async_ = asynchronous
    if command is None:
      assert (self.COMMAND is not None)
      self.command = [ self.COMMAND, ]
    else:
      if (isinstance(command, str)):
        self.command = [ command, ]
      else :
        self.command = command

class sge_interface(queue_interface):
  """
  Interface to Sun Grid Engine (SGE)
  """

  COMMAND = "qsub"

  def __call__(self, name, out, err):

    cmd = self.command + [ "-S", "/bin/sh", "-cwd", "-N", name, "-o", out,
      "-e", err ]
    if (not self.async_):
      cmd.extend(["-sync", "y"])
    return cmd


class lsf_interface(queue_interface):
  """
  Interface to Load Sharing Facility (LSF)
  """

  COMMAND = "bsub"

  def __call__(self, name, out, err):

    cmd = self.command +  [ "-J", name, "-o", out, "-e", err ]
    if (not self.async_):
      cmd.append("-K")
    return cmd

class pbs_interface(queue_interface):
  """
  Interface to Portable Batch System (PBS)
  """

  COMMAND = "qsub"

  def __call__(self, name, out, err):

    return self.command + [ "-d", ".", "-N", name, "-o", out, "-e", err ]

def qsub(target,
          name="libtbx_python",
          platform="sge",
          command=None,
          asynchronous=True):
  assert hasattr(target, "__call__")
  if (platform == "sge"):
    return SGEJob(
      target=target,
      name=name,
      qinterface=sge_interface(command, asynchronous))
  elif (platform == "pbs"):
    return PBSJob(
      target=target,
      name=name,
      qinterface=pbs_interface(command, asynchronous))
  elif (platform == "lsf"):
    return Job(
      target=target,
      name=name,
      qinterface=lsf_interface(command, asynchronous))


 *******************************************************************************


 *******************************************************************************
libtbx/queuing_system_utils/looping.py
from __future__ import absolute_import, division, print_function
from six.moves import range
def isodd(n):
  return n & 1 and True or False

def both_same_odd_or_even(n,m):
  if isodd(n)==isodd(m):
    return True
  return False

def generate_balanaced_list(n, only=None):
  for i in range(n):
    if only is not None and only!=i: continue
    for j in range(n):
      if j>i:
        if both_same_odd_or_even(i+1,j+1):
          pass
        else:
          continue
      elif j<i:
        if not both_same_odd_or_even(i+1, j+1):
          pass
        else:
          continue
      else:
        continue
      yield i,j
    if only==i: break

def generate_balanaced_list_from_list(l, only=None):
  if only is not None:
    assert only in l
    for i, item in enumerate(l):
      if only==item: break
    only = i
  for i,j in generate_balanaced_list(len(l), only=only):
    yield l[i], l[j]

if __name__=="__main__":
  print('test generate_balanaced_list')
  pairs = {}
  for i,j in generate_balanaced_list(7):
    print(i,j)
    pairs.setdefault(i, [])
    assert j not in pairs[i]
    pairs[i].append(j)
    pairs.setdefault(j, [])
    assert i not in pairs[j]
    pairs[j].append(i)
  print(pairs)
  for key in pairs:
    assert len(pairs[key])==6
    assert key not in pairs[key]
  print("OK")
  for k, (i,j) in enumerate(generate_balanaced_list(7, only=0)):
    print(k,i,j)
  assert k==2
  print('test generate_balanaced_list_from_list')
  l=list("abcdef")
  for i,j in generate_balanaced_list_from_list(l):
    print(i,j)
  print('-'*10)
  for i,j in generate_balanaced_list_from_list(l, only=l[1]):
    print(i,j)
  print("OK")


 *******************************************************************************


 *******************************************************************************
libtbx/queuing_system_utils/lsf.py
from __future__ import absolute_import, division, print_function
from six.moves import cPickle as pickle
import subprocess
#import sys
import os
from six.moves import range

class Job:
  def __init__(self, name, execObj, modules=[], pythonExec='python'):
    '''name must be unique, execObj must be pickle_able and have a run method
        pythonExec is the python command to use e.g. phenix.python'''
    self.name=name
    self.execObj=execObj
    self.submitted=False
    self.pythonExec=pythonExec
    self.modules=modules

  def start(self):
    self.pickleInputFileName=self.name+'_input.pkl'
    self.pickleOutputFileName=self.name+'_output.pkl'
    pickleFile=open(self.pickleInputFileName,'wb')
    pickle.dump(self.execObj,pickleFile)
    pickleFile.close()
    self.scriptFileName=self.name+'.py'
    scriptFile=open(self.scriptFileName,'w')
    print('import pickle', file=scriptFile)
    for m in self.modules:
      print('from %s import *' % m, file=scriptFile)
    print('f=open("%s","rb")' % self.pickleInputFileName, file=scriptFile)
    print('execObj=pickle.load(f)', file=scriptFile)
    print('f.close()', file=scriptFile)
    print('result=execObj.run()', file=scriptFile)
    print('f=open("%s","wb")' % self.pickleOutputFileName, file=scriptFile)
    print('pickle.dump(result,f)', file=scriptFile)
    print('f.close()', file=scriptFile)
    scriptFile.close()
    cmd='bsub -K %s %s' % (
      self.pythonExec,
      self.scriptFileName)

    #print cmd
    self.process = subprocess.Popen(cmd,stdout=subprocess.PIPE,
                                    stderr=subprocess.PIPE,shell=True)
    self.submitted=True

  def isAlive(self):
    return self.process.poll() is None

  def get(self):
    if self.isAlive():
      raise Exception('Job not finished')
    f=open(self.pickleOutputFileName,'rb')
    result=pickle.load(f)
    f.close()
    if self.process.poll() != 0:
      raise Exception("process %s failed" % self.name)
    os.remove(self.pickleInputFileName)
    os.remove(self.pickleOutputFileName)
    os.remove(self.scriptFileName)
    return result

class testObj:
  def __init__(self):
    self.data=1
    self.result=[]

  def run(self):
    import math
    for i in range(10000):
      self.result.append(math.sin(i+self.data))
    return self

if __name__=='__main__':
  import time
  o=testObj()
  j=Job('j1',o,modules=['lsf'],pythonExec='phenix.python')
  j.start()
  print(j.isAlive())
  while j.isAlive():
    time.sleep(10)
  r=j.get()
  print(r.result[0])
  print(len(r.result))


 *******************************************************************************


 *******************************************************************************
libtbx/queuing_system_utils/pbs_utils.py
"Portable Batch System (PBS) utilities"
from __future__ import absolute_import, division, print_function

import sys
import os

def eval_env(variable_name):
  return eval(os.environ.get(variable_name, "None"))

def raise_if_none(value, variable_name):
  if (value is None):
    raise RuntimeError(
      "Required environment variable not defined: %s" % variable_name)

class chunk_info(object):

  def __init__(self):
    self.pbs_array_size = eval_env("MY_PBS_ARRAY_SIZE")
    self.pbs_arrayid_offset = eval_env("MY_PBS_ARRAYID_OFFSET")
    self.pbs_arrayid = eval_env("PBS_ARRAYID")
    self.mpi_num_procs = eval_env("OMPI_MCA_ns_nds_num_procs")
    self.mpi_vpid = eval_env("OMPI_MCA_ns_nds_vpid")

  def show(self, out=None, prefix="", even_if_none=False):
    if (out is None): out = sys.stdout
    if (self.pbs_array_size is not None or even_if_none):
      print(prefix+"MY_PBS_ARRAY_SIZE =", self.pbs_array_size, file=out)
    if (self.pbs_arrayid_offset is not None or even_if_none):
      print(prefix+"MY_PBS_ARRAYID_OFFSET =", self.pbs_arrayid_offset, file=out)
    if (self.pbs_arrayid is not None or even_if_none):
      print(prefix+"PBS_ARRAYID =", self.pbs_arrayid, file=out)
    if (self.mpi_num_procs is not None or even_if_none):
      print(prefix+"OMPI_MCA_ns_nds_num_procs =", self.mpi_num_procs, file=out)
    if (self.mpi_vpid is not None or even_if_none):
      print(prefix+"OMPI_MCA_ns_nds_vpid =", self.mpi_vpid, file=out)
    return self

  def have_array(self):
    return self.pbs_array_size is not None

  def as_n_i_pair(self):
    if (not self.have_array()): return 1, 0
    raise_if_none(self.pbs_arrayid_offset, "MY_PBS_ARRAYID_OFFSET")
    raise_if_none(self.pbs_arrayid, "PBS_ARRAYID")
    if (self.mpi_num_procs is None and self.mpi_vpid is None):
      n = self.pbs_array_size
      i = self.pbs_arrayid-1 + self.pbs_arrayid_offset
    else:
      raise_if_none(self.mpi_num_procs, "OMPI_MCA_ns_nds_num_procs")
      raise_if_none(self.mpi_vpid, "OMPI_MCA_ns_nds_vpid")
      n = self.pbs_array_size * self.mpi_num_procs
      i = (self.pbs_arrayid-1 + self.pbs_arrayid_offset) * self.mpi_num_procs + self.mpi_vpid
    return n, i

# XXX this is probably only going to work with newer versions, e.g. Torque
def qstat_parse():
  from libtbx.queuing_system_utils.sge_utils import qstat_items
  from libtbx import easy_run
  from xml.dom import minidom
  qstat_out = easy_run.fully_buffered(
    command="qstat -x").raise_if_errors().stdout_lines
  result = []
  if (len(qstat_out) == 0):
    return result
  xml = minidom.parseString("\n".join(qstat_out))
  jobs = xml.getElementsByTagName("Job")
  def get_tag_content(node, tag_name):
    node = node.getElementsByTagName(tag_name)[0].childNodes[0]
    assert (node.nodeType == node.TEXT_NODE)
    return node.data
  for job in jobs :
    id = get_tag_content(job, "Job_Id")
    name = get_tag_content(job, "Job_Name")
    user = get_tag_content(job, "Job_Owner").split("@")[0]
    state = get_tag_content(job, "job_state")
    prior = get_tag_content(job, "Priority")
    submit = get_tag_content(job, "start_time")
    queue = get_tag_content(job, "queue")
    nodect = get_tag_content(job, "nodect")
    result.append(qstat_items(
      job_id=id,
      prior=prior,
      name=name,
      user=user,
      state=state,
      submit=submit,
      queue=queue,
      slots=nodect,
      ja_task_id="",
      qtype="pbs")) # XXX is there any equivalent for this?
  return result

if (__name__ == "__main__"):
  n,i = chunk_info().show(prefix="*** ", even_if_none=True).as_n_i_pair()
  print("n,i:", n,i)
  print("OK")


 *******************************************************************************


 *******************************************************************************
libtbx/queuing_system_utils/processing/__init__.py
"""
Generic module to provide parallel job execution on queuing systems

Provides drop-in replacement classes to those defined in the multiprocessing
module (Queue and Process), with certain restrictions placed by the pickle
module
"""
from __future__ import absolute_import, division, print_function, with_statement

from six.moves import cPickle as pickle
import os
import time
import itertools
import glob
from six.moves.queue import Empty as QueueEmptyException

class InstantTimeout(object):
  """
  Timeout immediately
  """

  def delay(self, waittime):

    raise QueueEmptyException("No data found in queue")


class TimedTimeout(object):
  """
  Timeout after given time
  """

  def __init__(self, max_delay):

    self.max_delay = max_delay


  def delay(self, waittime):

    if 0 < self.max_delay:
      waittime = min( self.max_delay, waittime )
      self.max_delay -= waittime
      time.sleep( waittime )

    else:
      raise QueueEmptyException("No data found in queue within timeout")


class NoTimeout(object):
  """
  No timeout
  """

  def delay(self, waittime):

    time.sleep( waittime )


class Queue(object):
  """
  Queue object to receive data from jobs running on remote machines

  Data transfer is achieved via files. It is safe to use any number of
  Queue objects in the same directory, even with a matching identifier
  """

  def __init__(self, identifier, waittime = 1):

    self.waittime = waittime
    self.root = "%s_%d_%d" % ( identifier, os.getpid(), id( self ) )
    self.count = itertools.count()


  def put(self, obj):

    index = next(self.count)
    # Writing a tempfile and renaming it may prevent reading incomplete files
    tmp_name = "tmp_%s.%d" % ( self.root, index )
    assert not os.path.exists( tmp_name )

    with open( tmp_name, "wb" ) as ofile:
      pickle.dump( obj, ofile )

    target_name = "%s.%d" % ( self.root, index )
    assert not os.path.exists( target_name )
    os.rename( tmp_name, target_name )


  def get(self, block = True, timeout = None):

    if not block:
      predicate = InstantTimeout()

    else:
      if timeout is not None:
        predicate = TimedTimeout( max_delay = timeout )

      else:
        predicate = NoTimeout()

    while True:
      try:
        data = next(self)

      except StopIteration:
        predicate.delay( waittime = self.waittime )
        continue

      break

    return data


  def next(self):

    waiting = []

    for fname in glob.glob( "%s.*" % self.root ):
      ( root, ext ) = os.path.splitext( fname )

      try:
        number = int( ext[1:] )

      except ValueError:
        continue

      waiting.append( ( number, fname ) )

    if not waiting:
      raise StopIteration

    selected = min( waiting, key = lambda p: p[0] )[1]
    data = pickle.load( open( selected, "rb" ) )
    os.remove( selected )
    return data


class Job(object):
  """
  Job object to execute function calls on remote machines accessible via
  queuing systems

  Restrictions: target, args and kwargs has to be pickleable
  """

  def __init__(self, qinterface, target, args = (), kwargs = {}):

    self.qinterface = qinterface

    self.target = target
    self.args = args
    self.kwargs = kwargs

    from libtbx.queuing_system_utils.processing import status
    self.status = status.NotSubmitted


  @property
  def name(self):

    return "%s_%d" % ( self.qinterface.root, id( self ) )


  @property
  def jobid(self):

    return getattr( self.status, "jobid", None )


  @property
  def exitcode(self):

    return self.status.exit_code()


  def start(self):

    if self.status.is_submitted():
      raise RuntimeError("start called second time")

    data = self.qinterface.input(
      name = self.name,
      target = self.target,
      args = self.args,
      kwargs = self.kwargs,
      )

    self.status = self.qinterface.submitter(
      name = self.name,
      executable = self.qinterface.executable,
      script = self.qinterface.script % data.script(),
      include = self.qinterface.include,
      cleanup = data.files(),
      )


  def is_alive(self):

    return not self.status.is_finished()


  def join(self):

    while self.is_alive():
      time.sleep( 0.1 )

    outcome = self.status.outcome(
      timeout = self.qinterface.join_timeout,
      polltime = self.qinterface.polltime,
      )
    self.status.cleanup()
    self.status = outcome

    if self.status.stdout:
      print(self.status.stdout)

    if self.status.stderr and self.qinterface.display_stderr:
      import sys
      sys.stderr.write( self.status.stderr )
      sys.stderr.write( "\n" )

    if self.qinterface.save_error and self.exitcode != 0:
      self.stacktrace = self.status.stderr

    self.status.strip()


  def terminate(self):

    self.status.terminate()
    outcome = self.status.outcome(
      timeout = self.qinterface.termination_timeout,
      polltime = self.qinterface.polltime,
      )
    self.status.cleanup()
    self.status = outcome


  def __str__(self):

    return "%s(name = '%s')" % ( self.__class__.__name__, self.name )


class QueueHandler(object):
  """
  Handles submission requests for common queuing systems
  """
  SCRIPT = \
"""\
from six.moves import cPickle as pickle
%s
target( *args, **kwargs )
"""

  def __init__(
    self,
    submitter,
    input,
    include,
    root,
    executable = "libtbx.python",
    script = SCRIPT,
    save_error = False,
    display_stderr = True,
    join_timeout = None,
    termination_timeout = 5,
    polltime = 1,
    ):

    self.submitter = submitter
    self.root = "%s%s" % ( root, os.getpid() )
    self.input = input
    self.include = include

    self.executable = executable
    self.script = script

    self.save_error = save_error
    self.display_stderr = display_stderr

    self.join_timeout = join_timeout
    self.termination_timeout = termination_timeout
    self.polltime = polltime


  def Job(self, target, args = (), kwargs = {}):

    return Job(
      qinterface = self,
      target = target,
      args = args,
      kwargs = kwargs,
      )


def get_libtbx_env_setpaths():

  import libtbx.load_env
  return libtbx.env.under_build( "setpaths.sh" )


def SGE(
  name = "libtbx_python",
  command = None,
  asynchronous = True,
  input = None,
  include = None,
  poller = None,
  handler = None,
  save_error = False,
  display_stderr = True,
  ):

  command = process_command_line( command = command, default = [ "qsub" ] )

  from libtbx.queuing_system_utils.processing import submission

  if asynchronous:
    if handler is None:
      from libtbx.queuing_system_utils.processing import status
      handler = status.StdStreamStrategy

    if poller is None:
      from libtbx.queuing_system_utils.processing import polling
      poller = polling.CentralPoller.SGE()

    submitter = submission.AsynchronousCmdLine.SGE(
      poller = poller,
      handler = handler,
      command = command,
      )

  else:
    submitter = submission.Synchronous.SGE( command = command )

  if input is None:
    from libtbx.queuing_system_utils.processing import transfer
    input = transfer.TemporaryFile

  if include is None:
    include = get_libtbx_env_setpaths()

  return QueueHandler(
    submitter = submitter,
    input = input,
    include = include,
    root = name,
    save_error = save_error,
    display_stderr = display_stderr,
    )


def LSF(
  name = "libtbx_python",
  command = None,
  asynchronous = True,
  input = None,
  include = None,
  poller = None,
  handler = None,
  save_error = False,
  display_stderr = True,
  ):

  command = process_command_line( command = command, default = [ "bsub" ] )

  from libtbx.queuing_system_utils.processing import submission

  if asynchronous:
    if poller is None:
      from libtbx.queuing_system_utils.processing import polling
      poller = polling.CentralPoller.LSF()

    submitter = submission.AsynchronousCmdLine.LSF(
      poller = poller,
      command = command,
      )

  else:
    submitter = submission.Synchronous.LSF( command = command )

  if input is None:
    from libtbx.queuing_system_utils.processing import transfer
    input = transfer.TemporaryFile

  if include is None:
    include = get_libtbx_env_setpaths()

  return QueueHandler(
    submitter = submitter,
    input = input,
    include = include,
    root = name,
    save_error = save_error,
    display_stderr = display_stderr,
    )


def PBS(
  name = "libtbx_python",
  command = None,
  asynchronous = True,
  input = None,
  include = None,
  poller = None,
  handler = None,
  save_error = False,
  display_stderr = True,
  ):

  command = process_command_line( command = command, default = [ "qsub" ] )

  from libtbx.queuing_system_utils.processing import submission

  if asynchronous:
    if poller is None:
      from libtbx.queuing_system_utils.processing import polling
      poller = polling.CentralPoller.PBS()

    submitter = submission.AsynchronousCmdLine.PBS(
      poller = poller,
      command = command,
      )

  else:
    raise RuntimeError("PBS does not support synchronous submission")

  if input is None:
    from libtbx.queuing_system_utils.processing import transfer
    input = transfer.TemporaryFile

  if include is None:
    include = get_libtbx_env_setpaths()

  return QueueHandler(
    submitter = submitter,
    input = input,
    include = include,
    root = name,
    save_error = save_error,
    display_stderr = display_stderr,
    )


def PBSPro(
  name = "libtbx_python",
  command = None,
  asynchronous = True,
  input = None,
  include = None,
  poller = None,
  handler = None,
  save_error = False,
  display_stderr = True,
  ):

  command = process_command_line( command = command, default = [ "qsub" ] )

  from libtbx.queuing_system_utils.processing import submission

  if asynchronous:
    if poller is None:
      from libtbx.queuing_system_utils.processing import polling
      poller = polling.CentralPoller.PBSPro()

    submitter = submission.AsynchronousCmdLine.PBSPro(
      poller = poller,
      command = command,
      )

  else:
    raise RuntimeError("Synchronous submission for PBSPro is not supported")

  if input is None:
    from libtbx.queuing_system_utils.processing import transfer
    input = transfer.TemporaryFile

  if include is None:
    include = get_libtbx_env_setpaths()

  return QueueHandler(
    submitter = submitter,
    input = input,
    include = include,
    root = name,
    save_error = save_error,
    display_stderr = display_stderr,
    )


def Condor(
  name = "libtbx_python",
  command = None,
  asynchronous = True,
  input = None,
  include = None,
  poller = None,
  handler = None,
  save_error = False,
  display_stderr = True,
  ):

  command = process_command_line( command = command, default = [ "condor_submit" ] )

  from libtbx.queuing_system_utils.processing import submission

  if asynchronous:
    if poller is None:
      from libtbx.queuing_system_utils.processing import polling
      poller = polling.CentralPoller.Condor()

    submitter = submission.AsynchronousScript.Condor(
      poller = poller,
      command = command,
      )

  else:
    raise RuntimeError("Condor does not support synchronous submission")

  if input is None:
    from libtbx.queuing_system_utils.processing import transfer
    input = transfer.TemporaryFile

  if include is None:
    include = get_libtbx_env_setpaths()

  return QueueHandler(
    submitter = submitter,
    input = input,
    include = include,
    root = name,
    save_error = save_error,
    display_stderr = display_stderr,
    )


def Slurm(
  name = "libtbx_python",
  command = None,
  asynchronous = True,
  input = None,
  include = None,
  poller = None,
  handler = None,
  save_error = False,
  display_stderr = True,
  ):

  from libtbx.queuing_system_utils.processing import submission

  if asynchronous:
    command = process_command_line( command = command, default = [ "sbatch" ] )

    if poller is None:
      from libtbx.queuing_system_utils.processing import polling
      poller = polling.CentralPoller.Slurm()

    submitter = submission.AsynchronousCmdLine.Slurm(
      poller = poller,
      command = command,
      )

  else:
    command = process_command_line( command = command, default = [ "srun" ] )
    submitter = submission.SlurmStream( command = command )

  if input is None:
    from libtbx.queuing_system_utils.processing import transfer
    input = transfer.TemporaryFile

  if include is None:
    include = get_libtbx_env_setpaths()

  return QueueHandler(
    submitter = submitter,
    input = input,
    include = include,
    root = name,
    save_error = save_error,
    display_stderr = display_stderr,
    )


def process_command_line(command, default):

  if command is None:
    return default

  else:
    import shlex
    return shlex.split( command )


def sge_evaluate(command):

  from libtbx.queuing_system_utils.processing import polling

  return polling.SinglePoller(
    command = process_command_line( command = command, default = [ "qstat", "-j" ] ),
    evaluate = polling.sge_single_evaluate,
    )


def lsf_evaluate(command):

  from libtbx.queuing_system_utils.processing import polling

  return polling.SinglePoller(
    command = process_command_line( command = command, default = [ "bjobs" ] ),
    evaluate = polling.lsf_single_evaluate,
    )


def pbs_evaluate(command):

  from libtbx.queuing_system_utils.processing import polling

  return polling.SinglePoller(
    command = process_command_line( command = command, default = [ "qstat", "-f" ] ),
    evaluate = polling.pbs_single_evaluate,
    )


def pbspro_evaluate(command):

  from libtbx.queuing_system_utils.processing import polling

  return polling.CentralPoller(
    command = process_command_line( command = command, default = [ "qstat" ] ),
    evaluate = polling.pbspro_text_evaluate,
    )


def condor_evaluate(command):

  from libtbx.queuing_system_utils.processing import polling

  return polling.CentralPoller(
    command = process_command_line(
      command = command,
      default = [ "condor_q", "-xml" ],
      ),
    evaluate = polling.condor_xml_evaluate,
    )


def slurm_evaluate(command):

  from libtbx.queuing_system_utils.processing import polling

  return polling.SinglePoller(
    command = process_command_line(
      command = command,
      default = [ "squeue", "-o", "%t", "-h", "-j" ],
      ),
    evaluate = polling.slurm_single_evaluate,
    )


INTERFACE_FOR = {
  "sge": ( SGE, sge_evaluate ),
  "lsf": ( LSF, lsf_evaluate ),
  "pbs": ( PBS, pbs_evaluate ),
  "pbspro": (PBSPro, pbspro_evaluate ),
  "condor": ( Condor, condor_evaluate ),
  "slurm": ( Slurm, slurm_evaluate ),
  }

def qsub(
  target,
  name="libtbx_python",
  platform="sge",
  command=None,
  polling_command=None,
  ):

  assert hasattr(target, "__call__")

  if platform not in INTERFACE_FOR:
    raise RuntimeError("Unknown platform: %s" % platform)

  ( factory, poller_factory ) = INTERFACE_FOR[ platform ]

  qinterface = factory(
      name = name,
      command = command,
      poller = poller_factory( command = polling_command ),
      asynchronous = True,
      )

  return qinterface.Job( target = target )


class JobFactory(object):
  """
  Creator for Queue.Job objects
  """

  def __init__(self,
    platform,
    name = None,
    command = None,
    asynchronous = True,
    use_target_file = True,
    preserve_exception_message = True,
    **kwargs
    ):

    from libtbx.queuing_system_utils.processing import transfer
    self.qinterface = INTERFACE_FOR[ platform ][0](
      name = "libtbx_python" if name is None else name,
      command = command,
      asynchronous = asynchronous,
      input = transfer.TemporaryFile if use_target_file else transfer.Stdin,
      save_error = preserve_exception_message,
      display_stderr = False,
      )


  def __call__(self, target, args = (), kwargs = {}):

    return self.qinterface.Job( target = target, args = args, kwargs = kwargs)


 *******************************************************************************


 *******************************************************************************
libtbx/queuing_system_utils/processing/errors.py
from __future__ import absolute_import, division, print_function

class ProcessingError(Exception):
  """
  Base class
  """


# Job control and information
class BatchQueueError(ProcessingError):
  """
  Error with the underlying batch queuing system
  """


class ExecutableError(BatchQueueError):
  """
  There was a problem with executing the requested program
  """


class AbnormalExitError(BatchQueueError):
  """
  The requested program returned an error status code
  """


# Output processing errors
class ExtractionError(ProcessingError):
  """
  Output from the underlying program is not what is expected
  """


 *******************************************************************************


 *******************************************************************************
libtbx/queuing_system_utils/processing/polling.py
from __future__ import absolute_import, division, print_function

import subprocess
import time

from libtbx.queuing_system_utils.processing import util
from libtbx.queuing_system_utils.processing import errors

# These are not very efficient
SGE_REGEX = util.get_lazy_initialized_regex( pattern = r"Following jobs do not exist" )


def sge_single_evaluate(out, err):
  "Evaluates SGE text output in single mode"

  if err:
    if SGE_REGEX().search( err ):
      return True

    else:
      raise errors.AbnormalExitError("SGE error:\n%s" % err)

  else:
    return False


def lsf_single_evaluate(out, err):
  "Evaluates LSF text output in single mode"

  if err:
    return True

  else:
    return False


PBS_SEARCH_REGEX = util.get_lazy_initialized_regex( pattern = r"Unknown Job Id" )
PBS_EVAL_REGEX = util.get_lazy_initialized_regex( pattern = r"job_state\s*=\s*(\w+)" )


def pbs_single_evaluate(out, err):
  "Evaluates PBS text output in single mode"

  if err:
    if PBS_SEARCH_REGEX().search( err ):
      return True

    else:
      raise errors.AbnormalExitError("PBS error:\n%s" % err)

  state = PBS_EVAL_REGEX().search( out )

  if not state:
    raise errors.ExtractionError("Unexpected response from PBS: %r" % out)

  return state.group(1) == "C"


SLURM_SEARCH_REGEX = util.get_lazy_initialized_regex(
  pattern = r"slurm_load_jobs error: Invalid job id specified",
  )
SLURM_CODES = set( [ "PD", "R", "CA", "CF", "CG", "CD", "F", "TO", "NF" ] )

def slurm_single_evaluate(out, err):
  "Evaluates Slurm text output in single mode"

  if err:
    if SLURM_SEARCH_REGEX().search( err ):
      return True

    else:
      raise errors.AbnormalExitError("Slurm error:\n%s" % err)

  state = out.strip()

  if state not in SLURM_CODES:
    raise errors.ExtractionError("Unexpected response from Slurm: %r" % out)

  return state == "CD"


class SinglePoller(object):
  """
  Polls status for single jobs
  """

  def __init__(self, command, evaluate):

    self.command = command
    self.evaluate = evaluate


  def is_finished(self, jobid):

    try:
      process = subprocess.Popen(
        self.command + [ jobid ],
        stdout = subprocess.PIPE,
        stderr = subprocess.PIPE
        )

    except OSError as e:
      cmdline = " ".join( self.command + [ jobid ] )
      raise errors.ExecutableError("'%s': %s" % ( cmdline, e ))

    ( out, err ) = process.communicate()

    return self.evaluate( out = out, err = err )


  def new_job_submitted(self, jobid):

    pass


  @classmethod
  def SGE(cls):

    return cls( command = [ "qstat", "-j" ], evaluate = sge_single_evaluate )


  @classmethod
  def LSF(cls):

    return cls( command = [ "bjobs" ], evaluate = lsf_single_evaluate )


  @classmethod
  def PBS(cls):

    return cls( command = [ "qstat", "-f" ], evaluate = pbs_single_evaluate )


  @classmethod
  def Slurm(cls):

    return cls(
      command = [ "squeue", "-o", "%t", "-h", "-j" ],
      evaluate = slurm_single_evaluate,
      )


class CentralPoller(object):
  """
  Polls job status in batch
  """

  def __init__(self, command, evaluate, waittime = 5):

    self.waittime = waittime
    self.command = command
    self.evaluate = evaluate
    self.running = set()
    self.completed = set()
    self.update()


  def update(self):

    try:
      process = subprocess.Popen(
        self.command,
        stdout = subprocess.PIPE,
        stderr = subprocess.PIPE
        )

    except OSError as e:
      raise errors.ExecutableError("'%s': %s" % ( " ".join( self.command ), e ))

    ( out, err ) = process.communicate()

    if process.poll():
      message = "Poll error: '%s' exited abnormally (code %s, message %s)" % (
        " ".join( self.command ),
        process.poll(),
        err,
        )
      raise errors.AbnormalExitError(message)

    self.running.clear()
    self.completed.clear()

    self.evaluate( out = out, running = self.running, completed = self.completed )
    self.polltime = time.time()


  def is_finished(self, jobid):

    now = time.time()

    if self.waittime < ( now - self.polltime ) or now < self.polltime:
      self.update()

    if jobid in self.running:
      return False

    elif jobid in self.completed:
      return True

    else:
      raise ValueError("Unknown job id")


  def new_job_submitted(self, jobid):

    self.running.add( jobid )


  @classmethod
  def SGE(cls, waittime = 5):

    return cls(
      command = [ "qstat", "-xml" ],
      evaluate = sge_xml_evaluate,
      waittime = waittime,
      )


  @classmethod
  def LSF(cls, waittime = 5):

    return cls(
      command = [ "bjobs" ],
      evaluate = lsf_text_evaluate,
      waittime = waittime,
      )


  @classmethod
  def PBS(cls, waittime = 5):

    return cls(
      command = [ "qstat", "-x" ],
      evaluate = pbs_xml_evaluate,
      waittime = waittime,
      )


  @classmethod
  def PBSPro(cls, waittime = 5):

    return cls(
      command = [ "qstat" ],
      evaluate = pbspro_text_evaluate,
      waittime = waittime,
      )


  @classmethod
  def Condor(cls, waittime = 5):

    return cls(
      command = [ "condor_q", "-xml" ],
      evaluate = condor_xml_evaluate,
      waittime = waittime,
      )


  @classmethod
  def Slurm(cls, waittime = 0.5):

    return cls(
      command = [ "squeue", "-h", "-o", "%i %t" ],
      evaluate = slurm_text_evaluate,
      waittime = waittime,
      )


def sge_xml_evaluate(out, running, completed):
  "Parses SGE xml output"

  import xml.etree.ElementTree as ET
  root = ET.fromstring( out )

  for n in root.iter( "job_list" ):
    status_node = n.find( "JB_job_number" )
    assert status_node is not None
    running.add( status_node.text )


CONDOR_XML_OUTPUT_REGEX = util.get_lazy_initialized_regex(
  pattern = r"^.*?(?=<\?xml)",
  flags = [ "DOTALL" ],
  )

def condor_xml_evaluate(out, running, completed):
  "Parses Condor xml output"

  # Necessary to fix broken XML from Condor 7.2
  xml = CONDOR_XML_OUTPUT_REGEX().sub( "", out )

  import xml.etree.ElementTree as ET
  root = ET.fromstring( xml )

  for job_node in root.iter( "c" ):
    res = [ e for e in job_node.iter( "a" ) if e.attrib.get( "n" ) == "ClusterId" ]
    assert res
    number_node = res[-1].find( "i" )
    assert number_node is not None
    running.add( number_node.text )


def pbs_xml_evaluate(out, running, completed):
  "Parses PBS xml output"

  if not out:
    return

  import xml.etree.ElementTree as ET
  root = ET.fromstring( out )
  for n in root.iter( "Job" ):
    status_node = n.find( "job_state" )
    assert status_node is not None

    if status_node.text == "C":
      completed.add( n.text )

    else:
      running.add( n.text )


LSF_CENTRAL_NO_RESULTS_REGEX = util.get_lazy_initialized_regex(
  pattern = r"No unfinished job found"
  )
LSF_CENTRAL_HEADER_REGEX = util.get_lazy_initialized_regex(
  pattern = r"JOBID\s+USER\s+STAT\s+QUEUE\s+FROM_HOST\s+EXEC_HOST\s+JOB_NAME\s+SUBMIT_TIME"
  )
LSF_CENTRAL_JOBID_REGEX = util.get_lazy_initialized_regex(
  pattern = r"\s*(\S+)\s.+"
  )

def lsf_text_evaluate(out, running, completed):
  "Parses LSF bjobs text output"

  if LSF_CENTRAL_NO_RESULTS_REGEX().search( out ):
    return

  if not LSF_CENTRAL_HEADER_REGEX().match( out ):
    raise errors.ExtractionError("Unexpected response from LSF: %r" % out)

  regex = LSF_CENTRAL_JOBID_REGEX()

  for line in out.splitlines()[1:]:
    match = regex.match( line )

    if not match:
      raise errors.ExtractionError("Unexpected response from LSF: %r" % out)

    jobid = match.group( 1 )
    running.add( jobid )


PBSPRO_CENTRAL_HEADER_REGEX = util.get_lazy_initialized_regex(
  pattern = r"Job id\s+Name\s+User\s+Time Use\s+S\s+Queue\s*\n[ -]*"
  )
PBSPRO_CENTRAL_JOBID_REGEX = LSF_CENTRAL_JOBID_REGEX

def pbspro_text_evaluate(out, running, completed):
  "Parses PBSPro qstat text output"

  if not out.strip():
    return

  if not PBSPRO_CENTRAL_HEADER_REGEX().match( out ):
    raise errors.ExtractionError("Unexpected response from PBSPro: %r" % out)

  regex = PBSPRO_CENTRAL_JOBID_REGEX()

  for line in out.splitlines()[2:]:
    match = regex.match( line )

    if not match:
      raise errors.ExtractionError("Unexpected response from PBSPro: %r" % out)

    jobid = match.group( 1 )
    running.add( jobid )


def slurm_text_evaluate(out, running, completed):
  "Parses Slurm squeue text output"

  for line in out.splitlines():
    pieces = line.split()

    if len( pieces ) != 2:
      raise errors.ExtractionError("Unexpected response from Slurm: %r" % line)

    ( jobid, status ) = pieces

    assert status in SLURM_CODES

    if status == "CD":
      completed.add( jobid )

    else:
      running.add( jobid )


 *******************************************************************************


 *******************************************************************************
libtbx/queuing_system_utils/processing/status.py
from __future__ import absolute_import, division, print_function

import os
import subprocess
import time

from libtbx.queuing_system_utils.processing import errors

# Timeouts
class TimedTimeout(object):
  """
  Timeout after given time
  """

  def __init__(self, max_delay):

    self.max_delay = max_delay


  def __call__(self, waittime):

    if 0 < self.max_delay:
      waittime = min( self.max_delay, waittime )
      self.max_delay -= waittime
      time.sleep( waittime )
      return False

    return True


def NoTimeout(waittime):
  """
  No timeout
  """

  time.sleep( waittime )
  return False


# Status
class NotSubmitted(object):
  """
  Signal that the job has not been submitted yet
  Note: this is a singleton
  """

  @staticmethod
  def is_finished():

    raise RuntimeError("job has not been submitted yet")


  @staticmethod
  def is_submitted():

    return False


  @staticmethod
  def terminate():

    raise RuntimeError("job has not been submitted yet")


  @staticmethod
  def exit_code():

    raise RuntimeError("job has not been submitted yet")


  @staticmethod
  def outcome(timeout, polltime):

    raise RuntimeError("job has not been submitted yet")


  @staticmethod
  def cleanup():

    raise RuntimeError("job has not been submitted yet")


class Finished(object):
  """
  Signals that the job is complete
  """

  def __init__(self, stdout, stderr, exitcode):

    self.stdout = stdout
    self.stderr = stderr
    self.exitcode = exitcode


  @staticmethod
  def is_finished():

    return True


  @staticmethod
  def is_submitted():

    return True


  @staticmethod
  def terminate():

    pass


  def exit_code(self):

    return self.exitcode


  def outcome(self, timeout, polltime):

    return self


  @staticmethod
  def cleanup():

    pass


  # Extra method only available for this class
  def strip(self):

    self.stdout = None
    self.stderr = None


class JobStatus(object):
  """
  Status class
  """

  def __init__(self, outfile, errfile, additional):

    self.outfile = outfile
    self.errfile = errfile
    self.files = [ self.outfile, self.errfile ]
    self.files.extend( additional )


  @staticmethod
  def is_submitted():

    return True


  def cleanup(self):

    for fname in self.files:
      if os.path.exists( fname ):
        os.remove( fname )

    self.outfile = None
    self.errfile = None


  # Protected methods
  def get_stdout(self, timeout, polltime):

    return self.get_file_content(
      filename = self.outfile,
      timeout = timeout,
      polltime = polltime,
      )


  def get_stderr(self, timeout, polltime):

    return self.get_file_content(
      filename = self.errfile,
      timeout = timeout,
      polltime = polltime,
      )


  def get_file_content(self, filename, timeout, polltime):

    if filename is not None:
      timeoutobj = self.get_timeout( timeout = timeout )

      while True:
        try:
          with open( filename ) as infile:
            return infile.read().lstrip()

        except IOError:
          pass

        if timeoutobj( waittime = polltime ):
          return None

    else:
      return None


  # Internal
  def get_timeout(self, timeout):

    if timeout is not None:
      return TimedTimeout( max_delay = timeout )

    else:
      return NoTimeout


class Synchronous(JobStatus):
  """
  Determines job status for synchronous jobs
  """

  def __init__(self, process, outfile, errfile, additional):

    super( Synchronous, self ).__init__(
      outfile = outfile,
      errfile = errfile,
      additional = additional,
      )
    self.process = process


  def is_finished(self):

    return self.process.poll() is not None


  def terminate(self):

    self.process.terminate()


  def exit_code(self):

    self.process.poll()


  def outcome(self, timeout, polltime):

    exitcode = self.exit_code()
    assert exitcode is not None

    stdout = self.get_stdout( timeout = timeout, polltime = polltime )
    stderr = self.get_stderr( timeout = timeout, polltime = polltime )

    if stdout is None or stderr is None:
      stdout = self.process.stdout.read()
      stderr = self.process.stderr.read()
      raise errors.AbnormalExitError("Queue error:%s\n%s" % ( stdout, stderr ))

    return Finished( stdout = stdout, stderr = stderr, exitcode = exitcode )


class SlurmStream(object):
  """
  Specialized handler for Slurm, because file handles of the shell are redirected

  Note that this can fill the buffers, resulting in the job never completing
  """

  def __init__(self, process, additional):

    self.process = process
    self.additional = additional


  def is_finished(self):

    return self.process.poll() is not None


  @staticmethod
  def is_submitted():

    return True


  def terminate(self):

    self.process.terminate()


  def exit_code(self):

    self.process.poll()


  def outcome(self, timeout, polltime):

    exitcode = self.exit_code()
    assert exitcode is not None

    stdout = self.process.stdout.read()
    stderr = self.process.stderr.read()

    return Finished( stdout = stdout, stderr = stderr, exitcode = exitcode )


  def cleanup(self):

    for fname in self.additional:
      if os.path.exists( fname ):
        os.remove( fname )


class Asynchronous(JobStatus):
  """
  Handler for asynchronous jobs
  """

  def __init__(self, jobid, poller, outfile, errfile, additional, qdel):

    super( Asynchronous, self ).__init__(
      outfile = outfile,
      errfile = errfile,
      additional = additional,
      )
    self.jobid = jobid
    self.poller = poller
    self.qdel = qdel

    # Allow poller to update without actual refresh
    self.poller.new_job_submitted( jobid = self.jobid )


  def is_finished(self):

    try:
      result = self.poller.is_finished( jobid = self.jobid )

    except ValueError:
      result = True

    return result


  def terminate(self):

    try:
      process = subprocess.Popen( self.qdel + [ self.jobid ] )

    except OSError as e:
      raise errors.ExecutableError("'%s %s': %s" % ( self.qdel, self.jobid, e ))

    process.communicate()


  def exit_code(self):

    return None


  @classmethod
  def script(cls, include, executable, script, cwd = "."):

    return cls.SCRIPT % ( cwd, include, executable, script )


class StdStreamStrategy(Asynchronous):
  """
  Establishes exit code through stderr
  """

  SCRIPT = \
"""\
cd %s
source %s
%s 2>&1 << EOF
%s
EOF

echo exit_status $? 1>&2
"""

  def outcome(self, timeout, polltime):

    # Error file is used to return exit code
    stderr = self.get_stderr( timeout = timeout, polltime = polltime )

    if stderr is not None:
      try:
        exit_code = extract_exit_code_text( output = stderr )

      except errors.ExtractionError:
        exit_code = 1 # Assume that job died/cancelled

    else:
      exit_code = 0

    stdout = self.get_stdout( timeout = timeout, polltime = polltime )

    if exit_code != 0:
      return Finished( stdout = "", stderr = stdout, exitcode = exit_code )

    else:
      return Finished( stdout = stdout, stderr = "", exitcode = exit_code )


class SlurmStdStreamStrategy(StdStreamStrategy):
  """
  Stdstream strategy specifically for Slurm (different script)
  """

  SCRIPT = \
"""\
#!/bin/sh
cd %s
source %s
srun %s 2>&1 << EOF
%s
EOF

echo exit_status $? 1>&2
"""


class AccountingStrategy(Asynchronous):
  """
  Establishes exit code through accounting
  """

  SCRIPT = \
"""\
cd %s
source %s
%s << EOF
%s
EOF
"""

  def outcome(self, timeout, polltime):

    # Use accounting file to return exit code
    try:
      process = subprocess.Popen(
        [ "qacct", "-j", self.jobid ],
        stdout = subprocess.PIPE,
        stderr = subprocess.PIPE
        )

    except OSError as e:
      raise errors.ExecutableError("'qacct -j %s': %s" % ( self.jobid, e ))

    ( out, err ) = process.communicate()

    if process.poll():
      raise errors.AbnormalExitError("Accounting error:\n%s" % err)

    exit_code = extract_exit_code_text( output = out )

    return Finished(
      stdout = self.get_stdout( timeout = timeout, polltime = polltime ),
      stderr = self.get_stderr( timeout = timeout, polltime = polltime ),
      exitcode = exit_code,
      )


# Helper method
def extract_exit_code_text(output):

  for line in output.splitlines():
    tokens = line.split()

    if len( tokens ) == 2 and tokens[0] == "exit_status":
      exit_code = int( tokens[1] ) # this should work
      return exit_code

  else:
    raise errors.ExtractionError("Unexpected output: %r" % output)


class LogfileStrategy(Asynchronous):
  """
  Establishes exit code through queuing system logfile
  """

  SCRIPT = \
"""\
cd %s
#!/bin/sh
source %s
%s << EOF
%s
EOF
"""

  def __init__(self, jobid, poller, outfile, errfile, logfile, additional, qdel):

    self.logfile = logfile
    super( LogfileStrategy, self ).__init__(
      jobid = jobid,
      poller = poller,
      outfile = outfile,
      errfile = errfile,
      additional = additional + [ self.logfile ],
      qdel = qdel,
      )


  def get_stdlog(self, timeout, polltime):

    return self.get_file_content(
      filename = self.logfile,
      timeout = timeout,
      polltime = polltime,
      )


  def outcome(self, timeout, polltime):

    # Use logfile to return exit code
    stdlog = self.get_stdlog( timeout = timeout, polltime = polltime )

    if stdlog is not None:
      import xml.etree.ElementTree as ET
      logxml = "<events>%s</events>" % stdlog.strip()
      root = ET.fromstring( logxml )
      res = [ e for e in root.iter( "a" ) if e.attrib.get( "n" ) == "ReturnValue" ]

      if not res:
        exit_code = 1 # assume an error has happened

      else:
        exit_code = int( res[-1].find( "i" ).text )

    else:
      exit_code = 0

    return Finished(
      stdout = self.get_stdout( timeout = timeout, polltime = polltime ),
      stderr = self.get_stderr( timeout = timeout, polltime = polltime ),
      exitcode = exit_code,
      )


  def cleanup(self):

    super( LogfileStrategy, self ).cleanup()
    self.logfile = None



 *******************************************************************************


 *******************************************************************************
libtbx/queuing_system_utils/processing/submission.py
from __future__ import absolute_import, division, print_function, with_statement

import subprocess

from libtbx.queuing_system_utils.processing import errors
from libtbx.utils import to_bytes, to_str

class Submission(object):
  """
  Handles job submissions
  """

  def __init__(self, cmds, name, out, err, namelength = None):

    self.cmds = cmds
    self.name = name
    self.out = out
    self.err = err
    self.namelength = namelength


  def create(self, name):

    outfile = "%s.out" % name
    errfile = "%s.err" % name
    commands = self.cmds + [
      self.name, name[ :self.namelength ],
      self.out, outfile,
      self.err, errfile,
      ]

    return ( execute( args = commands ), outfile, errfile )


class Synchronous(Submission):
  """
  Submits jobs synchronously
  """

  SCRIPT = \
"""\
source %s
%s << EOF
%s
EOF
"""

  def __call__(self, name, executable, script, include, cleanup):

    ( process, outfile, errfile ) = self.create( name = name )
    process.stdin.write( self.SCRIPT % ( include, executable, script ) )
    process.stdin.close()

    from libtbx.queuing_system_utils.processing import status

    return status.Synchronous(
      outfile = outfile,
      errfile = errfile,
      additional = cleanup,
      process = process,
      )


  @classmethod
  def SGE(cls, command = [ "qsub" ]):

    return cls(
      cmds = command + [ "-S", "/bin/sh", "-cwd", "-sync", "y" ],
      name = "-N",
      out = "-o",
      err = "-e",
      )


  @classmethod
  def LSF(cls, command = [ "bsub" ]):

    return cls(
      cmds = command + [ "-K" ],
      name = "-J",
      out = "-o",
      err = "-e",
      )


class SlurmStream(object):
  """
  Submits jobs synchronously to Slurm srun, using redirected file handles for
  standard streams
  """

  def __init__(self, command):

    self.cmds = command + [ "-Q" ]


  def __call__(self, name, executable, script, include, cleanup):

    process = execute( args = self.cmds + [ executable ] )
    process.stdin.write( script )
    process.stdin.close()

    from libtbx.queuing_system_utils.processing import status

    return status.SlurmStream( process = process, additional = cleanup )


class AsynchronousCmdLine(Submission):
  """
  Submits jobs asynchronously
  """

  def __init__(self, cmds, qdel, name, out, err, extract, poller, handler, namelength = None, cwd = "."):

    super( AsynchronousCmdLine, self ).__init__(
      cmds = cmds,
      name = name,
      out = out,
      err = err,
      namelength = namelength,
      )
    self.qdel = qdel
    self.extract = extract
    self.poller = poller
    self.handler = handler
    self.cwd = cwd


  def __call__(self, name, executable, script, include, cleanup):

    ( process, outfile, errfile ) = self.create( name = name )

    ( out, err ) = process.communicate(
      input = to_bytes(self.handler.script( include = include, executable = executable, script = script, cwd = self.cwd ))
      )

    if process.poll():
      message = "Submission error: '%s' exited abnormally (code %s, message %s)" % (
        " ".join( self.cmds ),
        process.poll(),
        err,
        )
      raise errors.AbnormalExitError(message)

    return self.handler(
      jobid = self.extract( output = out ),
      poller = self.poller,
      outfile = outfile,
      errfile = errfile,
      additional = cleanup,
      qdel = self.qdel,
      )


  @classmethod
  def SGE(cls, poller, handler, command = [ "qsub" ]):

    return cls(
      cmds = command + [ "-S", "/bin/sh", "-cwd", "-terse" ],
      qdel = [ "qdel" ],
      name = "-N",
      out = "-o",
      err = "-e",
      extract = generic_jobid_extract,
      poller = poller,
      handler = handler,
      )


  @classmethod
  def LSF(cls, poller, command = [ "bsub" ]):

    from libtbx.queuing_system_utils.processing import status

    return cls(
      cmds = command,
      qdel = [ "bkill" ],
      name = "-J",
      out = "-o",
      err = "-e",
      extract = lsf_jobid_extract,
      poller = poller,
      handler = status.StdStreamStrategy,
      )


  @classmethod
  def PBS(cls, poller, command = [ "qsub" ]):

    from libtbx.queuing_system_utils.processing import status

    return cls(
      cmds = command + [ "-d", "." ],
      qdel = [ "qdel" ],
      name = "-N",
      out = "-o",
      err = "-e",
      extract = generic_jobid_extract,
      poller = poller,
      handler = status.StdStreamStrategy,
      )


  @classmethod
  def PBSPro(cls, poller, command = [ "qsub" ]):

    from libtbx.queuing_system_utils.processing import status
    import os

    return cls(
      cmds = command,
      qdel = [ "qdel" ],
      name = "-N",
      out = "-o",
      err = "-e",
      extract = generic_jobid_extract,
      poller = poller,
      handler = status.StdStreamStrategy,
      namelength = 15,
      cwd = os.getcwd(),
      )


  @classmethod
  def Slurm(cls, poller, command = [ "sbatch" ]):

    from libtbx.queuing_system_utils.processing import status

    return cls(
      cmds = command,
      qdel = [ "scancel" ],
      name = "-J",
      out = "-o",
      err = "-e",
      extract = slurm_jobid_extract,
      poller = poller,
      handler = status.SlurmStdStreamStrategy,
      )


class AsynchronousScript(object):
  """
  Submits jobs asynchronously. Options are passed using a script
  """

  CONDOR_SCRIPT = \
"""
Executable     = %s
Universe       = vanilla
initialdir = .

Output  = %s
Error   = %s
Log = %s
Log_xml = True
Notification = Never
Queue
"""

  def __init__(self, cmds, qdel, script, extract, poller, handler):

    self.cmds = cmds
    self.qdel = qdel
    self.script = script
    self.extract = extract
    self.poller = poller
    self.handler = handler


  def create(self):

    return execute( args = self.cmds )


  def __call__(self, name, executable, script, include, cleanup):

    outfile = "%s.out" % name
    errfile = "%s.err" % name
    scriptfile = "%s.condor.script" % name
    logfile = "%s.condor.xml" % name

    with open( scriptfile, "w" ) as ofile:
      ofile.write(
        self.handler.script( include = include, executable = executable, script = script )
        )

    import os
    import stat
    os.chmod( scriptfile, stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR )

    process = self.create()

    ( out, err ) = process.communicate(
      input = self.script % ( scriptfile, outfile, errfile, logfile )
      )

    if process.poll():
      message = "Submission error: '%s' exited abnormally (code %s, message %s)" % (
        " ".join( self.cmds ),
        process.poll(),
        err,
        )
      raise errors.AbnormalExitError(message)

    return self.handler(
      jobid = self.extract( output = out ),
      poller = self.poller,
      outfile = outfile,
      errfile = errfile,
      logfile = logfile,
      additional = cleanup + [ scriptfile ],
      qdel = self.qdel,
      )

  @classmethod
  def Condor(cls, poller, command = [ "condor_submit" ], script = CONDOR_SCRIPT):

    from libtbx.queuing_system_utils.processing import status

    return cls(
      cmds = command,
      qdel = [ "condor_rm" ],
      script = script,
      extract = condor_jobid_extract,
      poller = poller,
      handler = status.LogfileStrategy,
      )


# Helpers
def execute(args):

  try:
    process = subprocess.Popen(
      args,
      stdin = subprocess.PIPE,
      stdout = subprocess.PIPE,
      stderr = subprocess.PIPE,
      )

  except OSError as e:
    raise errors.ExecutableError("'%s': %s" % ( " ".join( args ), e ))

  return process


def lsf_jobid_extract(output):

  match = LSF_JOBID_EXTRACT_REGEX().search( output )

  if not match:
    raise errors.ExtractionError("Unexpected response from LSF: %r" % output)

  return match.group(1)


def generic_jobid_extract(output):

  return output.strip()


def condor_jobid_extract(output):

  match = CONDOR_JOBID_EXTRACT_REGEX().search( output )

  if not match:
    raise errors.ExtractionError("Unexpected response from Condor: %r" % output)

  return match.group(1)


def slurm_jobid_extract(output):

  match = SLURM_JOBID_EXTRACT_REGEX().search( to_str(output) )

  if not match:
    raise errors.ExtractionError("Unexpected response from Slurm: %r" % output)

  return match.group(1)


# Regex caching
from libtbx.queuing_system_utils.processing import util

LSF_JOBID_EXTRACT_REGEX = util.get_lazy_initialized_regex(
  pattern = r"Job <(\d+)> is submitted",
  )

CONDOR_JOBID_EXTRACT_REGEX = util.get_lazy_initialized_regex(
  pattern = r"\d+ job\(s\) submitted to cluster (\d+)\.",
  )

SLURM_JOBID_EXTRACT_REGEX = util.get_lazy_initialized_regex(
  pattern = r"Submitted batch job (\d+)"
  )


 *******************************************************************************


 *******************************************************************************
libtbx/queuing_system_utils/processing/transfer.py
from __future__ import absolute_import, division, print_function, with_statement

from six.moves import cPickle as pickle

class TemporaryFile(object):
  """
  Sends data by writing out a temporary file
  """

  SCRIPT= "( target, args, kwargs ) = pickle.load( open( \"%s\" ) )"

  def __init__(self, name, target, args, kwargs):

    self.target = "%s.target" % name

    with open( self.target, "wb" ) as ifile:
      pickle.dump( ( target, args, kwargs ), ifile )


  def script(self):

    return self.SCRIPT % self.target


  def files(self):

    return [ self.target ]


class Stdin(object):
  """
  Sends data by sending along the command file as a pickled string
  """

  SCRIPT = "( target, args, kwargs ) = pickle.loads( %r )"

  def __init__(self, name, target, args, kwargs):

    self.target = target
    self.args = args
    self.kwargs = kwargs


  def script(self):

    data = pickle.dumps( ( self.target, self.args, self.kwargs ) )
    return self.SCRIPT % data


  def files(self):

    return []


 *******************************************************************************


 *******************************************************************************
libtbx/queuing_system_utils/processing/util.py
from __future__ import absolute_import, division, print_function
from functools import reduce

def get_regex(pattern, flags):

  import re
  import operator
  return re.compile(
    pattern,
    reduce( operator.or_, [ getattr( re, f ) for f in flags ], 0 ),
    )


def get_lazy_initialized_regex(pattern, flags = []):

  from libtbx.object_oriented_patterns import lazy_initialization

  return lazy_initialization( func = get_regex, pattern = pattern, flags = flags )


 *******************************************************************************


 *******************************************************************************
libtbx/queuing_system_utils/remote.py
from __future__ import absolute_import, division, print_function

from libtbx.queuing_system_utils import communication
from six.moves import cPickle as pickle
import threading
from six.moves import queue
import time

class SchedulerActor(object):
  """
  Interface to a scheduling.Manager object that is polled by another thread
  """

  def __init__(self, manager, waittime):

    self.manager = manager
    self.waittime = waittime
    self.jobs_in = queue.Queue()
    self.jobs_out = queue.Queue()

    self.identifier_for = {}
    self.active = True
    self.daemon = threading.Thread( target = self.run )
    self.daemon.daemon = True
    self.daemon.start()


  def run(self):

    while self.active:
      self.manager.poll()

      while self.manager.completed_results:
        current = self.manager.completed_results[0]
        assert current.identifier in self.identifier_for

        try:
          self.jobs_out.put(
            ( self.identifier_for[ current.identifier ], current ),
            block = False,
            )

        except queue.Full:
          break

        self.manager.completed_results.popleft()
        del self.identifier_for[ current.identifier ]

      while True:
        try:
          ( identifier, ( target, args, kwargs ) ) = self.jobs_in.get( block = False )

        except queue.Empty:
          break

        i = self.manager.submit( target = target, args = args, kwargs = kwargs )
        assert i not in self.identifier_for
        self.identifier_for[ i ] = identifier

      if not self.manager.completed_results:
        time.sleep( self.waittime )

    self.manager.join()

    for result in self.manager.results:
      assert result.identifier in self.identifier_for
      self.jobs_out.put( ( self.identifier_for[ result.identifier ], result ) )
      del self.identifier_for[ result.identifier ]


  def shutdown(self):

    self.active = False
    self.daemon.join()


class SubmitJobs(communication.Command):
  """
  Request for job submission
  """

  def __init__(self, job_infos):

    self.job_infos = job_infos


  def process(self, environment):

    for ( identifier, callparms ) in self.job_infos:
      environment.jobs_in.put( ( identifier, callparms ) )


class GetJobs(communication.Command):

  def process(self, environment):

    jobs = []

    while True:
      try:
        jres = environment.jobs_out.get( block = False )

      except queue.Empty:
        break

      jobs.append( jres )

    return jobs


class SchedulerServer(communication.Server):
  """
  Server-side component
  """

  def __init__(self, instream, outstream, manager, waittime = 0.1):

    super( SchedulerServer, self ).__init__(
      instream = instream,
      outstream = outstream,
      environment = SchedulerActor( manager = manager, waittime = waittime ),
      )


class SchedulerClient(communication.Client):
  """
  Client-side connection to a potentially remote SchedulerServer
  """

  def __init__(self, instream, outstream, waittime = 1, submittime = 0.5, poolsize = 4):

    super(SchedulerClient, self).__init__(
      instream = instream,
      outstream = outstream,
      )
    self.running = set()
    self.result_for = {}
    self.waiting = []
    self.waittime = waittime
    self.submittime = submittime
    self.polltime = time.time() # assume queue is empty
    self.enqueuetime = self.polltime
    self.poolsize = poolsize


  def is_known(self, identifier):

    return identifier in self.running or identifier in self.result_for


  def submit(self, identifier, target, args, kwargs):

    assert not self.is_known( identifier = identifier )

    if not self.waiting:
      self.enqueuetime = time.time()

    self.waiting.append( ( identifier, ( target, args, kwargs ) ) )
    self.running.add( identifier )
    self.poll()


  def is_alive(self, identifier):

    self.poll()
    return identifier not in self.result_for


  def get_result(self, identifier):

    return self.result_for[ identifier ]


  def remove(self, identifier):

    del self.result_for[ identifier ]


  def poll(self):

    now = time.time()

    if self.running and ( self.waittime < ( now - self.polltime ) or now < self.polltime ):
      response = self.send( command = GetJobs() )

      jobs = response()

      for ( identifier, value ) in jobs:
        assert identifier in self.running
        self.running.remove( identifier )

        assert identifier not in self.result_for
        self.result_for[ identifier ] = value

      now = time.time()
      self.polltime = now

    if (
      ( self.poolsize <= len( self.waiting ) )
      or ( self.waiting
        and ( self.submittime < ( now - self.enqueuetime ) or now < self.enqueuetime )
        )
      ):
      response = self.send( command = SubmitJobs( job_infos = self.waiting ) )

      try:
        response()

      except Exception as e:
        raise RuntimeError("Submission failure: %s" % e)

      self.waiting = []


  def Job(self, target, args = (), kwargs = {}):

    return Job(
      handler = self,
      target = target,
      args = args,
      kwargs = kwargs,
      )


class PreSubmissionStatus(object):
  """
  A job that has not been submitted yet
  """

  @staticmethod
  def start(job):

    job.handler.submit(
      identifier = job.name,
      target = job.target,
      args = job.args,
      kwargs = job.kwargs,
      )

    job.status = RunningStatus


  @staticmethod
  def is_alive(job):

    raise RuntimeError("job has not been submitted yet")


class RunningStatus(object):
  """
  A job that has been submitted
  """

  @staticmethod
  def start(job):

    raise RuntimeError("start called second time")


  @staticmethod
  def is_alive(job):

    alive = job.handler.is_alive( identifier = job.name )

    if not alive:
      job.result = job.handler.get_result( identifier = job.name )
      job.handler.remove( identifier = job.name )
      job.status = FinishedStatus

    return alive


class FinishedStatus(object):
  """
  A job that has been submitted
  """

  @staticmethod
  def start(job):

    raise RuntimeError("start called second time")


  @staticmethod
  def is_alive(job):

    return False


class Job(object):
  """
  Job object to execute function calls on remote machines accessible via
  a network channel

  Restrictions: target, args and kwargs has to be pickleable
  """

  def __init__(self, handler, target, args = (), kwargs = {}):

    self.handler = handler

    self.target = target
    self.args = args
    self.kwargs = kwargs

    self.status = PreSubmissionStatus
    self.result = None
    self.exitcode = 0


  @property
  def name(self):

    return "remote_job_%d" % id( self )


  def start(self):

    self.status.start( job = self )


  def is_alive(self):

    return self.status.is_alive( job = self )


  def join(self):

    while self.is_alive():
      time.sleep( 0.1 )


  def __str__(self):

    return "%s(name = '%s')" % ( self.__class__.__name__, self.name )


class RemoteFactory(object):
  """
  Remote instance method factory. There is no check that the instance has
  a method passed along with the constructor
  """

  def __init__(self, calculation, method):

    from libtbx.object_oriented_patterns import lazy_initialization
    self.instance = lazy_initialization( func = calculation )
    self.method = method


  def __call__(self, *args, **kwargs):

    return getattr( self.instance(), self.method )( *args, **kwargs )


def object_to_argument(obj):

  return pickle.dumps( obj, 0 )


def argument_to_object(arg):

  return pickle.loads( arg.decode( "string-escape" ) )


def command_merge(cmdline):

  return "%s %r %r %s" % cmdline


def command_unmerge(cmdline):

  return cmdline


def server_process_command_line(
  job_factory,
  queue_factory,
  executable = "libtbx.remote_processing",
  folder = ".",
  transformation = command_merge,
  ):

  return transformation(
    cmdline = (
      executable,
      object_to_argument( obj = job_factory ),
      object_to_argument( obj = queue_factory ),
      "--folder=%s" % folder,
      ),
    )


 *******************************************************************************


 *******************************************************************************
libtbx/queuing_system_utils/scheduling.py
from __future__ import absolute_import, division, print_function

from six.moves.queue import Empty, Full
from libtbx.scheduling import result

import time
from collections import deque
from six.moves import range
from six.moves import zip

class Identifier(object):
  """
  Job identifier
  """

  def __init__(self, target, args, kwargs):

    self.target = target
    self.args = args
    self.kwargs = kwargs


  def __repr__(self):

    return "Identifier(id = %s)" % id( self )


class Result(object):
  """
  Empty calculation result
  """

  def __init__(self, identifier, value):

    self.identifier = identifier
    self.value = value


  def __repr__(self):

    return "Result(id = %s)" % id( self.identifier )


  def __call__(self):

    return self.value()


class ProcessingException(Exception):
  """
  An exception signalling temporary error with post-processing, e.g. a timeout
  """


class NullProcessor(object):
  """
  A singleton class to not do anything
  """

  @staticmethod
  def prepare(target):

    return target


  @staticmethod
  def finalize(identifier):

    return Result( identifier = identifier, value = result.success( value = None ) )


  @staticmethod
  def reset():

    pass


class RetrieveTarget(object):
  """
  A simple mangled target
  """

  def __init__(self, queue, target):

    self.queue = queue
    self.target = target


  def __call__(self, *args, **kwargs):

    result = self.target( *args, **kwargs )
    self.queue.put( result )


class RetrieveProcessor(object):
  """
  Adds a retrieve step
  """

  def __init__(self, queue, block = True, timeout = None, grace = None):

    self.queue = queue
    self.block = block
    self.timeout = timeout
    self.grace = max( timeout, grace )
    self.request_timeout = None


  def prepare(self, target):

    return RetrieveTarget( queue = self.queue, target = target )


  def finalize(self, job):

    try:
      job_result = self.queue.get( block = self.block, timeout = self.timeout )

    except Empty:
      now = time.time()

      if self.request_timeout is None:
        self.request_timeout = now
        raise ProcessingException("Timeout on %s" % self.queue)

      if ( now - self.request_timeout ) <= self.grace:
        raise ProcessingException("Timeout on %s" % self.queue)

      self.reset()
      raise RuntimeError("Timeout on %s" % self.queue)

    self.request_timeout = None

    return job_result


  def reset(self):

    self.request_timeout = None


class FetchProcessor(object):
  """
  Adds a fetch step. Only affects postprocessing. Can only be combined with
  job classes that save the return value as property
  """

  def __init__(self, transformation):

    self.transformation = transformation


  def prepare(self, target):

    return target



  def finalize(self, job):

    return self.transformation( job.result )


  @classmethod
  def Unpack(cls):

    return cls( transformation = lambda r: r() )


  @classmethod
  def Identity(cls):

    return cls( transformation = lambda r: r )


class ExecutionUnit(object):
  """
  Wrapper to make factory functions countable
  """

  def __init__(self, factory, priority = 0, processor = NullProcessor):

    self.factory = factory
    self.priority = priority
    self.processor = processor


  def start(self, target, args, kwargs):

    job = self.factory(
      target = self.processor.prepare( target = target ),
      args = args,
      kwargs = kwargs,
      )
    job.start()
    return job


  def finalize(self, job):

    return self.processor.finalize( job = job )


  def reset(self):

    self.processor.reset()


  def __str__(self):

    return "%s(id = %s)" % ( self.__class__.__name__, id( self ) )


class WorkerProcess(object):
  """
  A class that runs a worker process
  """

  def __call__(self, inqueue, outqueue):

    # Check-in
    outqueue.put( None )

    while True:
      data = inqueue.get()

      # Sentinel
      if not data:
        outqueue.put( None )
        break

      assert len( data ) == 3
      ( target, args, kwargs ) = data
      result = target( *args, **kwargs )
      outqueue.put( result )


class TerminatedException(Exception):
  """
  Exception class signalling that process has died
  """


class WorkerMaintenance(object):
  """
  Methods to create/destroy workers
  """

  def __init__(self, job_factory, queue_create, queue_teardown, grace = 10):

    self.job_factory = job_factory
    self.queue_create = queue_create
    self.queue_teardown = queue_teardown
    self.grace = grace


  def launch(self, executor, inqueue, outqueue):

    process = self.job_factory(
      target = executor,
      kwargs = { "inqueue": inqueue, "outqueue": outqueue },
      )
    process.start()
    return process


  def create(self, executor):

    q1 = self.queue_create()
    q2 = self.queue_create()
    process = self.launch( executor = executor, inqueue = q1, outqueue = q2 )
    return ( process, q2, q1 )


  def connect(self, process, inqueue):

    if process.is_alive():
      result = inqueue.get( block = False )
      assert result is None


  def functional(self, process):

    return process.is_alive()


  def cleanup(self, process, inqueue, outqueue):

    process.join()
    self.queue_teardown( inqueue )
    self.queue_teardown( outqueue )


  def shutdown(self, process, inqueue, outqueue):

    if process.is_alive():
      outqueue.put( None )

      try:
        inqueue.get( timeout = self.grace )

      except Empty:
        process.terminate()

    self.cleanup( process = process, inqueue = inqueue, outqueue = outqueue )


class Worker(object):
  """
  An existing process that runs jobs
  """

  def __init__(self, maintenance, priority = 0):

    self.maintenance = maintenance
    self.priority = priority
    self.startup()


  def startup(self):

    ( self.process, self.inqueue, self.outqueue ) = self.maintenance.create(
      executor = WorkerProcess()
      )
    self.connected = False


  def connect(self):

    assert not self.connected
    self.maintenance.connect( process = self.process, inqueue = self.inqueue )
    self.connected = True


  def shutdown(self):

    self.maintenance.shutdown(
      process = self.process,
      inqueue = self.inqueue,
      outqueue = self.outqueue,
      )


  def restart(self):

    self.shutdown()
    self.startup()


  def functional(self):

    return self.maintenance.functional( process = self.process )


  def start(self, target, args, kwargs):

    self.outqueue.put( ( target, args, kwargs ) )
    return WorkerJob( worker = self )


  def get(self, block):

    if not self.connected:
      self.connect()

    try:
      return self.inqueue.get( block = block )

    except Exception:
      if not self.functional():
        self.restart()
        raise TerminatedException("Worker died")

      else:
        raise


  def finalize(self, job):

    job.join()
    assert job.exitcode == 0

    return job.result


  def reset(self):

    pass


  def __str__(self):

    return "%s(id = %s)" % ( self.__class__.__name__, id( self ) )


class WorkerJob(object):
  """
  A job that is dispatched to a worker
  """

  def __init__(self, worker):

    self.worker = worker
    self.result = None
    self.exitcode = None


  def poll(self, block):

    if self.exitcode is None:
      try:
        self.result = self.worker.get( block = block )
        self.exitcode = 0

      except Empty:
        return True

      except Exception as e:
        self.result = result.error( exception = e )
        self.exitcode = 1
        self.err = e

    return False


  def is_alive(self):

    return self.poll( block = False )


  def join(self):

    self.poll( block = True )


class RunningState(object):
  """
  A job that is currently running
  """

  def __init__(self, job):

    self.job = job


  def is_alive(self):

    return self.job.is_alive()


  def initiate_postprocessing(self, job):

    self.job.join()

    exit_code = getattr( self.job, "exitcode", 0 ) # Thread has no "exitcode" attribute

    if exit_code == 0:
      job.status = PostprocessingState( job = self.job )

    else:
      err = getattr( self.job, "err", RuntimeError( "exit code = %s" % exit_code ) )
      job.status = ValueState( value = result.error( exception = err ) )


  def perform_postprocessing(self, job):

    self.initiate_postprocessing( job = job )
    job.perform_postprocessing()


  def get(self, job):

    self.initiate_postprocessing( job = job )
    return job.get()


  def __str__(self):

    return "running"


class PostprocessingState(object):
  """
  A job that is trying to postprocess
  """

  def __init__(self, job):

    self.job = job


  def is_alive(self):

    return False


  def initiate_postprocessing(self, job):

    raise RuntimeError("initiate_postprocess called second time")


  def perform_postprocessing(self, job):

    try:
      value = result.success( value = job.unit.finalize( job = self.job ) )

    except ProcessingException as e:
      raise

    except Exception as e:
      value = result.error( exception = e )

    job.status = ValueState( value = value )


  def get(self, job):

    self.perform_postprocessing( job = job )
    return job.get()


  def __str__(self):

    return "postprocessing"


class ValueState(object):
  """
  A job that either finished, exited with an error or failed postprocessing
  """

  def __init__(self, value):

    self.value = value


  def is_alive(self):

    return False


  def initiate_postprocessing(self, job):

    raise RuntimeError("initiate_postprocess called second time")


  def perform_postprocessing(self, job):

    pass


  def get(self, job):

    return self.value


  def __str__(self):

    return "finished"


class ExecutingJob(object):
  """
  A job that is executing on a Unit
  """

  def __init__(self, unit, identifier):

    self.unit = unit
    self.identifier = identifier
    self.status = RunningState(
      job = self.unit.start(
        target = identifier.target,
        args = identifier.args,
        kwargs = identifier.kwargs,
        )
      )


  def is_alive(self):

    return self.status.is_alive()


  def initiate_postprocessing(self):

    self.status.initiate_postprocessing( job = self )


  def perform_postprocessing(self):

    self.status.perform_postprocessing( job = self )


  def get(self):

    return Result(
      identifier = self.identifier,
      value = self.status.get( job = self ),
      )


  def __str__(self):

    return "%s(\n  unit = %s,\n  identifier = %s,\n  status = %s\n)" % (
      self.__class__.__name__,
      self.unit,
      self.identifier,
      self.status,
      )


# Manager implementation
"""
Process queue Manager interface

Manager properties:
  waiting_jobs
  running_jobs
  completed_jobs
  known_jobs
  completed_results

Manager iterators:
  results

Manager methods:
  submit(target, args = (), kwargs = {}):
  is_empty():
  is_full():
  wait():
  wait_for(identifier):
  result_for(identifier):
  join():
  poll():
"""


class Allocated(object):
  """
  The number of units is know upfront
  """

  def __init__(self, units):

    self.units = set( units )


  def empty(self):

    return not bool( self.units )


  def put(self, unit):

    self.units.add( unit )


  def get(self):

    unit = max( self.units, key = lambda s: s.priority )
    self.units.remove( unit )
    return unit


class Unlimited(object):
  """
  No limit on the number of execution units
  """

  def __init__(self, factory):

    self.factory = factory


  def empty(self):

    return False


  def put(self, unit):

    pass


  def get(self):

    return self.factory()


class UnlimitedCached(object):
  """
  No limit on the number of execution units. Created units are cached for reuse
  """

  def __init__(self, factory):

    self.factory = factory
    self.cache = set()


  def empty(self):

    return False


  def put(self, unit):

    self.cache.add( unit )


  def get(self):

    try:
      unit = self.cache.pop()

    except KeyError:
      unit = self.factory()

    return unit


class Scheduler(object):
  """
  Pure scheduler
  Job startup and pulldown is delegated to a handler object
  """

  def __init__(self, handler, polling_interval = 0.01):

    self.handler = handler
    self.executing = set()
    self.waiting_jobs = deque()
    self.postprocessing = deque()
    self.completed_results = deque()
    self.polling_interval = polling_interval


  @property
  def executing_jobs(self):

    return [ ej.identifier for ej in self.executing ]


  @property
  def postprocessing_jobs(self):

    return [ ej.identifier for ej in self.postprocessing ]


  @property
  def running_jobs(self):

    return self.executing_jobs + self.postprocessing_jobs


  @property
  def completed_jobs(self):

    return [ result.identifier for result in self.completed_results ]


  @property
  def known_jobs(self):

    return list( self.waiting_jobs ) + self.running_jobs + self.completed_jobs


  @property
  def results(self):

    while self.known_jobs:
      self.wait()
      yield self.completed_results.popleft()


  def submit(self, target, args = (), kwargs = {}):

    identifier = Identifier( target = target, args = args, kwargs = kwargs )
    self.waiting_jobs.append( identifier )
    self.poll()
    return identifier


  def is_empty(self):

    return not ( self.executing or self.postprocessing )


  def is_full(self):

    return self.handler.empty()


  def wait(self):

    while not self.completed_results and not self.is_empty():
      self.poll()
      time.sleep( self.polling_interval )


  def wait_for(self, identifier):

    if identifier not in self.known_jobs:
      raise RuntimeError("Job identifier not known")

    while identifier not in self.completed_jobs and not self.is_empty():
      self.poll()
      time.sleep( self.polling_interval )


  def result_for(self, identifier):

    self.wait_for( identifier = identifier )
    index = self.completed_jobs.index( identifier )
    result = self.completed_results[ index ]
    self.completed_results.remove( result )

    return result


  def join(self):

    while not self.is_empty():
      self.poll()
      time.sleep( self.polling_interval )


  def poll(self):

    # Process finished jobs
    for job in list( self.executing ):
      if not job.is_alive():
        self.executing.remove( job )
        job.initiate_postprocessing()
        self.postprocessing.append( job )

    # Postprocess jobs
    for i in range( len( self.postprocessing ) ):
      ej = self.postprocessing.popleft()

      try:
        ej.perform_postprocessing()

      except ProcessingException:
        self.postprocessing.append( ej )
        continue

      self.completed_results.append( ej.get() )
      self.handler.put( ej.unit )

    # Submit new jobs
    while not self.is_full() and self.waiting_jobs:
      identifier = self.waiting_jobs.popleft()
      unit = self.handler.get()
      job = ExecutingJob( unit = unit, identifier = identifier )
      self.executing.add( job )


# Backward compatibility
def Manager(units, polling_interval = 0.01):

  return Scheduler(
    handler = Allocated( units = units ),
    polling_interval = polling_interval,
    )


class Adapter(object):
  """
  Behaves like a manager, but uses an external resource to run the jobs

  Unintuitive feature:
    adapter.is_empty() == True and adapter.is_full() == True is possible
    simultaneously, even if the number of cpus is not zero. This is because
    adapter.is_empty() report the status of the adapter queue, while
    adapter.is_full() reports that of the manager. This gives the behaviour
    one normally expects, i.e. no submission if there are no free cpus, and
    empty status when jobs submitted through the adaptor have all been
    processed.
  """

  def __init__(self, manager):

    self.manager = manager
    self.completed_results = deque()
    self.active_jobs = set()


  @property
  def waiting_jobs(self):

    return [ j for j in self.manager.waiting_jobs if j in self.active_jobs ]


  @property
  def running_jobs(self):

    return [ j for j in self.manager.running_jobs if j in self.active_jobs ]


  @property
  def completed_jobs(self):

    return [ result.identifier for result in self.completed_results ]


  @property
  def known_jobs(self):

    return self.waiting_jobs + self.running_jobs + self.completed_jobs


  @property
  def results(self):

    while self.known_jobs:
      self.wait()
      yield self.completed_results.popleft()


  def submit(self, target, args = (), kwargs = {}):

    identifier = self.manager.submit( target = target, args = args, kwargs = kwargs )
    self.active_jobs.add( identifier )
    return identifier


  def is_empty(self):

    return ( not self.running_jobs and not self.waiting_jobs )


  def is_full(self):

    return self.manager.is_full()


  def wait(self):

    self.transfer_finished_jobs()

    while not self.completed_jobs and not self.is_empty():
      self.poll()
      time.sleep( self.manager.polling_interval )


  def wait_for(self, identifier):

    if identifier not in self.completed_jobs:
      if identifier not in self.known_jobs:
        raise RuntimeError("Job identifier not known")

      assert identifier in self.manager.known_jobs
      self.manager.wait_for( identifier = identifier )
      self.transfer_finished_jobs()

    assert identifier in self.completed_jobs


  def result_for(self, identifier):

    self.wait_for( identifier = identifier )
    index = self.completed_jobs.index( identifier )
    result = self.completed_results[ index ]
    self.completed_results.remove( result )

    return result


  def join(self):

    while not self.is_empty():
      self.poll()
      time.sleep( self.manager.polling_interval )


  def poll(self):

    self.manager.poll()
    self.transfer_finished_jobs()


  def transfer_finished_jobs(self):

    for result in list( self.manager.completed_results ):
      if result.identifier in self.active_jobs:
        self.manager.completed_results.remove( result )
        self.active_jobs.remove( result.identifier )
        self.completed_results.append( result )


class MainthreadJob(object):
  """
  Adaptor object that behaves like a job factory, but executes the calculation
  on the main thread. This can be used for debugging, or when only one CPU is
  available, as it has the lowest startup overhead.
  It is possible to use more than one, but it does not necessarily makes sense
  """

  def __init__(self, target, args, kwargs):

    self.target = target
    self.args = args
    self.kwargs = kwargs
    self.exception = None


  def start(self):

    try:
      self.target( *self.args, **self.kwargs )

    except Exception as e:
      self.exitcode = 1
      self.err = e


  def join(self):

    pass


  def is_alive(self):

    return False


class MainthreadQueue(object):
  """
  A queue to be used with MainthreadJob
  """

  def __init__(self):

    self.deque = deque()


  def put(self, obj, block = True, timeout = None):

    # NB: block and timeout are ignored, because it is not safe to use this
    #     with multiple threads
    self.deque.append( obj )


  def get(self, block = True, timeout = None):

    # NB: block and timeout are ignored, because it is not safe to use this
    #     with multiple threads
    return self.deque.popleft()


class MainthreadManager(object):
  """
  Dummy process queue executing on the main thread. Implements the Manager
  interface
  """

  def __init__(self, processor):

    self.waiting_jobs = deque()
    self.completed_results = deque()

    self.unit = ExecutionUnit( factory = MainthreadJob, processor = processor )


  @property
  def executing_jobs(self):

    return []


  @property
  def postprocessing_jobs(self):

    return []


  @property
  def running_jobs(self):

    return self.executing_jobs + self.postprocessing_jobs


  @property
  def completed_jobs(self):

    return [ result.identifier for result in self.completed_results ]


  @property
  def known_jobs(self):

    return list( self.waiting_jobs ) + self.running_jobs + self.completed_jobs


  @property
  def results(self):

    while self.known_jobs:
      self.wait()
      yield self.completed_results.popleft()


  def submit(self, target, args = (), kwargs = {}):

    identifier = Identifier( target = target, args = args, kwargs = kwargs )
    self.waiting_jobs.append( identifier )
    return identifier


  def is_empty(self):

    return not self.is_full()


  def is_full(self):

    return bool( self.waiting_jobs )


  def wait(self):

    if not self.completed_results:
      self.poll()


  def wait_for(self, identifier):

    if identifier not in self.known_jobs:
      raise RuntimeError("Job identifier not known")

    while identifier not in self.completed_jobs and not self.is_empty():
      self.poll()


  def result_for(self, identifier):

    self.wait_for( identifier = identifier )
    index = self.completed_jobs.index( identifier )
    result = self.completed_results[ index ]
    self.completed_results.remove( result )

    return result


  def join(self):

    while not self.is_empty():
      self.poll()


  def poll(self):

    if not self.is_empty():
      identifier = self.waiting_jobs.popleft()
      job = ExecutingJob( unit = self.unit, identifier = identifier )
      assert not job.is_alive()
      job.initiate_postprocessing()

      while True:
        try:
          job.perform_postprocessing()

        except ProcessingException:
          time.sleep( 0.1 )
          continue

        break

      self.completed_results.append( job.get() )


# Pool implementation
"""
Process queue Pool interface

Pools offer lower control over execution, as it is not possible to know the
state of individual workers

Pool properties:
  known_jobs
  completed_jobs
  completed_results

  job_count

Pool iterators:
  results

Pool methods:
  submit(target, args = (), kwargs = {}):
  is_empty():
  is_full():
  wait():
  join():
  poll():
  shutdown():
"""

class Immortal(object):
  """
  A worker that is not supposed to die
  """

  def is_alive(self):

    return True


  def job_start(self):

    pass


  def job_done(self):

    pass


class JobCount(object):
  """
  A worker that completes N jobs, and then exits
  """

  def __init__(self, maxjobs):

    self.jobs_to_go = maxjobs


  def is_alive(self):

    return 0 < self.jobs_to_go


  def job_start(self):

    pass


  def job_done(self):

    self.jobs_to_go -= 1


class MaxTime(object):
  """
  A worker that exits when a time limit has been reached
  """

  def __init__(self, seconds):

    self.endtime = time.time() + seconds


  def is_alive(self):

    return time.time() < self.endtime


  def job_start(self):

    pass


  def job_done(self):

    pass


class MaxWallClock(object):
  """
  A worker that exits when a wall clock time limit has been reached
  """

  def __init__(self, seconds):
    from libtbx.development.timers import work_clock
    self.maxtime = work_clock() + seconds


  def is_alive(self):

    from libtbx.development.timers import work_clock
    return work_clock() < self.maxtime


  def job_start(self):

    pass


  def job_done(self):

    pass


def pool_process_cycle(pid, inqueue, outqueue, waittime, lifeman, sentinel):

  manager = lifeman()
  outqueue.put( ( worker_startup_event, pid ) )

  while manager.is_alive():
    try:
      data = inqueue.get( timeout = waittime )

    except Empty:
      continue

    if data == sentinel:
      outqueue.put( ( worker_shutdown_event, pid ) )
      break

    assert len( data ) == 4
    ( jobid, target, args, kwargs ) = data
    outqueue.put( ( job_started_event, ( jobid, pid ) ) )
    manager.job_start()
    result = target( *args, **kwargs )
    outqueue.put( ( job_finished_event, ( jobid, pid, result ) ) )
    manager.job_done()

  else:
    outqueue.put( ( worker_termination_event, pid ) )


class ProcessRegister(object):
  """
  A simple class that encapsulates job management
  """

  def __init__(self):

    self.running_on = {}
    self.terminateds = deque()
    self.requested_shutdowns = deque()
    self.results = deque()


  @property
  def process_count(self):

    return len( self.running_on )


  def record_process_startup(self, pid):

    if pid in self.running_on:
      raise RuntimeError("Existing worker with identical processID")

    self.running_on[ pid ] = None


  def record_job_start(self, jobid, pid):

    if pid not in self.running_on:
      raise RuntimeError("Unknown processID")

    if self.running_on[ pid ] is not None:
      raise RuntimeError("Attempt to start process on busy worker")

    self.running_on[ pid ] = jobid


  def record_job_finish(self, jobid, pid, value):

    if pid not in self.running_on:
      raise RuntimeError("Unknown processID")

    if self.running_on[ pid ] != jobid:
      raise RuntimeError("Inconsistent register information: jobid/pid mismatch")

    self.running_on[ pid ] = None
    self.results.append( ( jobid, result.success( value = value ) ) )


  def record_process_shutdown(self, pid):

    self.record_process_exit( pid = pid, container = self.requested_shutdowns )


  def record_process_termination(self, pid):

    self.record_process_exit( pid = pid, container = self.terminateds )


  def record_process_exit(self, pid, container):

    if pid not in self.running_on:
      raise RuntimeError("Unknown processID")

    if self.running_on[ pid ] is not None:
      raise RuntimeError("Shutdown of busy worker")

    container.append( pid )
    del self.running_on[ pid ]


  def record_process_crash(self, pid, exception):

    if pid not in self.running_on:
      raise RuntimeError("Unknown processID")

    jobid = self.running_on[ pid ]

    if jobid is not None:
      self.results.append( ( jobid, result.error( exception = exception ) ) )

    del self.running_on[ pid ]
    self.terminateds.append( pid )


# Stand-alone functions for pickling
def job_started_event(register, data):

  ( jobid, pid ) = data
  register.record_job_start( jobid = jobid, pid = pid )


def job_finished_event(register, data):

  ( jobid, pid, value ) = data
  register.record_job_finish( jobid = jobid, pid = pid, value = value )


def worker_startup_event(register, data):

  register.record_process_startup( pid = data )


def worker_shutdown_event(register, data):

  register.record_process_shutdown( pid = data )


def worker_termination_event(register, data):

  register.record_process_termination( pid = data )


def worker_crash_event(register, data):

  ( pid, exception ) = data
  register.record_process_crash( pid = pid, exception = exception )


class ConstantStrategy(object):
  """
  Keep pool size constant
  """

  def __init__(self, capacity):

    self.capacity = capacity


  def target(self, job_count, process_count):

    return self.capacity


def normal_state(strategy, target, process_count):

  if target < process_count:
    strategy.state = downsize_state()
    target = process_count

  return target


class downsize_state(object):

  def __init__(self):

    self.starttime = time.time()


  def __call__(self, strategy, target, process_count):

    if process_count <= target or strategy.grace < time.time() - self.starttime:
      strategy.state = normal_state

    else:
      target = process_count

    return target


class BreathingStrategy(object):
  """
  Varies pool size according to load
  """

  def __init__(self, minimum, maximum, buffering, grace = 1):

    self.minimum = minimum
    self.maximum = maximum
    self.buffering = buffering
    self.grace = grace
    self.state = normal_state


  @property
  def capacity(self):

    return self.maximum


  def target(self, job_count, process_count):

    target = max(
      min( self.maximum, job_count + self.buffering ),
      self.minimum,
      )

    return self.state(
      strategy = self,
      target = target,
      process_count = process_count,
      )


class ProcessPool(object):
  """
  Process pool
  """

  SENTINEL = None

  def __init__(self,
    inqueue,
    outqueue,
    job_factory,
    loadmanager,
    lifemanager,
    waittime = 0.01,
    grace = 2,
    ):

    self.job_factory = job_factory
    self.loadmanager = loadmanager
    self.lifemanager = lifemanager

    self.inqueue = inqueue
    self.outqueue = outqueue

    self.waittime = waittime
    self.grace = grace

    self.register = ProcessRegister()

    from itertools import count

    self.pid_assigner = count()
    self.process_numbered_as = {}
    self.recycleds = deque()
    self.terminatings = set()
    self.unreporteds = deque()
    self.outstanding_shutdown_requests = 0

    self.identifier_for = {}
    self.completed_results = deque()

    self.manage()


  @property
  def job_count(self):

    return len( self.identifier_for )


  @property
  def completed_jobs(self):

    return [ result.identifier for result in self.completed_results ]


  @property
  def known_jobs(self):

    return list(self.identifier_for.values()) + self.completed_jobs


  @property
  def process_count(self):

    return max(
      ( len( self.process_numbered_as ) + len( self.terminatings )
        - self.outstanding_shutdown_requests ),
      0,
      )


  @property
  def results(self):

    while self.known_jobs:
      self.wait()
      yield self.completed_results.popleft()


  def submit(self, target, args = (), kwargs = {}):

    identifier = Identifier( target = target, args = args, kwargs = kwargs )
    jobid = id( identifier )
    self.outqueue.put( ( jobid, target, args, kwargs ) )
    self.identifier_for[ jobid ] = identifier
    return identifier


  def is_empty(self):

    return not self.identifier_for


  def is_full(self):

    return self.loadmanager.capacity <= self.job_count


  def wait(self):

    while not self.completed_results and not self.is_empty():
      self.poll()
      time.sleep( self.waittime )


  def poll(self):

    for ( pid, process ) in self.process_numbered_as.items():
      if not process.is_alive():
        process.join()

        exit_code = getattr( process, "exitcode", 0 ) # Thread has no "exitcode" attribute

        if exit_code != 0:
          err = getattr(
            process,
            "err",
            RuntimeError( "exit code = %s" % exit_code ),
            )
          self.unreporteds.append( ( worker_crash_event, ( pid, err ) ) )

        self.terminatings.add( pid )
        del self.process_numbered_as[ pid ]

    while self.unreporteds:
      try:
        self.inqueue.put( self.unreporteds[0], timeout = self.grace )

      except Full:
        break

      self.unreporteds.popleft()

    while True:
      try:
        ( event, data ) = self.inqueue.get( timeout = self.waittime )

      except Empty:
        break

      event( register = self.register, data = data )

    while self.register.terminateds:
      pid = self.register.terminateds.popleft()

      try:
        self.terminatings.remove( pid )

      except KeyError:
        self.register.terminateds.appendleft( pid )
        break

      self.recycleds.append( pid )

    while self.register.requested_shutdowns:
      pid = self.register.requested_shutdowns.popleft()

      try:
        self.terminatings.remove( pid )

      except KeyError:
        self.register.requested_shutdowns.appendleft( pid )
        break

      self.recycleds.append( pid )
      assert 0 < self.outstanding_shutdown_requests
      self.outstanding_shutdown_requests -= 1

    while self.register.results:
      ( jobid, value ) = self.register.results.popleft()
      self.completed_results.append(
        Result( identifier = self.identifier_for[ jobid ], value = value ),
        )
      del self.identifier_for[ jobid ]

    self.manage()


  def join(self):

    self.shutdown()

    while self.process_numbered_as:
      self.poll()
      time.sleep( self.waittime )

    self.poll()


  def shutdown(self):

    self.loadmanager = ConstantStrategy( capacity = 0 )
    self.manage()


  def terminate(self):

    self.shutdown()
    time.sleep( self.waittime )
    self.poll()

    for ( pid,  process ) in self.process_numbered_as.items():
      if process.is_alive():
        if hasattr( process, "terminate" ): # Thread has no terminate
          try:
            process.terminate()
            process.join()

          except Exception:
            pass

        self.terminatings.add( pid )
        del self.process_numbered_as[ pid ]

    self.poll()


  # Internal methods
  def start_process(self):

    try:
      pid = self.recycleds.popleft()

    except IndexError:
      pid = next(self.pid_assigner)

    process = self.job_factory(
      target = pool_process_cycle,
      kwargs = {
        "pid": pid,
        "inqueue": self.outqueue,
        "outqueue": self.inqueue,
        "waittime": self.waittime,
        "lifeman": self.lifemanager,
        "sentinel": self.SENTINEL,
        },
      )

    process.start()
    self.process_numbered_as[ pid ] = process


  def stop_process(self):

    self.outqueue.put( self.SENTINEL )
    self.outstanding_shutdown_requests += 1


  def manage(self):

    count = self.process_count
    target = self.loadmanager.target(
      job_count = self.job_count,
      process_count = count,
      )

    if count < target:
      while self.process_count < target:
        self.start_process()

    elif target < count:
      while target < self.process_count:
        self.stop_process()


class MainthreadPool(object):
  """
  Single process pool that executes on the main thread

  This is used for all single-CPU jobs, as it is more efficient than spawning a
  single job
  """

  def __init__(self):

    self.waiting_jobs = deque()
    self.completed_results = deque()


  @property
  def job_count(self):

    return len( self.waiting_jobs )


  @property
  def completed_jobs(self):

    return [ result.identifier for result in self.completed_results ]


  @property
  def known_jobs(self):

    return list( self.waiting_jobs ) + self.completed_jobs


  @property
  def process_count(self):

    return 1


  @property
  def results(self):

    while self.known_jobs:
      self.wait()
      yield self.completed_results.popleft()


  def submit(self, target, args = (), kwargs = {}):

    identifier = Identifier( target = target, args = args, kwargs = kwargs )
    self.waiting_jobs.append( identifier )
    return identifier


  def is_empty(self):

    return not self.waiting_jobs


  def is_full(self):

    return 1 <= self.job_count


  def wait(self):

    while not self.completed_results and not self.is_empty():
      self.poll()


  def poll(self):

    if self.waiting_jobs:
      current = self.waiting_jobs.popleft()

      try:
        value = current.target( *current.args, **current.kwargs )

      except Exception as e:
        res = result.error( exception = e )

      else:
        res = result.success( value = value )

      self.completed_results.append( Result( identifier = current, value = res ) )


  def join(self):

    while self.waiting_jobs:
      self.poll()


  def shutdown(self):

    pass


  def terminate(self):

    pass


# Parallel execution iterator
class RechargeableIterator(object):
  """
  Allows new elements to be filled in
  """

  def __init__(self):

    self.intermittent = deque()


  def next(self):

    try:
      return self.intermittent.popleft()

    except IndexError:
      raise StopIteration


  def has_next(self):

    return bool( self.intermittent )


  def append(self, value):

    self.intermittent.append( value )


  def extend(self, iterable):

    self.intermittent.extend( iterable )


class NoPooler(object):
  """
  Does not group jobs
  """

  @classmethod
  def pack(cls, calcsiter, manager):

    ( target, args, kwargs ) = next(calcsiter) # raise StopIteration
    identifier = manager.submit(
      target = target,
      args = args,
      kwargs = kwargs,
      )
    return [ identifier ]


  @classmethod
  def unpack(self, result, resiter):

    resiter.append( result )


class PooledRun(object):
  """
  Runs multiple jobs
  """

  def __init__(self, identifiers):

    self.identifiers = identifiers


  def __call__(self):

    results = []

    for iden in self.identifiers:
      try:
        res = iden.target( *iden.args, **iden.kwargs )

      except Exception as e:
        results.append( ( True, e ) )

      else:
        results.append( ( False, res ) )

    return results


  def results(self, raw):

    results = []

    try:
      res = raw()

    except Exception as e:
      results.extend(
        [
          Result(
            identifier = i,
            value = result.error( exception = e ),
            )
          for i in self.identifiers
          ]
        )

    else:
      assert len( res ) == len( self.identifiers )

      for ( identifier, ( failure, data ) ) in zip( self.identifiers, res ):
        if failure:
          results.append(
            Result( identifier = identifier, value = result.error( exception = data ) )
            )

        else:
          results.append(
            Result( identifier = identifier, value = result.success( value = data ) )
            )

    return results


class Pooler(object):
  """
  Pools up a number of jobs and runs this way

  pool - number of jobs to pool
  """

  def __init__(self, size):

    assert 0 < size
    self.size = size


  def pack(self, calcsiter, manager):

    identifiers = [
      Identifier( target = target, args = args, kwargs = kwargs )
      for ( index, ( target, args, kwargs ) )
      in zip( range(self.size), calcsiter )
      ]

    if not identifiers:
      raise StopIteration

    manager.submit(
      target = PooledRun( identifiers = identifiers ),
      args = (),
      kwargs = {},
      )

    return identifiers


  def unpack(self, result, resiter):

    resiter.extend( result.identifier.target.results( raw = result ) )


def get_pooler(size):

  if size == 1:
    return NoPooler

  elif 1 < size:
    return Pooler( size = size )

  else:
    raise ValueError("Invalid pool size: %s" % size)



class ParallelForIterator(object):
  """
  Creates an iterator that executes calls on a Manager-like object

  calculations - an iterable of calculations yielding ( target, args, kwargs ) tuples
  manager - execution manager
  """

  def __init__(self, calculations, manager, pool = 1):

    self.manager = manager
    self.pooler = get_pooler( size = pool )
    self.resiter = RechargeableIterator()
    self.calcsiter = iter( calculations )
    self.resume()


  def _ongoing(self, orderer):

    while not self.manager.is_full():
      try:
        identifiers = self.pooler.pack(
          calcsiter = self.calcsiter,
          manager = self.manager,
          )

      except StopIteration:
        self.suspend()
        break

      for identifier in identifiers:
        orderer.job_submitted( identifier = identifier )


  def _terminating(self, orderer):

    pass


  def next(self, orderer):

    if not self.resiter.has_next():
      result = next(self.manager.results) # raise StopIteration
      self.pooler.unpack( result = result, resiter = self.resiter )

    self.process( orderer = orderer )
    assert self.resiter.has_next()
    r = next(self.resiter)
    return ( r.identifier, r )


  def suspend(self):

    self.process = self._terminating


  def resume(self):

    self.process = self._ongoing


class Ordering(object):
  """
  Base class for ordering classes
  """

  def __init__(self, parallel_for):

    self.parallel_for = parallel_for
    self.parallel_for.process( orderer = self )


  def __del__(self):

    self.parallel_for.suspend()

    # Fetch all processing values
    for elem in self:
      pass

    self.parallel_for.resume()


  def __iter__(self):

    return self


class FinishingOrder(Ordering):
  """
  Results returned as jobs finish
  """

  def __init__(self, parallel_for):

    super( FinishingOrder, self ).__init__( parallel_for = parallel_for )


  def job_submitted(self, identifier):

    pass


  def next(self):

    ( identifier, result ) = self.parallel_for.next( orderer = self )
    return ( ( identifier.target, identifier.args, identifier.kwargs ), result )


class SubmissionOrder(Ordering):
  """
  Results returned as they are submitted
  """

  def __init__(self, parallel_for):

    self.submitteds = deque()
    self.result_for = {}
    super( SubmissionOrder, self ).__init__( parallel_for = parallel_for )


  def job_submitted(self, identifier):

    self.submitteds.append( identifier )


  def next(self):

    while not self.submitteds or self.submitteds[0] not in self.result_for:
      ( identifier, result ) = self.parallel_for.next( orderer = self )
      self.result_for[ identifier ] = result

    first = self.submitteds.popleft()
    result = self.result_for[ first ]
    del self.result_for[ first ]
    return ( ( first.target, first.args, first.kwargs ), result )


 *******************************************************************************


 *******************************************************************************
libtbx/queuing_system_utils/scheduling_helpers.py
from __future__ import absolute_import, division, print_function

import threading
import multiprocessing

class Thread(threading.Thread):
  """
  A Thread-specialization that adds the exitcode attribute

  http://stackoverflow.com/questions/986616/python-thread-exit-code
  """

  def run(self):

    try:
      super( Thread, self ).run()

    except Exception as e:
      #import traceback
      #print traceback.format_exc()
      self.exitcode = 1
      self.err = e

    else:
      self.exitcode = 0
      self.err = None


class Process(multiprocessing.Process):
  """
  A Process-specialization that adds the .err attribute
  """

  def __init__(
    self,
    group = None,
    target = None,
    name = None,
    display_stderr = False,
    args = (),
    kwargs = {},
    ):

    super( Process, self ).__init__(group, target, name, args, kwargs)
    self.display_stderr = display_stderr

    import tempfile
    self.stderr = tempfile.TemporaryFile()


  def run(self):

    import sys
    stderr_fileno = sys.stderr.fileno()
    sys.stderr = self.stderr # adjust Python level
    import os
    os.dup2( self.stderr.fileno(), stderr_fileno )

    super( Process, self ).run()


  def join(self, timeout = None):

    super( Process, self ).join( timeout )

    if not self.stderr:
      return

    self.stderr.seek( 0 )
    stderr = self.stderr.read()
    self.stderr.close()
    self.stderr = None

    if self.display_stderr:
      import sys
      sys.stderr.write( stderr )
      sys.stderr.write( "\n" )

    if self.exitcode != 0:
      self.err = RuntimeError( stderr )

    else:
      self.err = None


class ThreadFactory(object):
  """
  Creator for threading.Thread or scheduling_helpers.Thread
  """

  def __init__(self, name = None, preserve_exception_message = False, **kwargs):

    if preserve_exception_message:
      self.factory = Thread

    else:
      self.factory = threading.Thread

    self.name = name


  def __call__(self, target, args = (), kwargs = {}):

    return self.factory(
      name = self.name,
      target = target,
      args = args,
      kwargs = kwargs,
      )


class ProcessFactory(object):
  """
  Creator for multiprocessing.Process or scheduling_helpers.Process
  """

  def __init__(self, name = None, preserve_exception_message = False, **kwargs):

    if preserve_exception_message:
      self.factory = Process

    else:
      self.factory = multiprocessing.Process

    self.name = name


  def __call__(self, target, args = (), kwargs = {}):

    return self.factory(
      name = self.name,
      target = target,
      args = args,
      kwargs = kwargs,
      )


class QQFactory(object):
  """
  Creator pattern for Queue.Queue, also include destruction

  Note this is a singleton object
  """

  @staticmethod
  def create():

    from six.moves import queue
    return queue.Queue()


  @staticmethod
  def destroy(queue):

    pass


class MPQFactory(object):
  """
  Creator pattern for multiprocessing.Queue, also include destruction

  Note this is a singleton object
  """

  @staticmethod
  def create():

    return multiprocessing.Queue()


  @staticmethod
  def destroy(queue):

    pass


class MPManagerQFactory(object):
  """
  Creator pattern for multiprocessing.Manager.Queue, also include destruction

  Note this is a not singleton object
  """

  def __init__(self):

    self.manager = multiprocessing.Manager()


  def create(self):

    return self.manager.Queue()


  def destroy(self, queue):

    pass


 *******************************************************************************


 *******************************************************************************
libtbx/queuing_system_utils/sge_utils.py
"Sun Grid Engine utilities"
from __future__ import absolute_import, division, print_function

import sys
import os
from six.moves import range

def int_or_none(v):
  if (v is None or v == "undefined"): return None
  return int(v)

def job_id():
  return int_or_none(os.environ.get("JOB_ID"))

def arch():
  return os.environ.get("SGE_ARCH")

class task_info(object):

  def __init__(self):
    self.first = int_or_none(os.environ.get("SGE_TASK_FIRST"))
    self.last = int_or_none(os.environ.get("SGE_TASK_LAST"))
    self.id = int_or_none(os.environ.get("SGE_TASK_ID"))
    assert [self.first, self.last, self.id].count(None) in [0, 3]

  def show(self, out=None, prefix="", even_if_none=False):
    if (out is None): out = sys.stdout
    if (self.first is not None or even_if_none):
      print(prefix+"SGE_TASK_FIRST =", self.first, file=out)
    if (self.last is not None or even_if_none):
      print(prefix+"SGE_TASK_LAST =", self.last, file=out)
    if (self.id is not None or even_if_none):
      print(prefix+"SGE_TASK_ID =", self.id, file=out)
    return self

  def have_array(self):
    return self.first is not None

  def as_n_i_pair(self):
    if (not self.have_array()): return 1, 0
    assert self.first == 1
    return self.last, self.id-1

  def skip_loop_iteration(self):
    return skip_loop_iteration(*self.as_n_i_pair())

def skip_loop_iteration(n, i):
  j = 0
  while True:
    yield (j % n != i)
    j += 1

class info(task_info):

  def __init__(self):
    task_info.__init__(self)
    self.job_id = job_id()
    self.arch = arch()

  def show(self, out=None, prefix="", even_if_none=False):
    if (out is None): out = sys.stdout
    if (self.job_id is not None or even_if_none):
      print(prefix+"JOB_ID =", self.job_id, file=out)
    if (self.arch is not None or even_if_none):
      print(prefix+"SGE_ARCH =", self.arch, file=out)
    task_info.show(self, out=out, prefix=prefix, even_if_none=even_if_none)

class qstat_items(object):

  def __init__(self, job_id, prior, name, user, state,
                     submit, queue, slots, ja_task_id, qtype="sge"):
    self.job_id = job_id
    self.prior = prior
    self.name = name
    self.user = user
    self.state = state
    self.submit = submit
    self.queue = queue
    self.slots = slots
    self.ja_task_id = ja_task_id
    self.qtype = qtype

  def counts(self):
    ja_task_id = self.ja_task_id
    if (len(ja_task_id) == 0): return 1
    m = ja_task_id.find("-")
    c = ja_task_id.find(":")
    a = ja_task_id.find(",")
    if (a >= 0):
      assert m < 0
      assert c < 0
      flds = ja_task_id.split(",")
      for f in flds:
        assert str(int(f)) == f
      return len(flds)
    if (m < 0):
      assert c < 0
      assert str(int(ja_task_id)) == ja_task_id
      return 1
    assert c > 0
    assert c > m
    f = int(ja_task_id[:m])
    l = int(ja_task_id[m+1:c])
    s = int(ja_task_id[c+1:])
    return len(range(f,l+1,s))

  def oe_name(self, oe):
    ja_task_id = self.ja_task_id
    if (len(ja_task_id) == 0):
      return "%s.%s%s" % (self.name, oe, self.job_id)
    if (ja_task_id.find("-") >= 0): return None
    if (ja_task_id.find(":") >= 0): return None
    return "%s.%s%s.%s" % (self.name, oe, self.job_id, ja_task_id)

  def o_name(self):
    return self.oe_name(oe="o")

  def e_name(self):
    return self.oe_name(oe="e")

def qstat_parse():
  from libtbx import easy_run
  qstat_out = easy_run.fully_buffered(
    command="qstat").raise_if_errors().stdout_lines
  result = []
  if (len(qstat_out) == 0):
    return result
  qstat = iter(qstat_out)
  try: header = next(qstat)
  except StopIteration: header = ""
  expected_header = \
    "job-ID prior name user state submit/start at queue slots ja-task-ID"
  if (" ".join(header.split()) != expected_header):
    raise RuntimeError("Unexpected qstat header: %s" % header)
  line = next(qstat)
  if (len(line.strip().replace("-","")) != 0):
    raise RuntimeError("Unexpected qstat header seperator line: %s" % line)
  i_job_id = 0
  i_prior = header.index(" prior ") + 1
  i_name = header.index(" name ") + 1
  i_user = header.index(" user ") + 1
  i_state = header.index(" state ") + 1
  i_submit = header.index(" submit/start at ") + 1
  i_queue = header.index(" queue ") + 1
  i_slots = header.index(" slots ") + 1
  i_ja_task_id = header.index(" ja-task-ID") + 1
  for line in qstat:
    result.append(qstat_items(
      job_id=line[i_job_id:i_prior].strip(),
      prior=line[i_prior:i_name].strip(),
      name=line[i_name:i_user].strip(),
      user=line[i_user:i_state].strip(),
      state=line[i_state:i_submit].strip(),
      submit=line[i_submit:i_queue].strip(),
      queue=line[i_queue:i_slots].strip(),
      slots=line[i_slots:i_ja_task_id].strip(),
      ja_task_id=line[i_ja_task_id:].strip()))
  return result

def qsub(file_name, qsub_args, use_default_output_names=False):
  from libtbx import easy_run
  if not use_default_output_names :
    out_file = os.path.splitext(file_name)[0] + "_out.txt"
    err_file = os.path.splitext(file_name)[0] + "_err.txt"
    cmd = "qsub -b y -o \"%s\" -e \"%s\" %s %s" % (out_file, err_file,
      qsub_args, file_name)
  else :
    cmd = "qsub -b y %s %s" % (qsub_args, file_name)
  qsub_out = easy_run.fully_buffered(
    command=cmd).raise_if_errors().stdout_lines
  job_id = None
  for line in qsub_out :
    if line.startswith("Your job"):
      job_id = int(line.split()[2])
      break
  return job_id

def qdel(job_id=None, job_ids=None):
  assert (job_id is None) or (job_ids is None)
  assert (job_id is not None) or (isinstance(job_ids, list))
  from libtbx import easy_run
  if job_id is not None :
    args = str(job_id)
  else :
    args = " ".join(job_ids)
  qdel_out = easy_run.fully_buffered(
    command="qdel %s" % args).raise_if_errors().stdout_lines
  print("\n".join(qdel_out))
  for line in qdel_out :
    if "denied" in line :
      raise RuntimeError("\n".join(qdel_out))
  return True

if (__name__ == "__main__"):
  info().show(prefix="*** ", even_if_none=True)
  print("OK")


 *******************************************************************************


 *******************************************************************************
libtbx/queuing_system_utils/tst.py
from __future__ import absolute_import, division, print_function
from six.moves import range

# XXX this is intended to be a simple template for debugging queueing system
# support issues, not a full regression test.

class target(object):
  def __init__(self, x):
    self.x = x

  def __call__(self):
    import math
    results = []
    for n in range(x):
      nn = math.sqrt(n**3)
      print(nn)
      results.append(nn)

def exercise():
  from libtbx.queuing_system_utils import generic as queuing
  t = target(1000000)
  job = queuing.qsub(
    target=t,
    platform="sge")
  job.start()
  assert (isinstance(job.jobid, int))
  while job.is_alive():
    pass
  print("done")

if (__name__ == "__main__"):
  exercise()


 *******************************************************************************


 *******************************************************************************
libtbx/resource_monitor.py
from __future__ import division

from collections import UserDict
from contextlib import ContextDecorator
from dataclasses import astuple, dataclass
from datetime import datetime, timedelta
from enum import Enum
from functools import cached_property
from glob import glob
import itertools
import logging
from pathlib import Path
import platform
import re
import os
from statistics import mean
import subprocess
import threading
import time
from typing import Iterable, Type, Union

import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
import numpy as np

from libtbx.mpi4py import MPI


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~ TYPING AND UTILITY ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


comm = MPI.COMM_WORLD
PathLike = Union[str, bytes, os.PathLike]


def _mkdir_if_missing(path: PathLike) -> None:
  path: Path = Path(path)
  if comm.rank == 0:
    if dir_name := Path(path).parent:
      dir_name.mkdir(parents=True, exist_ok=True)
  comm.barrier()


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ LOGGING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


class ResourceLogManager:
  """Create appropriate resource logger for each rank and control its format"""
  date_fmt = '%Y-%m-%d %H:%M:%S,%f'  # exposed logging default, do not change
  fmt = '%(asctime)s - %(message)s'
  formatter = logging.Formatter(fmt=fmt)
  line_regex = re.compile(
    r'(\d{2,4}-\d\d-\d\d \d\d:\d\d:\d\d,\d+) - '
    r'ResourceStats\(cpu_usage=(-?\d+\.?\d*), ?cpu_memory=(-?\d+\.?\d*), ?'
    r'gpu_usage=(-?\d+\.?\d*), ?gpu_memory=(-?\d+\.?\d*)')

  def __init__(self, logger_name: str) -> None:
    self.log = logging.getLogger(logger_name)

  def get_file_logger(self, file_path: PathLike) -> logging.Logger:
    self.log.setLevel(logging.DEBUG)
    file_handler = logging.FileHandler(filename=file_path)
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(self.formatter)
    self.log.addHandler(file_handler)
    return self.log

  def get_null_logger(self) -> logging.Logger:
    self.log.setLevel(logging.CRITICAL + 1)
    self.log.addHandler(logging.NullHandler())
    return self.log


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ STORING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


class PerCentFloat(float):
  """Convenience wrapper that clamps float between 0 and 100"""
  def __new__(cls, value) -> 'PerCentFloat':
    return float.__new__(float, max(min(float(value), 100.0), 0.0))


@dataclass
class ResourceStats:
  """Convenient dataclass used to pass info about CPU/GPU usage/memory"""
  cpu_usage: PerCentFloat = 0.0
  cpu_memory: PerCentFloat = 0.0
  gpu_usage: PerCentFloat = 0.0
  gpu_memory: PerCentFloat = 0.0

  def __post_init__(self):
    for attr in ['cpu_usage', 'cpu_memory', 'gpu_usage', 'gpu_memory']:
      setattr(self, attr, PerCentFloat(getattr(self, attr)))

  @property
  def vector(self) -> np.ndarray:
    return np.array(list(astuple(self)), dtype=float)

  def __add__(self, other: 'ResourceStats') -> 'ResourceStats':
    return self.__class__(*(self.vector + other.vector))  # noqa

  def __mul__(self, other: float) -> 'ResourceStats':
    return self.__class__(*(self.vector * other))  # noqa

  def __truediv__(self, other: float) -> 'ResourceStats':
    return self.__class__(*(self.vector / other))  # noqa


try:
  _UD = UserDict[datetime, ResourceStats]
except TypeError:
  _UD = UserDict # Needed for PyVer < 3.9
class ResourceStatsHistory(_UD):
  """Store and easily handle a dictionary of datetime-ResourceStats pairs"""

  @classmethod
  def from_file(cls, path: PathLike) -> 'ResourceStatsHistory':
    new = cls()
    with open(path, 'r') as file:
      for line in file.readlines():
        if match := ResourceLogManager.line_regex.match(line):
          t = datetime.strptime(match.group(1), ResourceLogManager.date_fmt)
          new[t] = ResourceStats(*match.group(2, 3, 4, 5))
    return new

  def get_deltas_array(self) -> np.ndarray:  # result in minutes
    times = np.array(list(self.keys()), dtype=np.datetime64)
    return (times - times[0]) / np.timedelta64(1, 'm') \
        if times.size else np.empty((0,), dtype=np.float64)

  def get_stats_array(self, key: str) -> np.ndarray:
    return np.array([getattr(u, key) for u in self.values()])


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PROBING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


class ResourceProbeException(OSError):
  """Raised if a `ResourceProbe` fails during `get_resource_stats()`"""


class NoSuitableResourceProbeException(OSError):
  """Raised if all of available `ResourceProbe`s fail `get_resource_stats()`"""


class BaseResourceProbeType(type):
  """Metaclass that implements automatic probe `REGISTRY` and `discover`y"""
  REGISTRY: dict

  def __new__(mcs, *args, **kwargs) -> Union[Type['BaseCPUResourceProbe'],
                                             Type['BaseGPUResourceProbe']]:
    new_cls = type.__new__(mcs, *args, **kwargs)
    if kind := getattr(new_cls, 'kind', ''):
      mcs.REGISTRY[kind] = new_cls
    return new_cls

  @classmethod
  def discover(mcs) -> Union[Type['BaseCPUResourceProbe'],
                             Type['BaseGPUResourceProbe']]:
    for _, probe_class in mcs.REGISTRY.items():
      try:
        probe_instance = probe_class()
        _ = probe_instance.get_resource_stats()
      except ResourceProbeException:
        continue
      else:
        return probe_class
    raise NoSuitableResourceProbeException


class DummyResourceProbe:
  kind = 'dummy'

  def get_name(self) -> str:  # noqa
    return 'unknown'

  def get_resource_stats(self) -> ResourceStats:  # noqa
    return ResourceStats(-1., -1., -1., -1.)


class CPUResourceProbeType(BaseResourceProbeType):
  """Metaclass for CPU resource probes"""
  REGISTRY = {}


class BaseCPUResourceProbe(metaclass=CPUResourceProbeType):
  """
  Base class for CPU resource probes to be inherited by all CPU probes.
  Every subclass should define `kind`, `get_name()`, `get_resource_stats()`.
  """
  kind = None

  def get_name(self) -> str:  # noqa
    return platform.node()


class PsutilCPUResourceProbe(BaseCPUResourceProbe):
  """CPU resource probe that attempts collecting CPU usage via psutil"""
  kind = 'psutil'

  def __init__(self) -> None:
    try:
      import psutil
    except ImportError as e:
      raise ResourceProbeException from e
    self.process = psutil.Process()

  def get_resource_stats(self) -> ResourceStats:
    cpu_usage = PerCentFloat(self.process.cpu_percent(interval=None))
    cpu_memory = PerCentFloat(self.process.memory_percent())
    return ResourceStats(cpu_usage=cpu_usage, cpu_memory=cpu_memory)


class DummyCPUResourceProbe(BaseCPUResourceProbe, DummyResourceProbe):
  """CPU resource probe that reports dummy data when no good probe is found"""
  kind = 'dummy'


class GPUResourceProbeType(BaseResourceProbeType):
  """Metaclass for GPU resource probes"""
  REGISTRY = {}


class BaseGPUResourceProbe(metaclass=GPUResourceProbeType):
  """
  Base class for GPU resource probes to be inherited by all GPU probes
  Every subclass should define `kind`, `get_name()`, `get_resource_stats()`.
  """
  kind = None


class NvidiaGPUResourceProbe(BaseGPUResourceProbe):
  """GPU resource probe that attempts collecting GPU resource via nvidia-smi"""
  kind = 'Nvidia'

  def get_name(self) -> str:  # noqa
    args = ['nvidia-smi', '--list-gpus']
    out = subprocess.run(args, stdout=subprocess.PIPE).stdout.decode('utf-8')
    return out.strip()

  def get_resource_stats(self) -> ResourceStats:  # noqa
    args = ['nvidia-smi',
            '--query-gpu=utilization.gpu,memory.used,memory.total',
            '--format=csv,noheader,nounits']
    try:
      out = subprocess.run(args, stdout=subprocess.PIPE).stdout.decode('utf-8')
      values = [float(v) for v in out.replace(',', ' ').strip().split()]
      gpu_usage = PerCentFloat(mean(values[::3]))
      gpu_memory = PerCentFloat(100. * mean(values[1::3]) / mean(values[2::3]))
      return ResourceStats(gpu_usage=gpu_usage, gpu_memory=gpu_memory)
    except (FileNotFoundError, ValueError) as e:
      raise ResourceProbeException from e


class DummyGPUResourceProbe(BaseGPUResourceProbe, DummyResourceProbe):
  """CPU resource probe that reports dummy data when no good probe is found"""
  kind = 'dummy'


class RankInfo:
  """Manage info about this and neighbor ranks using auto-discovered probes"""
  def __init__(self) -> None:
    self.rank = comm.rank
    self.cpu_probe = CPUResourceProbeType.discover()()
    self.gpu_probe = GPUResourceProbeType.discover()()
    self.node = self.cpu_probe.get_name()
    self.gpu = self.gpu_probe.get_name()
    neighborhood = comm.allgather((self.rank, self.node, self.gpu))
    self.same_node_ranks = [r for r, n, g in neighborhood if n == self.node]
    self.same_gpu_ranks = [r for r, n, g in neighborhood if g == self.gpu]
    self.is_gpu_ambassador = self.rank == min(self.same_gpu_ranks)
    self.is_node_ambassador = self.rank == min(self.same_node_ranks)

  def get_rank_resource_stats(self) -> ResourceStats:
    cpu_resource_stats = self.cpu_probe.get_resource_stats()
    gpu_stats = self.gpu_probe.get_resource_stats()
    return cpu_resource_stats + gpu_stats


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ MONITORING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


class ResourceMonitor(ContextDecorator):
  """
  Collect and log information about CPU & GPU resources in decorated processes.
  In each case the information is limited to single rank to avoid mpi comm.
  This class can be used in several ways listed below:

  - As a decorator  to monitor every call of a given function, decorate the
    function definition itself. The monitor will start every time the function
    is called and stop every time the function is terminated:
    ```
    @ResourceMonitor(*args)
    def function_to_be_monitored()
      stuff_to_be_timed()
    ```

  - As a context manager  to monitor a part of the code, place it withing
    a context manager using a `with` statement. The monitor will start at the
    start of the `with` block and terminate at the end of the `with` block:
    ```
    with ResourceMonitor(*args):
      stuff_to_be_timed()
    ```

  - As a standalone instance  for more advanced or customizable application,
    the context manager can be manually instantiated, started, stopped, etc.
    The following would be roughly equivalent to context manager approach:
    ```
    rm = ResourceMonitor(*args):
    try:
      um.start()
      stuff_to_be_timed()
    finally:
      self.stop()
    ```
  """

  class Detail(Enum):
    node = 'node'
    rank = 'rank'
    rank0 = 'rank0'
    none = 'none'

  def __init__(self,
               detail: str = 'rank',
               period: float = 5.0,
               plot: bool = True,
               prefix: str = 'monitor',
               write: bool = True,
               ) -> None:
    self.active: bool = False
    self.daemon: threading.Thread = None
    self.detail: 'ResourceMonitor.Detail' = self.Detail(detail)
    self.period: float = period  # <5 sec. de-prioritizes sub-procs & they stop
    self.plot: bool = plot
    self.prefix: PathLike = prefix if prefix else 'monitor'
    self.write: bool = write
    self.rank_info: RankInfo = RankInfo()
    _mkdir_if_missing(prefix)
    logger_name = 'libtbx.resource_monitor.' + Path(self.prefix).name
    self.log_manager = ResourceLogManager(logger_name)
    self.log: logging.Logger = self.get_logger()
    self.log.info(f'Collecting CPU stats with {self.rank_info.cpu_probe.kind=}')
    self.log.info(f'Collecting GPU stats with {self.rank_info.gpu_probe.kind=}')
    self.resource_stats_history = ResourceStatsHistory()

  def __enter__(self) -> None:
    self.start()

  def __exit__(self, exc_type, exc_val, exc_tb) -> None:
    self.stop()

  @property
  def resource_stats(self) -> ResourceStats:
    return {
      self.Detail.node: self.rank_info.get_rank_resource_stats,
      self.Detail.rank: self.rank_info.get_rank_resource_stats,
      self.Detail.rank0: self.rank_info.get_rank_resource_stats,
      self.Detail.none: (lambda _: None)
    }[self.detail]()

  @cached_property
  def is_logging(self) -> bool:
    return {
      self.Detail.node: self.rank_info.is_node_ambassador,
      self.Detail.rank: True,
      self.Detail.rank0: self.rank_info.rank == 0,
      self.Detail.none: False
    }[self.detail]

  @cached_property
  def log_path(self) -> str:
    return {
      self.Detail.node: f'{self.prefix}_{self.rank_info.node}.log',
      self.Detail.rank: f'{self.prefix}_{self.rank_info.rank}.log',
      self.Detail.rank0: f'{self.prefix}.log',
      self.Detail.none: None
    }[self.detail]

  def get_logger(self) -> logging.Logger:
    return self.log_manager.get_file_logger(self.log_path) \
      if self.is_logging and self.write else self.log_manager.get_null_logger()

  def log_current_resource_stats(self) -> None:
    """Get current usage stats and log + save them"""
    resource_stats = self.resource_stats
    if self.is_logging:
      self.resource_stats_history[datetime.now()] = resource_stats
    self.log.info(msg=f'{resource_stats}')

  def log_resource_stats_every_period(self) -> None:
    """Call in thread only, can be stopped only upon `self.active` = False"""
    start = datetime.now()
    log_iter_counter = itertools.count(start=0, step=1)
    for log_iter in log_iter_counter:
      self.snapshot()
      next_iter_time = start + timedelta(seconds=self.period) * (log_iter + 1)
      time.sleep(max((next_iter_time - datetime.now()).total_seconds(), 0))
      if not self.active:
        break

  def start(self) -> None:
    """Call to start probing resources"""
    self.active = True
    self.daemon = threading.Thread(target=self.log_resource_stats_every_period,
                                   args=(), daemon=True)
    self.daemon.start()

  def snapshot(self) -> None:
    """Call to probe resources in current single moment only"""
    threading.Thread(target=self.log_current_resource_stats, args=()).start()

  def stop(self) -> None:
    """Call to stop probing resources"""
    self.active = False
    if self.plot:
      self.plot_resource_stats_history()

  def plot_resource_stats_history(self) -> None:
    resource_stats_histories = comm.gather(self.resource_stats_history, root=0)
    if self.rank_info.rank == 0:
      resource_stats_histories = [h for h in resource_stats_histories if h]
      rsa = ResourceStatsArtist()
      rsa.plot(resource_stats_histories, f'{self.prefix}.png')


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ PLOTTING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


class ResourceStatsArtist:
  def __init__(self) -> None:
    self.colormap = plt.get_cmap('tab10')
    self.colormap_period = 10
    self.fig = plt.figure(tight_layout=True, figsize=(12, 10))
    gs = GridSpec(4, 1, figure=self.fig, hspace=0, wspace=0)
    self.ax_cu = self.fig.add_subplot(gs[0, 0])  # cu = (c)pu (u)sage
    self.ax_cm = self.fig.add_subplot(gs[1, 0], sharex=self.ax_cu)
    self.ax_gu = self.fig.add_subplot(gs[2, 0], sharex=self.ax_cu)
    self.ax_gm = self.fig.add_subplot(gs[3, 0], sharex=self.ax_cu)

  def plot(self,
           resource_stats_histories: Iterable[ResourceStatsHistory],
           save_path: PathLike = None,
           ) -> None:
    axes = self.ax_cu, self.ax_cm, self.ax_gu, self.ax_gm
    stats = ['cpu_usage', 'cpu_memory', 'gpu_usage', 'gpu_memory']
    labels = ['CPU usage', 'CPU memory', 'GPU usage', 'GPU memory']
    for i, rsh in enumerate(resource_stats_histories):
      minutes = rsh.get_deltas_array()
      color = self.colormap(i % self.colormap_period)
      for ax, stat in zip(axes, stats):
        ax.plot(minutes, rsh.get_stats_array(stat), color=color)
    self.ax_gm.set_xlabel('Time since first probe [min]', fontsize=12)
    for ax, label in zip(axes, labels):
      ax.grid(True)
      ax.tick_params(axis='both', which='major', labelsize=12)
      ax.set_ylabel(label + ' [%]', fontsize=12)
    if save_path:
      self.fig.savefig(f'{save_path}')
    else:
      plt.show()


def plot_logs(log_glob: PathLike, save_path: PathLike) -> None:
  """Manually plot if ResourceMonitor terminates unexpectedly i.e.due to OOM"""
  histories = [ResourceStatsHistory.from_file(Path(p)) for p in glob(log_glob)]
  rsa = ResourceStatsArtist()
  rsa.plot(resource_stats_histories=histories, save_path=save_path)


 *******************************************************************************


 *******************************************************************************
libtbx/run_tests.py
from __future__ import absolute_import, division, print_function
from libtbx import test_utils
import sys
import libtbx.load_env

tst_list_base = [
  "$D/metric_prefixes.py",
  "$D/tst_utils.py",
  "$D/tst_word_index_generator.py",
  "$D/test_utils/__init__.py",
  "$D/queuing_system_utils/pbs_utils.py",
  "$D/queuing_system_utils/sge_utils.py",
  "$D/introspection.py",
  "$D/tst_thread_utils.py",
  "$D/tst_easy_mp.py",
  "$D/tst_easy_mp_state.py",
  "$D/tst_easy_pickle.py",
  "$D/tst_fully_buffered_timeout.py",
  "$D/tst_scheduling.py",
  "$D/easy_run.py",
  "$D/tst_containers.py",
  "$D/tst_path.py",
  "$D/tst_math_utils.py",
  "$D/assert_utils.py",
  "$D/tst_str_utils.py",
  "$D/table_utils.py",
  "$D/tst_dlite.py",
  "$D/phil/tst_tokenizer.py",
  "$D/phil/tst.py",
  "$D/phil/tst_experimental.py",
  "$D/phil/tst_interface.py",
  "$D/tst_object_oriented_patterns.py",
  "$D/find_reference_cycles.py",
  "$D/tst_binary_search.py",
  "$D/tst_topological_sort.py",
  "$D/clusterTests.py",
  "$D/tst_citations.py",
  "$D/tst_python_code_parsing.py",
  "$D/tst_representation.py",
  "$D/tst_find_unused_imports.py",
  "$D/tst_program_template.py",
  "$D/tst_version.py",
  '$D/tst_easy_mp_multicore.py',
  ]

# generally failing tests
tst_list_fail = [
  "$D/tst_xmlrpc_utils.py",
  ]
# failing tests on Windows, Python 2.7
tst_list_windows_fail = [
  "$D/tst_runtime_utils.py",
  ]
if sys.platform == 'win32':
  tst_list_fail += tst_list_windows_fail
else:
  tst_list_base += tst_list_windows_fail

# final lists
tst_list = tst_list_base
tst_list_expected_failures = tst_list_fail

def run():
  build_dir = None
  dist_dir = libtbx.env.dist_path("libtbx")

  test_utils.run_tests(build_dir, dist_dir, tst_list)

if (__name__ == "__main__"):
  run()


 *******************************************************************************


 *******************************************************************************
libtbx/runtime_utils.py

# detached process management (used in Phenix GUI)
# This is intended to imitate running a process using libtbx.thread_utils,
# but split across truly independent processes (potentially on different
# systems).  I use something like this to start a server:
#
#   easy_run.call("libtbx.start_process run.pkl &")

# FIXME this duplicates code in libtbx.thread_utils; it is also not especially
# well tested except to the extent it is used daily in the Phenix GUI

from __future__ import absolute_import, division, print_function
from six.moves import cStringIO as StringIO
from libtbx.utils import Sorry, Abort, multi_out, host_and_user
from libtbx import easy_pickle
from libtbx import adopt_init_args, group_args
import libtbx.load_env
import libtbx.phil
import traceback
import signal
import stat
import time
import os
import sys
from six.moves import range

process_master_phil = libtbx.phil.parse("""
run_file = None
  .type = path
prefix = None
  .type = str
output_dir = None
  .type = path
tmp_dir = None
  .type = path
debug = False
  .type = bool
timeout = 200
  .type = int
buffer_stdout = False
  .type = bool
fsync = True
  .type = bool
""")

class simple_target(object):
  def __init__(self, args, output_dir=None):
    adopt_init_args(self, locals())
    if output_dir is None :
      self.output_dir = os.getcwd()

  def __call__(self):
    return True

class target_with_save_result(object):
  def __init__(self, args, file_name, output_dir=None, log_file=None,
      job_title=None):
    from six import string_types
    assert isinstance(file_name, string_types)
    assert (isinstance(args, list) or isinstance(args, tuple))
    assert (output_dir is None) or isinstance(output_dir, string_types)
    assert (log_file is None) or isinstance(log_file, string_types)
    adopt_init_args(self, locals())
    if (output_dir is None):
      self.output_dir = os.getcwd()
    self._out = None

  def __call__(self):
    if (self.log_file is not None):
      log = open(self.log_file, "w")
      new_out = multi_out()
      new_out.register("log", log)
      new_out.register("stdout", sys.stdout)
      sys.stdout = new_out
      self._out = new_out
    result = self.run()
    easy_pickle.dump(self.file_name, result)
    if (self._out is not None) and (not getattr(self._out, "closed", False)):
      self._out.flush()
      # FIXME
      #self._out.close()
    return result

  def run(self):
    raise NotImplementedError()

class detached_process_driver(object):
  def __init__(self, target):
    adopt_init_args(self, locals())

  def __call__(self):
    result = self.target()
    return result

class detached_process_driver_mp(detached_process_driver):
  def __call__(self, args, kwds, child_conn):
    result = self.target()
    return result

class detached_base(object):
  def __init__(self, params):
    adopt_init_args(self, locals())
    self._accumulated_callbacks = []
    if params.prefix is None :
      params.prefix = ""
    if params.tmp_dir is not None :
      self.set_file_names(params.tmp_dir)
    elif params.output_dir is not None :
      self.set_file_names(params.output_dir)
    else :
      self.set_file_names(os.getcwd())

  def set_file_names(self, tmp_dir):
    prefix = os.path.join(tmp_dir, self.params.prefix)
    self.start_file = prefix + ".libtbx_start"
    self.stdout_file = prefix + ".libtbx_stdout"
    self.error_file =  prefix + ".libtbx_error"
    self.stop_file = prefix + ".libtbx_STOP"
    self.abort_file =  prefix + ".libtbx_abort"
    self.result_file = prefix + ".libtbx_result"
    self.info_file = prefix + ".libtbx_info"
    self.state_file =  prefix + ".libtbx_state"
    self.info_lock = self.info_file + ".LOCK"
    self.state_lock = self.state_file + ".LOCK"
    self.prefix = prefix

  def isAlive(self):
    return False

  def callback_start(self, data):
    pass

  def callback_stdout(self, data):
    pass

  def callback_error(self, error, traceback_info):
    pass

  def callback_abort(self):
    pass

  def callback_final(self, result):
    pass

  def callback_other(self, status):
    pass

class stdout_redirect(object):
  def __init__(self, handler):
    adopt_init_args(self, locals())

  def write(self, data):
    self.handler.callback_stdout(data)

  def flush(self):
    pass

  def close(self):
    pass

class detached_process_server(detached_base):
  def __init__(self, target, *args, **kwds):
    detached_base.__init__(self, *args, **kwds)
    self.target = target
    assert hasattr(self.target, "__call__")

  # XXX support for libtbx.queueing_system_utils.generic.Job
  def __call__(self, *args, **kwds):
    return self.run()

  def run(self):
    self.callback_start()
    self._stdout = multi_out()
    self._tmp_stdout = open(self.stdout_file, "w")
    self._stdout.register("Communication log", self._tmp_stdout)
    old_stdout = sys.stdout
    sys.stdout = stdout_redirect(self)
    import libtbx.callbacks
    libtbx.call_back.register_handler(self.callback_wrapper)
    try :
      return_value = self.target()
    except Abort : # FIXME why is this not working properly?
      self.callback_abort()
    except Exception as e :
      print(type(e).__name__, file=sys.stderr)
      if (type(e).__name__ == "Abort"):
        self.callback_abort()
      else :
        if e.__class__.__module__ == "Boost.Python" :
          e = RuntimeError("Boost.Python.%s: %s" % (e.__class__.__name__,
            str(e)))
        elif hasattr(e, "reset_module"):
          e.reset_module()
        traceback_str = "\n".join(traceback.format_tb(sys.exc_info()[2]))
        try:
          self.callback_error(e, traceback_str)
        except Exception as ee:
          self.callback_error(str(e), traceback_str)
    else :
      #time.sleep(1)
      self.callback_final(return_value)
    sys.stdout = old_stdout

  def callback_wrapper(self, message, data, accumulate=True, cached=True):
    if cached :
      self.callback_other(data=group_args(
        message=message,
        data=data,
        accumulate=accumulate,
        cached=cached))

  def callback_start(self, data=None):
    info = host_and_user()
    assert (info.pid is not None)
    f = open(self.start_file, "w")
    host_name = info.get_host_name()
    if (host_name is None) and (sys.platform == "darwin"):
      host_name = os.uname()[1]
    f.write("%s %d" % (host_name, info.pid))
    f.close()

  def callback_stdout(self, data):
    self._stdout.write(data)
    self._stdout.flush()
    if self.params.fsync :
      os.fsync(self._tmp_stdout.fileno())
    if os.path.isfile(self.stop_file):
      raise Abort()

  def callback_error(self, error, traceback_info):
    self.cleanup()
    easy_pickle.dump(self.error_file, (error, traceback_info))

  def callback_abort(self):
    self.cleanup()
    easy_pickle.dump(self.abort_file, True)

  def callback_final(self, result):
    self.cleanup()
    easy_pickle.dump(self.result_file, result)

  def callback_pause(self) : # TODO
    pass

  def callback_resume(self) : # TODO
    pass

  def wait_for_lock_to_vanish(self, local_wait_time = 0.1,
      max_local_wait_time = 5):
    # Catch case where jobs are very short and returns overlap...wait for lock
    # XXX looks like info_lock was never actually implemented, just set up.
    t0 = time.time()
    while os.path.isfile(self.info_lock):
        if time.time() - t0 > max_local_wait_time:
          break  # just crash
        else:
          time.sleep(local_wait_time)

  def callback_other(self, data, local_wait_time = 0.1,
     max_local_wait_time = 1):
    if not data.cached :
      return
    if data.accumulate :
      self._accumulated_callbacks.append(data)
      self.wait_for_lock_to_vanish()
      try:
        touch_file(self.info_lock)
        easy_pickle.dump(self.info_file, self._accumulated_callbacks)
        os.remove(self.info_lock)
      except Exception as e:
        pass # failed, can happen if user deletes directory
    else :
      self.wait_for_lock_to_vanish()
      try:
        touch_file(self.state_lock)
        easy_pickle.dump(self.state_file, data)
        os.remove(self.state_lock)
      except Exception as e:
        pass # failed, can happen if user deletes directory

  def cleanup(self):
    self._stdout.flush()
    self._stdout.close()

# TODO pause/resume?
class detached_process_client(detached_base):
  def __init__(self, *args, **kwds):
    detached_base.__init__(self, *args, **kwds)
    self._logfile = None
    self._info_mtime = 0.0 # time.time()
    self._state_mtime = 0.0 # time.time()
    self.running = False
    self.finished = False
    self._process_host = None
    self._process_pid = None
    self.update_progress = True

  def __del__(self):
    if self._logfile is not None:
      self._logfile.close()

  def isAlive(self):
    return (not self.finished)

  def is_started(self):
    return self.running

  def run(self):
    timeout = self.params.timeout
    while True :
      self.update()
      if self.finished :
        break
      else :
        time.sleep(timeout * 0.001)
    return True

  def update(self):
    if not self.running and os.path.exists(self.start_file):
      self.running = True
      with open(self.start_file, "r") as f:
        data = f.read()
      try :
        host, pid = data.split()
        self._process_host = host
        self._process_pid = int(pid)
      except Exception as e :
        print("Error acquiring runtime info:")
        print(e)
      self.callback_start(data)
    if self.update_progress :
      self.check_stdout()
      self.check_status()
    if os.path.exists(self.error_file):
      try :
        (error, traceback_info) = easy_pickle.load(self.error_file)
      except (EOFError, TypeError):
        pass
      else :
        self.callback_error(error, traceback_info)
    elif os.path.exists(self.abort_file):
      self.callback_abort()
    elif os.path.exists(self.result_file):
      max_retries = 5
      error_text = None
      for retry in range(max_retries):
        try:
          result = easy_pickle.load(self.result_file)
        except Exception:
          pass
        else:
          time.sleep(1)
          self.check_stdout()
          self.check_status()
          self.callback_final(result)
          break
        time.sleep(retry+1)
      if retry == max_retries-1:
        print("There was an error with loading '%s'." % self.result_file)
    else :
      self.finished = False
      return
    self.finished = True

  def check_stdout(self):
    if self._logfile is None and os.path.exists(self.stdout_file):
      self._logfile = open(self.stdout_file, "r")
      stat = os.stat(self.stdout_file)
      self._logfile.seek(stat.st_size)
    if self._logfile is not None :
      last = self._logfile.tell()
      data = self._logfile.read()
      if data == '' :
        self._logfile.seek(last)
      else :
        self.callback_stdout(data)

  def reset_logfile(self):
    if self._logfile is not None :
      self._logfile.seek(0)

  def check_status(self):
    if os.path.exists(self.info_file):
      mtime = os.path.getmtime(self.info_file)
      if mtime > self._info_mtime and not os.path.isfile(self.info_lock):
        self._info_mtime = mtime
        try :
          accumulated_status = easy_pickle.load(self.info_file)
        except KeyboardInterrupt :
          raise
        except EOFError :
          pass
        except Exception as e :
          print(e)
        else :
          n_cb = len(accumulated_status)
          n_cb_old = len(self._accumulated_callbacks)
          for i in range(n_cb_old, n_cb):
            new_cb = accumulated_status[i]
            self._accumulated_callbacks.append(new_cb)
            self.callback_other(new_cb)
    if os.path.exists(self.state_file):
      mtime = os.path.getmtime(self.state_file)
      if mtime > self._state_mtime and not os.path.isfile(self.state_lock):
        self._state_mtime = mtime
        try :
          current_status = easy_pickle.load(self.state_file)
        except KeyboardInterrupt :
          raise
        except EOFError :
          pass
        except Exception as e :
          print(e)
        else :
          self.callback_other(current_status)

  # XXX in practice the phenix GUI only sets force=True for jobs running on the
  # same machine; qdel is pretty thorough anyway.
  def abort(self, force=None):
    touch_file(self.stop_file)
    if (force) and (not None in [self._process_host, self._process_pid]):
      info = host_and_user()
      if (info.get_host_name() == self._process_host):
        os.kill(self._process_pid, signal.SIGTERM)
        self.running = False
        self.callback_abort()

  # XXX See also libtbx.thread_utils implementation
  def send_signal(self, signal_number) : # XXX experimental
    """
    Signals the process using os.kill, which despite the name, can also
    pause or resume processes on Unix.
    """
    assert (self._process_pid is not None) and (sys.platform != "win32")
    try :
      os.kill(self._process_pid, signal_number)
    except OSError as e :
      print(e)
      #self.callback_abort()
      return False
    else :
      return True

  def pause(self) : # XXX experimental, Unix only
    if (self.send_signal(signal.SIGSTOP)):
      self.callback_pause()

  def resume(self) : # XXX experimental, Unix only
    if (self.send_signal(signal.SIGCONT)):
      self.callback_resume()

  def callback_pause(self) : # TODO
    pass

  def callback_resume(self) : # TODO
    pass

  def purge_files(self):
    files = ["start","stdout","error","stop","abort","result","info","state"]
    for fn in files :
      file_name = getattr(self, "%s_file" % fn)
      if os.path.exists(file_name):
        try :
          os.remove(file_name)
        except Exception as e :
          print(e)

def touch_file(file_name):
  f = open(file_name, "w").close()

def write_params(params, file_name):
  param_phil = process_master_phil.format(python_object=params)
  f = open(file_name, "w")
  param_phil.show(out=f)
  f.close()

def write_run_script(file_name, cmds):
  f = open(file_name, "w")
  os.fchmod(f.fileno(),
    stat.S_IRUSR|stat.S_IWUSR|stat.S_IXUSR|stat.S_IRGRP|stat.S_IROTH)
  f.write("#!/bin/sh\n\n")
  use_cctbx_setpaths = True
  if "PHENIX" in os.environ and not "PHENIX_CUSTOM_ENV" in os.environ :
    env_file = os.path.join(os.environ["PHENIX"], "phenix_env.sh")
    if os.path.isfile(env_file):
      f.write("source %s\n" % env_file)
      use_cctbx_setpaths = False
  if use_cctbx_setpaths :
    f.write("source %s\n" % libtbx.env.under_build("setpaths.sh"))
  f.write("%s" % " ".join(cmds))
  f.close()

# XXX command-line launcher
def run(args):
  user_phil = []
  for arg in args :
    if os.path.isfile(arg):
      file_name = os.path.abspath(arg)
      base, ext = os.path.splitext(file_name)
      if ext in [".params", ".eff", ".def", ".phil"] :
        user_phil.append(libtbx.phil.parse(file_name=file_name))
      elif ext in [".pkl", ".pickle"] :
        input_string = "run_file = %s" % arg
        user_phil.append(libtbx.phil.parse(input_string))
    else :
      try :
        arg_phil = libtbx.phil.parse(arg)
      except RuntimeError as e :
        print(e)
      else :
        user_phil.append(arg_phil)
  working_phil = process_master_phil.fetch(sources=user_phil)
  params = working_phil.extract()
  if params.run_file is None :
    working_phil.show()
    raise Sorry("Pickled target function run_file not defined.")
  target = easy_pickle.load(params.run_file)
  server = detached_process_server(target, params=params)
  server.run()

########################################################################
# testing classes (see tst_runtime_utils.py for usage)
class simple_client(detached_process_client):
  def __init__(self, *args, **kwds):
    self.n_cb = 0
    self.out = StringIO()
    detached_process_client.__init__(self, *args, **kwds)

  def callback_error(self, error, traceback_info):
    raise error

  def callback_aborted(self):
    raise Sorry("aborted as planned.")

  def callback_stdout(self, data):
    self.out.write(data)
    #for line in data.splitlines():

  def callback_other(self, data):
    self.n_cb += 1

  def callback_final(self, result):
    self.result = result

class simple_run(object):
  def __init__(self, output_dir):
    adopt_init_args(self, locals())

  def __call__(self):
    pu_total = 0
    for run in range(0, 4):
      (x, n) = (0.1 * (run+1) , 20000)
      mu = 10.0
      pu = 0.0
      pol =[0] * 100
      r = list(range(0,100))
      libtbx.call_back("run %d" % run, None, accumulate=True)
      time.sleep(1)
      for i in range(0,n):
        for j in r:
          pol[j] = mu = (mu + 2.0) / 2.0
        su = 0.0
        for j in r:
          su = x * su + pol[j]
        pu = pu + su
      pu_total += pu
      libtbx.call_back("current_total", pu, accumulate=False)
      print("current is %f" % pu)
    return pu_total

class simple_func(object):
  def __init__(self, x):
    self.x = x
  def __call__(self):
    print(self.x)


 *******************************************************************************


 *******************************************************************************
libtbx/scheduling/__init__.py
"""
Generic job scheduling
"""

from __future__ import absolute_import, division, print_function


class SchedulingError(Exception):
  """
  Package exception
  """


class SetupError(SchedulingError):
  """
  Error from the software environment
  """


class identifier(object):
  """
  Job identifier
  """

  def __init__(self):

    self.jobid = id( self )


  def __eq__(self, other):

    return self.jobid == other.jobid


  def __ne__(self, other):

    return not ( self == other )


  def __hash__(self):

    return hash( self.jobid )


  def __str__(self):

    return "identifier( jobid = %s )" % self.jobid


  def __repr__(self):

    return str( self )


class ignore(object):
  """
  Ignore extra stacktrace information

  Note this a singleton
  """

  @staticmethod
  def enter():

    pass


  @staticmethod
  def exit(exc_type, exc_val, exc_tb):

    if exc_type is None:
      from libtbx.scheduling import stacktrace
      stacktrace.cleanup()


class excepthook(object):
  """
  Use custom sys.excepthook to print error, replace when finished

  Note this is a singleton
  """

  @staticmethod
  def enter():

    pass


  @staticmethod
  def exit(exc_type, exc_val, exc_tb):

    from libtbx.scheduling import stacktrace

    if exc_type is not None:
      stacktrace.enable()

    else:
      stacktrace.cleanup()


class decorate(object):
  """
  Append stacktrace to exception message

  Note this is a singleton
  """

  @staticmethod
  def enter():

    pass


  @staticmethod
  def exit(exc_type, exc_val, exc_tb):

    from libtbx.scheduling import stacktrace

    if exc_type is not None:
      data = stacktrace.exc_info()

      if exc_val is data[0]:
        message = "%s. Embedded exception follows:\n%s" % (
          exc_val,
          "".join( data[1] ).rstrip(),
          )
        exc_val.args = ( message, ) + exc_val.args[1:]

    stacktrace.cleanup()


class holder(object):
  """
  Context manager for a manager
  """

  def __init__(self, creator, stacktrace = ignore):

    self.creator = creator
    self.manager = None
    self.stacktrace = stacktrace


  def __enter__(self):

    self.stacktrace.enter()
    self.manager = self.creator.create()
    return self.manager


  def __exit__(self, exc_type, exc_val, exc_tb):

    self.creator.destroy( manager = self.manager )
    self.stacktrace.exit( exc_type = exc_type, exc_val = exc_val, exc_tb = exc_tb )

    return False


 *******************************************************************************


 *******************************************************************************
libtbx/scheduling/cluster_handler.py
from __future__ import absolute_import, division, print_function

from libtbx.queuing_system_utils import processing
from libtbx.queuing_system_utils.processing import errors

from libtbx.scheduling import SetupError

class Job(processing.Job):
  """
  Customisation of the processing.Job class that translates errors into SetupError
  """

  def __exception_converting_call(self, name):

    try:
      return getattr( super( Job, self ), name )()

    except errors.BatchQueueError as e:
      raise SetupError("Queue error: %s" % e)


  @property
  def exitcode(self):

    try:
      return super( Job, self ).exitcode

    except errors.BatchQueueError as e:
      raise SetupError("Queue error: %s" % e)


  def start(self):

    return self.__exception_converting_call( name = "start" )


  def is_alive(self):

    return self.__exception_converting_call( name = "is_alive" )


  def join(self):

    return self.__exception_converting_call( name = "join" )


  def terminate(self):

    return self.__exception_converting_call( name = "terminate" )


class JobFactory(object):
  """
  Creator for Job objects

  Note this bypasses the Qinteface's own Job method
  """

  def __init__(self,
    platform,
    name = "libtbx_python",
    command = None,
    asynchronous = True,
    use_target_file = True,
    include = None,
    poller = None,
    handler = None,
    save_error = True,
    ):

    from libtbx.queuing_system_utils.processing import transfer

    try:
      self.qinterface = processing.INTERFACE_FOR[ platform ][0](
        name = name,
        command = command,
        asynchronous = asynchronous,
        input = transfer.TemporaryFile if use_target_file else transfer.Stdin,
        include = include,
        poller = poller,
        handler = handler,
        save_error = save_error,
        display_stderr = False,
        )

    except errors.BatchQueueError as e:
      raise SetupError("Queue error: %s" % e)


  def __call__(self, target, args = (), kwargs = {}):

    return Job(
      qinterface = self.qinterface,
      target = target,
      args = args,
      kwargs = kwargs,
      )


 *******************************************************************************


 *******************************************************************************
libtbx/scheduling/file_queue.py
from __future__ import absolute_import, division, print_function

from six.moves.queue import Empty
from six.moves import cPickle as pickle
import fcntl
import tempfile
import os
from six.moves import range


class instant(object):
  """
  Timeout immediately
  """

  def __call__(self):

    raise Empty("No data found in queue")


class timed(object):
  """
  Timeout after given time
  """

  def __init__(self, timeout, waittime):

    self.timeout = timeout
    self.waittime = waittime


  def __call__(self):

    if 0 < self.timeout:
      import time
      waittime = min( self.timeout, self.waittime )
      self.timeout -= self.waittime
      time.sleep( waittime )

    else:
      raise Empty("No data found in queue within timeout")


class eternal(object):
  """
  No timeout
  """

  def __init__(self, waittime):

    self.waittime = waittime


  def __call__(self):

    import time
    time.sleep( self.waittime )


def get_timeout_object(block, timeout, waittime):

  if not block:
    return instant()

  else:
    if timeout is not None:
      return timed( timeout = timeout, waittime = waittime )

    else:
      return eternal( waittime = waittime )


def temp_name(prefix, suffix, folder):

  ( fd, fname ) = tempfile.mkstemp( suffix = suffix, prefix = prefix, dir = folder )
  os.close( fd )
  return fname


def _lock(fobj, timeout):

  while True:
    try:
      fcntl.lockf( fobj.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB )
      break

    except IOError:
      timeout()


def _unlock(fobj):

  fcntl.lockf( fobj.fileno(), fcntl.LOCK_UN )


class Queue(object):
  """
  File-based queue. Can be used cross-host via an NFS 3.0 filesystem.
  """

  def __init__(self, prefix = "tmp", folder = ".", waittime = 0.1):

    self.waittime = waittime

    self._put_file = temp_name( prefix = prefix, suffix = ".put", folder = folder )
    self._get_file = temp_name( prefix = prefix, suffix = ".get", folder = folder )
    self._offset_file = temp_name( prefix = prefix, suffix = ".offset", folder = folder )

    # No need for locking, as queue has not been constructed yet
    with open( self._offset_file, "wb" ) as foff:
      pickle.dump( 0, foff )


  def put(self, value, block = True, timeout = None):

    tobj = get_timeout_object(
      block = block,
      timeout = timeout,
      waittime = self.waittime,
      )

    with open( self._put_file, "r+b" ) as fput:
      _lock( fput, tobj )

      try:
        fput.seek( 0, os.SEEK_END )
        pickle.dump( value, fput )
        fput.flush()

      finally:
        _unlock( fput )


  def get_next_item(self, offset, timeout):

    with open( self._get_file, "r+b" ) as fget:
      fget.seek( offset, os.SEEK_SET )

      try:
        value = pickle.load( fget )
        offset = fget.tell()

      except EOFError:
        while os.path.getsize( self._put_file ) == 0:
          timeout()

        fget.seek( 0, os.SEEK_SET )
        value = self.read_one_and_transfer_remanining_put_contents(
          fget = fget,
          timeout = timeout,
          )
        fget.truncate()
        offset = 0

    return ( value, offset )


  def read_one_and_transfer_remanining_put_contents(self, fget, timeout):

    import shutil

    with open( self._put_file, "r+b" ) as fput:
      _lock( fput, timeout )

      try:
        value = pickle.load( fput )
        shutil.copyfileobj( fput, fget )
        fput.seek( 0, os.SEEK_SET )
        fput.truncate()
        fput.flush()

      finally:
        _unlock( fput )

    return value


  def get(self, block = True, timeout = None):

    tobj = get_timeout_object(
      block = block,
      timeout = timeout,
      waittime = self.waittime,
      )

    with open( self._offset_file, "r+b" ) as foff:
      _lock( foff, tobj )

      try:
        offset = pickle.load( foff )
        ( value, offset ) = self.get_next_item( offset = offset, timeout = tobj )
        foff.seek( 0 )
        pickle.dump( offset, foff )
        foff.truncate()

      finally:
        _unlock( foff )

    return value


  def close(self):

    for fname in [ self._get_file, self._put_file, self._offset_file ]:
      if os.path.exists( fname ):
        os.remove( fname )


class MultiFileQueue(object):
  """
  File-based queue using multiple files for better throughput

  CAVEAT: does not preserve strict data order
  """

  def __init__(self, count, prefix = "tmp", folder = ".", waittime = 0.1):

    assert 0 < count

    from collections import deque
    self.queues = deque(
      [
        Queue(
          prefix = "%s_channel_%s" % ( prefix, i ),
          folder = folder,
          waittime = waittime,
          )
        for i in range( count )
        ]
      )
    self.waittime = waittime


  def put(self, value, block = True, timeout = None):

    timeout = get_timeout_object(
      block = block,
      timeout = timeout,
      waittime = self.waittime,
      )

    while True:
      for ( i, q ) in enumerate( self.queues ):
        try:
          q.put( value, block = False )

        except Empty:
          pass

        else:
          self.queues.rotate( i )
          return

      timeout()


  def get(self, block = True, timeout = None):

    timeout = get_timeout_object(
      block = block,
      timeout = timeout,
      waittime = self.waittime,
      )

    while True:
      for ( i, q ) in enumerate( self.queues ):
        try:
          value = q.get( block = False )

        except Empty:
          pass

        else:
          self.queues.rotate( i )
          return value

      timeout()


  def close(self):

    for q in self.queues:
      q.close()


class qfactory(object):
  """
  Creator pattern for file queue, also include destruction
  """

  def __init__(self, prefix = "tmp", folder = ".", waittime = 0.1):

    self.prefix = prefix
    self.folder = folder
    self.waittime = waittime


  def create(self):

    return Queue( prefix = self.prefix, folder = self.folder, waittime = self.waittime )


  @staticmethod
  def destroy(queue):

    queue.close()


class mqfactory(object):
  """
  Creator pattern for multi-file queue, also include destruction
  """

  def __init__(self, count, prefix = "tmp", folder = ".", waittime = 0.1):

    assert 0 < count
    self.count = count
    self.prefix = prefix
    self.folder = folder
    self.waittime = waittime


  def create(self):

    return MultiFileQueue(
      count = self.count,
      prefix = self.prefix,
      folder = self.folder,
      waittime = self.waittime,
      )


  @staticmethod
  def destroy(queue):

    queue.close()


 *******************************************************************************
