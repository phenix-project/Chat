

 *******************************************************************************
mmtbx/secondary_structure/find_ss_from_ca.py
from __future__ import absolute_import, division, print_function
# LIBTBX_SET_DISPATCHER_NAME phenix.find_ss_from_ca

# find_ss_from_ca.py
# a tool to find helices, strands, non-helices/strands in segments of
#  structure
#


from libtbx import adopt_init_args
from iotbx.pdb import resseq_encode
import iotbx.phil
import os,sys
from libtbx.utils import Sorry
from libtbx import group_args
from scitbx.matrix import col
from scitbx.math import superpose, matrix
from scitbx.array_family import flex
from copy import deepcopy
from iotbx.pdb import secondary_structure
from six.moves import zip
from six.moves import range
from six.moves import cStringIO as StringIO
from iotbx.pdb.utils import get_pdb_hierarchy

master_phil = iotbx.phil.parse("""

  input_files {
    pdb_in = None
      .type = path
      .help = Input PDB file
      .short_caption = Input PDB file

    secondary_structure_input = None
      .type = path
      .help = Optional input secondary-structure file (can be a PDB or just \
              text) with secondary-structure (HELIX/SHEET) records. If \
              supplied, this secondary structure is used as a starting point \
              and additional information is added if possible.  If \
              force_ss_in=True, then this is used exactly as input. \
      .short_caption = Input secondary structure

    force_secondary_structure_input = False
      .type = bool
      .help = Force use of input secondary_structure without changes (even \
              if H-bonds are not withing max_h_bond_length)
      .short_caption = Force input secondary structure

  }
  output_files {
    pdb_records_file = None
      .type = path
      .help = Output file with HELIX/SHEET records
      .short_caption = Output HELIX/SHEET records file
  }

  find_ss_structure {  # note values from regularize_from_pdb overwrite these
                       # Also values for ss_by_chain and
                       #   max_rmsd are set in
                       #   mmtbx/secondary_structure/__init__.py
     ss_by_chain = True
       .type = bool
       .help = Only applies if search_method = from_ca. \
              Find secondary structure only within individual chains. \
               Alternative is to allow H-bonds between chains. Can be \
               much slower with ss_by_chain=False. If your model is complete \
               use ss_by_chain=True. If your model is many fragments, use \
               ss_by_chain=False.
       .short_caption = Secondary structure by chain
       .expert_level = 1

     auto_choose_ca_vs_ca_n_o = True
       .type = bool
       .help = Automatically identify whether chains are mostly CA or mostly \
                contain CA/N/O atoms (requires min_ca_n_o_completeness).
       .short_caption = Auto choose CA vs CA/N/O

     min_ca_n_o_completeness = 0.95
       .type = float
       .help = Minimum completeness of CA/N/O atoms to not use CA-only
       .short_caption = Minimum completeness of CA/N/O

     max_rmsd = 1
       .type = float
       .help = Maximum rmsd to consider two chains with identical sequences \
               as the same for ss identification
       .short_caption = Maximum rmsd
       .expert_level = 3
     use_representative_chains = True
       .type = bool
       .help = Use a representative of all chains with the same sequence. \
               Alternative is to examine each chain individually. Can be \
               much slower with use_representative_of_chain=False if there \
               are many symmetry copies. Ignored unless ss_by_chain is True.
       .short_caption = Use representative chains
       .expert_level = 3
     max_representative_chains = 100
       .type = float
       .help = Maximum number of representative chains
       .short_caption = Maximum representative chains
       .expert_level = 3
     find_alpha = True
       .type = bool
       .help = Find alpha helices
       .short_caption = Find alpha helices

     helices_are_alpha = False
       .type = bool
       .help = Find alpha helices and not three_ten or pi
       .short_caption = Helices are alpha

     find_three_ten = True
       .type = bool
       .help = Find three_ten helices
       .short_caption = Find three_ten helices

     find_pi = True
       .type = bool
       .help = Find pi helices
       .short_caption = Find pi helices

     find_beta = True
       .type = bool
       .help = Find beta structure
       .short_caption = Find beta structure

     find_other = False
       .type = bool
       .help = Find other structure
       .short_caption = Find other structure

     exclude_alpha_in_beta  = False
       .type = bool
       .help = Exclude regions already identified as alpha from three_ten, pi,\
               and beta
       .short_caption = Exclude alpha from beta

     make_unique = True
       .type = bool
       .help = Assign each residue to a unique type of structure
       .short_caption = Assign residues to unique structure

     cut_up_segments = False
       .type = bool
       .help = Cut up segments (make short segments of secondary structure)
       .short_caption = Cut up segments

     extend_segments = False
       .type = bool
       .help = Try to extend segments in both directions one residue at a time
       .short_caption = Extend segments

     set_up_helices_sheets = True
       .type = bool
       .help = Set up HELIX and SHEET records
       .short_caption = Set up HELIX/SHEET records

     write_helix_sheet_records = True
       .type = bool
       .help = Write HELIX and SHEET records
       .short_caption = Write HELIX/SHEET records

     include_single_strands = False
       .type = bool
       .help = Write SHEET records that contain a single strand
       .short_caption = Write single strands

     remove_missing_atom_annotation = False
       .type = bool
       .help = Remove annotation that refers to atoms that are not present
       .short_caption = Remove missing atom annotation

     max_h_bond_length = 3.5
       .type = float
       .help = Maximum H-bond length to include in secondary structure
       .short_caption = Maximum H-bond length

    search_secondary_structure = True
      .type = bool
      .help = Search for secondary structure in input model. \
              (Alternative is to just use secondary structure from \
              secondary_structure_input.)
      .short_caption = Find secondary structure

    combine_annotations = True
      .type = bool
      .help = Combine annotations if an input annotation is provided
      .short_caption = Combine annotations

    require_h_bonds = False
      .type = bool
      .help = Remove all secondary structure records that have fewer than \
              minimum_h_bonds good hydrogen bonds
      .short_caption = Require H-bonds

    minimum_h_bonds = 1
      .type = int
      .help = Minimum number of good hydrogen bonds to keep secondary \
              structure if require_h_bonds is set
      .short_caption = Minimum number of H bonds

    maximum_poor_h_bonds = None
      .type = int
      .help = Maximum number of poor hydrogen bonds to keep secondary \
              structure element (helix/sheet) if require_h_bonds is set. \
              Note: None means ignore this test, 0 means allow no poor H-bonds.
      .short_caption = Maximum number of poor H bonds

    tolerant = None
      .type = bool
      .help = Set values for tolerant search
      .short_caption = Tolerant search

     tolerant_max_h_bond_length = 5
       .type = float
       .help = Tolerant maximum H-bond length to include in \
           secondary structure
       .short_caption = Tolerant maximum H-bond length
  }

  alpha {
    include scope mmtbx.secondary_structure.secondary_structure_params.alpha_params
  }

  three_ten {
    include scope mmtbx.secondary_structure.secondary_structure_params.three_ten_params
  }

  pi {
    include scope mmtbx.secondary_structure.secondary_structure_params.pi_params
  }

  beta {
    include scope  mmtbx.secondary_structure.secondary_structure_params.beta_params
  }

  other {
    include scope  mmtbx.secondary_structure.secondary_structure_params.other_params
  }


  extract_segments_from_pdb {
    extract = *None alpha beta other
       .type = choice
       .help = Extract all segments (alpha/beta) from a PDB file. \
               Used to create libraries of segments
       .short_caption = Extract segments from PDB
  }

  control {
      verbose = False
        .type = bool
        .help = Verbose output
        .short_caption = Verbose output

  }
""", process_includes=True)
master_params = master_phil

######## Methods for getting selection strings from a model ############

def get_selection_from_chain_dict(chain_dict, minimum_segment_length = None,
   return_as_dict = False, skip_chain_id = False):
  ''' Create a selection based on a dict of residues present in each chain.
   If minimum_segment_length is set, remove segments shorter than this.
  if skip_chain_id, ignore chain ID values'''

  from iotbx.pdb import resseq_encode
  int_type = type(1)
  selection_list = []
  if return_as_dict: info_dict = {}
  if skip_chain_id:
    all_chain_list = []
    for chain_id in list(chain_dict.keys()):
      all_chain_list += chain_dict[chain_id]
    new_chain_dict = {}
    for chain_id in list(chain_dict.keys()):
      new_chain_dict[chain_id] = all_chain_list
    chain_dict = new_chain_dict
  for chain_id in list(chain_dict.keys()):
    residues_to_keep = chain_dict[chain_id]
    if return_as_dict:
      info_dict[chain_id] = []
    for c in compress_indices(residues_to_keep):
      if type(c) == int_type:
        if minimum_segment_length and minimum_segment_length > 1:
          continue # too short
        i = resseq_encode(c).replace(" ","")
        if skip_chain_id:
          selection_list.append(" ( resseq %s:%s) " %(
            i,i))
        else:  # usual
          selection_list.append(" ( chain '%s' and resseq %s:%s) " %(
            chain_id, i,i))
      else:
        if minimum_segment_length and \
           minimum_segment_length > (c[1] - c[0]) + 1:
          continue # too short
        i = resseq_encode(c[0]).replace(" ","")
        j = resseq_encode(c[1]).replace(" ","")
        if skip_chain_id:
          selection_list.append(" (resseq %s:%s) " %(
            i,j))
        else: # usual
          selection_list.append(" (chain '%s' and resseq %s:%s) " %(
            chain_id, i,j))
      if return_as_dict:
        info_dict[chain_id].append([c[0],c[1]])
  if return_as_dict:
    return info_dict
  else: # usual
    return " or ".join(selection_list)

def get_residue_ranges_from_model(model = None):
  chain_dict = get_chain_dict(model.get_hierarchy())
  return get_selection_from_chain_dict(chain_dict, return_as_dict = True)

def get_selection_string_from_model(model = None,
   hierarchy = None, minimum_segment_length = None, skip_chain_id = False):
   ''' Get selection string based on the residues present in selected_model
    If minimum_segment_length is set, remove segments shorter than this '''
   if not hierarchy:
     hierarchy = model.get_hierarchy()
   chain_dict = get_chain_dict(hierarchy)
   return get_selection_from_chain_dict(chain_dict,
     minimum_segment_length = minimum_segment_length,
     skip_chain_id = skip_chain_id)

def get_chain_dict(ph):
    ''' Get dict of chains and all residue numbers in the chains'''
    chain_dict = {}
    for model in ph.models()[:1]:
      for chain in model.chains():
        if not chain.id in list(list(chain_dict.keys())):
          chain_dict[chain.id] = []
        for rg in chain.residue_groups():
          resseq = rg.resseq_as_int()
          if not resseq in chain_dict[chain.id]:
            chain_dict[chain.id].append(resseq)
    for chain_id in list(list(chain_dict.keys())):
      chain_dict[chain.id].sort()
    return chain_dict
def compress_indices(values):
  # Take sorted list of values and compress sequential indices
  compressed_values = []
  # Save list of ranges or lists of individual values (if not sequential)
  n = len(values)
  n1 = n - 1
  working_group = None
  for i in range(n):
    if working_group and working_group[1] + 1  == values[i]:
      working_group[1] = values[i]
    else:
      working_group = [values[i],values[i]]
      compressed_values.append(working_group)
  new_compressed_values = []
  for c1,c2 in compressed_values:
    if c1==c2:
      new_compressed_values.append(c1)
    else:
      new_compressed_values.append([c1,c2])
  return new_compressed_values

######## END Methods for getting selection strings from a model ############

def is_close_to(r,last_r,distance_cutoff=None,use_default_distance_cutoff=True):
  if not r or not last_r:
    return None
  r_atom=None
  atom_type=None
  last_atom_type=None
  for atom in r.atoms():
    if atom.name.strip() in ["CA","P"]:
       r_atom=atom
       atom_type=atom.name.strip()
  last_r_atom=None
  for atom in last_r.atoms():
    if atom.name.strip() in ["CA","P"]:
       last_r_atom=atom
       last_atom_type=atom.name.strip()
  if not r_atom or not last_r_atom: return None
  if last_atom_type != atom_type: return None # change of type
  if distance_cutoff is None and use_default_distance_cutoff:
     if atom_type=="P":
        distance_cutoff=10.  # pretty sure these are not connected
     else:
        distance_cutoff=6.

  dd=col(r_atom.xyz)-col(last_r_atom.xyz)
  if dd.length()<distance_cutoff:
    return True
  else:
    return False

def make_four_char_unique_chain_id(id,used_chain_ids=None):
  # Make a unique chain id that looks like id but has extra characters as
  #  necessary.  Use first 2 chars as input part, last 2 chars of 4 as unique
  original_id=id
  id=id.strip()
  if len(id)<1:
    id="XX"
  elif len(id)<2:
    id="%sX" %(id)
  elif len(id)==2:
    pass # already good
  else:
    raise Sorry(
      "Cannot use longer-than-2 chain ids in make_four_char_unique_chain_id")
  new_chars=" abcdefghijklmnopqrstuvwxyz"
  new_chars+="abcdefghijklmnopqrstuvwxyz".upper()
  new_chars+="0123456789"
  for a in new_chars[1:]:
    for b in new_chars:
      new_id=id+a+b.strip()
      if not new_id in used_chain_ids:
        used_chain_ids.append(new_id)
        return new_id,used_chain_ids
  raise Sorry("Unable to generate a new unique chain ID for %s" %(original_id))

def split_model(model=None,hierarchy=None,verbose=False,info=None,
     only_first_model=None,distance_cutoff=None,
     use_default_distance_cutoff=True,
     out=sys.stdout):
  # XXX NOTE: this splits model at all icode residues (one model per residue)
  # The routine extract_segment below assumes that the residues in an individual
  #  model are sequential (no insertion codes)
  # if CA-CA or P-P distance is > distance-cutoff then split there
  # NOTE: Returns a list of model_info objects (not mmtbx.model objects). These
  #  objects have an attribute: hierarchy which you can use.
  model_list=[]
  if hierarchy:
    if not info: info={}
  elif hasattr(model,'hierarchy'): # a model object with hierarchy and info
    hierarchy=model.hierarchy
    info=model.info
  elif hasattr(model,'get_hierarchy'): # mmtbx.model object
    hierarchy=model.get_hierarchy()
    info = {}
  else:
    return []  # nothing here

  if not hierarchy or hierarchy.overall_counts().n_residues<1:
    return [] # nothing to do
  total_models=0
  base_chain_ids=[]
  for m in hierarchy.models():
    total_models+=1
    if total_models>1:
      raise Sorry("Sorry, find_ss_from_ca cannot use multi-model files. "+\
        "Please use phenix.pdbtools to select just one model from your file.")
    for chain in m.chains():
      new_hierarchy=iotbx.pdb.input(
         source_info="Model", lines=flex.split_lines("")).construct_hierarchy()
      mm=iotbx.pdb.hierarchy.model()
      cc=iotbx.pdb.hierarchy.chain()
      cc.id=chain.id  # copy chain ID
      new_hierarchy.append_model(mm)
      mm.append_chain(cc)

      last_resseq=None
      last_r=None
      is_linked=None
      for r in chain.residue_groups():
        if distance_cutoff is not None or use_default_distance_cutoff:
          is_linked=is_close_to(r,last_r,distance_cutoff=distance_cutoff,
             use_default_distance_cutoff=use_default_distance_cutoff)

        if (last_resseq is not None )  and (r.resseq_as_int()!=last_resseq+1
           or (
            (distance_cutoff is not None or use_default_distance_cutoff) and
            (not is_linked) )):

          # save and make new model
          new_model_info=model_info(hierarchy=new_hierarchy,info=deepcopy(info))
          model_list.append(new_model_info)
          new_model_info.info['chain_number']=len(model_list)

          # and make a new one
          new_hierarchy=iotbx.pdb.input(
             source_info="Model",
             lines=flex.split_lines("")).construct_hierarchy()
          mm=iotbx.pdb.hierarchy.model()
          cc=iotbx.pdb.hierarchy.chain()
          cc.id=chain.id  # copy chain ID
          new_hierarchy.append_model(mm)
          mm.append_chain(cc)
          last_resseq=None
          last_r=None
        # add on a residue...
        cc.append_residue_group(r.detached_copy())
        last_resseq=r.resseq_as_int()
        last_r=r
      new_hierarchy.reset_atom_i_seqs()
      new_model_info=model_info(hierarchy=new_hierarchy,info=deepcopy(info))
      model_list.append(new_model_info)
      new_model_info.info['chain_number']=len(model_list)
    if only_first_model:
      break

  if verbose:
    print("Models after splitting:", file=out)
    for m in model_list:
      print("Chain: %d  Residues: %d" %(
        m.info.get('chain_number'),
        m.hierarchy.overall_counts().n_residues), file=out)
  return model_list

def sort_models_and_sequences(models,sequences):
  # sort based on chain type if available
  sort_dict={}
  for m,s in zip(models,sequences):
    if not m or not m.hierarchy: continue
    chain_type=str(m.info.get('chain_type'))
    if not chain_type in sort_dict: sort_dict[chain_type]=[]
    sort_dict[chain_type].append([m,s])
  keys=list(sort_dict.keys())
  if len(keys)<2:
    return models,sequences # do nothing if only one chain type

  keys.sort()
  keys.reverse()
  new_models=[]
  new_sequences=[]
  for key in keys:
    for [m,s] in sort_dict[key]:
      new_models.append(m)
      new_sequences.append(s)
  return new_models,new_sequences

def merge_hierarchies_from_models(models=None,resid_offset=None,
    renumber=None,first_residue_number=None,
    sequences=None,chain_id=None,trim_side_chains=None,
    remove_ter_records=False,
    remove_break_records=False,
    replace_hetatm=False,
     ):
  # assumes one chain from each model
  # if resid_offset, space by to next even n of this number of residues
  # otherwise if renumber, start at first_residue_number and sequence
  #  consecutively
  # If sequence or chain_id are supplied, use them
  # Trim off side chains (and CB for GLY) if trim_side_chains
  # sort by chain_type if provided in one or more
  # replace hetero=True by hetero=False if replace_hetatm is set

  new_hierarchy=create_new_hierarchy()
  mm=iotbx.pdb.hierarchy.model()
  new_hierarchy.append_model(mm)
  if renumber:  # just get chain once in advance
    cc=iotbx.pdb.hierarchy.chain()
    if chain_id:
      cc.id=chain_id
    mm.append_chain(cc)

  # set resid if necessary
  if resid_offset:
    if first_residue_number:
      resid=first_residue_number
    else:
      resid=1
  elif renumber:
    if first_residue_number is None:
      raise Sorry(
        "Need first_residue_number for merge_hierarchies_from_models if "+
         "renumber=True")
    resid=first_residue_number

  info={}
  from iotbx.pdb import resseq_encode

  if not sequences: sequences=len(models)*[None]
  models,sequences=sort_models_and_sequences(models,sequences)
  for model,sequence in zip(models,sequences):
    if not model or not model.hierarchy: continue
    if not model.info:
        model.info={}
    if not info: info=model.info
    i=0
    for m in model.hierarchy.models():
      for chain in m.chains():
        if not renumber:
          cc=iotbx.pdb.hierarchy.chain()
          if chain_id:
            cc.id=chain_id
          elif chain.id: # take what is already there
            cc.id=chain.id
          mm.append_chain(cc)
        for r in chain.residue_groups():
          rr=r.detached_copy()
          if resid_offset or renumber:
            rr.resseq=resseq_encode(resid)
            resid+=1
          if sequence:
            resname=sequence[i]
            i+=1
            for atom_group in rr.atom_groups():
              atom_group.resname=resname
          cc.append_residue_group(rr)
        if resid_offset and resid_offset>1:
          nn=resid_offset*((resid+resid_offset-1)//resid_offset)
          if nn-resid<2: nn+=resid_offset
          resid=nn
  if replace_hetatm:
    for atom in new_hierarchy.atoms():
      atom.hetero = False

  new_hierarchy.reset_atom_i_seqs()
  if trim_side_chains:
    atom_selection=\
      "name ca or name c or name o or name n or (name cb and not resname gly)"
    new_hierarchy=new_hierarchy.apply_atom_selection(atom_selection)

  if remove_ter_records or remove_break_records:
    new_hierarchy = remove_ter_or_break(new_hierarchy)

  new_model=model_info(hierarchy=new_hierarchy,info=info)

  return new_model

def pdb_or_mmcif_fn_info(fn):
  b0,e0 = os.path.splitext(fn)
  fn_pdb = "%s.pdb" %(b0)
  fn_cif = "%s.cif" %(b0)
  if os.path.isfile(fn_pdb):
    return group_args(
       file_name = fn_pdb,
       format = 'pdb')
  elif os.path.isfile(fn_cif):
    return group_args(
       file_name = fn_cif,
       format = 'mmcif')
    return fn_cif
  else:
    return None

def remove_ter_or_break(ph):
  for m0 in ph.models():
      for c0 in m0.chains():
       first = True
       for residue_group in c0.residue_groups():
         if not first:
           residue_group.link_to_previous = True
         first = False
  return ph

def get_average_direction(diffs=None, i=None,j=None):
    if not diffs: return None
    if i is None and j is None:
      i=0
      j=len(diffs)-1
    average_direction=col(diffs[i])
    nn=1.
    for j in range(i+1,j):
      nn+=1.
      average_direction+=col(diffs[j])
    average_direction/=nn
    return average_direction.normalize()

def get_first_chain_id_and_resno(hierarchy):
  text = ""
  if not hierarchy:
    return text
  for model in hierarchy.models():
    for chain in model.chains():
      for rg in chain.residue_groups():
        return "%s%s" %(chain.id,rg.resseq_as_int())

def get_last_chain_id_and_resno(hierarchy):
  text = ""
  if not hierarchy:
    return text
  for model in hierarchy.models():
    for chain in model.chains():
      for rg in chain.residue_groups()[-1:]:
        text = "%s%s" %(chain.id,rg.resseq_as_int())
  return text

def remove_all_models_except_first(hierarchy):
  ''' removes all models except the first in this hierarchy'''
  model = hierarchy.models()[0]
  new_hierarchy=iotbx.pdb.input(
         source_info="Model",
             lines=flex.split_lines("")).construct_hierarchy()
  new_hierarchy.append_model(model.detached_copy())
  return new_hierarchy


def offset_residue_numbers(hierarchy, offset = 0):
  for model in hierarchy.models():
    for chain in model.chains():
      for rg in chain.residue_groups():
        current_resno = rg.resseq_as_int()
        rg.resseq = resseq_encode(current_resno + offset)

def merge_and_renumber_everything(hierarchy, current_resno = 1,
    chain_id = 'A'):
  new_hierarchy, mm = create_new_hierarchy_and_model()
  cc = iotbx.pdb.hierarchy.chain()
  cc.id = chain_id
  mm.append_chain(cc)

  for model in hierarchy.models():
    for chain in model.chains():
      for rg in chain.residue_groups():
        new_rg = rg.detached_copy()
        new_rg.resseq = resseq_encode(current_resno)
        cc.append_residue_group(new_rg)
        current_resno += 1
  return new_hierarchy


def renumber_residues(hierarchy, first_resno = 1,
    fixed_offset = False, increment_if_insertion_code = True):

  if fixed_offset:
    offset = None
    last_resno = None
    for model in hierarchy.models():
      for chain in model.chains():
        for rg in chain.residue_groups():
          if first_resno is None:
            first_resno = rg.resseq_as_int()
          if offset is None:
            offset = first_resno - rg.resseq_as_int()
          current_resno = rg.resseq_as_int() + offset
          if current_resno == last_resno  and increment_if_insertion_code:
            offset += 1
            current_resno += 1
          prev = rg.resseq
          rg.resseq = resseq_encode(current_resno)
          last_resno = current_resno
    return

  # Usual
  for model in hierarchy.models():
    for chain in model.chains():
      current_resno = first_resno
      for rg in chain.residue_groups():
        if current_resno is None:
          current_resno = rg.resseq_as_int()
        rg.resseq = resseq_encode(current_resno)
        current_resno += 1

def create_new_hierarchy():
  from iotbx.pdb import hierarchy
  return hierarchy.root()

def create_new_hierarchy_and_model():
  import iotbx.pdb
  new_hierarchy = create_new_hierarchy()
  mm = iotbx.pdb.hierarchy.model()
  new_hierarchy.append_model(mm)
  return new_hierarchy, mm

def reorder_residues(hierarchy, merge_chains = False, chain_id = None):
  import iotbx.pdb
  residue_dict = {}
  for model in hierarchy.models():
    for chain in model.chains():
       if merge_chains and chain_id is None:
         chain_id = chain.id
       elif merge_chains:
         pass # already have chain_id
       else:
         chain_id = chain.id # keep it
       if not chain_id in residue_dict.keys():
         residue_dict[chain_id] = {}
       for rg in chain.residue_groups():
          residue_dict[chain_id][rg.resseq_as_int()] = rg.detached_copy()
  keys = list(residue_dict.keys())
  keys = sorted(keys)
  new_hierarchy, model = create_new_hierarchy_and_model()
  for chain_id in keys:
    cc = iotbx.pdb.hierarchy.chain()
    cc.id = chain_id
    model.append_chain(cc)
    residue_numbers = sorted(list(residue_dict[chain_id].keys()))
    for resseq in residue_numbers:
      rg = residue_dict[chain_id][resseq]
      cc.append_residue_group(rg)
  return new_hierarchy

def set_chain_id(hierarchy, chain_id = None, original_id = None):
  assert chain_id
  for model in hierarchy.models():
    for chain in model.chains():
      if (original_id is None) or (chain.id == original_id):
        chain.id = chain_id

def get_atom_list(hierarchy):
  atom_list=[]
  if not hierarchy:
    return atom_list
  for model in hierarchy.models():
    for chain in model.chains():
      for rg in chain.residue_groups():
        for atom_group in rg.atom_groups():
          for atom in atom_group.atoms():
            atom_list.append(atom.name)
  return atom_list

def get_atom_from_residue(residue=None,
  atom_name=None,allow_ca_only_model=False,
  skip_n_for_pro=False):
  if not residue:
    return None,None
  # just make sure that atom_name is there
  for atom in residue.atoms():
    if atom.name.replace(" ","")==atom_name.replace(" ",""):
      if skip_n_for_pro and atom.name.replace(" ","")=='N' and \
         residue.resname.replace(" ","").upper()=="PRO":
        return None,None
      else:
        return atom.name,atom.xyz
  if allow_ca_only_model:
    return atom_name,None
  else:
    return None,None

def get_indexed_residue(hierarchy,index=0):
  if not hierarchy:
    return None
  count=0
  for model in hierarchy.models():
    for chain in model.chains():
      for conformer in chain.conformers():
        for residue in conformer.residues():
          if count==index:
            return residue
          count+=1

def get_first_residue(hierarchy):
  if not hierarchy:
    return None
  for model in hierarchy.models():
    for chain in model.chains():
      for conformer in chain.conformers():
        for residue in conformer.residues():
          return residue

def get_last_residue(hierarchy):
  if not hierarchy:
    return None
  last_residue=None
  for model in hierarchy.models():
    for chain in model.chains():
      for conformer in chain.conformers():
        for residue in conformer.residues()[-1:]:
          last_residue=residue
  return last_residue

def has_atom(hierarchy,name=None):
  if not hierarchy:
    return None
  for model in hierarchy.models():
    for chain in model.chains():
      for conformer in chain.conformers():
        for residue in conformer.residues():
          for atom in residue.atoms():
            if atom.name.replace(" ","")==name.replace(" ",""):
              return True
  return False

def get_all_resno(hierarchy):
  resno_list=[]
  if not hierarchy:
    return resno_list
  for model in hierarchy.models():
    for chain in model.chains():
      for rg in chain.residue_groups():
        resno_list.append(rg.resseq_as_int())
  return resno_list

def get_middle_resno(hierarchy,first_resno=None,last_resno=None):
  if not hierarchy:
    return None
  if first_resno is None:
     first_resno=hierarchy.first_resseq_as_int()
  if last_resno is None:
     last_resno=hierarchy.last_resseq_as_int()
  target_resno=int(0.5+(first_resno+last_resno)/2)
  for model in hierarchy.models():
    for chain in model.chains():
      for rg in chain.residue_groups():
        middle_resno=rg.resseq_as_int()
        if middle_resno >= target_resno:
          return middle_resno
  return last_resno

def verify_existence(hierarchy=None,
   prev_hierarchy=None,strand=None,registration=None,helix=None):
  ok=True

  if hierarchy and is_ca_only_hierarchy(hierarchy):
    is_ca_only = True
  else:
    is_ca_only = False

  for sh in [strand,helix]:
    if sh:
      start_atom=get_atom_index(hierarchy=hierarchy,
        atom_name=None,
        resname=sh.start_resname,
        chain_id=sh.start_chain_id,
        resseq=sh.start_resseq,
        icode=sh.start_icode)
      if start_atom is None:
        raise Sorry(
        "The starting residue in an annotation is missing in the input model:"+
        "%s %s %s %s" %(
         sh.start_resname,sh.start_chain_id,sh.start_resseq,sh.start_icode))

    if sh:
      end_atom=get_atom_index(hierarchy=hierarchy,
        atom_name=None,
        resname=sh.end_resname,
        chain_id=sh.end_chain_id,
        resseq=sh.end_resseq,
        icode=sh.end_icode)
      if end_atom is None:
        raise Sorry(
        "The ending residue in an annotation is missing in the input model:"+
        "%s %s %s %s" %(
         sh.end_resname,sh.end_chain_id,sh.end_resseq,sh.end_icode))

  if registration:
    cur_atom=get_atom_index(hierarchy=hierarchy,
      atom_name= None if is_ca_only else registration.cur_atom,
      resname=registration.cur_resname,
      chain_id=registration.cur_chain_id,
      resseq=registration.cur_resseq,
      icode=registration.cur_icode)
    if cur_atom is None:
        raise Sorry(
        "A residue in a strand registration is missing in the input model:"+
        "%s %s %s %s %s" %(
        registration.cur_atom,registration.cur_resname,
        registration.cur_chain_id,registration.cur_resseq,
        registration.cur_icode))

  if registration and prev_hierarchy:
    prev_atom=get_atom_index(hierarchy=prev_hierarchy,
      atom_name=None if is_ca_only else registration.prev_atom,
      resname=registration.prev_resname,
      chain_id=registration.prev_chain_id,
      resseq=registration.prev_resseq,
      icode=registration.prev_icode)
    if prev_atom is None:
      raise Sorry(
        "A residue in a previous strand registration is missing in the input model:"+
        "%s %s %s %s %s" %(
        registration.prev_atom,registration.prev_resname,
        registration.prev_chain_id,registration.prev_resseq,
        registration.prev_icode))

def get_atom_index(hierarchy=None,atom_name=None,resname=None,
 chain_id=None,resseq=None,icode=None,
   take_first_if_missing = None):
  # NOTE: could speed up using atom cache and iselection
  # find the index in hierarchy of this atom
  if not hierarchy:
    return None
  count=-1
  first_present = None
  for model in hierarchy.models():
    for chain in model.chains():
      for conformer in chain.conformers():
        for residue in conformer.residues():
          count+=1
          if not chain.id==chain_id: continue
          if not residue.resseq.replace(" ","")==str(resseq).replace(" ","") \
             or not residue.icode==icode:
            continue
          for atom in residue.atoms():
            if first_present is None:
              first_present = count
            if atom_name is None or atom.name==atom_name:
              return count
  if take_first_if_missing:  # XXX Added in case it is ca model and force
    return first_present
  else:
    return None

def have_n_or_o(models):
    have_n=False
    have_o=False
    for model in models:
      if model.has_n():
        have_n=True
        break
    for model in models:
      if model.has_o():
        have_o=True
        break
    if have_n and have_o:
      return True
    return False

def are_sites_parallel(sites_1,sites_2, try_reverse=True):
      point = flex.vec3_double()
      point.append(sites_1[0])
      dist, id1, id2 =  point.min_distance_between_any_pair_with_id(sites_2)
      point = flex.vec3_double()
      point.append(sites_1[-1])
      dist, id1, id2b =  point.min_distance_between_any_pair_with_id(sites_2)
      if id2b < id2:  # antiparallel
        return False
      elif id2b > id2: # parallel
        return True
      elif not try_reverse:
        return None
      else:
        return are_sites_parallel(sites_2,sites_1,try_reverse=False)

def evaluate_sheet_topology(annotation, hierarchy = None,
   chain_id = None,
   out = sys.stdout):
  print("\nEvaluating sheet topology", file = out)
  ca_ph=hierarchy.apply_atom_selection("name ca")
  if chain_id:
    ca_ph=ca_ph.apply_atom_selection("chain '%s'" %(chain_id))
  unique_chain_ids = ca_ph.chain_ids(unique_only = True)
  if len(unique_chain_ids) != 1:
    raise Sorry("Need just 1 chain for evaluate_sheet_topology (found %s)" %(
      " ".join(unique_chain_ids)))

  rh_connections = 0
  lh_connections = 0
  mixed_on_same_side_of_sheet = 0
  connection_list = []
  for sheet in annotation.sheets:
    rh_on_up_side = 0
    lh_on_up_side = 0
    rh_on_down_side = 0
    lh_on_down_side = 0
    if len(sheet.strands) <  2: continue # nothing to do
    # Get first, last residue number in each strand
    # Get directions for each strand in the sheet
    new_strands = []
    previous_direction = 1
    for strand in sheet.strands:
      new_strand = deepcopy(strand)
      if new_strand.sense == -1: # antiparallel to previous
        new_strand.direction = -1 * previous_direction
      elif new_strand.sense == 0:
        new_strand.direction = 1
      else:
        new_strand.direction = previous_direction
      previous_direction = new_strand.direction
      new_strands.append(new_strand)

    strand_list = sorted(new_strands, key = lambda sheet: sheet.start_resseq)
    overall_up = None
    for ii in range(len(strand_list)-1):
     s1 = strand_list[ii]
     for jj in range(ii+1, len(strand_list)):
      s2 = strand_list[jj]
      if s2.get_start_resseq_as_int() - s1.get_end_resseq_as_int() < 3:
        continue  # just a continuation
      sites_1 =ca_ph.apply_atom_selection("resseq %s:%s" %(
        s1.get_start_resseq_as_int(),
        s1.get_end_resseq_as_int())).atoms().extract_xyz()
      sites_2 =ca_ph.apply_atom_selection("resseq %s:%s" %(
        s2.get_start_resseq_as_int(),
        s2.get_end_resseq_as_int())).atoms().extract_xyz()
      sites_between = ca_ph.apply_atom_selection("resseq %s:%s" %(
        s1.get_end_resseq_as_int() + 1,
        s2.get_start_resseq_as_int() - 1)).atoms().extract_xyz()
      if sites_1.size() < 2 or sites_2.size()< 2 or sites_between.size()<2:
        continue  # nothing to do

      if s1.direction == 0 or s2.direction == 0 or s1.direction != s2.direction:
        continue # not parallel


      # Pare down sites to be only close ones
      direction_1 = get_direction(sites_1)
      direction_2 = get_direction(sites_2)
      up = get_up(sites_1, sites_2)
      direction_along_strands = direction_1
      sites_1 = get_close_sites(sites_1, close_to = sites_2,
        direction_along_strands = direction_along_strands, max_dist = 10)
      sites_2= get_close_sites(sites_2, close_to = sites_1,
         direction_along_strands = direction_along_strands, max_dist = 10)
      if sites_1.size() < 2 or sites_2.size()< 2 or sites_between.size()<2:
        continue  # nothing to do

      direction_1 = get_direction(sites_1)
      direction_2 = get_direction(sites_2)
      up = get_up(sites_1, sites_2)
      direction_between_strands = direction_1.cross(up)
      sites_1 = get_close_sites_perp(sites_1, close_to = sites_2,
        direction_between_strands = direction_between_strands, max_dist = 10)
      sites_2= get_close_sites_perp(sites_2, close_to = sites_1,
         direction_between_strands = direction_between_strands, max_dist = 15)
      if sites_1.size() < 2 or sites_2.size()< 2 or sites_between.size()<2:
        continue  # nothing to do

      direction_1 = get_direction(sites_1)
      direction_2 = get_direction(sites_2)
      up = get_up(sites_1, sites_2)

      # Get direction of "up" if we didn't already. It is direction_1 x (
      #   (sites_2[0] - sites_1[-1]).  If sites_2 is to left of sites_1, then
      #   "up"  is up.
      #  Now if the chain goes "up" then the connection is left_handed
      if not overall_up: overall_up = up

      #Get location of connection. Find point closest to a point up or down
      #  from middle of sheet.
      all_sites = sites_1.deep_copy()
      all_sites.extend(sites_2)
      center = col(all_sites.mean())
      point_up = center + 10 * up
      point_down = center - 10 * up
      point = flex.vec3_double()
      point.append(point_up)
      dist_up, id1, id2 =  point.min_distance_between_any_pair_with_id(
         sites_between)
      point = flex.vec3_double()
      point.append(point_down)
      dist_down, id1_down, id2_down =  \
          point.min_distance_between_any_pair_with_id(sites_between)
      if dist_up < dist_down:
        is_right_handed = False
      else:
        is_right_handed = True
      if up.dot(overall_up) > 0:
        crosses_on_overall_up_side = True
        if is_right_handed:
          rh_on_up_side += 1
        else:
          lh_on_up_side += 1
      else:
        crosses_on_overall_up_side = False
        if is_right_handed:
          rh_on_down_side += 1
        else:
          lh_on_down_side += 1

      result = group_args(
       group_args_type = 'connections in sheet',
       s1 = "resseq %s:%s" %(
         s1.get_start_resseq_as_int(),
         s1.get_end_resseq_as_int()),
       s2 = "resseq %s:%s" %(
         s2.get_start_resseq_as_int(),
         s2.get_end_resseq_as_int()),
       is_right_handed = is_right_handed,
       crosses_on_overall_up_side = crosses_on_overall_up_side)
      connection_list.append(result)

      if rh_on_up_side and lh_on_up_side:
        mixed_on_same_side_of_sheet += (rh_on_up_side + lh_on_up_side)
      rh_connections += (rh_on_up_side + rh_on_down_side)
      lh_connections += (lh_on_up_side + lh_on_down_side)

  print("Total right-handed connections: %s" %(rh_connections), file = out)
  print("Total left-handed connections: %s" %(lh_connections), file = out)
  print("Total mixed connections on same side of sheet: %s" %(
     mixed_on_same_side_of_sheet), file = out)
  return group_args(
    group_args_type = 'sheet connectivity analysis',
       rh_connections = rh_connections,
       lh_connections = lh_connections,
       mixed_on_same_side_of_sheet = mixed_on_same_side_of_sheet,
       connection_list = connection_list,
     )

def get_close_sites_perp(sites_1,
     close_to = None, direction_between_strands = None,
       max_dist = 15):
  close = flex.vec3_double()
  if sites_1.size()< 1 or close_to.size()<1:
    return close
  for i in range(sites_1.size()):
    dist, id1, id2 =  sites_1[i:i+1].min_distance_between_any_pair_with_id(
         close_to)
    vector_between = col(close_to[id2]) - col(sites_1[i])
    dist_perp= direction_between_strands.dot(vector_between)
    if abs(dist) <= max_dist:
       close.append(sites_1[i])
  return close

def get_close_sites(sites_1, close_to = None, direction_along_strands = None,
       max_dist = 8):
  close = flex.vec3_double()
  if sites_1.size()< 1 or close_to.size()<1:
    return close
  for i in range(sites_1.size()):
    dist, id1, id2 =  sites_1[i:i+1].min_distance_between_any_pair_with_id(
         close_to)
    vector_between = col(close_to[id2]) - col(sites_1[i])
    dist_parallel = direction_along_strands.dot(vector_between)
    if abs(dist_parallel)<= max_dist:
       close.append(sites_1[i])
  return close


def get_up(sites_1, sites_2):
  direction_1 = get_direction(sites_1)
  one_to_two = normalize(col(sites_2[0]) - col(sites_1[-1]))
  up = direction_1.cross(one_to_two)
  return normalize(up)

def normalize(site):
  dist = site.length()
  if dist > 0:
    return site * (1./dist)
  else:
    return site

def get_direction(sites_cart):
  if sites_cart.size() < 2:
    return None
  return normalize(col(sites_cart[-1]) - col(sites_cart[0]))

def remove_bad_annotation(annotation,hierarchy=None,
     maximize_h_bonds=True,
     max_h_bond_length=None,
     maximum_length_difference=None,
     minimum_overlap=None,
     remove_overlaps=True,
     out=sys.stdout):
  # XXX perhaps move this to secondary_structure.py

  # remove parts of annotation that do not exist in the hierarchy or
  #  that overlap with other annotations
  new_annotation=annotation.split_sheets()

  deleted_something=False
  new_helices=[]
  for helix in new_annotation.helices:
    ph=hierarchy.apply_atom_selection(get_string_or_first_element_of_list(
       helix.as_atom_selections()))
    try:
        verify_existence(hierarchy=ph,helix=helix)
        new_helices.append(helix)
    except Exception as e: # the helix needs to be deleted
        print("%s\nDeleting this helix" %(str(e)), file=out)
        deleted_something=True
  if deleted_something:
    new_annotation.helices=new_helices

  new_sheets=[]
  for sheet in new_annotation.sheets:
    prev_hierarchy=None
    strands_ok=True
    registrations_ok=True
    for strand,registration in zip(sheet.strands,sheet.registrations):
      # verify that first and last atom selections in strand exist
      ph=hierarchy.apply_atom_selection(
         get_string_or_first_element_of_list(strand.as_atom_selections()))
      try:
        verify_existence(hierarchy=ph,prev_hierarchy=prev_hierarchy,
         strand=strand)
      except Exception as e: # the strand needs to be deleted
        print("%s\nDeleting this strand" %(str(e)), file=out)
        strands_ok=False
        deleted_something=True
      try:
        verify_existence(hierarchy=ph,prev_hierarchy=prev_hierarchy,
         registration=registration)
      except Exception as e: # the registration needs to be deleted
        print("Deleting registration\n%s" %(str(e)), file=out)
        registrations_ok=False
        deleted_something=True
      prev_hierarchy=ph
    if strands_ok:
      if not registrations_ok:
        sheet.registrations=[None,None]
      new_sheets.append(sheet)
  new_annotation.sheets=new_sheets

  if deleted_something:
    new_annotation.merge_sheets()
    if new_annotation.as_pdb_or_mmcif_str():
      print("User annotation after removing bad parts:", file=out)
      print(new_annotation.as_pdb_or_mmcif_str(), file=out)
    else:
      print("No annotation left after removing bad parts", file=out)
      return None

  if not remove_overlaps:
    return new_annotation

  # Now remove overlaps

  no_overlap_annotation=new_annotation.remove_overlapping_annotations(
    hierarchy=hierarchy,
    maximize_h_bonds=maximize_h_bonds,
    max_h_bond_length=max_h_bond_length,
    maximum_length_difference=maximum_length_difference,
    minimum_overlap=minimum_overlap)

  if not no_overlap_annotation.is_same_as(other=new_annotation):
    print("Edited annotation without overlaps:", file=out)
    print(no_overlap_annotation.as_pdb_or_mmcif_str(), file=out)
    print(file=out)

  return no_overlap_annotation

def hierarchy_from_chain(chain):
  # create a hierarchy from a chain
  new_hierarchy=iotbx.pdb.input(
     source_info="Model", lines=flex.split_lines("")).construct_hierarchy()
  mm=iotbx.pdb.hierarchy.model()
  cc=chain.detached_copy()
  cc.id=chain.id  # copy chain ID
  new_hierarchy.append_model(mm)
  mm.append_chain(cc)
  return new_hierarchy


def get_string_or_first_element_of_list(something):
  if type(something)==type([1,2,3]):
    return something[0]
  else:
    return something

def sequence_from_hierarchy(hierarchy,chain_type='PROTEIN'):
  from iotbx.pdb import amino_acid_codes
  ott = amino_acid_codes.one_letter_given_three_letter

  sequence=""
  for model in hierarchy.models()[:1]:
    for chain in model.chains()[:1]:
      for rr in chain.residue_groups():
        for atom_group in rr.atom_groups()[:1]:
           sequence+=ott.get(atom_group.resname,"")
  return sequence

def sites_are_similar(sites1,sites2,max_rmsd=1):
  # return True if sites can be superimposed
  if max_rmsd is None:
    return True
  if max_rmsd < 0:
    return False
  if sites1.size() != sites2.size():
    return False
  if sites1.size() < 3:
    return False
  lsq_fit_obj=superpose.least_squares_fit(
       reference_sites=sites1,
       other_sites=sites2)
  sites2_fitted=lsq_fit_obj.other_sites_best_fit()
  rmsd = sites1.rms_difference(sites2_fitted)
  if rmsd <=max_rmsd:
    return True
  else:
    return False


def is_ca_only_hierarchy(hierarchy):
  if not hierarchy: return None
  return hierarchy.is_ca_only()

def ca_n_and_o_always_present(hierarchy):
  if not hierarchy: return None
  total_residues=hierarchy.overall_counts().n_residues
  asc=hierarchy.atom_selection_cache()
  for n in ("ca","n","o"):
    atom_selection="name %s" %(n)
    sel = asc.selection(string = atom_selection)
    if sel.count(True) != total_residues:
      return False
  return True


def get_fraction_complete_backbone(hierarchy):
  if not hierarchy: return 0
  total_residues=hierarchy.overall_counts().n_residues
  asc=hierarchy.atom_selection_cache()
  minimum_complete=total_residues
  for n in ("ca","n","o"):
    atom_selection="name %s" %(n)
    sel = asc.selection(string = atom_selection)
    if sel.count(True) < minimum_complete:
      minimum_complete=sel.count(True)
  if minimum_complete==total_residues:
    return 1 # just to make sure it is exactly 1
  else:
    return minimum_complete/max(1,total_residues)

def choose_ca_or_complete_backbone(hierarchy, params=None):
  # Purpose: return a hierarchy that is either completely CA or has no CA-only
  #   residues
  fraction_complete_backbone=get_fraction_complete_backbone(hierarchy)
  if fraction_complete_backbone == 1:
    return hierarchy # nothing to do
  if fraction_complete_backbone < \
       params.find_ss_structure.min_ca_n_o_completeness:
    # just use CA-only
    return hierarchy.apply_atom_selection('name CA')
  else:  # remove CA-only residues
    hierarchy.remove_incomplete_main_chain_protein()
    return hierarchy

def sites_and_seq_from_hierarchy(hierarchy):
  atom_selection="name ca"
  sele=hierarchy.apply_atom_selection(atom_selection)
  if sele.overall_counts().n_residues==0:
    sites=flex.vec3_double()
    sequence=""
  else:
    sites=sele.extract_xray_structure(min_distance_sym_equiv=0 # REQUIRED
        ).sites_cart()
    sequence=sequence_from_hierarchy(sele)
  start_resno=sele.first_resseq_as_int()
  end_resno=sele.last_resseq_as_int()
  return sites,sequence,start_resno,end_resno

class model_info: # mostly just a holder
  """Holder for information about a segment"""
  def __init__(self,hierarchy=None,id=0,info={},
      find_alpha=None,find_beta=None,find_other=None,
      find_three_ten=None,find_pi=None):
    self.hierarchy=hierarchy
    self.info=info
    self.id=id
    self.find_alpha=find_alpha
    self.find_three_ten=find_three_ten
    self.find_pi=find_pi
    self.find_beta=find_beta
    self.find_other=find_other

  def show_summary(self,out=sys.stdout):
    """Summarize this model_info object"""
    keys=list(self.info.keys())
    keys.sort()
    print("\nModel %d" %(self.id), file=out)
    for key in keys:
      print("%s: %s" %(key,str(self.info[key])), file=out)

  def has_n(self):
    """Return True if the hierarchy contains an N atom"""
    return has_atom(self.hierarchy,name="N")

  def has_o(self):
    """Return True if the hierarchy contains an O atom"""
    return has_atom(self.hierarchy,name="O")

  def first_residue(self):
    """Return the first residue number (resseq_as_int()))"""
    return self.hierarchy.first_resseq_as_int()

  def last_residue(self):
    """Return the last residue number (resseq_as_int()))"""
    return self.hierarchy.last_resseq_as_int()

  def length(self):
    """Return the length of this chain"""
    return self.last_residue()-self.first_residue()+1

  def set_chain_type(self): # XXX specific for atom names N O2'/O2*
    """Set the chain type if not already set"""
    if self.info and self.info.get('chain_type'): return
    if not self.info: self.info={}
    if has_atom(self.hierarchy,name="N") or has_atom(self.hierarchy,name="CA"):
      self.info['chain_type']="PROTEIN"
    else:
      if has_atom(self.hierarchy,name="O2'") or  \
          has_atom(self.hierarchy,name="O2*"):
        self.info['chain_type']="RNA"
      else:
        self.info['chain_type']="DNA"

class segment:
  """Object for holding a helix or a strand or other"""

  def setup(self,sites=None,start_resno=None,hierarchy=None,
     segment_class=None,
     segment_type='None',name=None,
     span=None,target_rise=None,residues_per_turn=None,
     rise_tolerance=None,dot_min=None,dot_min_single=None,
     target_i_ip3=None,tol_i_ip3=None,
     minimum_length=None,
     buffer_residues=None,
     standard_length=None,
     is_n_terminus=None,
     is_c_terminus=None,
     frequency=None,
     base_score=None,
     optimal_delta_length=None,
     verbose=None,
     out=sys.stdout):

    """Set up this segment_object"""
    self.segment_class=segment_class
    self.out=out
    self.verbose=verbose
    self.orientation_points=None
    self.orientation_points_start=None
    self.orientation_points_end=None
    self.is_n_terminus=is_n_terminus
    self.is_c_terminus=is_c_terminus
    self.buffer_residues=buffer_residues
    self.standard_length=standard_length
    self.minimum_length=minimum_length
    self.segment_type=segment_type
    self.frequency=frequency
    self.base_score=base_score
    self.rise_tolerance=rise_tolerance
    self.target_i_ip3=target_i_ip3
    self.tol_i_ip3=tol_i_ip3
    self.dot_min=dot_min
    self.dot_min_single=dot_min_single
    self.diffs_single=None
    self.name=name # alpha helix, etc
    self.span=span # 3.5 for helices, 2 for strands, 0 for other
    self.target_rise=target_rise # 1.54 for helices, 3.3 strands
    self.residues_per_turn=residues_per_turn #3.6 alpha, 3 3-10, 4 pi, strand na
    self.optimal_delta_length=optimal_delta_length # target change
        # in number of residues for this
        # segment (based on segment type and end-to-end distance)
    self.hierarchy=hierarchy # optional full hierarchy of this segment

    self.sites=None
    if sites:
      self.sites=sites
    elif self.hierarchy:
      self.get_sites_from_hierarchy()
    if start_resno is not None:
      self.start_resno=start_resno
    elif self.hierarchy:
      self.start_resno=self.hierarchy.first_resseq_as_int()
      assert start_resno is None
    else:
      start_resno=1
      self.start_resno=start_resno

  def summary(self):
    """Return summary of this segment_object"""
    return self.show_summary()

  def show_summary(self,return_text=False):
    """Return or print summary of this segment_object"""
    text="Class: %6s  Length: %d  Start: %d  End: %d" %(
     self.segment_type,self.length(),self.get_start_resno(),
      self.get_start_resno()+self.length()-1)
    if return_text:
      return text
    else:
      print(text, file=self.out)

  def get_diffs_and_norms_3(self):
    """Report distances between i and i+3"""
    sites_offset_3=self.sites[3:]
    diffs=sites_offset_3-self.sites[:-3]
    return diffs.norms()

  def get_min_diff_i_i3(self):
    """Report minimum distance between i and i+3"""
    norms=self.get_diffs_and_norms_3()
    if norms is None: return
    mm=norms.min_max_mean()
    return mm.min

  def trim_ends(self,start_pos=None,end_pos=None):
    """Trim back from start_pos to end_pos (not residue number, position in
      existing segment"""
    start_res=self.get_start_resno()+start_pos
    end_res=self.get_start_resno()+end_pos
    if start_pos==0 and end_pos==self.length()-1:
      return # nothing to do
    atom_selection="resid %s through %s" %(resseq_encode(start_res),
       resseq_encode(end_res))

    self.hierarchy=self.hierarchy.apply_atom_selection(atom_selection)
    self.start_resno=self.start_resno+start_pos
    self.get_sites_from_hierarchy()
    if self.optimal_delta_length:
      self.optimal_delta_length=0 # we no longer know how long it should be

  def contains_ends(self,first_resno_of_chain=None,last_resno_of_chain=None):
     """Return whether first resno in chain is present, last resno in
     chain is present"""
     if first_resno_of_chain>=self.get_start_resno() and \
        first_resno_of_chain<=self.get_end_resno():
       contains_left_end_of_chain=True
     else:
       contains_left_end_of_chain=False
     if last_resno_of_chain>=self.get_start_resno() and \
        last_resno_of_chain<=self.get_end_resno():
       contains_right_end_of_chain=True
     else:
       contains_right_end_of_chain=False
     return contains_left_end_of_chain,contains_right_end_of_chain

  def get_sites_from_hierarchy(self):
    """Get sites from CA atoms in a hierarchy"""
    atom_selection="name ca"
    sele=self.hierarchy.apply_atom_selection(atom_selection)
    if sele.overall_counts().n_residues==0:
      self.sites=flex.vec3_double()
    else:
      # extract coordinates
      self.sites=sele.atoms().extract_xyz()

    if sele.overall_counts().n_residues==0:
      self.sites=flex.vec3_double()
    else:
      # extract coordinates
      self.sites=sele.atoms().extract_xyz()

  def get_start_resno(self):
    """Return starting resno in segment"""
    return self.start_resno

  def get_end_resno(self):
    """Return ending resno in segment"""
    return self.start_resno+self.length()-1

  def length(self):
    """Return length of segment"""
    return self.sites.size()

  def optimal_delta_length(self):
    """Return value of self.optimal_delta_length"""
    return self.optimal_delta_length

  def get_norms(self):
    """Report norms of distances between i and average of i+3,i+4."""
    diffs,norms=self.get_diffs_and_norms()
    return self.norms

  def get_rise(self):
    """Report mean distance from i to i+span-1"""
    if not self.span: return 0.
    return self.get_norms().min_max_mean().mean/self.span

  def segment_average_direction(self):
    """Get average direction of this segment"""
    diffs,norms=self.get_diffs_and_norms()
    if not diffs: return 0.
    return get_average_direction(diffs=diffs)

  def get_cosine(self):
    """Report mean normalized dot product of diffs with mean direction"""
    self.mean_dot_single=None
    diffs,norms=self.get_diffs_and_norms()
    if not diffs: return 0.
    average_direction=get_average_direction(diffs=diffs)
    cosines=flex.double()
    for j in range(len(diffs)):
      dot=col(diffs[j]).dot(average_direction)
      cosines.append(dot)
    mean_dot=cosines.min_max_mean().mean

    # now get mean cross product and dot with average_direction...
    diffs_single=self.get_diffs_single()
    cosines=flex.double()
    for j in range(len(diffs_single)):
      dot=col(diffs_single[j]).dot(average_direction)
      cosines.append(dot)
    self.mean_dot_single=cosines.min_max_mean().mean

    return mean_dot

  def get_diffs_single(self):
    """Get diffs between i+1 and i"""
    if self.diffs_single:
      return self.diffs_single
    else:
      sites_offset_1=self.sites[1:]
      self.diffs_single=sites_offset_1-self.sites[:-1]
      norms=self.diffs_single.norms()
      norms.set_selected(norms==0,1.e-10)
      self.diffs_single=self.diffs_single/norms
    return self.diffs_single

  def get_orientation_points(self,start_res=None,end_res=None): # for strand
    """Get 2 points along strand axis and two point at a CA position at end
    These can be used to superimpose strands
    do not depend on the details of the strands"""

    n=self.length()
    if n < self.minimum_length: return None
    if start_res is None: start_res=self.get_start_resno()
    if end_res is None: end_res=start_res+n-1

    if 2*( (end_res-start_res+1)//2)==end_res-start_res+1: # even
      # take 2 residues centered at middle of chain
      i1=(start_res+end_res)//2 #
      center_middle=self.get_centroid(start_res=i1,end_res=i1+1)
    else:  # odd number of residues average just before and after
      im=(start_res+end_res)//2 # middle residue
      cm1=self.get_centroid(start_res=im-1,end_res=im)
      cm2=self.get_centroid(start_res=im,end_res=im+1)
      center_middle=0.5*(matrix.col(cm1)+matrix.col(cm2))

    center_1=self.get_centroid(start_res=start_res,end_res=start_res+1)
    center_2=self.get_centroid(start_res=end_res-1,end_res=end_res)
    ca_1=self.get_site(resno=start_res)
    ca_2=self.get_site(resno=end_res)
    orientation_points=flex.vec3_double((center_1,center_2,ca_1,ca_2,
      center_middle))
    return orientation_points

  def get_targets(self):
    """Set parameters for tolerances"""
    target=self.target_rise*self.span
    tol=self.rise_tolerance
    dot_min=self.dot_min
    target_i_ip3=self.target_i_ip3
    tol_i_ip3=self.tol_i_ip3
    tol=tol*self.span # rise_tolerance * span
    minimum_length=self.minimum_length
    return target,tol,dot_min,minimum_length,target_i_ip3,tol_i_ip3


  def is_ok(self,check_sub_segments=False,sub_segment_length=8):

    """Return True if this segment is ok"""
    ##########################################
    # Option to check sub-segments instead of the whole thing (not used)
    if check_sub_segments and len(self.sites) > sub_segment_length:
      dd=max(1,sub_segment_length//2)
      start=0
      end=len(self.sites)-sub_segment_length
      for i in range(start,end+dd,dd):
        ii=min(i,end)
        h=self.segment_class(params=self.params,
          sites=self.sites[ii:ii+sub_segment_length])
        if not h.is_ok(check_sub_segments=False):
          return False
      return True
    ##########################################

    # check rise and cosine and dot product of single pairs with average
    #  direction and compare to target
    if (self.minimum_length is not None and self.length()<self.minimum_length) \
      or self.length() <1:
      return False
    rise=self.get_rise()
    dot=self.get_cosine()
    dot_single=self.mean_dot_single # set by get_cosine()
    min_diff_i_i3=self.get_min_diff_i_i3()
    if self.dot_min is not None and dot < self.dot_min:
      return False
    elif self.target_rise is not None and self.rise_tolerance is not None and \
       (rise < self.target_rise-self.rise_tolerance or \
           rise > self.target_rise+self.rise_tolerance):
      return False
    elif self.dot_min_single is not None \
       and dot_single < self.dot_min_single:
      return False
    elif self.target_i_ip3 is not None and self.tol_i_ip3 is not None and \
       min_diff_i_i3 < self.target_i_ip3-self.tol_i_ip3:
      return False
    else:
      return True

  def get_lsq_fit(self,other=None,start_res=None,end_res=None):
    """Superimpose other segment (sucha as a helix ) on this one using
     points. start_res,end_res are start end in other to match
    The lengths do not have to match.

    the orientation points for helix are center of
    N-term of helix, center of C-term of
    helix, and CA of first and last residues in the helix.
    """

    self_orientation_points=self.get_orientation_points()
    other_orientation_points=other.get_orientation_points(
      start_res=start_res,end_res=end_res)

    if not self_orientation_points or not other_orientation_points:
      return None # none found

    if self_orientation_points.size()!=other_orientation_points.size():
      return None # did not match size

    if not self_orientation_points or not other_orientation_points:
      return None # could not make a fit

    # now superimpose other on to self...
    lsq_fit_obj=superpose.least_squares_fit(
       reference_sites=self_orientation_points,
       other_sites=other_orientation_points)

    # check it
    new_xyz=flex.vec3_double()
    for x in other_orientation_points:
      new_xyz.append(lsq_fit_obj.r * matrix.col(x) + lsq_fit_obj.t)
    delta=self_orientation_points-new_xyz
    lsq_fit_obj.rms=delta.norms().min_max_mean().mean

    return lsq_fit_obj

  def apply_lsq_fit(self,lsq_fit_obj=None,hierarchy=None,
     start_res=None,end_res=None):
    """Apply lsq_fit obj to a hierarchy"""
    atom_selection="resid %s through %s" %(resseq_encode(start_res),
       resseq_encode(end_res))

    asc=hierarchy.atom_selection_cache()
    sel = asc.selection(string = atom_selection)
    new_ph = hierarchy.select(sel).deep_copy()
    xyz = new_ph.atoms().extract_xyz()
    new_xyz = lsq_fit_obj.r.elems * xyz + lsq_fit_obj.t.elems
    new_ph.atoms().set_xyz(new_xyz)
    new_ph.sort_atoms_in_place()
    new_ph.atoms().reset_i_seq()
    return new_ph

  def get_site(self,resno=None):
    """Get coordinates for site (CA) of resno'th residue starting with 0"""
    first_residue=self.get_start_resno()
    return self.sites[resno-first_residue]

  def get_sites(self,start_res=None,end_res=None):
    """Get sites for residues from start_res to end_res"""
    if start_res is None or end_res is None:
      return self.sites
    else:
      first_residue=self.get_start_resno()
      return self.sites[start_res-first_residue:end_res-first_residue+1]

  def get_centroid(self,start_res=None,end_res=None):
    """Get centroid of residues from start_res to end_res"""
    sites=self.get_sites(start_res=start_res,end_res=end_res)
    if sites.size() < 1:
      return None
    return sites.mean()

class helix(segment):
  """Methods specific to helices"""

  def __init__(self,params=None,sites=None,start_resno=None,hierarchy=None,
     is_n_terminus=None,
     is_c_terminus=None,
     frequency=None,
     base_score=None,
     optimal_delta_length=None,
     verbose=None,
     out=sys.stdout):
    self.params=params
    self.setup(sites=sites,start_resno=start_resno,hierarchy=hierarchy,
     segment_type='helix',
     segment_class=helix,
     minimum_length=params.minimum_length,
     buffer_residues=params.buffer_residues,
     standard_length=params.standard_length,
     frequency=frequency,
     base_score=params.base_score,
     is_n_terminus=is_n_terminus,
     is_c_terminus=is_c_terminus,
     name=params.name,
     span=params.span,
     target_rise=params.rise,
     residues_per_turn=params.residues_per_turn,
     rise_tolerance=params.rise_tolerance,
     target_i_ip3=params.target_i_ip3,
     tol_i_ip3=params.tol_i_ip3,
     dot_min=params.dot_min,
     dot_min_single=params.dot_min_single,
     optimal_delta_length=optimal_delta_length,
     verbose=verbose,
     out=out)

  def get_diffs_and_norms(self):
    """Report distances between i and average of i+3,i+4 (diffs),
      and norms() of these diffs.
     """
    if not hasattr(self,'diffs'):
      sites_offset_3=self.sites[3:-1]
      sites_offset_4=self.sites[4:]
      if self.residues_per_turn<=3:
        average_offset=sites_offset_3
      elif self.residues_per_turn>=4:
        average_offset=sites_offset_4
      else: # alpha helix, take average
        average_offset=0.5*(sites_offset_3+sites_offset_4)
      self.diffs=average_offset-self.sites[:-4]
      self.norms=self.diffs.norms()
      self.norms.set_selected(self.norms<1.e-10,1.e-10)
      self.diffs=self.diffs/self.norms

    return self.diffs,self.norms


  def get_orientation_points(self,start_res=None,end_res=None):
    """Get 3 points along helix axis, two CA at ends
    These can be used to superimpose helices and
    do not depend on the details of the helix"""

    n=self.length()
    if n < self.minimum_length: return None
    if start_res is None: start_res=self.get_start_resno()
    if end_res is None: end_res=start_res+n-1

    if 2*( (end_res-start_res+1)//2)==end_res-start_res+1: # even
      # take 4 residues centered at middle of chain
      i1=(start_res+end_res)//2-1 #
      center_middle=self.get_centroid(start_res=i1,end_res=i1+3)
    else:  # odd number of residues average just before and after
      im=(start_res+end_res)//2 # middle residue
      cm1=self.get_centroid(start_res=im-2,end_res=im+1)
      cm2=self.get_centroid(start_res=im-1,end_res=im+2)
      if (cm1 is not None) and (cm2 is not None):
        center_middle=0.5*(matrix.col(cm1)+matrix.col(cm2))
      else:
        return None # cannot do it
    center_1=self.get_centroid(start_res=start_res,end_res=start_res+3)
    center_2=self.get_centroid(start_res=end_res-3,end_res=end_res)
    ca_1=self.get_site(resno=start_res)
    ca_2=self.get_site(resno=end_res)
    orientation_points=flex.vec3_double(
      (center_1,center_2,ca_1,ca_2,center_middle))
    return orientation_points

class strand(segment):

  """Methods specific to strands"""

  def __init__(self,params=None,sites=None,start_resno=None,hierarchy=None,
     is_n_terminus=None,
     is_c_terminus=None,
     frequency=None,
     base_score=None,
     optimal_delta_length=None,
     verbose=None,
     out=sys.stdout):
    self.params=params

    self.setup(sites=sites,start_resno=start_resno,hierarchy=hierarchy,
      segment_type='strand',
      segment_class=strand,
      name=params.name,
      span=params.span,
      minimum_length=params.minimum_length,
      buffer_residues=params.buffer_residues,
      standard_length=params.standard_length,
      frequency=frequency,
      base_score=params.base_score,
      is_n_terminus=is_n_terminus,
      is_c_terminus=is_c_terminus,
      target_rise=params.rise,
      rise_tolerance=params.rise_tolerance,
      target_i_ip3=params.target_i_ip3,
      tol_i_ip3=params.tol_i_ip3,
      dot_min=params.dot_min,
      dot_min_single=params.dot_min_single,
      optimal_delta_length=optimal_delta_length,
      verbose=verbose,
      out=out)

  def get_diffs_and_norms(self):
    """Report distances between i and i+2"""
    assert self.span==2
    if not hasattr(self,'diffs'):
      sites_offset_2=self.sites[2:]
      self.diffs=sites_offset_2-self.sites[:-2]
      self.norms=self.diffs.norms()
      if self.norms.count(0) > 0:
        self.norms.set_selected(self.norms==0,1.e-10)
      self.diffs=self.diffs/self.norms

    return self.diffs,self.norms

  def get_orientation_points(self,start_res=None,end_res=None): # for strand
    """Get 2 points along strand axis and two point at a CA position at end
    These can be used to superimpose strands
    do not depend on the details of the strands """
    n=self.length()
    if n < self.minimum_length: return None
    if start_res is None: start_res=self.get_start_resno()
    if end_res is None: end_res=start_res+n-1

    if 2*( (end_res-start_res+1)//2)==end_res-start_res+1: # even
      # take 2 residues centered at middle of chain
      i1=(start_res+end_res)//2 #
      center_middle=self.get_centroid(start_res=i1,end_res=i1+1)
    else:  # odd number of residues average just before and after
      im=(start_res+end_res)//2 # middle residue
      cm1=self.get_centroid(start_res=im-1,end_res=im)
      if cm1 is None: return None
      cm2=self.get_centroid(start_res=im,end_res=im+1)
      if cm2 is None: return None
      center_middle=0.5*(matrix.col(cm1)+matrix.col(cm2))

    center_1=self.get_centroid(start_res=start_res,end_res=start_res+1)
    center_2=self.get_centroid(start_res=end_res-1,end_res=end_res)
    ca_1=self.get_site(resno=start_res)
    ca_2=self.get_site(resno=end_res)
    orientation_points=flex.vec3_double((center_1,center_2,ca_1,ca_2,
      center_middle))
    return orientation_points

class other(segment):

  """Methods specific to other (not strand, not helix).
     Looks like strand mostly"""

  def __init__(self,params=None,sites=None,start_resno=None,hierarchy=None,
     is_n_terminus=None,
     is_c_terminus=None,
     frequency=None,
     base_score=None,
     optimal_delta_length=None,
     verbose=None,
     out=sys.stdout):

    self.params=params
    self.setup(sites=sites,start_resno=start_resno,hierarchy=hierarchy,
      segment_type='other',
      segment_class=other,
      name=params.name,
      span=params.span,
      minimum_length=params.minimum_length,
      buffer_residues=params.buffer_residues,
      standard_length=params.standard_length,
      base_score=params.base_score,
      is_n_terminus=is_n_terminus,
      is_c_terminus=is_c_terminus,
      target_rise=params.rise,
      rise_tolerance=params.rise_tolerance,
      target_i_ip3=params.target_i_ip3,
      tol_i_ip3=params.tol_i_ip3,
      dot_min=params.dot_min,
      dot_min_single=params.dot_min_single,
      frequency=frequency,
      optimal_delta_length=optimal_delta_length,
      verbose=verbose,
      out=out)

  def get_diffs_and_norms(self):
    """No diffs and norms for this class"""
    self.diffs=None
    self.norms=None
    return self.diffs,self.norms

  def get_orientation_points(self,start_res=None,end_res=None): # for other
    """Just use all points for orientation for this class"""
    if not self.orientation_points or \
        not start_res==self.orientation_points_start or \
        not end_res==self.orientation_points_end: # calculate it
      n=self.length()
      if n < self.minimum_length:
         self.orientation_points=None
      else:
        first_residue=self.get_start_resno()
        self.orientation_points=flex.vec3_double(self.get_sites(
          start_res=start_res,end_res=end_res))
        self.orientation_points_start=start_res
        self.orientation_points_end=end_res
    return self.orientation_points

class find_segment:
  """Look for a type of segment (e.g., helices or strands) in a chain"""

  def setup(self,params=None,model=None,segment_type='helix',
      extract_segments_from_pdb=None,
      make_unique=None,
      cut_up_segments=None,
      extend_segments=None,
      model_as_segment=None, # take the whole model as a segment
      verbose=None,
      out=sys.stdout):

    """ Set up to find a type of segment
    Assumes model is just 1 chain of sequential residues
    obtained with split_model"""

    self.out=out
    self.extract_segments_from_pdb=extract_segments_from_pdb
    self.make_unique=make_unique
    self.cut_up_segments=cut_up_segments
    self.extend_segments=extend_segments
    self.model_as_segment=model_as_segment
    self.verbose=verbose
    self.params=params
    self.model=model
    self.name=params.name
    self.n_link_min=params.n_link_min

    self.segment_type=segment_type
    if self.segment_type=='helix':
      self.segment_class=helix
      self.allow_insertions=params.allow_insertions
      self.allow_deletions=params.allow_deletions
    elif self.segment_type=='strand':
      self.segment_class=strand
      self.allow_insertions=params.allow_insertions
      self.allow_deletions=params.allow_deletions
    elif self.segment_type=='other':
      self.segment_class=other
      self.allow_insertions=params.allow_insertions
      self.allow_deletions=params.allow_deletions

    h=self.segment_class(params=params) # contains numbers we need
    self.target_rise=h.target_rise
    self.span=h.span
    self.buffer_residues=h.buffer_residues
    self.standard_length=h.standard_length

    # number of residues in segment compared to number of diffs
    # (4 more than diffs for helix; 2 more for strand)
    if self.span is not None:
      self.last_residue_offset=int(self.span+0.99)
    else:
      self.last_residue_offset=0

    # set start residue number if not set.
    self.start_resno=self.model.hierarchy.first_resseq_as_int()

    self.segments=[]

    # select ca atoms
    sites=self.get_sites() # just a list of sites (CA atoms for entire chain)
    if not sites:
      return # nothing to do

    # get list of difference vectors i
    #  Then get list of segments in segment_dict
    diffs,norms,segment_dict=self.find_segments(params=params,sites=sites)

    # NOTE: separate find_segments method for each kind of segment
    # figure out how many residues really should be in these segments
    optimal_delta_length_dict,norm_dict=self.get_optimal_lengths(
       segment_dict=segment_dict,norms=norms)

    # create segment object (helix,strand) for each segment
    # If longer than the standard length, make a series of shorter segments

    keys=list(segment_dict.keys())
    keys.sort()
    # Specify which residues are in each segment
    #    (self.last_residue_offset past start)
    # and offset to start with first residue number

    # Possibly cut into pieces of length self.standard_length plus
    #   buffer_residues on each end for total length of
    #   standard_length+2*buffer_residues

    # If number of residues is to be changed, use just full length of the
    # segment (do not cut) and do not add buffer residues

    # NOTE: buffered on end with n_buffer residues unless
    #   at very ends of chain in which case set is_n_terminus or is_c_terminus

    start_end_list=[]
    # NOTE: self.segment_class is a substitution for helix/strand classes
    for i in keys:
      # this segment starts at i and ends at segment_dict[i]
      overall_start_res=i
      overall_end_res=segment_dict[i]+self.last_residue_offset
      overall_length=overall_end_res-overall_start_res+1
      optimal_delta_length=optimal_delta_length_dict.get(overall_start_res, 0)
      if (optimal_delta_length > 0 and not self.allow_insertions) or \
         (optimal_delta_length < 0 and not self.allow_deletions):
        optimal_delta_length=0
      # add to self.segments
      # cut into pieces of size self.standard_length
      if (not self.cut_up_segments) or overall_length<=self.standard_length:
        start_end_list.append([overall_start_res,overall_end_res])
      else:
        for start_res_use in range(
           overall_start_res,
           overall_start_res+overall_length-self.standard_length+1):
          start_end_list.append(
             [start_res_use,start_res_use+self.standard_length-1])
    self.segment_start_end_dict={}
    for start_res_use,end_res_use in start_end_list:
        self.segment_start_end_dict[start_res_use]=end_res_use
        ok=self.extract_segment(params=params,
         start_res=start_res_use,end_res=end_res_use,sites=sites,
         optimal_delta_length=optimal_delta_length)
        if not ok:
          del self.segment_start_end_dict[start_res_use]

  def show_summary(self,out=None):
    """Summarize this find_segment object"""
    if not out: out=self.out
    for h in self.segments:
      print("Class: %12s  N: %d Start: %d End: %d " %(
          self.name,h.get_end_resno()-h.get_start_resno()+1,
           h.get_start_resno(),h.get_end_resno(),
         ) +" Rise:%5.2f A Dot:%5.2f" %(
          h.get_rise(),h.get_cosine()), file=out)

  def get_used_residues_list(self,end_buffer=1):
    """Just return a list of used residues in this find_segment object"""
    used_residues=[]
    if hasattr(self,'segment_dict'):
      for i in self.segment_start_end_dict.keys():
        for j in range(
             i+end_buffer,self.segment_start_end_dict[i]+1-end_buffer):
          # Note this segment_start_end_dict lists exact residues used,
          #  not starting points as in segment_dict
          if not j in used_residues:
            used_residues.append(j)
    return used_residues

  def extract_segment(self,params=None,start_res=None,end_res=None,sites=None,
       optimal_delta_length=None):
      """Extract coordinates of atoms and secondary structure
         information for a segment of secondary structure"""
      start_res_with_buffer=start_res  # -self.buffer_residues
      end_res_with_buffer=end_res   #  +self.buffer_residues

      if start_res_with_buffer<0: # at the n-terminus
        start_res_with_buffer=start_res
        is_n_terminus=True
      else:
        is_n_terminus=False
      if end_res_with_buffer>len(sites)-1: # at c-term
        end_res_with_buffer=end_res
        is_c_terminus=True
      else:
        is_c_terminus=False

      if len(sites) < end_res: # make sure we are in bounds
        return False

      # get the hierarchy if necessary
      if self.extract_segments_from_pdb or self.model_as_segment:
        start_resno=start_res_with_buffer+self.start_resno
        end_resno=end_res_with_buffer+self.start_resno
        # XXX NOTE: we assume that the model has been broken up so that there
        # is at most one residue with each residue number (broken up at
        # insertion codes).  If this changes this will not work properly.
        atom_selection="resseq %s:%s" %(
           resseq_encode(start_resno).replace(" ",""),
           resseq_encode(end_resno).replace(" ",""))
        hierarchy=self.model.hierarchy.apply_atom_selection(atom_selection)
        if hierarchy.overall_counts().n_residues==0:
          return False # did not find anything here and needed to
      else:
        hierarchy=None

      h=self.segment_class(params=params,
        sites=sites[start_res_with_buffer:end_res_with_buffer+1],
        start_resno=start_res_with_buffer+self.start_resno,
        optimal_delta_length=optimal_delta_length,hierarchy=hierarchy,
        is_n_terminus=is_n_terminus,
        is_c_terminus=is_c_terminus,
          )
      if h.is_ok() or self.model_as_segment:
        self.segments.append(h)
        return True
      else:
        return False

  def get_sites(self):
    """Get CA sites from hierarchy"""
    atom_selection="name ca"
    sele=self.model.hierarchy.apply_atom_selection(atom_selection)
    if not sele.overall_counts().n_residues:
      return []
    else:
      # extract coordinates
      sites=sele.extract_xray_structure(min_distance_sym_equiv=0 # REQUIRED
         ).sites_cart()
      return sites

  def get_segments(self):
    """Return segments of secondary structure in this find_segments object"""
    return self.segments

  def find_segments(self,params=None,sites=None): # helix/strand
    """Set up segment class for this kind of segment
    for example, helix(sites=sites)"""

    h=self.segment_class(params=params,sites=sites)

    # get difference vectors i to i+2 (strands) or i to avg of i+3/i+4 (helix)
    diffs,norms=h.get_diffs_and_norms()  # difference vectors and lengths
    norms_3=h.get_diffs_and_norms_3()

    target,tol,dot_min,minimum_length,target_i_ip3,tol_i_ip3=h.get_targets()

    # now find sequence of N residues where diffs are parallel and values of
    # diffs are all about the same and about span*rise = 5.4 A for helix

    n=len(diffs)
    segment_dict={}
    used_residues=[]

    if len(sites)>0 and self.model_as_segment:  # take the whole thing
      segment_dict[0]=n-1
      return diffs,norms,segment_dict

    for i in range(n):
      if i in used_residues or abs(norms[i]-target)>tol: continue
      if i in self.previously_used_residues: continue
      # i is start of segment
      segment_dict[i]=i  # lists end of segment
      used_residues.append(i)
      for j in range(i+1,n):
        if abs(norms[j]-target)>tol: break
        if target_i_ip3 is not None and tol_i_ip3 is not None and \
           j<len(norms_3) and abs(norms_3[j]-target_i_ip3)>tol_i_ip3: break
        if j in self.previously_used_residues: break
        dot=col(diffs[j]).dot(col(diffs[j-1]))
        if dot < dot_min: break
        segment_dict[i]=j
        used_residues.append(j)

    # skip if only as long as a single span (requires 2 to be sure)
    for i in list(segment_dict.keys()):
      segment_length=segment_dict[i]+1+self.last_residue_offset-i
      if segment_length<minimum_length:
        del segment_dict[i]

    # prune out anything not really in this segment type based on direction

    segment_dict=self.remove_bad_residues(segment_dict=segment_dict,
      diffs=diffs,dot_min=dot_min,minimum_length=minimum_length)

    # merge any segments that can be combined
    segment_dict=self.merge_segments(params=params,
       segment_dict=segment_dict,sites=sites)

    # try to extend by 1 in each direction if it doesn't overlap anything
    if self.extend_segments:
      segment_dict=self.try_to_extend_segments(params=params,
        segment_dict=segment_dict,sites=sites)

    # trim ends of any segments that are connected by fewer than n_link_min=3
    #  residues and that go in opposite directions (connected by tight turns).

    segment_dict=self.trim_short_linkages(
     params=params,segment_dict=segment_dict,sites=sites)

    # again merge any segments that can be combined
    segment_dict=self.merge_segments(params=params,
       segment_dict=segment_dict,sites=sites)

    return diffs,norms,segment_dict


  def try_to_extend_segments(self,params=None,segment_dict=None,sites=None):
    """Try to extend segments if they do not overlap"""
    found=True
    n_cycles=0
    while found and n_cycles <=len(segment_dict):
      found=False
      n_cycles+=1
      keys=list(segment_dict.keys())
      keys.sort()  # sorted on start
      n_space=2  # at least 2 apart in the end
      for i1,i2,i3 in zip([None]+keys[:-2],keys,keys[1:]+[None]):
        if found: break
        if i2 > 0 and ( i1 is None or
            i2 > segment_dict[i1]+self.last_residue_offset+1+n_space):
          # try adding before i2
          h=self.segment_class(params=params,
             sites=sites[i2-1:segment_dict[i2]+self.last_residue_offset+1],
             start_resno=i2-1+self.start_resno)
          if h.is_ok():
           found=True
           segment_dict[i2-1]=segment_dict[i2]
           del segment_dict[i2]
        if found: break
        if segment_dict[i2]+self.last_residue_offset+1+1 < sites.size()  and (
           i3 is None or
            segment_dict[i2]+self.last_residue_offset+1+n_space  < i3):
          # try adding after i2
          h=self.segment_class(params=params,
             sites=sites[i2:segment_dict[i2]+self.last_residue_offset+1+1],
             start_resno=i2+self.start_resno)
          if h.is_ok():
           found=True
           segment_dict[i2]=segment_dict[i2]+1

    return segment_dict


  def merge_segments(self,params=None,segment_dict=None,sites=None):

    """Merge any segments that can be combined"""

    found=True
    n_cycles=0
    while found and n_cycles <=len(segment_dict):
      found=False
      n_cycles+=1
      keys=list(segment_dict.keys())
      keys.sort()  # sorted on start
      for i1,i2 in zip(keys[:-1],keys[1:]):
        if found: break
        if i2 <= segment_dict[i1]+self.last_residue_offset+1:
          # could merge. Test it
          h=self.segment_class(params=params,
             sites=sites[i1:segment_dict[i2]+self.last_residue_offset+1],
             start_resno=i1+self.start_resno)
          if h.is_ok():
           found=True
           segment_dict[i1]=segment_dict[i2]
           if i1 != i2:
             del segment_dict[i2]
    return segment_dict

  def remove_bad_residues(self,segment_dict=None,diffs=None,dot_min=None,
    minimum_length=None):
    """Remove bad residues from segments"""
    still_changing=True
    n_cycles=0
    max_cycles=2
    while still_changing and n_cycles < max_cycles:
      still_changing=False
      new_segment_dict={}
      keys=list(segment_dict.keys())
      keys.sort()
      for i in keys:
        average_direction=get_average_direction(
          diffs=diffs,i=i,j=segment_dict[i])
        segment_start=None
        for j in range(i,segment_dict[i]+1):
          dot=col(diffs[j]).dot(average_direction)
          if dot >= dot_min:
            if segment_start is None:
              segment_start=j
            new_segment_dict[segment_start]=j
          else:
            segment_start=None
            still_changing=True
      segment_dict=new_segment_dict
      for i in list(segment_dict.keys()):
        segment_length=segment_dict[i]+1+self.last_residue_offset-i
        if segment_length<minimum_length:
          del segment_dict[i] # not long enough
      n_cycles+=1
    return segment_dict

  def trim_short_linkages(self,params=None,segment_dict=None,sites=None):
    """Trim ends of any segments that are connected by fewer than n_link_min=3
    residues and that go in opposite directions (connected by tight turns)."""

    found=True
    n_cycles=0
    while found and n_cycles <=len(segment_dict):
      found=False
      n_cycles+=1
      keys=list(segment_dict.keys())
      keys.sort()  # sorted on start
      for i1,i2 in zip(keys[:-1],keys[1:]):
        if found: break
        end_1=segment_dict[i1]+self.last_residue_offset
        start_2=i2
        delta=i2-(segment_dict[i1]+self.last_residue_offset)
        if delta >=self.n_link_min: continue
        h1=self.segment_class(params=params,
          sites=sites[i1:segment_dict[i1]+self.last_residue_offset+1],
             start_resno=i1+self.start_resno)
        h2=self.segment_class(params=params,
          sites=sites[i2:segment_dict[i2]+self.last_residue_offset+1],
             start_resno=i2+self.start_resno)
        if h1.segment_average_direction().dot(h2.segment_average_direction())>0:
          continue # (in generally the same direction...ignore)
        # in opposite direction
        residues_to_cut=(delta+1)//2
        found=True
        segment_dict[i1]=segment_dict[i1]-residues_to_cut
        ok=False
        if segment_dict[i1]>=i1: # check and keep
          h1=self.segment_class(params=params,
            sites=sites[i1:segment_dict[i1]+self.last_residue_offset+1],
             start_resno=i1+self.start_resno)
          ok=h1.is_ok()
        if not ok:
            del segment_dict[i1]

        ok=False
        new_i2=i2+residues_to_cut
        if segment_dict[i2]>=new_i2: # check and keep
          h2=self.segment_class(params=params,
            sites=sites[new_i2:segment_dict[i2]+self.last_residue_offset+1],
               start_resno=new_i2+self.start_resno)
          ok=h2.is_ok()
        if ok:
          segment_dict[new_i2]=segment_dict[i2]
        # remove original
        if new_i2 != i2:
          del segment_dict[i2]

    return segment_dict


  def get_optimal_lengths(self,segment_dict=None,norms=None):
    """Figure out how many residues really should be in these segments"""

    keys=list(segment_dict.keys())
    keys.sort()

    # get average distance i to i+3/i+4 for this segment (to see if it is
    #   stretched out as can happen at lowres)
    # or for strand, distance i to i+2

    optimal_delta_length_dict={}
    norm_dict={}
    standard_rise=self.target_rise # 5.5 A for 3.5 residues
    for i in keys:
      norms_used=norms[i:segment_dict[i]+1]
      mean_norm=norms_used.min_max_mean().mean
      if mean_norm is None: # give up
        return optimal_delta_length_dict,norm_dict
      # guess number of residues it should be: standard rise
      segment_rise=mean_norm/self.span  # we used mean of i+3 and i+4 for helix
      segment_length=segment_dict[i]+1+self.last_residue_offset-i
      segment_distance=segment_rise*(segment_length-1)
      optimal_length=int(0.5+segment_distance/standard_rise)+1
      optimal_delta_length_dict[i]=optimal_length-segment_length
      norm_dict[i]=mean_norm
      if self.verbose:
        print("Chain start: %d  end: %d  N: %d  Rise: %7.2f  Optimal length: %d" %(
         i+self.start_resno,i+segment_length-1+self.start_resno,
         segment_length,segment_rise,optimal_length), file=self.out)

    return optimal_delta_length_dict,norm_dict

class find_helix(find_segment):
  """Look for helices in a chain"""
  def __init__(self,params=None,model=None,verbose=None,
    extract_segments_from_pdb=None,
    make_unique=None,
    previously_used_residues=None,
    model_as_segment=None,
    cut_up_segments=None,
    extend_segments=None,
    out=sys.stdout):
    self.previously_used_residues=previously_used_residues

    self.setup(params=params,model=model,segment_type='helix',
     extract_segments_from_pdb=extract_segments_from_pdb,
     make_unique=make_unique,
     model_as_segment=model_as_segment,
     cut_up_segments=cut_up_segments,
     extend_segments=extend_segments,
     verbose=verbose,out=out)

  def pdb_records(self,segment_list=None,last_id=0,helix_type='alpha',
     max_h_bond_length=None,
     force_secondary_structure_input=None,
     require_h_bonds=None,
     minimum_h_bonds=None,
     maximum_poor_h_bonds=None,
     allow_ca_only_model=None,out=sys.stdout): # helix
    """Construct pdb records for segments in this find_helix object"""
    records=[]
    number_of_good_h_bonds=0
    number_of_poor_h_bonds=0
    k=last_id
    for s in segment_list:
      if not s.hierarchy: continue
      f=StringIO()
      all_h_bonds,n_good,n_poor=self.list_h_bonds(
          segment=s,helix_type=helix_type,
          max_h_bond_length=max_h_bond_length,
          force_secondary_structure_input=force_secondary_structure_input,
          allow_ca_only_model=allow_ca_only_model,out=f)
      if require_h_bonds:
        if n_good<minimum_h_bonds or (
            maximum_poor_h_bonds is not None and n_poor>maximum_poor_h_bonds):
          continue

      print(f.getvalue(), end=' ', file=out)
      number_of_good_h_bonds+=n_good
      number_of_poor_h_bonds+=n_poor
      start=get_first_residue(s.hierarchy)
      end=get_last_residue(s.hierarchy)
      chain_id=s.hierarchy.first_chain_id()
      k=k+1
      record = secondary_structure.pdb_helix(
        serial=k,
        helix_id=k,
        start_resname=start.resname,
        start_chain_id=chain_id,
        start_resseq=start.resseq,
        start_icode=start.icode,
        end_resname=end.resname,
        end_chain_id=chain_id,
        end_resseq=end.resseq,
        end_icode=end.icode,
        helix_class=secondary_structure.pdb_helix.helix_class_to_int(
           helix_type), # 1=alpha 3=pi  5=3_10
        comment="",
        length=s.length())
      records.append(record)
    return records,number_of_good_h_bonds,number_of_poor_h_bonds

  def list_h_bonds(self,segment=None,helix_type='alpha',
     max_h_bond_length=None,
     force_secondary_structure_input=None,
     allow_ca_only_model=None,out=sys.stdout):

    """List H-bonds in this find_helix object"""
    helix_class=secondary_structure.pdb_helix.helix_class_to_int(
           helix_type) # 1=alpha 3=pi  5=3_10

    # residue that residue i is h-bonded to
    next_i_dict={
      1:4,   # alpha:  O of residue i H-bonds to N of residue i+4
      3:5,   # pi:     O of residue i H-bonds to N of residue i+5
      5:3,   # 3_10:   O of residue i H-bonds to N of residue i+3
     }
    next_i=next_i_dict[helix_class]

    all_h_bonds=[]
    number_of_good_h_bonds=0
    number_of_poor_h_bonds=0

    for i in range(segment.length()-next_i):
      cur_residue=get_indexed_residue(
        segment.hierarchy,index=i)
      cur_atom,cur_xyz=get_atom_from_residue(
        residue=cur_residue,
        atom_name=' O  ',allow_ca_only_model=allow_ca_only_model)

      next_residue=get_indexed_residue(
        segment.hierarchy,index=i+next_i)
      next_atom,next_xyz=get_atom_from_residue(
        residue=next_residue,
        atom_name=' N  ',allow_ca_only_model=allow_ca_only_model,
        skip_n_for_pro=True)

      if cur_xyz and next_xyz:  # both present at least
        dd=col(cur_xyz)-col(next_xyz)
        dist=dd.length()
        if dist <=max_h_bond_length:
          bad_one=""
          number_of_good_h_bonds+=1
        else:
          bad_one="**"
          number_of_poor_h_bonds+=1

        new_h_bond=h_bond(
           prev_atom=cur_atom,
           prev_resname=cur_residue.resname,
           prev_chain_id=segment.hierarchy.first_chain_id(),
           prev_resseq=cur_residue.resseq,
           prev_icode=cur_residue.icode,
           cur_atom=next_atom,
           cur_resname=next_residue.resname,
           cur_chain_id=segment.hierarchy.first_chain_id(),
           cur_resseq=next_residue.resseq,
           cur_icode=next_residue.icode,
           dist=dist,
           bad_one=bad_one,
           anything_is_ok=force_secondary_structure_input,
         )
        new_h_bond.show_summary(out=out,show_non_existent=False)
        all_h_bonds.append(new_h_bond)
      else:
        bad_one=None
        dist=None
    return all_h_bonds,number_of_good_h_bonds,number_of_poor_h_bonds

class find_beta_strand(find_segment):
  """Look for beta_strands in a chain"""

  def __init__(self,params=None,model=None,verbose=None,
      extract_segments_from_pdb=None,
      make_unique=None,
      cut_up_segments=None,
      extend_segments=None,
      model_as_segment=None,
      previously_used_residues=None,
      out=sys.stdout):

    self.previously_used_residues=previously_used_residues

    self.setup(params=params,
      model=model,segment_type='strand',
      extract_segments_from_pdb=extract_segments_from_pdb,
      make_unique=make_unique,
      model_as_segment=model_as_segment,
      cut_up_segments=cut_up_segments,
      extend_segments=extend_segments,
      verbose=verbose,out=out)

  def get_pdb_strand(self,sheet_id=None,strand_id=1,segment=None,
     sense=0,start_index=None,end_index=None):
    """Return a secondary_structure.pdb_strand object for strand_id"""
    if start_index is None:
      start=get_first_residue(segment.hierarchy)
    else:
      start=get_indexed_residue(segment.hierarchy,index=start_index)
    if end_index is None:
      end=get_last_residue(segment.hierarchy)
    else:
      end=get_indexed_residue(segment.hierarchy,index=end_index)
    if start is None or end is None:
      return None

    chain_id=segment.hierarchy.first_chain_id()
    pdb_strand = secondary_structure.pdb_strand(
        sheet_id=sheet_id,
        strand_id=strand_id,
        start_resname=start.resname,
        start_chain_id=chain_id,
        start_resseq=start.resseq,
        start_icode=start.icode,
        end_resname=end.resname,
        end_chain_id=chain_id,
        end_resseq=end.resseq,
        end_icode=end.icode,
        sense=sense)
    return pdb_strand

  def get_required_start_end(self,sheet=None,info_dict=None):
      """Return start_dict and end_dict for this sheet"""
      start_dict={}
      end_dict={}
      for i in sheet:
        start_dict[i]=None
        end_dict[i]=None
      for i,j in zip(sheet[:-1],sheet[1:]):
        key="%d:%d" %(i,j)
        first_last_1_and_2=info_dict[key]
        first_ca_1,last_ca_1,first_ca_2,last_ca_2,is_parallel,i_index,j_index=\
           first_last_1_and_2

        if start_dict[i] is None or start_dict[i]>first_ca_1:
           start_dict[i]=first_ca_1
        if end_dict[i] is None or end_dict[i]<last_ca_1:
           end_dict[i]=last_ca_1

        if start_dict[j] is None or start_dict[j]>first_ca_2:
           start_dict[j]=first_ca_2
        if end_dict[j] is None or end_dict[j]<last_ca_2:
           end_dict[j]=last_ca_2

      return start_dict,end_dict

  def pdb_records(self,segment_list=None,   # sheet
     sheet_list=None,info_dict=None,allow_ca_only_model=None,
     force_secondary_structure_input=None,
     max_h_bond_length=None,
     require_h_bonds=None,
     minimum_h_bonds=None,
     maximum_poor_h_bonds=None,
     out=sys.stdout):

    """Construct pdb records for segments in this find_sheet object.
    sheet_list is list of sheets. Each sheet is a list of strands (the index
    of the strand in segment_list). Info_dict has the relationship between
    pairs of strands, indexed with the key "%d:%d:" %(i,j) where i and j are
    the indices of the two strands. The dictionary returns
    [first_ca_1,last_ca_1,first_ca_2,last_ca_2,is_parallel,i_index,j_index]
    for the two strands"""

    records=[]
    number_of_good_h_bonds=0
    number_of_poor_h_bonds=0
    sheet_id=0
    for sheet in sheet_list:
      good_in_sheet=0
      poor_in_sheet=0
      f=StringIO()

      sheet_id+=1
      strand_id=1
      k=sheet[0]
      remainder=sheet[1:] # all the others
      s=segment_list[k]
      if not s.hierarchy or not s.hierarchy.overall_counts().n_residues:
        continue
      current_sheet = secondary_structure.pdb_sheet(
        sheet_id=sheet_id,
        n_strands=len(sheet),
        strands=[],
        registrations=[])
      ok=True

      # figure out what residues to include in each sheet. It is not a well-
      #   defined problem because a middle strand might H-bond to one but not
      #   both of its neighbors even if both neighbors are beta-strands
      start_dict,end_dict=self.get_required_start_end(sheet=sheet,
          info_dict=info_dict)

      first_strand = self.get_pdb_strand(sheet_id=sheet_id,strand_id=strand_id,
        segment=s,sense=0,start_index=start_dict[k],end_index=end_dict[k])
      if first_strand is None:
        print("Note: failed to identify strand %s in sheet %s index %s" %(
          k,strand_id,sheet_id), file=out)
        ok=False
        continue # found nothing (something went wrong in get_pdb_strand)

      current_sheet.add_strand(first_strand)
      current_sheet.add_registration(None)
      previous_s=s
      i=k
      for j in remainder: # previous strand is i, current is j
        if not ok: break
        s=segment_list[j]
        strand_id+=1
        if not s.hierarchy or not s.hierarchy.overall_counts().n_residues:
          continue

        key="%d:%d" %(i,j)
        first_last_1_and_2=info_dict[key]
        first_ca_1,last_ca_1,first_ca_2,last_ca_2,is_parallel,i_index,j_index=\
           first_last_1_and_2
        # sense is whether previous and current strands are parallel (1) or
        #   antiparallel (-1)

        if is_parallel:
          sense=1
        else:
          sense=-1
        next_strand = self.get_pdb_strand(sheet_id=sheet_id,strand_id=strand_id,
          segment=s,sense=sense,start_index=start_dict[j],end_index=end_dict[j])
        if next_strand is None:
          print("Note: failed to "+\
            "identify strand %s in sheet %s index %s" %(j,strand_id,sheet_id), file=out)
          ok=False
          continue# found nothing (something went wrong in get_pdb_strand)

        current_sheet.add_strand(next_strand)
        all_h_bonds,n_good,n_poor=self.list_h_bonds(
          segment=s,
          max_h_bond_length=max_h_bond_length,
          previous_segment=previous_s,first_last_1_and_2=first_last_1_and_2,
          force_secondary_structure_input=force_secondary_structure_input,
          allow_ca_only_model=allow_ca_only_model,out=f)
        good_in_sheet+=n_good
        poor_in_sheet+=n_poor
        register=self.get_pdb_strand_register(segment=s,
          previous_segment=previous_s,first_last_1_and_2=first_last_1_and_2,
          allow_ca_only_model=allow_ca_only_model,
          all_h_bonds=all_h_bonds)
        current_sheet.add_registration(register)
        previous_s=s
        i=j

      if not ok: continue # skip

      if require_h_bonds:
        if good_in_sheet<minimum_h_bonds or (
            maximum_poor_h_bonds is not None and n_poor>maximum_poor_h_bonds):
          sheet_id-=1
          continue
      print(f.getvalue(), end=' ', file=out)
      number_of_good_h_bonds+=good_in_sheet
      number_of_poor_h_bonds+=poor_in_sheet
      records.append(current_sheet)


    return records,number_of_good_h_bonds,number_of_poor_h_bonds


  def is_even(self,i):
    """Return True if i is even"""
    if 2*(i//2)==i: return True
    return False

  def get_pdb_strand_register(self,segment=None,previous_segment=None,
     first_last_1_and_2=None,allow_ca_only_model=None,
     all_h_bonds=None):
    """Return a pdb_strand_register object for the first H-bond that is OK"""
    for h_bond in all_h_bonds: # choose first that is ok
      if not h_bond.is_ok():
        continue

      from iotbx.pdb.secondary_structure import pdb_strand_register
      register=pdb_strand_register(
             cur_atom=h_bond.cur_atom,
             cur_resname=h_bond.cur_resname,
             cur_chain_id=h_bond.cur_chain_id,
             cur_resseq=h_bond.cur_resseq,
             cur_icode=h_bond.cur_icode,
             prev_atom=h_bond.prev_atom,
             prev_resname=h_bond.prev_resname,
             prev_chain_id=h_bond.prev_chain_id,
             prev_resseq=h_bond.prev_resseq,
             prev_icode=h_bond.prev_icode,)
      return register
      # just take the first good one

  def list_h_bonds(self,segment=None,previous_segment=None,
     max_h_bond_length=None,
     force_secondary_structure_input=None,
     first_last_1_and_2=None,allow_ca_only_model=None,out=sys.stdout):

    """List H-bonds in this find_beta_trand object"""
    #  Looking down a strand in direction from N to C...
    #    the CA go up-down-up-down.
    #    The ones that are up have their O pointing to the right
    #    Those that are down have O pointing to the left
    #  So...if we orient strand n+1 from N to C...if
    #    strand n is to the right then choose an "up" residue of strand n+1 for
    #    the matching to strand n.  If strand n is to the left choose a "down"
    #    one.

    #  If CA residue i of strand n matches with residue i' of strand n+1:

    #  For antiparallel strands:
    #  O of residue i in strand n H-bonds to N of residue i' in strand n+1

    #  For parallel strands:
    #  O of residue i in strand n H-bonds to N of residue i'+1 in
    #   strand n+1.

    # Here strand n is previous_segment and n+1 is segment

    # Get entire list of H-bonded residues between these segments.
    # Residues in previous_segment go from first_ca_1 to last_ca_1.
    # We have already specified that i_index of previous_segment
    #   atom O H-bonds to j_index of segment atom N.
    # For antiparallel strands, other H-bond is N of i_index with O of j_index
    # For parallel strands, other H-bond is N of i_index with O of j_index-2

    # Increase i_index by 2 and decrease j_index
    #  by 2 and the same pattern occurs.

    # look at entire segments, not just the part we are including

    all_h_bonds=[]
    number_of_poor_h_bonds=0
    number_of_good_h_bonds=0

    first_ca_1,last_ca_1,first_ca_2,last_ca_2,is_parallel,i_index,j_index=\
           first_last_1_and_2
    for i in range(previous_segment.length()):
      if i_index is None or j_index is None: continue
      if not self.is_even(i-i_index): continue
      local_i_index=i_index+(i-i_index)
      if is_parallel:
        local_j_index=j_index+(i-i_index)
      else:
        local_j_index=j_index-(i-i_index)

      # make sure we are in range
      if local_i_index<0 or local_i_index>previous_segment.length()-1: continue

      local_prev_residue=get_indexed_residue(
        previous_segment.hierarchy,index=local_i_index)
      for o_to_n in [True,False]:
        if o_to_n:
          if local_j_index<0 or local_j_index>segment.length()-1: continue

          local_cur_residue=get_indexed_residue(
            segment.hierarchy,index=local_j_index)
          local_prev_atom,local_prev_xyz=get_atom_from_residue(
            residue=local_prev_residue,
            atom_name=' O  ',allow_ca_only_model=allow_ca_only_model)
          local_cur_atom,local_cur_xyz=get_atom_from_residue(
            residue=local_cur_residue,
            atom_name=' N  ',allow_ca_only_model=allow_ca_only_model,
            skip_n_for_pro=True)
        else:
          if is_parallel:
            if local_j_index-2<0 or local_j_index-2>segment.length()-1: continue
            local_cur_residue=get_indexed_residue(
              segment.hierarchy,index=local_j_index-2)
          else:
            if local_j_index<0 or local_j_index> segment.length()-1: continue
            local_cur_residue=get_indexed_residue(
              segment.hierarchy,index=local_j_index)
          local_prev_atom,local_prev_xyz=get_atom_from_residue(
            residue=local_prev_residue,
            atom_name=' N  ',allow_ca_only_model=allow_ca_only_model,
            skip_n_for_pro=True)
          local_cur_atom,local_cur_xyz=get_atom_from_residue(
            residue=local_cur_residue,
            atom_name=' O  ',allow_ca_only_model=allow_ca_only_model)

        # skip if there is no atom pair (e.g., if N and PRO )
        if not local_prev_atom or not local_cur_atom: continue

        if local_cur_xyz and local_prev_xyz:
          dd=col(local_cur_xyz)-col(local_prev_xyz)
          dist=dd.length()
          if dist <=max_h_bond_length:
            bad_one=""
          else:
            bad_one="**"
        else:
          bad_one=None
          dist=None

        # Save those that are outside range we are keeping only if good:
        if local_j_index<first_ca_2 or local_j_index>last_ca_2 or \
           local_i_index<first_ca_1 or local_i_index>last_ca_1:
          if bad_one!="": continue
          # and mark as not included
          bad_one="(Not included) "

        if bad_one=="":
          number_of_good_h_bonds+=1
        elif bad_one=="**":
          number_of_poor_h_bonds+=1

        new_h_bond=h_bond(
             prev_atom=local_prev_atom,
             prev_resname=local_prev_residue.resname,
             prev_chain_id=previous_segment.hierarchy.first_chain_id(),
             prev_resseq=local_prev_residue.resseq,
             prev_icode=local_prev_residue.icode,
             cur_atom=local_cur_atom,
             cur_resname=local_cur_residue.resname,
             cur_chain_id=segment.hierarchy.first_chain_id(),
             cur_resseq=local_cur_residue.resseq,
             cur_icode=local_cur_residue.icode,
             dist=dist,
             bad_one=bad_one,
             anything_is_ok=force_secondary_structure_input,
         )
        new_h_bond.show_summary(out=out,show_non_existent=False)
        all_h_bonds.append(new_h_bond)
    return all_h_bonds,number_of_good_h_bonds,number_of_poor_h_bonds

class h_bond:
  """Holder for a pair of atoms involved in an H-bond"""
  def __init__(self,
             prev_atom=None,
             prev_resname=None,
             prev_chain_id=None,
             prev_resseq=None,
             prev_icode=None,
             cur_atom=None,
             cur_resname=None,
             cur_chain_id=None,
             cur_resseq=None,
             cur_icode=None,
             dist=None,
             bad_one=None,
             anything_is_ok=False):
    adopt_init_args(self, locals())

  def is_ok(self):
    """Return True if this object is ok"""
    if self.anything_is_ok:
      return True
    if self.dist is None:  # was CA-only so no information
       return True
    elif self.dist and not self.bad_one: # was ok H-bond
       return True
    else:
      return False

  def show_summary(self,show_non_existent=False,out=sys.stdout):
    """Summarize this h_bond object"""
    if self.dist is not None:
      print(" %4s%4s%4s%5s%s : %4s%4s%4s%5s%s :: %5.2f   %s" %(
             self.prev_atom,
             self.prev_resname,
             self.prev_chain_id,
             self.prev_resseq,
             self.prev_icode,
             self.cur_atom,
             self.cur_resname,
             self.cur_chain_id,
             self.cur_resseq,
             self.cur_icode,
             self.dist,
             self.bad_one), file=out)
    elif self.bad_one is not None:
      print(" %4s%4s%4s%5s%s : %4s%4s%4s%5s%s" %(
             self.prev_atom,
             self.prev_resname,
             self.prev_chain_id,
             self.prev_resseq,
             self.prev_icode,
             self.cur_atom,
             self.cur_resname,
             self.cur_chain_id,
             self.cur_resseq,
             self.cur_icode,), file=out)


class find_other_structure(find_segment):
  """Look for other_structure in a chain"""
  def __init__(self,previously_used_residues=None,
      params=None,model=None,
      extract_segments_from_pdb=None,
      make_unique=None,
      cut_up_segments=None,
      extend_segments=None,
      verbose=None,out=sys.stdout):

    self.previously_used_residues=previously_used_residues

    self.setup(params=params,model=model,segment_type='other',
      extract_segments_from_pdb=extract_segments_from_pdb,
      make_unique=make_unique,
      cut_up_segments=cut_up_segments,
      extend_segments=extend_segments,
      verbose=verbose,out=out)

  def pdb_records(self,last_id=0,out=sys.stdout):   #other (nothing)
    """No pdb records for this type of secondary structure"""
    return []

  def get_optimal_lengths(self,segment_dict=None,norms=None):
    """Always zero for other_structure"""
    optimal_delta_length_dict={}
    norm_dict={}
    for key in segment_dict:
      optimal_delta_length_dict[key]=0
      norm_dict[key]=None
    return optimal_delta_length_dict,norm_dict

  def add_start_end_to_segment_dict(self,
       params,n=None,n_buf=None,segment_start=None,
       segment_end=None,segment_dict=None):
    """Add a start and end to segment dict for this other_structure"""
    if self.make_unique:
      nn=0
    else:
      nn=n_buf
    start_pos=max(0,segment_start-nn)
    end_pos=min(segment_end+nn,n-1)
    if start_pos==0 and end_pos<params.minimum_length-1:
      end_pos=min(params.minimum_length-1,end_pos)
    if end_pos==n-1 and start_pos > n-params.minimum_length:
      start_pos=max(0,n-params.minimum_length)
    segment_dict[start_pos]=end_pos
    return segment_dict

  def find_segments(self,params=None,sites=None): # other
    """Find everything that is not alpha and not beta, put buffer_residues
    buffer on the end of each one."""

    # set up segment class for this kind of segment SPECIFIC FOR OTHER
    # for example, helix(sites=sites)
    h=self.segment_class(params=params,sites=sites)

    n=len(sites)
    if n>0 and self.model_as_segment:  # take the whole thing
      segment_dict[0]=n-1-self.last_residue_offset
      return None,None,segment_dict

    # cross off used residues except for buffer of buffer_residues
    n_buf=params.buffer_residues
    used_residues=n*[False]
    used_residues=[]
    for i in range(n+1):
      if i in self.previously_used_residues:
        used_residues.append(True)
      else:
        used_residues.append(False)

    segment_dict={}
    segment_start=None
    segment_end=None
    for i,used in zip(range(n),used_residues):
      if not used: # use it
        segment_end=i
        if segment_start is None:
          segment_start=i
      else:
        if segment_start is not None: # save it
          segment_dict=self.add_start_end_to_segment_dict(
             params,n=n,n_buf=n_buf,segment_start=segment_start,
             segment_end=segment_end,segment_dict=segment_dict)
        segment_start=None
        segment_end=None

    if segment_start is not None: # save it
      segment_dict=self.add_start_end_to_segment_dict(
             params,n=n,n_buf=n_buf,segment_start=segment_start,
             segment_end=segment_end,segment_dict=segment_dict)

    return None,None,segment_dict

class helix_strand_segments:
  """Holder for all helices and strands in a model"""
  def __init__(self):
    self.h_bond_text=""
    self.all_strands=[]
    self.all_alpha_helices=[]
    self.all_three_ten_helices=[]
    self.all_pi_helices=[]
    self.sheet_list=[]
    self.used_strands=[]
    self.pair_dict={}
    self.info_dict={}

    self.pdb_alpha_helix_list=[]
    self.pdb_three_ten_helix_list=[]
    self.pdb_pi_helix_list=[]
    self.pdb_sheet_list=[]
    self._have_annotations = False

  def have_annotations(self):
    """Return True if annotations are present"""
    return self._have_annotations

  def add_from_model(self,model):
      """Add in strands, helices from a local model object that has
        find_beta, find_alpha etc already present"""
      if model.find_beta:
        self.all_strands+=model.find_beta.segments
      if model.find_alpha:
        self.all_alpha_helices+=model.find_alpha.segments
      if model.find_three_ten:
        self.all_three_ten_helices+=model.find_three_ten.segments
      if model.find_pi:
        self.all_pi_helices+=model.find_pi.segments

  def find_sheets(self,out=sys.stdout,
     max_sheet_ca_ca_dist=6.,
     min_sheet_length=4,
     include_single_strands=None):
    """Find sheets from previously-identified strands"""
    if not self.all_strands: return
    print("\nFinding sheets from %d strands" %(len(
        self.all_strands)), file=out)
    self.get_strand_pairs(tol=max_sheet_ca_ca_dist,
        min_sheet_length=min_sheet_length)
    # pair_dict is list of all the strands that each strand matches with
    # self.info_dict is information on a particular pair
    #   of strands:
    #  self.info_dict["%d:%d" %(i,j)]=
    #     [first_ca_1,last_ca_1,first_ca_2,last_ca_2,is_parallel]

    # Create sheets from paired strands.
    # keep track of which ones we have assigned
    self.used_strands=[]

    # single_strands are those with no matching strands
    single_strands=self.get_strands_by_pairs(pairs=0)
    pair_strands=self.get_strands_by_pairs(pairs=1)
    triple_strands=self.get_strands_by_pairs(pairs=2)
    multiple_strands=self.get_strands_by_pairs(pairs=None)

    self.used_strands=[] # initialize again
    self.used_strands+=single_strands
    # we are going to ignore these

    self.sheet_list=[]

    if include_single_strands:# include singles
      for i in single_strands:
        self.sheet_list.append([i])

    # find all sheets with edges (beginning with a paired strand)
    self.sheet_list+=self.get_sheets_from_edges(
      pair_strands=pair_strands)
    self.sheet_list+=self.get_sheets_from_edges(
      pair_strands=triple_strands)

    # any pairs remaining? Create specialized "sheet" for each one
    existing_pairs_in_sheets=self.get_existing_pairs_in_sheets()
    missing_pairs=[]
    for i in range(len(self.all_strands)):
      for j in self.pair_dict.get(i,[]):
        if not [i,j] in existing_pairs_in_sheets and \
           not [i,j] in missing_pairs and not [j,i] in missing_pairs:
          missing_pairs.append([i,j])
          self.sheet_list.append([i,j])
          if not i in self.used_strands:
            self.used_strands.append(i)
          if not j in self.used_strands:
            self.used_strands.append(j)

    # Now we are ready to create sheets from sheet_list, self.pair_dict and
    #   self.info_dict

  def get_existing_pairs_in_sheets(self=None):
    """Get existing pairs of sheets in sheet_list"""
    existing_pairs=[]
    for sheet in self.sheet_list:
      for i,j in zip(sheet[:-1],sheet[1:]):
        existing_pairs.append([i,j])
        existing_pairs.append([j,i])
    return existing_pairs

  def get_sheets_from_edges(self,pair_strands=None):
    """Find sheets from paired strands"""
    sheet_list=[]
    for i in pair_strands:
      if i in self.used_strands:continue
      strand_list=[i]
      current_strand=i
      while current_strand is not None:
        current_strand=self.get_available_strand(
          current_strand=current_strand,
          strand_list=strand_list)
        if current_strand is not None:
          strand_list.append(current_strand)
      if len(strand_list)>1: # require an actual sheet
        self.used_strands+=strand_list
        sheet_list.append(strand_list)
    return sheet_list


  def get_available_strand(self,current_strand=None,strand_list=None):
    """Find a strand that is not used and not in strand_list"""
    for i in self.pair_dict.get(current_strand,[]):
      if not i in self.used_strands and not i in strand_list:
         return i
    return None

  def get_strands_by_pairs(self,pairs=None):
    """Collect strands in groups of n=pairs"""
    strand_list=[]
    while 1:  # get all strands in groups of n=pairs
      i=self.get_unused_strand(n=len(self.all_strands),
         used_strands=self.used_strands,pairs=pairs)
      if i is None: break
      self.used_strands.append(i)
      strand_list.append(i)
    return strand_list

  def get_unused_strand(self,n=None,used_strands=None,pairs=None):
    """Find an unused strand that can be part of a sheet with n=pairs
     strands"""
    for i in range(n):
      if i in used_strands: continue
      if pairs is None or len(self.pair_dict.get(i,[]))==pairs:
        return i
    return None

  def get_strand_pairs(self,tol=None,min_sheet_length=None):
    """Get pairs of strands and save in self.pair_dict where
     self.pair_dict[i] contains j if strands i and j could be paired"""
    self.info_dict={}
    self.pair_dict={}
    for i in range(len(self.all_strands)):
      self.pair_dict[i]=[]
    for i in range(len(self.all_strands)):
      for j in range(i+1,len(self.all_strands)):
        self.ca1=None
        self.ca2=None
        if self.ca_pair_is_close(self.all_strands[i],self.all_strands[j],
            tol=tol):

          # figure out alignment and whether it really is ok
          first_last_1_and_2=self.align_strands(
            self.all_strands[i],self.all_strands[j],tol=tol,
            min_sheet_length=min_sheet_length)

          if first_last_1_and_2:
            # we have a match
            [first_ca_1,last_ca_1,first_ca_2,last_ca_2,is_parallel]=\
               first_last_1_and_2

            # figure out which "O" of strand i should H-bond with which "N" of j
            # it is either going to be first_ca_1 or first_ca_1+1

            i_index,j_index=self.get_ind_h_bond_sheet(
              first_last_1_and_2=first_last_1_and_2,i=i,j=j,switch_i_j=False)

            self.pair_dict[i].append(j)
            self.info_dict["%d:%d" %(i,j)]=\
               [first_ca_1,last_ca_1,first_ca_2,last_ca_2,is_parallel,
                i_index,j_index]

            # and make an entry for the other way around
            i_index,j_index=self.get_ind_h_bond_sheet(
              first_last_1_and_2=first_last_1_and_2,i=i,j=j,switch_i_j=True)
            self.pair_dict[j].append(i)
            self.info_dict["%d:%d" %(j,i)]=\
               [first_ca_2,last_ca_2,first_ca_1,last_ca_1,is_parallel,
                i_index,j_index]

  def get_ind_h_bond_sheet(self,
      first_last_1_and_2,i=None,j=None,switch_i_j=False,
      registration=None,force_secondary_structure_input=None,
      sense=None):

    """
    Identify residues i_index,j_index that are next to each other in two
     adjacent strands of a sheet
    """
    if switch_i_j:
      xx=i
      i=j
      j=xx
      [first_ca_2,last_ca_2,first_ca_1,last_ca_1,is_parallel]=first_last_1_and_2
    else:
      [first_ca_1,last_ca_1,first_ca_2,last_ca_2,is_parallel]=first_last_1_and_2

    strand_i=self.all_strands[i]
    strand_j=self.all_strands[j]

    #-----------------force_secondary_structure_input------------
    if force_secondary_structure_input:
      if registration and strand_i.hierarchy and strand_j.hierarchy:
        # use the registration to id the paired atoms if provided
        if switch_i_j:
          prev_atom=registration.cur_atom
          i_bond=get_atom_index(hierarchy=strand_i.hierarchy,
           atom_name=registration.cur_atom,
           resname=registration.cur_resname,
           chain_id=registration.cur_chain_id,
           resseq=registration.cur_resseq,
           icode=registration.cur_icode,
           take_first_if_missing = True)
          j_bond=get_atom_index(hierarchy=strand_j.hierarchy,
           atom_name=registration.prev_atom,
           resname=registration.prev_resname,
           chain_id=registration.prev_chain_id,
           resseq=registration.prev_resseq,
           icode=registration.prev_icode,
           take_first_if_missing = True)
        else:
          prev_atom=registration.prev_atom
          strand_i=self.all_strands[i]
          strand_j=self.all_strands[j]
          i_bond=get_atom_index(hierarchy=strand_i.hierarchy,
           atom_name=registration.prev_atom,
           resname=registration.prev_resname,
           chain_id=registration.prev_chain_id,
           resseq=registration.prev_resseq,
           icode=registration.prev_icode,
           take_first_if_missing = True)
          j_bond=get_atom_index(hierarchy=strand_j.hierarchy,
           atom_name=registration.cur_atom,
           resname=registration.cur_resname,
           chain_id=registration.cur_chain_id,
           resseq=registration.cur_resseq,
           icode=registration.cur_icode,
           take_first_if_missing = True)

        assert sense in [-1,1]
        i_index=i_bond
        if sense==1: #  parallel strands:
          #  O of residue i in strand n H-bonds to N of i'+1 in strand n+1.
          #  N of residue i in strand n H-bonds to O of i'-1 in strand n+1.
          # O of i_index (prev) H-bonds to N of j_index (cur)
          # N of i_index H-bonds to O of j_index-2

          assert prev_atom.replace(" ","") in ["O","N"]
          if prev_atom.replace(" ","")=="N":
            # H-bond is to j_bond which is j_index-2
            j_index=j_bond+2
            if j_index>strand_j.sites.size()-1:
              j_index-=2
              i_index-=2
              if i_index>strand_i.sites.size()-1 or \
                  j_index>strand_j.sites.size()-1:
                return None,None # failed
          else:
            # H-bond is to j_bond which is j_index
            j_index=j_bond

        else: #  antiparallel strands:
          #  O of residue i in strand n H-bonds to N of residue i' in strand n+1
          #  N of residue i in strand n H-bonds to O of residue i' in strand n+1
          j_index=j_bond# everything is ok already

        return i_index,j_index

      else:
        return None,None
    #-----------------end force_secondary_structure_input------------



    i_index=first_ca_1
    if is_parallel:
      j_index=first_ca_2+1
    else:
      j_index=last_ca_2

    # View strand i from N to C with strand j to the right.  Every other
    #  residue in strand i has CA up/down/up/down.  Choose either
    #  residue first_ca_1 or first_ca_1+1, whichever is more up in this
    #  reference frame.
    #  "Up" here is
    # CA(i_index) -> CA(j_index)  X strand_i.segment_average_direction()

    inter_strand_vector=col(strand_j.get_sites()[j_index])-\
                        col(strand_i.get_sites()[i_index])
    if inter_strand_vector.is_zero():
      return None,None # give up (could not find a suitable H-bond)

    inter_strand_vector=inter_strand_vector.normalize()
    up_direction=inter_strand_vector.cross(
      strand_i.segment_average_direction())
    if up_direction.is_zero():
      return None,None # give up (could not find a suitable H-bond)

    up_direction=up_direction.normalize()

    n_dot=0.
    sum_dot=0.
    last_offset_index=len(strand_i.get_sites())-i_index-2
    for i in range(last_offset_index//2+1):
      offset=2*i
      delta=col(strand_i.get_sites()[i_index+1+offset])- \
            col(strand_i.get_sites()[i_index+offset])
      dot=up_direction.dot(delta)
      n_dot+=1
      sum_dot+=dot
    if n_dot:
      dot=sum_dot/n_dot
    else:
      return None,None # give up (could not find a suitable direction

    if dot > 0: # i_index is down. (dot is positive). Move 1 residue ahead

      i_index=i_index+1
      if is_parallel:
        j_index=j_index+1
      else:
        j_index=j_index-1

    if i_index+1>len(strand_i.get_sites()) or \
        j_index+1>len(strand_j.get_sites()) or \
        j_index< 0:
      return None,None # give up (could not find a suitable H-bond)
    return i_index,j_index


  def align_strands(self,s1,s2,tol=None,
     min_sheet_length=None):
    """Figure out best alignment and directions. Require at least 2 residues"""
    sites1=s1.get_sites()
    sites2=s2.get_sites()
    sites2_reversed=sites2.deep_copy().reversed()

    # self.ca1 and self.ca2 are pos of closest residues from ca_pair_is_close
    best_offset=None
    best_reverse=None
    best_keep_1=None
    best_keep_2=None
    best_score=None


    for offset in [0]:  # using other offsets did not help and sometimes worse
      if self.ca1+offset < 0 or self.ca1+offset > len(sites1)-1: continue
      dd_list,keep_1,keep_2=self.get_residue_pairs_in_sheet(sites1,sites2,
       center1=self.ca1+offset,center2=self.ca2,tol=tol)
      dd_list_reverse,keep_1_reverse,keep_2_reverse=\
         self.get_residue_pairs_in_sheet(sites1,sites2_reversed,
         center1=self.ca1+offset,center2=len(sites2)-self.ca2-1,tol=tol)

      if len(keep_1)<min_sheet_length and len(keep_1_reverse)<min_sheet_length:
        continue
      elif len(keep_1_reverse)>len(keep_1):
        score=len(keep_1_reverse)-0.001*flex.double(dd_list_reverse).norm()
        use_reverse=True
        if best_score is None or score>best_score:
          best_keep_1=keep_1_reverse
          best_keep_2=keep_2_reverse
          best_reverse=True
          best_offset=offset
          best_score=score
      else:

        score=len(keep_1)-0.001*flex.double(dd_list).norm()
        use_reverse=False
        if best_score is None or score>best_score:
          best_keep_1=keep_1
          best_keep_2=keep_2
          best_reverse=False
          best_offset=offset
          best_score=score

    if not best_score:
      return None

    first_ca_1=best_keep_1[0]
    last_ca_1=best_keep_1[-1]
    if best_reverse: # reversed
      first_ca_2=len(sites2)-(best_keep_2[-1]+1)
      last_ca_2=len(sites2)-(best_keep_2[0]+1)
      is_parallel=False
    else:  # forward
      first_ca_2=best_keep_2[0]
      last_ca_2=best_keep_2[-1]
      is_parallel=True

    return [first_ca_1,last_ca_1,first_ca_2,last_ca_2,is_parallel]


  def get_residue_pairs_in_sheet(self,sites1,sites2,
     center1=None,center2=None,tol=None):
    """Figure out pairs that are within tol. Center pairs are center1-center2"""
    dd=(col(sites1[center1])-col(sites2[center2])).norm_sq()
    keep1_list=[]
    keep2_list=[]
    dd_list=[]
    if dd<=tol**2: # plausible at least
      start_offset=max(-center1,-center2)
      end_offset=min(len(sites1)-(center1+1),len(sites2)-(center2+1))
      for offset in range(start_offset,end_offset+1):
        i1=center1+offset
        i2=center2+offset
        dd=(col(sites1[i1])-col(sites2[i2])).norm_sq()
        if dd <= tol**2:
          keep1_list.append(i1)
          keep2_list.append(i2)
          dd_list.append(dd**0.5)
        elif offset>0: # passed the middle, so end it
          break
        else: # have a bad one and have not gotten to middle yet. Start over
          keep_1_list=[]
          keep_2_list=[]
          dd_list=[]

    return dd_list,keep1_list,keep2_list

  def ca_pair_is_close(self,s1,s2,tol=None,
      dist_per_residue=3.5,jump=4):
    """Return True if CA pair is close enough to consider for H-bonding
    """
    best_dist_sq=None
    self.ca1=None
    self.ca2=None
    sites1=s1.get_sites()
    sites2=s2.get_sites()
    while jump > 0:
      # see if it is even close
      for i in range(jump//2,len(sites1),jump):
        for j in range(jump//2,len(sites2),jump):
          dd=(col(sites1[i])-col(sites2[j])).norm_sq()
          if best_dist_sq is None or dd < best_dist_sq:
            best_dist_sq=dd
            self.ca1=i
            self.ca2=j
      if best_dist_sq is None or \
         best_dist_sq**0.5 > (jump+1)*dist_per_residue+tol: # no hope
           break
      if jump==1:
           break
      jump=max(1,jump//2)  # try finer search
    if best_dist_sq is not None and best_dist_sq <= tol**2:
      return True
    else:
      return False

  def set_up_pdb_records(self,allow_ca_only_model=None,
    max_h_bond_length=None,
    force_secondary_structure_input=None,
    require_h_bonds=None,
    minimum_h_bonds=None,
    maximum_poor_h_bonds=None,
    models=None,out=sys.stdout):

    """Set up PDB records for all helices and strands.
    Skip any secondary structure elements that have fewer than minimum_h_bonds.
    if require_h_bonds is True"""

    number_of_good_h_bonds=0
    number_of_poor_h_bonds=0
    # save everything as pdb_alpha_helix or pdb_sheet objects
    if not models:
      return number_of_good_h_bonds,number_of_poor_h_bonds

    # determine if there are N and O atoms present.
    # If not, set allow_ca_only_model=True
    if (allow_ca_only_model is None):
      allow_ca_only_model=(not have_n_or_o(models))
    if force_secondary_structure_input:
      allow_ca_model_only = True

    f=StringIO()
    print("\nList of H-bonds expected from helices and from strand pairings", file=f)
    print("Distances > %3.1f A indicated by **" %(max_h_bond_length), file=f)
    print("H-bonds not included in HELIX/SHEET records marked 'Not included'", file=f)
    print("\n      ATOM 1               ATOM 2           Dist (A)\n", file=f)
    def get_fa(find_what='find_alpha',models=None):
       """Method to find something (like alpha)"""
       for model in models:
        if getattr(model,find_what):
          return getattr(model,find_what)
    fa=get_fa(find_what='find_alpha',models=models)
    if fa and self.all_alpha_helices:
      self.pdb_alpha_helix_list,n_good,n_poor=fa.pdb_records(
         segment_list=self.all_alpha_helices,
         helix_type='alpha',
         max_h_bond_length=max_h_bond_length,
         force_secondary_structure_input=force_secondary_structure_input,
         require_h_bonds=require_h_bonds,
         minimum_h_bonds=minimum_h_bonds,
         maximum_poor_h_bonds=maximum_poor_h_bonds,
         allow_ca_only_model=allow_ca_only_model,out=f)
      number_of_good_h_bonds+=n_good
      number_of_poor_h_bonds+=n_poor

    fa=get_fa(find_what='find_three_ten',models=models)
    if fa and self.all_three_ten_helices:
      self.pdb_three_ten_helix_list,n_good,n_poor=fa.pdb_records(
         segment_list=self.all_three_ten_helices,
         helix_type='3_10',
         max_h_bond_length=max_h_bond_length,
         require_h_bonds=require_h_bonds,
         minimum_h_bonds=minimum_h_bonds,
         maximum_poor_h_bonds=maximum_poor_h_bonds,
         force_secondary_structure_input=force_secondary_structure_input,
         allow_ca_only_model=allow_ca_only_model,out=f)
      number_of_good_h_bonds+=n_good
      number_of_poor_h_bonds+=n_poor

    fa=get_fa(find_what='find_pi',models=models)
    if fa and self.all_pi_helices:
      self.pdb_pi_helix_list,n_good,n_poor=fa.pdb_records(
        segment_list=self.all_pi_helices,helix_type='pi',
        max_h_bond_length=max_h_bond_length,
        require_h_bonds=require_h_bonds,
        minimum_h_bonds=minimum_h_bonds,
        maximum_poor_h_bonds=maximum_poor_h_bonds,
        force_secondary_structure_input=force_secondary_structure_input,
        allow_ca_only_model=allow_ca_only_model,out=f)
      number_of_good_h_bonds+=n_good
      number_of_poor_h_bonds+=n_poor

    fa=get_fa(find_what='find_beta',models=models)
    if fa and self.sheet_list and self.all_strands:
      self.pdb_sheet_list,n_good,n_poor=fa.pdb_records(
        segment_list=self.all_strands,
       sheet_list=self.sheet_list,
       info_dict=self.info_dict,
       max_h_bond_length=max_h_bond_length,
       require_h_bonds=require_h_bonds,
       minimum_h_bonds=minimum_h_bonds,
       maximum_poor_h_bonds=maximum_poor_h_bonds,
       force_secondary_structure_input=force_secondary_structure_input,
       allow_ca_only_model=allow_ca_only_model,out=f)
      number_of_good_h_bonds+=n_good
      number_of_poor_h_bonds+=n_poor
    text=f.getvalue()
    print(text, file=out)
    self.h_bond_text=text
    return number_of_good_h_bonds,number_of_poor_h_bonds

  def get_annotation(self):
    """Get annotation object for this analysis"""
    all_helices = []
    all_sheets = []
    self._have_annotations = False
    self.all_selection_records = []
    for helix in self.pdb_alpha_helix_list:
      all_helices.append(helix)
      self.all_selection_records+=helix.as_atom_selections()
      self._have_annotations = True

    for helix in self.pdb_three_ten_helix_list:
      all_helices.append(helix)
      self.all_selection_records+=helix.as_atom_selections()
      self._have_annotations = True

    for helix in self.pdb_pi_helix_list:
      all_helices.append(helix)
      self.all_selection_records+=helix.as_atom_selections()
      self._have_annotations = True

    for sheet in self.pdb_sheet_list:
      all_sheets.append(sheet)
      self.all_selection_records+=sheet.as_atom_selections()
      self._have_annotations = True
    from iotbx.pdb.secondary_structure import annotation
    all_annotations = annotation(helices = all_helices, sheets = all_sheets)
    return all_annotations

  def get_pdb_alpha_helix_list(self):
    """Get annotations for alpha helix"""
    if hasattr(self,'pdb_alpha_helix_list'):
      return self.pdb_alpha_helix_list

  def get_pdb_three_ten_helix_list(self):
    """Get annotations for three_ten helix"""
    if hasattr(self,'pdb_three_ten_helix_list'):
      return self.pdb_three_ten_helix_list

  def get_pdb_pi_helix_list(self):
    """Get annotations for pi helix"""
    if hasattr(self,'pdb_pi_helix_list'):
      return self.pdb_pi_helix_list

  def get_pdb_sheet_list(self):
    """Get annotations for sheets"""
    if hasattr(self,'pdb_sheet_list'):
      return self.pdb_sheet_list

  def get_all_selection_records(self):
    """Get selections for secondary structure"""
    if not hasattr(self,'all_selection_records'):
       return
    text='"'
    first=True
    for sel in self.all_selection_records:
      if not first:
        text+=" or "
      first=False
      text+=" ( "+sel.replace('"','')+") "
    text+='"'
    return text

class fss_result_object:
  """A holder for results of find_secondary_structure"""
  def __init__(self,
      id=None,
      chain_id=None,
      hierarchy=None,
      number_of_good_h_bonds=None,
      number_of_poor_h_bonds=None,
      h_bond_text=None,
      annotation=None,
      sequence=None,
      sites=None,
      start_resno=None,
      end_resno=None,
      max_rmsd=1):
    adopt_init_args(self, locals())

    if hierarchy:  #
      self.sites,self.sequence,self.start_resno,self.end_resno=\
         sites_and_seq_from_hierarchy( hierarchy)

    self.chain_id_list=[]
    if self.chain_id:
      self.chain_id_list.append(self.chain_id)

  def show_summary(self,out=sys.stdout):
    """Summarize fss_result_object object"""
    print("\nSummary of find_secondary_structure object %s" %(self.id), end=' ', file=out)
    print("for sequence: %s_%s::%s\n" %(self.start_resno,self.end_resno,
       self.sequence), file=out)
    print("Chain ID's where this applies: %s" %(
      " ".join(self.get_chain_id_list())), file=out)
    print("Good H-bonds: %s  Poor H-bonds: %s " %(
      self.number_of_good_h_bonds,self.number_of_poor_h_bonds), file=out)
    print("H-bond text:\n%s" %(self.h_bond_text), file=out)
    print("Annotation:\n%s" %(self.get_annotation()), file=out)

  def add_chain_id(self,chain_id=None):
    """Add a chain_id to this fss_result_object"""
    if chain_id is not None:
      self.chain_id_list.append(chain_id)


  def add_info(self,chain_id=None,
      number_of_good_h_bonds=None,
      number_of_poor_h_bonds=None,
      h_bond_text=None,
      annotation=None,):
    """Add info to this fss_result_object"""
    if chain_id is not None:
      self.chain_id_list.append(chain_id)
    if number_of_good_h_bonds is not None:
      self.number_of_good_h_bonds=number_of_good_h_bonds
    if number_of_poor_h_bonds is not None:
      self.number_of_poor_h_bonds=number_of_poor_h_bonds
    if h_bond_text is not None:
      self.h_bond_text=h_bond_text
    if annotation is not None:
      self.annotation=annotation

  def get_annotation(self):
    """Return annotation for this fss_result_object"""
    return self.annotation

  def get_chain_id_list(self):
    """Return list of chain IDs in this fss_result_object"""
    return self.chain_id_list

  def is_similar_fss_result(self,other):
    """Return True if this fss_result_object is similar to other"""
    if self.sequence != other.sequence:
       return False
    if not sites_are_similar(self.sites,other.sites,max_rmsd=self.max_rmsd):
       return False
    if self.start_resno != other.start_resno:
       return False
    if self.end_resno != other.end_resno:
       return False
    return True

class conformation_group:
  """A group of fss_results with the same sequence but different
  conformations"""
  def __init__(self,
    fss_result=None,
    ):

    self.fss_results=[]
    self.last_id=0
    if fss_result:
       self.last_id+=1
       fss_result.id=self.last_id
       self.fss_results.append(fss_result)

  def __repr__(self):
    text="Conformation group with %s fss_results" %(
       len(self.get_fss_result_list()))
    if self.get_fss_result_list():
      text+="\nSequence: %s" %( self.get_fss_result_list()[0].sequence)
    return text

  def get_fss_result_list(self):
    """Return all find_secondary_structure results"""
    return self.fss_results

  def add_fss_result(self,fss_result=None):
    """Add a find_secondary_structure result"""
    if fss_result:
       self.last_id+=1
       fss_result.id=self.last_id
       self.fss_results.append(fss_result)

  def get_similar_fss_result(self,fss_result=None):
    """Return an fss_result from this group that is similar to supplied
       fss_result, if any"""
    for fss_r in self.fss_results:
      if fss_r.is_similar_fss_result(fss_result):
        return fss_r
    return None


class find_secondary_structure:
  """Class to look for secondary structure"""

  def __init__(self,params=None,args=None,hierarchy=None,models=None,
      user_annotation_text=None,max_h_bond_length=None,
      force_secondary_structure_input=None,
      search_secondary_structure=None,
      combine_annotations=None,
      require_h_bonds=None,
      minimum_h_bonds=None,
      maximum_poor_h_bonds=None,
      helices_are_alpha=False,
      ss_by_chain=None,
      evaluate_sheet_topology=None,
      use_representative_chains=None,
      max_representative_chains=None,
      max_rmsd=None,
      verbose=None,out=sys.stdout):
    adopt_init_args(self, locals())

    if not args: args=[]

    if not params:  # get params from args if necessary
      params=self.get_params(args,out=out)

    if verbose is not None:
      params.control.verbose=verbose
    verbose=params.control.verbose


    if helices_are_alpha or params.find_ss_structure.helices_are_alpha:
      params.find_ss_structure.find_three_ten=False
      params.find_ss_structure.find_pi=False
    if require_h_bonds is not None:
      params.find_ss_structure.require_h_bonds=require_h_bonds
    if minimum_h_bonds is not None:
      params.find_ss_structure.minimum_h_bonds=minimum_h_bonds
    if maximum_poor_h_bonds is not None:
      params.find_ss_structure.maximum_poor_h_bonds=maximum_poor_h_bonds
    if max_h_bond_length is not None:
      params.find_ss_structure.max_h_bond_length=max_h_bond_length
    if combine_annotations is not None:
      params.find_ss_structure.combine_annotations=combine_annotations
    if force_secondary_structure_input is not None:
      params.input_files.force_secondary_structure_input=\
         force_secondary_structure_input
    force_secondary_structure_input=\
      params.input_files.force_secondary_structure_input
    if ss_by_chain is not None:
      params.find_ss_structure.ss_by_chain=ss_by_chain
    if evaluate_sheet_topology is not None:
      params.find_ss_structure.evaluate_sheet_topology=evaluate_sheet_topology
    if max_rmsd is not None:
      params.find_ss_structure.max_rmsd=max_rmsd
    if use_representative_chains is not None:
      params.find_ss_structure.use_representative_chains=\
        use_representative_chains
    if max_representative_chains is not None:
      params.find_ss_structure.max_representative_chains=\
        max_representative_chains

    # Overwrite parameters for tolerant search
    if params.find_ss_structure.tolerant:
      print("Setting parameters for tolerant search", file = out)
      if params.find_ss_structure.ss_by_chain:
        params.find_ss_structure.ss_by_chain=False
        print("Set ss_by_chain=%s" %(
          params.find_ss_structure.ss_by_chain), file = out)
      if not params.find_ss_structure.include_single_strands:
        params.find_ss_structure.include_single_strands=True
        print("Set include_single_strands=%s" %(
          params.find_ss_structure.include_single_strands), file = out)
      if params.find_ss_structure.max_h_bond_length < \
          params.find_ss_structure.tolerant_max_h_bond_length:
        params.find_ss_structure.max_h_bond_length = \
          params.find_ss_structure.tolerant_max_h_bond_length
        print("Set max_h_bond_length=%s" %(
          params.find_ss_structure.tolerant_max_h_bond_length),
          file = out)
      if params.beta.max_sheet_ca_ca_dist < \
          params.beta.tolerant_max_sheet_ca_ca_dist:
        params.beta.max_sheet_ca_ca_dist = \
          params.beta.tolerant_max_sheet_ca_ca_dist
        print("Set max_sheet_ca_ca_dist=%s" %(
          params.beta.tolerant_max_sheet_ca_ca_dist),
          file = out)
      if params.beta.min_sheet_length > \
          params.beta.tolerant_min_sheet_length:
        params.beta.min_sheet_length = \
          params.beta.tolerant_min_sheet_length
        print("Set min_sheet_length=%s" %(
          params.beta.tolerant_min_sheet_length),
          file = out)


    secondary_structure_input=params.input_files.secondary_structure_input

    if search_secondary_structure is not None:
      params.find_ss_structure.search_secondary_structure=\
         search_secondary_structure
    search_secondary_structure=\
       params.find_ss_structure.search_secondary_structure

    if (not params.find_ss_structure.search_secondary_structure) and (not
      params.input_files.secondary_structure_input) and (not
      user_annotation_text):
      raise Sorry(
       "Need either secondary_structure_input or search_secondary_structure=True")

    self.helix_strand_segments=helix_strand_segments()
    self.user_helix_strand_segments=helix_strand_segments()

    self.user_models=[]
    self.number_of_good_h_bonds=0
    self.number_of_poor_h_bonds=0
    self.user_number_of_good_h_bonds=0
    self.user_number_of_poor_h_bonds=0
    self.h_bond_text=""

    if verbose:
      local_out=out
    else:
      from libtbx.utils import null_out
      local_out=null_out()

    if hierarchy:
      hierarchy=hierarchy.deep_copy()
    elif (not models) or params.input_files.secondary_structure_input:
      # need to get a hierarchy
      if models:
        combined_model=merge_hierarchies_from_models(models=models)
        hierarchy=combined_model.hierarchy.deep_copy()
      else:  # read it in
        if not params.input_files.pdb_in or \
            not os.path.isfile(params.input_files.pdb_in):
          raise Sorry("Missing file: %s" %(params.input_files.pdb_in))
        hierarchy=get_pdb_hierarchy(text=open(params.input_files.pdb_in).read())
    if hierarchy:
      hierarchy.remove_alt_confs(always_keep_one_conformer=False)
      atom_selection="protein"
      try:
        hierarchy=hierarchy.apply_atom_selection(atom_selection)
      except Exception as e:
        hierarchy=None

    if hierarchy and params.find_ss_structure.auto_choose_ca_vs_ca_n_o:
      hierarchy=choose_ca_or_complete_backbone(hierarchy,params=params)

    if force_secondary_structure_input and not \
        (params.input_files.secondary_structure_input or user_annotation_text):
      raise Sorry(
         "Need secondary_structure_input for force_secondary_structure_input")

    # Get user ss information if any into composite_user_annotation
    if user_annotation_text or params.input_files.secondary_structure_input:
      composite_user_annotation=self.get_user_ss(
        params=params,hierarchy=hierarchy,
        user_annotation_text=user_annotation_text,out=out)
      if not params.input_files.secondary_structure_input:
        params.input_files.secondary_structure_input=True # so we can check
        secondary_structure_input=True # so we can check
    else:
      composite_user_annotation=None

    if models:
      self.models=models
    else:
      self.models=split_model(hierarchy=hierarchy)

    # Decide if we are going to run in parts and just extend those to all
    #   copies
    self.args=args
    self.params=params
    self.hierarchy=hierarchy
    if self.need_to_run_in_parts():
       self.run_in_parts()
       return  # done

    if force_secondary_structure_input or (not
       params.find_ss_structure.search_secondary_structure):
      working_annotation=composite_user_annotation
    else:
      for model in self.models:
        self.find_ss_in_model(params=params,model=model,out=out)
        self.helix_strand_segments.add_from_model(model)

      if self.helix_strand_segments and \
         params.find_ss_structure.set_up_helices_sheets:
        self.helix_strand_segments.find_sheets(
         include_single_strands=params.find_ss_structure.include_single_strands,
         max_sheet_ca_ca_dist=params.beta.max_sheet_ca_ca_dist,
         min_sheet_length=params.beta.min_sheet_length,
         out=out) # organize strands into sheets

      if self.helix_strand_segments:
        self.number_of_good_h_bonds,self.number_of_poor_h_bonds=\
          self.helix_strand_segments.set_up_pdb_records(models=self.models,
          max_h_bond_length=params.find_ss_structure.max_h_bond_length,
          force_secondary_structure_input=force_secondary_structure_input,
          allow_ca_only_model=params.beta.allow_ca_only_model,out=local_out)
        self.h_bond_text=self.helix_strand_segments.h_bond_text
        print("\nNumber of good H-bonds: %d  Number of poor H-bonds: %d" %(
          self.number_of_good_h_bonds,self.number_of_poor_h_bonds), file=local_out)

      # get annotation:
      print("\nNew working annotation:", file=out)
      working_annotation=self.helix_strand_segments.get_annotation()
      print(working_annotation.as_pdb_or_mmcif_str(), file=out)

      if params.find_ss_structure.combine_annotations and \
          composite_user_annotation:
        print("\nMerging edited input annotation and working annotation", file=out)
        working_annotation=composite_user_annotation.combine_annotations(
          hierarchy=hierarchy, other=working_annotation)
        print("\nMerged annotation:\n", file=out)
        print(working_annotation.as_pdb_or_mmcif_str(), file=out)


    #  Remove annotation that does not match model
    if params.find_ss_structure.remove_missing_atom_annotation:
      working_annotation=remove_bad_annotation(
        working_annotation,
        hierarchy=hierarchy,
        max_h_bond_length=params.find_ss_structure.max_h_bond_length,
        remove_overlaps=False, # XXX Required to prevent recursion
        out=out)

    # Now get final values of H-bonds etc with our final annotation
    if params.find_ss_structure.require_h_bonds:
      remove_text=""
      if params.find_ss_structure.minimum_h_bonds>0:
        remove_text+=\
         "\nRemoving any secondary structure with fewer than %d H-bonds"  %(
        params.find_ss_structure.minimum_h_bonds)
      if params.find_ss_structure.maximum_poor_h_bonds and \
         params.find_ss_structure.maximum_poor_h_bonds>0:
        remove_text+=\
         "\nRemoving any secondary structure with more than %d poor H-bonds"  %(
        params.find_ss_structure.maximum_poor_h_bonds)

    else:
      remove_text=""

    if self.helix_strand_segments and not secondary_structure_input:
      # Use analysis from working annotation (no user input)
      if remove_text: print(remove_text, file=out)
      print("\nGetting H-bonds from working annotation", file=out)
      self.number_of_good_h_bonds,self.number_of_poor_h_bonds=\
         self.helix_strand_segments.set_up_pdb_records(models=self.models,
         max_h_bond_length=params.find_ss_structure.max_h_bond_length,
         force_secondary_structure_input=force_secondary_structure_input,
         allow_ca_only_model=params.beta.allow_ca_only_model,
         require_h_bonds=params.find_ss_structure.require_h_bonds,
         minimum_h_bonds=params.find_ss_structure.minimum_h_bonds,
         maximum_poor_h_bonds=params.find_ss_structure.maximum_poor_h_bonds,
         out=out)
      self.h_bond_text=self.helix_strand_segments.h_bond_text
      working_annotation=self.helix_strand_segments.get_annotation()

    elif self.user_helix_strand_segments and secondary_structure_input and \
        not params.find_ss_structure.combine_annotations:
      # use analysis of user input (as is)
      if remove_text and not force_secondary_structure_input:
        print(remove_text, file=out)
        require_h_bonds=params.find_ss_structure.require_h_bonds
        minimum_h_bonds=params.find_ss_structure.minimum_h_bonds
        maximum_poor_h_bonds=params.find_ss_structure.maximum_poor_h_bonds,
      else:
        remove_text=None
        require_h_bonds=None
        minimum_h_bonds=None
        maximum_poor_h_bonds=None

      print("\nGetting H-bonds from user annotation", file=out)
      self.number_of_good_h_bonds,self.number_of_poor_h_bonds=\
         self.user_helix_strand_segments.set_up_pdb_records(
         models=self.user_models,
         max_h_bond_length=params.find_ss_structure.max_h_bond_length,
         force_secondary_structure_input=force_secondary_structure_input,
         require_h_bonds=require_h_bonds,
         minimum_h_bonds=minimum_h_bonds,
         maximum_poor_h_bonds=params.find_ss_structure.maximum_poor_h_bonds,
         allow_ca_only_model=params.beta.allow_ca_only_model,
         out=out)
      self.h_bond_text=self.user_helix_strand_segments.h_bond_text
      working_annotation=self.user_helix_strand_segments.get_annotation()

    else: # need to redo it from the beginning with our new annotation
      # user annotation combined with new annotation
      if remove_text: print(remove_text, file=out)
      print("\nRunning analysis with new annotation", file=out)
      if working_annotation and working_annotation.as_pdb_or_mmcif_str():
        fss=find_secondary_structure(hierarchy=hierarchy,
          user_annotation_text=working_annotation.as_pdb_or_mmcif_str(),
          force_secondary_structure_input=True,
          combine_annotations=False,
          max_h_bond_length=params.find_ss_structure.max_h_bond_length,
          require_h_bonds=params.find_ss_structure.require_h_bonds,
          minimum_h_bonds=params.find_ss_structure.minimum_h_bonds,
          maximum_poor_h_bonds=params.find_ss_structure.maximum_poor_h_bonds,
          ss_by_chain=params.find_ss_structure.ss_by_chain,
          use_representative_chains=\
            params.find_ss_structure.use_representative_chains,
          max_representative_chains=\
            params.find_ss_structure.max_representative_chains,
          max_rmsd=params.find_ss_structure.max_rmsd,
          out=local_out)
        print(fss.h_bond_text, file=out)
        self.number_of_good_h_bonds=fss.number_of_good_h_bonds
        self.number_of_poor_h_bonds=fss.number_of_poor_h_bonds
        working_annotation=fss.get_annotation()
      else:
        self.number_of_good_h_bonds=0
        self.number_of_poor_h_bonds=0

    print("\nNumber of good H-bonds: %d  Number of poor H-bonds: %d" %(
          self.number_of_good_h_bonds,self.number_of_poor_h_bonds), file=out)

    self.annotation=working_annotation

    self.show_summary(verbose=params.control.verbose,
      pdb_records_file=params.output_files.pdb_records_file,out=out)

  def need_to_run_in_parts(self,
     min_residues_for_parts=None,
     min_average_chain_length=None,):
    """Run in parts if lots of ncs or big chains.
    Don't if lots of little fragments or model objects are supplied."""
    if not self.params.find_ss_structure.ss_by_chain:
      return # not going to do this at all
    if not self.hierarchy:
      return # not going to do this at all. Only from hierarchy

    oc=self.hierarchy.overall_counts()
    if min_residues_for_parts and oc.n_residues < min_residues_for_parts:
      return
    if min_average_chain_length and \
       oc.n_residues/max(1,oc.n_chains) < min_average_chain_length:
      return

    # If not a CA-only model, require N and O to be present on all residues
    #   to run with representative chains (otherwise there may be some N/O
    #   that are present in only some chains)
    if (not is_ca_only_hierarchy(self.hierarchy))  and (
          not ca_n_and_o_always_present(self.hierarchy)):
      return

    # Worth running on individual chains
    return True

  def run_in_parts(self):
    """Just run through all the chains and get their ss.  If duplicate chains
    #   and use_representative_chains, copy results"""
    print("\nRunning on full chains (no "+\
        "intra-chain secondary structure)", file=self.out)

    local_params=deepcopy(self.params)
    local_params.find_ss_structure.ss_by_chain=False
    if self.params.control.verbose:
      local_out=self.out
    else:
      from libtbx.utils import null_out
      local_out=null_out()
    result_dict={} # fss_conformation_groups keyed by sequence to find quickly
    unique_sequence_list=[]

    for model in self.hierarchy.models()[:1]:
      for chain in model.chains():
        chain_id=chain.id
        local_hierarchy=hierarchy_from_chain(chain)
        # get fss_result holder
        current_fss_result=fss_result_object(chain_id=chain_id,
           hierarchy=local_hierarchy,
           max_rmsd=self.params.find_ss_structure.max_rmsd)
        if self.params.find_ss_structure.use_representative_chains and \
          len(unique_sequence_list)< \
            self.params.find_ss_structure.max_representative_chains:
          # See if this sequence has been analyzed already:
          test_sequence_string="%s_%s::%s" %(
            current_fss_result.start_resno,current_fss_result.end_resno,
            current_fss_result.sequence)
          cg=result_dict.get(test_sequence_string,conformation_group())
          # cg is either empty or a conformation_group with current sequence
          existing_fss_result=cg.get_similar_fss_result(current_fss_result)
          # if present, existing_fss_result is same conformation as current
        else:
          cg=conformation_group()
          existing_fss_result=None
        if existing_fss_result:
          existing_fss_result.add_chain_id(chain_id=chain_id)
        else: # get the analysis of this chain
          fss=find_secondary_structure(
            params=local_params,hierarchy=local_hierarchy,
            user_annotation_text=self.user_annotation_text,
            max_h_bond_length=self.max_h_bond_length,
            force_secondary_structure_input=\
              self.force_secondary_structure_input,
            search_secondary_structure=self.search_secondary_structure,
            combine_annotations=self.combine_annotations,
            require_h_bonds=self.require_h_bonds,
            minimum_h_bonds=self.minimum_h_bonds,
            maximum_poor_h_bonds=self.maximum_poor_h_bonds,
            verbose=self.verbose,out=local_out)
          current_fss_result.add_info(  # add new information
             number_of_good_h_bonds=fss.number_of_good_h_bonds,
             number_of_poor_h_bonds=fss.number_of_poor_h_bonds,
             h_bond_text=fss.h_bond_text,
             annotation=fss.get_annotation())
          # add new fss_result to empty conformation_group and save
          cg.add_fss_result(fss_result=current_fss_result)
          sequence_string="%s_%s::%s" %(
            current_fss_result.start_resno,current_fss_result.end_resno,
            current_fss_result.sequence)
          result_dict[sequence_string]=cg
          if not sequence_string in unique_sequence_list:
            unique_sequence_list.append(sequence_string)


    # Go through all chains and save annotation and number of good/poor h bonds
    print("\nAnalysis using %s unique sequences:" %(
       len(unique_sequence_list)), file=self.out)
    print("Unique part of the analysis:", file=self.out)
    all_sheets=[]
    all_helices=[]
    number_of_good_h_bonds=0
    number_of_poor_h_bonds=0
    import iotbx.pdb.secondary_structure as ioss
    i=0
    for sequence_string in unique_sequence_list:
      cg=result_dict[sequence_string]
      i+=1
      print(80*"=", file=self.out)
      print("\nAnalysis of chains with sequence %s: %s\n" %(
        i,sequence_string), file=self.out)
      print(80*"=", file=self.out)
      for fss_result in cg.get_fss_result_list():
        fss_result.show_summary(out=self.out)
        chain_id_list=fss_result.get_chain_id_list()
        chain_id=chain_id_list[0]
        annotation=fss_result.get_annotation().deep_copy()
        n=len(chain_id_list)
        if len(chain_id_list)>1:
          chain_id_list=chain_id_list[1:]
          annotation.multiply_to_asu_2(chain_ids_dict={chain_id:chain_id_list})
        all_helices+=annotation.helices
        all_sheets+=annotation.sheets
        number_of_good_h_bonds+=n*fss_result.number_of_good_h_bonds
        number_of_poor_h_bonds+=n*fss_result.number_of_poor_h_bonds

    self.annotation=ioss.annotation(sheets=all_sheets,helices=all_helices)
    self.annotation.renumber_helices_and_sheets()
    self.number_of_good_h_bonds=number_of_good_h_bonds
    self.number_of_poor_h_bonds=number_of_poor_h_bonds
    print(80*"=", file=self.out)
    print("\nFinal annotation and selections", file=self.out)
    print(80*"=", file=self.out)
    self.show_summary(out=self.out)



  def show_summary(self,verbose=None,pdb_records_file=None,out=sys.stdout):
    """Summarize find_secondary_structure object"""
    for model in self.models:
      if verbose:
        print("\nModel %d  N: %d  Start: %d End: %d" %(
          model.info.get('chain_number',0),
          model.length(),model.first_residue(),model.last_residue()), file=out)
      if verbose:
        if model.find_alpha:
          model.find_alpha.show_summary(out=out)
        if model.find_three_ten:
          model.find_three_ten.show_summary(out=out)
        if model.find_pi:
          model.find_pi.show_summary(out=out)
        if model.find_beta:
          model.find_beta.show_summary(out=out)
        if model.find_other:
          model.find_other.show_summary(out=out)
    if self.annotation and self.annotation.as_pdb_or_mmcif_str():
      print("\nFINAL PDB RECORDS:", file=out)
      print(self.annotation.as_pdb_or_mmcif_str(), file=out)

      if self.params.control.verbose:
        print("\n\nFINAL HELIX selections:", file=out)
        print('"%s"' %(self.annotation.overall_helix_selection()), file=out)
        print("\n\nFINAL SHEET selections:", file=out)
        print('"%s"' %(self.annotation.overall_sheet_selection()), file=out)

      print("\n\nFINAL PDB selections:", file=out)
      print('"%s"' %(self.annotation.overall_selection()), file=out)


    if pdb_records_file and self.annotation:
      f=open(pdb_records_file,'w')
      print(self.annotation.as_pdb_or_mmcif_str(), file=f)
      f.close()
      print("\nRecords written to %s\n" %(
         pdb_records_file), file=out)

  def get_results(self):
    """Return annotation for this find_secondary_structure result"""
    return self.get_annotation()

  def get_annotation(self):
    """Return annotation for this find_secondary_structure result"""
    if hasattr(self,'annotation'):
      return self.annotation

  def get_user_ss(self,params=None,hierarchy=None,
     user_annotation_text=None,out=sys.stdout):
    """Return user-supplied annotation"""
    if not user_annotation_text:
      file_name=params.input_files.secondary_structure_input
      if file_name and not os.path.isfile(file_name):
       raise Sorry("The secondary_structure_input file '%s' is missing" %(
         str(file_name)))

      # Read ss structure from this file now
      print("\nReading secondary structure records from %s\n" %(file_name), file=out)
      user_annotation_text=open(file_name).read()
    import iotbx.pdb
    user_annotation=iotbx.pdb.input(source_info=None,
       lines = flex.split_lines(
       user_annotation_text)).extract_secondary_structure()

    print("\nUser helix/strand records as input:\n", file=out)
    print(user_annotation.as_pdb_or_mmcif_str(), file=out)
    if params.input_files.force_secondary_structure_input:
      if params.find_ss_structure.combine_annotations:
       print("\nThis secondary structure annotation will be taken as is and"+\
       " then will be \ncombined with an edited version (updating H-bonding)",
         file=out)
      else:
        print("\nThis secondary structure annotation will be used as is.\n",
             file=out)
      remove_overlaps=False
    else:
      print(
      "\nThis secondary structure annotation will be modified if necessary\n",
           file=out)
      remove_overlaps=True
    # Remove any parts of this annotation that do not exist in the hierarchy
    user_annotation=remove_bad_annotation(
        user_annotation,
        hierarchy=hierarchy,
        max_h_bond_length=params.find_ss_structure.max_h_bond_length,
        remove_overlaps=remove_overlaps,
        out=out)
    if not user_annotation:
        return None

    if params.control.verbose or \
        (not params.input_files.force_secondary_structure_input) or \
        params.find_ss_structure.combine_annotations:
      local_out=out
    else:
      from libtbx.utils import null_out
      local_out=null_out()

    # Set up our alpha_helix_list etc from this...(just copy)

    # Helix classes:   'alpha', 'pi', '3_10',
    for helix in user_annotation.helices:
      ph=hierarchy.apply_atom_selection(
       get_string_or_first_element_of_list(helix.as_atom_selections()))
      model=model_info(hierarchy=ph,info={'class':helix.helix_class})
      self.user_models.append(model)
      if helix.helix_class=='alpha':
        self.user_helix_strand_segments.pdb_alpha_helix_list.append(helix)
        model.find_alpha=find_helix(params=params.alpha,model_as_segment=True,
          model=model,verbose=params.control.verbose)
      elif helix.helix_class=='pi':
        self.user_helix_strand_segments.pdb_pi_helix_list.append(helix)
        model.find_pi=find_helix(params=params.three_ten,
          model_as_segment=True,
          model=model,verbose=params.control.verbose)
      elif helix.helix_class=='3_10':
        self.user_helix_strand_segments.pdb_three_ten_helix_list.append(helix)
        model.find_three_ten=find_helix(params=params.pi,
          model_as_segment=True,
          model=model,verbose=params.control.verbose)
      else:
        raise Sorry("Unknown helix type: '%s'" %(helix.helix_class))
      self.user_helix_strand_segments.add_from_model(model)

    self.user_helix_strand_segments.sheet_list=[]
    for sheet in user_annotation.sheets:
      strand_id_in_sheet=[]
      self.user_helix_strand_segments.sheet_list.append(strand_id_in_sheet)
      prev_strand_as_segment=None
      prev_strand_id=None
      if len(sheet.registrations)!=len(sheet.strands):
        raise Sorry("\nNot 1:1 registrations (%d) to strands (%d) " %(
          len(sheet.registrations),len(sheet.strands)))

      prev_strand=None
      prev_hierarchy=None
      for strand,registration in zip(sheet.strands,sheet.registrations):
        if prev_strand:
          is_parallel=(
             (prev_strand.sense==0 and strand.sense==1) or
             (strand.sense==prev_strand.sense)
          )
        else:
          is_parallel=None

        ph=hierarchy.apply_atom_selection(
         get_string_or_first_element_of_list(strand.as_atom_selections()))

        model=model_info(hierarchy=ph,info={'class':'strand'})
        self.user_models.append(model)
        model.find_beta=find_beta_strand(params=params.beta,
          model_as_segment=True,
          model=model,verbose=params.control.verbose)
        n_strands_prev=len(self.user_helix_strand_segments.all_strands)
        self.user_helix_strand_segments.add_from_model(model)
        n_strands_cur=len(self.user_helix_strand_segments.all_strands)
        assert n_strands_cur==n_strands_prev+1

        current_strand_as_segment=\
              self.user_helix_strand_segments.all_strands[-1]
        current_strand_id=len(self.user_helix_strand_segments.all_strands)-1
        strand_id_in_sheet.append(current_strand_id)

        if prev_strand is not None: # add entries to pair_dict
          first_ca_1=0
          last_ca_1=prev_strand_as_segment.length()-1
          first_ca_2=0
          last_ca_2=current_strand_as_segment.length()-1

          if not prev_strand_id in \
              self.user_helix_strand_segments.pair_dict:
            self.user_helix_strand_segments.pair_dict[prev_strand_id]=[]
          if not current_strand_id in \
              self.user_helix_strand_segments.pair_dict:
            self.user_helix_strand_segments.pair_dict[current_strand_id]=[]

          # identify residues i_index,j_index that are next to each other
          first_last_1_and_2=\
             [first_ca_1,last_ca_1,first_ca_2,last_ca_2,is_parallel]
          i_index,j_index=self.user_helix_strand_segments.get_ind_h_bond_sheet(
              first_last_1_and_2=first_last_1_and_2,
              i=prev_strand_id,j=current_strand_id,switch_i_j=False,
              registration=registration,
              sense=strand.sense,
              force_secondary_structure_input=\
                params.input_files.force_secondary_structure_input)
          self.user_helix_strand_segments.pair_dict[prev_strand_id].append(
             current_strand_id)
          key12="%d:%d" %(prev_strand_id,current_strand_id)
          self.user_helix_strand_segments.info_dict[key12]=\
            [first_ca_1,last_ca_1,first_ca_2,last_ca_2,is_parallel,i_index,j_index]

          # Now reversed
          i_index,j_index=self.user_helix_strand_segments.get_ind_h_bond_sheet(
              first_last_1_and_2=first_last_1_and_2,
              i=prev_strand_id,j=current_strand_id,switch_i_j=True,
              registration=registration,
              sense=strand.sense,
              force_secondary_structure_input=\
                params.input_files.force_secondary_structure_input)

          self.user_helix_strand_segments.pair_dict[current_strand_id].append(
             prev_strand_id)
          key21="%d:%d" %(current_strand_id,prev_strand_id)
          self.user_helix_strand_segments.info_dict[key21]=\
            [first_ca_2,last_ca_2,first_ca_1,last_ca_1,is_parallel,i_index,j_index]
        prev_strand=strand
        prev_strand_as_segment=self.user_helix_strand_segments.all_strands[-1]
        prev_strand_id=len(self.user_helix_strand_segments.all_strands)-1
        prev_hierarchy=ph



    self.user_number_of_good_h_bonds,self.user_number_of_poor_h_bonds=\
     self.user_helix_strand_segments.set_up_pdb_records(models=self.user_models,
         max_h_bond_length=params.find_ss_structure.max_h_bond_length,
         force_secondary_structure_input=\
                params.input_files.force_secondary_structure_input,
         allow_ca_only_model=params.beta.allow_ca_only_model,out=local_out)
    print("\nNumber of good H-bonds: %d  Number of poor H-bonds: %d" %(
        self.user_number_of_good_h_bonds,self.user_number_of_poor_h_bonds),
          file=out)
    edited_annotation=self.user_helix_strand_segments.get_annotation()


    if self.user_helix_strand_segments.have_annotations():
      if params.input_files.force_secondary_structure_input:
        print("\nWorking PDB RECORDS (equivalent to input records):", file=out)
      else:
        print("\nInput PDB RECORDS as modified:", file=out)
      print(edited_annotation.as_pdb_or_mmcif_str(), file=out)

    if params.find_ss_structure.combine_annotations:
      print("\nMerging input and edited annotation", file=out)
      edited_annotation=self.user_helix_strand_segments.get_annotation()
      print("\nEdited annotation:", file=out)
      print(edited_annotation.as_pdb_or_mmcif_str(), file=out)

      print("\nUser annotation:", file=out)
      print(user_annotation.as_pdb_or_mmcif_str(), file=out)

      combined_annotation=edited_annotation.combine_annotations(
        hierarchy=hierarchy, other=user_annotation) # will take edited if equal
      if combined_annotation:
        print("\nMerged annotation (input and edited input annotation):\n", file=out)
        print(combined_annotation.as_pdb_or_mmcif_str(), file=out)

      return combined_annotation
    else:
      return edited_annotation

  def find_ss_in_model(self,params=None,model=None,out=sys.stdout):
    """Find secondary structure in model"""
    previously_used_residues=[]
    if params.find_ss_structure.find_alpha:
      model.find_alpha=find_helix(params=params.alpha,
        model=model,verbose=params.control.verbose,
        make_unique=params.find_ss_structure.make_unique,
        extract_segments_from_pdb=params.extract_segments_from_pdb.extract,
        cut_up_segments=params.find_ss_structure.cut_up_segments,
        extend_segments=params.find_ss_structure.extend_segments,
        previously_used_residues=previously_used_residues,
        out=out)
      if params.find_ss_structure.exclude_alpha_in_beta:
        previously_used_residues+=model.find_alpha.get_used_residues_list()

    if params.find_ss_structure.find_three_ten:
      model.find_three_ten=find_helix(params=params.three_ten,
        model=model,verbose=params.control.verbose,
        make_unique=params.find_ss_structure.make_unique,
        extract_segments_from_pdb=params.extract_segments_from_pdb.extract,
        cut_up_segments=params.find_ss_structure.cut_up_segments,
        extend_segments=params.find_ss_structure.extend_segments,
        previously_used_residues=previously_used_residues,
        out=out)
      if params.find_ss_structure.exclude_alpha_in_beta:
        previously_used_residues+=model.find_three_ten.get_used_residues_list()

    if params.find_ss_structure.find_pi:
      model.find_pi=find_helix(params=params.pi,
        model=model,verbose=params.control.verbose,
        make_unique=params.find_ss_structure.make_unique,
        extract_segments_from_pdb=params.extract_segments_from_pdb.extract,
        cut_up_segments=params.find_ss_structure.cut_up_segments,
        extend_segments=params.find_ss_structure.extend_segments,
        previously_used_residues=previously_used_residues,
        out=out)
      if params.find_ss_structure.exclude_alpha_in_beta:
        previously_used_residues+=model.find_pi.get_used_residues_list()

    self.beta_list_by_model=[]
    if params.find_ss_structure.find_beta:

      model.find_beta=find_beta_strand(params=params.beta,
        model=model,verbose=params.control.verbose,
        make_unique=params.find_ss_structure.make_unique,
        extract_segments_from_pdb=params.extract_segments_from_pdb.extract,
        cut_up_segments=params.find_ss_structure.cut_up_segments,
        extend_segments=params.find_ss_structure.extend_segments,
        previously_used_residues=previously_used_residues,
        out=out)
      if params.find_ss_structure.exclude_alpha_in_beta:
        previously_used_residues+=model.find_beta.get_used_residues_list()

    if params.find_ss_structure.make_unique: # do this before finding other
      # get unique residues in alpha, beta if desired
      self.make_unique(model)

    if params.find_ss_structure.find_other:
      model.find_other=find_other_structure(params=params.other,
        model=model,verbose=params.control.verbose,
        make_unique=params.find_ss_structure.make_unique,
        extract_segments_from_pdb=params.extract_segments_from_pdb.extract,
        cut_up_segments=params.find_ss_structure.cut_up_segments,
        extend_segments=params.find_ss_structure.extend_segments,
        previously_used_residues=previously_used_residues,
        out=out)

  def make_unique(self,model):
    """Iteratively work down residues in this model starting with helix, then
    strand, then anything left
    trim in from end if overlapping with previous, delete to nearest end
    if a used residue in the middle."""

    is_used_list=model.length()*[None]
    first_res=model.first_residue()

    for find_group in ['find_alpha','find_three_ten','find_pi',
         'find_beta','find_other']:
      fg=getattr(model,find_group)
      if not fg: continue
      new_segment_list=[]
      for s in fg.segments:
        start_resno=s.get_start_resno()
        first_pos=start_resno-first_res
        last_pos=s.get_end_resno()-first_res
        already_used=is_used_list[first_pos:last_pos+1]

        if not True in already_used:  # cross of used ones
           new_start=0
           new_end=len(already_used)-1
        else:
          # find range of ok positions...
          new_start,new_end=self.get_start_end(already_used=already_used)
          if new_start is None: # delete it
            continue
          s.trim_ends(start_pos=new_start,end_pos=new_end)
          if not s.is_ok():
            continue

        # mark used residues
        for i in range(first_pos+new_start,first_pos+new_end+1):
          is_used_list[i]=True

        #save it
        new_segment_list.append(s) # keep it if we get this far
      fg.segments=new_segment_list


  def get_start_end(self,already_used=None):
    """Goes from first available to end of
       available (not necessarily optimal)"""
    new_start=None
    new_end=None
    for i in range(len(already_used)):
      if already_used[i]:
        if new_end is not None:
          return new_start,new_end
        else:
          pass # have not yet started
      else:
        if new_end is None:
          new_start=i
          new_end=i
        else:
          new_end=i
    return new_start,new_end

  def get_params(self,args,out=sys.stdout):
    """Get params from args"""
    command_line = iotbx.phil.process_command_line_with_files(
      args=args,
      master_phil=master_phil,
      pdb_file_def="input_files.pdb_in")

    params = command_line.work.extract()
    print("\nFind secondary structure in hierarchy", file=out)
    master_phil.format(python_object=params).show(out=out)
    return params


if __name__=="__main__":
  args=sys.argv[1:]
  fss=find_secondary_structure(args=args,out=sys.stdout)

  """
  # How to get cctbx helix/sheet objects:
  alpha_helices=fss.get_pdb_alpha_helix_list()
  sheets=fss.get_pdb_sheet_list()
  print "\nHelix Summary"
  for helix in alpha_helices:
    print helix.as_pdb_or_mmcif_str()
  print "\nSheet Summary"
  for sheet in sheets:
    print sheet.as_pdb_or_mmcif_str()
  """


 *******************************************************************************


 *******************************************************************************
mmtbx/secondary_structure/nucleic_acids.py
from __future__ import absolute_import, division, print_function
from iotbx.pdb import common_residue_names_get_class
import sys
from scitbx.array_family import flex
import math
from iotbx.pdb import get_one_letter_rna_dna_name
from libtbx.utils import Sorry
from cctbx import geometry_restraints
from mmtbx.monomer_library import bondlength_defaults
import iotbx.phil

import six
from six.moves import range

origin_ids = geometry_restraints.linking_class.linking_class()

dna_rna_params_str = """
hbond_distance_cutoff = 3.4
  .type = float
  .short_caption = Distance cutoff for hydrogen bonds
  .help = Hydrogen bonds with length exceeding this limit will not be \
    established
scale_bonds_sigma = 1.
  .type = float
  .short_caption = Scale h-bond sigma
  .help = All sigmas for h-bond length will be multiplied \
    by this number. The smaller number is tighter restraints.
  .expert_level = 3

base_pair
  .multiple = True
  .style = noauto
{
  enabled = True
    .type = bool
    .help = Restraint this particular base-pair
  base1 = None
    .type = atom_selection
    .help = Selection string selecting at least one atom in the desired residue
  base2 = None
    .type = atom_selection
    .help = Selection string selecting at least one atom in the desired residue
  saenger_class = 0
    .type = int
    .optional = True
    .caption = Saenger number of basepairing type, 0 if unknown
    .help = Type of base-pairing
  restrain_planarity = False
    .type = bool
    .help = Apply planarity restraint to this base-pair
  planarity_sigma = 0.176
    .type = float
  restrain_hbonds = True
    .type = bool
    .help = Restrain hydrogen bonds length for this base-pair
  restrain_hb_angles = True
    .type = bool
    .help = Restrain angles around hydrogen bonds for this base-pair
  restrain_parallelity = True
    .type = bool
    .help = Apply parallelity restraint to this base-pair
  parallelity_target = 0
    .type = float
  parallelity_sigma = 0.0335
    .type = float
}
stacking_pair
  .multiple = True
  .style = noauto
{
  enabled = True
    .type = bool
    .help = Restraint this particular base-pair
  base1 = None
    .type = atom_selection
    .help = Selection string selecting at least one atom in the desired residue
  base2 = None
    .type = atom_selection
    .help = Selection string selecting at least one atom in the desired residue
  angle = 0
    .type = float
  sigma = 0.027
    .type = float
}
"""

def output_hbonds(hbond_proxies, pdb_atoms):
  # actually just to save the piece of code.
  if hbond_proxies is not None:
    dashes = open('dashes.pml', 'w')
    for p in hbond_proxies:
      awl1 = pdb_atoms[p.i_seqs[0]].fetch_labels()
      awl2 = pdb_atoms[p.i_seqs[1]].fetch_labels()
      ps = "dist chain \"%s\" and resi %s and name %s, chain \"%s\" and resi %s and name %s\n" % (
        awl1.chain_id, awl1.resseq, awl1.name, awl2.chain_id, awl2.resseq, awl2.name)
      dashes.write(ps)
    dashes.close()


def additional_check_Gendron_2001(r_i, r_j):
  # distance between rings < 5.5A
  ring_i = []
  ring_j = []
  center_i = flex.vec3_double([[0,0,0]])
  center_j = flex.vec3_double([[0,0,0]])
  for ring, res, center in [(ring_i, r_i, center_i),
                          (ring_j, r_j, center_j)]:
    for atom_name in [" N1 ", " C2 ", " N3 ", " C4 ", " C5 ", " C6 ",
                      " N9 ", " C8 ", " N7 "]:
      atom = res.find_atom_by(name=atom_name)
      if atom is not None:
        center += flex.vec3_double([atom.xyz])
        ring.append(flex.vec3_double([atom.xyz]))
    if len(ring) > 2:
      center *= 1./float(len(ring))
    else:
      return False
  d = center_j-center_i
  dn = d.norm()
  # angle between 2 normals is < 30 degrees
  r_i1 = ring_i[0]-center_i
  r_i2 = ring_i[1]-center_i
  n_i = r_i1.cross(r_i2)
  r_j1 = ring_j[0]-center_j
  r_j2 = ring_j[1]-center_j
  n_j = r_j1.cross(r_j2)
  cos_ni_nj = n_i.dot(n_j)/n_i.norm()/n_j.norm()
  angle_ni_nj_degrees = math.degrees(math.acos(abs(cos_ni_nj[0])))
  # angle between center line and normal
  cos_d_ni =  d.dot(n_i)/dn/n_i.norm()
  angle_d_ni_degrees = math.degrees(math.acos(abs(cos_d_ni[0])))
  result = (dn < 5.5 and angle_ni_nj_degrees < 30 and
    angle_d_ni_degrees < 40)
  return result

def format_base_string(base_str, residue, segid=None):
  segid_add = "and segid '%s'" % segid if segid is not None else ""
  resid = "%s" % (
      residue.resid() if hasattr(residue, "resid") else residue.parent().resid())
  chain_add = "chain '%s' and " % residue.parent().parent().id
  base = "%s = %s resid %s %s\n" % (base_str, chain_add,
      resid, segid_add)
  return base

def make_phil_stacking_pair_record(residue1, residue2, params=None,
    add_segid=None, nesting_depth=1):
  res = "%sstacking_pair {\n" % ("  "*nesting_depth)
  res += "%s%s" % ("  "*(nesting_depth+1), format_base_string(
      "base1", residue1, add_segid))
  res += "%s%s" % ("  "*(nesting_depth+1), format_base_string(
      "base2", residue2, add_segid))
  # add non-defaults!
  if params is not None and len(params.stacking_pair) > 0:
    master_phil = iotbx.phil.parse(dna_rna_params_str)
    actual_params = master_phil.format(params)
    w_phil = master_phil.fetch_diff(actual_params).extract()
    if hasattr(w_phil, 'stacking_pair'):
      for k, v in six.iteritems(w_phil.stacking_pair[0].__dict__):
        if not k.startswith('_'):
          res += "%s%s = %s\n" % ("  "*(nesting_depth+1), k, str(v))
  res += "%s}\n" % ("  "*nesting_depth)
  return res

def make_phil_base_pair_record(residue1, residue2, params=None,
    saenger_class=None, add_segid=None, nesting_depth=1):
  res = "%sbase_pair {\n" % ("  "*nesting_depth)
  res += "%s%s" % ("  "*(nesting_depth+1), format_base_string(
      "base1", residue1, add_segid))
  res += "%s%s" % ("  "*(nesting_depth+1), format_base_string(
      "base2", residue2, add_segid))
  if saenger_class is not None:
    res += "%ssaenger_class = %d\n" % ("  "*(nesting_depth+1), saenger_class)
  # add non-defaults!
  if params is not None and len(params.base_pair) > 0:
    master_phil = iotbx.phil.parse(dna_rna_params_str)
    actual_params = master_phil.format(params)
    w_phil = master_phil.fetch_diff(actual_params).extract()
    if hasattr(w_phil, 'base_pair'):
      for k, v in six.iteritems(w_phil.base_pair[0].__dict__):
        if not k.startswith('_'):
          res += "%s%s = %s\n" % ("  "*(nesting_depth+1), k, str(v))
  res += "%s}\n" % ("  "*nesting_depth)
  return res

def get_phil_stacking_pairs(pdb_hierarchy, skip_gendron_check=False,
    prefix=None, params=None, log=sys.stdout, add_segid=None):
  pairs = []
  for model in pdb_hierarchy.models():
    for chain in model.chains():
      if chain.is_na():
        for conformer in chain.conformers():
          prev_res = None
          cur_res = None
          for i_residue, residue in enumerate(conformer.residues()):
            prev_res = cur_res
            cur_res = residue
            if prev_res is not None and cur_res is not None:
              if (not skip_gendron_check and
                  additional_check_Gendron_2001(prev_res, cur_res)):
                p = make_phil_stacking_pair_record(prev_res, cur_res, params, add_segid)
                pairs.append(p)
  phil_str = ""
  for pair_phil in pairs:
    phil_str += pair_phil
  result = ""
  if prefix is not None:
    result = "%s {\n%s}" % (prefix, phil_str)
  else:
    result = phil_str
  return result

def consecutive_residues(atom1, atom2):
  awl1 = atom1.fetch_labels()
  awl2 = atom2.fetch_labels()
  if ((awl1.chain_id == awl2.chain_id) and
      abs(awl1.resseq_as_int() - awl2.resseq_as_int()) < 2 ):
    return True
  return False

def unify_residue_names_and_order(r1, r2):
  r1n = get_one_letter_rna_dna_name(r1.resname)
  r2n = get_one_letter_rna_dna_name(r2.resname)

  message_template = "Residue with name '%s' cannot be processed "
  message_template += "automatically \nfor nucleic acid basepair restraints "
  message_template += "because of non-standard residue name."
  if r1n is None:
    raise Sorry(message_template % r1.resname)
  if r2n is None:
    raise Sorry(message_template % r2.resname)
  # Translate DNA resname to RNA for unification
  # RNA
  if r1n > r2n:
    t = r1
    r1 = r2
    r2 = t
    t = r1n
    r1n = r2n
    r2n = t
  r1n = 'U' if r1n == "T" else r1n
  r2n = 'U' if r2n == "T" else r2n
  return r1, r2, r1n, r2n

def get_h_bonds_for_basepair(a1, a2, distance_cutoff=100, log=sys.stdout, verbose=-1):
  # a1, a2.parent().atom_group_size have to  == 1
  new_hbonds = []
  r1 = a1.parent()
  r2 = a2.parent()
  r1, r2, r1n, r2n = unify_residue_names_and_order(a1.parent(), a2.parent())
  best_possible_link_list = []
  best_score = 100.
  best_class_number = None
  for class_number, data in six.iteritems(bondlength_defaults.basepairs_lengths):
    d1 = 0
    if (r1n, r2n) == data[0]:
      for l in data[1:]:
        a1 = r1.get_atom(l[0])
        a2 = r2.get_atom(l[1])
        # Bug notification: a2 could be None: bug report form
        # ms@mrc-lmb.cam.ac.uk on Nov 30, 2015, file was not provided.
        # UPD. a1 also could be None, investigated and improved potential
        # hbond filtering procedure. Hopefully, this will resolve the issue.
        if a1 is None or a2 is None:
          missing_atom = l[0] if a1 is None else l[1]
          missing_res = r1 if a1 is None else r2
          print("Warning! %s atom is missing from residue %s " % (
              missing_atom, missing_res.id_str()), file=log)
          if verbose > 1:
            print("Atoms present in the residue:", file=log)
            for a in missing_res.atoms():
              print(a.id_str(), file=log)
          print("  Was trying to link: %s%s with %s%s, Saenger class: %d" % (
              r1.id_str(), l[0], r2.id_str(), l[1], class_number), file=log)
          # a1_id = a1.id_str() if a1 is not None else "None"
          # a2_id = a2.id_str() if a2 is not None else "None"
          # msg = "Something is wrong in .pdb file around '%s' or '%s'.\n" % (
          #     a1_id, a2_id)
          # msg += "If it is not clear to you, please contact developers and "
          # msg += "supply above message and .pdb file used."
          continue
          # raise Sorry(msg)
        d1 += abs(a1.distance(a2)-2.89)
      n_links_in_data = len(data[1:])
      if n_links_in_data < 1:
        raise Sorry("Corrupted dictionary in bondlength_defaults.py")
      d1 /=n_links_in_data
      if verbose > 2:
        print("  Class %d penalty=%.3f" % (class_number, d1), file=log)
      if best_score > d1:
        best_possible_link_list = [(x[:2]) for x in data[1:]]
        best_score = d1
        best_class_number = class_number
  for n1, n2 in best_possible_link_list:
    a1 = r1.get_atom(n1)
    a2 = r2.get_atom(n2)
    if verbose > 2:
      print("    %s --> %s distance = %.3f" % (
          a1.id_str(), a2.id_str(), a1.distance(a2)), file=log)
    if a1 is not None and a2 is not None and a1.distance(a2)<distance_cutoff:
      new_hbonds.append(tuple([a1, a2] if a1.i_seq<a2.i_seq else [a2, a1]))
      # new_hbonds.append(tuple(sorted([a1.i_seq, a2.i_seq])))
  return new_hbonds, best_class_number

def final_link_direction_check(atom1, atom2, rna_dna_angle_cutoff=35):
  import math
  a1p = atom1.parent().get_atom('C4')
  a2p = atom1.parent().get_atom('C5')
  a3p = atom1.parent().get_atom('C6')
  if a1p is None or a2p is None or a3p is None:
    # this is something strange, but a user managed to get here and sent
    # bug report
    return False
  v1p = flex.vec3_double([(a1p.xyz)]) - flex.vec3_double([(a2p.xyz)])
  v2p = flex.vec3_double([(a2p.xyz)]) - flex.vec3_double([(a3p.xyz)])
  vn = v1p.cross(v2p)
  vl = flex.vec3_double([(atom2.xyz)]) - flex.vec3_double([(atom1.xyz)])
  cos_phi = vn.dot(vl)/vn.norm()/vl.norm()
  #print "cos_phi:", cos_phi[0], "phi:", math.acos(cos_phi[0])*360/math.pi, abs(cos_phi[0]) < 0.55
  # we have cosine between normal to plane group and link, and want this angle
  # to be around 90 degrees
  return 90 - math.degrees(math.acos(abs(cos_phi[0]))) < rna_dna_angle_cutoff

def get_phil_base_pairs(pdb_hierarchy, nonbonded_proxies,
    prefix=None, params=None,
    log=sys.stdout, add_segid=None, verbose=-1):
  hbond_distance_cutoff = 3.4
  if params is not None:
    hbond_distance_cutoff = params.hbond_distance_cutoff
  hbonds = []
  result = ""
  atoms = pdb_hierarchy.atoms()
  sites_cart = atoms.extract_xyz()
  get_sorted_result = nonbonded_proxies.get_sorted(
      by_value="delta",
      sites_cart=sites_cart)
  if get_sorted_result is None:
    return result
  sorted_nonb, n_not_shown = get_sorted_result

  # Get potential hbonds
  n_nonb = len(sorted_nonb)
  i = 0
  while i < n_nonb and sorted_nonb[i][3] < hbond_distance_cutoff:
    (labels, i_seq, j_seq, dist, vdw_distance, sym_op_j, rt_mx) = sorted_nonb[i]
    a1 = atoms[i_seq]
    ag1 = a1.parent()
    a2 = atoms[j_seq]
    ag2 = a2.parent()
    if (common_residue_names_get_class(ag1.resname, consider_ccp4_mon_lib_rna_dna=True) in \
          ["common_rna_dna", "ccp4_mon_lib_rna_dna"] and
        common_residue_names_get_class(ag2.resname, consider_ccp4_mon_lib_rna_dna=True) in \
          ["common_rna_dna", "ccp4_mon_lib_rna_dna"] and
        (a1.element in ["N", "O"] and a2.element in ["N", "O"]) and
        a1.name.find("P") < 0 and a2.name.find("P") < 0 and
        a1.name.find("'") < 0 and a2.name.find("'") < 0 and
        not consecutive_residues(a1, a2) and
        (ag1.altloc.strip() == ag2.altloc.strip()) and
        final_link_direction_check(a1, a2)):
      hbonds.append((i_seq, j_seq))
    i += 1
  # check and define basepairs
  pairs = []
  for hb in hbonds:
    if verbose > 1:
      print("Making pair with", atoms[hb[0]].id_str(), atoms[hb[1]].id_str(), file=log)
    new_hbonds, class_number = get_h_bonds_for_basepair(
        atoms[hb[0]],
        atoms[hb[1]],
        distance_cutoff=hbond_distance_cutoff,
        log=log,
        verbose=verbose)
    if verbose > 1:
      print("  Picked class: %d, number of h-bonds under cutoff:%d" % (class_number, len(new_hbonds)), end=' ', file=log)
    if len(new_hbonds) > 1:
      p = make_phil_base_pair_record(atoms[hb[0]].parent(), atoms[hb[1]].parent(),
          params, saenger_class=class_number, add_segid=add_segid)
      if verbose > 1:
        print("  OK", file=log)
      pairs.append(p)
    else:
      if verbose > 0:
        s = " ".join(["Basepairing for residues '%s' and '%s'" % (
            atoms[hb[0]].id_str()[10:-1], atoms[hb[1]].id_str()[10:-1]),
          "was rejected because only 1 h-bond was found"])
        if verbose > 1:
          print("Rejected", file=log)

  phil_str = ""
  # print "N basepairs:", len(pairs)
  for pair_phil in pairs:
    phil_str += pair_phil
  if prefix is not None:
    result = "%s {\n%s}" % (prefix, phil_str)
  else:
    result = phil_str
  return result

def get_plane_i_seqs_from_residues(r1, r2, grm,mon_lib_srv, plane_cache):
  # Return [([i_seqA],[j_seqA]),([i_seqB],[j_seqB])] of
  # atoms in planar groups for given atom_group r. Handles up to 2 alternative
  # conformations (the lenght of resulting array will be appropriate: A and B
  # in i_seqA i_seqB - altlocs) If there is no alt confs - the list will
  # only one tuple.
  # Uses geometry_restraints_manager that should already have basic planarity
  # restraints to find the largest plane group in residue, which should be the
  # nucleobase in case of nucleic acids.
  def convert_to_good_name(rn):
    result = rn
    if len(rn)>1 and rn[0] == 'D':
      result = rn[1]+rn[0]
    return result
  def print_warning_msg(rn):
    # print resname
    # print r.resname
    # print new_res.resname.strip()
    print("Warning, Cannot make NA restraints for %s residue (no planarity definition)" % rn)
  i_seqs = []
  result = []
  r1_i_seqs = {}
  r2_i_seqs = {}
  new_r1 = {}
  new_r2 = {}
  for r, r_i_seqs, new_r in [(r1, r1_i_seqs, new_r1), (r2, r2_i_seqs, new_r2)]:
    for conf in r.parent().conformers():
      for c_residue in conf.residues():
        if c_residue.resseq == r.parent().resseq:
          new_res = c_residue
          break
      r_i_seqs[conf.altloc] = []
      new_r[conf.altloc] = []

      # new getting i_seqs in plane
      resname = convert_to_good_name(new_res.resname.strip())
      if resname not in plane_cache:
        libdef = mon_lib_srv.get_comp_comp_id_direct(resname)
        if libdef is None:
          print_warning_msg(resname)
          continue
          # raise Sorry('Cannot make NA restraints for %s residue' % resname)
        planes = libdef.get_planes()
        if planes is not None and len(planes) > 0:
          best_index = 0
          best_len = len(planes[0].plane_atoms)
          for i in range(1, len(planes)):
            if len(planes[i].plane_atoms) > best_len:
              best_len = len(planes[i].plane_atoms)
              best_index = i
          plane_cache[resname] = planes[best_index].plane_atoms
        else:
          print_warning_msg(resname)
          continue
      # now this residue is in cache even if it wasn't before
      for plane_atom in plane_cache.get(resname,[]):
        good_atom_id = plane_atom.atom_id.replace("*","'")
        if good_atom_id == "C5M":
          good_atom_id = "C7"
        good_atom_id = " %s" % good_atom_id
        if len(good_atom_id) == 3:
          good_atom_id += " "
        a = new_res.find_atom_by(name=good_atom_id)
        if a is not None:
          new_r[conf.altloc].append(a.i_seq)
      new_r[conf.altloc] = sorted(new_r[conf.altloc])
      r_i_seqs[conf.altloc] = new_r[conf.altloc]
  if len(r1_i_seqs) > len(r2_i_seqs):
    t = r1_i_seqs
    r1_i_seqs = r2_i_seqs
    r2_i_seqs = t
  assert len(r1_i_seqs) > 0 and len(r1_i_seqs) > 0
  if len(r1_i_seqs) == 1:
    if len(r2_i_seqs) == 1:
      if (('' in r1_i_seqs or '' in r2_i_seqs)
          or (list(r1_i_seqs.keys())[0] == list(r2_i_seqs.keys())[0])):  # FIXME: indexing keys breaks compat py2/3 if more than 1 key
        result.append((r1_i_seqs[list(r1_i_seqs.keys())[0]],
                       r2_i_seqs[list(r2_i_seqs.keys())[0]]))
    else:
      if ('' in r1_i_seqs):
        for k,v in six.iteritems(r2_i_seqs):
          result.append((r1_i_seqs[''], v))
  else:
    for k, v in six.iteritems(r1_i_seqs):
      if k in r2_i_seqs:
        result.append((v, r2_i_seqs[k]))
  # check whether sets of iseqs are different
  if len(result) > 1 and result[0] == result[1]:
    # STOP()
    return [result[0]]
  return result

def get_stacking_proxies(pdb_hierarchy, stacking_phil_params, grm,
    mon_lib_srv, plane_cache):
  result = []
  assert pdb_hierarchy is not None
  if len(stacking_phil_params) < 1:
    return result
  if grm is None:
    return result
  selection_cache = pdb_hierarchy.atom_selection_cache()
  pdb_atoms = pdb_hierarchy.atoms()
  for stacking_pair in stacking_phil_params:
    if (stacking_pair.base1 is not None and stacking_pair.base2 is not None
        and stacking_pair.enabled):
      selected_atoms_1 = selection_cache.iselection(stacking_pair.base1)
      selected_atoms_2 = selection_cache.iselection(stacking_pair.base2)
      if len(selected_atoms_1) == 0:
        raise Sorry("Selection %s in stacking_pair resulted in 0 atoms." % (
            stacking_pair.base1))
      if len(selected_atoms_2) == 0:
        raise Sorry("Selection %s in stacking_pair resulted in 0 atoms." % (
            stacking_pair.base2))
      a1 = pdb_atoms[selected_atoms_1[0]]
      a2 = pdb_atoms[selected_atoms_2[0]]
      r1 = a1.parent()
      r2 = a2.parent()
      seqs = get_plane_i_seqs_from_residues(r1, r2, grm, mon_lib_srv, plane_cache)
      for i_seqs, j_seqs in seqs:
        if len(i_seqs) > 2 and len(j_seqs) > 2:
          if stacking_pair.sigma < 1e-5:
            raise Sorry("Sigma for stacking restraints should be > 1e-5")
          proxy=geometry_restraints.parallelity_proxy(
            i_seqs=flex.size_t(i_seqs),
            j_seqs=flex.size_t(j_seqs),
            weight=1/(stacking_pair.sigma**2),
            target_angle_deg=0,
            slack=0,
            top_out=False,
            limit=1,
            origin_id=origin_ids.get_origin_id('basepair stacking'))
          result.append(proxy)
  return result

def get_hb_lenght_targets(atoms):
  restraint_values = { 'N6 O4' : (3.00, 0.11),
                       'O6 N4' : (2.93, 0.10),
                       'N2 O2' : (2.78, 0.10)
  }
  anames = [atoms[0].name.strip(),
            atoms[1].name.strip()]
  rnames = [atoms[0].parent().resname,
            atoms[1].parent().resname]
  if sorted(anames) == ['N1', 'N3']:
    if get_one_letter_rna_dna_name(rnames[0]) in ['G', 'C']:
      return (2.88, 0.07) # G-C
    else:
      return (2.82, 0.08) # A-T
  else:
    key1 = "%s %s" % (anames[0], anames[1])
    key2 = "%s %s" % (anames[1], anames[0])
    vals = restraint_values.get(key1, None)
    if vals is not None:
      return vals
    vals = restraint_values.get(key2, None)
    if vals is not None:
      return vals
  return (2.91, 0.15) # values for all other bonds

def get_angle_proxies_for_bond(atoms):
  angle_values = {'O6 N4': [(122.8, 3.00), (117.3, 2.86)],
                  'N4 O6': [(117.3, 2.86), (122.8, 3.00)],
                  'N2 O2': [(122.2, 2.88), (120.7, 2.20)],
                  'O2 N2': [(120.7, 2.20), (122.2, 2.88)],
                  'N6 O4': [(115.6, 8.34), (121.2, 4.22)],
                  'O4 N6': [(121.2, 4.22), (115.6, 8.34)]}
  proxies = []
  anames = [atoms[0].name.strip(),
            atoms[1].name.strip()]
  rnames = [atoms[0].parent().resname,
            atoms[1].parent().resname]
  if sorted(anames) == ['N1', 'N3']:
    if get_one_letter_rna_dna_name(rnames[0]) in ['G', 'C']:
      if anames[0] == 'N1':
        vals = [(119.1, 2.59), (116.3, 2.66)]
      else:
        vals = [(116.3, 2.66), (119.1, 2.59)]
    else:
      if anames[0] == 'N1':
        vals = [(116.2, 3.46), (115.8, 2.88)]
      else:
        vals = [(115.8, 2.88), (116.2, 3.46)]
  else:
    key = "%s %s" % (anames[0], anames[1])
    vals = angle_values.get(key, None)
  if vals is not None:
    for i in range(2):
      atoms_for_angle = [None, None, None]
      aname = anames[i]
      if (aname == 'N1' or aname == 'N2' or aname == 'N3' or aname == 'O2'):
        atoms_for_angle[0] = atoms[i].parent().get_atom('C2')
      elif (aname == 'N4' or aname == 'O4'):
        atoms_for_angle[0] = atoms[i].parent().get_atom('C4')
      elif (aname == 'N6' or aname == 'O6'):
        atoms_for_angle[0] = atoms[i].parent().get_atom('C6')
      if atoms_for_angle[0] is not None:
        atoms_for_angle[1] = atoms[i]
        atoms_for_angle[2] = atoms[1-i]
      if atoms_for_angle.count(None) == 0:
        i_seqs_for_angle = [x.i_seq for x in atoms_for_angle]
        p = geometry_restraints.angle_proxy(
          i_seqs=i_seqs_for_angle,
          angle_ideal=vals[i][0],
          weight=1./vals[i][1]**2,
          origin_id=origin_ids.get_origin_id('hydrogen bonds'))
        proxies.append(p)
  return proxies

def get_h_bonds_for_particular_basepair(atoms, saenger_class=0):
  result = []
  if saenger_class == 0:
    return result
  if saenger_class == 16:
    # We don't have values for this one.
    return result
  assert len(atoms) == 2
  new_hbonds = []
  r1, r2, r1n, r2n = unify_residue_names_and_order(
      atoms[0].parent(), atoms[1].parent())
  if bondlength_defaults.basepairs_lengths.get(saenger_class, None) == None:
    raise Sorry("""
Bad or unknown Saenger class. Presently we don't have enough
data to support Saenger class #16. """)
  if bondlength_defaults.basepairs_lengths[saenger_class][0] != (r1n, r2n):
    print(bondlength_defaults.basepairs_lengths[saenger_class][0], r1n, r2n,saenger_class)
    print(r1.id_str(), r2.id_str())
    raise Sorry("Saenger class does not match residue names")
  hbonds = []
  for b in bondlength_defaults.basepairs_lengths[saenger_class][1:]:
    hba1 = r1.get_atom(b[0])
    hba2 = r2.get_atom(b[1])
    hbonds.append((hba1, hba2))
  return hbonds

def get_basepair_proxies(
    pdb_hierarchy,
    bp_phil_params,
    grm,
    mon_lib_srv,
    plane_cache,
    hbond_distance_cutoff=3.4,
    scale_bonds_sigma=1.):
  assert pdb_hierarchy is not None
  bond_proxies_result = []
  angle_proxies_result = []
  result_planarities = []
  result_parallelities = []
  if len(bp_phil_params) < 1:
    return bond_proxies_result, angle_proxies_result, result_planarities, result_parallelities
  if grm is None:
    return bond_proxies_result, angle_proxies_result, result_planarities, result_parallelities
  selection_cache = pdb_hierarchy.atom_selection_cache()
  pdb_atoms = pdb_hierarchy.atoms()
  for base_pair in bp_phil_params:
    if (base_pair.base1 is not None and base_pair.base2 is not None
        and base_pair.enabled):
      selected_atoms_1 = selection_cache.iselection(base_pair.base1)
      selected_atoms_2 = selection_cache.iselection(base_pair.base2)
      if len(selected_atoms_1) == 0:
        raise Sorry("Selection %s in base_pair resulted in 0 atoms." % (
            base_pair.base1))
      if len(selected_atoms_2) == 0:
        raise Sorry("Selection %s in base_pair resulted in 0 atoms." % (
            base_pair.base2))
      a1 = pdb_atoms[selected_atoms_1[0]]
      a2 = pdb_atoms[selected_atoms_2[0]]
      # get hbonds
      bp_proxies, ap_proxies = get_bp_hbond_proxies(
          a1, a2, base_pair, hbond_distance_cutoff, scale_bonds_sigma)
      bond_proxies_result += bp_proxies
      angle_proxies_result += ap_proxies
      # get planarity/parallelity
      plan_p, parr_p = get_bp_plan_proxies(
          a1, a2, base_pair, grm, mon_lib_srv, plane_cache)
      result_planarities += plan_p
      result_parallelities += parr_p
  return bond_proxies_result, angle_proxies_result, result_planarities, result_parallelities

def get_bp_hbond_proxies(a1, a2, base_pair, hbond_distance_cutoff,
    scale_bonds_sigma):
  bp_result = []
  ap_result = []
  if base_pair.saenger_class == 0:
    hbonds, saenger_class = get_h_bonds_for_basepair(
      a1, a2, distance_cutoff=hbond_distance_cutoff,
      log=sys.stdout, verbose=-1)
    base_pair.saenger_class = saenger_class
  hbonds = get_h_bonds_for_particular_basepair((a1, a2), base_pair.saenger_class)
  for hb in hbonds:
    if hb[0] is None or hb[1] is None:
      print("NA hbond rejected because one of the atoms is absent")
      continue
    dist = hb[0].distance(hb[1])
    if dist < hbond_distance_cutoff:
      if base_pair.restrain_hbonds:
        hb_target, hb_sigma = get_hb_lenght_targets(hb)
        p = geometry_restraints.bond_simple_proxy(
          i_seqs=[hb[0].i_seq, hb[1].i_seq],
          distance_ideal=hb_target,
          weight=1.0/(hb_sigma*scale_bonds_sigma)**2,
          slack=0,
          top_out=False,
          limit=1,
          origin_id=origin_ids.get_origin_id('hydrogen bonds'))
        bp_result.append(p)
        # print "bond:", hb[0].id_str(), hb[1].id_str(), "(%4.2f, %4.2f)" % (hb_target, hb_sigma)
        # s1 = pdb_atoms[hb[0].i_seq].id_str()
        # s2 = pdb_atoms[hb[1].i_seq].id_str()
        # ps = "dist chain \"%s\" and resi %s and name %s, chain \"%s\" and resi %s and name %s\n" % (
        #   s1[14:15], s1[15:19], s1[5:8], s2[14:15], s2[15:19], s2[5:8])
        # dashes.write(ps)
      if base_pair.restrain_hb_angles:
        ap_result += get_angle_proxies_for_bond(hb)
    else:
      # Use log channel!
      pass
      #print("NA hbond rejected:",hb[0].id_str(), hb[1].id_str(), "distance=%.2f" % dist)
  return bp_result, ap_result

def get_bp_plan_proxies(a1, a2, base_pair, grm, mon_lib_srv, plane_cache):
  result_plan_p = []
  result_parr_p = []
  seqs = get_plane_i_seqs_from_residues(
      a1.parent(), a2.parent(), grm,mon_lib_srv, plane_cache)
  for i_seqs, j_seqs in seqs:
    if len(i_seqs) > 2 and len(j_seqs) > 2:
      if base_pair.restrain_parallelity:
        if base_pair.parallelity_sigma < 1e-5:
          raise Sorry("Sigma for parallelity basepair restraints should be > 1e-5")
        proxy=geometry_restraints.parallelity_proxy(
          i_seqs=flex.size_t(i_seqs),
          j_seqs=flex.size_t(j_seqs),
          weight=1/(base_pair.parallelity_sigma**2),
          target_angle_deg=0,
          slack=0,
          top_out=False,
          limit=1,
          origin_id=origin_ids.get_origin_id('basepair parallelity'))
        result_parr_p.append(proxy)
      if base_pair.restrain_planarity:
        if base_pair.planarity_sigma < 1e-5:
          raise Sorry("Sigma for planarity basepair restraints should be > 1e-5")
        w = 1./(base_pair.planarity_sigma**2)
        proxy=geometry_restraints.planarity_proxy(
          i_seqs=flex.size_t(i_seqs+j_seqs),
          weights=[w]*len(i_seqs+j_seqs),
          origin_id=origin_ids.get_origin_id('basepair planarity'))
        result_plan_p.append(proxy)
  return result_plan_p, result_parr_p


 *******************************************************************************


 *******************************************************************************
mmtbx/secondary_structure/proteins.py
from __future__ import absolute_import, division, print_function
import iotbx.phil
from libtbx.utils import Sorry
from math import sqrt
from cctbx import geometry_restraints
from cctbx.geometry_restraints import linking_class
from cctbx.array_family import flex
import sys
from six.moves import range

origin_ids = linking_class.linking_class()

helix_group_params_str = """
helix
  .multiple = True
  .optional = True
  .style = noauto
{
  serial_number = None
    .type = str
    .optional = True
  helix_identifier = None
    .type = str
    .optional = True
  enabled = True
    .type = bool
    .help = Restrain this particular helix
  selection = None
    .type = atom_selection
    .style = bold
  helix_type = *alpha pi 3_10 unknown
    .type = choice
    .help = Type of helix, defaults to alpha.  Only alpha, pi, and 3_10 \
      helices are used for hydrogen-bond restraints.
    .style = bold
  sigma = 0.05
    .type = float
  slack = 0
    .type = float
  top_out = False
    .type = bool
  angle_sigma_scale = 1
    .type = float
    .help = Multiply sigmas for h-bond angles by this value. Original sigmas \
      range from 5 to 10.
  angle_sigma_set = None
    .type = float
    .help = Use this parameter to set sigmas for h-bond angles to a particular \
      value
  hbond
    .multiple = True
    .optional = True
    .style = noauto
  {
    donor = None
      .type = atom_selection
    acceptor = None
      .type = atom_selection
  }
}"""

sheet_group_params_str = """
sheet
  .multiple = True
  .optional = True
  .style = noauto
{
  enabled = True
    .type = bool
    .help = Restrain this particular sheet
  first_strand = None
    .type = atom_selection
    .style = bold
  sheet_id = None
    .type = str
  strand
    .multiple = True
    .optional = True
  {
    selection = None
      .type = atom_selection
      .style = bold
    sense = parallel antiparallel *unknown
      .type = choice
      .style = bold
    bond_start_current = None
      .type = atom_selection
      .style = bold
    bond_start_previous = None
      .type = atom_selection
      .style = bold
  }
  sigma = 0.05
    .type = float
  slack = 0
    .type = float
  top_out = False
    .type = bool
  angle_sigma_scale = 1
    .type = float
    .help = Multiply sigmas for h-bond angles by this value. Original sigmas \
      range from 5 to 10.
  angle_sigma_set = None
    .type = float
    .help = Use this parameter to set sigmas for h-bond angles to a particular \
      value
  hbond
    .multiple = True
    .optional = True
    .style = noauto
  {
    donor = None
      .type = atom_selection
    acceptor = None
      .type = atom_selection
  }
}
"""

master_helix_phil = iotbx.phil.parse(helix_group_params_str)

# [{alpha outside}, {alpha inside}, {beta}]
angle_restraints_values = [
  {"C"  : (155, 10),
   "CA" : (116, 10),
   "C-1": (121, 10)},
  {"C"  : (155, 5),
   "CA" : (116, 5),
   "C-1": (121, 5)},
  {"C"  : (155, 9),
   "CA" : (124, 7),
   "C-1": (113, 6)},
  ]

def _ac_match(a1, a2):
  altloc1 = a1.parent().altloc
  altloc2 = a2.parent().altloc
  if altloc1.strip() == "" or altloc2.strip() == "":
    return True
  if altloc1.strip() == altloc2.strip():
    return True
  return False

def _create_hbond_angles_proxies(
    N_atom,
    O_atom,
    prev_atoms,
    angle_restraint_type=0, # 0-No restraint, 1-outside, 2-inside, 3-beta
    angle_sigma_scale=1.,
    angle_sigma_set=None,
    ):
  result = []
  if angle_restraint_type == 0:
    return result
  # print "Making angles for bond:", N_atom.id_str(), "---", O_atom.id_str()
  # print "prev_atoms:"
  # for a in prev_atoms:
  #   print "  ", a.id_str()
  C_atoms = []
  CA_atoms = []
  C_1_atoms = []
  # print "  C_atoms:",
  for atom in O_atom.parent().atoms():
    if atom.name == " C  " and _ac_match(O_atom, atom):
      C_atoms.append(atom)
      # print "    ",atom.id_str()
  # print "  CA_atoms:",
  for atom in N_atom.parent().atoms():
    if atom.name == " CA " and _ac_match(N_atom, atom):
      CA_atoms.append(atom)
      # print "    ",atom.id_str()
  # print "  C_1_atoms:",
  for atom in prev_atoms:
    if atom.name == " C  " and _ac_match(N_atom, atom):
      C_1_atoms.append(atom)
      # print "    ",atom.id_str()
  # building angle restraints
  for C_atom in C_atoms:
    if angle_sigma_set is not None:
      sigma = angle_sigma_set
    else:
      sigma = angle_restraints_values[angle_restraint_type-1]["C"][1] * angle_sigma_scale
    p = geometry_restraints.angle_proxy(
        i_seqs=[C_atom.i_seq, O_atom.i_seq, N_atom.i_seq],
        angle_ideal=angle_restraints_values[angle_restraint_type-1]["C"][0],
        weight=1./sigma**2,
        origin_id=origin_ids.get_origin_id('hydrogen bonds'))
    result.append(p)
  for CA_atom in CA_atoms:
    if angle_sigma_set is not None:
      sigma = angle_sigma_set
    else:
      sigma = angle_restraints_values[angle_restraint_type-1]["CA"][1] * angle_sigma_scale
    p = geometry_restraints.angle_proxy(
        i_seqs=[CA_atom.i_seq, N_atom.i_seq, O_atom.i_seq],
        angle_ideal=angle_restraints_values[angle_restraint_type-1]["CA"][0],
        weight=1./sigma**2,
        origin_id=origin_ids.get_origin_id('hydrogen bonds'))
    result.append(p)
  for C_1_atom in C_1_atoms:
    if angle_sigma_set is not None:
      sigma = angle_sigma_set
    else:
      sigma = angle_restraints_values[angle_restraint_type-1]["C-1"][1] * angle_sigma_scale
    p = geometry_restraints.angle_proxy(
        i_seqs=[C_1_atom.i_seq, N_atom.i_seq, O_atom.i_seq],
        angle_ideal=angle_restraints_values[angle_restraint_type-1]["C-1"][0],
        weight=1./sigma**2,
        origin_id=origin_ids.get_origin_id('hydrogen bonds'))
    result.append(p)
  return result

def _create_hbond_proxy(
    acceptor_atoms,
    donor_atoms,
    hbond_counts,
    distance_ideal,
    distance_cut,
    remove_outliers,
    prev_atoms=None,
    angle_restraint_type=0, # 0-No restraint, 1-outside, 2-inside
    weight=1.0,
    sigma=None,
    slack=None,
    angle_sigma_scale=1.,
    angle_sigma_set=None,
    top_out=False,
    log=sys.stdout):
  assert sigma is not None
  assert slack is not None
  donors = []
  acceptors = []
  for atom in acceptor_atoms :
    if (atom.name == ' O  '):
      acceptors.append(atom)
  for atom in donor_atoms :
    if (atom.name == ' N  '):
      donors.append(atom)
  result = []
  angle_proxies = []
  if len(donors) > 0 and len(acceptors) > 0:
    # make pairs of connecting atoms
    donor_acceptor_pairs = []
    if len(donors) == 1:
      for acc in acceptors:
        donor_acceptor_pairs.append((donors[0], acc))
    elif len(donors) == 2 and len(acceptors) == 2:
      donor_acceptor_pairs.append((donors[0], acceptors[0]))
      donor_acceptor_pairs.append((donors[1], acceptors[1]))
    elif len(donors) == 2 and len(acceptors) == 1:
      for donor in donors:
        donor_acceptor_pairs.append((donor, acceptors[0]))
    for donor, acceptor in donor_acceptor_pairs:
      # print "  linking:", donor.id_str(), acceptor.id_str()
      if (hbond_counts[donor.i_seq] > 0):
        print("      WARNING: donor atom is already bonded, skipping", file=log)
        print("    %s" % donor_labels.id_str(), file=log)
        return result, angle_proxies
      elif (hbond_counts[acceptor.i_seq] > 0):
        print("      WARNING: acceptor atom is already bonded, skipping", file=log)
        print("    %s" % acceptor_labels.id_str(), file=log)
        return result, angle_proxies
      if (remove_outliers) and (distance_cut > 0):
        dist = donor.distance(acceptor)
        if (dist > distance_cut):
          print("      removed outlier: %.3fA  %s --> %s (cutoff:%.3fA)"%(
              dist, donor.id_str(), acceptor.id_str(), distance_cut), file=log)
          return result, angle_proxies
        if dist > 10:
          print("      removed unreasonable: %.3fA  %s --> %s (cutoff:%.3fA)"%(
              dist, donor.id_str(), acceptor.id_str(), distance_cut), file=log)
          return results, angle_proxies
      limit = -1
      if (top_out):
        limit = (distance_cut - distance_ideal)**2 * weight/(sigma**2)
        print("limit: %.2f" % limit)
      proxy = geometry_restraints.bond_simple_proxy(
        i_seqs=(donor.i_seq, acceptor.i_seq),
        distance_ideal=distance_ideal,
        weight=weight/(sigma ** 2),
        slack=slack,
        top_out=top_out,
        limit=limit,
        origin_id=origin_ids.get_origin_id('hydrogen bonds'))
      result.append(proxy)
      if angle_restraint_type != 0 and prev_atoms is not None:
        ap = _create_hbond_angles_proxies(
            N_atom=donor,
            O_atom=acceptor,
            prev_atoms=prev_atoms,
            angle_restraint_type=angle_restraint_type,
            angle_sigma_scale=angle_sigma_scale,
            angle_sigma_set=angle_sigma_set,
            )
        angle_proxies += ap
    return result, angle_proxies
  else :
    print("WARNING: missing atoms!", file=log)
    return result, angle_proxies

def create_helix_hydrogen_bond_proxies(
    params,
    pdb_hierarchy,
    selection_cache,
    weight,
    hbond_counts,
    distance_ideal,
    distance_cut,
    remove_outliers,
    restrain_hbond_angles,
    log=sys.stdout):
  assert (not None in [distance_ideal, distance_cut])
  generated_proxies = geometry_restraints.shared_bond_simple_proxy()
  hb_angle_proxies = []
  helix_class = params.helix_type
  if helix_class == "alpha" :
    helix_step = 4
  elif helix_class == "pi" :
    helix_step = 5
  elif helix_class == "3_10" :
    helix_step = 3
  else :
    print("  Don't know bonding for helix class %s." % helix_class, file=log)
    return generated_proxies, hb_angle_proxies
  try :
    helix_selection = selection_cache.selection(params.selection)
  except Exception as e :
    print(str(e), file=log)
    return generated_proxies, hb_angle_proxies
  assert (helix_step in [3, 4, 5])
  helix_rgs = _get_residue_groups_from_selection(pdb_hierarchy, helix_selection)
  i = 0
  just_after_pro = False
  while i < len(helix_rgs)-helix_step:
    if helix_rgs[i+helix_step].atom_groups()[0].resname.strip() == "PRO":
      print("      Proline residue: %s - end of helix" % \
        (helix_rgs[i+helix_step].id_str()), file=log)
      i += 3
      just_after_pro = True
      continue # XXX is this safe?
    angle_restraint_type = 0
    if restrain_hbond_angles and helix_class == "alpha":
      if i == 0 or i == len(helix_rgs)-helix_step-1 or just_after_pro:
        angle_restraint_type = 1
        just_after_pro = False
      else:
        angle_restraint_type = 2
    proxies, angle_proxies = _create_hbond_proxy(
      acceptor_atoms=helix_rgs[i].atoms(),
      donor_atoms=helix_rgs[i+helix_step].atoms(),
      hbond_counts=hbond_counts,
      distance_ideal=distance_ideal,
      distance_cut=distance_cut,
      remove_outliers=remove_outliers,
      prev_atoms=helix_rgs[i+helix_step-1].atoms(),
      angle_restraint_type=angle_restraint_type,
      weight=weight,
      sigma=params.sigma,
      slack=params.slack,
      angle_sigma_scale=params.angle_sigma_scale,
      angle_sigma_set=params.angle_sigma_set,
      log=log)
    for proxy in proxies:
      generated_proxies.append(proxy)
    hb_angle_proxies += angle_proxies
    i += 1
  return generated_proxies, hb_angle_proxies

def create_sheet_hydrogen_bond_proxies(
    sheet_params,
    pdb_hierarchy,
    selection_cache,
    weight,
    hbond_counts,
    distance_ideal,
    distance_cut,
    remove_outliers,
    restrain_hbond_angles,
    log=sys.stdout):
  assert (not None in [distance_ideal, distance_cut])
  angle_restraint_type = 0
  if restrain_hbond_angles:
    angle_restraint_type = 3
  prev_strand = sheet_params.first_strand
  prev_selection = selection_cache.selection(prev_strand)
  hb_angle_proxies = []
  prev_rgs = _get_residue_groups_from_selection(
      pdb_hierarchy=pdb_hierarchy,
      bool_selection=prev_selection)
  n_proxies = 0
  k = 0
  generated_proxies = geometry_restraints.shared_bond_simple_proxy()
  while k < len(sheet_params.strand):
    curr_strand = sheet_params.strand[k]
    curr_selection = selection_cache.selection(curr_strand.selection)
    curr_start = None
    prev_start = None
    if curr_strand.bond_start_current is not None:
      curr_start = selection_cache.selection(curr_strand.bond_start_current)
    if curr_strand.bond_start_previous is not None:
      prev_start = selection_cache.selection(curr_strand.bond_start_previous)
    curr_rgs = _get_residue_groups_from_selection(
        pdb_hierarchy=pdb_hierarchy,
        bool_selection=curr_selection)
    i = j = 0
    len_prev_residues = len(prev_rgs)
    len_curr_residues = len(curr_rgs)
    if curr_start is not None and prev_start is not None:
      if curr_start.count(True) < 1 or prev_start.count(True) < 1:
        error_msg = """\
Wrong registration in SHEET record. One of these selections
"%s" or "%s"
yielded zero or several atoms. Possible reasons for this are:
  - the presence of insertion codes or alternative conformations
    for one of these residues;
  - the model file was edited without updating SHEET records;
  - SHEET definition in model header or parameter file mismatch
    (delete or update SHEET definitions).""" \
% (curr_strand.bond_start_current, curr_strand.bond_start_previous)
        raise Sorry(error_msg)

      current_start_res_is_donor = pdb_hierarchy.atoms().select(curr_start)[0].name.strip() == 'N'
      if (len_curr_residues > 0) and (len_prev_residues > 0):
        i = _find_start_residue(
          residues=prev_rgs,
          start_selection=prev_start)
        j = _find_start_residue(
          residues=curr_rgs,
          start_selection=curr_start)
        if (i >= 0) and (j >= 0):
          # move i,j pointers from registration residues to the beginning of
          # beta-strands
          while (1 < i and
              ((1 < j and curr_strand.sense == "parallel") or
              (j < len_curr_residues-2 and curr_strand.sense == "antiparallel"))):
            if curr_strand.sense == "parallel":
              i -= 2
              j -= 2
            elif curr_strand.sense == "antiparallel":
              i -= 2
              j += 2
          if (curr_strand.sense == "parallel"):
            # some tweaking for ensure correct donor assignment
            if i >= 2 and not current_start_res_is_donor:
              i -= 2
              current_start_res_is_donor = not current_start_res_is_donor
            if j >= 2 and current_start_res_is_donor:
              j -= 2
              current_start_res_is_donor = not current_start_res_is_donor
            while (i < len_prev_residues) and (j < len_curr_residues):
              prev_atoms = None
              if current_start_res_is_donor:
                donor_residue = curr_rgs[j]
                if j > 0:
                  prev_atoms = curr_rgs[j-1].atoms()
                acceptor_residue = prev_rgs[i]
                i += 2
              else:
                donor_residue = prev_rgs[i]
                if i > 0:
                  prev_atoms = prev_rgs[i-1].atoms()
                acceptor_residue = curr_rgs[j]
                j += 2
              current_start_res_is_donor = not current_start_res_is_donor
              if donor_residue.atom_groups()[0].resname.strip() != "PRO":
                proxies, angle_proxies = _create_hbond_proxy(
                    acceptor_atoms=acceptor_residue.atoms(),
                    donor_atoms=donor_residue.atoms(),
                    hbond_counts=hbond_counts,
                    distance_ideal=distance_ideal,
                    distance_cut=distance_cut,
                    remove_outliers=remove_outliers,
                    prev_atoms=prev_atoms,
                    angle_restraint_type=angle_restraint_type,
                    weight=weight,
                    sigma=sheet_params.sigma,
                    slack=sheet_params.slack,
                    top_out=sheet_params.top_out,
                    angle_sigma_scale=sheet_params.angle_sigma_scale,
                    angle_sigma_set=sheet_params.angle_sigma_set,
                    log=log)
                for proxy in proxies:
                  generated_proxies.append(proxy)
                hb_angle_proxies += angle_proxies
          elif (curr_strand.sense == "antiparallel"):
            while(i < len_prev_residues and j >= 0):
              prev_atoms = None
              if (prev_rgs[i].atom_groups()[0].resname.strip() != "PRO"):
                if i > 0:
                  prev_atoms = prev_rgs[i-1].atoms()
                proxies, angle_proxies = _create_hbond_proxy(
                  acceptor_atoms=curr_rgs[j].atoms(),
                  donor_atoms=prev_rgs[i].atoms(),
                  hbond_counts=hbond_counts,
                  distance_ideal=distance_ideal,
                  distance_cut=distance_cut,
                  remove_outliers=remove_outliers,
                  prev_atoms=prev_atoms,
                  angle_restraint_type=angle_restraint_type,
                  weight=weight,
                  sigma=sheet_params.sigma,
                  slack=sheet_params.slack,
                  top_out=sheet_params.top_out,
                  angle_sigma_scale=sheet_params.angle_sigma_scale,
                  angle_sigma_set=sheet_params.angle_sigma_set,
                  log=log)
                for proxy in proxies:
                  generated_proxies.append(proxy)
                hb_angle_proxies += angle_proxies

              prev_atoms = None
              if (curr_rgs[j].atom_groups()[0].resname.strip() != "PRO"):
                if j > 0:
                  prev_atoms = curr_rgs[j-1].atoms()
                proxies, angle_proxies = _create_hbond_proxy(
                  acceptor_atoms=prev_rgs[i].atoms(),
                  donor_atoms=curr_rgs[j].atoms(),
                  hbond_counts=hbond_counts,
                  distance_ideal=distance_ideal,
                  distance_cut=distance_cut,
                  remove_outliers=remove_outliers,
                  prev_atoms=prev_atoms,
                  angle_restraint_type=angle_restraint_type,
                  weight=weight,
                  sigma=sheet_params.sigma,
                  slack=sheet_params.slack,
                  top_out=sheet_params.top_out,
                  angle_sigma_scale=sheet_params.angle_sigma_scale,
                  angle_sigma_set=sheet_params.angle_sigma_set,
                  log=log)
                for proxy in proxies:
                  generated_proxies.append(proxy)
                hb_angle_proxies += angle_proxies
              i += 2;
              j -= 2;
          else :
            print("  WARNING: strand direction not defined!", file=log)
            print("    previous: %s" % prev_strand, file=log)
            print("    current: %s" % curr_strand.selection, file=log)
        else :
          print("  WARNING: can't find start of bonding for strands!", file=log)
          print("    previous: %s" % prev_strand, file=log)
          print("    current: %s" % curr_strand.selection, file=log)
      else :
        print("  WARNING: can't find one or more strands!", file=log)
        print("    previous: %s" % prev_strand, file=log)
        print("    current: %s" % curr_strand.selection, file=log)
    k += 1
    prev_strand = curr_strand.selection
    prev_selection = curr_selection
    prev_rgs = curr_rgs
  return generated_proxies, hb_angle_proxies

def _find_start_residue(
    residues,
    start_selection):
  start_i_seqs = start_selection.iselection()
  for i, residue in enumerate(residues):
    atom_i_seqs = residue.atoms().extract_i_seq()
    if (atom_i_seqs.intersection(start_i_seqs).size() > 0):
      return i
  return -1

def _get_residue_groups_from_selection(pdb_hierarchy, bool_selection):
  # Selection should cover only one chain
  assert isinstance(bool_selection, flex.bool)
  i_seqs = bool_selection.iselection()
  if len(i_seqs) == 0:
    raise Sorry(
        "Error in SS definitions, most likely atoms are absent for one of them.")
  a = pdb_hierarchy.atoms()[i_seqs[0]]
  ch_id = a.parent().parent().parent().id
  rgs = []
  for model in pdb_hierarchy.models():
    for chain in model.chains():
      if chain.id == ch_id:
        for rg in chain.residue_groups():
          if bool_selection[rg.atoms()[0].i_seq]:
            rgs.append(rg)
        if len(rgs)>0:
          return rgs
  return rgs

########################################################################
# ANNOTATION
#
# Won't be used soon
#
class find_helices_simple(object):
  """
  Identify helical regions, defined as any three or more contiguous residues
  with phi and psi within specified limits:
    -120 < phi < -20
    -80 < psi < -10
  This is much more tolerant of distorted models than KSDSSP, but will still
  miss helices in poor structures.
  """
  def __init__(self, pdb_hierarchy):
    self.pdb_hierarchy = pdb_hierarchy
    self._helices = []
    self._current_helix = []
    self.run()

  def process_break(self):
    if (len(self._current_helix) > 3):
      self._helices.append(self._current_helix)
    self._current_helix = []

  def push_back(self, chain_id, resseq):
    if (len(self._current_helix) > 0):
      last_chain, last_resseq = self._current_helix[-1]
      if (last_chain == chain_id):
        self._current_helix.append((chain_id,resseq))
    else :
      self._current_helix = [(chain_id, resseq)]

  def run(self):
    import mmtbx.rotamer
    import iotbx.pdb
    get_class = iotbx.pdb.common_residue_names_get_class
    atoms = self.pdb_hierarchy.atoms()
    sites_cart = atoms.extract_xyz()
    current_helix = []
    for model in self.pdb_hierarchy.models():
      for chain in model.chains():
        main_conf = chain.conformers()[0]
        residues = main_conf.residues()
        for i_res in range(len(residues) - 1):
          residue1 = residues[i_res]
          if (get_class(residue1.resname) == "common_amino_acid"):
            residue0 = None
            if (i_res > 0):
              residue0 = residues[i_res - 1]
            residue2 = residues[i_res+1]
            resseq1 = residue1.resseq_as_int()
            resseq2 = residue2.resseq_as_int()
            if (residue0 is not None):
              if ((resseq2 == (resseq1 + 1)) or
                  ((resseq2 == resseq1) and
                   (residue1.icode != residue2.icode))):
                resseq0 = residue0.resseq_as_int()
                if ((resseq0 == (resseq1 - 1)) or ((resseq0 == resseq1) and
                    (residue0.icode != residue1.icode))):
                  phi_psi_i_seqs = mmtbx.rotamer.get_phi_psi_indices(
                    prev_res=residue0,
                    residue=residue1,
                    next_res=residue2)
                  if (phi_psi_i_seqs.count(None) > 0):
                    continue
                  (phi, psi) = mmtbx.rotamer.phi_psi_from_sites(
                    i_seqs=phi_psi_i_seqs,
                    sites_cart=sites_cart)
                  if is_approximately_helical(phi, psi):
                    self.push_back(chain.id, resseq1)
                    continue
                  else :
                    pass
            self.process_break()

  def build_selections(self):
    atom_selections = []
    for helix in self._helices :
      chain_id = helix[0][0]
      selection = """chain '%s' and resseq %d:%d""" % (helix[0][0],
        helix[0][1], helix[-1][1])
      atom_selections.append(selection)
    return atom_selections

  def as_restraint_group_phil(self):
    phil_strs = []
    for selection in self.build_selections():
      helix_str = """helix {\n  selection = "%s"\n}""" % selection
      phil_strs.append(helix_str)
    if (len(phil_strs) > 0):
      master_phil = iotbx.phil.parse(helix_group_params_str)
      helix_phil = iotbx.phil.parse("\n".join(phil_strs))
      return master_phil.fetch(source=helix_phil)
    return None

  def as_restraint_groups(self):
    helix_phil = self.as_restraint_group_phil()
    if (helix_phil is not None):
      return helix_phil.extract()
    return None

  def as_pdb_records(self):
    pass

  def show(self, out=sys.stdout):
    if (len(self._helices) == 0):
      print("No recognizable helices.", file=out)
    else :
      print("%d helix-like regions found:" % len(self._helices), file=out)
    for selection in self.build_selections():
      print("  %s" % selection, file=out)

def is_approximately_helical(phi, psi):
  if (-120 < phi < -20) and (-80 < psi < -10):
    return True
  return False

# FIXME
def _find_strand_bonding_start(atoms,
    prev_strand_donors,
    prev_strand_acceptors,
    curr_strand_donors,
    curr_strand_acceptors,
    sense,
    max_distance_cutoff=4.5):
  assert sense != "unknown"
  assert prev_strand_donors.size() == prev_strand_acceptors.size()
  assert curr_strand_donors.size() == curr_strand_acceptors.size()
  sites_cart = atoms.extract_xyz()
  min_dist = max_distance_cutoff
  best_pair = (None, None)
  for donor_i_seq in prev_strand_donors :
    for acceptor_j_seq in curr_strand_acceptors :
      (x1, y1, z1) = sites_cart[donor_i_seq]
      (x2, y2, z2) = sites_cart[acceptor_j_seq]
      dist = sqrt((x2-x1)**2 + (y2-y1)**2 + (z2-z1)**2)
      if (dist < min_dist):
        best_pair = (donor_i_seq, acceptor_i_seq)
  return best_pair


 *******************************************************************************


 *******************************************************************************
mmtbx/secondary_structure/regularize_from_pdb.py
from __future__ import absolute_import, division, print_function

# regularize_from_pdb.py
# a tool to replace segments of a structure with similar segment with good
# geometry from the PDB

import math
from operator import itemgetter
from iotbx.pdb import resseq_encode
import iotbx.phil
import os,sys
from libtbx.utils import Sorry,null_out
from scitbx.matrix import col
from scitbx.array_family import flex
from mmtbx.secondary_structure.find_ss_from_ca import \
   find_secondary_structure, \
   find_helix,find_beta_strand,find_other_structure,helix,strand,other,\
   get_atom_list,\
   model_info,split_model
from mmtbx.secondary_structure.find_ss_from_ca import \
   merge_hierarchies_from_models
from iotbx.pdb.utils import get_pdb_hierarchy
from six.moves import zip
from six.moves import range

master_phil = iotbx.phil.parse("""

  input_files {
    map_coeffs_file = None
      .type = path
      .help = File with map coefficients
      .short_caption = Map coefficients
      .style = bold file_type:hkl input_file process_hkl child:fobs:data_labels\
        child:space_group:space_group child:unit_cell:unit_cell

    map_coeffs_labels = None
      .type = str
      .input_size = 160
      .help = Optional label specifying which columns of of map coefficients \
          to use
      .short_caption = Map coeffs label
      .style = bold renderer:draw_fobs_label_widget

    seq_file = None
      .type = path
      .help = Sequence file
      .short_caption = Sequence file

    pdb_in = None
      .type = path
      .help = Input PDB file
      .short_caption = Input PDB file

    secondary_structure_input = None
      .type = bool
      .help = Not used
      .style = hidden

    force_secondary_structure_input = None
      .type = bool
      .help = Not used
      .style = hidden

  }
  output_files {

    pdb_out = placed.pdb
      .type = path
      .help = Output PDB file with placed segments only
      .short_caption = Output PDB file with placed segments only

    pdb_records_file = None
      .type = path
      .help = Not used
      .style = hidden

  }
  crystal_info {
     resolution = None
       .type = float
       .help = High-resolution limit. Data will be truncated at this\
               resolution. If a map is supplied, it will be Fourier \
               filtered at this resolution. Required if input is a map and \
                only_original_map is not set.
       .short_caption = High-resolution limit
       .style = resolution
     space_group = None
       .type = space_group
       .short_caption = Space Group
       .help = Space group (normally read from the data file)
     unit_cell = None
       .type = unit_cell
       .short_caption = Unit Cell
       .help = Unit Cell (normally read from the data file)

     sequence = None
       .type = str
       .short_caption = Sequence
       .help = Sequence as text string. Normally supply a sequence file instead

  }
  find_ss_structure { # Note overwrites values in find_ss_structure.py

     ss_by_chain = None
       .type = bool
       .help = Only applies if search_method = from_ca. \
              Find secondary structure only within individual chains. \
               Alternative is to allow H-bonds between chains. Can be \
               much slower with ss_by_chain=False. If your model is complete \
               use ss_by_chain=True. If your model is many fragments, use \
               ss_by_chain=False.  Not used in regularize_pdb
       .short_caption = Secondary structure by chain
       .style = hidden

     auto_choose_ca_vs_ca_n_o = True
       .type = bool
       .help = Automatically identify whether chains are mostly CA or mostly \
                contain CA/N/O atoms (requires min_ca_n_o_completeness).
       .short_caption = Auto choose CA vs CA/N/O

     min_ca_n_o_completeness = 0.95
       .type = float
       .help = Minimum completeness of CA/N/O atoms to not use CA-only
       .short_caption = Minimum completeness of CA/N/O

     max_rmsd = 1
       .type = float
       .help = Maximum rmsd to consider two chains with identical sequences \
               as the same for ss identification. \
               Not used in regularize_pdb
       .short_caption = Maximum rmsd
       .style = hidden
     use_representative_chains = True
       .type = bool
       .help = Use a representative of all chains with the same sequence. \
               Alternative is to examine each chain individually. Can be \
               much slower with use_representative_of_chain=False if there \
               are many symmetry copies. Ignored unless ss_by_chain is True. \
               Not used in regularize_pdb
       .style = hidden
     max_representative_chains = 100
       .type = float
       .help = Maximum number of representative chains\
               Not used in regularize_pdb
       .short_caption = Maximum representative chains
       .style = hidden

     find_alpha = True
       .type = bool
       .help = Find alpha helices
       .short_caption = Find alpha helices

     helices_are_alpha = True
       .type = bool
       .help = Find alpha helices and not three_ten or pi
       .short_caption = Helices are alpha

     find_three_ten = False
       .type = bool
       .help = Find three_ten helices
       .short_caption = Find three_ten helices

     find_pi = False
       .type = bool
       .help = Find pi helices
       .short_caption = Find pi helices

     find_beta = True
       .type = bool
       .help = Find beta structure
       .short_caption = Find beta structure

     find_other = True
       .type = bool
       .help = Find other structure
       .short_caption = Find other structure

     exclude_alpha_in_beta  = False
       .type = bool
       .help = Exclude regions already identified as alpha from three_ten, pi,\
               and beta
       .short_caption = Exclude alpha from beta

     make_unique = False
       .type = bool
       .help = Assign each residue to a unique type of structure
       .short_caption = Assign residues to unique structure

     cut_up_segments = True
       .type = bool
       .help = Cut up segments (make short segments of secondary structure)
       .short_caption = Cut up segments

     extend_segments = False
       .type = bool
       .help = Try to extend segments in both directions one residue at a time
       .short_caption = Extend segments

     write_helix_sheet_records = False
       .type = bool
       .help = Write HELIX and SHEET records
       .short_caption = Write HELIX/SHEET records

     set_up_helices_sheets = False
       .type = bool
       .help = Set up HELIX and SHEET records
       .short_caption = Set up HELIX/SHEET records

     include_single_strands = False
       .type = bool
       .help = Write SHEET records that contain a single strand
       .short_caption = Write single strands

     remove_missing_atom_annotation = False
       .type = bool
       .help = Remove annotation that refers to atoms that are not present
       .short_caption = Remove missing atom annotation

     max_h_bond_length = 3.5
       .type = float
       .help = Maximum H-bond length to include in secondary structure
       .short_caption = Maximum H-bond length

    search_secondary_structure = True
      .type = bool
      .help = Search for secondary structure in input model. \
              (Alternative is to just use secondary structure from \
              secondary_structure_input.)
      .short_caption = Find secondary structure

    combine_annotations = True
      .type = bool
      .help = Combine annotations if an input annotation is provided
      .short_caption = Combine annotations

    require_h_bonds = False
      .type = bool
      .help = Remove all secondary structure records that have fewer than \
              minimum_h_bonds good hydrogen bonds
      .short_caption = Require H-bonds

    minimum_h_bonds = 1
      .type = int
      .help = Minimum number of good hydrogen bonds to keep secondary \
              structure element (helix/sheet) if require_h_bonds is set
      .short_caption = Minimum number of H bonds

    maximum_poor_h_bonds = None
      .type = int
      .help = Maximum number of poor hydrogen bonds to keep secondary \
              structure element (helix/sheet) if require_h_bonds is set. \
              Note: None means ignore this test, 0 means allow no poor H-bonds.
      .short_caption = Maximum number of poor H bonds

    tolerant = None
      .type = bool
      .help = Set values for tolerant search
      .short_caption = Tolerant search

     tolerant_max_h_bond_length = 5
       .type = float
       .help = Tolerant maximum H-bond length to include in \
           secondary structure
       .short_caption = Tolerant maximum H-bond length

  }

  extract_segments_from_pdb {
    extract = *None alpha beta other
       .type = choice
       .help = Extract all segments (alpha/beta) from a PDB file. \
               Used to create libraries of segments
       .short_caption = Extract segments from PDB
  }
  alpha {
     include scope mmtbx.secondary_structure.secondary_structure_params.alpha_params

     library = 'helices.pdb'
       .type = path
       .help = Helix library
       .short_caption = Helix library

     replace = True
       .type = bool
       .help = Replace alpha helices
       .short_caption = Replace alpha helices

     maximum_rmsd = 1.5
       .type = float
       .help = Maximum rmsd of CA atoms to template for helices
       .short_caption = Maximum rmsd of CA atoms to template for helices

     index_delta_x = None
       .type = float
       .help = Index gridding in x. \
                 Segments are indexed by 1-N distance and 1-N/2 distance in \
                 bins of index_delta_x and index_delta_y .  \
                 1 A is usually about right for both.
       .short_caption = Index gridding in x

     index_delta_y = None
       .type = float
       .help = Index gridding in y. \
                 Segments are indexed by 1-N distance and 1-N/2 distance in \
                 bins of index_delta_x and index_delta_y .  \
                 1 A is usually about right for both.

  }
  beta {
     include scope mmtbx.secondary_structure.secondary_structure_params.beta_params

     library = 'strands.pdb'
       .type = path
       .help = Strand library
       .short_caption = Strand library

     replace = True
       .type = bool
       .help = Replace beta structure
       .short_caption = Replace beta structure

     maximum_rmsd = 1.5
       .type = float
       .help = Maximum rmsd of CA atoms to template for beta
       .short_caption = Maximum rmsd of CA atoms to template for beta

     index_delta_x = None
       .type = float
       .help = Index gridding in x. \
                 Segments are indexed by 1-N distance and 1-N/2 distance in \
                 bins of index_delta_x and index_delta_y .  \
                 1 A is usually about right for both.
       .short_caption = Index gridding in x

     index_delta_y = None
       .type = float
       .help = Index gridding in y. \
                 Segments are indexed by 1-N distance and 1-N/2 distance in \
                 bins of index_delta_x and index_delta_y .  \
                 1 A is usually about right for both.

  }
  other {
     include scope mmtbx.secondary_structure.secondary_structure_params.other_params

     library = 'lib8.pdb'
       .type = path
       .help = Other library
       .short_caption = Other library

     replace = True
       .type = bool
       .help = Replace other structure
       .short_caption = Replace other structure

     maximum_rmsd = 1.5
       .type = float
       .help = Maximum rmsd of CA atoms to template for other
       .short_caption = Maximum rmsd of CA atoms to template for other

     index_delta_x = 1.
       .type = float
       .help = Index gridding in x. \
                 Segments are indexed by 1-N distance and 1-N/2 distance in \
                 bins of index_delta_x and index_delta_y .  \
                 1 A is usually about right for both.
       .short_caption = Index gridding in x

     index_delta_y = 1.
       .type = float
       .help = Index gridding in y. \
                 Segments are indexed by 1-N distance and 1-N/2 distance in \
                 bins of index_delta_x and index_delta_y .  \
                 1 A is usually about right for both.
       .short_caption = Index gridding in y

  }
  regularization {

     include_side_chain = False
       .type = bool
       .help = include side chain (beyond CB) in replacement
       .short_caption = include side chains beyond CB

     maximum_overlap = 2
       .type = int
       .help = Maximum overlap of fragments in assembly
       .short_caption = Maximum overlap

     maximum_junction_rmsd = 1.0
       .type = float
       .help = Maximum rmsd of main-chain atoms at residues where fragments \
                are joined
       .short_caption = Maximum rmsd at junctions

     good_enough_ratio = 0.5
       .type = float
       .help = Accept a segment if rmsd to target is good_enough_ratio times \
            the maximum_rmsd or better (and do not look further). Note that \
            segments are sorted on frequency of occurrence so those at the \
            top of the list are more likely to occur than those at the end.
       .short_caption = Good enough ratio for maximum_rmsd

  }
  control {
      resolve_size = None
        .type = str
        .help = "Size of resolve to use. "
        .short_caption = Size of RESOLVE to use
      verbose = False
        .type = bool
        .help = Verbose output
        .short_caption = Verbose output
  }
""", process_includes=True)
master_params = master_phil

def get_and_split_model(pdb_hierarchy=None,
     pdb_in=None,get_start_end_length=None,out=sys.stdout):
    if not pdb_hierarchy:
      print("Reading model from %s" %(pdb_in), file=out)
      if not pdb_in:
        return []
      if not os.path.isfile (pdb_in):
        raise Sorry("Missing the file %s" %(pdb_in))
      with open(pdb_in) as f:
        text = f.read()
      pdb_hierarchy=get_pdb_hierarchy(text=text)
    model=model_info(
      hierarchy=pdb_hierarchy,
      info={})
    models=split_model(model=model,verbose=False)

    for model in models:
      model.hierarchy.remove_alt_confs(always_keep_one_conformer=False)
      if get_start_end_length:
        model.info['model_start_resno']=model.hierarchy.first_resseq_as_int()
        model.info['model_end_resno']=model.hierarchy.last_resseq_as_int()
        id=model.info['chain_number']
        ll=model.info['model_end_resno']-model.info['model_start_resno']+1
        model.info['length']=ll
    return models

class segment_library:

  def __init__(self,params=None,
     segment_class=None,out=sys.stdout):

    self.segment_class=segment_class # strand helix other
    self.index_length=None

    # get the segments as model_info objects
    if params.library is None:
      params.library=""
    elif not os.path.isfile(params.library):
      import libtbx.load_env
      name=os.path.join('cctbx_project','mmtbx',
         'secondary_structure','regularize_from_pdb_lib',params.library)
      full_name=libtbx.env.find_in_repositories(
         relative_path=name, test=os.path.exists)
      if not full_name:
        raise Sorry("Cannot find the library file %s" %(params.library))
      params.library=full_name

    self.models=get_and_split_model(pdb_in=params.library,out=out)

    # get information about the segments, if available
    info_file=params.library.replace(".pdb",".info")
    self.get_info_file(info_file=info_file,out=out)

    # set up the segments with information if available
    self.set_up_segments(params=params,out=out)

    # set up indexing so that we can find segments quickly
    if self.have_info:
      self.set_up_indexing()

  def get_info_file(self,info_file=None,out=sys.stdout):
    self.max_count=None
    if os.path.isfile(info_file):
      print("Reading other library info from %s" %(info_file), file=out)
      self.model_number_list=[]
      self.model_n_obs=[]
      self.model_rms=[]
      info_number_of_segments=None
      with open(info_file) as f:
        lines = f.readlines()
      for line in lines:
        spl=line.split()
        if info_number_of_segments is None:
          info_number_of_segments=int(spl[-1])
        else:
          self.model_number_list.append(int(spl[1]))
          nn=int(spl[4])
          self.model_n_obs.append(int(spl[4]))
          self.model_rms.append(float(spl[-1]))
          if self.max_count is None or nn>self.max_count:
            self.max_count=nn
      if len(self.model_number_list)!=info_number_of_segments:
       print("Model numbers: %d  Segments: %d " %(
         len(self.model_number_list),info_number_of_segments))
      assert len(self.model_number_list)==info_number_of_segments
      assert len(self.model_number_list)==len(self.models)
      self.have_info=True
    else:
      self.model_number_list=[None]*len(self.models)
      self.model_n_obs=[None]*len(self.models)
      self.model_rms=[None]*len(self.models)
      self.have_info=False

  def set_up_segments(self,params=None,out=sys.stdout):
    self.segments=[]
    frequency=None
    for model,count in zip(self.models,self.model_n_obs):
      if self.max_count:
        frequency=count/self.max_count
      self.segments.append(
        self.segment_class(
          params=params,hierarchy=model.hierarchy,frequency=frequency,out=out))

  def set_up_indexing(self):
    # set up indexing to find correct ones quickly
    index_dict={}
    for segment in self.segments:
      if self.index_length is None:
        self.index_length=segment.standard_length
      assert self.index_length==segment.standard_length
      index_list=self.get_index_list(segment=segment)
      for index in index_list:
        if not index in index_dict:
          index_dict[index]=[]
        index_dict[index].append(segment)
    self.index_dict=index_dict

  def get_index_from_x_y(self,x,y,delta_x=None,delta_y=None):
     index_x=int(0.5+x/delta_x)
     index_y=int(0.5+y/delta_y)
     return "%d:%d" %(index_x,index_y)

  def get_index_list(self,segment=None,sites=None,
      delta_x=1.,delta_y=1.,only_best=False):
     # get list of indices for this segment.  They correspond to the
     # values of dist(1,n) and dist(1,n/2) in this segment
     if sites is None:
       sites=segment.get_sites()
     if self.index_length!=len(sites):
       return None # the indexing only applies if we have the same length segment
     n=len(sites)-1
     n2=n//2
     dist1n=col(sites[n])-col(sites[0])
     dist1n=dist1n.length()
     dist1n2=col(sites[n2])-col(sites[0])
     dist1n2=dist1n2.length()
     if only_best:
       return self.get_index_from_x_y(
         dist1n,dist1n2,delta_x=delta_x,delta_y=delta_y)

     # Here when we are setting up the indices (only_best=False)
     index_list=[]
     for i in [-delta_x,0,delta_x]:
       for j in [-delta_y,0,delta_y]:
         index_list.append(
           self.get_index_from_x_y(
             dist1n+i,dist1n2+j,delta_x=delta_x,delta_y=delta_y))
     return index_list

  def get_xyz_in_order(self,xyz=None,original_order=None,
        target_order=None):
    #put xyz in the order required to match order to target_order
    if original_order==target_order:
      return xyz  # already was ok

    recover_target_order=range(len(target_order))
    recover_sort_list=[]
    for target,recover in zip(target_order,recover_target_order):
      recover_sort_list.append([target,recover])
    recover_sort_list.sort(key=itemgetter(0)) # sorted on target_order, tag is place these go

    sort_list=[]
    for x,orig in zip(xyz,original_order):
      sort_list.append([orig,x])
    sort_list.sort(key=itemgetter(0))  # sorted on original order, tag is xyz


    second_sort_list=[]
    for [orig,x],[target,recover] in zip(sort_list,recover_sort_list):
      second_sort_list.append([recover,x,orig,target])
    second_sort_list.sort(key=itemgetter(0))

    new_xyz=flex.vec3_double()
    new_order=[]
    for [recover,x,orig,target] in second_sort_list:
      new_xyz.append(x)
      new_order.append(orig)

    assert new_order==target_order

    return new_xyz

  def get_main_chain_rmsd(self,lsq_fit=None,before_junction=None,
            start_res=None,end_res=None,  # in other_segment
            model_segment=None,other_segment=None):
    # find matching residues in model_segment and other_segment that have
    #  low rms for main_chain rmsd. Must have residues on up to before_junction
    #   and higher than before_junction
    # how to select matching residues...
    first_res_in_model_segment=model_segment.get_start_resno()
    last_res_in_model_segment=model_segment.get_end_resno()
    assert last_res_in_model_segment-first_res_in_model_segment==end_res-start_res
    placed_other_hierarchy=model_segment.apply_lsq_fit(
         lsq_fit_obj=lsq_fit,
          hierarchy=other_segment.hierarchy,
          start_res=start_res,
          end_res=end_res)

    rms_list=[]
    model_resseq_list=[]
    other_resseq_list=[]
    for i,j in zip(
       range(first_res_in_model_segment,last_res_in_model_segment+1),
       range(start_res,end_res+1)):
      atom_selection=\
         "resid %s through %s and (name n or name c or name ca or name o)" %(
          resseq_encode(i),resseq_encode(i))
      model_res_hierarchy=model_segment.hierarchy.apply_atom_selection(
        atom_selection)
      atom_selection=\
         "resid %s through %s and (name n or name c or name ca or name o)" %(
          resseq_encode(j),resseq_encode(j))
      other_res_hierarchy=placed_other_hierarchy.apply_atom_selection(
        atom_selection)
      xyz1=model_res_hierarchy.atoms().extract_xyz()
      xyz2=other_res_hierarchy.atoms().extract_xyz()
      assert xyz1.size()==xyz2.size()
      # make sure xyz2 is in same order as xyz1 based
      atom_list_1=get_atom_list(model_res_hierarchy)
      atom_list_2=get_atom_list(other_res_hierarchy)
      xyz2_sorted=self.get_xyz_in_order(xyz=xyz2,original_order=atom_list_2,
        target_order=atom_list_1)
      diffs=xyz1-xyz2_sorted
      rms_list.append(diffs.rms_length())
      model_resseq_list.append(i)
      other_resseq_list.append(j)

    # get rms by residue pair
    best_left_rms=None
    best_right_rms=None
    best_left_crossover=None
    best_right_crossover=None
    best_left_crossover_other=None
    best_right_crossover_other=None
    for rms,model_resseq,other_resseq in zip(
      rms_list,model_resseq_list,other_resseq_list):
      if model_resseq<=before_junction:
        if best_left_rms is None or best_left_rms>rms:
          best_left_rms=rms
          best_left_crossover=model_resseq
          best_left_crossover_other=other_resseq
      else:
        if best_right_rms is None or best_right_rms>rms:
          best_right_rms=rms
          best_right_crossover=model_resseq
          best_right_crossover_other=other_resseq

    return best_left_rms,best_right_rms,\
       best_left_crossover,best_right_crossover,\
       best_left_crossover_other,best_right_crossover_other


  def get_replacement_segment(self,model_segment=None,
       maximum_rmsd=None,good_enough_rmsd=None,
       score_by_main_chain_rmsd=None,before_junction=None,
       start_position=None,
       delta_residues=0,
       verbose=None,out=sys.stdout):
    # find a segment in segment_lib that matches model_segment.  Choose
    #  n residues in segment from segment lib where n=length of model_segment
    # delta_residues is change in length of model_segment to insert from
    #   segment_lib
    n=model_segment.length()
    best_segment=None
    best_lsq_fit=None
    best_rms=None
    best_start_res=None
    best_end_res=None
    best_id=None
    best_left_crossover=None
    best_right_crossover=None

    if delta_residues is None:
      delta_residues=0

    placed_hierarchy=None
    all_too_short=True

    nn=0
    found=False
    segments_to_use=None
    if self.have_info and not score_by_main_chain_rmsd:
      index=self.get_index_list(model_segment,only_best=True)
      segments_to_use=self.index_dict.get(index)

    if segments_to_use is None:
      segments_to_use=self.segments

    for segment in segments_to_use:
      if found: break
      nn+=1
      m=segment.length()
      extra_residues=m-n-delta_residues
      if extra_residues<0:
        continue # skip if too short
      if start_position is None or start_position>extra_residues:
         range_to_use=range(extra_residues+1)
      else:  # just use start_position
         range_to_use=[start_position]
      all_too_short=False
      # get transformation to map segment on to model_segment
      for offset in range_to_use:
        if found: break
        start_res=offset+segment.get_start_resno() # start res in seg library
        end_res=start_res+n+delta_residues-1

        lsq_fit=model_segment.get_lsq_fit(other=segment,
          start_res=start_res,end_res=end_res)
        if not lsq_fit: continue
        if lsq_fit.rms > maximum_rmsd: continue
        rms=lsq_fit.rms

        if score_by_main_chain_rmsd: # see if we can find a pair of residues
        #  that overlap acceptably...
          left_rms,right_rms,left_crossover,right_crossover,\
             left_crossover_other,right_crossover_other=\
             self.get_main_chain_rmsd(lsq_fit=lsq_fit,
            before_junction=before_junction,
            start_res=start_res,end_res=end_res,
            model_segment=model_segment,other_segment=segment)
          rms=max(left_rms,right_rms)
        else:
          left_crossover,right_crossover,\
             left_crossover_other,right_crossover_other=None,None,None,None
        if best_rms is None or rms<best_rms:
            best_rms=rms
            best_segment=segment
            best_lsq_fit=lsq_fit
            best_start_res=start_res
            best_end_res=end_res
            best_id=nn
            best_left_crossover=left_crossover
            best_right_crossover=right_crossover
            best_left_crossover_other=left_crossover_other
            best_right_crossover_other=right_crossover_other
            if best_rms<=good_enough_rmsd:
              found=True
    if all_too_short:
      if verbose:
        print("No templates long enough for segment: %s " %(
          model_segment.summary()), file=out)
    if best_segment:
      # apply rt to the hierarchy in segment
      placed_hierarchy=model_segment.apply_lsq_fit(lsq_fit_obj=best_lsq_fit,
          hierarchy=best_segment.hierarchy,
          start_res=best_start_res,end_res=best_end_res)
      frequency=best_segment.frequency
    else:
      return None,None,None,None,None,None,None
    if frequency is not None:
      if verbose:
        print("Used segment # %d (examined %d) with freq %5.3f and rmsd %5.2f" %(
          best_id,nn,frequency,best_rms))
      log_frequency=math.log(frequency)
    else:
      log_frequency=None
      if verbose:
        print("Used segment # %d (examined %d) and rmsd %5.2f" %(
          best_id,nn,best_rms))
    return placed_hierarchy,best_rms,log_frequency,\
          best_left_crossover,best_right_crossover,\
             best_left_crossover_other,best_right_crossover_other

class connected_group_segment:
  def __init__(self,segment=None,start_resno=None,end_resno=None):
    # start_resno is where we will start this segment in this connected group
    #  (can be from segment.info['target_start_resno'] to
    #      segment.info['target_end_resno'])
    self.segment=segment
    self.start_resno=start_resno
    self.end_resno=end_resno
    self.delta_length=\
       self.segment.info['length']-self.segment.info['target_length']
    self.first_resno_of_hierarchy=self.segment.hierarchy.first_resseq_as_int()
    self.get_score()


  #  resseq numbers that identify where this segment is going to go relative
  #  to the sequence of the target chain

  def get_target_start_resno(self): # resseq in target corresponding to first
    # residue in the hierarchy for this segment
    return self.segment.info['target_start_resno']

  def get_target_end_resno(self): # resseq in target corresponding to last
    # residue in the hierarchy for this segment
    return self.segment.info['target_end_resno']

  def get_start_resno(self): # resseq in target we want to start at
    # This is the left end of the final sequence for this target
    return self.start_resno

  def get_end_resno(self): # resseq in target we want to end at
    # This is the right end of the final sequence for this target
    return self.end_resno

  def get_delta_length(self): # the difference between the length to be inserted
    # and the number of residues it will replace. Usually zero.
    return self.delta_length

  def get_length(self): # length of the part of the segment to be used.
    #  Does not include delta_length (it is the length of the segment that
    #  it would replace)
    return self.end_resno-self.start_resno+1

  #  offsets and resseq numbers that identify the residue numbers in the
  #  (arbitrary) hierarchy for this segment that we are using in this
  #  segment

  def get_first_resno_of_hierarchy(self): # first resseq in hierarchy
    # This is the first residue number in the hierarchy (the whole thing,
    #  not just the part we are going to use)
    return self.first_resno_of_hierarchy

  def get_start_offset(self):
    # how far in to the hierarchy in this cgs.segment we start:
    return self.start_resno-self.get_target_start_resno()

  def get_start_resno_of_hierarchy(self): # resseq in hierarchy to start at
    # this is the left end of the part of the hierarchy to use
    return self.get_first_resno_of_hierarchy()+self.get_start_offset()

  def get_end_resno_of_hierarchy(self): # resseq in hierarchy to end at
    # this is the right end of the part of the hierarchy to use
    # Note if there is an insertion then this may be a longer segment
    #  than the target
    return self.get_start_resno_of_hierarchy()+\
        self.get_length()-1+self.get_delta_length()

  def has_left_end(self):
    return self.segment.info.get('contains_left_end_of_chain')

  def has_right_end(self):
    return self.segment.info.get('contains_right_end_of_chain')

  def get_score(self,
     rms_weight=-0.2,
     use_frequency_score=True,
     freq_factor=0.02,
     length_factor=1.,
     rms_factor=1.):
    # For now, score + on length and - on length*rmsd from target structure
    length=self.end_resno-self.start_resno+self.delta_length
    # do not include end but do include extra residues inserted
    rms=self.segment.info['rms'] # deviation from target
    base_score=self.segment.info['base_score']
    if use_frequency_score:
      log_frequency=self.segment.info['log_frequency']
      if log_frequency is not None:
        self.score=base_score*length*(
          length_factor+rms_factor*math.exp(-rms)+freq_factor*log_frequency)
      else: # for helices etc use freq=1
        self.score=base_score*length*math.exp(-rms)
    else:
      self.score=base_score*length* (1+rms_weight*rms)   # old version
    return self.score

  def show_summary(self,out=None,return_text=True):
    if self.delta_length:
      extra_text="(%d)" %(self.end_resno+self.delta_length)
    else:
      extra_text="    "
    text="CS: %d:%d%s Hierarchy: %s:%s Score: %7.2f" %(
        self.start_resno,self.end_resno,extra_text,
        str(self.get_start_resno_of_hierarchy()),
        str(self.get_end_resno_of_hierarchy()),
        self.get_score())
    if return_text: return text
    if out is None: out=sys.stdout
    print(text, file=out)
    return ""


class connected_group:
  def __init__(self,segment=None,start_resno=None,end_resno=None):
    self.connected_group_segments=[]
    if segment:
      self.connected_group_segments.append(
       connected_group_segment(segment=segment,
       start_resno=start_resno,end_resno=end_resno))
    self.find_left_connection()
    self.find_right_connection()
    self.connection_rms_dict={}
    self.get_score()
    self.model_is_complete=None
    self.model_rms=None
    self.model_rms_n=None

  def customized_copy(self):
    new_copy=connected_group()
    for cgs in self.connected_group_segments:
      new_copy.connected_group_segments.append(cgs)
    new_copy.score=self.score
    new_copy.left_connection=self.left_connection
    new_copy.right_connection=self.right_connection
    return new_copy

  def as_model(self,renumber=True,is_left_end=None,is_right_end=None,
     offset_to_renumber_right_end_at_target=False,
     model_to_match=None,
     use_resname_from_model_to_match=None,
     verbose=None,
     out=sys.stdout):
    # if model_to_match is provided, use chain name and residue numbers
    # Also use residue name from that model if use_resname_from_model_to_match
    # Also keep only up to CB and discard CB if GLY

    models=[]
    diffs=flex.vec3_double()
    skipped=0
    sequence=[]
    sequences=[]  # list of sequences, one for each model
    if model_to_match:
      chain_id=model_to_match.hierarchy.first_chain_id()
    else:
      chain_id='A'
    overall_start_resno=None
    overall_delta_residues=0
    for cgs,is_first,is_last in zip(self.connected_group_segments,
      [True]+[False]*(len(self.connected_group_segments)-1),
      [False]*(len(self.connected_group_segments)-1)+[True],
       ):
      # 2015-04-09 cgs.start_resno and cgs.end_resno define what we use

      if is_first and is_left_end: # keep original start of this segment
        target_start_resno=cgs.segment.info['target_start_resno']
      else:
        target_start_resno=cgs.start_resno
      if overall_start_resno is None:
        overall_start_resno=target_start_resno

      if is_last and is_right_end: # keep original end of this segment
        target_end_resno=cgs.segment.info['target_end_resno']
      else:
        target_end_resno=cgs.end_resno

      # how far in to the hierarchy in this cgs.segment we start:
      start_offset=target_start_resno-cgs.segment.info['target_start_resno']

      # where we start (by resid) in the hierarchy
      start_resno=cgs.segment.hierarchy.first_resseq_as_int()+start_offset
      length=target_end_resno-target_start_resno+1
      end_resno=start_resno+length-1+cgs.delta_length # insert may be different
      overall_delta_residues+=cgs.delta_length


      if not is_last:  # skip last residue as it is first of next one
        length-=1
        end_resno-=1
        target_end_resno-=1

      if length==0:
        continue

      atom_selection="resid %s through %s" %(resseq_encode(start_resno),
       resseq_encode(end_resno))
      h=cgs.segment.hierarchy.apply_atom_selection(atom_selection)

      if model_to_match: # get comparison and sequence
        atom_selection="name ca and resid %s through %s" %(
         resseq_encode(target_start_resno),
         resseq_encode(target_end_resno))
        original_ca=model_to_match.hierarchy.apply_atom_selection(
         atom_selection)
        original_xyz=original_ca.atoms().extract_xyz()
        original_sequence=original_ca.as_list_of_residue_names()

        atom_selection="name ca "
        replacement_ca=h.apply_atom_selection(atom_selection)
        replacement_xyz=replacement_ca.atoms().extract_xyz()
        replacement_sequence=replacement_ca.as_list_of_residue_names()

        if original_xyz.size()==replacement_xyz.size():
          diffs.extend(original_xyz-replacement_xyz)
          new_sequence=original_sequence
        else:
          skipped+=original_xyz.size()
          new_sequence=original_sequence+(['GLY']*max(0,
            len(replacement_sequence)-len(original_sequence))
            )[:len(replacement_sequence)]
        sequences.append(new_sequence)
        sequence+=new_sequence


      m=model_info(hierarchy=h,
       info={ 'target_start_residue':target_start_resno,
              'target_end_residue':target_end_resno,
              'delta_length':cgs.delta_length,
              'inserted_length':target_end_resno-target_start_resno+1+\
                 cgs.delta_length } )
      models.append(m)

    if diffs:
      print("RMSD (%d residues): %5.2f A" %(
          diffs.size(),diffs.rms_length()), end=' ', file=out)
      self.model_rms=diffs.rms_length()
      self.model_rms_n=diffs.size()
      if skipped:
        print("\n(Not including %d residues in insertions/deletions)" %(skipped), file=out)
      else: print(file=out)
      if verbose:
        print("Sequence: %s" %(" ".join(sequence)), file=out)


    if offset_to_renumber_right_end_at_target:
      overall_start_resno=overall_start_resno-overall_delta_residues
    new_model=merge_hierarchies_from_models(models=models,renumber=True,
      first_residue_number=overall_start_resno,sequences=sequences,
      chain_id=chain_id,trim_side_chains=True,replace_hetatm=True)
    return new_model

  def has_insertions_deletions(self):
    for cgs in self.connected_group_segments:
      if cgs.delta_length !=0:
         return True
    return False

  def overlaps_with(self,other=None):
    # either end of other overlaps with something between left and right
    #  means it overlaps
    if self.get_left_connection() <= other.get_left_connection() and \
       self.get_right_connection() >= other.get_left_connection():
      return True
    elif self.get_left_connection() <= other.get_right_connection() and \
       self.get_right_connection() >= other.get_right_connection():
      return True
    else:
      return False

  def contains(self,other=None):
    if self.get_left_connection() <= other.get_left_connection() and \
       self.get_right_connection() >= other.get_right_connection():
      return True
    else:
      return False

  def is_duplicate(self,other_list=[]):
    for other in other_list:
      if self.is_same_as(other=other):
        return True
    return False

  def is_same_as(self,other=None):
    if self.connected_group_segments==other.connected_group_segments:
      return True
    else:
      return False

  def shared_residue_range(self,other=None):
    start_res=max(self.get_left_connection(),other.get_left_connection())
    end_res=min(self.get_right_connection(),other.get_right_connection())
    if end_res>=start_res:
      return [start_res,end_res]
    else:
      return None

  def shared_segment(self,other=None):
    for cgs in self.connected_group_segments:
      for cgs1 in other.connected_group_segments:
        if cgs==cgs1: return cgs

  def get_cgs_containing_resno(self,resno):
    self_cgs=None
    for cgs in self.connected_group_segments:
      if resno>=cgs.start_resno and resno<=cgs.end_resno:
         self_cgs=cgs
         break
    return self_cgs

  def join_by_residue(self,other=None,
      shared_residue_range=shared_residue_range,
      maximum_junction_rmsd=None):
    # find rms for each possible crossover
    start_resno,end_resno=shared_residue_range
    best_resno=None
    best_rms=None

    for resno in range(start_resno,end_resno+1):
      self_cgs=self.get_cgs_containing_resno(resno)
      other_cgs=other.get_cgs_containing_resno(resno)
      rms=self.get_connection_rms(
        self_cgs,other_cgs,resno,allow_unit_length=True)
      if rms is not None and \
          rms < maximum_junction_rmsd and (best_rms is None or rms<best_rms):
        best_rms=rms
        best_resno=resno
    if best_resno is None:
      return  # failed to find a connection

    # figure out which one goes to the left
    if self.get_left_connection()<=other.get_left_connection():
      left_cg=self
      right_cg=other
    else:
      left_cg=other
      right_cg=self

    # make sure the new right one is not contained within the left:
    if left_cg.get_right_connection()>=right_cg.get_right_connection():
      return False

    # create new connected group starting with left left_cg, including part of
    #  the cgs that connects, and then the right one

    left_junction_cgs=left_cg.get_cgs_containing_resno(best_resno)
    right_junction_cgs=right_cg.get_cgs_containing_resno(best_resno)
    # modify these two so that together they cover the junction

    lcgs=connected_group_segment(segment=left_junction_cgs.segment,
        start_resno=left_junction_cgs.start_resno,end_resno=best_resno)
    lcgs.get_score()

    rcgs=connected_group_segment(segment=right_junction_cgs.segment,
        start_resno=best_resno,end_resno=right_junction_cgs.end_resno)
    rcgs.get_score()

    # now construct new connected group with left half of left_cg, the two
    # joining segments and the right half of right_cg
    new_cg=connected_group()

    # put in segments from the left
    for cgs in left_cg.connected_group_segments:
      if cgs==left_junction_cgs: break
      new_cg.connected_group_segments.append(cgs)
    # put in our new two segments making the connection
    if lcgs.get_length()>=2: # do not include segments of length 1
      new_cg.connected_group_segments.append(lcgs)
    if rcgs.get_length()>=2:
      new_cg.connected_group_segments.append(rcgs)

    # add on segments to the right
    found=False
    for cgs in right_cg.connected_group_segments:
      if cgs==right_junction_cgs:
        found=True
        continue
      if not found:
        continue
      new_cg.connected_group_segments.append(cgs)

    # make these this cg
    self.connected_group_segments=new_cg.connected_group_segments
    # sort and score
    self.sort_connected_group_segments()
    self.get_score()
    self.find_left_connection()
    self.find_right_connection()
    return True

  def get_left_right_cg(self,cg1,cg2):
    if cg1.get_left_connection()<=cg2.get_left_connection():
      left_cg=cg1
      right_cg=cg2
    else:
      left_cg=cg2
      right_cg=cg1
    return left_cg,right_cg

  def find_residue_range(self,trim_dict=None):
    # find longest segment that is not trimmed
    keys=list(trim_dict.keys())
    keys.sort()
    res_start=None
    res_end=None
    best_res_range=None
    for key in keys:
      if trim_dict[key]: # bad residue
        if res_start is None: # nothing to do
          pass
        else:  # end what we have
          if best_res_range is None or \
             res_end-res_start > best_res_range[1]-best_res_range[0]:
            best_res_range=[res_start,res_end]
            res_start=None
            res_end=None
      else:
        if res_start is not None: # continue
          res_end=key
        else: # start it
          res_start=key
          res_end=key
    if res_start is not None and (best_res_range is None or \
      res_end-res_start > best_res_range[1]-best_res_range[0]):
         best_res_range=[res_start,res_end]
    return best_res_range


  def trim_residues(self,trim_dict=None,min_length=2):
    # trim out all residues (ranges) that are marked in trim_dict
    # find longest segment that is not trimmed
    # if result is less than min_length, drop it
    res_range=self.find_residue_range(trim_dict)
    if res_range is None or res_range[1]-res_range[0]+1<min_length:
      return False# remove the connected group
    elif res_range[0]==self.get_left_connection() and \
        res_range[1]==self.get_right_connection():
      return True # fine as is
    else:
      return self.keep_residue_range(start_res=res_range[0],end_res=res_range[1])

  def keep_residue_range(self,start_res=None,end_res=None):
    # remove everything except for this range
    # Any segment less than min_length is to be removed (typically 1 long)

    # put in segments from the left starting with segment that contains
    # leftmost residue (start_res)
    first_segment_to_use=self.get_cgs_containing_resno(start_res)
    last_segment_to_use=self.get_cgs_containing_resno(end_res)

    new_cg=connected_group()
    if first_segment_to_use==last_segment_to_use:
      # just take part of this one
      cgs=connected_group_segment(segment=first_segment_to_use.segment,
              start_resno=start_res,end_resno=end_res)
      cgs.get_score()
      new_cg.connected_group_segments.append(cgs)

    else:
      started=False
      for cgs in self.connected_group_segments:
        if not started:
          if cgs==first_segment_to_use:
            started=True
            # put in the right part of this segment
            lcgs=connected_group_segment(segment=cgs.segment,
              start_resno=start_res,end_resno=cgs.end_resno)
            lcgs.get_score()
            if lcgs.get_length()>1: # never keep shorter than 2
              new_cg.connected_group_segments.append(lcgs)
          else: # do nothing
            pass
        else: # already started
          if cgs==last_segment_to_use:
            # put in the right part of this segment
            rcgs=connected_group_segment(segment=cgs.segment,
              start_resno=cgs.start_resno,end_resno=end_res)
            rcgs.get_score()
            if rcgs.get_length()>1: # never keep shorter than 2
              new_cg.connected_group_segments.append(rcgs)
            break # all done
          else:  # put it in
            new_cg.connected_group_segments.append(cgs)

    # make these this cg
    self.connected_group_segments=new_cg.connected_group_segments
    # sort and score
    self.sort_connected_group_segments()
    self.get_score()
    self.find_left_connection()
    self.find_right_connection()
    if len(self.connected_group_segments)>0:
      return True
    else:
      return False


  def join(self,other=None):
    shared_cgs=self.shared_segment(other=other)
    new_segment_list=[]
    if self.get_left_connection()<other.get_left_connection() and \
       self.get_right_connection()<other.get_right_connection():
      # join self on left
      left_cg=self
      right_cg=other
    elif self.get_left_connection()>other.get_left_connection() and \
       self.get_right_connection()>other.get_right_connection():
      # join on right
      left_cg=other
      right_cg=self
    else:  # skip (probably the same)
      return

    for cgs in left_cg.connected_group_segments:
      if cgs==shared_cgs: break
      new_segment_list.append(cgs)
    found=False
    for cgs in right_cg.connected_group_segments:
      if cgs==shared_cgs: found=True
      if found: new_segment_list.append(cgs)

    self.connected_group_segments=new_segment_list
    self.sort_connected_group_segments()
    self.get_score()
    self.find_left_connection()
    self.find_right_connection()
    return True

  def add_connected_group_to_right(self,cg):
    if self.get_left_connection()!=cg.get_right_connection() and \
       self.get_right_connection()!=cg.get_left_connection():
      raise Sorry("No connection between these connected groups:"+\
        self.show_summary()+":"+cg.show_summary())
    for cgs in cg.connected_group_segments:
      self.connected_group_segments.append(cgs)
    self.sort_connected_group_segments()
    self.get_score()
    self.find_left_connection()
    self.find_right_connection()

  def sort_connected_group_segments(self):
    sort_list=[]
    for cgs in self.connected_group_segments:
      sort_list.append([cgs.start_resno,cgs])
    sort_list.sort(key=itemgetter(0))
    self.connected_group_segments=[]
    for id,cgs in sort_list:
      self.connected_group_segments.append(cgs)

  def get_left_connection(self):
    return self.left_connection

  def get_right_connection(self):
    return self.right_connection

  def find_left_connection(self):
    lc=None
    for cg in self.connected_group_segments:
      if lc is None or cg.start_resno< lc:
        lc=cg.start_resno
    self.left_connection=lc

  def find_right_connection(self):
    lc=None
    for cg in self.connected_group_segments:
      if lc is None or cg.end_resno> lc:
        lc=cg.end_resno
    self.right_connection=lc

  def get_left_segment_hierarchy(self):
    return self.connected_group_segments[0].segment.hierarchy

  def get_right_segment_hierarchy(self):
    return self.connected_group_segments[-1].segment.hierarchy

  def is_complete(self):
    return self.model_is_complete

  def get_rms(self):
    return self.model_rms

  def get_rms_n(self):
    return self.model_rms_n

  def get_junction_rms(self):
    junction_rms=0.
    junction_rms_n=0.
    if len(self.connected_group_segments)>1:
      for s1,s2 in zip(
        self.connected_group_segments[:-1],self.connected_group_segments[1:]):
        junction_rms+=self.get_connection_rms(s1,s2)**2
        junction_rms_n+=1.
      junction_rms=(junction_rms/junction_rms_n)**0.5
    return junction_rms

  def get_junction_rms_n(self):
    return max(0,len(self.connected_group_segments)-1)

  def get_score(self,segment_weight=1.,connection_weight=-0.5,
     end_score=500.,complete_score=500):
    # score based on individual segments, minus penalty for connections, plus
    # bonus for getting ends and no gaps
    self.model_is_complete=False
    self.score=0.
    have_left_end=False
    have_right_end=False
    # score from individual segments
    for s in self.connected_group_segments:
      self.score+=s.get_score()*segment_weight
      if s.has_left_end(): have_left_end=True
      if s.has_right_end(): have_right_end=True
    if have_left_end:  # to ensure that we cover the ends
      self.score+=end_score
    if have_right_end:
      self.score+=end_score

    if have_left_end and have_right_end:
      expected_length=self.get_right_connection()-self.get_left_connection()+1
      actual_length=self.get_length()
      if expected_length==actual_length:
        self.score+=complete_score
        self.model_is_complete=True

    # score based on connections
    if len(self.connected_group_segments)>1:
      for s1,s2 in zip(
        self.connected_group_segments[:-1],self.connected_group_segments[1:]):
        self.score+=connection_weight*self.get_connection_rms(s1,s2)

    return self.score

  def get_length(self):  # does not include delta_resno (just the length it
    # would replace
    n=0
    for s in self.connected_group_segments:
      n+=s.get_length()
    # subtract off 1 for all but one (connection overlap)
    n=n-len(self.connected_group_segments)+1
    return n

  def get_connection_rms(self,s1,s2,
      resno=None,allow_unit_length=False):
    # get rms of atoms in connecting residue of segment 1 and segment 2

    # get main chain of last residue of s1 and first residue of s2 and compare
    if resno is None:
      s1_connection_resno=s1.end_resno
    else:
      s1_connection_resno=resno
    start_offset_s1=s1.start_resno-s1.segment.info['target_start_resno']
    last_resno_s1=s1.segment.hierarchy.first_resseq_as_int()+\
       start_offset_s1+s1_connection_resno-s1.start_resno
    last_resno_s1+=s1.delta_length
    # do we already have it:
    dd=self.connection_rms_dict.get(s1.segment)
    if dd:
      dd=dd.get(s2.segment)
      if dd:
        value=dd.get(last_resno_s1)
        if value is not None: return value

    if resno is None:
      s2_connection_resno=s2.start_resno
    else:
      s2_connection_resno=resno
    start_offset_s2=s2_connection_resno-s2.segment.info['target_start_resno']
    first_resno_s2=s2.segment.hierarchy.first_resseq_as_int()+start_offset_s2

    # make sure we create something long enough (not length 1):
    first_resno_s1=s1.segment.hierarchy.first_resseq_as_int()
    start_offset_s2=s2.start_resno-s2.segment.info['target_start_resno']
    last_resno_s2=s2.segment.hierarchy.first_resseq_as_int()+\
        start_offset_s2+s2.end_resno-s2.start_resno

    if not allow_unit_length and (
       last_resno_s1-first_resno_s1 < 1 or \
       last_resno_s2-first_resno_s2 < 1):  # would create a connection length 1
      rms=None
    else:
      rms=self.get_rms_pair_of_residues_main_chain(
        h1=s1.segment.hierarchy,resno1=last_resno_s1,
        h2=s2.segment.hierarchy,resno2=first_resno_s2)

    # save it so we don't calculate twice...
    if not s1.segment in self.connection_rms_dict:
      self.connection_rms_dict[s1.segment]={}
    if not s2.segment in self.connection_rms_dict[s1.segment]:
      self.connection_rms_dict[s1.segment][s2.segment]={}
    self.connection_rms_dict[s1.segment][s2.segment][last_resno_s1]=rms

    return rms

  def get_rms_pair_of_residues_main_chain(self,h1=None,resno1=None,
     h2=None,resno2=None,value_if_missing=10.):
    xyz1=self.get_one_residue_main_chain(hierarchy=h1,resno=resno1)
    xyz2=self.get_one_residue_main_chain(hierarchy=h2,resno=resno2)
    if xyz1.size()!=xyz2.size(): return value_if_missing
    diffs=xyz1-xyz2
    return diffs.rms_length()

  def get_one_residue_main_chain(self,hierarchy=None,resno=None):
    atom_selection="resseq %s and (name ca or name c or name o or name n)"  %(
     resseq_encode(resno))

    sele=hierarchy.apply_atom_selection(atom_selection)
    return sele.atoms().extract_xyz()


  def show_summary(self,out=None,return_text=True):
    rc=self.get_right_connection()
    lc=self.get_left_connection()
    if rc is None or lc is None:
      rc=0
      lc=0
    text="CG: Segments:%d Score: %7.2f Left: %d Right: %d Length %d" %(
     len(self.connected_group_segments),self.score,lc,rc,rc-lc+1)
    if return_text: return text
    if out is None: out=sys.stdout
    print(text, file=out)
    return ""

  def show_comprehensive_summary(self,out=None,return_text=True):
    text=self.show_summary(out=out,return_text=return_text)
    for cs,cs1 in zip(
        self.connected_group_segments,self.connected_group_segments[1:]+[None]):
      text+="\n%s" %(cs.show_summary())
      text+=" RMS:%5.2f A " %(cs.segment.info['rms'])
      if cs.segment.info['log_frequency']:
         text+=" ln(freq):%5.3f " %(cs.segment.info['log_frequency'])


      if cs1:
        text+=" Junction:%5.2f A " %(self.get_connection_rms(cs,cs1))
    if return_text: return text
    if out is None: out=sys.stdout
    print(text, file=out)
    return ""

class replacement_segment_summary:
  def __init__(self,model=None):
    self.model=model
    self.id=model.info['chain_number']
    self.chain_id=self.model.hierarchy.first_chain_id()
    self.replacement_model=None
    self.model_is_complete=None
    self.connected_groups=None
    self.model_has_insertions_deletions=None

  def show_summary(self,out=sys.stdout):
    print("\nID: %d ChainID: '%s'  RMSD: %5.2f A  (n=%d) " %(
       self.get_segment_id(),
       self.get_chain_id(),self.get_rms(),self.get_rms_n(),
        )+"Junction RMSD: %5.2f A (n=%d)" %(
      self.get_junction_rms(),self.get_junction_rms_n(),
       ), file=out)
    print("Complete: %s  Insertions/deletions: %s" %(
       self.is_complete(),self.has_insertions_deletions()), file=out)
    print("Input model start: %d  end: %d  length: %d " %(
         self.input_first_resno(),
         self.input_last_resno(),
         self.input_number_of_residues(),)+\
       "\nReplacement start: %d  end: %d  length: %d" %(
         self.output_first_resno(),
         self.output_last_resno(),
         self.output_number_of_residues(),
         ), file=out)

  def get_segment_id(self):
    return self.id

  def get_chain_id(self):
    return self.chain_id
  def add_replacement_model(self,replacement_model):
    self.replacement_model=replacement_model

  def add_connected_groups(self,connected_groups):
    self.connected_groups=connected_groups

  def get_rms(self):
    if self.connected_groups:
      sum_rms=0.
      sum_rms_n=0.
      for cg in self.connected_groups:
        if cg.get_rms():
          sum_rms+=cg.get_rms()**2*cg.get_rms_n()
          sum_rms_n+=cg.get_rms_n()
      if sum_rms_n:
        return (sum_rms/sum_rms_n)**0.5

    return 0.

  def get_rms_n(self):
    sum_rms_n=0.
    if self.connected_groups:
      for cg in self.connected_groups:
        if cg.get_rms_n():
          sum_rms_n+=cg.get_rms_n()
    return sum_rms_n

  def get_junction_rms(self):
    if self.connected_groups:
      sum_junction_rms=0.
      sum_junction_rms_n=0.
      for cg in self.connected_groups:
        if cg.get_junction_rms():
          sum_junction_rms+=cg.get_junction_rms()**2*cg.get_junction_rms_n()
          sum_junction_rms_n+=cg.get_junction_rms_n()
      if sum_junction_rms_n:
        return (sum_junction_rms/sum_junction_rms_n)**0.5

    return 0.

  def get_junction_rms_n(self):
    sum_junction_rms_n=0.
    if self.connected_groups:
      for cg in self.connected_groups:
        if cg.get_junction_rms_n():
          sum_junction_rms_n+=cg.get_junction_rms_n()
    return sum_junction_rms_n

  def set_is_complete(self,is_complete):
    self.model_is_complete=is_complete

  def is_complete(self):
    return self.model_is_complete

  def set_has_insertions_deletions(self,has_insertions_deletions):
    self.model_has_insertions_deletions=has_insertions_deletions

  def has_insertions_deletions(self):
    return self.model_has_insertions_deletions

  def input_number_of_residues(self):
    return self.model.hierarchy.overall_counts().n_residues

  def output_number_of_residues(self):
    if self.replacement_model:
      return self.replacement_model.hierarchy.overall_counts().n_residues
    else:
      return 0

  def input_first_resno(self):
    return self.model.hierarchy.first_resseq_as_int()

  def input_last_resno(self):
    return self.model.hierarchy.last_resseq_as_int()

  def output_first_resno(self):
    if self.replacement_model:
      return self.replacement_model.hierarchy.first_resseq_as_int()
    else:
      return 0

  def output_last_resno(self):
    if self.replacement_model:
      return self.replacement_model.hierarchy.last_resseq_as_int()
    else:
      return 0

class replace_with_segments_from_pdb:

  def __init__(self,pdb_hierarchy=None,args=None,
       helices_are_alpha=None,resid_offset=None,
       out=sys.stdout):

    self.replacement_model=model_info() # empty object
    self.model_output_number_of_residues_by_segment={}
    self.model_segment_ids=[]

    # get parameters
    params=self.get_params(args,out=out)

    if helices_are_alpha:
      params.find_ss_structure.helices_are_alpha=True

    if params.extract_segments_from_pdb.extract in [None,'None']:
      params.extract_segments_from_pdb.extract=None
      # get libraries
      helix_lib,strand_lib,other_lib=self.get_libraries(
         params,out=out)
    else:
      helix_lib,strand_lib,other_lib=None,None,None,None

    # find segments of secondary structure
    models=self.find_ss_from_pdb(params,pdb_hierarchy=pdb_hierarchy,out=out)

    # see if we can replace any secondary structure
    all_replacement_models=self.replace_secondary_structure(
        params,models=models,
        helix_lib=helix_lib,strand_lib=strand_lib,other_lib=other_lib,
         out=out)
    replacement_model=merge_hierarchies_from_models(
       models=all_replacement_models, resid_offset=resid_offset)

    # write out results
    if params.output_files.pdb_out:
      params.output_files.pdb_out = \
         replacement_model.hierarchy.write_pdb_or_mmcif_file(
           target_filename = params.output_files.pdb_out)
      print("\nWriting output PDB file to %s" %(
        params.output_files.pdb_out), file=out)
    self.replacement_model=replacement_model

  def replacement_hierarchy(self):
    return self.replacement_model.hierarchy

  def output_number_of_residues_by_segment(self):
    return self.model_output_number_of_residues_by_segment

  def get_params(self,args,out=sys.stdout):
    command_line = iotbx.phil.process_command_line_with_files(
      reflection_file_def="input_files.map_coeffs_file",
      map_file_def="input_files.map_file",
      pdb_file_def="input_files.pdb_in",
      args=args,
      master_phil=master_phil)
    params = command_line.work.extract()
    print("\nRegularize from pdb: Use PDB segment to replace parts of a model", file=out)
    master_phil.format(python_object=params).show(out=out)
    return params


  def find_ss_from_pdb(self,params,pdb_hierarchy=None,out=sys.stdout):
    # read in model and split into segments
    models=get_and_split_model(pdb_in=params.input_files.pdb_in,
      pdb_hierarchy=pdb_hierarchy,
      get_start_end_length=True,out=out)

    # Note what we have
    self.model_replacement_segment_summaries=[]
    for model in models:
      rss=replacement_segment_summary(model=model)
      self.model_replacement_segment_summaries.append(rss)

    # identify secondary structure
    fss=find_secondary_structure(params=params,models=models,
     ss_by_chain=False,  # required (actually not as ignored for model input)
     helices_are_alpha=params.find_ss_structure.helices_are_alpha,
       out=out)
    return fss.models

  def get_libraries(self,params,out=sys.stdout):
    # read in libraries
    print("Reading libraries...", file=out)
    if params.control.verbose:
      lout=out
    else:
      lout=null_out()
    helix_lib=segment_library(segment_class=helix,params=params.alpha,out=lout)
    strand_lib=segment_library(segment_class=strand,params=params.beta,out=lout)
    other_lib=segment_library(segment_class=other,params=params.other,out=lout)

    print("Libraries read with %d helices and %d strands and %d other\n" %(
      len(helix_lib.segments),len(strand_lib.segments),len(other_lib.segments)), file=out)
    return helix_lib,strand_lib,other_lib

  def get_ss_structure(self,params,model=None,out=sys.stdout):
    if params.control.verbose:
      print("\nLooking for secondary structure in model %d with %d residues" %(
        model.info['chain_number'],model.hierarchy.overall_counts().n_residues)+\
        " starting at %d" %(model.hierarchy.first_resseq_as_int()), file=out)

    if params.alpha.find_alpha:
      find_alpha=find_helix(params=params.alpha,model=model,
       extract_segments_from_pdb=params.extract_segments_from_pdb.extract,
         verbose=params.control.verbose,out=out)
      if params.control.verbose:
        find_alpha.show_summary(out=out)
    else:
      find_alpha=None

    if params.beta.find_beta:
      find_beta=find_beta_strand(params=params.beta,model=model,
         extract_segments_from_pdb=params.extract_segments_from_pdb.extract,
         verbose=params.control.verbose,out=out)
      if params.control.verbose:
        find_beta.show_summary(out=out)
    else:
      find_beta=None

    if params.other.find_other:
      find_other=find_other_structure(params=params.other,model=model,
         find_alpha=find_alpha,find_beta=find_beta,
         extract_segments_from_pdb=\
            params.extract_segments_from_pdb.extract,
         verbose=params.control.verbose,out=out)
      if params.control.verbose:
        find_other.show_summary(out=out)
    else:
      find_other=None

    return find_alpha,find_beta,find_other

  def sort_connected_groups(self,connected_groups,sort_by_start=False,
       small_number=-0.0001):
    sort_list=[]
    for cg in connected_groups:
      if sort_by_start:
        sort_list.append(
          [cg.get_left_connection()+cg.get_score()*small_number,cg])
      else:
        sort_list.append([cg.get_score(),cg])
    sort_list.sort(key=itemgetter(0))
    if not sort_by_start: # high to low in score, low to high in sort_by_start
      sort_list.reverse()
    connected_groups=[]
    for sc,cg in sort_list:
      connected_groups.append(cg)
    return connected_groups

  def any_cg_has_insertions_deletions(self,connected_groups):
    for cg in connected_groups:
      if cg.has_insertions_deletions():  return True
    return False

  def remove_element(self,list_elements=[],element=None):
    if not element in list_elements:
      return list_elements
    else:
      new_list_elements=[]
      for x in list_elements:
         if x!=element: new_list_elements.append(x)
      return new_list_elements

  def combine_groups_by_residue(self,
      connected_groups,maximum_junction_rmsd=None,
      verbose=None,out=sys.stdout):
    connected_groups=self.sort_connected_groups(connected_groups)
    # sorted on score
    found=True
    while found:
      used_groups=[]
      found=False
      for cg in connected_groups:
        if found: break
        if cg in used_groups: continue
        used_groups.append(cg)
        for cg1 in connected_groups[1:]:
          if found: break
          if cg1 in used_groups: continue
          shared_residue_range=cg.shared_residue_range(cg1)
          if shared_residue_range:
            left_cg,right_cg=cg.get_left_right_cg(cg,cg1)
            original_right_cg_score=right_cg.get_score()

            found=left_cg.join_by_residue(other=right_cg,
              shared_residue_range=shared_residue_range,
              maximum_junction_rmsd=maximum_junction_rmsd)
            if found:
              combined_score=left_cg.get_score()
              used_groups=self.remove_element(
                 list_elements=used_groups,element=left_cg)
              new_groups=[]
              for poss_cg in connected_groups:
                if poss_cg != right_cg or original_right_cg_score>combined_score:
                  new_groups.append(poss_cg)
              connected_groups=new_groups
              connected_groups=self.sort_connected_groups(connected_groups)
              break

    connected_groups=self.sort_connected_groups(
       connected_groups,sort_by_start=True)
    if verbose:
      print("\nNumber of groups after combining by shared residue: %d"%(
         len(connected_groups)), file=out)
      print("\nConnected groups after combining by shared residue:", file=out)
      for cg in connected_groups:
        print(cg.show_summary(out=out))

    # remove overlapping now
    connected_groups=self.sort_and_remove_overlapping_groups(connected_groups,
      trim=True,contained_only=False)

    return connected_groups

  def combine_groups(self,connected_groups,verbose=None,out=sys.stdout):
    connected_groups=self.sort_connected_groups(connected_groups)
    # sorted on score
    found=True
    while found:
      used_groups=[]
      found=False
      new_cg=None
      for cg in connected_groups:
        if cg in used_groups: continue
        used_groups.append(cg)
        if found: break
        for cg1 in connected_groups[1:]:
          if cg1 in used_groups: continue
          if found: break
          if cg.shared_segment(cg1):
            found=cg.join(cg1)
            if found:
              connected_groups=self.sort_and_remove_overlapping_groups(
                connected_groups)
              connected_groups=self.sort_connected_groups(connected_groups)
    connected_groups=self.sort_connected_groups(
       connected_groups,sort_by_start=True)

    if verbose:
      print("\nNumber of groups after combining by shared segment: %d"%(
         len(connected_groups)), file=out)
      print("\nConnected groups after combining by shared segment:", file=out)
      for cg in connected_groups:
        print(cg.show_summary(out=out))

    connected_groups=self.sort_and_remove_overlapping_groups(
       connected_groups,out=out)

    return connected_groups

  def sort_and_remove_overlapping_groups(self,
       connected_groups,contained_only=True,
       trim=False,verbose=None,out=sys.stdout):

    connected_groups=self.sort_connected_groups(
        connected_groups) # Sorted on score

    if trim:
      connected_groups=self.trim_overlapping_groups(connected_groups)
    else:
      connected_groups=self.remove_overlapping_groups(
         connected_groups,contained_only=contained_only)

    # now sort on start_position
    connected_groups=self.sort_connected_groups(
       connected_groups,sort_by_start=True)

    if verbose:
      print("\nNumber of groups after removing overlapping: %d"%(
         len(connected_groups)), file=out)
      print("\nConnected groups after removing overlapping:", file=out)
      for cg in connected_groups:
        print(cg.show_summary(out=out))

    return connected_groups

  def trim_overlapping_groups(self,connected_groups):

    cg_dict={}
    for cg in connected_groups:
      cg_dict[cg]={}
      for i in range(cg.get_left_connection(),cg.get_right_connection()+1):
        cg_dict[cg][i]=None  # present

    new_groups=[]
    # mark all residues covered by a better segment and keep remaining good ones
    for i in range(len(connected_groups)):
      cg=connected_groups[i]
      keys=list(cg_dict[cg].keys())
      keys.sort()
      ok=cg.trim_residues(trim_dict=cg_dict[cg])
      if ok:
        new_groups.append(cg)
        for j in range(i+1,len(connected_groups)):
          cg_j=connected_groups[j]
          for k in range(cg.get_left_connection(),cg.get_right_connection()+1):
            if k in cg_dict[cg_j].keys():
              cg_dict[cg_j][k]=True


    return new_groups

  def remove_overlapping_groups(self,connected_groups,contained_only=False):
     removed_list=[None]*len(connected_groups)
     for i in range(len(connected_groups)):
       for j in range(i+1,len(connected_groups)):
         if removed_list[j]:
            continue # already removed
         if contained_only:
           if connected_groups[i].contains(connected_groups[j]):
             removed_list[j]=True
         elif connected_groups[i].overlaps_with(connected_groups[j]):
            removed_list[j]=True
     new_list=[]
     for cg,rm in zip(connected_groups,removed_list):
       if not rm:
         new_list.append(cg)
     return new_list


  def get_left_and_right_connections_for_segment(self,rs,minimum_length=2,
      maximum_overlap=1):
    target_start=rs.info['target_start_resno']
    target_end=rs.info['target_end_resno']
    length=rs.info['length']
    minimum_overlap=rs.info['minimum_overlap']
    target_length=rs.info['target_length']
    connection_pairs=[]
    if target_length!=length:  # allows variable lengths
      connection_pairs.append([target_start,target_end])
    else:
      start_of_left_connections=target_start+minimum_overlap
      end_of_left_connections=target_start+maximum_overlap
      start_of_right_connections=target_end-maximum_overlap
      end_of_right_connections=target_end-minimum_overlap
      if end_of_left_connections>=end_of_right_connections:
        end_of_left_connections=end_of_right_connections-1
      if start_of_right_connections<=start_of_left_connections:
        start_of_right_connections=start_of_left_connections+1

      for i in range(start_of_left_connections,end_of_left_connections+1):
        for j in range(start_of_right_connections,end_of_right_connections+1):
          if i>=j: continue
          connection_pairs.append([i,j])

    return connection_pairs

  def get_model_from_connected_groups(self,connected_groups,
       model_to_match=None,renumber=None,verbose=None,out=sys.stdout):
    models=[]
    for cg,is_left_end,is_right_end in zip(
       connected_groups,
       [True]+[False]*(len(connected_groups)-1),
       [False]*(len(connected_groups)-1)+[True]):
      models.append(
        cg.as_model(is_left_end=is_left_end,is_right_end=is_right_end,
        model_to_match=model_to_match,verbose=verbose,out=out))
    if not models:
       return None
    if renumber:
      new_model=merge_hierarchies_from_models(models=models,resid_offset=100,
        first_residue_number=models[0].hierarchy.first_resseq_as_int())
    else:
      new_model=merge_hierarchies_from_models(models=models,renumber=False)
    return new_model

  def sort_connection_list(self,connection_list):
    connection_list.sort()
    connection_list.reverse()
    connections=[]
    best_score=0.0
    first=True
    for [score,connection] in connection_list:
      connections.append(connection)
      if first:
        best_score=score
        first=False
    return connections,best_score

  def get_all_connections(self,cg=None,used_connected_groups=None,
        possible_left_connection_dict=None,
        possible_right_connection_dict=None,
        maximum_junction_rmsd=None,look_ahead_score_only=False,
        make_right_connections=True,make_left_connections=True):
    found=False
    best_score=0.
    # make all right and left connections possible to this cg
    right_connection_list=[]
    right_connections=[]
    for possible_right_connection in possible_right_connection_dict.get(
           cg.get_right_connection(),[]):
      if not make_right_connections:continue
      if possible_right_connection in used_connected_groups:
        continue
      # make sure this connection does not duplicate one we have
      if cg.get_right_segment_hierarchy()==\
              possible_right_connection.get_left_segment_hierarchy():
        continue
      rms=cg.get_connection_rms(cg.connected_group_segments[-1],
             possible_right_connection.connected_group_segments[0])
      if rms is None or rms > maximum_junction_rmsd:
        continue
      score=possible_right_connection.get_score()
      if not look_ahead_score_only:
        best_score=self.get_all_connections(
          cg=possible_right_connection,
          used_connected_groups=used_connected_groups,
          possible_left_connection_dict=possible_left_connection_dict,
          possible_right_connection_dict=possible_right_connection_dict,
          maximum_junction_rmsd=maximum_junction_rmsd,make_left_connections=False,
          look_ahead_score_only=True)
        score+=best_score # adds on score for best right connection
      right_connection_list.append([score,possible_right_connection])
    right_connections,best_score=self.sort_connection_list(right_connection_list)
    if look_ahead_score_only:
      return best_score
    # Add on the best one including look-ahead score
    if right_connections:
      cg.add_connected_group_to_right(right_connections[0])
      used_connected_groups+=right_connections
      found=True

    # and now for left direction
    left_connection_list=[]
    left_connections=[]
    for possible_left_connection in possible_left_connection_dict.get(
           cg.get_left_connection(),[]):
      if not make_left_connections:continue
      if possible_left_connection in used_connected_groups:
        continue
      # make sure this connection does not duplicate one we have
      if cg.get_left_segment_hierarchy()==\
              possible_left_connection.get_right_segment_hierarchy():
        continue
      rms=cg.get_connection_rms(
             possible_left_connection.connected_group_segments[-1],
             cg.connected_group_segments[0],)
      if rms is None or rms > maximum_junction_rmsd:
        continue
      score=possible_left_connection.get_score()
      if not look_ahead_score_only:
        best_score=get_all_connections(
          cg=possible_left_connection,
          used_connected_groups=used_connected_groups,
          possible_left_connection_dict=possible_left_connection_dict,
          possible_right_connection_dict=possible_right_connection_dict,
          maximum_junction_rmsd=maximum_junction_rmsd,
          make_right_connections=False,
          look_ahead_score_only=True)
        score+=best_score # adds on score for best right connection
      left_connection_list.append(
        [score,possible_left_connection])
    left_connections,best_score=self.sort_connection_list(left_connection_list)
    if look_ahead_score_only:
      return best_score
    # Add on the best one
    if left_connections:
      new_cg=left_connections[0]
      new_cg.add_connected_group_to_right(cg)
      used_connected_groups+=left_connections
      cg=new_cg
      found=True
    return cg,used_connected_groups,found

  def link_groups(self,params=None,model=None,connected_groups=None,
      maximum_junction_rmsd=None,
      other_lib=None,out=sys.stdout):

    found=False
    if len(connected_groups) < 2: return connected_groups,found

    if params.control.verbose:
      print("\nLinking groups that are not yet connected", file=out)
    connected_groups=self.sort_connected_groups(connected_groups,
      sort_by_start=True) # get them in order
    new_connected_groups=[]
    good_enough_rmsd=params.regularization.good_enough_ratio*\
                 params.other.maximum_rmsd
    for cg1,cg2 in zip(connected_groups[:-1],connected_groups[1:]):
      if cg1.get_right_connection()+1==cg2.get_left_connection():

        # Create a segment with the junction in it. Then see if we can replace
        #   it with something that works and matches all the main-chain atoms
        #   at crossover points

        # see if we can find a segment from our other_lib that covers some of
        # the residues in this junction.  Try 1234 residues in from either end
        # create a hierarchy with cg1+cg2

        # if cg1 has an insert, start it with lower residue numbers to compensate
        cg1_model=cg1.as_model(offset_to_renumber_right_end_at_target=True)
        cg2_model=cg2.as_model()

        combined_model=merge_hierarchies_from_models(
          models=[cg1_model,cg2_model],
          resid_offset=1,
          first_residue_number=cg1_model.hierarchy.first_resseq_as_int(),
          renumber=True)

        sites=combined_model.hierarchy.atoms().extract_xyz()

        # now select a few possibilities with at least 1 res overlap on each end
        #  and no more than other.standard_length residues
        first_resno=combined_model.hierarchy.first_resseq_as_int()
        last_resno=combined_model.hierarchy.last_resseq_as_int()
        before_junction=cg1_model.hierarchy.last_resseq_as_int()

        target_overlap=2
        i_start=max(first_resno,before_junction-target_overlap+1)
        i_end=min(last_resno-2*target_overlap+1,before_junction)
        if i_start>i_end: continue # nothing to do

        if other_lib.index_length is None:
          start_position=None
        else:
          start_position=max(0,1+(other_lib.index_length-2*target_overlap)//2)

        best_cg=None
        best_rms=None
        segment_list=[]
        for i in range(i_start,i_end+1):
          atom_selection=\
           "resid %s through %s and (name n or name c or name ca or name o)" %(
            resseq_encode(i),resseq_encode(i+2*target_overlap-1))
          segment_list.append(other(params=params.other,
            start_resno=i,
            hierarchy= combined_model.hierarchy.apply_atom_selection(
            atom_selection)))
        for segment in segment_list:
          # now find replacements for this segment
          rs,rms,log_frequency,lc,rc,lc_other,rc_other=\
             other_lib.get_replacement_segment(
              model_segment=segment,
              maximum_rmsd=params.other.maximum_rmsd,
              good_enough_rmsd=good_enough_rmsd,
              start_position=start_position,
              score_by_main_chain_rmsd=True,
              before_junction=before_junction,
              delta_residues=0,out=out)
          if rms is not None and rms < params.other.maximum_rmsd:
            if best_cg is None or best_rms> rms:
              # make a connected_segment object with residues from
              #lc_other to rc_other and link it
              contains_left_end_of_chain,contains_right_end_of_chain=\
                segment.contains_ends(
                  first_resno_of_chain=model.info['model_start_resno'],
                  last_resno_of_chain=model.info['model_end_resno'])
              replacement_segment=model_info(hierarchy=rs,
                info={'target_start_resno':segment.get_start_resno(),
                     'target_end_resno':segment.get_end_resno(),
                     'target_length':
                        segment.get_end_resno()-segment.get_start_resno()+1,
                     'length':rs.overall_counts().n_residues,
                     'minimum_overlap':params.other.minimum_overlap,
                     'rms':rms,
                     'base_score':segment.base_score,
                     'contains_right_end_of_chain':contains_right_end_of_chain,
                     'contains_left_end_of_chain':contains_left_end_of_chain,
                     'log_frequency':log_frequency},
                )
              best_cg=connected_group(segment=replacement_segment,
                 start_resno=lc,end_resno=rc)
              best_rms=rms
              if rms <= good_enough_rmsd:
                break # done
        if best_rms:
           best_cg.get_score()
           new_connected_groups.append(best_cg)
           found=True
    connected_groups+=new_connected_groups

    if params.control.verbose:
      print("\nGroups after adding linker groups:", file=out)
      for cg in connected_groups: print(cg.show_comprehensive_summary())
      for cg in connected_groups:
        print("\nGROUP:")
        print(cg.as_model(is_left_end=True).hierarchy.as_pdb_string()) # PDB OK debug only

    connected_groups=self.sort_and_remove_overlapping_groups(connected_groups)

    if params.control.verbose:
      print("\nGroups after removing overlapping:", file=out)
      for cg in connected_groups: print(cg.show_comprehensive_summary())
      for cg in connected_groups:
        print("\nGROUP:")
        print(cg.as_model(is_left_end=True).hierarchy.as_pdb_string()) # PDB OK debug only

    connected_groups=self.combine_groups_by_residue(connected_groups,
       maximum_junction_rmsd=maximum_junction_rmsd,
       verbose=params.control.verbose,out=out)

    return connected_groups,found

  def get_connected_groups_from_segments(self,params=None,
       replacement_segments=None,
       maximum_overlap=None,
       verbose=None,out=sys.stdout):
    # Set up connected groups
    connected_groups=[]
    possible_right_connection_dict={}
    possible_left_connection_dict={}
    for rs in replacement_segments:
      connection_pairs=self.get_left_and_right_connections_for_segment(rs,
        maximum_overlap=maximum_overlap)
      for lc,rc in connection_pairs:
          cg=connected_group(segment=rs,start_resno=lc,end_resno=rc)
          connected_groups.append(cg)
          if not lc in possible_right_connection_dict:
            possible_right_connection_dict[lc]=[]
          possible_right_connection_dict[lc].append(cg)
          if not rc in possible_left_connection_dict:
            possible_left_connection_dict[rc]=[]
          possible_left_connection_dict[rc].append(cg)
    # here possible_right_connection_dict[rc] is a list of connected_groups that
    #   can be linked to the right with their left end residue equal to rc

    connected_groups=self.sort_connected_groups(
       connected_groups,sort_by_start=True)

    if verbose:
      print("\nNumber of starting connected groups: %d"%(
          len(connected_groups)), file=out)
      print("\nConnected groups:", file=out)
      for cg in connected_groups:
        print(cg.show_summary(out=out))

    return connected_groups,possible_left_connection_dict,\
        possible_right_connection_dict

  def connect_groups(self,connected_groups=None,
     possible_left_connection_dict=None,
     possible_right_connection_dict=None,
     maximum_junction_rmsd=None,
     verbose=None,out=sys.stdout):

    if verbose:
      print("\nCreating new connected groups", file=out)

    used_connected_groups=[]
    final_connected_groups=[]
    for cg in connected_groups:  # work down the list
      if cg in used_connected_groups: continue
      used_connected_groups.append(cg)
      while 1:
        cg,used_connected_groups,found=self.get_all_connections(cg=cg,
          used_connected_groups=used_connected_groups,
          possible_left_connection_dict=possible_left_connection_dict,
          possible_right_connection_dict=possible_right_connection_dict,
          maximum_junction_rmsd=maximum_junction_rmsd)
        if not found: break
      final_connected_groups.append(cg)
    connected_groups=final_connected_groups
    connected_groups=self.sort_and_remove_overlapping_groups(connected_groups)
    connected_groups=self.sort_connected_groups(
       connected_groups,sort_by_start=True)

    if verbose:
      print("\nNumber of connected groups after connecting: %d"%(
        len(connected_groups)), file=out)
      print("\nConnected groups after connecting:", file=out)
      for cg in connected_groups:
        print(cg.show_summary(out=out))

    return connected_groups

  def get_connections(self,params,model=None,
     replacement_segments=None,maximum_overlap=None,
     maximum_junction_rmsd=None,other_lib=None,out=sys.stdout):
    # make longest possible contiguous segments from the connections available

    # A replacement_segment is a model_info object with a hierarchy that
    #  covers the whole segment and has info['target_start_resno'] etc that
    #  specifies what start residue
    #  we plan to use for this hierarchy (will not normally match the residue
    #  number in the hierarchy which is arbitrary)

    if params.control.verbose:
      print("\nReplacement segments:", file=out)
      for rs in replacement_segments:
        print(rs.show_summary(out=out))

    # Initial single connected groups and possible left and right connections
    connected_groups,possible_left_connection_dict,\
         possible_right_connection_dict=\
      self.get_connected_groups_from_segments(params=params,
        maximum_overlap=maximum_overlap,
        replacement_segments=replacement_segments,out=out)

    # link up the connected groups using left and right connections
    connected_groups=self.connect_groups(connected_groups=connected_groups,
        possible_left_connection_dict=possible_left_connection_dict,
        possible_right_connection_dict=possible_right_connection_dict,
        maximum_junction_rmsd=maximum_junction_rmsd,
        verbose=params.control.verbose,out=out)

    # combine groups by shared segments
    connected_groups=self.combine_groups(connected_groups,
      verbose=params.control.verbose,out=out)

    # combine groups by shared residue
    connected_groups=self.combine_groups_by_residue(connected_groups,
       maximum_junction_rmsd=maximum_junction_rmsd,
       verbose=params.control.verbose,out=out)

    n=0
    found=True
    while found:
      n+=1
      # combine groups by finding spanning segment
      connected_groups,found=self.link_groups(params=params,
        model=model,connected_groups=connected_groups,
        other_lib=other_lib,maximum_junction_rmsd=maximum_junction_rmsd,
        out=out)

      # final cleanup
      connected_groups=self.sort_and_remove_overlapping_groups(connected_groups,
        trim=True,contained_only=False)
      if n>1: break # makes infinite loop

    if params.control.verbose:
      print("\nFinal groups for this segment: ", file=out)
      for cg in connected_groups:
        print(cg.show_comprehensive_summary(), file=out)

    return connected_groups

  def assemble_segments(self,params,model=None,
        other_lib=None,
        replacement_segments=None,
        out=sys.stdout):
    # We have a set of segments with target_start_resno, target_end_resno,
    #    length (actual), target_length.  If length==target_length, any residue
    #    can be used as crossover, otherwise, only the end residues

    if params.control.verbose:
      print("\nAssembling segments for model. Start:"+\
         " %d length: %d Replacement segments: %d" %(
       model.hierarchy.first_resseq_as_int(),
       model.hierarchy.overall_counts().n_residues,
       len(replacement_segments)), file=out)
    connected_groups=self.get_connections(params,
      replacement_segments=replacement_segments,
      model=model,
      other_lib=other_lib,
      maximum_junction_rmsd=params.regularization.maximum_junction_rmsd,
      maximum_overlap=params.regularization.maximum_overlap,out=out)
    return connected_groups

  def replace_secondary_structure(self,params,models=None,
      helix_lib=None,strand_lib=None,other_lib=None,
       out=sys.stdout):
    all_replacement_models=[]
    completeness_of_all_replacement_models=[]
    insertions_deletions_of_all_replacement_models=[]
    for rss in self.model_replacement_segment_summaries:
      model=rss.model
      # get segments that might replace structure in this model (1 chain)
      replacement_segments=self.get_replacement_segments(
        params,model=model,helix_lib=helix_lib,strand_lib=strand_lib,
        other_lib=other_lib,out=out)

      if params.extract_segments_from_pdb.extract is not None:
        all_replacement_models+=replacement_segments
        insertions_deletions_of_all_replacement_models.append(True)
        completeness_of_all_replacement_models.append(True)
      else:
        print("\nReplacing segment %d (n=%d," %(
            model.info['chain_number'],
            model.hierarchy.overall_counts().n_residues)+\
            " %d - %d) ..." %(
             model.hierarchy.first_resseq_as_int(),
             model.hierarchy.last_resseq_as_int()), file=out)

        connected_groups=self.assemble_segments(params,model=model,
          other_lib=other_lib,
          replacement_segments=replacement_segments,out=out)
        if connected_groups:
          is_complete=(
            len(connected_groups)==1 and connected_groups[0].is_complete())
          has_insertions_deletions=self.any_cg_has_insertions_deletions(
            connected_groups)

          replacement_model=self.get_model_from_connected_groups(
            connected_groups,
            renumber=has_insertions_deletions,
            model_to_match=model,verbose=params.control.verbose,out=out)

          rss.add_connected_groups(connected_groups)
          rss.add_replacement_model(replacement_model)
          rss.set_is_complete(is_complete)
          rss.set_has_insertions_deletions(
            self.any_cg_has_insertions_deletions(connected_groups))

          all_replacement_models.append(replacement_model)
          completeness_of_all_replacement_models.append(is_complete)
          insertions_deletions_of_all_replacement_models.append(
            has_insertions_deletions)
          id=model.info['chain_number']
          start_residue=replacement_model.hierarchy.first_resseq_as_int()
          self.model_output_number_of_residues_by_segment[id]=\
            replacement_model.hierarchy.overall_counts().n_residues
          if params.control.verbose:
            print("Replacement model for segment %d with %d residues " %(
              model.info['chain_number'],
              replacement_model.hierarchy.overall_counts().n_residues) + \
             " from %d to %d:" %(
             model.hierarchy.first_resseq_as_int(),
             model.hierarchy.last_resseq_as_int()), file=out)
        else:
          print("No replacement model found for this segment", file=out)
          all_replacement_models.append(None)
          completeness_of_all_replacement_models.append(None)
          insertions_deletions_of_all_replacement_models.append(None)

    self.is_complete = (completeness_of_all_replacement_models.count(True) ==
       len(completeness_of_all_replacement_models))

    return all_replacement_models

  def get_replacement_segments(self,params,model=None,
      helix_lib=None,strand_lib=None,other_lib=None,
      out=sys.stdout):

      replacement_segments=[]

      for replacement_type,segment_lib,\
          segment_params  in zip(
         ['alpha','beta','other'],
         [helix_lib,strand_lib,other_lib],
         [params.alpha,params.beta,params.other]):
        find_object=getattr(model,'find_%s' %(replacement_type))
        if not find_object or not segment_params.replace :continue
        for segment in find_object.get_segments():
          if params.extract_segments_from_pdb.extract is not None:
            if params.extract_segments_from_pdb.extract==replacement_type:
              replacement_segments.append(model_info(hierarchy=segment.hierarchy))
            else:
              pass # do nothing, just writing out one type of structure
          else: # usual
            delta_residues=segment.optimal_delta_length
            if delta_residues and params.control.verbose:
              print("\nReplacing segment from model length " + \
                "%d with library segment length %d"%(
                segment.length(),segment.optimal_delta_length+segment.length(),), file=out)
            elif params.control.verbose:
              print("\nReplacing segment from model length " +\
                "%d with library strand" %(segment.length()), file=out)
            rs,rms,log_frequency,lc,rc,lc_other,rc_other=\
               segment_lib.get_replacement_segment(
              model_segment=segment,
              maximum_rmsd=segment_params.maximum_rmsd,
              good_enough_rmsd=\
              params.regularization.good_enough_ratio*segment_params.maximum_rmsd,
              delta_residues=delta_residues,
              verbose=params.control.verbose,out=out)
            if rs: # Note length will be: segment.length()+delta_residues
              contains_left_end_of_chain,contains_right_end_of_chain=\
                segment.contains_ends(
                  first_resno_of_chain=model.info['model_start_resno'],
                  last_resno_of_chain=model.info['model_end_resno'])

              replacement_segments.append(model_info(hierarchy=rs,
                info={'target_start_resno':segment.get_start_resno(),
                     'target_end_resno':segment.get_end_resno(),
                     'target_length':
                        segment.get_end_resno()-segment.get_start_resno()+1,
                     'length':rs.overall_counts().n_residues,
                     'minimum_overlap':segment_params.minimum_overlap,
                     'rms':rms,
                     'base_score':segment.base_score,
                     'contains_right_end_of_chain':contains_right_end_of_chain,
                     'contains_left_end_of_chain':contains_left_end_of_chain,
                     'log_frequency':log_frequency},
                ))

      return replacement_segments


if __name__=="__main__":

  from mmtbx.secondary_structure.regularize_from_pdb import replace_with_segments_from_pdb
  r=replace_with_segments_from_pdb(args=sys.argv[1:],pdb_hierarchy=None,
    out=sys.stdout)

  print_output=False
  if print_output:
    print("Output model:\n%s" %(r.replacement_hierarchy().as_pdb_string())) # PDB OK debug only


  print("\nSummary by segment:")
  for rss in r.model_replacement_segment_summaries:
    rss.show_summary()


 *******************************************************************************


 *******************************************************************************
mmtbx/secondary_structure/secondary_structure_params.py
from __future__ import absolute_import, division, print_function

import iotbx.phil

# standard input params for find_secondary_structure

alpha_params=iotbx.phil.parse("""

     name = Alpha helix
       .type = str
       .help = Secondary structure name (helix)

     span = 3.5
       .type = float
       .help = number of residues included in i to i+span vector differences
       .short_caption = span for alpha structure

     buffer_residues = 0
       .type = int
       .help = number of residues included on ends of segments
       .short_caption = buffer residues for alpha structure

     n_link_min = 0
       .type = int
       .help = Minimum number of residues linking adjacent segments \
           in H-bonded structure (i.e., HELIX/SHEET records)
       .short_caption = Minimum link residues between segments

     standard_length = 6
       .type = int
       .help = standard length for alpha structure
       .short_caption = Standard length for alpha structure

     minimum_length = 6
       .type = int
       .help = Minimum length for alpha structure
       .short_caption = Minimum length for alpha structure

     residues_per_turn = 3.6
       .type = float
       .help = Alpha helix residues per turn
       .short_caption = Alpha helix residues per turn

     rise = 1.54
       .type = float
       .help = Alpha helix rise per reside
       .short_caption = Alpha helix rise

     minimum_overlap = 1
       .type = int
       .help = Minimum overlap at ends for alpha structure
       .short_caption = Minimum overlap for alpha structure

     rise_tolerance = 0.5
       .type = float
       .help = Tolerance in rise of helices in input file
       .short_caption = Tolerance in rise of helices

     target_i_ip3 = None
       .type = float
       .help = None (target i->i+3 distance)
       .short_caption = none

     tol_i_ip3 = None
       .type = float
       .help = None (tolerance in target i->i+3 distance)
       .short_caption = none

     dot_min_single = 0.3
       .type = float
       .help = Target dot product of i->i+1 with overall direction
       .short_caption = Target dot i to i+1

     dot_min =  0.9
       .type = float
       .help = minimum dot product of directions of average \
          of i-to-i+3  and i-to i+4 vectors to overall average direction

     allow_insertions = True
       .type = bool
       .help = Allow insertions in helices (adding residues)
       .short_caption= Allow insertions in helices

     allow_deletions = False
       .type = bool
       .help = Allow deletions in helices (adding residues)
       .short_caption= Allow deletions in helices

     base_score = 100.
       .type = float
       .help = Base score for helices
       .short_caption = Base score for helices

""")

three_ten_params=iotbx.phil.parse("""
     name = 3-10 helix
       .type = str
       .help = Secondary structure name (alpha helix)

     span = 3
       .type = float
       .help = number of residues included in i to i+span vector differences.\
               This is for three-ten helices
       .short_caption = span for three-ten structure

     buffer_residues = 0
       .type = int
       .help = number of residues included on ends of segments for three-ten
       .short_caption = buffer residues for three-10 structure

     n_link_min = 0
       .type = int
       .help = Minimum number of residues linking adjacent segments \
           in H-bonded structure (i.e., HELIX/SHEET records)
       .short_caption = Minimum link residues between segments

     standard_length = 6
       .type = int
       .help = standard length for three_ten structure
       .short_caption = Standard length for three_ten structure

     minimum_length = 6
       .type = int
       .help = Minimum length for three_ten structure
       .short_caption = Minimum length for three_ten structure

     residues_per_turn = 3
       .type = float
       .help = Three-ten helix residues per turn
       .short_caption = Three-ten helix residues per turn

     rise = 2.0
       .type = float
       .help = Three ten helix rise per reside
       .short_caption = Three ten helix rise

     minimum_overlap = 1
       .type = int
       .help = Minimum overlap at ends for three_ten structure
       .short_caption = Minimum overlap for three_ten structure

     rise_tolerance = 0.5
       .type = float
       .help = Tolerance in rise of helices in input file
       .short_caption = Tolerance in rise of helices

     target_i_ip3 = None
       .type = float
       .help = None (target i->i+3 distance)
       .short_caption = none

     tol_i_ip3 = None
       .type = float
       .help = None (tolerance in target i->i+3 distance)
       .short_caption = none

     dot_min_single = 0.5
       .type = float
       .help = Target dot product of i->i+1 with overall direction
       .short_caption = Target dot i to i+1

     dot_min =  0.9
       .type = float
       .help = minimum dot product of directions of average \
          of i-to-i+3  and i-to i+4 vectors to overall average direction

     allow_insertions = False
       .type = bool
       .help = Allow insertions in helices (adding residues)
       .short_caption= Allow insertions in helices

     allow_deletions = False
       .type = bool
       .help = Allow deletions in helices (adding residues)
       .short_caption= Allow deletions in helices

     base_score = 1.
       .type = float
       .help = Base score for 3_10 helices
       .short_caption = Base score for 3_10 helices

""")

pi_params=iotbx.phil.parse("""
     name = Pi helix
       .type = str
       .help = Secondary structure name (pi helix)

     span = 4.0
       .type = float
       .help = number of residues included in i to i+span vector differences
       .short_caption = span for pi structure

     buffer_residues = 0
       .type = int
       .help = number of residues included on ends of segments
       .short_caption = buffer residues for pi structure

     n_link_min = 0
       .type = int
       .help = Minimum number of residues linking adjacent segments \
           in H-bonded structure (i.e., HELIX/SHEET records)
       .short_caption = Minimum link residues between segments

     standard_length = 6
       .type = int
       .help = standard length for pi structure
       .short_caption = Standard length for pi structure

     minimum_length = 6
       .type = int
       .help = Minimum length for pi structure
       .short_caption = Minimum length for pi structure

     residues_per_turn = 4.1
       .type = float
       .help = Pi helix residues per turn
       .short_caption = Pi helix residues per turn

     rise = 0.95
       .type = float
       .help = Pi helix rise per reside
       .short_caption = Pi helix rise

     minimum_overlap = 1
       .type = int
       .help = Minimum overlap at ends for pi structure
       .short_caption = Minimum overlap for pi structure

     rise_tolerance = 0.5
       .type = float
       .help = Tolerance in rise of pi helices
       .short_caption = Tolerance in rise of pi helices

     target_i_ip3 = None
       .type = float
       .help = None (target i->i+3 distance)
       .short_caption = none

     tol_i_ip3 = None
       .type = float
       .help = None (tolerance in target i->i+3 distance)
       .short_caption = none

     dot_min_single = 0.1
       .type = float
       .help = Target dot product of i->i+1 with overall direction for pi
       .short_caption = Target dot i to i+1 for pi

     dot_min =  0.9
       .type = float
       .help = minimum dot product of directions of average \
          of i-to-i+3  and i-to i+4 vectors to overall average direction

     allow_insertions = False
       .type = bool
       .help = Allow insertions in helices (adding residues)
       .short_caption= Allow insertions in helices

     allow_deletions = False
       .type = bool
       .help = Allow deletions in helices (adding residues)
       .short_caption= Allow deletions in helices

     base_score = 1.
       .type = float
       .help = Base score for pi helices
       .short_caption = Base score for pi helices

""")

beta_params=iotbx.phil.parse("""

     name = Beta strand
       .type = str
       .help = Secondary structure name (strand)

     span = 2
       .type = float
       .help = number of residues included in i to i+span differences
       .short_caption = span for beta structure

     buffer_residues = 0
       .type = int
       .help = number of residues included on ends of segments
       .short_caption = buffer residues for beta structure

     n_link_min = 3
       .type = int
       .help = Minimum number of residues linking adjacent segments \
           in H-bonded structure (i.e., HELIX/SHEET records)
       .short_caption = Minimum link residues between segments

     standard_length = 4
       .type = int
       .help = standard length for beta structure
       .short_caption = Standard length for beta structure

     minimum_length = 4
       .type = int
       .help = Minimum length for beta structure
       .short_caption = Minimum length for beta structure

     minimum_overlap = 1
       .type = int
       .help = Minimum overlap at ends for beta structure
       .short_caption = Minimum overlap for beta structure

     rise = 3.3
       .type = float
       .help = rise per reside for beta structure (3.2-3.4 is typical)
       .short_caption = rise per residue for beta structure

     rise_tolerance = 0.5
       .type = float
       .help = Tolerance in rise for beta structure in input file
       .short_caption = Tolerance in rise for beta structure

     target_i_ip3 = 10.
       .type = float
       .help = Target i->i+3 distance
       .short_caption = Target i to i+3 distance

     tol_i_ip3 = 1.5
       .type = float
       .help = Tolerance in target i->i+3 distance
       .short_caption = Tolerance in target i to i+3 distance

     dot_min_single = 0.5
       .type = float
       .help = Target dot product of i->i+1 with overall direction
       .short_caption = Target dot i to i+1

     dot_min =  0.75
       .type = float
       .help = minimum dot product of directions of \
          i-to-i+2 vectors to overall average direction

     allow_insertions = False
       .type = bool
       .help = Allow insertions in strands (adding residues)
       .short_caption= Allow insertions in strands

     allow_deletions = False
       .type = bool
       .help = Allow deletions in strands (adding residues)
       .short_caption= Allow deletions in strands

     base_score = 10.
       .type = float
       .help = Base score for strands
       .short_caption = Base score for strands

     max_sheet_ca_ca_dist = 6.
       .type = float
       .help = Maximum CA-CA distance between paired strands in sheets
       .short_caption = Max CA-CA distance between strands in sheets

     tolerant_max_sheet_ca_ca_dist = 8.
       .type = float
       .help = Tolerant maximum CA-CA distance between paired strands \
           in sheets
       .short_caption = Tolerant max CA-CA distance between strands \
              in sheets

     min_sheet_length = 4
       .type = int
       .help = Minimum H-bonded segment to include in a sheet
       .short_caption = Min length of segment in a sheet

     tolerant_min_sheet_length = 2
       .type = int
       .help = Tolerant minimum H-bonded segment to include in a sheet
       .short_caption = Tolerant min length of segment in a sheet

     allow_ca_only_model = True
       .type = bool
       .help = Allow the use of CA-only models, defining H-bonding for \
               N and O as if they were present
       .short_caption= Allow CA-only models



""")

other_params = iotbx.phil.parse("""
     name = Other structure
       .type = str
       .help = Secondary structure name (other)

     span = None
       .type = float
       .help = None
       .short_caption = None

     buffer_residues = 2
       .type = int
       .help = number of residues included on ends of segments
       .short_caption = buffer residues for other structure

     n_link_min = 0
       .type = int
       .help = Minimum number of residues linking adjacent segments \
           in H-bonded structure (i.e., HELIX/SHEET records)
       .short_caption = Minimum link residues between segments

     standard_length = 8
       .type = int
       .help = standard length for other structure
       .short_caption = Standard length for other structure

     minimum_length = 4
       .type = int
       .help = Minimum length for other structure
       .short_caption = Minimum length for other structure

     minimum_overlap = 1
       .type = int
       .help = Minimum overlap at ends for other structure
       .short_caption = Minimum overlap for other structure

     rise = None
       .type = float
       .help = None
       .short_caption = None

     rise_tolerance = None
       .type = float
       .help = None
       .short_caption = None

     target_i_ip3 = None
       .type = float
       .help = None (target i->i+3 distance)
       .short_caption = none

     tol_i_ip3 = None
       .type = float
       .help = None (tolerance in target i->i+3 distance)
       .short_caption = none

     dot_min_single = None
       .type = float
       .help = Target dot product of i->i+1 with overall direction
       .short_caption = Target dot i to i+1

     dot_min =  None
       .type = float
       .help = None

     allow_insertions = False
       .type = bool
       .help = Allow insertions in other (adding residues)
       .short_caption= Allow insertions in other

     allow_deletions = False
       .type = bool
       .help = Allow deletions in other (adding residues)
       .short_caption= Allow deletions in other

     base_score = 1.
       .type = float
       .help = Base score for other
       .short_caption = Base score for other
""")


 *******************************************************************************
