

 *******************************************************************************
cctbx/maptbx/fsc_curve.py
#
# Ported "AS IS" by P. Afonine, 10-JUN-2025
# Subject to revision for coding complience and formal testing (no tests!)
#
#

#
#   calculating FSC-limit value using 1/d**3 scale histograms     A.Urzhumtsev, 2025-06-10
#
from __future__ import absolute_import, division, print_function
import math

# input parameters

# 5 arrays for the complex Fourier coefficients :
# ss2
# Fx1, Fy1 - real and imaginary part of the Fourier coefficients, set1
# Fx2, Fy2 - real and imaginary part of the Fourier coefficients, set1
# FSC_cut   = 0.143 - FSC cut-off
# nbin_min  = 100   - miminal allower number of reflections per bin
# precision = 0.01  - relative (with respect to the value) precision of the anwser

# output values

# res_val     - estimated resolution corresponding to the FSC cut-off
# res_del     - estimated precision, in A
# res_min     - bottom bound of the estimation interval
# res_max     - upper bound of the estimation interval
# Ncycle      - numer of cycles done
# hist_middle - histogram, when the searched bin is at a middle of the histogram
#               indicated by reason = 'hist'
# hist_final  - histogram at the end of the search; indicated by reason = 'last'
# reason      - termination flag ('none' if failed to find the value)

# each histogram line contains 9 numbers (hist_save write such histograms to file):
# Nbin                     - number of corefficnets per bin)
# ss3min ss3max            - bin resolution limits, in 1/d**3)
# resmin                   - bin resoluion in A, corresponding to ss3max
# FSC                      - mean FSC value)
# Fav/Fmax(1), Fav/Fmax(2) - ratio of the mean to the maximum amplitude in bin,
#                            sets 1 and 2
# Fav/Fav0(1), Fav/Fav0(2) - ratio of the mean amplitudes: in this and in the 1st bin
#                            sets 1 and 2

def run(f1, f2):
  from scitbx.array_family import flex
  Fx1, Fy1, Fx2, Fy2 = [],[], [],[]
  ss2 = list( 1./flex.pow2(f1.d_spacings().data()) )
  for d1,d2 in zip(f1.data(), f2.data()):
    Fx1.append( d1.real )
    Fy1.append( d1.imag )
    Fx2.append( d2.real )
    Fy2.append( d2.imag )
  r = FSC_limit(ss2=ss2, Fx1=Fx1, Fy1=Fy1, Fx2=Fx2, Fy2=Fy2,
                    FSC_cut=0.143, nbin_min=100, precision=0.01)
  res_val, res_del, res_min, res_max, Ncycle, hist_middle, hist_final, reason = r
  return res_val, res_del

def FSC_limit(ss2, Fx1, Fy1, Fx2, Fy2, FSC_cut, nbin_min, precision, verbose=False) :
   if verbose:
     print()
     print('resolution estimation corresponding to the FSC cut-off =',f'{FSC_cut:6.3f}')
     print('relative presition required',26*' ',f'= {precision:6.3f}')
     print('minimal allowed number of coefficients per bin         = ',f'{nbin_min:5}')
     print()

   Ncoef = len(ss2)

   ss3  = [0.0 for icoef in range(Ncoef)]
   Fsq1 = [Fx1[icoef]*Fx1[icoef] + Fy1[icoef]*Fy1[icoef] for icoef in range(Ncoef)]
   Fsq2 = [Fx2[icoef]*Fx2[icoef] + Fy2[icoef]*Fy2[icoef] for icoef in range(Ncoef)]
   Fr12 = [Fx1[icoef]*Fx2[icoef] + Fy1[icoef]*Fy2[icoef] for icoef in range(Ncoef)]

   dhigh       = 0.0
   Ncycle      = 0
   reason      = 'none'
   hist_middle = []
   hist_final  = []

   Ncoef_hist = Ncoef
   ss2min    = ss2[0]
   ss2max    = 0.0
   for icoef in range(Ncoef) :
       ss2coef = ss2[icoef]
       if ss2min > ss2coef : ss2min = ss2coef
       if ss2max < ss2coef : ss2max = ss2coef
       ss3coef = ss2coef * math.sqrt(ss2coef)
       ss3[icoef] = ss3coef
   ss3max = ss2max * math.sqrt(ss2max)

   res_min0 = 1.0 / math.sqrt(ss2max)
   res_max0 = 1.0 / math.sqrt(ss2min)
   if verbose:
     print('total number of coefficients    ',f'{Ncoef:15}')
     print('in the resolution range (A)',f'{res_min0:10.4f}{res_max0:10.4f}')
     print()
     print('In the table below :')
     print('N        - consecutive iteration number')
     print('FSC_res  - estimated resolution for the FSC cut-off')
     print('res_prec - precision of the estimate')
     print('res_prev - higher-resolution estimate bound')
     print('res_next - lower-resolution estimate bound')
     print('Ntotal   - total number of coefficients in the histogram')
     print('Nused    - number of coefficients till the bin with FSC < cut-off')
     print('Ncoef(1) - number of coefficients in the first bin')
     print('Ncoef(F) - number of cofficients in the last bin')
     print('Ncoef(N) - number of cofficients in the bin with FSC < cut-off')
     print('Nbins    - total number of bin')
     print('Fbin     - bin number with FSC < cut-off')
     print('bintype  - type of the interval with FSC < cut-off')
     print()
     print('  N FSC_res res_prec  res_prev  res_next     Ntotal    Nused',
           'Ncoef(1) Ncoef(F) Ncoef(N) Nbins Fbin type')

   while True :

      Ncycle += 1

#     calculate histogram

      hist, shrink = Calc_Hist(ss3, Fsq1, Fsq2, Fr12, ss3max, Ncoef_hist, nbin_min, precision)

#     process histogram

      ss3max, ss3min, ss3_val, dss3, Ncoef_used, ihist, Nint, reason = \
                               Proc_Hist(hist, ss3max, FSC_cut, nbin_min, precision, reason)

      Nhist  = len(hist)
      Ncoef0 = hist[0][0]
      NcoefN = hist[Nhist-1][0]
      res_min = ss3max ** float(-1.0/3.0)
      if ss3min > 0.0 :
         res_max = ss3min ** float(-1.0/3.0)
      else :
         res_max = res_max0
      res_val = ss3_val ** float(-1.0/3.0)
      res_del = (ss3max-dss3) ** float(-1.0/3.0) - res_min
      if verbose:
        print(f'{Ncycle:3} {res_val:7.3f}{res_del:9.3f} {res_min:9.3f} {res_max:9.3f}',
              f'{Ncoef_hist:10}{Ncoef_used:9}{Ncoef0:8} {Nint:8} {NcoefN:8}{Nhist:7}',
              f'{ihist+1:4}',reason)

      Ncoef_hist = Ncoef_used

      if reason == 'hist'                     : hist_middle = hist
      if reason == 'last' or reason == 'none' : hist_final  = hist

      if   reason == 'none' or reason == 'last' :
         break

   if verbose:
     print()
     if reason == 'none' :
        print('resolution estimate is beyond the dataset resolution {res_min0:8.3f}')
     else :
        print(f'resolution estimate {res_val:8.3f} in the bounds {res_min:8.3f} {res_max:8.3f}')
        if (shrink < 1.0) :
           print('warning : too few data to get a required accuracy')

   return res_val, res_del, res_min, res_max, Ncycle, hist_middle, hist_final, reason

#-------------------------------------------

def Proc_Hist(hist, ss3max, FSC_cut, nbin_min, precision, reason) :

   Nhist = len(hist)
   dss3 = ss3max / float(Nhist)

   Nint    = 0
   ss3min  = 0.0
   ss3_val = ss3max

   Ncoef_used = 0

#  check bins

   for ihist in range(0,Nhist) :

       Ncoef_used       += hist[ihist][0]
       hist_value = hist[ihist][4]
       if hist_value < FSC_cut :
          ss3min =  ihist    * dss3
          ss3max = (ihist+1) * dss3

#         calculate the mode of the found interval and inetrpolated cut-off

          if ihist == 0 :
             reason  = 'first'
             ss3_val = dss3
          else :
             if ihist * 4 < Nhist :
                reason = 'begin'
             else :
                if reason == 'none' or reason == 'first' or reason == 'begin' :
                   reason = 'hist'
                else :
                   reason = 'end'

             if ihist == Nhist-1   : reason = 'last'
             hist_prev = hist[ihist-1][4]
             ss3_val = ss3min + dss3 * (hist_prev - FSC_cut) / (hist_prev - hist_value)

          Nint = hist[ihist][0]
          break

   return ss3max, ss3min, ss3_val, dss3, Ncoef_used, ihist, Nint, reason

#-------------------------------------------

def Calc_Hist(ss3, Fsq1, Fsq2, Fr12, ss3max, Ncoef_hist, nbin_min, precision) :

   Ncoef = len(ss3)
   Nhist = int (0.33333333 / precision)
   Nin_bin = float(Ncoef_hist) / float(Nhist)
   shrink = Nin_bin / float(nbin_min)
   if shrink < 1.0 : Nhist = int(Nhist * shrink)


   hist = [[0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] for ihist in range(Nhist)]

   dss3 = ss3max / float(Nhist)

#  collect statistics

   for icoef in range(Ncoef) :
       ss3icoef = ss3[icoef]
       ihist   = int(ss3icoef / dss3)
       if ihist < Nhist :
          F1icoef  = Fsq1[icoef]
          F2icoef  = Fsq2[icoef]
          hist[ihist][0] += 1
          hist[ihist][1] += F1icoef
          hist[ihist][2] += F2icoef
          hist[ihist][4] += Fr12[icoef]
          if hist[ihist][7] < F1icoef : hist[ihist][7] = F1icoef
          if hist[ihist][8] < F2icoef : hist[ihist][8] = F2icoef

#  process statistics

   Ncoef0    = hist[0][0]
   Fsq1Mean0 = float ( hist[0][1] / Ncoef0 )
   Fsq2Mean0 = float ( hist[0][2] / Ncoef0 )

   for ihist in range(Nhist) :
       Ncoefi = float (hist[ihist][0])
       Fsq1i  = hist[ihist][1]
       Fsq2i  = hist[ihist][2]
       hist[ihist][1] = ihist     * dss3
       ss3val         = (ihist+1) * dss3
       hist[ihist][2] = ss3val
       hist[ihist][3] = ss3val ** (-1.0 / 3.0)

       if Ncoefi > 0 :
          Fsq1Meani = Fsq1i / Ncoefi
          Fsq2Meani = Fsq2i / Ncoefi
          hist[ihist][4] = hist[ihist][4] / math.sqrt(Fsq1i * Fsq2i)
          hist[ihist][5] = math.sqrt(Fsq1Meani / hist[ihist][7])
          hist[ihist][6] = math.sqrt(Fsq2Meani / hist[ihist][8])
          hist[ihist][7] = math.log10(Fsq1Meani / Fsq1Mean0) / 2.0
          hist[ihist][8] = math.log10(Fsq2Meani / Fsq2Mean0) / 2.0

   return hist, shrink

#-------------------------------------------

def hist_save(FileHist,hist_middle, hist_final) :

   FileOut  = open(FileHist, 'w')

   Nhist = len(hist_middle)
   print('Middle-position histogram',file=FileOut)
   print(' Ncoef   ss3min    ss3max   resmin     FSC    Fav/Fmax-1 Fav/Fmax-2 Fav/Fav0-1 Fav/Fav0-2',
         file=FileOut)

   for ihist in range(Nhist) :
       (Ncoef, ss3min, ss3max, resmin, FSC_val, Fmax1, Fmax2, Faver1, Faver2) = hist_middle[ihist]
       print(f'{Ncoef:6}{ss3min:10.7f}{ss3max:10.7f}{resmin:8.3f}{FSC_val:11.7f}',
             f'{Fmax1:10.7f}{Fmax2:11.7f}{Faver1:11.7f}{Faver2:11.7f}',file=FileOut)

   Nhist = len(hist_final)
   print('',file=FileOut)
   print('Final histogram',file=FileOut)
   print(' Ncoef   ss3min    ss3max   resmin     FSC    Fav/Fmax-1 Fav/Fmax-2 Fav/Fav0-1 Fav/Fav0-2',
         file=FileOut)

   for ihist in range(Nhist) :
       (Ncoef, ss3min, ss3max, resmin, FSC_val, Fmax1, Fmax2, Faver1, Faver2) = hist_final[ihist]
       print(f'{Ncoef:6}{ss3min:10.7f}{ss3max:10.7f}{resmin:8.3f}{FSC_val:11.7f}',
             f'{Fmax1:10.7f}{Fmax2:11.7f}{Faver1:11.7f}{Faver2:11.7f}',file=FileOut)

   return


 *******************************************************************************


 *******************************************************************************
cctbx/maptbx/map_symmetry.py
# -*- coding: utf-8 -*-

from __future__ import absolute_import, division, print_function
import sys, os
from libtbx import adopt_init_args
from libtbx.utils import null_out
from scitbx.matrix import col

#  map_symmetry
#  tool to identify and evaluate reconstruction symmetry in a map

class map_symmetry:

  def __init__(self,params=None,
      map_data=None,
      map_coeffs=None,
      crystal_symmetry=None,
      ncs_object=None,
      fourier_filter = None,
      log=sys.stdout):
    adopt_init_args(self, locals())
    self.cc=None
    self.score=None
    self.original_ncs_object=None
    if ncs_object:
      self.original_ncs_object=ncs_object.deep_copy()
    self.ncs_object=ncs_object

    if self.params and self.params.control.verbose:
      self.local_log=log
    else:
      self.local_log=null_out()
    if self.params.reconstruction_symmetry.find_ncs_directly and \
       not self.map_coeffs:
      from cctbx.maptbx.segment_and_split_map import get_f_phases_from_map
      self.map_coeffs,dummy=get_f_phases_from_map(map_data=self.map_data,
        crystal_symmetry=self.crystal_symmetry,
        d_min=self.params.crystal_info.resolution,
        return_as_map_coeffs=True, # required
        out=self.log)


  def get_results(self):
    from libtbx import group_args
    if not self.ncs_object:
      from mmtbx.ncs.ncs import ncs
      self.ncs_object=ncs()
      self.score=None
      self.cc=None
    return group_args(
     cc = self.cc,
     ncs_object = self.ncs_object,
     ncs_name = str(self.ncs_object.get_ncs_name()),
     ncs_operators = self.ncs_object.max_operators(),
     score=self.score,
    )

  def clean_up(self):
    pass

  def run(self):

    # Print out values of parameters
    import iotbx.phil
    from phenix.programs.map_symmetry import master_phil_str
    master_phil=iotbx.phil.parse(master_phil_str)
    print ("\nInput parameters for map_symmetry:\n",file=self.log)
    master_phil.format(python_object=self.params).show(out=self.log)

    # Shift the map origin if necessary
    self.shift_origin()
    self.get_resolution()

    print ("Finding symmetry in map",file=self.log)

    if self.params.reconstruction_symmetry.find_ncs_directly:
      new_ncs_obj,ncs_cc,ncs_score=self.find_ncs_from_density()

    else:  # usual
      from cctbx.maptbx.segment_and_split_map import run_get_ncs_from_map
      new_ncs_obj,ncs_cc,ncs_score=run_get_ncs_from_map(params=self.params,
        map_data=self.map_data,
        crystal_symmetry=self.crystal_symmetry,
        ncs_obj=self.ncs_object,
        fourier_filter = self.fourier_filter,
        out=self.log)

    if not new_ncs_obj:
      print ("\nNo symmetry found..",file=self.log)
      return

    # Now shift back if necessary
    ncs_object=new_ncs_obj.coordinate_offset(
        coordinate_offset=col(self.origin_shift_cart))

    print ("\nFinal symmetry obtained:",file=self.log)
    if ncs_object.get_ncs_name():
      print ("NCS type: %s" %(ncs_object.get_ncs_name()),file=self.log)
    print ("Correlation of symmetry-related regions: %.2f   Copies: %d " %(
       ncs_cc,ncs_object.max_operators()), file=self.log)

    if self.params.control.verbose:
      ncs_object.display_all(log=self.log)
    # write to output file
    if self.params.output_files.symmetry_out:
      ncs_object.format_all_for_group_specification(
         file_name=self.params.output_files.symmetry_out)
      print ("\nWrote operators in .ncs_spec format to %s" %(
        self.params.output_files.symmetry_out),file=self.log)

    # Final results
    self.ncs_object=ncs_object
    self.cc=ncs_cc
    self.score=ncs_score

  def find_ncs_from_density(self):

    # First test to make sure the function is available
    try:
      from phenix.command_line.find_ncs_from_density import \
       find_ncs_from_density as find_ncs
    except Exception as e:
      ncs_cc=None
      ncs_object=None
      ncs_score=None
      return ncs_object,ncs_cc,ncs_score

    print ("Finding symmetry in map by search for matching density",
       file=self.log)
    # Write out mtz file to search in...
    temp_dir=self.params.output_files.temp_dir
    if not os.path.isdir(temp_dir):
      os.mkdir(temp_dir)
    map_coeffs_file=os.path.join(temp_dir,"map_coeffs.mtz")
    self.map_coeffs.as_mtz_dataset(
       column_root_label='FWT').mtz_object().write(file_name=map_coeffs_file)


    args=["%s" %(map_coeffs_file),"map_operators_inside_unit_cell=True"]
    if self.params.crystal_info.resolution:
      args.append("resolution=%s" %(self.params.crystal_info.resolution))
    find_ncs_from_density=find_ncs( args,out=self.log)
    if hasattr(find_ncs_from_density,'ncs_object') and \
        find_ncs_from_density.ncs_object:
      ncs_object=find_ncs_from_density.ncs_object
      ncs_cc=ncs_object.overall_cc()
      ncs_score=ncs_cc*(ncs_object.max_operators())**0.5
    else:
      ncs_cc=None
      ncs_object=None
      ncs_score=None

    return ncs_object,ncs_cc,ncs_score
  def get_resolution(self):
    if not self.params.crystal_info.resolution:
      from cctbx.maptbx import d_min_from_map
      self.params.crystal_info.resolution=\
        d_min_from_map(map_data=self.map_data,
         unit_cell=self.crystal_symmetry.unit_cell())
      print ("\nResolution estimated from map is %.1f A " %(
        self.params.crystal_info.resolution),file=self.log)

  def shift_origin(self):

     origin_shift_grid_units=self.map_data.origin()
     origin_shift=(
         self.map_data.origin()[0]/self.map_data.all()[0],
         self.map_data.origin()[1]/self.map_data.all()[1],
         self.map_data.origin()[2]/self.map_data.all()[2])
     origin_shift_cart=\
       self.crystal_symmetry.unit_cell().orthogonalize(origin_shift)

     acc=self.map_data.accessor()
     shift_needed = not \
        (self.map_data.focus_size_1d() > 0 and self.map_data.nd() == 3 and
         self.map_data.is_0_based())
     if(shift_needed):
       self.map_data = self.map_data.shift_origin()
       self.origin_frac=origin_shift
       self.origin_shift_cart=origin_shift_cart
       # Adjust likely center position
       new_location=[]
       for xx,osc in zip(self.params.reconstruction_symmetry.symmetry_center,
         origin_shift_cart):
         new_location.append(xx-osc)
       self.params.reconstruction_symmetry.symmetry_center=tuple(new_location)
       print("Shifted guess for symmetry center is at: (%.2f,%.2f,%.2f) A "%(
        self.params.reconstruction_symmetry.symmetry_center),file=self.log)
     else:
       self.origin_frac=(0.,0.,0.)
       self.origin_shift_cart=(0,0,0)

     if self.ncs_object:
        self.ncs_object=self.ncs_object.coordinate_offset(
        coordinate_offset=-1*col(self.origin_shift_cart))


 *******************************************************************************


 *******************************************************************************
cctbx/maptbx/mask.py
from __future__ import absolute_import, division, print_function
from libtbx.utils import null_out
from scitbx.array_family import flex
'''
Mask utilities
'''

class create_mask_around_atoms(object):

  '''
    Class to create a map_manager object containing a mask around atoms,
    smooth or adjust density outside, and apply to a supplied map_manager

  '''

  def __init__(self,
      model = None,
      xray_structure = None,
      mask_atoms_atom_radius = None,
      invert_mask = None,
      n_real = None,
      map_manager = None,
      wrapping = None):

    '''
     Create a mask (map object) with values of 1 near atoms in xray_structure

     Parameters are:
       model:   required source of information about where atoms are
       xray_structure:   alternate source of information about where atoms are
       mask_atoms_atom_radius:  radius around atoms to mask
       invert_mask:  outside is 1 and inside is 0
       n_real:  dimensions of map to create, e.g., existing map_data.all()
       map_manager: alternate source of information for n_real. origin (0, 0, 0)
         and source of wrapping
       wrapping:  Whether map wraps around unit cell boundaries, where unit
         cell is that defined by xray_structure or model
         Wrapping also defines whether sites are expanded to space group P1
         before creating the mask.  If wrapping is true and space group is not
         p1 then sites are expanded.
    '''

    assert (model is not None) or (xray_structure is not None)
    assert mask_atoms_atom_radius is not None
    assert (n_real is not None) or (map_manager is not None)
    assert (map_manager is not None) or isinstance(wrapping, bool)

    if not n_real:
      assert map_manager.map_data().origin() == (0, 0, 0)
      n_real = map_manager.map_data().all()


    if model:
      xray_structure = model.get_xray_structure()
      self._crystal_symmetry = model.crystal_symmetry()
    else:
      self._crystal_symmetry = xray_structure.crystal_symmetry()

    if map_manager and wrapping is None:
      wrapping = map_manager.wrapping()

    assert wrapping is not None

    if (not wrapping) or (self._crystal_symmetry.space_group_number==1):
      # usual
      sites_frac = xray_structure.sites_frac()
    else:  # wrapping and cs: need to expand
      sites_frac = xray_structure.expand_to_p1().sites_frac()

    import boost_adaptbx.boost.python as bp
    cctbx_maptbx_ext = bp.import_ext("cctbx_maptbx_ext")
    radii = flex.double(
      sites_frac.size(), mask_atoms_atom_radius)
    if invert_mask:
      mask_value_inside_molecule = 0
      mask_value_outside_molecule = 1
    else: # usual
      mask_value_inside_molecule = 1
      mask_value_outside_molecule = 0
    self._mask = cctbx_maptbx_ext.mask(
      sites_frac                  = sites_frac,
      unit_cell                   = xray_structure.unit_cell(),
      n_real                      = n_real,
      mask_value_inside_molecule  = mask_value_inside_molecule,
      mask_value_outside_molecule = mask_value_outside_molecule,
      radii                       = radii,
      wrapping                    = wrapping)
    # Set up map_manager with this mask
    if map_manager:
      self._map_manager = map_manager.customized_copy(map_data = self._mask)
    else:
      from iotbx.map_manager import map_manager as map_man
      self._map_manager = map_man(map_data = self._mask,
        unit_cell_crystal_symmetry = self.crystal_symmetry(),
        unit_cell_grid = self._mask.all(),
        wrapping = wrapping)

    self._map_manager.set_is_mask(True)

    # Initialize soft mask
    self._is_soft_mask = False
    self._is_soft_mask_around_edges = False

  def mask(self):
    return self._mask

  def map_manager(self):
    return self._map_manager

  def is_soft_mask_around_edges(self):
    return self._is_soft_mask_around_edges

  def is_soft_mask(self):
    return self._is_soft_mask

  def solvent_content(self):
    if hasattr(self, '_solvent_content'):
      return self._solvent_content

  def crystal_symmetry(self):
    return self._crystal_symmetry

  def soft_mask(self, soft_mask_radius = None):
    '''
    Make the mask a soft mask
    Parameter:
      soft_mask_radius:   defines distance over which mask is smoothed
    This has to be done in P1 as the mask might not follow map symmetry
      (for example a mask around edges does not)
    '''
    assert soft_mask_radius is not None
    assert not self.is_soft_mask()  # do not do it twice

    from cctbx.maptbx.segment_and_split_map import smooth_mask_data

    if self._crystal_symmetry.space_group_number() != 1:
      from cctbx import crystal
      crystal_symmetry = crystal.symmetry(
          self._crystal_symmetry.unit_cell().parameters(), 1)
    else:
      crystal_symmetry = self._crystal_symmetry

    self._mask = smooth_mask_data(mask_data = self._mask,
      crystal_symmetry = crystal_symmetry,
       rad_smooth = soft_mask_radius)

    # Note that mask still might not follow map symmetry if it did not before

    self._map_manager = self._map_manager.customized_copy(map_data = self._mask)
    self._map_manager.set_is_mask(True)
    self._is_soft_mask_around_edges = True

  def apply_mask_to_other_map_manager(self, other_map_manager = None,
     set_outside_to_mean_inside = False,
     set_mean_to_zero = False):

    '''
    Apply this mask to a map_manager object containing map_data.
    Creates new map_manager

    Parameters:
       other_map_manager: map_manager to be masked in place
       set_outside_to_mean_inside:  if True,
          set the value outside mask to the mean inside the mask
       set_mean_to_zero:  Adjust overall mean of map to zero after masking
    '''

    assert other_map_manager.is_similar(self.map_manager())
    assert other_map_manager.map_data().origin() == (0, 0, 0)

    from cctbx.maptbx.segment_and_split_map import apply_mask_to_map
    new_map_data = apply_mask_to_map(mask_data = self._mask,
      smoothed_mask_data = self._mask,
      set_outside_to_mean_inside = set_outside_to_mean_inside,
      set_mean_to_zero = set_mean_to_zero,
      map_data = other_map_manager.map_data(),
      out = null_out())

    return self.map_manager().customized_copy(map_data = new_map_data)

class create_mask_with_map_data(create_mask_around_atoms):

  '''
    Class to create a map_manager object containing a supplied mask

  '''

  def __init__(self, map_data,
      map_manager = None):

    '''
     Create a mask (map object) with values supplied in map_data
    '''
    assert isinstance(map_data, flex.double)
    assert map_manager.map_data().all() == map_data.all()
    assert map_data.origin() == (0,0,0)
    assert map_manager.origin_is_zero()

    self._crystal_symmetry = map_manager.crystal_symmetry()

    self._mask = map_data

    # Set up map_manager with this mask
    self._map_manager = map_manager.customized_copy(map_data = self._mask)
    self._map_manager.set_is_mask(True)

    # Initialize soft mask
    self._is_soft_mask = False
    self._is_soft_mask_around_edges = False

class create_mask_around_edges(create_mask_around_atoms):

  '''
    Class to create a map_manager object containing a mask a few pixels in
    around the boundaries of the map

  '''

  def __init__(self,
      boundary_radius = None,
      map_manager = None):

    '''
     Create a mask (map object) with values of 1 except at the map boundaries

     This will not be a soft mask. To make it a soft mask, follow this with
       soft_mask(soft_mask_radius = soft_mask_radius)

     Parameters are:
       boundary_radius:  radius for masking
       map_manager: source of information for n_real and crystal_symmetryi
          . origin (0, 0, 0)
    '''

    assert boundary_radius is not None
    assert (map_manager is not None)

    n_real = map_manager.map_data().all()

    self._crystal_symmetry = map_manager.crystal_symmetry()

    from cctbx.maptbx.segment_and_split_map import get_zero_boundary_map

    self._mask = get_zero_boundary_map(
      map_data = map_manager.map_data(),
      crystal_symmetry = self._crystal_symmetry,
      radius = boundary_radius)
    # Set up map_manager with this mask
    self._map_manager = map_manager.customized_copy(map_data = self._mask)
    self._map_manager.set_is_mask(True)

    # Initialize soft mask
    self._is_soft_mask = False
    self._is_soft_mask_around_edges = False

class create_mask_around_density(create_mask_around_atoms):

  '''
    Class to create a map_manager object containing a mask around density

  '''

  def __init__(self,
      map_manager = None,
      resolution = None,
      molecular_mass = None,
      sequence = None,
      solvent_content = None):

    '''
     Create a mask (map object) with values of 1 near molecule

     Parameters are:
       map_manager: source of information about density
       resolution : optional resolution of map
       molecular_mass: optional mass (Da) of object in density
       sequence: optional sequence of object in density
       solvent_content : optional solvent_content of map
    '''

    assert (map_manager is not None)

    if not resolution:
      from cctbx.maptbx import d_min_from_map
      resolution = d_min_from_map(
           map_data=map_manager.map_data(),
           unit_cell=map_manager.crystal_symmetry().unit_cell())

    self._crystal_symmetry = map_manager.crystal_symmetry()

    if (molecular_mass or sequence ) and (
          not solvent_content):
      # Try to get a good starting value of solvent_content

      from cctbx.maptbx.segment_and_split_map import get_solvent_fraction
      solvent_content = get_solvent_fraction(
           params = None,
           molecular_mass = molecular_mass,
           sequence = sequence,
           do_not_adjust_dalton_scale = True,
           crystal_symmetry = self._crystal_symmetry,
           out = null_out())

    # Now use automatic procedure to get a mask
    from cctbx.maptbx.segment_and_split_map import \
          get_iterated_solvent_fraction
    self._mask, self._solvent_content = get_iterated_solvent_fraction(
          crystal_symmetry = self._crystal_symmetry,
          fraction_of_max_mask_threshold = 0.05, #
          solvent_content = solvent_content,
          cell_cutoff_for_solvent_from_mask = 1, # Use low-res method always
          use_solvent_content_for_threshold = True,
          mask_resolution = resolution,
          return_mask_and_solvent_fraction = True,
          map = map_manager.map_data(),
          verbose = False,
          out = null_out())

    if self._solvent_content is None:
      from libtbx.utils import Sorry
      raise Sorry("Unable to get solvent content in auto-masking")


    # Set up map_manager with this mask
    self._map_manager = map_manager.customized_copy(map_data = self._mask)
    self._map_manager.set_is_mask(True)

    # Initialize soft mask
    self._is_soft_mask = False
    self._is_soft_mask_around_edges = False

class expand_mask(create_mask_around_atoms):

  '''
    Class to create a map_manager object containing a mask expanded from
     previous mask

  '''

  def __init__(self,
      map_manager = None,
      resolution = None,
      buffer_radius = 5,
      minimum_fraction_of_max = 0.01):


    '''
     Create a mask (map object) with values of 1 expanded by buffer_radius

     Parameters are:
       map_manager: source of previous mask.
       resolution : optional resolution of map
       buffer_radius:  radius to expand in A
       minimum_fraction_of_max: smallest-sized region to expand
         as ratio to biggest
    '''

    assert (map_manager is not None)

    if not resolution:
      from cctbx.maptbx import d_min_from_map
      resolution = d_min_from_map(
           map_data=map_manager.map_data(),
           unit_cell=map_manager.crystal_symmetry().unit_cell())

    self._crystal_symmetry = map_manager.crystal_symmetry()

    from cctbx.maptbx.segment_and_split_map import estimate_expand_size, get_co

    expand_size = estimate_expand_size(
         crystal_symmetry = map_manager.crystal_symmetry(),
         map_data = map_manager.map_data(),
         expand_target = buffer_radius,
         minimum_expand_size = 0,
         out = null_out())

    if expand_size < 1:
      return # do nothing
    co, sorted_by_volume, min_b, max_b = get_co(
       map_data = map_manager.map_data(),
       threshold = 0.5, wrapping = False)
    if len(sorted_by_volume)<2:
      return # do nothing

    minimum_size = sorted_by_volume[1][0] * minimum_fraction_of_max
    s = None
    for v1, i1 in sorted_by_volume[1:]:
      if v1 < minimum_size: break
      bool_region_mask = co.expand_mask(
        id_to_expand = i1, expand_size = expand_size)
      if s is None:
        s = (bool_region_mask == True)
      else:
        s |=  (bool_region_mask == True)
    self._mask = map_manager.deep_copy().map_data()
    self._mask.set_selected(s, 1)
    self._mask.set_selected(~s, 0)

    self._solvent_content = s.count(False)/s.size()


    # Set up map_manager with this mask
    self._map_manager = map_manager.customized_copy(map_data = self._mask)
    self._map_manager.set_is_mask(True)

    # Initialize soft mask
    self._is_soft_mask = False
    self._is_soft_mask_around_edges = False


 *******************************************************************************


 *******************************************************************************
cctbx/maptbx/mem.py
from __future__ import absolute_import, division, print_function
import math
from scitbx.array_family import flex
from cctbx import miller
from cctbx import maptbx
from libtbx.utils import Sorry
import sys
from six.moves import range

def Hn(m):
  m_ = m
  sc = math.log(m_.size())
  s = m_>0
  m_ = m_.select(s.iselection())
  m_ = m_/flex.sum(m_)
  return -flex.sum(m_*flex.log(m_))/sc

def Hw(m):
  s = m>0
  m_ = m
  m_ = m_.select(s.iselection())
  return -flex.sum(m_*flex.log(m_))

def Q_X(x,y):
  x = abs(x).data()
  y = abs(y).data()
  delta = flex.abs(x-y)
  return flex.sum(delta*delta)

def scale(x, y):
  assert type(x) == type(y)
  if(type(x) == miller.array):
    x = x.data()
    y = y.data()
  x = flex.abs(x)
  y = flex.abs(y)
  d = flex.sum(y*y)
  if d == 0: return 1
  else:
    return flex.sum(x*y)/d

def r_factor(x,y, use_scale):
  try:
    x = flex.abs(x.data())
    y = flex.abs(y.data())
  except Exception: pass
  sc=1
  if(use_scale): sc = scale(x,y)
  return flex.sum(flex.abs(x-sc*y))/flex.sum(x)

def show_map_stat(m, prefix, out=sys.stdout):
  print("%s (min/max/mean/sum, Hw, Hn): %9.6f %9.6f %9.6f %9.6f %9.6f %9.6f"%(
    prefix, flex.min(m), flex.max(m), flex.mean(m), flex.sum(m), Hw(m), Hn(m)), file=out)

class run(object):
  def __init__(self,
                f,
                f_000             = None,
                lam               = None,
                start_map         = "lde",
                resolution_factor = 0.25,
                mean_density      = 0.375,
                max_iterations    = 2000,
                beta              = 0.9,
                use_modification  = True,
                xray_structure    = None,
                verbose           = False,
                lambda_increment_factor = None,
                convergence_at_r_factor = 0,
                convergence_r_threshold = 0.1,
                detect_convergence = True,
                crystal_gridding  = None,
                use_scale         = True,
                log               = None):
    if (log is None) : log = sys.stdout
    self.log              = log
    self.start_map        = start_map
    assert start_map in ["flat", "lde", "min_shifted"]
    self.lam              = lam
    self.max_iterations   = max_iterations
    self.f_000            = f_000
    self.verbose          = verbose
    self.beta             = beta
    self.f                = f
    self.use_modification = use_modification
    self.meio_obj         = None
    self.xray_structure   = xray_structure
    self.lambda_increment_factor = lambda_increment_factor
    self.convergence_at_r_factor = convergence_at_r_factor
    self.detect_convergence      = detect_convergence
    self.crystal_gridding        = crystal_gridding
    self.use_scale               = use_scale
    self.convergence_r_threshold = convergence_r_threshold
    #
    if(self.f.anomalous_flag()):
       merged = self.f.as_non_anomalous_array().merge_equivalents()
       self.f = merged.array().set_observation_type( self.f )
    # current monitor and optimized functional values
    self.cntr         = None
    self.r            = None
    self.h_n          = None
    self.h_w          = None
    self.Z            = None
    self.scale_kc     = None
    self.a_gd         = None
    self.q_x          = None
    self.q_tot        = None
    self.tp           = None
    self.f_mem        = None
    self.header_shown = False
    self.cc           = None
    self.r_factors    = flex.double()
    self.cc_to_answer = flex.double()
    #
    if(self.crystal_gridding is None):
      self.crystal_gridding = self.f.crystal_gridding(
        d_min                   = self.f.d_min(),
        resolution_factor       = resolution_factor,
        grid_step               = None,
        symmetry_flags          = None,
        mandatory_factors       = None,
        max_prime               = 5,
        assert_shannon_sampling = True)
    self.n_real = self.crystal_gridding.n_real()
    max_index = [int((i-1)/2.) for i in self.n_real]
    self.N = self.n_real[0]*self.n_real[1]*self.n_real[2]
    self.full_set = self.f.complete_set(max_index=max_index)
    self.f_calc = None
    if(self.xray_structure is not None):
      self.f_calc = self.full_set.structure_factors_from_scatterers(
        xray_structure = self.xray_structure).f_calc()
    if(verbose):
      print("Resolution factor: %-6.4f"%resolution_factor, file=self.log)
      print("  N, n1,n2,n3:",self.N,self.n_real[0],self.n_real[1],self.n_real[2], file=self.log)
      print("Box: ", file=self.log)
      print("  resolution: %6.4f"%self.full_set.d_min(), file=self.log)
      print("  max. index |h|,|k|,|l|<nreal/2:", max_index, file=self.log)
      print("  n.refl.:", self.full_set.indices().size(), file=self.log)
    # STEP 1
    if(self.f_000 is None): self.f_000=mean_density*self.f.unit_cell().volume()
    Cobs = 1
    Ca = 0.37
    self.Agd = Ca/self.N
    if(verbose):
      print("Cobs, Ca, Agd, f_000:", Cobs, Ca, self.Agd, "%6.3f"%self.f_000, file=self.log)
      print("Cobs/(N*f_000): ",Cobs/(self.N*self.f_000), file=self.log)
      print("memory factor (beta):", self.beta, file=self.log)
    self.f = self.f.customized_copy(data=self.f.data()*Cobs/(self.N*self.f_000))
    fft_map = miller.fft_map(
      crystal_gridding     = self.crystal_gridding,
      fourier_coefficients = self.f)
    self.rho_obs = fft_map.real_map_unpadded()
    if(verbose):
      show_map_stat(m = self.rho_obs, prefix = "rho_obs (formula #13)",
        out=self.log)
    # STEP 2
    self.rho = self.normalize_start_map()
    if(verbose):
      show_map_stat(m = self.rho, prefix = "rho_0 (initial approximation)",
        out=self.log)
    # STEP 3
    self.iterations()

  def normalize_start_map(self):
    rho = self.rho_obs.deep_copy()
    if(self.start_map == "flat"):
      rho = flex.double(flex.grid(self.n_real), 1./self.N)
    elif(self.start_map == "lde"):
      eps = flex.max(rho)/100.
      selection_nonpositive = rho <= eps
      rho = rho.set_selected(selection_nonpositive, eps)
    elif(self.start_map == "min_shifted"):
      rho = rho - flex.min(rho)
    else: raise Sorry("Invalid initial map modification choice.")
    return rho / flex.sum(rho)

  def map_coefficients(self, d_min=None):
    o=maptbx.non_linear_map_modification_to_match_average_cumulative_histogram(
      map_1 = self.rho, map_2 = self.rho_obs)
    # XXX p1 must be equal to p2 very accurately; sometimes not the case
    # currently
    for x in [0.5,1,1.5]:
      p1 = (o.map_1()>x).count(True)*1./o.map_1().size()
      p2 = (o.map_2()>x).count(True)*1./o.map_2().size()
      print("Cumulative histograms match for two maps: %9.6f %9.6f" %(
        p1,p2), file=self.log)
    f1 = self.full_set.structure_factors_from_map(
      map            = o.map_1(),
      use_scale      = True,
      anomalous_flag = False,
      use_sg         = False)
    f2 = self.f.structure_factors_from_map(
      map            = o.map_2(),
      use_scale      = True,
      anomalous_flag = False,
      use_sg         = False)
    if(d_min is not None):
      f1 = f1.resolution_filter(d_min=d_min)
      f2 = f2.resolution_filter(d_min=d_min)
    return f1, f2

  def write_mtz_file(self, file_name = "me.mtz", column_root_label = "MEM",
      d_min=None):
    f1, f2 = self.map_coefficients(d_min=d_min)
    mtz_dataset = f1.as_mtz_dataset(column_root_label=column_root_label)
    mtz_dataset.add_miller_array(miller_array=f2, column_root_label="ORIG")
    mtz_object = mtz_dataset.mtz_object()
    mtz_object.write(file_name = file_name)

  def update_metrics(self):
    self.r        = r_factor(self.f, self.f_mem, use_scale=False)
    self.h_n      = self.meio_obj.hn()
    self.h_w      = self.meio_obj.hw()
    self.scale_kc = scale(self.f, self.f_mem)
    self.q_x      = Q_X(self.f_mem, self.f)
    self.q_tot    = self.h_w-self.lam/2*self.q_x*self.N
    self.tp       = self.meio_obj.tp()

  def show(self, verbose=True):
    if(self.verbose and not self.header_shown):
      print("lam:", self.lam, file=self.log)
      print("  nit    TP       Hw        Q_X              Qtot         Agd    scFc       Hn     R    * tpgd       lam        cc", file=self.log)
      self.header_shown = True
    self.update_metrics()
    fs = " ".join(["%5d"%self.cntr, "%8.6f"%self.tp, "%9.6f"%self.h_w,\
         "%12.8e"%self.q_x, "%11.6f"%self.q_tot, "%10.6f"%self.Agd,\
         "%7.4f"%self.scale_kc, "%9.6f"%self.h_n, "%6.4f"%self.r,\
         "*", "%8.6f"%self.Z, "%12.6f"%self.lam])
    if(self.cc is not None): cc = "%7.5f"%self.cc
    else: cc = self.cc
    print(fs, cc, file=self.log)
    return fs

  def is_converged(self, rho_trial):
    result = False
    r = r_factor(self.f, self.f_mem, use_scale=False)
    if(r < self.convergence_r_threshold):
      if(self.xray_structure is None):
        self.r_factors.append(r)
        size = self.r_factors.size()
        if(size>=3):
          tmp = flex.mean(self.r_factors[size-3:])
          if(tmp <= r or r <= self.convergence_at_r_factor): result = True
      else:
        f_mem = self.full_set.structure_factors_from_map(
          map            = rho_trial,
          use_scale      = False,
          anomalous_flag = False,
          use_sg         = False)
        self.cc = f_mem.map_correlation(other = self.f_calc)
        self.cc_to_answer.append(self.cc)
        def max_change_so_far(x):
          result = flex.double()
          if(self.cc_to_answer.size()):
            for i in range(self.cc_to_answer.size()):
              if(i>0):
                result.append(self.cc_to_answer[i]-self.cc_to_answer[i-1])
          return flex.max(result)
        size = self.cc_to_answer.size()
        if(size>=3):
          mcsf = max_change_so_far(x = self.cc_to_answer)
          tmp = flex.mean(self.cc_to_answer[size-3:])
          if(tmp >= self.cc-1.e-6 or mcsf/5 >= self.cc-tmp):
            result = True
    else: self.cc=None
    return result

  def iterations(self):
    self.cntr = 0
    while self.cntr < self.max_iterations:
      self.f_mem = self.f.structure_factors_from_map(
        map            = self.rho,
        use_scale      = False,
        anomalous_flag = False,
        use_sg         = False)
      self.f_mem = self.f_mem.customized_copy(data = self.f_mem.data()/self.N)
      fft_map = miller.fft_map(
        crystal_gridding     = self.crystal_gridding,
        fourier_coefficients = self.f_mem)
      rho_mod = fft_map.real_map_unpadded()
      rho_trial = self.rho.deep_copy()
      self.meio_obj = maptbx.mem_iteration(
        rho_mod,
        self.rho_obs,
        rho_trial,
        self.lam*self.N,
        self.n_real,
        self.Agd,
        self.beta,
        self.use_scale)
      if(self.detect_convergence and self.is_converged(rho_trial=rho_trial)):
        break
      else: self.rho = rho_trial
      self.Z = self.meio_obj.z()
      if(self.verbose): self.show()
      if(self.cntr%25==0): self.Agd = self.Agd/self.Z
      self.cntr += 1
      if(self.lambda_increment_factor is not None):
        self.lam *= self.lambda_increment_factor
    self.update_metrics()


 *******************************************************************************


 *******************************************************************************
cctbx/maptbx/minimization.py
from __future__ import absolute_import, division, print_function
from cctbx.array_family import flex
import scitbx.lbfgs
from cctbx import miller
from cctbx import maptbx
from libtbx import adopt_init_args
from cctbx.maptbx import real_space_refinement_simple # special import

def show(histogram):
  h_1 = histogram
  lc_1 = histogram.data_min()
  s_1 = enumerate(histogram.slots())
  for (i_1,n_1) in s_1:
    hc_1 = h_1.data_min() + h_1.slot_width() * (i_1+1)
    print("%8.3f - %8.3f: %5d" % (lc_1,hc_1,n_1))
    lc_1 = hc_1

class target_and_gradients(object):

  def __init__(self,
               unit_cell,
               map_target,
               real_space_gradients_delta,
               sites_frac,
               map_current = None,
               geometry_restraints_manager = None,
               real_space_target_weight = None,
               restraints_target_weight = None,
               sites_cart = None,
               target_type = "diffmap"):
    adopt_init_args(self, locals())
    assert target_type in ["simple", "diffmap"]
    if(geometry_restraints_manager is not None):
      assert [real_space_target_weight, restraints_target_weight, sites_cart
             ].count(None)==0
      self.geom_tg_obj = geometry_restraints_manager.energies_sites(
        sites_cart = sites_cart, compute_gradients = True)
    if(target_type == "diffmap"):
      assert map_current is not None
      self.rsr_tg_obj = maptbx.target_and_gradients_diffmap(
        unit_cell   = unit_cell,
        map_target  = map_target,
        map_current = map_current,
        step        = real_space_gradients_delta,
        sites_frac  = sites_frac)
    if(target_type == "simple"):
      assert sites_cart is not None
      self.rsr_tg_obj = maptbx.target_and_gradients_simple(
        unit_cell   = unit_cell,
        map_target  = map_target,
        sites_cart  = sites_cart,
        delta       = real_space_gradients_delta,
        selection   = flex.bool(sites_cart.size(),True))

  def target(self):
    rs_f = self.rsr_tg_obj.target()
    if(self.target_type=="simple"): rs_f *= -1.
    if(self.geometry_restraints_manager):
      result = self.real_space_target_weight * rs_f + \
               self.restraints_target_weight * self.geom_tg_obj.target
    else:
      result = rs_f
    return result

  def gradients(self):
    rs_g = self.rsr_tg_obj.gradients()
    if(self.target_type=="simple"): rs_g *= -1.
    if(self.geometry_restraints_manager):
      g = self.real_space_target_weight * rs_g + \
          self.restraints_target_weight * self.geom_tg_obj.gradients
    else:
      g = rs_g
    return g.as_double()

  def weight(self):
    gx = self.rsr_tg_obj.gradients()
    gc = self.geom_tg_obj.gradients
    tmp = flex.sqrt(gc.dot())
    sel = tmp < 3*flex.mean(tmp)
    gc = gc.select(sel)
    result = gx.norm()/gc.norm()
    return result


class run(object):
  def __init__(self,
        xray_structure,
        miller_array,
        crystal_gridding,
        map_target,
        step,
        target_type,
        max_iterations=30,
        min_iterations=25,
        geometry_restraints_manager=None,
        real_space_target_weight=1,
        restraints_target_weight=1):
    assert target_type in ["diffmap", "simple"]
    self.step = step
    self.geometry_restraints_manager = geometry_restraints_manager
    self.real_space_target_weight = real_space_target_weight
    self.sites_cart = xray_structure.sites_cart()
    self.x = self.sites_cart.as_double()
    self.xray_structure = xray_structure
    self.map_target = map_target
    self.miller_array = miller_array
    self.crystal_gridding = crystal_gridding
    self.restraints_target_weight = restraints_target_weight
    self.target_type = target_type
    self.minimizer = scitbx.lbfgs.run(
      target_evaluator=self,
      termination_params=scitbx.lbfgs.termination_parameters(
        max_iterations=max_iterations,
        min_iterations=min_iterations),
      exception_handling_params=scitbx.lbfgs.exception_handling_parameters(
        ignore_line_search_failed_rounding_errors=True,
        ignore_line_search_failed_step_at_lower_bound=True,
        ignore_line_search_failed_maxfev=True))
    self.sites_cart.clear()
    self.sites_cart.extend(flex.vec3_double(self.x))
    self.xray_structure = self.xray_structure.replace_sites_cart(
      self.sites_cart)

  def compute_functional_and_gradients(self):
    self.sites_cart = flex.vec3_double(self.x)
    self.xray_structure = self.xray_structure.replace_sites_cart(self.sites_cart)
    self.map_current = None
    if(self.target_type == "diffmap"):
      self.map_current = self.compute_map()
    tg_obj = target_and_gradients(
      unit_cell                   = self.xray_structure.unit_cell(),
      map_target                  = self.map_target,
      map_current                 = self.map_current,
      real_space_gradients_delta  = self.step,
      sites_frac                  = self.xray_structure.sites_frac(),
      geometry_restraints_manager = self.geometry_restraints_manager,
      real_space_target_weight    = self.real_space_target_weight,
      restraints_target_weight    = self.restraints_target_weight,
      sites_cart                  = self.sites_cart,
      target_type                 = self.target_type)
    return tg_obj.target(), tg_obj.gradients()

  def compute_map(self):
    map_coefficients = self.miller_array.structure_factors_from_scatterers(
      xray_structure = self.xray_structure).f_calc()
    fft_map = miller.fft_map(crystal_gridding     = self.crystal_gridding,
                             fourier_coefficients = map_coefficients)
    fft_map.apply_sigma_scaling()
    result = fft_map.real_map_unpadded()
    return result


 *******************************************************************************


 *******************************************************************************
cctbx/maptbx/prepare_map_for_docking.py
from __future__ import print_function
from __future__ import division

import math
from scitbx.array_family import flex
from scitbx.dtmin.minimizer import Minimizer
from scitbx.dtmin.refinebase import RefineBase
from scitbx.dtmin.reparams import Reparams
from scitbx.dtmin.bounds import Bounds
from cctbx import adptbx
from libtbx import group_args
from libtbx.utils import Sorry
from iotbx.map_model_manager import map_model_manager
from iotbx.data_manager import DataManager
import sys

class RefineCryoemSignal(RefineBase):
  # Set up refinement class for dtmin minimiser (based on Phaser minimiser)
  def __init__(self, sumfsqr_lm, f1f2cos_lm, deltafsqr_lm,
               r_star, ssqr_bins, target_spectrum, start_x):
    RefineBase.__init__(self)
    # Prepare data that will be used repeatedly for fgh evaluation
    # Sample local mean with spacing derived from averaging radius
    # Assume box is at least close to being a cube
    self.unit_cell = sumfsqr_lm.unit_cell()
    recip_params = self.unit_cell.reciprocal_parameters()
    (astar, bstar, cstar) = recip_params[:3]
    spacing = int(round(r_star/(2*max(astar,bstar,cstar))))
    self.subsample = spacing**3
    h,k,l = sumfsqr_lm.indices().as_vec3_double().parts()
    ih = h.iround()
    ik = k.iround()
    il = l.iround()
    # Modulus of negative element in flex array is negative or zero, but we need
    # it to be positive. Subtract 1 so that lowest resolution reflection is 1,1,1
    ihmod = (ih % spacing + spacing - 1) % spacing
    ikmod = (ik % spacing + spacing - 1) % spacing
    ilmod = (il % spacing + spacing - 1) % spacing
    modsum = ihmod + ikmod + ilmod
    sel = (modsum == 0)
    self.sumfsqr_miller = sumfsqr_lm.select(sel)
    self.sumfsqr_miller.use_binning_of(sumfsqr_lm) # Matches subset
    self.f1f2cos_miller = f1f2cos_lm.select(sel)
    self.f1f2cos_miller.use_binner_of(self.sumfsqr_miller)
    self.sigmaE_miller = (deltafsqr_lm.select(sel)) / 2. # Half of deltafsqr power
    self.sigmaE_miller.use_binner_of(self.sumfsqr_miller)

    self.n_bins = self.sumfsqr_miller.binner().n_bins_used()  # Assume consistent binning
    self.ssqr_bins = ssqr_bins
    assert (self.n_bins == len(ssqr_bins))
    self.target_spectrum = target_spectrum
    self.start_x = start_x
    self.x = start_x[:]         # Full set of parameters
    d_min = math.sqrt(1./flex.max(sumfsqr_lm.d_star_sq().data()))
    cell_tensor = flex.double((astar*astar, bstar*bstar, cstar*cstar, astar*bstar, astar*cstar, bstar*cstar))

    # Maximum beta should usually give an exponential argument < 50 at resolution limit
    self.max_beta = cell_tensor * 45 * d_min**2
    # But make sure that maximum is higher than initial estimate
    # This can be an issue for subvolumes with unusually low signal
    asqr_beta = flex.double(self.x[self.n_bins + 1:self.n_bins + 7])
    if (self.max_beta[0] < 3 * asqr_beta[0]):
      rescale = 3 * asqr_beta[0]/self.max_beta[0]
      self.max_beta = rescale * self.max_beta
    self.max_beta = list(self.max_beta)

    # Choose maximum shift in beta to change by less than factor of 2 at resolution limit
    self.large_shifts_beta = list(cell_tensor*math.log(2.)*d_min**2)

    # Apply a relatively weak restraint to sphericity of beta
    self.sigmaSphericityBeta = cell_tensor * 2*d_min**2
    # sigmaSphericityBiso = adptbx.u_as_b(adptbx.beta_as_u_cart(self.unit_cell, tuple(self.sigmaSphericityBeta)))
    # print("sigmaSphericityBiso: ",tuple(sigmaSphericityBiso))


  def sphericity_restraint(self, anisoBeta, do_gradient=True, do_hessian=True,
      sigma_factor = 1.):
    sigmaSphericityBeta = self.sigmaSphericityBeta * sigma_factor
    f = 0.
    g = flex.double(6, 0)
    h = flex.double(6 * 6, 0)
    h.reshape(flex.grid(6, 6))
    unit_cell = self.unit_cell
    aniso_u_iso = adptbx.beta_as_u_iso(unit_cell, anisoBeta)
    aniso_delta_beta = flex.double(adptbx.u_iso_as_beta(unit_cell, aniso_u_iso))
    anisoRemoveIso = flex.double(anisoBeta) - aniso_delta_beta
    dBetaIso_by_dBIso = list(adptbx.u_iso_as_beta(unit_cell, adptbx.b_as_u(1.)))
    mm = unit_cell.metrical_matrix()
    dBIso_by_dBetaAno = list(4./3.*flex.double(mm))

    for ni in range(6):
      f += math.pow(anisoRemoveIso[ni]/sigmaSphericityBeta[ni],2)/2.
      if (do_gradient):
        for nj in range(6):
          dBetaIso_by_dBetaAnoI = dBetaIso_by_dBIso[nj]*dBIso_by_dBetaAno[ni]
          if ni == nj:
            dBetaAno_by_dBetaAnoI = 1.
          else:
            dBetaAno_by_dBetaAnoI = 0.
          g[ni] += (anisoRemoveIso[nj]*(dBetaAno_by_dBetaAnoI-dBetaIso_by_dBetaAnoI) /
                        math.pow(sigmaSphericityBeta[nj],2) )
          if (do_hessian):
            for nk in range(6):
              dBetaIso_by_dBetaAnoI = dBetaIso_by_dBIso[nk]*dBIso_by_dBetaAno[ni]
              if nk == ni:
                dBetaAno_by_dBetaAnoI = 1.
              else:
                dBetaAno_by_dBetaAnoI = 0.
              dBetaIso_by_dBetaAnoJ = dBetaIso_by_dBIso[nk]*dBIso_by_dBetaAno[nj]
              if nk == nj:
                dBetaAno_by_dBetaAnoJ = 1.
              else:
                dBetaAno_by_dBetaAnoJ = 0.
              h[ni,nj] += ( (dBetaAno_by_dBetaAnoJ - dBetaIso_by_dBetaAnoJ) *
                            (dBetaAno_by_dBetaAnoI - dBetaIso_by_dBetaAnoI) /
                            math.pow(sigmaSphericityBeta[nk],2) )
    return (f, g, h)

  def target_gradient_hessian(self, do_gradient=True, do_hessian=True):
    if do_hessian:
      assert (do_gradient)
    # Extract parameters into variables with sensible names
    subsample = self.subsample
    n_bins = self.n_bins
    i_par = 0
    asqr_scale = self.x[i_par]
    i_par += 1
    sigmaT_bins = self.x[i_par:i_par + n_bins]
    i_par += n_bins
    asqr_beta = tuple(self.x[i_par:i_par + 6])
    i_par += 6
    assert (i_par == len(self.x))

    # Initialise function, gradient and Hessian with zeros
    f = 0.
    g = flex.double(self.nmp, 0)
    h = flex.double(self.nmp * self.nmp, 0)
    h.reshape(flex.grid(self.nmp, self.nmp))

    # Loop over bins to accumulate target, gradient, Hessian
    i_bin_used = 0 # Keep track in case full range of bins not used
    for i_bin in self.sumfsqr_miller.binner().range_used():
      sel = self.sumfsqr_miller.binner().selection(i_bin)
      sumfsqr_miller_sel = self.sumfsqr_miller.select(sel)
      sumfsqr = sumfsqr_miller_sel.data()
      f1f2cos = self.f1f2cos_miller.data().select(sel)
      sigmaE_terms = self.sigmaE_miller.data().select(sel)

      # Make Miller array as basis for computing aniso corrections in bin
      # Let u = A^2*sigmaT to simplify computation of derivatives
      ones_array = flex.double(sumfsqr.size(), 1)
      all_ones = sumfsqr_miller_sel.customized_copy(data=ones_array)
      beta_miller_Asqr = all_ones.apply_debye_waller_factors(
        u_star=adptbx.beta_as_u_star(asqr_beta))
      u_terms = (asqr_scale * sigmaT_bins[i_bin_used]
        * self.target_spectrum[i_bin_used]) * beta_miller_Asqr.data()

      # Use local sphere fsc to compute relative weight of local vs global fitting
      # Steepness of sigmoid controlled by factor applied to fsc.
      # Mean value of f1f2cos should dominate sigmaS_terms calculation when signal
      # is good but it should be completely determined by the anisotropic error model
      # when there is little signal.
      # Keep some influence of anisotropic error model throughout for training.
      # wt_terms weight the anisotropy model in sigmaS calculation, with this
      # weight falling off with increasing local fsc.
      # Behaviour of wt as a function of fsc determined by steepness parameter for
      # the sigmoid function and the maximum fractional weight for the local
      # statistics. Values of local_weight near 1 seem to be better than lower
      # values.
      steep = 9.
      local_weight = 0.95
      fsc = f1f2cos/(sumfsqr/2.)
      # Make sure fsc is in range 0 to 1
      fsc = (fsc + flex.abs(fsc)) / 2
      fsc = (fsc + 1 - flex.abs(fsc - 1)) / 2
      wt_terms = flex.exp(steep*fsc)
      wt_terms = 1. - local_weight * (wt_terms/(math.exp(steep/2) + wt_terms))
      meanwt = flex.mean_default(wt_terms,0.)
      sigmaS_terms = wt_terms*u_terms + (1.-wt_terms)*f1f2cos
      # Make sure sigmaS is non-negative
      sigmaS_terms = (sigmaS_terms + flex.abs(sigmaS_terms))/2

      s2sigE = 2*sigmaS_terms + sigmaE_terms
      var_terms = s2sigE*sigmaE_terms

      # Leave out constant twologpi * number of reflections involved
      assert (flex.min(var_terms) > 0.)
      minusLL_terms = (sumfsqr * (sigmaS_terms + sigmaE_terms)
        - 2 * sigmaS_terms * f1f2cos) / var_terms + flex.log(var_terms)
      f += subsample*flex.sum(minusLL_terms)

      fgh_a_restraint = self.sphericity_restraint(asqr_beta,
          do_gradient, do_hessian, sigma_factor = 2.) # Looser for amplitude-squared
      f += meanwt * fgh_a_restraint[0]

      if do_gradient:
        s2sigE2 = flex.pow2(s2sigE)
        sumsqrcos = sumfsqr + 2*f1f2cos
        if (self.refine_Asqr_scale or self.refine_sigmaT_bins
              or self.refine_Asqr_beta):
          dmLL_by_dsigmaS_terms = (2 * s2sigE - sumsqrcos) / s2sigE2
          dmLL_by_du_terms = wt_terms * dmLL_by_dsigmaS_terms
        if self.refine_Asqr_beta:
          h_as_double, k_as_double, l_as_double = (
            sumfsqr_miller_sel.indices().as_vec3_double().parts())
          hh = flex.pow2(h_as_double)
          kk = flex.pow2(k_as_double)
          ll = flex.pow2(l_as_double)
          hk = h_as_double * k_as_double
          hl = h_as_double * l_as_double
          kl = k_as_double * l_as_double

        i_par = 0 # Keep track of index for unrefined parameters
        i_ref = 0 # Keep track of refined parameters
        if self.refine_Asqr_scale: # Only affects U
          du_by_dAsqr_scale = u_terms / asqr_scale
          i_Asqr_scale = i_ref # Save for mixed second derivatives
          g[i_Asqr_scale] += subsample*flex.sum(dmLL_by_du_terms*du_by_dAsqr_scale)
          i_ref += 1
        i_par += 1
        if self.refine_sigmaT_bins: # Only affects U, just current bin
          du_by_dsigmaT_bin = u_terms / sigmaT_bins[i_bin_used]
          i_sigmaT_bin = i_ref+i_bin_used # Save for restraint terms below
          g[i_sigmaT_bin] += subsample*flex.sum(dmLL_by_du_terms*du_by_dsigmaT_bin)
          i_ref += self.n_bins
        i_par += self.n_bins
        if self.refine_Asqr_beta:  # Only affects U
          hh_factors = [-hh, -kk, -ll, -2*hk, -2*hl, -2*kl]
          du_by_dbetaA = []
          for i_beta in range(6):
            du_by_dbetaA.append(hh_factors[i_beta]*u_terms)
          for i_beta in range(6):
            g[i_ref+i_beta] += subsample*flex.sum(dmLL_by_du_terms * du_by_dbetaA[i_beta])
            g[i_ref+i_beta] += meanwt * fgh_a_restraint[1][i_beta] # Restraint term
          i_ref += 6
        i_par += 6

        assert (i_par == len(self.x))
        assert (i_ref == self.nmp)

        if do_hessian:
          s2sigE3 = s2sigE * s2sigE2
          if (self.refine_Asqr_scale or self.refine_sigmaT_bins
                or self.refine_Asqr_beta):

            d2mLL_by_dsigmaS2_terms = 4 * (
              sumsqrcos - 2 * sigmaS_terms - sigmaE_terms) / s2sigE3
            d2mLL_by_du2_terms = flex.pow2(wt_terms) * d2mLL_by_dsigmaS2_terms

          i_par = 0 # Keep track of index for unrefined parameters
          i_ref = 0  # Keep track of refined parameters
          # Note that second derivatives u wrt Asqr_scale and sigmaT_bins are 0
          if self.refine_Asqr_scale: # Only affects U
            h[i_ref,i_ref] += subsample*(flex.sum(d2mLL_by_du2_terms
              * flex.pow2(du_by_dAsqr_scale)))
            i_ref += 1
          i_par += 1
          if self.refine_sigmaT_bins: # Only affects U, current bin
            h[i_sigmaT_bin,i_sigmaT_bin] += subsample*flex.sum(
              d2mLL_by_du2_terms * flex.pow2(du_by_dsigmaT_bin))
            if self.refine_Asqr_scale:
              d2u_by_dAsqr_scale_by_dsigmaT_bin = du_by_dsigmaT_bin / asqr_scale
              cross_term = subsample*flex.sum(
                d2mLL_by_du2_terms * du_by_dAsqr_scale * du_by_dsigmaT_bin +
                dmLL_by_du_terms * d2u_by_dAsqr_scale_by_dsigmaT_bin)
              h[i_Asqr_scale,i_sigmaT_bin] += cross_term
              h[i_sigmaT_bin,i_Asqr_scale] += cross_term
            i_ref += self.n_bins
          i_par += self.n_bins
          if self.refine_Asqr_beta:  # Only affects U
            for i_beta in range(6):
              if self.refine_Asqr_scale: # Add Asqr_scale mixed derivatives
                d2u_by_dAsqr_scale_by_dbetaA = du_by_dbetaA[i_beta] / asqr_scale
                cross_term = subsample*flex.sum(
                  d2mLL_by_du2_terms * du_by_dAsqr_scale * du_by_dbetaA[i_beta] +
                  dmLL_by_du_terms * d2u_by_dAsqr_scale_by_dbetaA)
                h[i_Asqr_scale,i_ref+i_beta] += cross_term
                h[i_ref+i_beta,i_Asqr_scale] += cross_term
              for j_beta in range(6):
                h[i_ref+i_beta, i_ref+j_beta] += subsample*(
                  flex.sum(d2mLL_by_du2_terms * du_by_dbetaA[i_beta]*du_by_dbetaA[j_beta])
                  + flex.sum(dmLL_by_du_terms * hh_factors[i_beta]*hh_factors[j_beta]*u_terms) )
                # Also add restraint term
                h[i_ref+i_beta,i_ref+j_beta] += meanwt * fgh_a_restraint[2][i_beta, j_beta]
            i_ref += 6
          i_par += 6

          assert (i_par == len(self.x))
          assert (i_ref == self.nmp)

      # Restrain log of sigmaT_bins to 0, but downweighting low resolution
      d_bin = 1./math.sqrt(self.ssqr_bins[i_bin_used])
      sigmascale = 0.15 + 0.0005 * d_bin**3
      stbin = sigmaT_bins[i_bin_used]
      logbin = math.log(stbin)
      f += meanwt * (logbin/sigmascale)**2 / 2
      if do_gradient and self.refine_sigmaT_bins:
        g[i_sigmaT_bin] += meanwt * logbin / (stbin * sigmascale**2)
        if do_hessian:
          h[i_sigmaT_bin,i_sigmaT_bin] += meanwt * (1.-logbin)/(stbin*sigmascale)**2

      i_bin_used += 1

    return (f, g, h, False)

  def target(self):
    f_g_h = self.target_gradient_hessian(do_gradient=False, do_hessian=False)
    return f_g_h[0]

  def target_gradient(self):
    f_g_h = self.target_gradient_hessian(do_hessian=False)
    f = f_g_h[0]
    g = f_g_h[1]
    return (f, g)

  def get_macrocycle_parameters(self):
    if len(self.refine_mask) == 0: # All parameters being refined
      return self.x

    mp = []  # Parameters for this macrocycle
    for i in range(len(self.x)):
      if self.refine_mask[i]:
        mp.append(self.x[i])
    assert (len(mp) == self.nmp)
    return mp

  def set_macrocycle_parameters(self, newx):
    if len(self.refine_mask) == 0:  # All parameters being refined
      self.x = newx
    else:
      npref = 0
      for i in range(len(self.x)):
        if self.refine_mask[i]:
          self.x[i] = newx[npref]
          npref += 1
      assert (npref == self.nmp)

  def macrocycle_large_shifts(self):
    i_par = 0 # Keep track of index for unrefined parameters
    large_shifts = []
    if self.refine_Asqr_scale:
      large_shifts.append(self.x[i_par]/4.)
    i_par += 1
    if self.refine_sigmaT_bins:
      for i_bin in range(self.n_bins):
        large_shifts.append(self.x[i_par+i_bin]/4.)
    i_par += self.n_bins
    if self.refine_Asqr_beta:
      large_shifts.extend(self.large_shifts_beta)
    i_par += 6
    assert (i_par == len(self.x))
    assert (len(large_shifts) == self.nmp)
    return large_shifts

  def set_macrocycle_protocol(self, macrocycle_protocol):
    # Possible parameters include overall scale of signal,
    # bin parameters for signal (BEST-like curve), anisotropy tensor for signal
    self.refine_mask = []  # Indicates "all" if left empty
    self.refine_Asqr_scale = True
    self.refine_sigmaT_bins = True
    self.refine_Asqr_beta = True

    # For each protocol, define variables that aren't refined
    # Currently only have default protocol to refine everything.
    if macrocycle_protocol == ["default"]:
      pass

    else:
      raise Sorry("Macrocycle protocol", macrocycle_protocol, " not recognised")

    # Now accumulate mask
    self.nmp = 0

    if self.refine_Asqr_scale:
      self.refine_mask.append(True)
      self.nmp += 1
    else:
      self.refine_mask.append(False)

    if self.refine_sigmaT_bins:
      self.refine_mask.extend([True for i in range(self.n_bins)])
      self.nmp += self.n_bins
    else:
      self.refine_mask.extend([False for i in range(self.n_bins)])

    if self.refine_Asqr_beta:
      self.refine_mask.extend([True for i in range(6)])
      self.nmp += 6
    else:
      self.refine_mask.extend([False for i in range(6)])

    assert (len(self.refine_mask) == len(self.x))

  def macrocycle_parameter_names(self, full_list=False):
    parameter_names = []
    if full_list or self.refine_Asqr_scale:
      parameter_names.append("Asqr_scale")
    if full_list or self.refine_sigmaT_bins:
      for i in range(self.n_bins):
        parameter_names.append("SigmaT_bin#" + str(i + 1))
    if full_list or self.refine_Asqr_beta:
      parameter_names.append("Asqr_beta11")
      parameter_names.append("Asqr_beta22")
      parameter_names.append("Asqr_beta33")
      parameter_names.append("Asqr_beta12")
      parameter_names.append("Asqr_beta13")
      parameter_names.append("Asqr_beta23")

    if not full_list:
      assert (len(parameter_names) == self.nmp)
    else:
      assert (len(parameter_names) == len(self.x))
    return parameter_names

  def reparameterize(self):
    i_par = 0 # Keep track of index for unrefined parameters
    repar = []

    if self.refine_Asqr_scale:
      repar.append(Reparams(False))
    i_par += 1

    if self.refine_sigmaT_bins:
      repar.extend([Reparams(False) for i in range(self.n_bins)])
    i_par += self.n_bins

    if self.refine_Asqr_beta:
      repar.extend([Reparams(False) for i in range(6)])
    i_par += 6

    assert (i_par == len(self.x))
    assert (len(repar) == self.nmp)

    return repar

  def bounds(self):
    i_par = 0
    bounds_list = []

    if self.refine_Asqr_scale:
      this_bound = Bounds()
      this_bound.lower_on(0.0001*self.start_x[i_par])
      bounds_list.append(this_bound)
    i_par += 1

    if self.refine_sigmaT_bins:
      this_bound = Bounds()
      this_bound.lower_on(0.001)
      for i in range(self.n_bins):
        bounds_list.append(this_bound)
    i_par += self.n_bins

    if self.refine_Asqr_beta:
      this_bound = Bounds()
      # this_bound.off()
      for i in range(6):
        this_bound.on(-self.max_beta[i],self.max_beta[i])
        bounds_list.append(this_bound)
    i_par += 6

    assert (i_par == len(self.x))
    assert (len(bounds_list) == self.nmp)
    return bounds_list

  def current_statistics(self, level=3, full_list=False):
    self.log_tab_printf(1, level, "Log-likelihood: %10.6g\n", -self.target())
    self.log_blank(level)

    parameter_names = self.macrocycle_parameter_names(full_list=full_list)
    if full_list:
      self.log_tab(1, level, "All parameters")
    else:
      self.log_tab(1, level, "Refined parameters")
    list_all = (full_list or len(self.refine_mask) == 0)
    iref = 0
    for i in range(len(self.x)):
      if list_all or self.refine_mask[i]:
        self.log_tab_printf(2, level, "%-15s %10.5g\n", (parameter_names[iref], self.x[i]))
        iref += 1

  def initial_statistics(self):
    level=2
    self.log_blank(level)
    self.log_tab(1, level, "Initial statistics")
    self.current_statistics(level=level, full_list=True)

  def final_statistics(self):
    level=2
    self.log_blank(level)
    self.log_tab(1, level, "Final statistics")
    self.current_statistics(level=level, full_list=True)

  def cleanup(self):
    pass

def default_target_spectrum(ssqr):
  # Placeholder for something better based on analysis of cryoEM reconstructions
  # Scaled data from BEST curve. Original data obtained from Sasha Popov, then
  # rescaled to correspond at higher resolution to the average X-ray scattering
  # factor from proteins atoms (with average atomic composition)
  best_data = ((0.009, 3.40735),
              (0.013092, 2.9006),
              (0.0171839, 2.33083),
              (0.0212759, 1.80796),
              (0.0253679, 1.65133),
              (0.0294599, 1.75784),
              (0.0335518, 2.06865),
              (0.0376438, 2.57016),
              (0.0417358, 3.13121),
              (0.0458278, 3.62596),
              (0.0499197, 3.92071),
              (0.0540117, 3.98257),
              (0.0581037, 3.91846),
              (0.0621956, 3.80829),
              (0.0662876, 3.69517),
              (0.0703796, 3.59068),
              (0.0744716, 3.44971),
              (0.0785635, 3.30765),
              (0.0826555, 3.16069),
              (0.0867475, 2.98656),
              (0.0908395, 2.77615),
              (0.0949314, 2.56306),
              (0.0990234, 2.37314),
              (0.103115, 2.22874),
              (0.107207, 2.09477),
              (0.111299, 1.98107),
              (0.115391, 1.8652),
              (0.119483, 1.75908),
              (0.123575, 1.67093),
              (0.127667, 1.59257),
              (0.131759, 1.52962),
              (0.135851, 1.48468),
              (0.139943, 1.45848),
              (0.144035, 1.43042),
              (0.148127, 1.40953),
              (0.152219, 1.37291),
              (0.156311, 1.34217),
              (0.160403, 1.3308),
              (0.164495, 1.32782),
              (0.168587, 1.30862),
              (0.172679, 1.31319),
              (0.176771, 1.30907),
              (0.180863, 1.31456),
              (0.184955, 1.31055),
              (0.189047, 1.31484),
              (0.193139, 1.31828),
              (0.197231, 1.32321),
              (0.201323, 1.30853),
              (0.205415, 1.30257),
              (0.209507, 1.2851),
              (0.213599, 1.26912),
              (0.217691, 1.24259),
              (0.221783, 1.24119),
              (0.225875, 1.2382),
              (0.229967, 1.21605),
              (0.234059, 1.17269),
              (0.23815, 1.13909),
              (0.242242, 1.1165),
              (0.246334, 1.08484),
              (0.250426, 1.0495),
              (0.254518, 1.01289),
              (0.25861, 0.974819),
              (0.262702, 0.940975),
              (0.266794, 0.900938),
              (0.270886, 0.861657),
              (0.274978, 0.830192),
              (0.27907, 0.802167),
              (0.283162, 0.780746),
              (0.287254, 0.749194),
              (0.291346, 0.720884),
              (0.295438, 0.694409),
              (0.29953, 0.676239),
              (0.303622, 0.650672),
              (0.307714, 0.632438),
              (0.311806, 0.618569),
              (0.315898, 0.605762),
              (0.31999, 0.591398),
              (0.324082, 0.579308),
              (0.328174, 0.572076),
              (0.332266, 0.568138),
              (0.336358, 0.559537),
              (0.34045, 0.547927),
              (0.344542, 0.539319),
              (0.348634, 0.529009),
              (0.352726, 0.516954),
              (0.356818, 0.512218),
              (0.36091, 0.511836),
              (0.365002, 0.511873),
              (0.369094, 0.506957),
              (0.373186, 0.502738),
              (0.377278, 0.50191),
              (0.38137, 0.492422),
              (0.385462, 0.488461),
              (0.389553, 0.483436),
              (0.393645, 0.481468),
              (0.397737, 0.473786),
              (0.401829, 0.468684),
              (0.405921, 0.468291),
              (0.410013, 0.46645),
              (0.414105, 0.4643),
              (0.418197, 0.45641),
              (0.422289, 0.450462),
              (0.426381, 0.444678),
              (0.430473, 0.443807),
              (0.434565, 0.441158),
              (0.438657, 0.441303),
              (0.442749, 0.437144),
              (0.446841, 0.428504),
              (0.450933, 0.420459),
              (0.455025, 0.413754),
              (0.459117, 0.412064),
              (0.463209, 0.406677),
              (0.467301, 0.40253),
              (0.471393, 0.396454),
              (0.475485, 0.393192),
              (0.479577, 0.390452),
              (0.483669, 0.38408),
              (0.487761, 0.379456),
              (0.491853, 0.373123),
              (0.495945, 0.374026),
              (0.500037, 0.373344),
              (0.504129, 0.377639),
              (0.508221, 0.374029),
              (0.512313, 0.374691),
              (0.516405, 0.371632),
              (0.520497, 0.370724),
              (0.524589, 0.366095),
              (0.528681, 0.369447),
              (0.532773, 0.369043),
              (0.536865, 0.368967),
              (0.540956, 0.36583),
              (0.545048, 0.370593),
              (0.54914, 0.371047),
              (0.553232, 0.372723),
              (0.557324, 0.371915),
              (0.561416, 0.372882),
              (0.565508, 0.371052),
              (0.5696, 0.36775),
              (0.573692, 0.369884),
              (0.577784, 0.374098),
              (0.581876, 0.374169),
              (0.585968, 0.37261),
              (0.59006, 0.372356),
              (0.594152, 0.377055),
              (0.598244, 0.3817),
              (0.602336, 0.381867),
              (0.606428, 0.377746),
              (0.61052, 0.377157),
              (0.614612, 0.376604),
              (0.618704, 0.37532),
              (0.622796, 0.372488),
              (0.626888, 0.373312),
              (0.63098, 0.377505),
              (0.635072, 0.381011),
              (0.639164, 0.379326),
              (0.643256, 0.380193),
              (0.647348, 0.381122),
              (0.65144, 0.387213),
              (0.655532, 0.391928),
              (0.659624, 0.398986),
              (0.663716, 0.402951),
              (0.667808, 0.405893),
              (0.6719, 0.40217),
              (0.675992, 0.401806),
              (0.680084, 0.404238),
              (0.684176, 0.409404),
              (0.688268, 0.413486),
              (0.692359, 0.413167),
              (0.696451, 0.414008),
              (0.700543, 0.417128),
              (0.704635, 0.420275),
              (0.708727, 0.423617),
              (0.712819, 0.42441),
              (0.716911, 0.426445),
              (0.721003, 0.429012),
              (0.725095, 0.430132),
              (0.729187, 0.42992),
              (0.733279, 0.425202),
              (0.737371, 0.423159),
              (0.741463, 0.423913),
              (0.745555, 0.425542),
              (0.749647, 0.426682),
              (0.753739, 0.431186),
              (0.757831, 0.433959),
              (0.761923, 0.433839),
              (0.766015, 0.428679),
              (0.770107, 0.425968),
              (0.774199, 0.426528),
              (0.778291, 0.427093),
              (0.782383, 0.426848),
              (0.786475, 0.424549),
              (0.790567, 0.423785),
              (0.794659, 0.419892),
              (0.798751, 0.417391),
              (0.802843, 0.413128),
              (0.806935, 0.408498),
              (0.811027, 0.402764),
              (0.815119, 0.404852),
              (0.819211, 0.405915),
              (0.823303, 0.392919),
              (0.827395, 0.384632),
              (0.831487, 0.382626),
              (0.835579, 0.379891),
              (0.839671, 0.376414),
              (0.843762, 0.372915),
              (0.847854, 0.375089),
              (0.851946, 0.371918),
              (0.856038, 0.36652),
              (0.86013, 0.358529),
              (0.864222, 0.356496),
              (0.868314, 0.354707),
              (0.872406, 0.348802),
              (0.876498, 0.343693),
              (0.88059, 0.34059),
              (0.884682, 0.342432),
              (0.888774, 0.345099),
              (0.892866, 0.344524),
              (0.896958, 0.342489),
              (0.90105, 0.328009),
              (0.905142, 0.323685),
              (0.909234, 0.321378),
              (0.913326, 0.318832),
              (0.917418, 0.314999),
              (0.92151, 0.311775),
              (0.925602, 0.30844),
              (0.929694, 0.30678),
              (0.933786, 0.303484),
              (0.937878, 0.301197),
              (0.94197, 0.296788),
              (0.946062, 0.295353),
              (0.950154, 0.298028),
              (0.954246, 0.298098),
              (0.958338, 0.295081),
              (0.96243, 0.289337),
              (0.966522, 0.286116),
              (0.970614, 0.284319),
              (0.974706, 0.280972),
              (0.978798, 0.28015),
              (0.98289, 0.279016),
              (0.986982, 0.277532),
              (0.991074, 0.276013),
              (0.995165, 0.270923),
              (0.999257, 0.269446),
              (1.00335, 0.266567),
              (1.00744, 0.263561),
              (1.01153, 0.261002),
              (1.01563, 0.255349),
              (1.01972, 0.258644),
              (1.02381, 0.254974),
              (1.0279, 0.2523),
              (1.03199, 0.244489),
              (1.03609, 0.249418),
              (1.04018, 0.249519),
              (1.04427, 0.249316),
              (1.04836, 0.249197),
              (1.05245, 0.24415),
              (1.05655, 0.244556),
              (1.06064, 0.241169),
              (1.06473, 0.238484),
              (1.06882, 0.2392),
              (1.07291, 0.240651),
              (1.077, 0.243724),
              (1.0811, 0.243174),
              (1.08519, 0.239545),
              (1.08928, 0.239106),
              (1.09337, 0.238763),
              (1.09746, 0.238971),
              (1.10156, 0.229925),
              (1.10565, 0.225123),
              (1.10974, 0.226932),
              (1.11383, 0.23118),
              (1.11792, 0.228654),
              (1.12202, 0.225084),
              (1.12611, 0.225866),
              (1.1302, 0.227717),
              (1.13429, 0.229508),
              (1.13838, 0.227977),
              (1.14248, 0.226799),
              (1.14657, 0.228456),
              (1.15066, 0.22383),
              (1.15475, 0.22188),
              (1.15884, 0.219986),
              (1.16294, 0.217418),
              (1.16703, 0.214356),
              (1.17112, 0.211027),
              (1.17521, 0.210011),
              (1.1793, 0.210609),
              (1.1834, 0.210893),
              (1.18749, 0.212583),
              (1.19158, 0.208415),
              (1.19567, 0.204557),
              (1.19976, 0.198068),
              (1.20386, 0.197603),
              (1.20795, 0.196691),
              (1.21204, 0.200617),
              (1.21613, 0.199803),
              (1.22022, 0.199199),
              (1.22432, 0.196859),
              (1.22841, 0.197471),
              (1.2325, 0.19799))
  # 300 data points from 0.009 to 1.2325, so separated by 0.004091973
  s1 = (ssqr - 0.009) / 0.004091973
  is1 = int(math.floor(s1))
  if is1 < 0:
    return best_data[0][1] # Below low-res limit for BEST data
  elif is1 >= 299:
    return best_data[0][299]  # Above high-res limit, about 0.9A
  else:
    ds = s1 - is1
    is2 = is1 + 1
    best_val = (1.-ds)*best_data[is1][1] + ds*best_data[is2][1]
    return best_val

def sphere_enclosing_model(model):
  sites_cart = model.get_sites_cart()
  model_centre = sites_cart.mean()
  dsqrmax = flex.max((sites_cart - tuple(model_centre)).norms()) ** 2
  model_radius = math.sqrt(dsqrmax)
  return model_centre, model_radius

def mask_fixed_model_region(mmm, d_min,
                                 fixed_model=None,
                                 ordered_mask_id=None,
                                 fixed_mask_id='mask_around_atoms'):
  # Flatten the region of the ordered volume mask covered by the fixed model
  # and keep the mask for the fixed model.

  # Do nothing if no fixed model
  if fixed_model is None:
    return

  radius = 3.
  if d_min is not None:
    radius = max(radius, d_min)
  mmm.create_mask_around_atoms(model=fixed_model, mask_atoms_atom_radius=radius, mask_id=fixed_mask_id)
  fixed_model_mask = mmm.get_map_manager_by_id(map_id=fixed_mask_id)

  if ordered_mask_id is not None:
    # Invert fixed_model_mask before flattening
    fixed_model_mask.set_map_data(map_data = 1. - fixed_model_mask.map_data())
    mmm.apply_mask_to_map(map_id=ordered_mask_id, mask_id=fixed_mask_id)
    # Invert back so we can use to assess fraction of fixed model in masked sphere
    fixed_model_mask.set_map_data(map_data = 1. - fixed_model_mask.map_data())

def add_local_squared_deviation_map(
    mmm, coeffs_in, radius, d_min, map_id_out):
  """
  Add spherically-averaged squared map to map_model_manager

  Compulsory arguments:
  mmm:        map_model_manager to add new map
  coeffs_in:  Miller array with coefficients for input map
  radius:     radius of sphere over which squared deviation is averaged
  d_min:      resolution to use for calculation
  map_id_out: identifier of output map
  """

  map_out = coeffs_in.local_standard_deviation_map(radius=radius, d_min=d_min)
  # map_out is an fft_map object, which can't easily be added to
  # map_model_manager as a similar map_manager object, so cycle through FT
  mm_out = map_out.as_map_manager()
  mean_square_map_coeffs = mm_out.map_as_fourier_coefficients(d_min=d_min)
  mmm.add_map_from_fourier_coefficients(mean_square_map_coeffs,
      map_id=map_id_out)
  # All map values should be positive, but round trip through FT might change
  # this. Check and add an offset if required to make minimum non-negative.
  mm_out = mmm.get_map_manager_by_id(map_id=map_id_out)
  min_map_value = flex.min(mm_out.map_data())
  if min_map_value < 0:
    offset = -min_map_value
    mm_out.set_map_data(map_data = mm_out.map_data() + offset)

def auto_sharpen_by_FSC(mc1, mc2):
  # Simply sharpen by bin-wise <F^2> and multiply by FSC
  # This didn't work as well as auto_sharpen_isotropic, but may be worth trying
  # again with a smooth curve over resolution instead of bins

  mapCC = mc1.map_correlation(other=mc2)
  if mapCC >= 1.:
    raise Sorry("Half-maps must be non-identical")
  mc1.setup_binner_d_star_sq_bin_size()
  mc2.use_binner_of(mc1)
  mc1s = mc1.customized_copy(data = mc1.data())
  mc2s = mc2.customized_copy(data = mc2.data())

  for i_bin in mc1.binner().range_used():
    sel = mc1.binner().selection(i_bin)
    mc1sel = mc1.select(sel)
    mc2sel = mc2.select(sel)
    mapCC = mc1sel.map_correlation(other=mc2sel)
    mapCC = max(mapCC,0.001) # Avoid zero or negative values
    FSCref = math.sqrt(2./(1.+1./mapCC))
    fsq = flex.pow2(flex.abs(mc1sel.data()))
    meanfsq = flex.mean_default(fsq, 1.e-10)
    mc1s.data().set_selected(sel, mc1.data().select(sel) * FSCref/math.sqrt(meanfsq))
    mc2s.data().set_selected(sel, mc2.data().select(sel) * FSCref/math.sqrt(meanfsq))

  return mc1s, mc2s

def auto_sharpen_isotropic(mc1, mc2):
  # Use Wilson plot in which <F^2> is divided by FSC^2, so that sharpening
  # downweights data with poor FSC. Results of fit to Wilson plot could be
  # odd if stated d_min goes much beyond real signal.

  mapCC = mc1.map_correlation(other=mc2)
  if mapCC >= 1.:
    raise Sorry("Half-maps must be non-identical")
  mc1.setup_binner_d_star_sq_bin_size()
  mc2.use_binner_of(mc1)

  sumw = 0
  sumwx = 0.
  sumwy = 0.
  sumwx2 = 0.
  sumwxy = 0.
  for i_bin in mc1.binner().range_used():
    sel = mc1.binner().selection(i_bin)
    mc1sel = mc1.select(sel)
    mc2sel = mc2.select(sel)
    if mc1sel.size() < 2:
      continue
    mapCC = mc1sel.map_correlation(other=mc2sel)
    mapCC = max(mapCC,0.001) # Avoid zero or negative values
    FSCref = math.sqrt(2./(1.+1./mapCC))
    ssqr = mc1sel.d_star_sq().data()
    x = flex.mean_default(ssqr, 0) # Mean 1/d^2 for bin
    fsq = flex.pow2(flex.abs(mc1sel.data()))
    meanfsq = flex.mean_default(fsq, 0)
    y = math.log(meanfsq/(FSCref*FSCref))
    w = fsq.size() * FSCref
    sumw += w
    sumwx += w * x
    sumwy += w * y
    sumwx2 += w * x**2
    sumwxy += w * x * y

  slope = (sumw * sumwxy - (sumwx * sumwy)) / (sumw * sumwx2 - sumwx**2)
  b_sharpen = 2 * slope # Divided by 2 to apply to amplitudes
  all_ones = mc1.customized_copy(data = flex.double(mc1.size(), 1))
  b_terms_miller = all_ones.apply_debye_waller_factors(b_iso = b_sharpen)
  mc1s = mc1.customized_copy(data = mc1.data()*b_terms_miller.data())
  mc2s = mc2.customized_copy(data = mc2.data()*b_terms_miller.data())

  return mc1s, mc2s

def add_ordered_volume_mask(
    mmm, d_min, rad_factor=2, protein_mw=None, nucleic_mw=None,
    ordered_mask_id='ordered_volume_mask'):
  """
  Add map defining mask covering the volume of most ordered density required
  to contain the specified content of protein and nucleic acid, judged by
  local map variance.

  Compulsory arguments:
  mmm: map_model_manager containing input half-maps in default map_managers
  d_min: estimate of best resolution for map

  Optional arguments:
  rad_factor: factor by which d_min is multiplied to get radius for averaging
    sphere, defaults to 2
  protein_mw*: molecular weight of protein expected in map, if any
  nucleic_mw*: molecular weight of nucleic acid expected in map
  ordered_mask_id: identifier of output map, defaults to ordered_volume_mask

  * Note that at least one of protein_mw and nucleic_mw must be specified,
    and that the map must be complete
  """

  if (protein_mw is None) and (nucleic_mw is None):
    raise Sorry("At least one of protein_mw and nucleic_mw must be defined")
  if (not mmm.map_manager_1().is_full_size()) or  (
      not mmm.map_manager_2().is_full_size()):
    raise Sorry("Please supply half-maps that are full-size, not cut out from a larger box")

  if d_min is None or d_min <= 0:
    spacings = get_grid_spacings(mmm.map_manager().unit_cell(),
                                 mmm.map_manager().unit_cell_grid)
    d_min = 2.5 * max(spacings)

  # Compute local average of squared density, using a sphere that will cover a
  # sufficient number of independent points. A rad_factor of 2 should yield
  # 4*Pi/3 * (2*2)^3 or about 270 independent points for the average; fewer
  # if the higher resolution data barely contribute. Larger values give less
  # noise but lower resolution for producing a mask. A minimum radius of 5
  # is enforced to explore next-nearest-neighbour density. A maximum of 20
  # is enforced to keep some data at very low resolution
  radius = min(max(d_min*rad_factor, 5.),20.)

  d_work = (d_min + radius) # Save some time by lowering resolution
  mm1 = mmm.map_manager_1()
  mc1_in = mm1.map_as_fourier_coefficients(d_min=d_work)
  mm2 = mmm.map_manager_2()
  mc2_in = mm2.map_as_fourier_coefficients(d_min=d_work)
  if d_min < 10.: # Auto-sharpen unless very low resolution
    mc1s, mc2s = auto_sharpen_isotropic(mc1_in, mc2_in)
  else:
    mc1s = mc1_in
    mc2s = mc2_in
  mcs_mean  = mc1s.customized_copy(data = (mc1s.data() + mc2s.data())/2)

  add_local_squared_deviation_map(mmm, mcs_mean, radius, d_work,
      map_id_out='map_variance')
  mvmm = mmm.get_map_manager_by_id('map_variance')
  # mvmm.write_map("mvmm.map") # Uncomment to check intermediate result

  # Choose enough points in averaged squared density map to covered expected
  # ordered structure. An alternative that could be implemented is to assign
  # any parts of map unlikely to arise from noise as ordered density, without
  # reference to expected content, possibly like the false discovery rate approach.
  map_volume = mmm.map_manager().unit_cell().volume()
  mvmm_map_data = mvmm.map_data()
  numpoints = mvmm_map_data.size()
  # Convert content into volume using partial specific volumes
  target_volume = 0.
  if protein_mw is not None:
    target_volume += protein_mw*1.229
  if nucleic_mw is not None:
    target_volume += nucleic_mw*0.945
  # Expand volume by amount needed for sphere expanded by 1.5*d_min in radius
  equivalent_radius = math.pow(target_volume / (4*math.pi/3.),1./3)
  volume_factor = ((equivalent_radius+1.5*d_min)/equivalent_radius)**3
  expanded_target_volume = target_volume*volume_factor
  target_points = int(expanded_target_volume/map_volume * numpoints)

  # Find threshold for target number of masked points
  from cctbx.maptbx.segment_and_split_map import find_threshold_in_map
  threshold = find_threshold_in_map(target_points = target_points,
      map_data = mvmm_map_data)
  temp_bool_3D = (mvmm_map_data >=  threshold)
  # as_double method doesn't work for multidimensional flex.bool
  mask_shape = temp_bool_3D.all()
  overall_mask = temp_bool_3D.as_1d().as_double()
  overall_mask.reshape(flex.grid(mask_shape))
  new_mm = mmm.map_manager().customized_copy(map_data=overall_mask)

  # Clean up temporary map, then add ordered volume mask
  mmm.remove_map_manager_by_id('map_variance')
  mmm.add_map_manager_by_id(new_mm,map_id=ordered_mask_id)

def get_grid_spacings(unit_cell, unit_cell_grid):
  if unit_cell.parameters()[3:] != (90,90,90):
    raise Sorry("Unit cell must be orthogonal") # Required for this method
  sp = []
  for a,n in zip(unit_cell.parameters()[:3], unit_cell_grid):
    sp.append(a/n)
  return sp

def get_distance_from_center(c, unit_cell, unit_cell_grid = None,
    center = None):
  """
  Return a 3D flex array containing the distance of each grid point from the
  center of the map.
  Code provided by Tom Terwilliger
  """
  if unit_cell.parameters()[3:] != (90,90,90):
    raise Sorry("Unit cell must be orthogonal") # Required for this method

  acc = c.accessor()
  if not unit_cell_grid: # Assume c contains complete unit cell
    unit_cell_grid = acc.all()
  nu,nv,nw = unit_cell_grid
  dx,dy,dz = get_grid_spacings(unit_cell, unit_cell_grid)
  if not center:
    center = (nu//2,nv//2,nw//2)

  # d is initially going to be squared distance from center
  d = flex.double(nu*nv*nw, 0)
  d.reshape(acc)

  # sum over x,y,z in slices
  dx2 = dx**2
  for i in range(nu):
    dist_sqr = dx2 * (i - center[0])**2
    d[i:i+1,0:nv,0:nw] += dist_sqr
  dy2 = dy**2
  for j in range(nv):
    dist_sqr = dy2 * (j - center[1])**2
    d[0:nu,j:j+1,0:nw] += dist_sqr
  dz2 = dz**2
  for k in range(nw):
    dist_sqr = dz2 * (k - center[2])**2
    d[0:nu,0:nv,k:k+1] += dist_sqr

  # Take square root to get distances
  d = flex.sqrt(d)

  return d

def get_maximal_mask_radius(mm_ordered_mask):

  # Check assumption that this is a full map
  if not mm_ordered_mask.is_full_size():
    raise Sorry("Provided map or mask must be full-size")

  unit_cell = mm_ordered_mask.unit_cell()
  om_data = mm_ordered_mask.map_data()
  d_from_c = get_distance_from_center(om_data, unit_cell = unit_cell)
  sel = om_data > 0
  selected_grid_indices = sel.iselection()
  mask_distances = d_from_c.select(selected_grid_indices)
  maximal_radius = mask_distances.min_max_mean().max
  return maximal_radius

def get_mask_radius(mm_ordered_mask,frac_coverage):
  """
  Get radius of sphere around map center enclosing desired fraction of
  ordered density
  """

  # Check assumption that this is a full map
  if not mm_ordered_mask.is_full_size():
    raise Sorry("Provided map or mask must be full-size")

  unit_cell = mm_ordered_mask.unit_cell()
  om_data = mm_ordered_mask.map_data()
  d_from_c = get_distance_from_center(om_data, unit_cell = unit_cell)
  sel = om_data > 0
  selected_grid_indices = sel.iselection()
  mask_distances = d_from_c.select(selected_grid_indices)
  mask_distances = mask_distances.select(flex.sort_permutation(data=mask_distances))
  masked_points = mask_distances.size()
  mask_radius = mask_distances[int(math.floor(frac_coverage*masked_points))-1]
  return mask_radius

def get_ordered_volume_exact(mm_ordered_mask,sphere_center,sphere_radius):
  """
  Get volume of density flagged as ordered inside sphere.
  Useful as a followup to fast spherical average approach, which can be deceived
  by counting ordered volume in a neighbouring cell if the cryoEM map is cropped
  tightly compared to the sphere radius.
  mm_ordered_mask: mask defining ordered volume
  sphere_center: centre of sphere in orthogonal coordinates
  sphere_radius: radius of sphere
  """

  # Check assumption that this is a full map
  if not mm_ordered_mask.is_full_size():
    raise Sorry("Provided map or mask must be full-size")

  unit_cell = mm_ordered_mask.unit_cell()
  om_data = mm_ordered_mask.map_data()
  sphere_center_frac = unit_cell.fractionalize(tuple(sphere_center))
  sphere_center_grid = [round(n * f) for n,f in zip(mm_ordered_mask.map_data().all(),
      sphere_center_frac)]
  d_from_c = get_distance_from_center(om_data, unit_cell = unit_cell,
      center = sphere_center_grid)
  sel = d_from_c <= sphere_radius
  selected_grid_indices = sel.iselection()
  om_data_sel = om_data.select(selected_grid_indices)
  sel = om_data_sel > 0
  ordered_in_sphere = om_data_sel.select(sel)
  ordered_volume = (ordered_in_sphere.size()/om_data.size()) * unit_cell.volume()
  return ordered_volume

def get_flex_max(a,b):
  c = a.deep_copy()
  sel = (b>a)
  c.set_selected(sel,b.select(sel))
  return c

def write_mtz(miller, file_name, root):
  mtz_dataset = miller.as_mtz_dataset(column_root_label=root)
  mtz_object=mtz_dataset.mtz_object()
  dm = DataManager()
  dm.set_overwrite(True)
  dm.write_miller_array_file(mtz_object, filename=file_name)

def largest_prime_factor(i):
  from libtbx.math_utils import prime_factors_of
  pf = prime_factors_of(i)
  if len(pf) == 0: # 0 or 1
    return 1
  else:
    return pf[-1]

def next_allowed_grid_size(i, largest_prime=5):
  if i<2:
    j = 2
  elif i%2 == 1:
    j = i+1
  else:
    j = i
  while (largest_prime_factor(j) > largest_prime):
    j += 2
  return j

def get_sharpening_b(intensities):

  # Initialise data for Wilson plot
  sumw = 0.
  sumwx = 0.
  sumwy = 0.
  sumwxy = 0.
  sumwx2 = 0.

  # Assume that binning has been defined and get data in bins
  int_binned = intensities.customized_copy(data = intensities.data())
  int_binned.setup_binner_d_star_sq_bin_size()
  for i_bin in int_binned.binner().range_used():
    sel = int_binned.binner().selection(i_bin)
    int_sel = int_binned.select(sel)
    ssqr = int_sel.d_star_sq().data()
    x = flex.mean_default(ssqr, 0) # Mean 1/d^2 for bin
    mean_int_sel_data = flex.mean_default(int_sel.data(),0)
    if mean_int_sel_data > 0.:
      y = math.log(mean_int_sel_data)
      w = int_sel.size()
      sumw += w
      sumwx += w * x
      sumwy += w * y
      sumwxy += w * x * y
      sumwx2 += w * x * x

  slope = (sumw * sumwxy - (sumwx * sumwy)) / (sumw * sumwx2 - sumwx**2)
  b_sharpen = 4 * slope

  return b_sharpen

def intensities_as_expanded_map(mm,marray):
  '''
  Take miller array (assumed here to be real-valued and in P1, but could be
  generalised), place values in a map, offset to the center.
  '''
  h_indices, k_indices, l_indices = marray.indices().as_vec3_double().parts()
  h_indices = h_indices.iround()
  k_indices = k_indices.iround()
  l_indices = l_indices.iround()
  hmin = flex.min(h_indices)
  hmax = flex.max(h_indices)
  kmin = flex.min(k_indices)
  kmax = flex.max(k_indices)
  lmin = flex.min(l_indices)
  lmax = flex.max(l_indices)
  assert ((hmin < 0) and (hmax >= -hmin))
  assert ((kmin < 0) and (kmax >= -kmin))
  assert (lmin == 0)
  data = marray.data()

  # Shift hkl to center, add buffer on edges to get to sensible grid for FFT
  # Apply Friedel symmetry to get full sphere of Fourier coefficients
  h_range = next_allowed_grid_size(2*hmax+2)
  h_offset = h_range//2
  h_map_indices = h_offset + h_indices
  h_inverse_indices = h_offset - h_indices

  k_range = next_allowed_grid_size(2*kmax+2)
  k_offset = k_range//2
  k_map_indices = k_offset + k_indices
  k_inverse_indices = k_offset - k_indices

  l_range = next_allowed_grid_size(2*lmax+2)
  l_offset = l_range//2
  l_map_indices = l_offset + l_indices
  l_inverse_indices = l_offset - l_indices

  miller_as_map_data = flex.double(h_range*k_range*l_range,0.)
  miller_as_map_data.reshape(flex.grid(h_range,k_range,l_range))
  for ih,ik,il,coeff in zip(h_map_indices,k_map_indices,l_map_indices,data):
    miller_as_map_data[ih,ik,il] = coeff
  for ih,ik,il,coeff in zip(h_inverse_indices,k_inverse_indices,l_inverse_indices,data):
    miller_as_map_data[ih,ik,il] = coeff

  # Unit cell for reciprocal space sphere of data is obtained from the reciprocal cell
  # parameters for the coefficients computed from the boxed map, times the
  # extents in h,k,l
  from cctbx import crystal
  abcstar = mm.crystal_symmetry().unit_cell().reciprocal_parameters()[:3]
  ucr = tuple(list(flex.double(abcstar)*flex.double((h_range,k_range,l_range)))+[90,90,90])
  crystal_symmetry=crystal.symmetry(ucr,1)
  from iotbx.map_manager import map_manager
  mm_data = map_manager(map_data=miller_as_map_data,unit_cell_grid=miller_as_map_data.all(),
      unit_cell_crystal_symmetry=crystal_symmetry,wrapping=False)
  return group_args(mm_data = mm_data,
                    h_map_indices = h_map_indices,
                    k_map_indices = k_map_indices,
                    l_map_indices = l_map_indices)

def expanded_map_as_intensities(rmm, marray, h_map_indices, k_map_indices, l_map_indices):
  '''
  Fetch miller array data back from map. Only take unique values, ignoring
  Friedel mates.
  '''
  rmap_data = rmm.map_data()
  miller_data = flex.double()
  for ih,ik,il in zip(h_map_indices,k_map_indices,l_map_indices):
    miller_data.append(rmap_data[ih,ik,il])
  miller_array = marray.customized_copy(data = miller_data)
  return miller_array

def local_mean_intensities(mm, d_min, intensities, r_star, b_sharpen):
  """
  Compute local means of input intensities (or amplitudes) using a convolution
  followed optionally by a resolution-dependent renormalisation

  Compulsory argument:
  mm: map_manager corresponding to system from which intensities are obtained
  d_min: target resolution of locally-averaged intensities
  intensities: Terms to be locally averaged, extended by r_star from 1/d_min
  r_star: radius in reciprocal space for averaging
  """

  # Check that resolution range has been extended from d_min
  d_star_sq_max = flex.max(intensities.d_star_sq().data())
  assert d_star_sq_max > 1/d_min**2

  # Sharpen intensities to reduce dynamic range for numerical stability
  # Put the overall B back at end
  int_sharp = intensities.customized_copy(data = intensities.data())
  all_ones = int_sharp.customized_copy(data = flex.double(int_sharp.size(), 1))
  b_terms_miller = all_ones.apply_debye_waller_factors(b_iso = b_sharpen)
  int_sharp = int_sharp.customized_copy(data = intensities.data()*b_terms_miller.data())

  # Turn intensity values into a spherical map inside a cube
  results = intensities_as_expanded_map(mm,int_sharp)
  coeffs_as_map = results.mm_data # map_manager with intensities
  h_map_indices = results.h_map_indices # grid positions for original hkl in order
  k_map_indices = results.k_map_indices
  l_map_indices = results.l_map_indices

  # Put map into map_model_manager to ensure matching grid for spherically-averaged map
  rmmm = map_model_manager(map_manager = coeffs_as_map)

  # Cell for "map" of intensities is reciprocal of real-space cell
  # Compute reciprocal d_min as twice the grid spacing between intensities
  rcp = coeffs_as_map.unit_cell().parameters()
  nu, nv, nw = coeffs_as_map.map_data().all()
  rdmin = 2*flex.min(flex.double(rcp[:3])/flex.double((nu,nv,nw)))
  inverse_intensities = coeffs_as_map.map_as_fourier_coefficients(d_min=rdmin)

  # Compute G-function, avoiding divide by zero and numerical precision issues
  # for the origin term
  stol = flex.sqrt(inverse_intensities.sin_theta_over_lambda_sq().data())
  w = 4 * stol * math.pi * r_star
  sel = (w < 0.001)
  w.set_selected(sel,0.001)
  sphere_reciprocal = 3 * (flex.sin(w) - w * flex.cos(w))/flex.pow(w, 3)
  sphere_reciprocal.set_selected(sel,1.)

  # Spherically-averaged intensities from FT of product of FTs
  prod_coeffs = inverse_intensities.customized_copy(
      data = inverse_intensities.data()*sphere_reciprocal)
  rmmm.add_map_from_fourier_coefficients(prod_coeffs, map_id='local_mean_map')
  local_mean_as_map = rmmm.get_map_manager_by_id(map_id='local_mean_map')

  # Associate map values with hkl using saved indices, then restrict to dmin
  extended_mean = expanded_map_as_intensities(local_mean_as_map,int_sharp,
      h_map_indices,k_map_indices,l_map_indices)

  min_in = flex.min(int_sharp.data())
  if min_in >= 0: # Make sure strictly non-negative remains non-negative
    extended_mean = extended_mean.customized_copy(data =
      (extended_mean.data()+min_in + flex.abs(extended_mean.data()-min_in))/2 )

  # Select data to desired resolution, remove sharpening from above
  local_mean = extended_mean.select(extended_mean.d_spacings().data() >= d_min)
  all_ones = local_mean.customized_copy(data = flex.double(local_mean.size(), 1))
  b_terms_miller = all_ones.apply_debye_waller_factors(b_iso = -b_sharpen)
  local_mean = local_mean.customized_copy(data = local_mean.data()*b_terms_miller.data())

  return local_mean

def local_mean_density(mm, radius):
  """
  Compute spherically-averaged map

  Compulsory argument:
  mm: map_manager containing map to be averaged
  radius: radius of sphere for averaging
  """

  # Put map into map_model_manager to ensure matching grid for spherically-averaged map
  mmm = map_model_manager(map_manager = mm)
  mc = mm.map_as_fourier_coefficients(d_min=radius/5.)

  # Compute G-function, avoiding divide by zero and numerical precision issues
  # for the origin term
  stol = flex.sqrt(mc.sin_theta_over_lambda_sq().data())
  w = 4 * stol * math.pi * radius
  sel = (w == 0.)
  w.set_selected(sel,0.0001)
  sphere_reciprocal = 3 * (flex.sin(w) - w * flex.cos(w))/flex.pow(w, 3)
  sphere_reciprocal.set_selected(sel,1.)

  # Spherically-averaged map values from FT of product of FTs
  prod_coeffs = mc.customized_copy(data = mc.data()*sphere_reciprocal)
  mmm.add_map_from_fourier_coefficients(prod_coeffs, map_id='local_mean_map')
  local_mean_mm = mmm.get_map_manager_by_id(map_id='local_mean_map')

  return local_mean_mm

def sigmaA_from_model_in_map(expectE, dobs, Fcalc, over_sampling_factor,
                             verbosity=0, log=sys.stdout):
  """
  Compute sigmaA values needed to explain agreement of model with map given map errors

  Compulsory arguments:
  expectE: Emean giving expected E for map
  dobs:    weighting factor for map coefficient accuracy
  Fcalc:   calculated structure factors for placed model
  over_sampling_factor: account for oversampling in LLG calculation
                        relevant here for precision of sigmaA estimate
  Optional arguments:
  verbosity: levels greater than default of 0 progressively more verbose
  log:       destination for printing if any
  """
  model_sigmaA = expectE.customized_copy(data=flex.double(expectE.size(),0))
  Ecalc = Fcalc.customized_copy(data=Fcalc.data())
  xdat = []
  ydat = []
  wdat = []
  if expectE.binner() is None:
    expectE.setup_binner_d_star_sq_bin_size()
  if verbosity > 1:
    print("SigmaA curve for placed model contribution",file=log)
    print(" # of terms   mean(ssqr)  sigmaA   sigma(sigmaA)")
  for i_bin in expectE.binner().range_used():
    sel = expectE.binner().selection(i_bin)
    eEsel = expectE.select(sel)
    abseE = eEsel.amplitudes().data()
    p1 = eEsel.phases().data()
    Fcalc_sel = Fcalc.select(sel)
    absFc = Fcalc_sel.amplitudes().data()
    sigmaP = flex.mean_default(flex.pow2(absFc),0.)
    absEc = absFc/math.sqrt(sigmaP)
    Ecalc.data().set_selected(sel,
                  Fcalc.data().select(sel) / math.sqrt(sigmaP))
    p2 = Fcalc_sel.phases().data()
    cosdphi = flex.cos(p2 - p1)
    dobssel = dobs.data().select(sel)

    # Solve for sigmaA yielding 1st derivative of LLG approximately equal to zero (assuming only numerator matters)
    sum0 = flex.sum(2.*dobssel*abseE*absEc*cosdphi)
    sum1 = flex.sum(2*flex.pow2(dobssel) * (1. - flex.pow2(abseE) - flex.pow2(absEc)))
    sum2 = flex.sum(2*flex.pow(dobssel,3)*abseE*absEc*cosdphi)
    sum3 = flex.sum(- 2*flex.pow(dobssel,4))
    u1 = -2*sum2**3 + 9*sum1*sum2*sum3 - 27*sum0*sum3**2
    sqrt_arg = u1**2 - 4*(sum2**2 - 3*sum1*sum3)**3
    if sqrt_arg < 0: # Paranoia: haven't run into this in a wide variety of calculations
      raise Sorry("Argument of square root in sigmaA calculation is negative")
    third = 1./3
    x1 = (u1 + math.sqrt(sqrt_arg))**third
    sigmaA = ( (2 * 2.**third * sum2**2 - 6 * 2.**third*sum1*sum3
                - 2*sum2*x1 + 2.**(2*third) * x1**2) /
              (6 * sum3 * x1) )
    sigmaA = max(min(sigmaA,0.999),1.E-6)
    # Now work out approximate e.s.d. of sigmaA estimate from 2nd derivative of LLG at optimal sigmaA
    denom = flex.pow((1-flex.pow2(dobssel)*sigmaA**2),3)
    sum0 = flex.sum((2*flex.pow2(dobssel) * (1. - flex.pow2(abseE) - flex.pow2(absEc)))/denom)
    sum1 = flex.sum((12*flex.pow(dobssel,3)*abseE*absEc*cosdphi)/denom)
    sum2 = flex.sum((-6*flex.pow(dobssel,4)*(flex.pow2(abseE) + flex.pow2(absEc)))/denom)
    sum3 = flex.sum((4*flex.pow(dobssel,5)*abseE*absEc*cosdphi)/denom)
    sum4 = flex.sum((-2*flex.pow(dobssel,6))/denom)
    d2LLGbydsigmaA = sum0 + sum1*sigmaA + sum2*sigmaA**2 + sum3*sigmaA**3 + sum4*sigmaA**4
    d2LLGbydsigmaA /= over_sampling_factor
    sigma_sigmaA = 1./math.sqrt(abs(d2LLGbydsigmaA))
    ssqr = flex.mean_default(eEsel.d_star_sq().data(),0)
    ndat = dobssel.size()
    if verbosity > 1:
      print(ndat,ssqr,sigmaA,sigma_sigmaA,file=log)
    xdat.append(ssqr)
    ydat.append(math.log(sigmaA))
    wdat.append(abs(d2LLGbydsigmaA)) # Inverse variance estimate

  xdat = flex.double(xdat)
  ydat = flex.double(ydat)
  wdat = flex.double(wdat)
  W = flex.sum(wdat)
  Wx = flex.sum(wdat * xdat)
  Wy = flex.sum(wdat * ydat)
  Wxx = flex.sum(wdat * xdat * xdat)
  Wxy = flex.sum(wdat * xdat * ydat)
  slope = (W * Wxy - Wx * Wy) / (W * Wxx - Wx * Wx)
  intercept = (Wy * Wxx - Wx * Wxy) / (W * Wxx - Wx * Wx)
  frac_complete = math.exp(2.*intercept)
  if verbosity > 0:
    print("Slope and intercept of sigmaA plot for placed model contribution: ",slope,intercept,file=log)
    print("Approximate fraction of scattering: ",frac_complete,file=log)
  linlogsiga = flex.double()
  logsiga_combined = flex.double()
  sigma_linlog = math.log(1.5) # Allow 50% error in linear fit
  i_bin_used = 0
  for i_bin in expectE.binner().range_used():
    sigmaA = math.exp(ydat[i_bin_used])
    # For complete model, slope should be negative, but can be positive if the
    # modelled part is much better ordered than the larger unmodelled part
    linlog = min(math.log(0.999),(intercept + slope*xdat[i_bin_used]))
    linlogsiga.append(linlog)
    sigma_sigmaA = 1./math.sqrt(wdat[i_bin_used])
    sigma_lnsigmaA = math.exp(-ydat[i_bin_used])*sigma_sigmaA
    combined_logsiga = ((ydat[i_bin_used]/sigma_lnsigmaA**2 + linlog/sigma_linlog**2) /
                        (1./sigma_lnsigmaA**2 + 1./sigma_linlog**2) )
    logsiga_combined.append(combined_logsiga)
    combined_siga = math.exp(combined_logsiga)

    sel = expectE.binner().selection(i_bin)
    model_sigmaA.data().set_selected(sel, combined_siga)
    i_bin_used += 1

  if verbosity > 1:
    import matplotlib.pyplot as plt
    fig, ax = plt.subplots(2,1)
    ax[0].plot(xdat,ydat,label="ln(sigmaA)")
    ax[0].plot(xdat,linlogsiga,label="line fit")
    ax[0].plot(xdat,logsiga_combined,label="combined")
    ax[0].legend(loc="upper right")
    ax[1].plot(xdat,flex.exp(ydat),label="sigmaA")
    ax[1].plot(xdat,flex.exp(linlogsiga),label="line fit")
    ax[1].plot(xdat,flex.exp(logsiga_combined),label="combined")
    ax[1].legend(loc="lower left")
    plt.savefig("fixed_model_sigmaA_plot.png")

  return(group_args(model_sigmaA=model_sigmaA, Ecalc=Ecalc))

def assess_halfmap_errors(
          working_mmm, d_min, sphere_points=500, verbosity=1, log=sys.stdout):
  """
  Refine error parameters from half-maps, make weighted map coeffs for region.

  Compulsory arguments:
  working_mmm: map_model_manager object containing two half-maps from reconstruction
  d_min: target resolution, either best resolution for map or resolution for
    target region. Determined automatically if None.

  Optional arguments:
  sphere_points: number of reflections to use for local spherical average
  verbosity: 0/1/2/3/4 for mute/log/verbose/debug/testing
  log: destination for standard output
  """

  # Compute local averages needed for initial covariance terms
  # Do these calculations at extended resolution to avoid edge effects up to
  # desired resolution
  wuc = working_mmm.crystal_symmetry().unit_cell()
  unit_cell_grid = working_mmm.map_manager().unit_cell_grid
  ucpars = wuc.parameters()
  d_max = max(ucpars[0], ucpars[1], ucpars[2])
  work_mm = working_mmm.map_manager()
  v_star = 1./wuc.volume()
  r_star = math.pow(3*sphere_points*v_star/(4*math.pi),1./3.)
  spacings = get_grid_spacings(wuc,unit_cell_grid)

  # Check and potentially re-evaluate d_min
  guess_d_min = False
  minimum_possible_d_min = 2.*max(spacings)
  if d_min is None or d_min <= 0.:
    # Set first guess of d_min if no value provided
    # The majority of cryo-EM deposits have a ratio of nominal resolution to pixel
    # size between 2.5 and 3.5.
    guess_d_min = True
    d_min = 2.5 * max(spacings)
  elif d_min < minimum_possible_d_min:
    d_min = minimum_possible_d_min
    guess_d_min = True

  # Compute map coefficients from half-maps, then either proceed or evaluate
  # sensible d_min from different choices
  d_min_extended = 1./(1./d_min + r_star)
  mc1 = working_mmm.map_manager_1().map_as_fourier_coefficients(d_min=d_min_extended, d_max=d_max)
  mc2 = working_mmm.map_manager_2().map_as_fourier_coefficients(d_min=d_min_extended, d_max=d_max)

  while guess_d_min:
    # Check whether signal goes to higher resolution than initial guess
    d_min_from_fsc = mc1.d_min_from_fsc(other=mc2, fsc_cutoff=0.05).d_min
    # d_min_from_fsc returns None if the fsc_cutoff has not been passed by resolution limit
    if d_min_from_fsc is not None:
      d_min = max(d_min_from_fsc,minimum_possible_d_min)
      guess_d_min = False
    else:
      if d_min > minimum_possible_d_min: # Set d_min to highest possible value and repeat
        d_min = minimum_possible_d_min
      else:
        guess_d_min = False # d_min is already highest possible value
    d_min_extended = 1./(1./d_min + r_star)
    mc1 = working_mmm.map_manager_1().map_as_fourier_coefficients(d_min=d_min_extended, d_max=d_max)
    mc2 = working_mmm.map_manager_2().map_as_fourier_coefficients(d_min=d_min_extended, d_max=d_max)

  f1 = flex.abs(mc1.data())
  f2 = flex.abs(mc2.data())
  p1 = mc1.phases().data()
  p2 = mc2.phases().data()
  sumfsqr = mc1.customized_copy(data = flex.pow2(f1) + flex.pow2(f2))
  f1f2cos = mc1.customized_copy(data = f1 * f2 * flex.cos(p2 - p1))
  deltafsqr = mc1.customized_copy(data = flex.pow2(flex.abs(mc1.data()-mc2.data())))

  # Local mean calculations of covariance terms use data to extended resolution
  # but return results to desired resolution
  # Calculating sumfsqr_local_mean from f1f2cos and deltafsqr local means turns out to have
  # improved numerical stability, i.e. don't end up with negative or zero deltafsqr_local_mean
  b_sharpen = get_sharpening_b(sumfsqr)
  f1f2cos_local_mean = local_mean_intensities(work_mm, d_min, f1f2cos,
                                              r_star, b_sharpen)
  deltafsqr_local_mean = local_mean_intensities(work_mm, d_min, deltafsqr,
                                                r_star, b_sharpen)
  sumfsqr_local_mean = f1f2cos_local_mean.customized_copy(data = 2*f1f2cos_local_mean.data() + deltafsqr_local_mean.data())

  # Trim starting sumfsqr back to desired resolution limit, setup binning
  # NB: f1f2cos and deltafsqr aren't used any more
  mc1 = mc1.select(mc1.d_spacings().data() >= d_min)
  assert (mc1.size() == sumfsqr_local_mean.size())
  mc2 = mc2.select(mc2.d_spacings().data() >= d_min)
  sumfsqr = sumfsqr.select(sumfsqr.d_spacings().data() >= d_min)
  mc1.setup_binner_d_star_sq_bin_size()
  mc2.use_binner_of(mc1)
  sumfsqr.use_binner_of(mc1)
  sumfsqr_local_mean.use_binner_of(mc1)
  f1f2cos_local_mean.use_binner_of(mc1)

  # Prepare starting parameters for signal refinement
  mapCC = mc1.map_correlation(other=mc2)
  if mapCC >= 1.:
    raise Sorry("Half-maps must be non-identical")
  mapCC_bins = flex.double()
  ssqr_bins = flex.double()
  target_spectrum = flex.double()
  sumw = 0
  sumwx = 0.
  sumwy = 0.
  sumwx2 = 0.
  sumwxy = 0.
  nkept = 0
  for i_bin in sumfsqr.binner().range_used():
    sel = sumfsqr.binner().selection(i_bin)
    mc1sel = mc1.select(sel)
    mc2sel = mc2.select(sel)
    if mc1sel.size() == 0:
      continue
    mapCC = mc1sel.map_correlation(other=mc2sel)
    mapCC_bins.append(mapCC) # Store for before/after comparison
    sumfsqsel = sumfsqr.select(sel)
    meanfsq = flex.mean_default(sumfsqsel.data(),0.) / 2
    ssqr = mc1sel.d_star_sq().data()
    x = flex.mean_default(ssqr, 0) # Mean 1/d^2 for bin
    ssqr_bins.append(x)  # Save for later
    target_power = default_target_spectrum(x) # Could have a different target
    target_spectrum.append(target_power)
    if mapCC < 0.143 and nkept > 3: # Only fit initial signal estimate where clear overall
      continue
    signal = meanfsq * mapCC / target_power
    if signal <= 0.:
      continue
    y = math.log(signal)
    w = mc1sel.size()
    nkept += 1
    sumw += w
    sumwx += w * x
    sumwy += w * y
    sumwx2 += w * x**2
    sumwxy += w * x * y

  denominator = sumw * sumwx2 - sumwx**2
  if denominator == 0.:
    raise Sorry("Problem with fitting signal to mapCC curve")
  slope = (sumw * sumwxy - (sumwx * sumwy)) / denominator
  intercept = (sumwy - slope * sumwx) / sumw
  wilson_scale_signal = math.exp(intercept)
  wilson_b_signal = -4 * slope
  if verbosity > 0:
    print("\n  Wilson B for signal power: ", wilson_b_signal, file=log)
  n_bins = ssqr_bins.size()

  sigmaT_bins = [1.]*n_bins
  start_params = []
  start_params.append(wilson_scale_signal/3.5) # Asqr_scale, factor out low-res BEST value
  start_params.extend(sigmaT_bins)
  wilson_u=adptbx.b_as_u(wilson_b_signal)
  asqr_beta=list(adptbx.u_iso_as_beta(mc1.unit_cell(), wilson_u))
  start_params.extend(asqr_beta)

  # create inputs for the minimizer's run method
  # Constrained refinement using prior parameters could be revisited later.
  # However, this would require all cryo-EM maps to obey the assumption
  # that half-maps are completely unmasked.
  macro1 = ["default"]
  macro2 = ["default"]
  protocol = [macro1, macro2] # overall minimization protocol
  ncyc = 100                  # maximum number of microcycles per macrocycle
  minimizer_type = "bfgs"     # minimizer, bfgs or newton
  if verbosity < 4:
    study_params = False      # flag for calling studyparams procedure
  else:
    study_params = True
  level=verbosity      # 0/1/2/3/4 for mute/log/verbose/debug/testing

  # create instances of refine and minimizer
  refine_cryoem_signal = RefineCryoemSignal(
    sumfsqr_lm = sumfsqr_local_mean, f1f2cos_lm = f1f2cos_local_mean,
    deltafsqr_lm = deltafsqr_local_mean, r_star = r_star, ssqr_bins = ssqr_bins,
    target_spectrum = target_spectrum, start_x = start_params)
  minimizer = Minimizer(output_level=level)

  # Run minimizer
  minimizer.run(refine_cryoem_signal, protocol, ncyc, minimizer_type, study_params)
  refined_params=refine_cryoem_signal.x

  # Extract and report refined parameters
  i_par = 0
  asqr_scale = refined_params[i_par] # Not used for correction: leave map on original scale
  i_par += 1
  sigmaT_bins = refined_params[i_par:i_par + n_bins]
  i_par += n_bins
  asqr_beta = tuple(refined_params[i_par:i_par + 6])
  i_par += 6
  assert (i_par == len(refined_params))

  # Convert asqr_beta to a_beta for reporting
  a_beta = tuple(flex.double(asqr_beta)/2)

  # Convert beta parameters to Baniso for (optional) use and output
  a_baniso = adptbx.u_as_b(adptbx.beta_as_u_cart(mc1.unit_cell(), a_beta))

  # For debugging and data inspection, set make_intermediate_files true, but false normally
  make_intermediate_files = False

  if verbosity > 0:
    print("\nRefinement of scales and error terms completed\n", file=log)
    print("\nParameters for A and BEST curve correction", file=log)
    print("  A overall scale: ",math.sqrt(asqr_scale), file=log)
    for i_bin in range(n_bins):
      print("  Bin #", i_bin + 1, "BEST curve correction: ", sigmaT_bins[i_bin], file=log)
    print("  A tensor as beta:", a_beta, file=log)
    print("  A tensor as Baniso: ", a_baniso, file=log)
    es = adptbx.eigensystem(a_baniso)
    print("  Eigenvalues and eigenvectors:", file=log)
    for iv in range(3):
      print("  ",es.values()[iv],es.vectors(iv), file=log)

    sys.stdout.flush()

  # Loop over bins to compute expectedE and Dobs for each Fourier term
  # Start with mean of half-map Fourier terms and make Miller array for Dobs
  expectE = mc1.customized_copy(data = (mc1.data() + mc2.data())/2)
  expectE.use_binner_of(mc1)
  dobs = expectE.customized_copy(data=flex.double(expectE.size(),0))
  if make_intermediate_files:
    sigmaS = expectE.customized_copy(data=flex.double(expectE.size(),0))
    sigmaE = expectE.customized_copy(data=flex.double(expectE.size(),0))
  i_bin_used = 0 # Keep track in case full range of bins not used
  if verbosity > 0:
    print("MapCC before and after rescaling as a function of resolution", file=log)
    print("Bin   <ssqr>   Nterms   mapCC_before   mapCC_after", file=log)
  for i_bin in mc1.binner().range_used():
    sel = expectE.binner().selection(i_bin)
    eEsel = expectE.select(sel)

    # Make Miller array as basis for computing aniso corrections in this bin
    ones_array = flex.double(eEsel.size(), 1)
    all_ones = eEsel.customized_copy(data=ones_array)
    beta_miller_Asqr = all_ones.apply_debye_waller_factors(
      u_star=adptbx.beta_as_u_star(asqr_beta))
    u_terms = (asqr_scale * sigmaT_bins[i_bin_used]
      * target_spectrum[i_bin_used]) * beta_miller_Asqr.data()

    # Noise is half of deltafsqr power
    sigmaE_terms = (deltafsqr_local_mean.data().select(sel)) / 2.
    f1f2cos_terms = f1f2cos_local_mean.data().select(sel)
    sumfsqr_terms = sumfsqr_local_mean.data().select(sel)

    # Compute the relative weight between the local statistics and the overall
    # anisotropic signal model, as in refinement code.
    steep = 9.
    local_weight = 0.95
    fsc = f1f2cos_terms/(sumfsqr_terms/2.)
    # Make sure fsc is in range 0 to 1
    fsc = (fsc + flex.abs(fsc)) / 2
    fsc = (fsc + 1 - flex.abs(fsc - 1)) / 2
    wt_terms = flex.exp(steep*fsc)
    wt_terms = 1. - local_weight * (wt_terms/(math.exp(steep/2) + wt_terms))
    sigmaS_terms = wt_terms*u_terms + (1.-wt_terms)*f1f2cos_terms
    # Make sure sigmaS is non-negative
    sigmaS_terms = (sigmaS_terms + flex.abs(sigmaS_terms))/2
    scale_terms = 1./flex.sqrt(sigmaS_terms + sigmaE_terms/2.)
    # Arrange Dobs calculation so sigmaS can be zero
    dobs_terms = flex.sqrt(sigmaS_terms / (sigmaS_terms + sigmaE_terms/2.))
    expectE.data().set_selected(sel, expectE.data().select(sel) * scale_terms)
    dobs.data().set_selected(sel, dobs_terms)

    if make_intermediate_files:
      sigmaS.data().set_selected(sel, sigmaS_terms)
      sigmaE.data().set_selected(sel, sigmaE_terms)

    # Apply corrections to mc1 and mc2 to compute mapCC after rescaling
    # SigmaE variance is twice as large for half-maps before averaging
    scale_terms_12 = 1./flex.sqrt(sigmaS_terms + sigmaE_terms)
    # Overwrite mc1 and mc2, but not needed later.
    mc1.data().set_selected(sel, mc1.data().select(sel) * scale_terms_12)
    mc2.data().set_selected(sel, mc2.data().select(sel) * scale_terms_12)
    mc1sel = mc1.select(sel)
    mc2sel = mc2.select(sel)
    mapCC = mc1sel.map_correlation(other=mc2sel)
    if verbosity > 0:
      print(i_bin_used+1, ssqr_bins[i_bin_used], mc1sel.size(), mapCC_bins[i_bin_used], mapCC, file=log)
    mapCC_bins[i_bin_used] = mapCC # Update for returned output
    i_bin_used += 1

  # Compensate for numerical instability when sigmaS is extremely small, in which
  # case dobs is very small and expectE can be very large
  sel = dobs.data() < 0.00001
  expectE.data().set_selected(sel,1.)

  if make_intermediate_files: # For debugging and investigating signal/noise
    # Write out sigmaS, sigmaE and Dobs both as mtz files and intensities-as-maps

    write_mtz(sigmaS,"sigmaS.mtz","sigmaS")
    results = intensities_as_expanded_map(work_mm,sigmaS)
    coeffs_as_map = results.mm_data # map_manager with intensities
    coeffs_as_map.write_map("sigmaS.map")

    write_mtz(sigmaE,"sigmaE.mtz","sigmaE")
    results = intensities_as_expanded_map(work_mm,sigmaE)
    coeffs_as_map = results.mm_data # map_manager with intensities
    coeffs_as_map.write_map("sigmaE.map")

    write_mtz(dobs,"Dobs.mtz","Dobs")
    results = intensities_as_expanded_map(work_mm,dobs)
    coeffs_as_map = results.mm_data # map_manager with intensities
    coeffs_as_map.write_map("Dobs.map")

  return group_args(expectE=expectE, dobs=dobs)

def map_errors_from_d_min(working_mmm, d_min, verbosity=1, log=sys.stdout):
  """
  Use generic relationship between d_min and shape of FSC curve to compute
  radially-symmetry Dobs and Emean values from a single full map.
  Make weighted map coeffs for region.

  Compulsory arguments:
  working_mmm: map_model_manager object containing single map from reconstruction
  d_min: target resolution, either best resolution for map or resolution for
    target region. Legal value must be provided

  Optional arguments:
  verbosity: 0/1/2/3/4 for mute/log/verbose/debug/testing
  log: destination for standard output
  """

  wuc = working_mmm.crystal_symmetry().unit_cell()
  ucpars = wuc.parameters()
  d_max = max(ucpars[0], ucpars[1], ucpars[2])
  map_coeffs = working_mmm.map_as_fourier_coefficients(d_min=d_min, d_max=d_max)
  e_array = map_coeffs.amplitudes()
  e_array.setup_binner(auto_binning = True)
  e_array = e_array.quasi_normalize_structure_factors()
  expectE = e_array.phase_transfer(phase_source = map_coeffs.phases(deg=False), deg = False)
  # The signal_ratio and deltaB are computed from a database investigation of nearly
  # 500 EMDB entries, to match the FSC curves associated with different limits
  signal_ratio_0 = 34.468*math.exp(3.00533/d_min + 4.27895/d_min**2)
  deltaB = 21.3225*d_min**2 + 12.0213*d_min+17.1158
  signal_ratio = expectE.customized_copy(data=flex.double(expectE.size(), signal_ratio_0))
  signal_ratio = signal_ratio.apply_debye_waller_factors(b_iso=deltaB)
  dobs = expectE.customized_copy(data=signal_ratio.data()/(1.+signal_ratio.data())) # FSC
  dobs = dobs.customized_copy(data=flex.sqrt(2*dobs.data()/(1.+dobs.data()))) # FSC to Dobs
  return group_args(expectE=expectE, dobs=dobs)

def assess_cryoem_errors(
        mmm=None,
        d_min=None,
        half_maps_provided=False,
        determine_ordered_volume=True,
        sphere_cent=None,
        radius=None,
        double_map_box=False,
        sphere_points=500,
        shift_map_origin=True,
        ordered_mask_id=None,
        fixed_mask_id=None,
        verbosity=1,
        keep_full_map=False,
        log=sys.stdout):
  """
  Determine error parameters from half-maps (preferred) or full map, and make
  weighted map coeffs for region.

  Compulsory arguments:
  mmm: map_model_manager object containing map or half-maps from reconstruction
    and optionally fixed model plus fixed model mask
  d_min: target resolution, either best resolution for map or resolution for
    target region

  Optional arguments:
  determine_ordered_volume: flag for whether ordered volume should be assessed
    (only applicable for half-maps)
  ordered_mask_id: identifier for mask defining ordered volume
  sphere_cent: center of sphere defining target region for analysis
    default is center of map
  radius: radius of sphere
    default (when sphere center not defined either) is 1/3 narrowest map width
  sphere_points: number of reflections to use for local spherical average
  shift_map_origin: should map coefficients be shifted to correspond to
    original origin, rather than the origin being the corner of the box,
    default True
  define_ordered_volume: should ordered volume over full map be evaluated, to
    compare with cutout volume,
    default True
  keep_full_map: don't mask or box the input map
    default False, use with caution
  verbosity: 0/1/2/3/4 for mute/log/verbose/debug/testing
  """

  if verbosity > 0:
    print("\nPrepare map for docking by assessing signal and errors", file=log)

  # Start from two half-maps and ordered volume mask in map_model_manager
  # Determine ordered volume in whole reconstruction and fraction that will be in sphere
  unit_cell = mmm.map_manager().unit_cell()
  unit_cell_grid = mmm.map_manager().unit_cell_grid
  map_grid = mmm.map_manager().map_data().all()
  if (not mmm.map_manager().is_full_size()):
    print("\nERROR: Unit cell grid does not match map dimensions:")
    print("Unit cell grid: ", unit_cell_grid)
    print("Map dimensions: ", map_grid)
    raise Sorry("Provided maps must be full-size")
  spacings = get_grid_spacings(unit_cell,unit_cell_grid)
  ucpars = unit_cell.parameters()
  # Determine voxel count for fixed model mask
  mm_model_mask = mmm.get_map_manager_by_id(fixed_mask_id)
  voxels_in_fixed_model = 0
  if mm_model_mask is not None:
    model_mask_data = mm_model_mask.map_data()
    mask_sel = model_mask_data > 0
    voxels_in_fixed_model = mask_sel.count(True)

  # Keep track of shifted origin
  origin_shift = mmm.map_manager().origin_shift_grid_units
  if flex.sum(flex.abs(flex.double(origin_shift))) > 0:
    shifted_origin = True
  else:
    shifted_origin = False
  if sphere_cent is None:
    # Default to sphere in center of cell extending 2/3 distance to nearest edge
    # sphere_cent_grid and sphere_cent_map are relative to map origin
    sphere_cent_grid = [round(n/2) for n in unit_cell_grid]
    sphere_cent = [(g * s) for g,s in zip(sphere_cent_grid, spacings)]
    if radius is None:
      radius = min(ucpars[0], ucpars[1], ucpars[2])/3.
  else:
    if radius is None:
      raise Sorry("Radius must be provided if sphere_cent is defined")
    # Force sphere center to be on a map grid point for easier comparison of different spheres
    # Account for origin shift if present in input maps
    sphere_cent_frac = unit_cell.fractionalize(sphere_cent)
    sphere_cent_grid = [round(n * f) for n,f in zip(unit_cell_grid, sphere_cent_frac)]
    if shifted_origin:
      sphere_cent_grid = [(c - o) for c,o in zip(sphere_cent_grid, origin_shift)]
  for i in range(3):
    if sphere_cent_grid[i] > map_grid[i] or sphere_cent_grid[i] < 0:
      raise Sorry("Sphere centre is outside map grid")
  sphere_cent_map = [(g * s) for g,s in zip(sphere_cent_grid, spacings)]
  sphere_cent_map = flex.double(sphere_cent_map)

  if determine_ordered_volume:
    ordered_mm = mmm.get_map_manager_by_id(map_id=ordered_mask_id)
    total_ordered_volume = flex.mean(ordered_mm.map_data()) * unit_cell.volume()
    ordered_volume_in_sphere = get_ordered_volume_exact(ordered_mm, sphere_cent_map, radius)
    fraction_scattering = ordered_volume_in_sphere / total_ordered_volume
  else:
    fraction_scattering = None

  # Get map coefficients for maps after spherical masking
  # Define box big enough to hold sphere plus soft masking
  # Warn if sphere too near edge of full map for soft mask to reach zero
  # or if less than about 85% of sphere would be within full map
  # (outside by >= half-radius)
  boundary_to_smoothing_ratio = 2
  # d_min may be waiting to be evaluated, if there are half-maps
  sensible_d_min = 2.5*max(spacings) # Lower end of typical range from EMDB
  minimum_possible_d_min = 2.*max(spacings)
  if d_min is None or d_min < minimum_possible_d_min:
    if not half_maps_provided:
      print("Given map spacing, minimum possible value for d_min is ",
            minimum_possible_d_min)
      raise Sorry("Sensible value for d_min must be given for single map")
    soft_mask_radius = sensible_d_min
  else:
    soft_mask_radius = d_min
  padding = soft_mask_radius * boundary_to_smoothing_ratio
  cushion = flex.double(3,radius+padding)
  cart_min = flex.double(sphere_cent_map) - cushion
  cart_max = flex.double(sphere_cent_map) + cushion
  max_outside = 0.
  for i in range(3):
    if cart_min[i] < 0:
      max_outside = max(max_outside, -cart_min[i])
    if cart_max[i] > ucpars[i]-spacings[i]:
      max_outside = max(max_outside, cart_max[i]-(ucpars[i]-spacings[i]))
  if max_outside > padding + radius/2:
    print("\nWARNING: substantial fraction of sphere is outside map volume", file=log)
  elif max_outside > padding:
    print("\nWARNING: sphere is partially outside map volume", file=log)
  elif max_outside > 0:
    print("\nWARNING: sphere too near map edge to allow full extent of smooth masking", file=log)

  cs = mmm.crystal_symmetry()
  uc = cs.unit_cell()

  # Set some parameters that will be overwritten if masked and cut out
  over_sampling_factor = 1.
  box_volume = uc.volume()

  if keep_full_map: # Rearrange later so this choice comes first?
    working_mmm = mmm.deep_copy()
  else:
    # Box the map within xyz bounds, converted to map grid units
    lower_frac = uc.fractionalize(tuple(cart_min))
    upper_frac = uc.fractionalize(tuple(cart_max))
    all_orig = mmm.map_data().all()
    lower_bounds = [int(math.floor(f * n)) for f, n in zip(lower_frac, all_orig)]
    upper_bounds = [int(math.ceil( f * n)) for f, n in zip(upper_frac, all_orig)]
    working_mmm = mmm.extract_all_maps_with_bounds(
        lower_bounds=lower_bounds, upper_bounds=upper_bounds)
    # Make and apply spherical mask
    working_mmm.create_spherical_mask(
      soft_mask_radius=soft_mask_radius,
      boundary_to_smoothing_ratio=boundary_to_smoothing_ratio)
    working_mmm.apply_mask_to_maps()
    if double_map_box:
      delta_cushion = 2*radius - cushion[0]
      delta_cushion_grid = int(math.ceil(delta_cushion/spacings[0]))
      # Note that extract_all_maps_with_bounds works in terms of the lower
      # and upper ranges of the map being operated on. Here we are padding the
      # map out with zeroes on all sides.
      new_lower_bounds = tuple(-flex.int(3,delta_cushion_grid))
      new_upper_bounds = tuple(flex.int(upper_bounds) - flex.int(lower_bounds) + flex.int(3,delta_cushion_grid))
      working_mmm = working_mmm.extract_all_maps_with_bounds(
          lower_bounds=new_lower_bounds, upper_bounds=new_upper_bounds,
          stay_inside_current_map = False)
    mask_info = working_mmm.mask_info()

    working_map_size = working_mmm.map_data().size()
    box_volume = uc.volume() * (
        working_map_size/mmm.map_data().size())

    voxels_in_masked_model = 0
    if voxels_in_fixed_model > 0:
      mask_info = working_mmm.mask_info(mask_id=fixed_mask_id)
      mm_model_mask = working_mmm.get_map_manager_by_id(fixed_mask_id)
      model_mask_data = mm_model_mask.map_data()
      voxels_in_masked_model = working_map_size * mask_info.mean

    # Calculate an oversampling ratio defined as the ratio between the size of
    # the cut-out cube and the size of a cube that could contain a sphere
    # big enough to hold the volume of ordered density. Because the ratio of
    # the size of a cube to the sphere inscribed in it is about 1.9 (which is
    # close to the factor of two between typical protein volume and unit cell
    # volume in a crystal), this should yield likelihood scores on a similar
    # scale to crystallographic ones.
    if determine_ordered_volume:
      assert(ordered_volume_in_sphere > 0.)
      over_sampling_factor = box_volume / (ordered_volume_in_sphere * 6./math.pi)
    else:
      over_sampling_factor = 4. # Guess!

  if verbosity > 0:
    if determine_ordered_volume:
      print("Fraction of full map scattering: ",fraction_scattering, file=log)
    print("Over-sampling factor: ",over_sampling_factor, file=log)
    sys.stdout.flush()

  if half_maps_provided:
    results = assess_halfmap_errors(working_mmm, d_min,
                                    sphere_points=sphere_points,
                                    verbosity=verbosity,
                                    log=log)
  else:
    results = map_errors_from_d_min(working_mmm, d_min,
                                    verbosity=verbosity,
                                    log=log)
  expectE = results.expectE
  dobs = results.dobs
  # d_min may have been evaluated from half-maps
  eps = 1.E-6 # Add a little slop for rare rounding errors leading to mismatch of limits
  d_min = flex.min(expectE.d_spacings().data()) - eps
  d_max = flex.max(expectE.d_spacings().data()) + eps

  # If there is a fixed model and it occupies a significant part of the sphere,
  # compute the structure factors corresponding to the cutout density for the fixed
  # model.

  masked_model_contributes = False
  masked_model_E = None
  masked_model_sigmaA = None
  fraction_masked = voxels_in_masked_model/working_map_size
  if fraction_masked > 0.01:
    if verbosity > 1:
      print("Masked voxels from fixed model, total in box, fraction: ",
            voxels_in_masked_model, working_map_size, fraction_masked, file=log)
    masked_model_contributes = True
    mm_masked_model = working_mmm.get_map_manager_by_id(map_id='fixed_atom_map')
    # mm_masked_model.write_map('masked_model.map')
    masked_model_F = mm_masked_model.map_as_fourier_coefficients(d_min=d_min,d_max=d_max)

  if masked_model_contributes:
    results = sigmaA_from_model_in_map(expectE, dobs, masked_model_F, over_sampling_factor, verbosity)
    masked_model_sigmaA = results.model_sigmaA
    masked_model_E = results.Ecalc
    # Possibly temporary choice: modify expectE instead of adding fixed contribution in phasertng
    expectE_mod = expectE.customized_copy(data=expectE.data()
                  - masked_model_sigmaA.data()*masked_model_E.data())
  else:
    expectE_mod = expectE.deep_copy()

  # Return map_model_managers with weighted maps
  wEmean = dobs*expectE_mod
  working_mmm.add_map_from_fourier_coefficients(map_coeffs=wEmean, map_id='map_manager_wtd')
  sigmaA = 0.9 # Default for likelihood-weighted map
  dosa = sigmaA*dobs.data()
  lweight = dobs.customized_copy(data=(2*dosa)/(1.-flex.pow2(dosa)))
  wEmean2 = lweight*expectE_mod
  working_mmm.add_map_from_fourier_coefficients(map_coeffs=wEmean2, map_id='map_manager_lwtd')
  if half_maps_provided:
    working_mmm.remove_map_manager_by_id('map_manager_1')
    working_mmm.remove_map_manager_by_id('map_manager_2')

  shift_cart = working_mmm.shift_cart()
  if shift_map_origin:
    ucwork = expectE.crystal_symmetry().unit_cell()
    # shift_cart is position of original origin in boxed-map coordinate system
    # shift_frac should correspond to what has to be done to a model to put it
    # into the map, i.e. move it in the opposite direction
    shift_frac = ucwork.fractionalize(shift_cart)
    shift_frac = tuple(-flex.double(shift_frac))
    expectE = expectE.translational_shift(shift_frac)
    expectE_mod = expectE_mod.translational_shift(shift_frac)
    if masked_model_contributes:
      masked_model_E = masked_model_E.translational_shift(shift_frac)
      # write_mtz(expectE,"expectE.mtz","eE")
      # write_mtz(expectE_mod,"expectE_mod.mtz","expectEmod")
      # write_mtz(masked_model_E,"masked_model.mtz","masked_model")

  return group_args(
    new_mmm = working_mmm,
    shift_cart = shift_cart,
    sphere_center = list(sphere_cent_map),
    expectE = expectE_mod, dobs = dobs, # Temporary kludge
    # expectE = expectE, dobs = dobs,
    masked_model_E = masked_model_E, masked_model_sigmaA = masked_model_sigmaA,
    over_sampling_factor = over_sampling_factor,
    fraction_scattering = fraction_scattering)

# Command-line interface using argparse
def run():
  """
  Prepare cryo-EM map for docking by preparing weighted MTZ file.

  Command-line arguments (keyworded):
  Either two half-maps or a single full map must be provided
    --map:   name of file containing the full final reconstructed map
    --map1: name of file containing the first half-map from a reconstruction
    --map2: name of file containing the second half-map

    --d_min: desired resolution, either best for whole map or for local region

  Content of full reconstruction should be provided and is compulsory if the sphere
  parameters are not defined, either explicitly or in the form of a model
    --protein_mw: molecular weight expected for protein component of ordered density
          in full map
    --nucleic_mw: same for nucleic acid component

  Optional parameters specifying centre and radius of cut-out sphere
    --model: PDB file for placed model used to define the centre and radius of
          the cut-out sphere, used as an alternative to sphere_cent and radius
    --sphere_cent: Centre of sphere defining target map region (3 floats)
          defaults to centre of map
    --radius: radius of sphere (1 float)
          must be given if sphere_cent defined, otherwise
          defaults to either radius enclosing 98% of ordered volume (if determined)
          or narrowest extent of input map divided by 4

  Option to create output map box twice as wide as sphere, oversampling the
  transform of the sphere so that only alternate hkl terms are needed in likelihood
    --double_map_box: make box with double diameter of sphere
          default False

  Optional fixed model, contribution of which is removed from output data
    --fixed_model: PDB file for fixed background model

  Optional parameters with sensible defaults
    --sphere_points: number of reflections to be used for averaging in Fourier space
          default 500
    --no_shift_map_origin: don't shift output mtz file to match input map on its origin
          default False
    --no_determine_ordered_volume: don't determine ordered volume
          default False
    --file_root: root name for output files
    --write_maps: write sharpened and likelihood-weighted maps
  Control level of output. Default is logfile level of output
    --mute (or -m): mute output
    --verbose (or -v): verbose output
    --testing: extra verbose output for debugging
  """
  import argparse
  dm = DataManager()
  dm.set_overwrite(True)

  parser = argparse.ArgumentParser(
          description='Prepare cryo-EM map for docking')
  parser.add_argument('--map',
                      help='Map file for final reconstruction, instead of half-maps')
  parser.add_argument('--map1', help='Map file for half-map 1')
  parser.add_argument('--map2', help='Map file for half-map 2')
  parser.add_argument('--d_min', help='d_min for maps', type=float)
  parser.add_argument('--protein_mw',
                      help='Molecular weight of protein component of full map',
                      type=float)
  parser.add_argument('--nucleic_mw',
                      help='Molecular weight of nucleic acid component of full map',
                      type=float)
  parser.add_argument('--model',
                      help='''Optional roughly-placed model defining size and
                              position of cut-out sphere for docking.
                              Orientation of model is ignored.''')
  parser.add_argument('--sphere_cent',
                      help='Centre of sphere for docking, if no model provided',
                      nargs=3, type=float)
  parser.add_argument('--radius',
                      help='Radius of sphere for docking, if no model provided',
                      type=float)
  parser.add_argument('--double_map_box',
                      help='Make map box twice as wide as sphere',
                      action = 'store_true')
  parser.add_argument('--fixed_model',
                      help='Optional fixed model accounting for explained map features')
  parser.add_argument('--sphere_points',help='Target nrefs in averaging sphere',
                      type=float, default=500.)
  parser.add_argument('--no_shift_map_origin', action='store_true')
  parser.add_argument('--no_determine_ordered_volume', action='store_true')
  parser.add_argument('--file_root',
                      help='Root of filenames for output')
  parser.add_argument('--write_maps', help = 'Write sharpened and likelihood-weighted maps',
                      action = 'store_true')
  parser.add_argument('-m', '--mute', help = 'Mute output', action = 'store_true')
  parser.add_argument('-v', '--verbose', help = 'Set output as verbose',
                      action = 'store_true')
  parser.add_argument('--testing', help='Set output as testing level', action='store_true')
  args = parser.parse_args()
  d_min = args.d_min
  verbosity = 1
  if args.mute: verbosity = 0
  if args.verbose: verbosity = 2
  if args.testing: verbosity = 4
  shift_map_origin = not(args.no_shift_map_origin)
  determine_ordered_volume = not(args.no_determine_ordered_volume)

  sphere_points = args.sphere_points

  cutout_specified = False
  sphere_cent = None
  radius = None
  double_map_box = args.double_map_box

  model = None
  if args.model is not None:
    model_file = args.model
    model = dm.get_model(model_file)
    if args.sphere_cent is not None:
      raise Sorry('Either model or sphere specification may be given but not both')
    sphere_cent, radius = sphere_enclosing_model(model)
    radius = radius + d_min # Expand to allow width for density
    cutout_specified = True

  fixed_model = None
  if args.fixed_model is not None:
    model_file = args.fixed_model
    fixed_model = dm.get_model(model_file)

  if args.sphere_cent is not None:
    if args.radius is None:
      raise Sorry("Radius must be provided if sphere_cent is defined")
    sphere_cent = tuple(args.sphere_cent)
    radius = args.radius
    cutout_specified = True
  if (not determine_ordered_volume and not cutout_specified):
    raise Sorry("Must determine ordered volume if cutout not specified")

  protein_mw = None
  nucleic_mw = None
  if (args.protein_mw is None) and (args.nucleic_mw is None):
    if cutout_specified:
      determine_ordered_volume = False
    if determine_ordered_volume and not cutout_specified:
      raise Sorry("At least one of protein_mw or nucleic_mw must be given")
  if args.protein_mw is not None:
    protein_mw = args.protein_mw
  if args.nucleic_mw is not None:
    nucleic_mw = args.nucleic_mw

  # Create map_model_manager containing half-maps, or full map if no half-maps
  half_maps_provided = True
  if args.map1 is not None:
    if args.map2 is None:
      raise Sorry("Half-maps must be provided in a pair")
    map1_filename = args.map1
    mm1 = dm.get_real_map(map1_filename)
    map2_filename = args.map2
    mm2 = dm.get_real_map(map2_filename)
    mmm = map_model_manager(map_manager_1=mm1, map_manager_2=mm2)
  else:
    if args.map is None:
      raise Sorry("Either half-maps or full map must be provided")
    half_maps_provided = False
    map_filename = args.map
    mm = dm.get_real_map(map_filename)
    mmm = map_model_manager(map_manager=mm)

  if (fixed_model):
    mmm.add_model_by_id(model=fixed_model, model_id='fixed_model')
    mmm.generate_map(model=fixed_model, d_min=d_min, map_id='fixed_atom_map')

  ordered_mask_id = None
  if determine_ordered_volume:
    ordered_mask_id = 'ordered_volume_mask'
    add_ordered_volume_mask(mmm, d_min,
        protein_mw=protein_mw, nucleic_mw=nucleic_mw,
        ordered_mask_id=ordered_mask_id)
    if verbosity>1:
      ordered_mm = mmm.get_map_manager_by_id(map_id=ordered_mask_id)
      if args.file_root is not None:
        map_file_name = args.file_root + "_ordered_volume_mask.map"
      else:
        map_file_name = "ordered_volume_mask.map"
      ordered_mm.write_map(map_file_name)

  fixed_mask_id = None
  if fixed_model:
    fixed_mask_id = 'mask_around_atoms'
    mask_fixed_model_region(mmm, d_min,
                            fixed_model=fixed_model,
                            ordered_mask_id=ordered_mask_id,
                            fixed_mask_id=fixed_mask_id)

  # Default to sphere containing most of ordered volume
  if not cutout_specified:
    ucpars = mm1.unit_cell().parameters()
    sphere_cent = (ucpars[0]/2,ucpars[1]/2,ucpars[2]/2)
    if determine_ordered_volume:
      target_completeness = 0.98
      radius = get_mask_radius(mmm.get_map_manager_by_id(ordered_mask_id),target_completeness)
    else:
      radius = min(ucpars[0],ucpars[1],ucpars[2])/4.

  # Refine to get scale and error parameters for docking region
  results = assess_cryoem_errors(
                        mmm=mmm,
                        d_min=d_min,
                        half_maps_provided=half_maps_provided,
                        determine_ordered_volume=determine_ordered_volume,
                        sphere_cent=sphere_cent,
                        radius=radius,
                        double_map_box=double_map_box,
                        sphere_points=sphere_points,
                        shift_map_origin=shift_map_origin,
                        ordered_mask_id=ordered_mask_id,
                        fixed_mask_id=fixed_mask_id,
                        verbosity=verbosity)

  expectE = results.expectE
  mtz_dataset = expectE.as_mtz_dataset(column_root_label='Emean')
  dobs = results.dobs
  mtz_dataset.add_miller_array(
      dobs,column_root_label='Dobs',column_types='W')
  mtz_object=mtz_dataset.mtz_object()

  if args.file_root is not None:
    mtzout_file_name = args.file_root + ".mtz"
    mapout1_file_name = args.file_root + "_weighted.map"
    mapout2_file_name = args.file_root + "_likelihood_weighted.map"
  else:
    mtzout_file_name = "weighted_map_data.mtz"
    mapout1_file_name = "weighted.map"
    mapout2_file_name = "likelihood_weighted.map"
  print ("Writing mtz for docking as", mtzout_file_name)
  if not shift_map_origin:
    shift_cart = results.shift_cart
    print ("Origin of full map relative to mtz:", shift_cart)
  dm.write_miller_array_file(mtz_object, filename=mtzout_file_name)
  if args.write_maps:
    print ("Writing weighted map as", mapout1_file_name)
    print ("Writing likelihood-weighted map as", mapout2_file_name)
    new_mmm = results.new_mmm
    new_mmm.write_map(map_id='map_manager_wtd', file_name = mapout1_file_name)
    new_mmm.write_map(map_id='map_manager_lwtd', file_name = mapout2_file_name)
  over_sampling_factor = results.over_sampling_factor
  fraction_scattering = results.fraction_scattering
  print ("Over-sampling factor for Fourier terms:",over_sampling_factor)
  print ("Fraction of total scattering:",fraction_scattering)
  sys.stdout.flush()

if __name__ == "__main__":
  run()


 *******************************************************************************


 *******************************************************************************
cctbx/maptbx/prepare_map_for_refinement.py
from __future__ import absolute_import, division, print_function
from iotbx.map_model_manager import map_model_manager
from iotbx.data_manager import DataManager
from cctbx.maptbx.prepare_map_for_docking import mask_fixed_model_region
from cctbx.maptbx.prepare_map_for_docking import assess_cryoem_errors
from cctbx.maptbx.prepare_map_for_docking import sphere_enclosing_model
from libtbx.utils import Sorry
dm = DataManager()
dm.set_overwrite(True)

def prepare_map_for_refinement(map_filename, map1_filename, map2_filename, d_min,
            working_model_filename, fixed_model_filename=None, verbosity=1):

  working_model = dm.get_model(working_model_filename)
  sphere_cent, radius = sphere_enclosing_model(working_model)
  if d_min is not None:
    radius = radius + d_min # Expand to allow width for density
  else:
    radius = radius + 3.0

  # Create map_model_manager containing half-maps, or full map if no half-maps
  half_maps_provided = True
  if map1_filename is not None:
    if map2_filename is None:
      raise Sorry("Half-maps must be provided in a pair")
    mm1 = dm.get_real_map(map1_filename)
    mm2 = dm.get_real_map(map2_filename)
    mmm = map_model_manager(map_manager_1=mm1, map_manager_2=mm2)
  else:
    if map_filename is None:
      raise Sorry("Either half-maps or full map must be provided")
    half_maps_provided = False
    mm = dm.get_real_map(map_filename)
    mmm = map_model_manager(map_manager=mm)

  if fixed_model_filename is not None:
    fixed_model = dm.get_model(fixed_model_filename)
    mmm.add_model_by_id(model=fixed_model, model_id='fixed_model')
    mmm.generate_map(model=fixed_model, d_min=d_min, map_id='fixed_atom_map')

  ordered_mask_id = None
  # ordered_mask_id = 'ordered_volume_mask'
  # add_ordered_volume_mask(mmm, d_min,
  #     protein_mw=protein_mw, nucleic_mw=nucleic_mw,
  #     ordered_mask_id=ordered_mask_id)

  fixed_mask_id = None
  if fixed_model_filename is not None:
    fixed_mask_id = 'mask_around_atoms'
    mask_fixed_model_region(mmm, d_min,
                            fixed_model=fixed_model,
                            ordered_mask_id=ordered_mask_id,
                            fixed_mask_id=fixed_mask_id)

  # Refine to get scale and error parameters for docking region
  results = assess_cryoem_errors(
                            mmm=mmm,
                            d_min=d_min,
                            half_maps_provided=half_maps_provided,
                            determine_ordered_volume=False,
                            sphere_cent=sphere_cent,
                            radius=radius,
                            double_map_box=True,
                            ordered_mask_id=ordered_mask_id,
                            fixed_mask_id=fixed_mask_id,
                            verbosity=verbosity)

  return results

def run():
  """
  Prepare cryo-EM map for refinement of portion in context by preparing weighted MTZ file.

  Compulsory command-line arguments (keyworded):
  Either two half-maps or a single full map must be provided
    --map:   name of file containing the full final reconstructed map
    --map_1: name of file containing the first half-map from a reconstruction
    --map_2: name of file containing the second half-map

    --d_min: local resolution of map around working model

    --working_model: Model file for placed working model that will be refined

  Optional command-line arguments:
    --fixed_model: Model file for fixed model that partly explains rest of map
    --file_root: root name for output files
    --mute (or -m): mute output
    --verbose (or -v): verbose output
    --testing: extra verbose output for debugging
  """
  import argparse
  parser = argparse.ArgumentParser(
          description='Prepare cryo-EM map for refinement')
  parser.add_argument('--map',
                      help='Map file for final reconstruction, instead of half-maps')
  parser.add_argument('--map1', help='Map file for half-map 1')
  parser.add_argument('--map2', help='Map file for half-map 2')
  parser.add_argument('--d_min', help='local resolution', type=float)
  parser.add_argument('--working_model',
                      help='Model for portion that will be refined')
  parser.add_argument('--fixed_model',
                      help='Optional fixed model accounting for explained map features')
  parser.add_argument('--file_root',
                      help='Root of filenames for output')
  parser.add_argument('-m', '--mute', help = 'Mute output', action = 'store_true')
  parser.add_argument('-v', '--verbose', help = 'Set output as verbose',
                      action = 'store_true')
  parser.add_argument('--testing', help='Set output as testing level', action='store_true')

  args = parser.parse_args()
  map_filename  = args.map
  map1_filename = args.map1
  map2_filename = args.map2
  d_min = args.d_min

  # Verbosity levels: 0=mute, logfile=1, verbose=2, testing=4
  verbosity = 1
  if args.mute: verbosity = 0
  if args.verbose: verbosity = 2
  if args.testing: verbosity = 4
  working_model_filename = args.working_model
  fixed_model_filename = args.fixed_model

  results = prepare_map_for_refinement(map_filename, map1_filename, map2_filename, d_min,
            working_model_filename, fixed_model_filename=fixed_model_filename,
            verbosity=verbosity)

  # After working with a cell that is twice as wide as the sphere, cut this down
  # to keep only the unique data
  from io import StringIO # Needed to stop printing within apply_change_of_basis
  expectE = results.expectE
  expectE_cb, cb_op = expectE.apply_change_of_basis(change_of_basis='H/2,K/2,L/2',
                      out=StringIO())
  mtz_dataset = expectE_cb.as_mtz_dataset(column_root_label='Emean')
  dobs = results.dobs
  dobs_cb, cb_op = dobs.apply_change_of_basis(change_of_basis='H/2,K/2,L/2',
                   out=StringIO())
  mtz_dataset.add_miller_array(
      dobs_cb,column_root_label='Dobs',column_types='W')
  mtz_object=mtz_dataset.mtz_object()

  if args.file_root is not None:
    mtzout_file_name = args.file_root + "_weighted_map_data.mtz"
    mapout_file_name = args.file_root + "_likelihood_weighted.map"
  else:
    mtzout_file_name = "weighted_map_data.mtz"
    mapout_file_name = "likelihood_weighted.map"
  if verbosity > 1:
    print ("Writing mtz for refinement as", mtzout_file_name)
    print ("Writing likelihood-weighted map as", mapout_file_name)
  dm.write_miller_array_file(mtz_object, filename=mtzout_file_name)
  new_mmm = results.new_mmm
  new_mmm.write_map(map_id='map_manager_lwtd', file_name = mapout_file_name)

if __name__ == "__main__":
  run()


 *******************************************************************************


 *******************************************************************************
cctbx/maptbx/qscore.py
from __future__ import division
from collections import defaultdict
import warnings

from libtbx.utils import null_out
from cctbx.array_family import flex
from libtbx import easy_mp
import numpy as np
import numpy.ma as ma
from scipy.spatial import KDTree
import pandas as pd


master_phil_str = """
  qscore
  {

    nproc = 1
        .type = int
        .help = Number of processors to use
        .short_caption = Number of processors to use
        .expert_level = 1
    n_probes = 32
        .type = int
        .help = Number of radial probes to use
        .short_caption = Number of radial probes to use
        .expert_level = 1

    selection = None
      .type = str
      .help = Only calculate atoms within this selection
      .short_caption = Only test atoms within this selection
      .expert_level = 1

    shell_radius_start = 0.1
      .type = float
      .help = Start testing density at this radius from atom
      .short_caption = Start testing density at this radius from atom
      .expert_level = 1

    shell_radius_stop = 2
      .type = float
      .help = Stop testing density at this radius from atom
      .short_caption = Stop testing density at this radius from atom
      .expert_level = 1

    shell_radius_num = 20
      .type = int
      .help = The number of radial shells
      .short_caption = The number of radial shells (includes start/stop, so minimum 2)
      .expert_level = 1

    rtol = 0.9
      .type = float
      .help = Mapq rtol value, the "real" shell radii are r*rtol

    write_probes = False
      .type = bool
      .help = Write the qscore probes as a .bild file to visualize in Chimera

    write_to_bfactor_pdb = False
      .type = bool
      .help = Write out a pdb file with the Q-score per atom in the B-factor field
  }

  """

################################################################################
#### Probe generation functions
################################################################################



# Fast numpy version
def generate_probes_np(sites_cart, rad, n_probes):
  """
  Generate probes using the same methodology as Pintile mapq, but vectorized

  sites_cart: np array of shape (n_atoms,3)
  rad: the radius at which to place the probes
  N: the number of probes per atom

  Returns:
    probes (np.ndarray): shape (n_atoms,n_probes,3)
  """
  assert sites_cart.ndim == 2 and sites_cart.shape[-1]==3, (
    "Provide coordinates in shape (n_atoms,3)")

  N = n_probes
  h = -1.0 + (2.0 * np.arange(N) / float(N-1))
  phis = np.arccos(h)

  thetas = np.zeros(N)
  a = (3.6 / np.sqrt(N * (1.0 - h[1:-1]**2)))
  thetas[1:-1] = a
  thetas = np.cumsum(thetas)


  x = np.sin(phis) * np.cos(thetas)
  y = np.sin(phis) * np.sin(thetas)
  z = np.cos(phis)

  probes = rad * np.stack([x, y, z], axis=-1)

  # Adjusting location of generated points relative to point ctr
  probes = probes.reshape(-1, 1, 3) + sites_cart.reshape(1, -1, 3)

  # reshape (n_atoms,n_probes,3)
  probes = probes.swapaxes(0,1)
  return probes


################################################################################
#### Run shell functions for multiple shells(possibly in parallel)
################################################################################

class GatherProbes:
  def __init__(self,
               func,
               fixed_kwargs,
               ):
    self.func = func
    self.fixed_kwargs = fixed_kwargs

  def __call__(self,RAD):

    return self.func(RAD=RAD,**self.fixed_kwargs)


def get_probes(
    sites_cart=None,
    atoms_tree = None,
    shells = None,
    n_probes = None,
    rtol=None,
    nproc=1,
    selection_bool = None,
    worker_func=None,
    log = null_out()):

  """
  Generate probes for multiple radial shells (shells)
  """
  # Create before multiprocessing
  atoms_tree = KDTree(sites_cart)



  assert shells is not None, "Must provide explicit radial shells"
  fixed_kwargs = {
      "sites_cart": sites_cart,  # A numpy array of shape (N,3)
      "atoms_tree": atoms_tree,  # An atom_xyz scipy kdtree
      "selection_bool": selection_bool,  # Boolean atom selection
      "n_probes": n_probes,  # The desired number of probes per shell
      "rtol": rtol,  # Multiplied with RAD to get actual radius
      "log": log,
      }

  gather = GatherProbes(worker_func,fixed_kwargs)

  results = easy_mp.pool_map(
      processes=nproc,
      fixed_func=gather,
      args=shells)

  probe_xyz = [result[0] for result in results]
  probe_mask = [result[1] for result in results]

  n_shells = len(shells)
  probe_xyz_stacked = np.empty((n_shells,*probe_xyz[0].shape))
  for i,array in enumerate(probe_xyz):
    probe_xyz_stacked[i] = array
  return probe_xyz_stacked, np.array(probe_mask)


def shell_probes_precalculate(
      sites_cart=None,   # A numpy array of shape (N,3)
      atoms_tree=None,  # An atom_xyz scipy kdtree
      selection_bool=None,# Boolean atom selection
      n_probes=8,# The desired number of probes per shell
      RAD=1.5,          # The nominal radius of this shell
      rtol=0.9,         # Multiplied with RAD to get actual radius
      log = null_out(),
      strict = False,
      ):
  """
  Generate probes by precalculating for a single shell (radius)
  """

  # Do input validation
  if not atoms_tree:
    atoms_tree = KDTree(np.array(sites_cart))

  # Manage log
  if log is None:
    log = null_out()

  # manage selection input
  if selection_bool is None:
    selection_bool = np.full(len(sites_cart),True)


  # do selection
  sites_cart_sel = sites_cart[selection_bool]

  # get probe coordinates
  probe_xyz = generate_probes_np(sites_cart_sel, RAD, n_probes)
  n_atoms, n_probes, _ = probe_xyz.shape
  probe_xyz_flat = probe_xyz.reshape(-1,3)

  # modify "real" radius as in mapq
  outRAD = RAD*rtol

  # query kdtree to get neighbors and their distances
  dists, atom_indices = atoms_tree.query(probe_xyz_flat, k=2)
  dists = dists.reshape((n_atoms,n_probes,2))
  atom_indices = atom_indices.reshape((n_atoms,n_probes,2))

  # Build an index array that would be expected if each probe is near "its" atom
  row_indices = np.arange(n_atoms)[:, np.newaxis]

  # Mask for whether each probe's nearest atom is the one expected
  expected_atom_mask = atom_indices[:,:,0]==row_indices

  # A second mask to determine if the second nearest neighbor should be rejected
  #  (whether the second nearest neighbor is within the rejection radius)
  within_r_mask = dists[:,:,1]<outRAD #

  # Combine masks
  probe_mask = expected_atom_mask & ~within_r_mask

  # Debug/Validation on number of probes per atom
  n_probes_min = 4
  strict=False
  n_probes_per_atom = probe_mask.sum(axis=1)
  insufficient_probes = np.where(n_probes_per_atom<n_probes)[0]
  problematic_probes = np.where(n_probes_per_atom<n_probes_min)[0]
  if strict:
    if n_probes_per_atom.min() >= n_probes_min:
      print(
      f"Some atoms have less than {n_probes_min} probes. \
          ({len(problematic_probes)}). Consider raising n_probes",file=log)

  return probe_xyz, probe_mask


def calc_qscore(mmm,
                selection=None,
                shells=None,
                n_probes=8,
                rtol=0.9,
                nproc=1,
                log=null_out(),
                debug=False):
  """
  Calculate qscore from map model manager
  """
  model = mmm.model()
  # never do hydrogen
  model = model.remove_hydrogens()
  mmm.set_model(model)
  mm = mmm.map_manager()


  # Get atoms
  sites_cart = model.get_sites_cart().as_numpy_array()

  # do selection
  if selection != None:
    selection_bool = mmm.model().selection(selection) # boolean
    if selection_bool.sum() ==0:
      print("Finished... nothing selected",file=log)
      return {"qscore_per_atom":None}
  else:
    selection_bool = flex.bool(model.get_number_of_atoms(),True)


  # determine worker func
  worker_func=shell_probes_precalculate


  # Get probes and probe mask (probes to reject)
  probe_xyz,probe_mask = get_probes(
    sites_cart=sites_cart,
    atoms_tree = None,
    shells=shells,
    n_probes=n_probes,
    rtol=rtol,
    nproc=nproc,
    selection_bool = selection_bool,
    worker_func=worker_func,
    log = log,
    )

  # after the probe generation, versions 1 and 2 are the same

  # infer params from shape
  n_shells, n_atoms, n_probes, _ = probe_xyz.shape

  # flatten
  probe_xyz_flat = probe_xyz.reshape((n_atoms * n_shells * n_probes, 3))
  probe_mask_flat = probe_mask.reshape(-1)  # (n_shells*n_atoms*n_probes,)
  masked_probe_xyz_flat = probe_xyz_flat[probe_mask_flat]

  # interpolate
  volume = mm.map_data().as_numpy_array()
  voxel_size = mm.pixel_sizes()
  masked_density = mm.density_at_sites_cart(
    flex.vec3_double(masked_probe_xyz_flat)).as_numpy_array()

  d_vals = np.full((n_shells, n_atoms, n_probes),np.nan)
  d_vals[probe_mask] = masked_density

  # g vals
  # create the reference data
  M = volume
  maxD = min(M.mean() + M.std() * 10, M.max())
  minD = max(M.mean() - M.std() * 1, M.min())
  A = maxD - minD
  B = minD
  u = 0
  sigma = 0.6
  x = np.array(shells)
  y = A * np.exp(-0.5 * ((x - u) / sigma) ** 2) + B

  # Stack and reshape data for correlation calc

  # stack the reference to shape (n_shells,n_atoms,n_probes)
  g_vals = np.repeat(y[:, None], n_probes, axis=1)
  g_vals = np.expand_dims(g_vals, 1)
  g_vals = np.tile(g_vals, (n_atoms, 1))

  # set masked area to nan
  g_vals[~probe_mask] = np.nan

  # reshape
  g_vals_2d = g_vals.transpose(1, 0, 2).reshape(g_vals.shape[1], -1)
  d_vals_2d = d_vals.transpose(1, 0, 2).reshape(d_vals.shape[1], -1)
  mask_2d = probe_mask.transpose(1, 0, 2).reshape(probe_mask.shape[1], -1)

  # CALCULATE Q
  q = rowwise_corrcoef(g_vals_2d, d_vals_2d, mask=mask_2d)

  # round sensibly
  q = np.around(q,4)

  # aggregate per residue
  model = model.select(flex.bool(selection_bool))
  qscore_df = aggregate_qscore_per_residue(model,q,window=3)
  q = flex.double(q)
  qscore_per_residue = flex.double(qscore_df["Q-Residue"].values)

  # Output
  result = {
    "qscore_per_atom":q,
    "qscore_per_residue":qscore_per_residue,
    "qscore_dataframe":qscore_df
    }

  if debug:
    # Collect debug data
    result.update({
      "atom_xyz":sites_cart,
      "probe_xyz":probe_xyz,
      "probe_mask":probe_mask,
      "d_vals":d_vals,
      "g_vals":g_vals,
    })

  return result


def rowwise_corrcoef(A, B, mask=None):
  """Numpy masked array rowwise correlation coefficient"""
  assert A.shape == B.shape, (
      f"A and B must have the same shape, got: {A.shape} and {B.shape}")

  if mask is not None:
    assert mask.shape == A.shape, "mask must have the same shape as A and B"
    A = ma.masked_array(A, mask=np.logical_not(mask))
    B = ma.masked_array(B, mask=np.logical_not(mask))

  # Calculate means
  A_mean = ma.mean(A, axis=1, keepdims=True)
  B_mean = ma.mean(B, axis=1, keepdims=True)

  # Subtract means
  A_centered = A - A_mean
  B_centered = B - B_mean

  # Calculate sum of products
  sumprod = ma.sum(A_centered * B_centered, axis=1)

  # Calculate square roots of the sum of squares
  sqrt_sos_A = ma.sqrt(ma.sum(A_centered**2, axis=1))
  sqrt_sos_B = ma.sqrt(ma.sum(B_centered**2, axis=1))

  # Return correlation coefficients
  cc = sumprod / (sqrt_sos_A * sqrt_sos_B)
  return cc.data



def aggregate_qscore_per_residue(model,qscore_per_atom,window=3):
  # assign residue indices to each atom

  atoms = model.get_atoms()
  df = cctbx_atoms_to_df(atoms)
  df["Q-score"] = qscore_per_atom
  df["rg_index"] = df.groupby(["chain_id", "resseq", "resname"]).ngroup()
  grouped_means = df.groupby(['chain_id', "resseq", "resname", "rg_index"],
                             as_index=False)['Q-score'].mean().rename(
                               columns={'Q-score': 'Q-Residue'})

  #grouped_means['RollingMean'] = None  # Initialize column to avoid KeyError

  # Until pandas is updated, need to suppress warning
  warnings.filterwarnings("ignore", category=FutureWarning)
  # for chain_id, group in grouped_means.groupby("chain_id"):
  #   rolling_means = variable_neighbors_rolling_mean(group['Q-Residue'], window)
  #   grouped_means.loc[group.index, 'RollingMean'] = rolling_means.values



  # Merge the updated 'Q-Residue' and 'RollingMean' back into the original DataFrame
  df = df.merge(grouped_means[['rg_index', 'Q-Residue']], on='rg_index', how='left')
  df.drop("rg_index", axis=1, inplace=True)
  # df["Q-ResidueRolling"] = df["RollingMean"].astype(float)
  # df.drop(columns=["RollingMean"],inplace=True)
  return df

def variable_neighbors_rolling_mean(series, window=3):
  """
  Aggregate per-atom qscore to per-residue in the same
    manner as the original mapq program
  """
  # Container for the rolling means
  rolling_means = []

  # Calculate rolling mean for each index
  for i in range(len(series)):
    # Determine the start and end indices of the window
    start_idx = max(0, i - window)
    end_idx = min(len(series), i + window + 1)

    # Calculate mean for the current window
    window_mean = series.iloc[start_idx:end_idx].mean()
    rolling_means.append(window_mean)

  return pd.Series(rolling_means)


def write_bild_spheres(xyz,filename="sphere.bild",r=0.5):
  """
  Write a chimerax .bild file with spheres

  Params:
    xyz (np.array): Cartesian coordinates (N,3)
    filename (str): The filename for the .bild file
    r (float): sphere radius
  """
  out = ""
  for x,y,z in xyz:
    s = f".sphere {x} {y} {z} {r}\n"
    out+=s

  with open(filename,"w") as fh:
    fh.write(out)



def cctbx_atoms_to_df(atoms):
  """
  Build a pandas dataframe from a cctbx shared atoms object

  Params:
    atoms (iotbx_pdb_hierarchy_ext.af_shared_atom): The atom array

  Returns:
    df_atoms (pd.DataFrame): A pandas dataframe with the core data present
  """
  # Define values composition
  func_mapper = {
                        #"model_id", # model
                        "id":lambda atom: atom.i_seq,
                        "model_id":lambda atom: atom.parent().parent().parent().parent().id,
                        "chain_id":lambda atom: atom.parent().parent().parent().id,
                        "resseq":lambda atom: atom.parent().parent().resseq_as_int(),
                        "resname":lambda atom: atom.parent().parent().unique_resnames()[0],
                        "name":lambda atom: atom.name.strip(),
                        "element":lambda atom: atom.element,
                        "altloc": lambda atom: atom.parent().altloc
  }

  # Build as dictionaries
  data = defaultdict(list)
  for atom in atoms:
    for key,func in func_mapper.items():
      data[key].append(func(atom))


  # Include values non-composition
  xyz = atoms.extract_xyz().as_numpy_array()
  data["x"] = xyz[:,0]
  data["y"] = xyz[:,1]
  data["z"] = xyz[:,2]

  # Build dataframe from dictionaries
  df_atoms = pd.DataFrame(data,index=list(range(len(atoms))))

  return df_atoms


 *******************************************************************************


 *******************************************************************************
cctbx/maptbx/real_space_refinement_simple.py
from __future__ import absolute_import, division, print_function
from cctbx import maptbx
from cctbx.array_family import flex
import scitbx.lbfgs
from cctbx import xray
from scitbx import matrix

def local_standard_deviations_target_per_site(
      unit_cell, density_map, weight_map, weight_map_scale_factor,
      sites_cart, site_radii):
  if (weight_map is None):
    return maptbx.standard_deviations_around_sites(
      unit_cell=unit_cell,
      density_map=density_map,
      sites_cart=sites_cart,
      site_radii=site_radii)
  d = maptbx.real_space_target_simple_per_site(
    unit_cell=unit_cell,
    density_map=density_map,
    sites_cart=sites_cart)
  w = flex.pow2(maptbx.standard_deviations_around_sites(
    unit_cell=unit_cell,
    density_map=weight_map,
    sites_cart=sites_cart,
    site_radii=site_radii))
  w_min = 0.01
  w.set_selected((w < w_min), w_min)
  w = 1. / w
  if (weight_map_scale_factor is not None):
    assert weight_map_scale_factor > 0
    w *= weight_map_scale_factor
  return d / w

def local_standard_deviations_target(
      unit_cell, density_map, weight_map, weight_map_scale_factor,
      sites_cart, site_radii):
  return flex.sum(local_standard_deviations_target_per_site(
    unit_cell, density_map, weight_map, weight_map_scale_factor,
    sites_cart, site_radii))

def local_standard_deviations_gradients(
      unit_cell, density_map, weight_map, weight_map_scale_factor,
      sites_cart, site_radii, delta):
  # XXX inefficient implementation:
  # inner-most loop (in maptbx.standard_deviations_around_sites)
  # should be outer-most
  grad_cols = []
  for i_dim in [0,1,2]:
    shift = [0,0,0]
    targets = []
    for signed_delta in [delta, -delta]:
      shift[i_dim] = signed_delta
      sites_cart_shifted = sites_cart + shift
      targets.append(local_standard_deviations_target_per_site(
        unit_cell=unit_cell,
        density_map=density_map,
        weight_map=weight_map,
        weight_map_scale_factor=weight_map_scale_factor,
        sites_cart=sites_cart_shifted,
        site_radii=site_radii))
    grad_cols.append((targets[0]-targets[1])/(2*delta))
  return flex.vec3_double(*grad_cols)

class magnification_anisotropic_minimization(object):

  def __init__(O,
        sites_cart,
        density_map,
        unit_cell,
        K, # magnification 3*3  matrix, or a triplet (diagonal only)
        lbfgs_termination_params=None,
        lbfgs_exception_handling_params=None):
    assert [isinstance(K, matrix.sqr), isinstance(K, matrix.col)].count(True)==1
    O.density_map = density_map
    O.unit_cell = unit_cell
    O.sites_cart = sites_cart
    O.K = K
    O.x = flex.double(O.K.elems)
    O.number_of_function_evaluations = -1
    O.f_start, O.g_start = O.compute_functional_and_gradients()
    O.minimizer = scitbx.lbfgs.run(
      target_evaluator=O,
      termination_params=lbfgs_termination_params,
      exception_handling_params=lbfgs_exception_handling_params)
    O.f_final, O.g_final = O.compute_functional_and_gradients()
    del O.x

  def compute_functional_and_gradients(O):
    if (O.number_of_function_evaluations == 0):
      O.number_of_function_evaluations += 1
      return O.f_start, O.g_start
    O.number_of_function_evaluations += 1
    if(isinstance(O.K, matrix.sqr)):
      O.K = matrix.sqr(
        [O.x[0], O.x[1], O.x[2],
         O.x[3], O.x[4], O.x[5],
         O.x[6], O.x[7], O.x[8]])
    else:
      O.K = matrix.col([O.x[0], O.x[1], O.x[2]])
    o = maptbx.target_and_gradients_simple_magnification(
      unit_cell  = O.unit_cell,
      map_target = O.density_map,
      sites_cart = O.sites_cart,
      K          = O.K)
    return -1.* o.target(), o.gradients()*(-1.)

class lbfgs(object):

  def __init__(O,
        sites_cart,
        density_map,
        gradients_method,
        weight_map=None,
        unit_cell=None,
        selection_variable=None,
        selection_variable_real_space=None,
        geometry_restraints_manager=None,
        energies_sites_flags=None,
        gradient_only=False,
        line_search=True,
        real_space_target_weight=1,
        real_space_gradients_delta=None,
        local_standard_deviations_radius=None,
        weight_map_scale_factor=None,
        lbfgs_core_params=None,
        lbfgs_termination_params=None,
        lbfgs_exception_handling_params=None,
        states_collector=None):
    #assert [unit_cell, geometry_restraints_manager].count(None) == 1
    assert real_space_gradients_delta is not None
    if (unit_cell is None):
      unit_cell = geometry_restraints_manager.crystal_symmetry.unit_cell()
    if(selection_variable_real_space is not None):
      assert selection_variable_real_space.size() == sites_cart.size()
    else:
      selection_variable_real_space = flex.bool(sites_cart.size(), True)
    O.lbfgs_core_params = lbfgs_core_params
    O.gradients_method = gradients_method
    O.x_previous = None
    O.states_collector = states_collector
    O.gradient_only=gradient_only
    O.line_search=line_search
    O.density_map = density_map
    O.weight_map = weight_map
    O.unit_cell = unit_cell
    O.sites_cart = sites_cart
    O.geometry_restraints_manager = geometry_restraints_manager
    O.energies_sites_flags = energies_sites_flags
    O.real_space_target_weight = real_space_target_weight
    O.real_space_gradients_delta = real_space_gradients_delta
    O.local_standard_deviations_radius = local_standard_deviations_radius
    O.selection_variable_real_space = selection_variable_real_space
    if (O.local_standard_deviations_radius is None):
      O.site_radii = None
    else:
      O.site_radii = flex.double(
        O.sites_cart.size(), O.local_standard_deviations_radius)
    O.weight_map_scale_factor = weight_map_scale_factor
    O.selection_variable = selection_variable
    if (O.selection_variable is None):
      O.sites_cart = sites_cart
      O.x = sites_cart.as_double()
    else:
      O.sites_cart = sites_cart.deep_copy()
      O.x = sites_cart.select(O.selection_variable).as_double()
    O.number_of_function_evaluations = -1
    O.f_start, O.g_start = O.compute_functional_and_gradients()
    O.minimizer = scitbx.lbfgs.run(
      target_evaluator=O,
      gradient_only=O.gradient_only,
      line_search=O.line_search,
      core_params=O.lbfgs_core_params,
      termination_params=lbfgs_termination_params,
      exception_handling_params=lbfgs_exception_handling_params)
    O.f_final, O.g_final = O.compute_functional_and_gradients()
    del O.x
    del O.site_radii

  def compute_functional_and_gradients(O):
    if (O.number_of_function_evaluations == 0):
      O.number_of_function_evaluations += 1
      return O.f_start, O.g_start
    O.number_of_function_evaluations += 1
    #
    x_current = O.x
    if(O.x_previous is None):
      O.x_previous = x_current.deep_copy()
    else:
      xray.ext.damp_shifts(previous=O.x_previous, current=x_current,
        max_value=10.)
      O.x_previous = x_current.deep_copy()
    #
    O.sites_cart_variable = flex.vec3_double(x_current)
    if (O.real_space_target_weight == 0):
      rs_f = 0.
      rs_g = flex.vec3_double(O.sites_cart_variable.size(), (0,0,0))
    else:
      if (O.local_standard_deviations_radius is None):
        if(O.gradients_method=="fd"):
          o = maptbx.target_and_gradients_simple(
            unit_cell   = O.unit_cell,
            map_target  = O.density_map,
            sites_cart  = O.sites_cart_variable,
            delta       = O.real_space_gradients_delta,
            selection   = O.selection_variable_real_space)
        else:
          o = maptbx.target_and_gradients_simple(
            unit_cell     = O.unit_cell,
            map_target    = O.density_map,
            sites_cart    = O.sites_cart_variable,
            selection     = O.selection_variable_real_space,
            interpolation = O.gradients_method)
        rs_f = o.target()
        rs_g = o.gradients()
      else:
        rs_f = local_standard_deviations_target(
          unit_cell=O.unit_cell,
          density_map=O.density_map,
          weight_map=O.weight_map,
          weight_map_scale_factor=O.weight_map_scale_factor,
          sites_cart=O.sites_cart_variable,
          site_radii=O.site_radii)
        rs_g = local_standard_deviations_gradients(
          unit_cell=O.unit_cell,
          density_map=O.density_map,
          weight_map=O.weight_map,
          weight_map_scale_factor=O.weight_map_scale_factor,
          sites_cart=O.sites_cart_variable,
          site_radii=O.site_radii,
          delta=O.real_space_gradients_delta)
      rs_f *= -O.real_space_target_weight
      rs_g *= -O.real_space_target_weight
    if (O.geometry_restraints_manager is None):
      f = rs_f
      g = rs_g
    else:
      if (O.selection_variable is None):
        O.sites_cart = O.sites_cart_variable
      else:
        O.sites_cart.set_selected(O.selection_variable, O.sites_cart_variable)
        if(O.states_collector is not None):
          O.states_collector.add(sites_cart = O.sites_cart)
      gr_e = O.geometry_restraints_manager.energies_sites(
        sites_cart=O.sites_cart,
        compute_gradients=True)
      gr_e_gradients = gr_e.gradients
      if (O.selection_variable is not None):
        gr_e_gradients = gr_e.gradients.select(O.selection_variable)
      f = rs_f + gr_e.target
      g = rs_g + gr_e_gradients
    return f, g.as_double()


 *******************************************************************************


 *******************************************************************************
cctbx/maptbx/refine_scales_errors.py
from __future__ import print_function
from __future__ import division
import math
from scitbx.array_family import flex
from scitbx.dtmin.minimizer import Minimizer
from scitbx.dtmin.refinebase import RefineBase
from scitbx.dtmin.reparams import Reparams
from scitbx.dtmin.bounds import Bounds
from cctbx import adptbx
from libtbx import group_args

def best_curve(ssqr):
  # Scaled data from BEST curve. Original data obtained from Sasha Popov, then
  # rescaled to correspond at higher resolution to the average X-ray scattering
  # factor from proteins atoms (with average atomic composition)
  best_data = ((0.009, 3.40735),
              (0.013092, 2.9006),
              (0.0171839, 2.33083),
              (0.0212759, 1.80796),
              (0.0253679, 1.65133),
              (0.0294599, 1.75784),
              (0.0335518, 2.06865),
              (0.0376438, 2.57016),
              (0.0417358, 3.13121),
              (0.0458278, 3.62596),
              (0.0499197, 3.92071),
              (0.0540117, 3.98257),
              (0.0581037, 3.91846),
              (0.0621956, 3.80829),
              (0.0662876, 3.69517),
              (0.0703796, 3.59068),
              (0.0744716, 3.44971),
              (0.0785635, 3.30765),
              (0.0826555, 3.16069),
              (0.0867475, 2.98656),
              (0.0908395, 2.77615),
              (0.0949314, 2.56306),
              (0.0990234, 2.37314),
              (0.103115, 2.22874),
              (0.107207, 2.09477),
              (0.111299, 1.98107),
              (0.115391, 1.8652),
              (0.119483, 1.75908),
              (0.123575, 1.67093),
              (0.127667, 1.59257),
              (0.131759, 1.52962),
              (0.135851, 1.48468),
              (0.139943, 1.45848),
              (0.144035, 1.43042),
              (0.148127, 1.40953),
              (0.152219, 1.37291),
              (0.156311, 1.34217),
              (0.160403, 1.3308),
              (0.164495, 1.32782),
              (0.168587, 1.30862),
              (0.172679, 1.31319),
              (0.176771, 1.30907),
              (0.180863, 1.31456),
              (0.184955, 1.31055),
              (0.189047, 1.31484),
              (0.193139, 1.31828),
              (0.197231, 1.32321),
              (0.201323, 1.30853),
              (0.205415, 1.30257),
              (0.209507, 1.2851),
              (0.213599, 1.26912),
              (0.217691, 1.24259),
              (0.221783, 1.24119),
              (0.225875, 1.2382),
              (0.229967, 1.21605),
              (0.234059, 1.17269),
              (0.23815, 1.13909),
              (0.242242, 1.1165),
              (0.246334, 1.08484),
              (0.250426, 1.0495),
              (0.254518, 1.01289),
              (0.25861, 0.974819),
              (0.262702, 0.940975),
              (0.266794, 0.900938),
              (0.270886, 0.861657),
              (0.274978, 0.830192),
              (0.27907, 0.802167),
              (0.283162, 0.780746),
              (0.287254, 0.749194),
              (0.291346, 0.720884),
              (0.295438, 0.694409),
              (0.29953, 0.676239),
              (0.303622, 0.650672),
              (0.307714, 0.632438),
              (0.311806, 0.618569),
              (0.315898, 0.605762),
              (0.31999, 0.591398),
              (0.324082, 0.579308),
              (0.328174, 0.572076),
              (0.332266, 0.568138),
              (0.336358, 0.559537),
              (0.34045, 0.547927),
              (0.344542, 0.539319),
              (0.348634, 0.529009),
              (0.352726, 0.516954),
              (0.356818, 0.512218),
              (0.36091, 0.511836),
              (0.365002, 0.511873),
              (0.369094, 0.506957),
              (0.373186, 0.502738),
              (0.377278, 0.50191),
              (0.38137, 0.492422),
              (0.385462, 0.488461),
              (0.389553, 0.483436),
              (0.393645, 0.481468),
              (0.397737, 0.473786),
              (0.401829, 0.468684),
              (0.405921, 0.468291),
              (0.410013, 0.46645),
              (0.414105, 0.4643),
              (0.418197, 0.45641),
              (0.422289, 0.450462),
              (0.426381, 0.444678),
              (0.430473, 0.443807),
              (0.434565, 0.441158),
              (0.438657, 0.441303),
              (0.442749, 0.437144),
              (0.446841, 0.428504),
              (0.450933, 0.420459),
              (0.455025, 0.413754),
              (0.459117, 0.412064),
              (0.463209, 0.406677),
              (0.467301, 0.40253),
              (0.471393, 0.396454),
              (0.475485, 0.393192),
              (0.479577, 0.390452),
              (0.483669, 0.38408),
              (0.487761, 0.379456),
              (0.491853, 0.373123),
              (0.495945, 0.374026),
              (0.500037, 0.373344),
              (0.504129, 0.377639),
              (0.508221, 0.374029),
              (0.512313, 0.374691),
              (0.516405, 0.371632),
              (0.520497, 0.370724),
              (0.524589, 0.366095),
              (0.528681, 0.369447),
              (0.532773, 0.369043),
              (0.536865, 0.368967),
              (0.540956, 0.36583),
              (0.545048, 0.370593),
              (0.54914, 0.371047),
              (0.553232, 0.372723),
              (0.557324, 0.371915),
              (0.561416, 0.372882),
              (0.565508, 0.371052),
              (0.5696, 0.36775),
              (0.573692, 0.369884),
              (0.577784, 0.374098),
              (0.581876, 0.374169),
              (0.585968, 0.37261),
              (0.59006, 0.372356),
              (0.594152, 0.377055),
              (0.598244, 0.3817),
              (0.602336, 0.381867),
              (0.606428, 0.377746),
              (0.61052, 0.377157),
              (0.614612, 0.376604),
              (0.618704, 0.37532),
              (0.622796, 0.372488),
              (0.626888, 0.373312),
              (0.63098, 0.377505),
              (0.635072, 0.381011),
              (0.639164, 0.379326),
              (0.643256, 0.380193),
              (0.647348, 0.381122),
              (0.65144, 0.387213),
              (0.655532, 0.391928),
              (0.659624, 0.398986),
              (0.663716, 0.402951),
              (0.667808, 0.405893),
              (0.6719, 0.40217),
              (0.675992, 0.401806),
              (0.680084, 0.404238),
              (0.684176, 0.409404),
              (0.688268, 0.413486),
              (0.692359, 0.413167),
              (0.696451, 0.414008),
              (0.700543, 0.417128),
              (0.704635, 0.420275),
              (0.708727, 0.423617),
              (0.712819, 0.42441),
              (0.716911, 0.426445),
              (0.721003, 0.429012),
              (0.725095, 0.430132),
              (0.729187, 0.42992),
              (0.733279, 0.425202),
              (0.737371, 0.423159),
              (0.741463, 0.423913),
              (0.745555, 0.425542),
              (0.749647, 0.426682),
              (0.753739, 0.431186),
              (0.757831, 0.433959),
              (0.761923, 0.433839),
              (0.766015, 0.428679),
              (0.770107, 0.425968),
              (0.774199, 0.426528),
              (0.778291, 0.427093),
              (0.782383, 0.426848),
              (0.786475, 0.424549),
              (0.790567, 0.423785),
              (0.794659, 0.419892),
              (0.798751, 0.417391),
              (0.802843, 0.413128),
              (0.806935, 0.408498),
              (0.811027, 0.402764),
              (0.815119, 0.404852),
              (0.819211, 0.405915),
              (0.823303, 0.392919),
              (0.827395, 0.384632),
              (0.831487, 0.382626),
              (0.835579, 0.379891),
              (0.839671, 0.376414),
              (0.843762, 0.372915),
              (0.847854, 0.375089),
              (0.851946, 0.371918),
              (0.856038, 0.36652),
              (0.86013, 0.358529),
              (0.864222, 0.356496),
              (0.868314, 0.354707),
              (0.872406, 0.348802),
              (0.876498, 0.343693),
              (0.88059, 0.34059),
              (0.884682, 0.342432),
              (0.888774, 0.345099),
              (0.892866, 0.344524),
              (0.896958, 0.342489),
              (0.90105, 0.328009),
              (0.905142, 0.323685),
              (0.909234, 0.321378),
              (0.913326, 0.318832),
              (0.917418, 0.314999),
              (0.92151, 0.311775),
              (0.925602, 0.30844),
              (0.929694, 0.30678),
              (0.933786, 0.303484),
              (0.937878, 0.301197),
              (0.94197, 0.296788),
              (0.946062, 0.295353),
              (0.950154, 0.298028),
              (0.954246, 0.298098),
              (0.958338, 0.295081),
              (0.96243, 0.289337),
              (0.966522, 0.286116),
              (0.970614, 0.284319),
              (0.974706, 0.280972),
              (0.978798, 0.28015),
              (0.98289, 0.279016),
              (0.986982, 0.277532),
              (0.991074, 0.276013),
              (0.995165, 0.270923),
              (0.999257, 0.269446),
              (1.00335, 0.266567),
              (1.00744, 0.263561),
              (1.01153, 0.261002),
              (1.01563, 0.255349),
              (1.01972, 0.258644),
              (1.02381, 0.254974),
              (1.0279, 0.2523),
              (1.03199, 0.244489),
              (1.03609, 0.249418),
              (1.04018, 0.249519),
              (1.04427, 0.249316),
              (1.04836, 0.249197),
              (1.05245, 0.24415),
              (1.05655, 0.244556),
              (1.06064, 0.241169),
              (1.06473, 0.238484),
              (1.06882, 0.2392),
              (1.07291, 0.240651),
              (1.077, 0.243724),
              (1.0811, 0.243174),
              (1.08519, 0.239545),
              (1.08928, 0.239106),
              (1.09337, 0.238763),
              (1.09746, 0.238971),
              (1.10156, 0.229925),
              (1.10565, 0.225123),
              (1.10974, 0.226932),
              (1.11383, 0.23118),
              (1.11792, 0.228654),
              (1.12202, 0.225084),
              (1.12611, 0.225866),
              (1.1302, 0.227717),
              (1.13429, 0.229508),
              (1.13838, 0.227977),
              (1.14248, 0.226799),
              (1.14657, 0.228456),
              (1.15066, 0.22383),
              (1.15475, 0.22188),
              (1.15884, 0.219986),
              (1.16294, 0.217418),
              (1.16703, 0.214356),
              (1.17112, 0.211027),
              (1.17521, 0.210011),
              (1.1793, 0.210609),
              (1.1834, 0.210893),
              (1.18749, 0.212583),
              (1.19158, 0.208415),
              (1.19567, 0.204557),
              (1.19976, 0.198068),
              (1.20386, 0.197603),
              (1.20795, 0.196691),
              (1.21204, 0.200617),
              (1.21613, 0.199803),
              (1.22022, 0.199199),
              (1.22432, 0.196859),
              (1.22841, 0.197471),
              (1.2325, 0.19799))
  # 300 data points from 0.009 to 1.2325, so separated by 0.004091973
  s1 = (ssqr - 0.009) / 0.004091973
  is1 = int(math.floor(s1))
  if is1 < 0:
    return best_data[0][1] # Below low-res limit for BEST data
  elif is1 >= 299:
    return best_data[0][299]  # Above high-res limit, about 0.9A
  else:
    ds = s1 - is1
    is2 = is1 + 1
    best_val = (1.-ds)*best_data[is1][1] + ds*best_data[is2][1]
    return best_val

# Set up refinement class for dtmin minimiser (based on Phaser minimiser)
class RefineScalesErrors(RefineBase):
  def __init__(self, mc1, mc2, ssqr_bins, start_x):
    RefineBase.__init__(self)

    # Precompute data that will be used repeatedly for fgh evaluation
    f1 = flex.abs(mc1.data())
    f2 = flex.abs(mc2.data())
    p1 = mc1.phases().data()
    p2 = mc2.phases().data()
    self.sumfsqr_miller = mc1.customized_copy(data=flex.pow2(f1) + flex.pow2(f2))
    self.sumfsqr_miller.use_binner_of(mc1)
    self.f1f2cos_miller = mc1.customized_copy(data=f1 * f2 * flex.cos(p2 - p1))

    self.n_bins = mc1.binner().n_bins_used()  # Assume consistent binning
    self.ssqr_bins = ssqr_bins
    self.unit_cell = mc1.unit_cell()
    assert (self.n_bins == len(ssqr_bins))
    self.best_bins = []
    for i_bin in range(self.n_bins):
      best_val = best_curve(ssqr_bins[i_bin])
      self.best_bins.append(best_val)
    self.start_x = start_x
    self.x = start_x[:]         # Full set of parameters
    recip_params = self.unit_cell.reciprocal_parameters()
    astar = recip_params[0]
    bstar = recip_params[1]
    cstar = recip_params[2]
    self.large_shifts_beta = [astar * astar, bstar * bstar, cstar * cstar, astar * bstar, astar * cstar, bstar * cstar]

  def target_gradient_hessian(self, do_gradient=True, do_hessian=True):
    if (do_hessian):
      assert (do_gradient)
    # Extract parameters into variables with sensible names
    n_bins = self.n_bins
    i_par = 0
    asqr_scale = self.x[i_par]
    i_par += 1
    sigmaT_bins = self.x[i_par:i_par + n_bins]
    i_par += n_bins
    asqr_beta = tuple(self.x[i_par:i_par + 6])
    i_par += 6
    sigmaE_scale = self.x[i_par]
    i_par += 1
    sigmaE_bins = self.x[i_par:i_par + n_bins]
    i_par += n_bins
    sigmaE_beta = tuple(self.x[i_par:i_par + 6])
    i_par += 6
    assert (i_par == len(self.x))

    # Initialise function, gradient and Hessian with zeros
    f = 0.
    g = flex.double(self.nmp, 0)
    h = flex.double(self.nmp * self.nmp, 0)
    h.reshape(flex.grid(self.nmp, self.nmp))

    twologpi = 2 * math.log(math.pi)
    # Loop over bins to accumulate target, gradient, Hessian
    ma = self.sumfsqr_miller # Miller array to use for associated information
    i_bin_used = 0 # Keep track in case full range of bins not used
    for i_bin in ma.binner().range_used():
      sel = ma.binner().selection(i_bin)
      ma_sel = ma.select(sel)

      # Make Miller array as basis for computing aniso corrections in this bin
      # Let u = A^2*sigmaT to simplify computation of derivatives wrt parameters defining A and sigmaT
      ones_array = flex.double(ma_sel.size(), 1)
      all_ones = ma_sel.customized_copy(data=ones_array)
      beta_miller_A = all_ones.apply_debye_waller_factors(u_star=adptbx.beta_as_u_star(asqr_beta))
      u_terms = (asqr_scale * sigmaT_bins[i_bin_used] * self.best_bins[i_bin_used]) * beta_miller_A.data()
      beta_miller_E = all_ones.apply_debye_waller_factors(u_star=adptbx.beta_as_u_star(sigmaE_beta))
      sigmaE_terms = (sigmaE_scale * sigmaE_bins[i_bin_used]) * beta_miller_E.data()

      # Variance term per reflection is function of these terms
      u2sigE = 2 * u_terms + sigmaE_terms
      var_terms = u2sigE * sigmaE_terms

      f1f2cos = self.f1f2cos_miller.data().select(sel)
      sumfsqr = self.sumfsqr_miller.data().select(sel)
      minusLL_terms = (sumfsqr * (u_terms + sigmaE_terms)
        - 2 * u_terms * f1f2cos) / var_terms + flex.log(var_terms) # Leave out + twologpi*ma_sel.size()
      f += flex.sum(minusLL_terms)

      if do_gradient:
        # Define some intermediate results needed below
        u2sigE2 = flex.pow2(u2sigE)
        sigmaE_sqr = flex.pow2(sigmaE_terms)
        sumsqrcos = sumfsqr + 2 * f1f2cos
        hyposqr = sumfsqr - 2 * f1f2cos
        if (self.refine_Asqr_scale or self.refine_sigmaT_bins or self.refine_Asqr_beta):
          dmLL_by_du_terms = (2 * u2sigE - sumsqrcos) / u2sigE2
        if (self.refine_sigmaE_scale or self.refine_sigmaE_bins or self.refine_sigmaE_beta):
          dmLL_by_dsigE_terms = ((2 * sigmaE_terms - hyposqr) / (2 * sigmaE_sqr)
                                - sumsqrcos / (2 * u2sigE2) + 1. / (u2sigE))
        if (self.refine_Asqr_beta or self.refine_sigmaE_beta):
          h_as_double, k_as_double, l_as_double = ma_sel.indices().as_vec3_double().parts()
          hh = flex.pow2(h_as_double)
          kk = flex.pow2(k_as_double)
          ll = flex.pow2(l_as_double)
          hk = h_as_double * k_as_double
          hl = h_as_double * l_as_double
          kl = k_as_double * l_as_double

        i_par = 0 # Keep track of index for unrefined parameters
        i_ref = 0 # Keep track of refined parameters
        if self.refine_Asqr_scale: # Only affects U
          du_by_dAsqr_scale = u_terms / asqr_scale
          g[i_ref] += flex.sum(dmLL_by_du_terms * du_by_dAsqr_scale)
          i_ref += 1
        i_par += 1
        if self.refine_sigmaT_bins: # Only affects U, just current bin
          du_by_dsigmaT_bin = (asqr_scale * self.best_bins[i_bin_used]) * beta_miller_A.data()
          i_sigmaT_bin = i_ref+i_bin_used # Save for restraint terms below
          g[i_sigmaT_bin] += flex.sum(dmLL_by_du_terms * du_by_dsigmaT_bin)
          i_ref += self.n_bins
        i_par += self.n_bins
        if self.refine_Asqr_beta:  # Only affects U
          du_by_dbetaA11 = -hh * u_terms
          du_by_dbetaA22 = -kk * u_terms
          du_by_dbetaA33 = -ll * u_terms
          du_by_dbetaA12 = -2 * hk * u_terms
          du_by_dbetaA13 = -2 * hl * u_terms
          du_by_dbetaA23 = -2 * kl * u_terms
          g[i_ref] += flex.sum(dmLL_by_du_terms * du_by_dbetaA11)
          g[i_ref+1] += flex.sum(dmLL_by_du_terms * du_by_dbetaA22)
          g[i_ref+2] += flex.sum(dmLL_by_du_terms * du_by_dbetaA33)
          g[i_ref+3] += flex.sum(dmLL_by_du_terms * du_by_dbetaA12)
          g[i_ref+4] += flex.sum(dmLL_by_du_terms * du_by_dbetaA13)
          g[i_ref+5] += flex.sum(dmLL_by_du_terms * du_by_dbetaA23)
          i_ref += 6
        i_par += 6

        # Note that sigmaE_scale is fixed if sigmaE_bins and/or sigmaE_beta are refined
        if self.refine_sigmaE_scale:
          dsigE_by_dscaleE = sigmaE_terms/sigmaE_scale
          g[i_ref] += flex.sum(dmLL_by_dsigE_terms * dsigE_by_dscaleE)
          i_ref += 1
        i_par += 1
        if self.refine_sigmaE_bins: # Just current bin
          dsigE_by_dsigmaE_bin = sigmaE_scale*beta_miller_E.data()
          g[i_ref+i_bin_used] += flex.sum(dmLL_by_dsigE_terms * dsigE_by_dsigmaE_bin)
          i_ref += self.n_bins
        i_par += self.n_bins
        if self.refine_sigmaE_beta: # Only affects SigmaE
          dsigE_by_dbetaE11 = -hh * sigmaE_terms
          dsigE_by_dbetaE22 = -kk * sigmaE_terms
          dsigE_by_dbetaE33 = -ll * sigmaE_terms
          dsigE_by_dbetaE12 = -2 * hk * sigmaE_terms
          dsigE_by_dbetaE13 = -2 * hl * sigmaE_terms
          dsigE_by_dbetaE23 = -2 * kl * sigmaE_terms
          g[i_ref] += flex.sum(dmLL_by_dsigE_terms * dsigE_by_dbetaE11)
          g[i_ref+1] += flex.sum(dmLL_by_dsigE_terms * dsigE_by_dbetaE22)
          g[i_ref+2] += flex.sum(dmLL_by_dsigE_terms * dsigE_by_dbetaE33)
          g[i_ref+3] += flex.sum(dmLL_by_dsigE_terms * dsigE_by_dbetaE12)
          g[i_ref+4] += flex.sum(dmLL_by_dsigE_terms * dsigE_by_dbetaE13)
          g[i_ref+5] += flex.sum(dmLL_by_dsigE_terms * dsigE_by_dbetaE23)
          i_ref += 6
        i_par += 6
        assert (i_par == len(self.x))
        assert (i_ref == self.nmp)

        if do_hessian:
          u2sigE3 = u2sigE * u2sigE2
          if (self.refine_Asqr_scale or self.refine_sigmaT_bins or self.refine_Asqr_beta):
            d2mLL_by_du2_terms = 4 * (sumsqrcos - 2 * u_terms - sigmaE_terms) / u2sigE3
          if (self.refine_sigmaE_scale or self.refine_sigmaE_bins or self.refine_sigmaE_beta):
            d2mLL_by_dsigE2_terms = ( (hyposqr - sigmaE_terms) / (sigmaE_sqr * sigmaE_terms)
                                    + sumsqrcos / u2sigE3 - 1. / (u2sigE2))
          if (self.refine_Asqr_beta or self.refine_sigmaE_beta):
            hh_sqr = flex.pow2(hh)
            kk_sqr = flex.pow2(kk)
            ll_sqr = flex.pow2(ll)
            hk_sqr = flex.pow2(hk)
            hl_sqr = flex.pow2(hl)
            kl_sqr = flex.pow2(kl)

          i_par = 0 # Keep track of index for unrefined parameters
          i_ref = 0  # Keep track of refined parameters
          # Note that various second derivatives are zero, i.e. of:
          #   u wrt Asqr_scale and sigmaT_bins
          #   sigmaE wrt sigmaE_bins and sigmaE_scale
          if self.refine_Asqr_scale: # Only affects U
            h[i_ref,i_ref] += flex.sum(d2mLL_by_du2_terms * flex.pow2(du_by_dAsqr_scale))
            i_ref += 1
          i_par += 1
          if self.refine_sigmaT_bins: # Only affects U, just current bin
            h[i_sigmaT_bin,i_sigmaT_bin] += flex.sum(d2mLL_by_du2_terms * flex.pow2(du_by_dsigmaT_bin))
            i_ref += self.n_bins
          i_par += self.n_bins
          if self.refine_Asqr_beta:  # Only affects U
            d2u_by_dbetaA11_2 = hh_sqr * u_terms
            d2u_by_dbetaA22_2 = kk_sqr * u_terms
            d2u_by_dbetaA33_2 = ll_sqr * u_terms
            d2u_by_dbetaA12_2 = 4 * hk_sqr * u_terms
            d2u_by_dbetaA13_2 = 4 * hl_sqr * u_terms
            d2u_by_dbetaA23_2 = 4 * kl_sqr * u_terms
            h[i_ref, i_ref]    += ( flex.sum(d2mLL_by_du2_terms * flex.pow2(du_by_dbetaA11))
                                  + flex.sum(dmLL_by_du_terms * d2u_by_dbetaA11_2) )
            h[i_ref+1,i_ref+1] += ( flex.sum(d2mLL_by_du2_terms * flex.pow2(du_by_dbetaA22))
                                  + flex.sum(dmLL_by_du_terms * d2u_by_dbetaA22_2) )
            h[i_ref+2,i_ref+2] += ( flex.sum(d2mLL_by_du2_terms * flex.pow2(du_by_dbetaA33))
                                  + flex.sum(dmLL_by_du_terms * d2u_by_dbetaA33_2) )
            h[i_ref+3,i_ref+3] += ( flex.sum(d2mLL_by_du2_terms * flex.pow2(du_by_dbetaA12))
                                  + flex.sum(dmLL_by_du_terms * d2u_by_dbetaA12_2) )
            h[i_ref+4,i_ref+4] += ( flex.sum(d2mLL_by_du2_terms * flex.pow2(du_by_dbetaA13))
                                  + flex.sum(dmLL_by_du_terms * d2u_by_dbetaA13_2) )
            h[i_ref+5,i_ref+5] += ( flex.sum(d2mLL_by_du2_terms * flex.pow2(du_by_dbetaA23))
                                  + flex.sum(dmLL_by_du_terms * d2u_by_dbetaA23_2) )
            i_ref += 6
          i_par += 6

          # Note that sigmaE_scale and either sigmaE_bins or sigmaE_beta are
          # mutually exclusive in practice. If scale and bins were refined
          # simultaneously with no restraints, there would be one redundant parameter
          if self.refine_sigmaE_scale:
            h[i_ref, i_ref] += flex.sum(
              d2mLL_by_dsigE2_terms * flex.pow2(dsigE_by_dscaleE))
            i_ref += 1
          i_par += 1
          if self.refine_sigmaE_bins:
            h[i_ref+i_bin_used, i_ref+i_bin_used] += flex.sum(
              d2mLL_by_dsigE2_terms * flex.pow2(dsigE_by_dsigmaE_bin))
            i_ref += self.n_bins
          i_par += self.n_bins
          if self.refine_sigmaE_beta: # Only affects SigmaE
            d2sigE_by_dbetaE11_2 = hh_sqr * sigmaE_terms
            d2sigE_by_dbetaE22_2 = kk_sqr * sigmaE_terms
            d2sigE_by_dbetaE33_2 = ll_sqr * sigmaE_terms
            d2sigE_by_dbetaE12_2 = 4 * hk_sqr * sigmaE_terms
            d2sigE_by_dbetaE13_2 = 4 * hl_sqr * sigmaE_terms
            d2sigE_by_dbetaE23_2 = 4 * kl_sqr * sigmaE_terms
            h[i_ref, i_ref]    += ( flex.sum(d2mLL_by_dsigE2_terms * flex.pow2(dsigE_by_dbetaE11))
                                  + flex.sum(dmLL_by_dsigE_terms * d2sigE_by_dbetaE11_2) )
            h[i_ref+1,i_ref+1] += ( flex.sum(d2mLL_by_dsigE2_terms * flex.pow2(dsigE_by_dbetaE22))
                                  + flex.sum(dmLL_by_dsigE_terms * d2sigE_by_dbetaE22_2) )
            h[i_ref+2,i_ref+2] += ( flex.sum(d2mLL_by_dsigE2_terms * flex.pow2(dsigE_by_dbetaE33))
                                  + flex.sum(dmLL_by_dsigE_terms * d2sigE_by_dbetaE33_2) )
            h[i_ref+3,i_ref+3] += ( flex.sum(d2mLL_by_dsigE2_terms * flex.pow2(dsigE_by_dbetaE12))
                                  + flex.sum(dmLL_by_dsigE_terms * d2sigE_by_dbetaE12_2) )
            h[i_ref+4,i_ref+4] += ( flex.sum(d2mLL_by_dsigE2_terms * flex.pow2(dsigE_by_dbetaE13))
                                  + flex.sum(dmLL_by_dsigE_terms * d2sigE_by_dbetaE13_2) )
            h[i_ref+5,i_ref+5] += ( flex.sum(d2mLL_by_dsigE2_terms * flex.pow2(dsigE_by_dbetaE23))
                                  + flex.sum(dmLL_by_dsigE_terms * d2sigE_by_dbetaE23_2) )
            i_ref += 6
          i_par += 6
          assert (i_par == len(self.x))
          assert (i_ref == self.nmp)

      # Add restraint terms
      # Restrain log of sigmaT_bins to 0, but downweighting low resolution
      d_bin = math.sqrt(self.ssqr_bins[i_bin_used])
      sigmascale = 0.15 + 0.001 / d_bin ** 3
      stbin = sigmaT_bins[i_bin_used]
      logbin = math.log(stbin)
      f += (logbin/sigmascale)**2 / 2
      if (do_gradient and self.refine_sigmaT_bins):
        g[i_sigmaT_bin] += logbin / (stbin * sigmascale**2)
        if do_hessian:
          h[i_sigmaT_bin,i_sigmaT_bin] += (1.-logbin) / (stbin*sigmascale)**2

      i_bin_used += 1

    return (f, g, h, True)

  def target(self):
    f_g_h = self.target_gradient_hessian(do_gradient=False, do_hessian=False)
    return f_g_h[0]

  def target_gradient(self):
    f_g_h = self.target_gradient_hessian(do_hessian=False)
    f = f_g_h[0]
    g = f_g_h[1]
    return (f, g)

  def get_macrocycle_parameters(self):
    if len(self.refine_mask) == 0: # All parameters being refined
      return self.x

    mp = []  # Parameters for this macrocycle
    for i in range(len(self.x)):
      if self.refine_mask[i]:
        mp.append(self.x[i])
    assert (len(mp) == self.nmp)
    return mp

  def set_macrocycle_parameters(self, newx):
    if len(self.refine_mask) == 0:  # All parameters being refined
      self.x = newx
    else:
      npref = 0
      for i in range(len(self.x)):
        if self.refine_mask[i]:
          self.x[i] = newx[npref]
          npref += 1
      assert (npref == self.nmp)

  def macrocycle_large_shifts(self):
    i_par = 0 # Keep track of index for unrefined parameters
    large_shifts = []
    if self.refine_Asqr_scale:
      large_shifts.append(self.start_x[i_par]/10.)
    i_par += 1
    if self.refine_sigmaT_bins:
      for i_bin in range(self.n_bins):
        large_shifts.append(0.05)
    i_par += self.n_bins
    if self.refine_Asqr_beta:
      large_shifts.extend(self.large_shifts_beta)
    i_par += 6
    if self.refine_sigmaE_scale:
      large_shifts.append(0.01)
    i_par += 1
    if self.refine_sigmaE_bins:
      for i_bin in range(self.n_bins):
        large_shifts.append(self.start_x[i_par+i_bin]/10.)
    i_par += self.n_bins
    if self.refine_sigmaE_beta:
      large_shifts.extend(self.large_shifts_beta)
    i_par += 6
    assert (i_par == len(self.x))
    assert (len(large_shifts) == self.nmp)
    return large_shifts

  def set_macrocycle_protocol(self, macrocycle_protocol):
    # Possible parameters include overall scale of signal,
    # bin parameters for signal (BEST-like curve), anisotropy tensor for signal,
    # bin parameters for error, anisotropy tensor for error
    # Start with everything being refined, turn some things off for different protocols
    self.refine_mask = []  # Indicates "all" if left empty
    self.refine_Asqr_scale = True
    self.refine_sigmaT_bins = True
    self.refine_Asqr_beta = True
    self.refine_sigmaE_scale = True
    self.refine_sigmaE_bins = True
    self.refine_sigmaE_beta = True

    # For each protocol, define variables that aren't refined
    if macrocycle_protocol == ["default"]:
      self.refine_sigmaE_scale = False

    elif macrocycle_protocol == ["Eprior"]:
      self.refine_sigmaE_bins = False
      self.refine_sigmaE_beta = False

    else:
      print("Macrocycle protocol", macrocycle_protocol, " not recognised")
      exit

    # Now accumulate mask
    self.nmp = 0

    if self.refine_Asqr_scale:
      self.refine_mask.append(True)
      self.nmp += 1
    else:
      self.refine_mask.append(False)

    if self.refine_sigmaT_bins:
      self.refine_mask.extend([True for i in range(self.n_bins)])
      self.nmp += self.n_bins
    else:
      self.refine_mask.extend([False for i in range(self.n_bins)])

    if self.refine_Asqr_beta:
      self.refine_mask.extend([True for i in range(6)])
      self.nmp += 6
    else:
      self.refine_mask.extend([False for i in range(6)])

    if self.refine_sigmaE_scale:
      self.refine_mask.append(True)
      self.nmp += 1
    else:
      self.refine_mask.append(False)

    if self.refine_sigmaE_bins:
      self.refine_mask.extend([True for i in range(self.n_bins)])
      self.nmp += self.n_bins
    else:
      self.refine_mask.extend([False for i in range(self.n_bins)])

    if self.refine_sigmaE_beta:
      self.refine_mask.extend([True for i in range(6)])
      self.nmp += 6
    else:
      self.refine_mask.extend([False for i in range(6)])

    assert (len(self.refine_mask) == len(self.x))

  def macrocycle_parameter_names(self, full_list=False):
    parameter_names = []
    if full_list or self.refine_Asqr_scale:
      parameter_names.append("Asqr_scale")
    if full_list or self.refine_sigmaT_bins:
      for i in range(self.n_bins):
        parameter_names.append("SigmaT_bin#" + str(i + 1))
    if full_list or self.refine_Asqr_beta:
      parameter_names.append("Asqr_beta11")
      parameter_names.append("Asqr_beta22")
      parameter_names.append("Asqr_beta33")
      parameter_names.append("Asqr_beta12")
      parameter_names.append("Asqr_beta13")
      parameter_names.append("Asqr_beta23")
    if full_list or self.refine_sigmaE_scale:
      parameter_names.append("sigmaE_scale")
    if full_list or self.refine_sigmaE_bins:
      for i in range(self.n_bins):
        parameter_names.append("sigmaE_bin#" + str(i + 1))
    if full_list or self.refine_sigmaE_beta:
      parameter_names.append("sigmaE_beta11")
      parameter_names.append("sigmaE_beta22")
      parameter_names.append("sigmaE_beta33")
      parameter_names.append("sigmaE_beta12")
      parameter_names.append("sigmaE_beta13")
      parameter_names.append("sigmaE_beta23")

    if not full_list:
      assert (len(parameter_names) == self.nmp)
    else:
      assert (len(parameter_names) == len(self.x))
    return parameter_names

  def reparameterize(self):
    i_par = 0 # Keep track of index for unrefined parameters
    repar = []

    if self.refine_Asqr_scale:
      repar.append(Reparams(True,0.))
    i_par += 1

    if self.refine_sigmaT_bins:
      repar.extend([Reparams(True, 0.) for i in range(self.n_bins)])
    i_par += self.n_bins

    if self.refine_Asqr_beta:
      repar.extend([Reparams(False) for i in range(6)])
    i_par += 6

    if self.refine_sigmaE_scale:
      repar.append(Reparams(True,0.))
    i_par += 1

    if self.refine_sigmaE_bins:
      repar.extend([Reparams(True, 0.) for i in range(self.n_bins)])
    i_par += self.n_bins

    if self.refine_sigmaE_beta:
      repar.extend([Reparams(False) for i in range(6)])
    i_par += 6

    assert (i_par == len(self.x))
    assert (len(repar) == self.nmp)

    return repar

  def bounds(self):
    i_par = 0
    bounds_list = []

    if self.refine_Asqr_scale:
      this_bound = Bounds()
      this_bound.lower_on(0.001*self.start_x[i_par])
      bounds_list.append(this_bound)
    i_par += 1

    if self.refine_sigmaT_bins:
      this_bound = Bounds()
      this_bound.lower_on(0.001)
      for i in range(self.n_bins):
        bounds_list.append(this_bound)
    i_par += self.n_bins

    if self.refine_Asqr_beta:
      this_bound = Bounds()
      this_bound.off()
      for i in range(6):
        bounds_list.append(this_bound)
    i_par += 6

    if self.refine_sigmaE_scale:
      this_bound = Bounds()
      this_bound.lower_on(0.01)
      bounds_list.append(this_bound)
    i_par += 1

    if self.refine_sigmaE_bins:
      for i_bin in range(self.n_bins):
        this_bound = Bounds()
        this_bound.lower_on(0.001*self.start_x[i_par+i_bin])
        bounds_list.append(this_bound)
    i_par += self.n_bins

    if self.refine_sigmaE_beta:
      this_bound = Bounds()
      this_bound.off()
      for i in range(6):
        bounds_list.append(this_bound)
    i_par += 6

    assert (i_par == len(self.x))
    assert (len(bounds_list) == self.nmp)
    return bounds_list

  def current_statistics(self, level=3, full_list=False):
    self.log_tab_printf(1, level, "Log-likelihood: %10.6g\n", -self.target())
    self.log_blank(level)

    parameter_names = self.macrocycle_parameter_names(full_list=full_list)
    if full_list:
      self.log_tab(1, level, "All parameters")
    else:
      self.log_tab(1, level, "Refined parameters")
    list_all = (full_list or len(self.refine_mask) == 0)
    iref = 0
    for i in range(len(self.x)):
      if (list_all or self.refine_mask[i]):
        self.log_tab_printf(2, level, "%-15s %10.5g\n", (parameter_names[iref], self.x[i]))
        iref += 1

  def initial_statistics(self):
    level=2
    self.log_blank(level)
    self.log_tab(1, level, "Initial statistics")
    self.current_statistics(level=level, full_list=True)

  def final_statistics(self):
    level=2
    self.log_blank(level)
    self.log_tab(1, level, "Final statistics")
    self.current_statistics(level=level, full_list=True)

  def cleanup(self):
    # Take out overall scale and isotropic B from sigmaT_bins, put into asqr_scale and asqr_beta
    # Take out overall isotropic B from anisotropy in errors, put into bins

    n_bins = self.n_bins
    # Asqr_scale = self.x[0] # Unneeded parameters listed for completeness
    sigmaT_bins = self.x[1:n_bins + 1]
    # Asqr_beta = self.x[n_bins + 1 : n_bins + 7]
    # sigmaE_scale = self.x[n_bins + 7]
    # sigmaE_bins = self.x[n_bins + 8 : 2*n_bins + 8]
    sigmaE_beta = tuple(self.x[2*n_bins + 8 : 2*n_bins + 14])
    sumw = sumwx = sumwa = sumwx2 = sumwxa = 0.
    for i_bin in range(n_bins):
      x = self.ssqr_bins[i_bin]
      d_bin = math.sqrt(x)
      a = math.log(sigmaT_bins[i_bin])
      sigmascale = 0.15 + 0.001 / d_bin**3  # Downweight low resolution as in refinement
      w = 1./sigmascale**2
      sumw += w
      sumwx += w * x
      sumwa += w * a
      sumwx2 += w * x ** 2
      sumwxa += w * x * a

    if self.refine_sigmaT_bins:
      # Make sigmaT_bins values as close as possible to 1 by taking out overall scale
      # and B, and putting them into asqr_scale and asqr_beta terms
      slope_a = (sumw * sumwxa - (sumwx * sumwa)) / (sumw * sumwx2 - sumwx ** 2)
      intercept_a = (sumwa - slope_a * sumwx) / sumw
      scale_a = math.exp(intercept_a)
      deltaB_a = -4 * slope_a
      self.x[0] = self.x[0] * scale_a  # Update overall scale
      for i_bin in range(n_bins): # Take slope out of sigmaT_bins
        self.x[1 + i_bin] = self.x[1 + i_bin] / scale_a * math.exp(deltaB_a * self.ssqr_bins[i_bin] / 4)
      delta_beta_a = list(adptbx.u_iso_as_beta(self.unit_cell,adptbx.b_as_u(deltaB_a)))
      for i_beta in range(6):  # Then put slope into asqr_beta
        self.x[1 + n_bins + i_beta] = self.x[1 + n_bins + i_beta] + delta_beta_a[i_beta]

    if self.refine_sigmaE_beta:
      # Extract isotropic B from sigmaE_beta, put it into sigmaE_bins
      sigmaE_u_cart = adptbx.beta_as_u_cart(self.unit_cell, sigmaE_beta)
      sigmaE_u_iso = adptbx.u_cart_as_u_iso(sigmaE_u_cart)
      sigmaE_delta_beta = list(adptbx.u_iso_as_beta(self.unit_cell, sigmaE_u_iso))
      sigmaE_b_iso = adptbx.u_as_b(sigmaE_u_iso)
      for i_bin in range(n_bins):  # Put isotropic B into bins
        self.x[8 + n_bins + i_bin] = (self.x[8 + n_bins + i_bin] *
            math.exp(-sigmaE_b_iso * self.ssqr_bins[i_bin] / 4))
      for i_beta in range(6):  # Remove isotropic B from sigmaE_beta
        self.x[8 + 2*n_bins + i_beta] = self.x[8 + 2*n_bins + i_beta] - sigmaE_delta_beta[i_beta]

def get_d_star_sq_step(f_array, num_per_bin = 1000, max_bins = 50, min_bins = 6):
  d_spacings = f_array.d_spacings().data()
  num_tot = d_spacings.size()
  n_bins = round(max(min(num_tot / num_per_bin, max_bins),min_bins))
  d_min = flex.min(d_spacings)
  d_max = flex.max(d_spacings)
  d_star_sq_step = (1 / d_min ** 2 - 1 / d_max ** 2) / n_bins
  return d_star_sq_step

def runRefineScalesErrors(mmm, d_min,
    map_1_id="map_manager_1", map_2_id="map_manager_2",
    lower_cart=None, upper_cart=None,
    mask_sphere=True, verbosity=1, prior_params=None):
  # Set up calculation, refine anisotropic scale and error parameters, then
  # produce coefficients for optimal map
  from iotbx.map_model_manager import map_model_manager
  #
  # Start from two maps in map_model_manager plus optional mask specification
  # First get map coefficients for unmodified maps, plus model if present
  ucpars = mmm.map_manager().unit_cell().parameters()
  d_max = max(ucpars[0], ucpars[1], ucpars[2])
  model = mmm.model()

  if (model is None and lower_cart is None): # No masking, full map
    mc1 = mmm.map_as_fourier_coefficients(d_min=d_min, d_max=d_max, map_id=map_1_id)
    mc2 = mmm.map_as_fourier_coefficients(d_min=d_min, d_max=d_max, map_id=map_2_id)
    working_mmm = mmm.deep_copy()
  else:
    cushion = flex.double(3,5.) # 5A cushion
    if model is not None:
      # Define box enclosing model including default box_cushion
      sites_cart = model.get_sites_cart()
      cart_min = flex.double(sites_cart.min()) - cushion
      cart_max = flex.double(sites_cart.max()) + cushion

      if mask_sphere: # Replace box with cubic box enclosing sphere containing model
        model_midpoint = (cart_max + cart_min) / 2
        dsqrmax = flex.max(  (sites_cart - tuple(model_midpoint)).norms() )**2
        model_radius = math.sqrt(dsqrmax)
        cart_min_sphere = model_midpoint - flex.double(3,model_radius)
        cart_max_sphere = model_midpoint + flex.double(3,model_radius)
        for i in range(3): # Paranoia: ensure model box is enclosed in box around sphere
          cart_min[i] = min(cart_min[i], cart_min_sphere[i])
          cart_max[i] = max(cart_max[i], cart_max_sphere[i])

    else:  # Box defined explicitly
      # Maps may be shifted from absolute origin
      shift_cart = flex.double(mmm.shift_cart())
      cart_min = flex.double(lower_cart) + shift_cart
      cart_max = flex.double(upper_cart) + shift_cart
      if mask_sphere:  # Turn box into cube enclosing sphere enclosing box
        body_diagonal = cart_max - cart_min
        radius = flex.sum(body_diagonal**2)**0.5 / 2
        box_midpoint = (cart_max + cart_min) / 2
        cart_min = box_midpoint - flex.double(3,radius)
        cart_max = box_midpoint + flex.double(3,radius)

    # Box the map within xyz bounds, converted to map grid units
    if mask_sphere:
      # Allow room for soft masking of sphere to edge of box
      boundary_to_smoothing_ratio = 2
      soft_mask_radius = d_min
      padding = soft_mask_radius * boundary_to_smoothing_ratio
      cart_min = cart_min - flex.double(3,padding)
      cart_max = cart_max + flex.double(3,padding)

    cs = mmm.crystal_symmetry()
    uc = cs.unit_cell()
    cart_min = tuple(cart_min)
    lower_frac = uc.fractionalize(cart_min)
    cart_max = tuple(cart_max)
    upper_frac = uc.fractionalize(cart_max)
    map_data = mmm.map_data()
    all_orig = map_data.all()
    lower_bounds = [int(math.floor(f * n)) for f, n in zip(lower_frac, all_orig)]
    upper_bounds = [int(math.ceil( f * n)) for f, n in zip(upper_frac, all_orig)]
    working_mmm = mmm.extract_all_maps_with_bounds(lower_bounds=lower_bounds, upper_bounds=upper_bounds)

    if mask_sphere:
      working_mmm.create_spherical_mask(soft_mask_radius=soft_mask_radius,
        boundary_to_smoothing_ratio=boundary_to_smoothing_ratio)
      working_mmm.apply_mask_to_maps(map_ids=[map_1_id, map_2_id], mask_id='mask')
      mask_info = working_mmm.mask_info()
      # Keep track of volume of map for determining relative scale of sigmaE in different subvolumes
      weighted_points = mask_info.size*mask_info.mean # Max-weighted volume in voxels
    else:
      weighted_points = working_mmm.map_data().size()

    mc1 = working_mmm.map_as_fourier_coefficients(d_min=d_min, d_max=d_max, map_id=map_1_id)
    mc2 = working_mmm.map_as_fourier_coefficients(d_min=d_min, d_max=d_max, map_id=map_2_id)
  # End of map boxing section

  # Would like to use bins of equal width in d_star_sq, but this can give orphan
  # data with as little as one reflection per bin for very unequal cell dimensions
  # d_star_sq_step = get_d_star_sq_step(mc1)
  # mc1.setup_binner_d_star_sq_step(d_star_sq_step=d_star_sq_step)

  # Instead use default binning method of equal volumes in reciprocal space, but
  # choose number of bins to get about 1000 reflections per bin, within limits
  num_data = mc1.size()
  num_per_bin = 1000
  max_bins = 50
  min_bins = 6
  n_bins = int(round(max(min(num_data / num_per_bin, max_bins), min_bins)))
  mc1.setup_binner(n_bins=n_bins)
  mc2.use_binner_of(mc1)
  ssqmin = flex.min(mc1.d_star_sq().data())
  ssqmax = flex.max(mc1.d_star_sq().data())

  # Initialise parameters.  This requires slope and intercept of Wilson plot,
  # plus mapCC per bin.
  ssqr_bins = flex.double()
  meanfsq_bins = flex.double()
  mapCC_bins = flex.double()
  sumw = sumwx = sumwy = sumwx2 = sumwxy = 0.
  for i_bin in mc1.binner().range_used():
    sel = mc1.binner().selection(i_bin)
    mc1sel = mc1.select(sel)
    mc2sel = mc2.select(sel)
    mapCC = mc1sel.map_correlation(other=mc2sel)
    assert (mapCC < 1.) # Ensure these are really independent half-maps
    mapCC = max(mapCC,0.001) # Avoid zero or negative values
    mapCC_bins.append(mapCC)
    ssqr = mc1sel.d_star_sq().data()
    x = flex.mean_default(ssqr, 0) # Mean 1/d^2 for bin
    ssqr_bins.append(x)  # Save for later
    fsq = flex.pow2(flex.abs(mc1sel.data()))
    meanfsq = flex.mean_default(fsq, 0)
    meanfsq_bins.append(meanfsq)
    y = math.log(meanfsq)
    w = fsq.size()
    sumw += w
    sumwx += w * x
    sumwy += w * y
    sumwx2 += w * x**2
    sumwxy += w * x * y

  slope = (sumw * sumwxy - (sumwx * sumwy)) / (sumw * sumwx2 - sumwx**2)
  intercept = (sumwy - slope * sumwx) / sumw
  wilson_scale_intensity = math.exp(intercept)
  wilson_b_intensity = -4 * slope
  n_bins = ssqr_bins.size()

  if (prior_params is not None):
    if d_min < 0.99*math.sqrt(1./prior_params['ssqmax']):
      print("Requested resolution is higher than prior parameters support")
      exit

    from scipy import interpolate
    ssqr_prior = tuple(prior_params['ssqr_bins'])
    sigmaT_prior = tuple(prior_params['sigmaT_bins'])
    sigmaE_prior = tuple(prior_params['sigmaE_bins'])
    sTinterp = interpolate.interp1d(ssqr_prior,sigmaT_prior,fill_value="extrapolate")
    sEinterp = interpolate.interp1d(ssqr_prior,sigmaE_prior,fill_value="extrapolate")
    sigmaT_bins = flex.double(sTinterp(ssqr_bins))
    # Start sigmaE_scale at 1 after rescaling sigmaE_bins by volume comparison.
    # This is then refined because of uncertainty in weighting of volume and
    # also about whether masking might have been applied to the periphery of the
    # map used to obtained prior parameters.
    sigmaE_scale = 1.
    sigmaE_bins = flex.double(sEinterp(ssqr_bins))*(weighted_points/prior_params['weighted_points'])
    sigmaE_baniso = prior_params['sigmaE_baniso']
    sigmaE_beta = adptbx.u_star_as_beta(adptbx.u_cart_as_u_star(mc1.unit_cell(),adptbx.b_as_u(sigmaE_baniso)))
  else:
    sigmaT_bins = [1.]*n_bins  # SigmaT_bins correction term for BEST in SigmaT
    sigmaE_scale = 1. # Fix at 1
    sigmaE_bins = []
    for i_bin in range(n_bins):
      sigmaE = meanfsq_bins[i_bin] * (1.-mapCC_bins[i_bin])
      sigmaE_bins.append(sigmaE)  # Error bin parameter
    sigmaE_beta = list(adptbx.u_iso_as_beta(mc1.unit_cell(), 0.))

  start_params = []
  start_params.append(wilson_scale_intensity/3.5) # Asqr_scale, factor out low-res BEST value
  start_params.extend(sigmaT_bins)
  wilson_u=adptbx.b_as_u(wilson_b_intensity)
  asqr_beta=list(adptbx.u_iso_as_beta(mc1.unit_cell(), wilson_u))
  start_params.extend(asqr_beta)
  start_params.append(sigmaE_scale)
  start_params.extend(sigmaE_bins)
  start_params.extend(sigmaE_beta)

  # create inputs for the minimizer's run method
  if (prior_params is not None):
    macro = ["Eprior"]        # protocol: fix error terms using prior
  else:
    macro = ["default"]       # protocol: refine sigmaE terms too
  protocol = [macro, macro]   # overall minimization protocol
  ncyc = 50                   # maximum number of microcycles per macrocycle
  minimizer_type = "bfgs"     # minimizer, bfgs or newton
  study_params = False        # flag for calling studyparams procedure
  level=verbosity      # 0/1/2/3/4 for mute/log/verbose/debug/testing

  # create instances of refine and minimizer
  refineScalesErrors = RefineScalesErrors(mc1=mc1, mc2=mc2,
                       ssqr_bins = ssqr_bins, start_x=start_params)
  minimizer = Minimizer(output_level=level)

  # Run minimizer
  minimizer.run(refineScalesErrors, protocol, ncyc, minimizer_type, study_params)
  refined_params=refineScalesErrors.x

  # Extract and report refined parameters
  i_par = 0
  asqr_scale = refined_params[i_par] # Not used for correction: leave map on original scale
  i_par += 1
  sigmaT_bins = refined_params[i_par:i_par + n_bins]
  i_par += n_bins
  asqr_beta = tuple(refined_params[i_par:i_par + 6])
  i_par += 6
  sigmaE_scale = refined_params[i_par] # Used here but not saved later
  i_par += 1
  sigmaE_bins = list(sigmaE_scale * flex.double(refined_params[i_par:i_par + n_bins]))
  i_par += n_bins
  sigmaE_beta = tuple(refined_params[i_par:i_par + 6])
  i_par += 6
  assert (i_par == len(refined_params))

  # Convert asqr_beta to a_beta for application in weights
  a_beta = tuple(flex.double(asqr_beta)/2)

  if verbosity > 0:
    print("\nRefinement of scales and error terms completed\n")
    print("\nParameters for A and BEST curve correction")
    print("  A overall scale: ",math.sqrt(asqr_scale))
    for i_bin in range(n_bins):
      print("  Bin #", i_bin + 1, "BEST curve correction: ", sigmaT_bins[i_bin])
    print("  A tensor as beta:", a_beta)
    a_baniso = adptbx.u_as_b(adptbx.beta_as_u_cart(mc1.unit_cell(), a_beta))
    print("  A tensor as Baniso: ", a_baniso)
    es = adptbx.eigensystem(a_baniso)
    a_beta_ev = es.vectors
    print("  Eigenvalues and eigenvectors:")
    for iv in range(3):
      print("  ",es.values()[iv],es.vectors(iv))

    print("\nParameters for SigmaE")
    if (prior_params is not None):
      print("  SigmaE scale applied to prior bins:", sigmaE_scale)
    for i_bin in range(n_bins):
      print("  Bin #", i_bin + 1, "SigmaE base: ", sigmaE_bins[i_bin])
    print("  SigmaE tensor as beta:", sigmaE_beta)
    sigmaE_baniso = adptbx.u_as_b(adptbx.beta_as_u_cart(mc1.unit_cell(), sigmaE_beta))
    print("  SigmaE tensor as Baniso (intensity scale): ", sigmaE_baniso)
    es = adptbx.eigensystem(sigmaE_baniso)
    sigmaE_beta_ev = es.vectors
    print("  Eigenvalues and eigenvectors:")
    for iv in range(3):
      print("  ",es.values()[iv],es.vectors(iv))

  # Loop over bins to compute and apply map weighting
  mcmean = mc1.customized_copy(data = (mc1.data() + mc2.data())/2)
  i_bin_used = 0 # Keep track in case full range of bins not used
  if verbosity > 0:
    print("MapCC before and after rescaling as a function of resolution")
    print("Bin   <ssqr>   mapCC_before   mapCC_after")
  for i_bin in mc1.binner().range_used():
    sel = mc1.binner().selection(i_bin)
    mc1sel = mc1.select(sel)

    # Make Miller array as basis for computing aniso corrections in this bin
    ones_array = flex.double(mc1sel.size(), 1)
    all_ones = mc1sel.customized_copy(data=ones_array)
    beta_a_miller = all_ones.apply_debye_waller_factors(u_star=adptbx.beta_as_u_star(a_beta))
    beta_sE_miller = all_ones.apply_debye_waller_factors(u_star=adptbx.beta_as_u_star(sigmaE_beta))

    # Sharpen map coefficients on original scale (ignore A overall scale)
    abeta_terms = beta_a_miller.data() # Anisotropy correction per reflection
    sigmaE_terms = sigmaE_bins[i_bin_used] * beta_sE_miller.data()

    # SigmaT is obtained from BEST curve value times sigmaT_bins BEST curve correction factor
    sigmaT = sigmaT_bins[i_bin_used] * best_curve(ssqr_bins[i_bin_used])
    # scale_terms = 1./(abeta_terms + sigmaE_terms / (2 * asqr_scale * sigmaT * abeta_terms))
    scale_terms = (1./math.sqrt(sigmaT_bins[i_bin_used]))/(abeta_terms + sigmaE_terms / (2 * asqr_scale * sigmaT * abeta_terms))
    mcmean.data().set_selected(sel, mcmean.data().select(sel) * scale_terms)

    if (verbosity > 0):
      # Apply corrections to mc1 and mc2 to compute mapCC after rescaling
      # SigmaE variance is twice as large for half-maps before averaging
      scale_terms_12 = 1./(abeta_terms + sigmaE_terms / (asqr_scale * sigmaT * abeta_terms))
      mc1.data().set_selected(sel, mc1.data().select(sel) * scale_terms_12)
      mc2.data().set_selected(sel, mc2.data().select(sel) * scale_terms_12)
      mc1sel = mc1.select(sel)
      mc2sel = mc2.select(sel)
      mapCC = mc1sel.map_correlation(other=mc2sel)

      print(i_bin_used+1, ssqr_bins[i_bin_used], mapCC_bins[i_bin_used], mapCC)

    i_bin_used += 1

  # Add weighted map then extract all maps into final box
  working_mmm.add_map_from_fourier_coefficients(mcmean, map_id='map_manager')
  working_mmm.remove_map_manager_by_id(map_1_id)
  working_mmm.remove_map_manager_by_id(map_2_id)

  if model is not None:
    box_mmm = working_mmm.extract_all_maps_around_model()
  elif lower_cart is not None:
    cs = working_mmm.crystal_symmetry()
    uc = cs.unit_cell()
    shift_cart = flex.double(working_mmm.shift_cart())
    lower_shifted = flex.double(lower_cart) + shift_cart
    upper_shifted = flex.double(upper_cart) + shift_cart
    lower_frac = uc.fractionalize(tuple(lower_shifted))
    upper_frac = uc.fractionalize(tuple(upper_shifted))
    weighted_map_data = working_mmm.map_data()
    all_orig = weighted_map_data.all()
    lower_bounds = [int(round(math.floor(f * n ))) for f, n in zip(lower_frac, all_orig)]
    upper_bounds = [int(round(math.ceil(f * n))) for f, n in zip(upper_frac, all_orig)]
    box_mmm = working_mmm.extract_all_maps_with_bounds(
      lower_bounds=lower_bounds, upper_bounds=upper_bounds)
  else:
    box_mmm = working_mmm

  resultsdict = dict(
    n_bins = n_bins,
    ssqr_bins = ssqr_bins,
    ssqmin = ssqmin,
    ssqmax = ssqmax,
    weighted_points = weighted_points,
    asqr_scale = asqr_scale,
    sigmaT_bins = sigmaT_bins,
    asqr_beta = asqr_beta,
    sigmaE_bins = sigmaE_bins,
    sigmaE_baniso = sigmaE_baniso)
  return group_args(
    box_mmm = box_mmm,
    resultsdict = resultsdict)


# Command-line interface using argparse

def run():
  import argparse
  import pickle
  from iotbx.map_model_manager import map_model_manager
  from iotbx.data_manager import DataManager     # load in DataManager
  dm = DataManager()  # Get an initialized version as dm
  dm.set_overwrite(True)

  parser = argparse.ArgumentParser(description=
                    'Likelihood-based anisotropic map sharpening')
  parser.add_argument('map1',help='Map file for half-map 1')
  parser.add_argument('map2', help='Map file for half-map 2')
  parser.add_argument('d_min', help='d_min for maps', type=float)
  parser.add_argument('--file_root', help='Root of filenames for output map and parameters')
  parser.add_argument('--read_params', help='Filename for prior parameters')
  parser.add_argument('--write_params', help='Write out refined parameters', action='store_true')
  parser.add_argument('--model', help='Optional model to define map region and use for mapCC test')
  parser.add_argument('--mask', default='sphere', choices=('sphere', 'none'),
                    help = 'Mask with sphere around box, or none')
  parser.add_argument('--lower_cart',help='x, y and z lower bounds for output map', nargs="+", type=float)
  parser.add_argument('--upper_cart',help='x, y and z upper bounds for output map', nargs="+", type=float)
  parser.add_argument('-n', '--nowrite', dest = 'nowrite',
                    help = 'Do not write map file', action = 'store_true')
  parser.add_argument('-m', '--mute', dest = 'mute',
                    help = 'Mute output', action = 'store_true')
  parser.add_argument('-v', '--verbose', dest = 'verbose',
                    help = 'Set output as verbose', action = 'store_true')
  parser.add_argument('--testing', dest = 'testing',
                    help='Set output as testing', action='store_true')
  # parser.epilog('''Optionally define map target region by model or lower/upper
  #   xyz bounds but not both.''')
  args = parser.parse_args()
  d_min = args.d_min
  verbosity = 1
  if args.mute: verbosity = 0
  if args.verbose: verbosity = 2
  if args.testing: verbosity = 4

  model = None
  lower_cart = None
  upper_cart = None
  mask_sphere = (args.mask == 'sphere')
  if args.model is not None:
    assert (args.lower_cart is None) and (args.upper_cart is None)
    model_file = args.model
    model = dm.get_model(model_file)
  elif args.lower_cart is not None:
    assert args.upper_cart is not None
    assert (len(args.lower_cart) == 3) and (len(args.upper_cart) == 3)
    lower_cart = tuple(args.lower_cart)
    upper_cart = tuple(args.upper_cart)
  else:
    assert(mask_sphere == None) # Requires either model or bounds

  # Create map_model_manager containing half-maps
  map1_filename = args.map1
  mm1 = dm.get_real_map(map1_filename)
  map2_filename = args.map2
  mm2 = dm.get_real_map(map2_filename)
  mmm = map_model_manager(model=model, map_manager_1=mm1, map_manager_2=mm2)

  # Get prior parameters if provided
  if (args.read_params is not None):
    infile = open(args.read_params,"rb")
    prior_params = pickle.load(infile)
    infile.close()
  else:
    prior_params = None

  # Run refinement of scales and errors
  results = runRefineScalesErrors(mmm, d_min,
    lower_cart=lower_cart, upper_cart=upper_cart,
    mask_sphere=mask_sphere, verbosity=verbosity,
    prior_params=prior_params)
  mmm_new = results.box_mmm

  if (model is not None):
    # mmm_new.add_model_by_id(model,'model')
    mapCC = mmm_new.map_model_cc(map_id='map_manager',resolution=d_min)
    print ("CC of B=0 model with weighted map:",mapCC)

  if not args.nowrite:
    print ("Writing map")
    if (args.file_root is not None):
      mapout_file_name = args.file_root + ".map"
    else:
      mapout_file_name = "weighted_map.map"
    mmm_new.write_map(map_id='map_manager',file_name=mapout_file_name)
    # mtz_dataset = mcmean.as_mtz_dataset(column_root_label='FWT')
    # mtz_object=mtz_dataset.mtz_object()
    # dm.write_miller_array_file(mtz_object, filename="weighted_map_coeffs.mtz")
  if args.write_params:
    resultsdict = results.resultsdict
    paramsfile = args.file_root + ".pickle"
    outf = open(paramsfile,"wb")
    pickle.dump(resultsdict,outf,2)
    outf.close()

if (__name__ == "__main__"):
  run()


 *******************************************************************************


 *******************************************************************************
cctbx/maptbx/refine_sharpening.py
"""Helper tools for auto-sharpening"""
from __future__ import absolute_import, division, print_function

import sys,os
from libtbx.utils import Sorry
from cctbx.array_family import flex
from copy import deepcopy
from libtbx import group_args
from libtbx.utils import null_out

import scitbx.lbfgs
import math
from cctbx.maptbx.segment_and_split_map import map_and_b_object
from six.moves import range
from six.moves import zip
from scitbx import matrix
from cctbx import adptbx

def amplitude_quasi_normalisations(ma, d_star_power=1, set_to_minimum=None,
    pseudo_likelihood=False):
    """Calculations for normalizing miller arrays
    for pseudo-likelihood calculation"""
    epsilons = ma.epsilons().data().as_double()
    mean_f_sq_over_epsilon = flex.double()
    for i_bin in ma.binner().range_used():
      sel = ma.binner().selection(i_bin)
      if pseudo_likelihood:
        sel_f_sq = flex.pow2(ma.data().select(sel)) # original method used
      else: # usual
        sel_f_sq = ma.data().select(sel)
      if (sel_f_sq.size() > 0):
        sel_epsilons = epsilons.select(sel)
        sel_f_sq_over_epsilon = sel_f_sq / sel_epsilons
        mean_f_sq_over_epsilon.append(flex.mean(sel_f_sq_over_epsilon))
      else:
        mean_f_sq_over_epsilon.append(0)
    mean_f_sq_over_epsilon_interp = ma.binner().interpolate(
      mean_f_sq_over_epsilon, d_star_power)
    if set_to_minimum and not mean_f_sq_over_epsilon_interp.all_gt(0):
      # HACK NO REASON THIS SHOULD WORK BUT IT GETS BY THE FAILURE
      sel = (mean_f_sq_over_epsilon_interp <= set_to_minimum)
      mean_f_sq_over_epsilon_interp.set_selected(sel,-mean_f_sq_over_epsilon_interp)
      sel = (mean_f_sq_over_epsilon_interp <= set_to_minimum)
      mean_f_sq_over_epsilon_interp.set_selected(sel,set_to_minimum)
    assert mean_f_sq_over_epsilon_interp.all_gt(0)
    from cctbx.miller import array
    return array(ma, flex.sqrt(mean_f_sq_over_epsilon_interp))
    # XXX was below before 2017-10-25
    # return array(ma, mean_f_sq_over_epsilon_interp)

def quasi_normalize_structure_factors(ma, d_star_power=1, set_to_minimum=None,
     pseudo_likelihood=False):
    """Normalize miller arrays for likelihood calculation"""
    normalisations = amplitude_quasi_normalisations(ma, d_star_power,
       set_to_minimum=set_to_minimum,pseudo_likelihood=pseudo_likelihood)
    if pseudo_likelihood:
      print("Norms:")
      for n,d in zip(normalisations[:100],ma.data()[:100]): print(n,d)

    q = ma.data() / normalisations.data()
    from cctbx.miller import array
    return array(ma, q)

def get_array(file_name=None,labels=None):
  """Read from a reflection file. Used in testing these tools"""
  print("Reading from %s" %(file_name))
  from iotbx import reflection_file_reader
  reflection_file = reflection_file_reader.any_reflection_file(
       file_name=file_name)
  array_to_use=None
  if labels:
    for array in reflection_file.as_miller_arrays():
      if ",".join(array.info().labels)==labels:
        array_to_use=array
        break
  else:
    for array in reflection_file.as_miller_arrays():
      if array.is_complex_array() or array.is_xray_amplitude_array() or\
          array.is_xray_intensity_array():
        array_to_use=array
        break
  if not array_to_use:
    text=""
    for array in reflection_file.as_miller_arrays():
      text+=" %s " %(",".join(array.info().labels))

    raise Sorry("Cannot identify array to use...possibilities: %s" %(text))

  print("Using the array %s" %(",".join(array_to_use.info().labels)))
  return array_to_use


def get_amplitudes(args):
  """Read in map coefficients or amplitudes and sharpen, used in testing"""
  if not args or 'help' in args or '--help' in args:
    print("\nsharpen.py")
    print("Read in map coefficients or amplitudes and sharpen")
    return

  new_args=[]
  file_name=None
  for arg in args:
    if os.path.isfile(arg) and arg.endswith(".mtz"):
      file_name=arg
    else:
      new_args.append(arg)
  args=new_args
  labels=None

  array_list=[]

  array_list.append(get_array(file_name=file_name,labels=labels))
  array=array_list[-1]
  phases=None
  assert array.is_complex_array()
  return array


def get_effective_b_values(d_min_ratio=None,resolution_dependent_b=None,
    resolution=None):
  """Return effective b values at sthol2_1 2 and 3
  see adjust_amplitudes_linear below """

  d_min=resolution*d_min_ratio
  sthol2_2=0.25/resolution**2
  sthol2_1=sthol2_2*0.5
  sthol2_3=0.25/d_min**2
  b1=resolution_dependent_b[0]
  b2=resolution_dependent_b[1]
  b3=resolution_dependent_b[2]

  b3_use=b3+b2

  res_1=(0.25/sthol2_1)**0.5
  res_2=(0.25/sthol2_2)**0.5
  res_3=(0.25/sthol2_3)**0.5

  #  Scale factor is exp(b3_use) at res_3 for example
  #  f=exp(-b sthol2)
  #  b= - ln(f)/sthol2
  b1=-b1/sthol2_1
  b2=-b2/sthol2_2
  b3_use=-b3_use/sthol2_3


  return [res_1,res_2,res_3],[b1,b2,b3_use]

def adjust_amplitudes_linear(f_array,b1,b2,b3,resolution=None,
    d_min_ratio=None):
  """do something to the amplitudes.
  b1=delta_b at midway between d=inf and d=resolution,b2 at resolution,
  b3 at d_min (added to b2)
  pseudo-B at position of b1= -b1/sthol2_2= -b1*4*resolution**2
  or...b1=-pseudo_b1/(4*resolution**2)
  typical values of say b1=1 at 3 A -> pseudo_b1=-4*9=-36 """

  data_array=f_array.data()
  sthol2_array=f_array.sin_theta_over_lambda_sq()
  scale_array=flex.double()
  import math
  #d_min=f_array.d_min()
  #if resolution is None: resolution=d_min
  d_min=d_min_ratio*resolution

  sthol2_2=0.25/resolution**2
  sthol2_1=sthol2_2*0.5
  sthol2_3=0.25/d_min**2
  b0=0.0
  d_spacings=f_array.d_spacings()
  b3_use=b3+b2
  for x,(ind,sthol2),(ind1,d) in zip(data_array,sthol2_array,d_spacings):
      if sthol2 > sthol2_2:
        value=b2+(sthol2-sthol2_2)*(b3_use-b2)/(sthol2_3-sthol2_2)
      elif sthol2 > sthol2_1:
        value=b1+(sthol2-sthol2_1)*(b2-b1)/(sthol2_2-sthol2_1)
      else:
        value=b0+(sthol2-0.)*(b1-b0)/(sthol2_1-0.)
      scale_array.append(math.exp(value))
  data_array=data_array*scale_array
  return f_array.customized_copy(data=data_array)

def get_model_map_coeffs_normalized(pdb_hierarchy=None,
   si=None,
   f_array=None,
   overall_b=None,
   resolution=None,
   n_bins=None,
   target_b_iso_model_scale=0,
   target_b_iso_ratio = 5.9,  # empirical, see params for segment_and_split_map
   out=sys.stdout):
  """define Wilson B for the model, generate model map coeffs,
      and normalize the model-map coefficients"""
  if not pdb_hierarchy: return None
  if not si:
    from cctbx.maptbx.segment_and_split_map import sharpening_info
    si=sharpening_info(resolution=resolution,
     target_b_iso_model_scale=0,
     target_b_iso_ratio = target_b_iso_ratio,
     n_bins=n_bins)
  if overall_b is None:
    if si.resolution:
      overall_b=si.get_target_b_iso()*si.target_b_iso_model_scale
    else:
      overall_b=0
    print("Setting Wilson B = %5.1f A" %(overall_b), file=out)

  # create model map using same coeffs
  from cctbx.maptbx.segment_and_split_map import get_f_phases_from_model
  try:
    model_map_coeffs=get_f_phases_from_model(
     pdb_hierarchy=pdb_hierarchy,
     f_array=f_array,
     overall_b=overall_b,
     k_sol=si.k_sol,
     b_sol=si.b_sol,
     out=out)
  except Exception as e:
    print ("Failed to get model map coeffs...going on",file=out)
    return None


  from cctbx.maptbx.segment_and_split_map import map_coeffs_as_fp_phi,get_b_iso
  model_f_array,model_phases=map_coeffs_as_fp_phi(model_map_coeffs)
  (d_max,d_min)=f_array.d_max_min(d_max_is_highest_defined_if_infinite=True)
  model_f_array.setup_binner(n_bins=si.n_bins,d_max=d_max,d_min=d_min)

  # Set overall_b....
  final_b_iso=get_b_iso(model_f_array,d_min=resolution)
  print("Effective b_iso of "+\
     "adjusted model map:  %6.1f A**2" %(final_b_iso), file=out)
  model_map_coeffs_normalized=model_f_array.phase_transfer(
     phase_source=model_phases,deg=True)
  return model_map_coeffs_normalized

def get_b_eff(si=None,out=sys.stdout):
  """Get value of effective B factor from si object"""
  if si.rmsd is None:
    b_eff=None
  else:
    b_eff=8*3.14159*si.rmsd**2
    print("Setting b_eff for fall-off at %5.1f A**2 based on model uncertainty of %5.1f A" \
       %( b_eff,si.rmsd), file=out)
  return b_eff

def cc_fit(s_value_list=None,scale=None,value_zero=None,baseline=None,
     scale_using_last=None):
  """Fit CC values to simple exponential.
    For scale_using_last, fix final value at zero"""
  fit=flex.double()
  s_zero=s_value_list[0]
  for s in s_value_list:
    fit.append(value_zero*math.exp(-scale*(s-s_zero)))
  if scale_using_last:
    fit=fit-fit[-1]
  return fit

def get_baseline(scale=None,scale_using_last=None,max_cc_for_rescale=None):
  """Estimate baseline for exponential fit"""
  if not scale_using_last:
    return 0
  else:
    baseline=min(0.99,max(0.,scale[-scale_using_last:].min_max_mean().mean))
    if baseline > max_cc_for_rescale:
       return None
    else:
       return baseline

def fit_cc(cc_list=None,s_value_list=None,
    scale_min=None,scale_max=None,n_tries=None,scale_using_last=None):
  """find value of scale in range scale_min,scale_max that minimizes rms diff
  between cc_list and cc_list[0]*exp(-scale*(s_value-s_value_list[0]))
  for scale_using_last, require it to go to zero at end """

  best_scale=None
  best_rms=None
  for i in range(n_tries):
    scale=scale_min+(scale_max-scale_min)*i/n_tries
    fit=cc_fit(s_value_list=s_value_list,scale=scale,value_zero=cc_list[0],
       scale_using_last=scale_using_last)
    fit=fit-cc_list
    rms=fit.rms()
    if best_rms is None or rms<best_rms:
      best_rms=rms
      best_scale=scale
  return cc_fit(s_value_list=s_value_list,scale=best_scale,value_zero=cc_list[0],
      scale_using_last=scale_using_last)

def get_fitted_cc(cc_list=None,s_value_list=None, cc_cut=None,
   scale_using_last=None,keep_cutoff_point=False,force_scale_using_last=False,
   cutoff_after_last_high_point = False):
  """Alternative fit to CC values.
  only do this if there is some value of s where cc is at least 2*cc_cut or
  (1-c_cut/2), whichever is smaller. """

  min_cc=min(2*cc_cut,1-0.5*cc_cut)
  if cc_list.min_max_mean().max < min_cc and (not force_scale_using_last):
    return cc_list
  # 2020-10-08 instead last point where cc >=cc_cut cutoff_after_last_high_point
  # find first point after point where cc>=min_cc that cc<=cc_cut
  #   then back off by 1 point  # 2019-10-12 don't back off if keep_cutoff_point
  found_high=False
  s_cut=None
  i_cut=0
  if (not cutoff_after_last_high_point):
    for s,cc in zip(s_value_list,cc_list):
      if cc > min_cc:
        found_high=True
      if found_high and cc < cc_cut:
        s_cut=s
        break
      i_cut+=1
  else:
    ii=0
    for s,cc in zip(s_value_list,cc_list):
      if cc > min_cc:
        found_high=True
      if found_high and cc >= cc_cut:
        s_cut = s
        i_cut = ii
      ii += 1

  if force_scale_using_last:
    scale_using_last=True
    s_cut=s_value_list[0]
    i_cut=1
  if s_cut is None or i_cut==0:
    return cc_list

  if keep_cutoff_point:
    i_cut=max(1,i_cut-1)

  #Fit remainder
  s_value_remainder_list=s_value_list[i_cut:]
  cc_remainder_list=cc_list[i_cut:]
  n=cc_remainder_list.size()
  scale_min=10 # soft
  scale_max=500 # hard
  n_tries=200

  fitted_cc_remainder_list=fit_cc(
     cc_list=cc_remainder_list,s_value_list=s_value_remainder_list,
     scale_min=scale_min,scale_max=scale_max,n_tries=n_tries,
     scale_using_last=scale_using_last)

  new_cc_list=cc_list[:i_cut]
  new_cc_list.extend(fitted_cc_remainder_list)
  return new_cc_list

def estimate_cc_star(cc_list=None,s_value_list=None, cc_cut=None,
    scale_using_last=None,
    keep_cutoff_point=False):
  """Estimate value of CC* (True map correlation)
  cc ~ sqrt(2*half_dataset_cc/(1+half_dataset_cc))
  however for small cc the errors are very big and we think cc decreases
  rapidly towards zero once cc is small
  So find value of s_value_zero that gives cc about cc_cut...for
  s_value >s_value_zero use fit of cc_zero * exp(-falloff*(s_value-s_value_zero))
  for scale_using_last set: subtract off final values so it goes to zero."""

  fitted_cc=get_fitted_cc(
    cc_list=cc_list,s_value_list=s_value_list,cc_cut=cc_cut,
    scale_using_last=scale_using_last,
    keep_cutoff_point=keep_cutoff_point)

  cc_star_list=flex.double()
  for cc in fitted_cc:
     cc=max(cc,0.)
     cc_star=(2.*cc/(1.+cc))**0.5
     cc_star_list.append(cc_star)
  return cc_star_list


def rescale_cc_list(cc_list=None,scale_using_last=None,
    max_cc_for_rescale=None):
  """Rescale CC values by replacing cc with (cc-baseline)/(1-baseline)"""
  baseline=get_baseline(scale=cc_list,
      scale_using_last=scale_using_last,
      max_cc_for_rescale=max_cc_for_rescale)
  if baseline is None:
     return cc_list,baseline

  scaled_cc_list=flex.double()
  for cc in cc_list:
    scaled_cc_list.append((cc-baseline)/(1-baseline))
  return scaled_cc_list,baseline

def get_calculated_scale_factors(
      s_value_list=None,
      effective_b=None,
      b_zero = None,
      cc_list = None,
      dv = None,
      uc = None):
    """Calculate expected scale factors from s_value_list and cc_list"""
    recip_space_vectors = flex.vec3_double()
    scale_values = flex.double()
    original_scale_values = flex.double()
    indices = flex.miller_index()
    if effective_b is None and not cc_list:
      #no info
      return group_args(
        indices = indices,
        scale_values = scale_values,
        original_scale_values = original_scale_values)

    for s,cc in zip(s_value_list,cc_list):
      d = 1/s
      sthol2 = 0.25/d**2
      recip_space_vectors.append(matrix.col(dv) * s)
      if effective_b is not None:
        scale_values.append(b_zero*math.exp (max(-20.,min(20.,
         -effective_b * sthol2))))
      else:
        scale_values.append(cc)

      original_scale_values.append(cc)
    indices = flex.miller_index(tuple(get_nearest_lattice_points(
        uc,recip_space_vectors)))
    return group_args(
      indices = indices,
      scale_values = scale_values,
      original_scale_values = original_scale_values)

def get_rms_fo_values(fo = None, direction_vectors = None, d_avg = None,
      apply_aniso_dict_by_dv = None,
       aniso_scale_factor_array = None,
     i_bin = None,
     first_bin = None):
    '''Estimate rms Fo values along direction vectors.
       For first bin everything is just the average'''

    abs_fo_data = flex.abs(fo.data())
    sqr_fo = fo.customized_copy(data=flex.pow2(abs_fo_data))
    rms_fo= sqr_fo.data().min_max_mean().mean**0.5

    if first_bin:
      return flex.double(direction_vectors.size(), rms_fo)

    if aniso_scale_factor_array:
      abs_fo = fo.customized_copy(
        data = abs_fo_data * aniso_scale_factor_array.data())
    else:
      abs_fo = fo.customized_copy(data = abs_fo_data)
    sar = shell_aniso_refinery(abs_fo)
    sar.run()
    # Now apply the values to a dummy array with indices for our dv vectors
    recip_space_vectors = flex.vec3_double()
    apply_aniso_values = flex.double()
    i = -1
    for dv in direction_vectors:
      i+=1
      s = 1/d_avg
      recip_space_vectors.append(matrix.col(dv) * s)
      if  aniso_scale_factor_array:
        apply_aniso_values.append(apply_aniso_dict_by_dv[i][i_bin-1])
    from iotbx.map_model_manager import create_fine_spacing_array
    fo_to_get_values = create_fine_spacing_array( abs_fo.crystal_symmetry().unit_cell())
    indices = flex.miller_index(tuple(get_nearest_lattice_points(
        fo_to_get_values.crystal_symmetry().unit_cell(),recip_space_vectors)))

    fo_to_get_values = fo_to_get_values.customized_copy(indices = indices,
      data = flex.double(indices.size(),1))
    values_array = sar.get_calc_array(
      sar.x,
      array_with_indices=fo_to_get_values)
    # Return estimates of rms F in each direction
    values = values_array.data() * (1./.785)**0.5  # expect: <F>**2/<F**2> = 0.785
    if aniso_scale_factor_array:
       values *= apply_aniso_values
    return values

def calculate_fsc(**kw):
  '''
    Calculate FSC of 2 maps and estimate scale factors
    If direction_vector or direction_vectors supplied, calculate
     CC values weighted by abs() component along direction vector
    If list of direction vectors, return group_args with si objects and
     also include an overall si object
  '''
  si = kw.get('si',None)
  f_array = kw.get('f_array',None)
  map_coeffs = kw.get('map_coeffs',None)
  model_map_coeffs = kw.get('model_map_coeffs',None)
  external_map_coeffs = kw.get('external_map_coeffs',None)
  first_half_map_coeffs = kw.get('first_half_map_coeffs',None)
  second_half_map_coeffs = kw.get('second_half_map_coeffs',None)
  resolution = kw.get('resolution',None)
  n_bins_use = kw.get('n_bins_use',None)
  fraction_complete = kw.get('fraction_complete',None)
  min_fraction_complete = kw.get('min_fraction_complete', 0.05)
  is_model_based = kw.get('is_model_based',None)
  cc_cut = kw.get('cc_cut',None)
  scale_using_last = kw.get('scale_using_last',None)
  max_cc_for_rescale = kw.get('max_cc_for_rescale',None)
  equalize_power = kw.get('equalize_power',False)
  verbose = kw.get('verbose',None)
  maximum_scale_factor = kw.get('maximum_scale_factor',None)
  maximum_ratio = kw.get('maximum_ratio', 25)
  use_dv_weighting = kw.get('use_dv_weighting',None)
  run_analyze_anisotropy = kw.get('run_analyze_anisotropy',None)
  pseudo_likelihood = kw.get('pseudo_likelihood',False)
  skip_scale_factor = kw.get('skip_scale_factor',False)
  optimize_b_eff = kw.get('optimize_b_eff',None)
  direction_vector = kw.get('direction_vector',None)
  direction_vectors = kw.get('direction_vectors',None)
  smooth_fsc = kw.get('smooth_fsc',None)
  cutoff_after_last_high_point = kw.get('cutoff_after_last_high_point',None)
  expected_rms_fc_list = kw.get('expected_rms_fc_list',None)
  expected_ssqr_list = kw.get('expected_ssqr_list',None)
  rmsd_resolution_factor = kw.get('rmsd_resolution_factor',0.25)
  low_res_bins = kw.get('low_res_bins',3)
  low_res_bins = kw.get('low_res_bins',3)
  remove_anisotropy_before_analysis = kw.get(
      'remove_anisotropy_before_analysis',False)
  aniso_scale_factor_array = kw.get( 'aniso_scale_factor_array',None)
  aniso_scale_factor_as_u_cart = kw.get( 'aniso_scale_factor_as_u_cart',None)
  out = kw.get('out',sys.stdout)


  if direction_vectors and direction_vectors != [None]:
     kw['direction_vectors'] = None
     print("Getting overall analysis first",file = out)
     overall_si = calculate_fsc(**kw)
     print("Done with getting overall analysis ",file = out)
  else:
     overall_si = None

  # calculate anticipated fall-off of model data with resolution
  if si.rmsd is None and is_model_based:
    if not rmsd_resolution_factor:
      rmsd_resolution_factor = si.rmsd_resolution_factor
    if not resolution:
      resolution = si.resolution
    si.rmsd=resolution*rmsd_resolution_factor
    print("Setting rmsd to %5.1f A based on resolution of %5.1f A" %(
       si.rmsd,resolution), file=out)
  elif is_model_based:
    if not resolution:
      resolution = si.resolution
    print("RMSD is %5.1f A and resolution is %5.1f A" %(
       si.rmsd,resolution), file=out)

  # get f and model_f vs resolution and FSC vs resolution and apply

  # If external_map_coeffs then simply scale f to external_map_coeffs

  # scale to f_array and return sharpened map
  dsd = f_array.d_spacings().data()
  from cctbx.maptbx.segment_and_split_map import map_coeffs_to_fp

  if is_model_based:
    mc1=map_coeffs
    mc2=model_map_coeffs
    fo_map=map_coeffs # scale map_coeffs to model_map_coeffs*FSC
    fc_map=model_map_coeffs
    b_eff=get_b_eff(si=si,out=out)
  elif external_map_coeffs:
    mc1=map_coeffs
    mc2=external_map_coeffs
    fo_map=map_coeffs # scale map_coeffs to external_map_coeffs
    fc_map=external_map_coeffs
    b_eff=None

  else: # half_dataset
    mc1=first_half_map_coeffs
    mc2=second_half_map_coeffs
    fo_map=map_coeffs # scale map_coeffs to cc*
    fc_map=model_map_coeffs
    b_eff=None

  if not mc1 or not mc2:  # nothing to do
    si.target_scale_factors = None
    return si


  ratio_list=flex.double()
  target_sthol2=flex.double()
  s_value_list=flex.double()
  d_min_list=flex.double()
  rms_fo_list=flex.double()
  rms_fc_list=flex.double()
  max_possible_cc=None
  n_list = flex.double()

  if direction_vectors:
    pass # already ok
  elif direction_vector:
    direction_vectors = [direction_vector]
  else:
    direction_vectors = [None]

  cc_dict_by_dv = {}
  rms_fo_dict_by_dv = {}
  rms_fc_dict_by_dv = {}
  ratio_dict_by_dv = {}
  i = 0
  for dv in direction_vectors:
    cc_dict_by_dv [i] = flex.double()
    rms_fo_dict_by_dv [i] = flex.double()
    rms_fc_dict_by_dv [i] = flex.double()
    ratio_dict_by_dv [i] = flex.double()
    i += 1

  first_bin =True
  weights_para_list = [] # NOTE: this makes N * f_array.size() arrays!!
  for dv in direction_vectors:
    if dv:
      weights_para_list.append(
        get_normalized_weights_para(f_array,direction_vectors, dv,
          include_all_in_lowest_bin = True))
    else:
      weights_para_list.append(None)

  # Set up to work with anisotropy-removed data

  if remove_anisotropy_before_analysis and not aniso_scale_factor_as_u_cart:
   # Get aniso_scale_factor_as_u_cart
    aniso_scale_factor_as_u_cart = get_aniso_scale_info(
      fo_map, resolution = resolution)

  if remove_anisotropy_before_analysis and aniso_scale_factor_as_u_cart and (
     not aniso_scale_factor_array):
    # Get aniso_scale_factor_array

    # Array of scale factors to remove anisotropy
    aniso_scale_factor_array = get_aniso_scale_factor_array(fo_map,
      aniso_scale_factor_as_u_cart)
  if aniso_scale_factor_as_u_cart:
    apply_aniso_dict_by_dv = get_apply_aniso_dict_by_dv(direction_vectors,
       f_array, aniso_scale_factor_as_u_cart)
  else:
    apply_aniso_dict_by_dv = None



  if n_bins_use is None:
    n_bins = len(list(f_array.binner().range_used()))
    n_bins_use = min(n_bins,max(3,n_bins//3))
    set_n_bins_use = True
  else:
    set_n_bins_use = False

  ratio_fo_to_fc_zero = None
  for i_bin in f_array.binner().range_used():
    sel       = f_array.binner().selection(i_bin)
    d         = dsd.select(sel)
    if d.size()<1:
      raise Sorry("Please reduce number of bins (no data in bin "+
        "%s) from current value of %s" %(i_bin,f_array.binner().n_bins_used()))
    d_min     = flex.min(d)
    d_max     = flex.max(d)
    d_avg     = flex.mean(d)

    if set_n_bins_use and i_bin-1 > n_bins_use and (
          (not resolution) or (d_avg >= resolution)):
      n_bins_use = i_bin - 1

    n         = d.size()
    m1        = mc1.select(sel)
    m2        = mc2.select(sel)

    cc = None
    i = 0
    if fc_map:
      fc        = fc_map.select(sel)
    else:
      fc = None
    if fo_map:
          fo        = fo_map.select(sel)
    else:
      fo = None

    if remove_anisotropy_before_analysis and aniso_scale_factor_as_u_cart:
      rms_fo_values = len(direction_vectors) * [None]
    elif (direction_vectors and direction_vectors[0] != None) and (
        aniso_scale_factor_array is not None):
      # Let's fit rms fo in just this shell
      rms_fo_values = get_rms_fo_values(
       fo = fo,
       apply_aniso_dict_by_dv = apply_aniso_dict_by_dv,
       aniso_scale_factor_array = aniso_scale_factor_array.select(sel),
       direction_vectors = direction_vectors,
       d_avg = d_avg,
       i_bin = i_bin,
       first_bin = first_bin)
    else:
      rms_fo_values = len(direction_vectors) * [None]

    for dv, weights_para, rms_fo in zip(direction_vectors, weights_para_list,
         rms_fo_values):
      if dv:
        weights_para_sel = weights_para.select(sel)
        weights_para_sel_sqrt = flex.sqrt(weights_para_sel)
        if remove_anisotropy_before_analysis and aniso_scale_factor_as_u_cart\
            and aniso_scale_factor_array and aniso_scale_factor_array.data():
          scale_sel = aniso_scale_factor_array.data().select(sel)
        else:
          scale_sel = None

        if scale_sel:
          m1a=m1.customized_copy(
             data = m1.data() * weights_para_sel_sqrt * scale_sel)
          m2a=m2.customized_copy(
             data = m2.data() * weights_para_sel_sqrt * scale_sel)
        else: # usual
          m1a=m1.customized_copy(data = m1.data() * weights_para_sel_sqrt)
          m2a=m2.customized_copy(data = m2.data() * weights_para_sel_sqrt)
        cca        = m1a.map_correlation(other = m2a)

        if external_map_coeffs: # only for no direction vectors
          cc=1.
        if cca is None:
          cca=0.
        cc_dict_by_dv[i].append(cca)
        normalization = 1./max(1.e-10,weights_para_sel.rms())
        if scale_sel:
          fo_a = fo.customized_copy(data=fo.data()*weights_para_sel*scale_sel)
          f_array_fo=map_coeffs_to_fp(fo_a)
          rms_fo=normalization * f_array_fo.data().rms() \
              * apply_aniso_dict_by_dv[i][i_bin-1]
        elif (not rms_fo) and fo_map:
          fo_a = fo.customized_copy(data=fo.data()*weights_para_sel)
          f_array_fo=map_coeffs_to_fp(fo_a)
          rms_fo=normalization * f_array_fo.data().rms()
        elif (not rms_fo):
          rms_fo=1.

        if expected_rms_fc_list:
          rms_fc = expected_rms_fc_list[i_bin-1]
        elif fc_map:
          fc_a  = fc.customized_copy(data=fc.data()*weights_para_sel)
          f_array_fc=map_coeffs_to_fp(fc_a)
          rms_fc=normalization *f_array_fc.data().rms()
        else:
          rms_fc=1.
        # Normalize rms_fc to make rms_fc[0] == rms_fo[0]
        if rms_fo and rms_fc and not ratio_fo_to_fc_zero:
          ratio_fo_to_fc_zero = rms_fo/rms_fc
        rms_fc *= (1 if ratio_fo_to_fc_zero is None else ratio_fo_to_fc_zero)


        rms_fo_dict_by_dv[i].append(rms_fo)
        rms_fc_dict_by_dv[i].append(rms_fc)
        ratio_dict_by_dv[i].append(max(1.e-10,rms_fc)/max(1.e-10,rms_fo))

        i += 1
        if (cca is not None) and (cc is None):
          cc = cca # save first one
      else:
        cc        = m1.map_correlation(other = m2)
        if external_map_coeffs: # only for no direction vectors
          cc=1.
        if cc is None:
          cc= 0
        cc_dict_by_dv[i].append(cc)
        if fo_map:
          f_array_fo=map_coeffs_to_fp(fo)
          rms_fo=f_array_fo.data().rms()
        else:
          rms_fo=1.

        if expected_rms_fc_list:
          rms_fc = expected_rms_fc_list[i_bin-1]
        elif fc_map:
          f_array_fc=map_coeffs_to_fp(fc)
          rms_fc=f_array_fc.data().rms()
        else:
          rms_fc=1.4

        # Normalize rms_fc to make rms_fc[0] == rms_fo[0]
        if rms_fo and rms_fc and not ratio_fo_to_fc_zero:
          ratio_fo_to_fc_zero = rms_fo/rms_fc
        rms_fc *= (1 if ratio_fo_to_fc_zero is None else ratio_fo_to_fc_zero)

        rms_fo_dict_by_dv[i].append(rms_fo)
        rms_fc_dict_by_dv[i].append(rms_fc)
        ratio_dict_by_dv[i].append(max(1.e-10,rms_fc)/max(1.e-10,rms_fo))


    sthol2=0.25/d_avg**2 # note this is 0.25 * s_value**2
    target_sthol2.append(sthol2)
    s_value_list.append(1/d_avg)
    d_min_list.append(d_min)
    n_list.append(m1.size())


    if b_eff is not None:
      max_cc_estimate=cc* math.exp(min(20.,sthol2*b_eff))
    else:
      max_cc_estimate=cc
    max_cc_estimate=max(0.,min(1.,max_cc_estimate))
    if max_possible_cc is None or (
        max_cc_estimate > 0 and max_cc_estimate > max_possible_cc):
      max_possible_cc=max_cc_estimate
    if verbose:
      print("d_min: %5.1f  FC: %7.1f  FOBS: %7.1f   CC: %5.2f" %(
      d_avg,rms_fc,rms_fo,cc), file=out)
    first_bin = False

  input_info = group_args(
     f_array = f_array,
     n_list = n_list,
     target_sthol2 = target_sthol2,
     d_min_list = d_min_list,
     pseudo_likelihood = pseudo_likelihood,
     equalize_power = equalize_power,
     is_model_based = is_model_based,
     skip_scale_factor = skip_scale_factor,
     maximum_scale_factor = maximum_scale_factor,
     out = out)

  # Now apply analyses on each cc_list (if more than one)
  si_list = []
  for i in range(len(direction_vectors)):
    ratio_list = remove_values_if_necessary(ratio_dict_by_dv[i])
    rms_fo_list = remove_values_if_necessary(rms_fo_dict_by_dv[i])
    rms_fc_list = remove_values_if_necessary(rms_fc_dict_by_dv[i])
    cc_list = smooth_values(cc_dict_by_dv[i],
         overall_values=getattr(overall_si,'cc_list',None),
         smooth=smooth_fsc)
    if len(direction_vectors) > 1:
      working_si = deepcopy(si)
      dv = direction_vectors[i]
    else:
      dv = None
      working_si = si  # so we can modify it in place

    working_si = complete_cc_analysis(
       dv,
       cc_list,
       rms_fc_list,
       rms_fo_list,
       ratio_list,
       scale_using_last,
       max_cc_for_rescale,
       optimize_b_eff,
       is_model_based,
       s_value_list,
       cc_cut,
       max_possible_cc,
       fraction_complete,
       min_fraction_complete,
       low_res_bins,
       working_si,
       b_eff,
       input_info,
       cutoff_after_last_high_point,
       expected_rms_fc_list,
       expected_ssqr_list,
       overall_si,
       out)
    si_list.append(working_si)
  if direction_vectors == [None]:
    return si_list[0]

  # Results so far
  scale_factor_info = group_args(
     group_args_type = 'scaling_info objects, one set per direction_vector',
     direction_vectors = direction_vectors,
     scaling_info_list = si_list,
     overall_si = overall_si,
     )

  # Analyze anisotropy

  if run_analyze_anisotropy:
    aniso_info = analyze_anisotropy(
        mc1,
        mc2,
        f_array,
        overall_si,
        si_list,
        s_value_list,
        direction_vectors,
        weights_para_list,
        resolution,
        n_bins_use,
        use_dv_weighting,
        expected_ssqr_list,
        maximum_ratio = maximum_ratio,
        out = out)
  else:
    aniso_info = group_args(
     a_zero_values = None,
     fo_b_cart = None,
     fo_b_cart_as_u_cart = None,
     aa_b_cart = None,
     aa_b_cart_as_u_cart = None,
     bb_b_cart = None,
     bb_b_cart_as_u_cart = None,
     ss_b_cart = None,
     ss_b_cart_as_u_cart = None,
     uu_b_cart = None,
     uu_b_cart_as_u_cart = None,
     )

  # Merge in aniso info to scale_factor_info
  scale_factor_info.merge(aniso_info) # aniso_info overwrites scale_factor_info
  return scale_factor_info

def get_apply_aniso_dict_by_dv(direction_vectors,
    f_array, aniso_scale_factor_as_u_cart):
  """Calculate anisotropic values to apply for each direction vector"""
  s_value_list = flex.double()
  dsd = f_array.d_spacings().data()
  for i_bin in f_array.binner().range_used():
    sel       = f_array.binner().selection(i_bin)
    d         = dsd.select(sel)
    d_avg     = flex.mean(d)
    s_value_list.append(1/d_avg)

  apply_aniso_dict_by_dv = {}
  i = -1
  for dv in direction_vectors:
    i+=1

    if not dv:
      apply_aniso_dict_by_dv[i] = flex.double(s_value_list.size(),1.)
    else: # usual

      from iotbx.map_model_manager import create_fine_spacing_array
      fine_array = create_fine_spacing_array(
         f_array.crystal_symmetry().unit_cell())

      indices = get_calculated_scale_factors( # get the indices
          s_value_list=s_value_list,
          cc_list = flex.double(s_value_list.size(),1),
          dv = dv,
          uc = fine_array.crystal_symmetry().unit_cell(),
           ).indices
      fine_array=fine_array.customized_copy(indices = indices,
        data = flex.double(indices.size(), 1.))
      scale_array = get_aniso_scale_factor_array(fine_array,
        tuple(-flex.double(aniso_scale_factor_as_u_cart)))
      apply_aniso_dict_by_dv[i] = scale_array.data()
  return apply_aniso_dict_by_dv


def get_aniso_scale_factor_array(fo_map,
   aniso_scale_factor_as_u_cart):
  """Apply anisotropic scale factor"""
  scale_values_array = fo_map.customized_copy(
       data = flex.double(fo_map.size(),1.))
  u_star= adptbx.u_cart_as_u_star(
     scale_values_array.unit_cell(),
     tuple(matrix.col(aniso_scale_factor_as_u_cart)))
  from mmtbx.scaling import absolute_scaling
  return absolute_scaling.anisotropic_correction(
     scale_values_array,0.0, u_star ,must_be_greater_than=-0.0001)

def get_aniso_scale_info(fo_map, resolution = None):
  ''' Get overall anisotropic scale for fo_map'''
  if not fo_map:
    return
  assert resolution is not None
  from cctbx.maptbx.segment_and_split_map import map_coeffs_as_fp_phi
  f_local,phases_local=map_coeffs_as_fp_phi(fo_map)
  aniso_obj=analyze_aniso_object()
  aniso_obj.set_up_aniso_correction(f_array=f_local,d_min=resolution)
  if (not aniso_obj) or (not aniso_obj.b_cart):
    return

  return adptbx.b_as_u(aniso_obj.b_cart)



def analyze_anisotropy(
  half_map_coeffs_1,
  half_map_coeffs_2,
  f_array,
  overall_si,
  si_list,
  s_value_list,
  direction_vectors,
  weights_para_list,
  resolution,  # nominal resolution
  n_bins_use,
  use_dv_weighting,
  expected_ssqr_list,
  n_display = 6,
  weight_by_variance = True,
  minimum_sd = 0.1, # ratio to average
  maximum_ratio = None, # maximum ratio of target_scale_factor to overall
  update_scale_values_to_match_s_matrix = False,
  update_scale_values_to_match_qq_values= True,
  out = sys.stdout):
  """Analyze anisotropy in a map.

  Define:
  F1a_obs = rmsFc(|s|) * Ao(|s|) * (A(s) * F1a + B(s) * s1)
  ssqr = rms(s1)**2 = ssqr(|s|)
  At(|s|)  = rmsFc(|s|)

  Then we can ssqr(s) as:
  ssqr(|s|), ssqr(s)  = (1/CC_half(s)) - 1  # s**2, overall and  by direction
  rmsFo(|s|), rmsFo(s)  # rms F obs (mean of half maps) overall and by direction

  Define D(s), D(|s|):
  D(s) = 1./sqrt(1 +  0.5* ssqr(s))

  These can be used to calculate the anisotropy values A(s) and B(s):
  A(s) = (rmsFo(s)/rmsFo(|s|))*sqrt((1 +  0.5* ssqr(|s|))/(1 +  0.5* ssqr(s)))
  B(s) = A(s) * sqrt (ssqr(s) /ssqr(|s|))

  Ao(|s|)  = rmsFo(|s|) / (rmsFc(|s|) * sqrt(1 +  0.5* ssqr(|s|)))
    # overall true fall-off

  The scale factor to apply to Fobs is:
  Q(s) = (rmsFc(|s|)/rmsFo(s))  * sqrt(1 +  0.5 * ssqr(s))/(1 + ssqr(s))

  Normalize these to Q(s=0).
  """
  print ("\n",79*"=","\nAnalyzing anisotropy","\n",79*"=", file = out)

  cc_b_cart = get_overall_anisotropy(
     # Note: sets a_zero_values in overall_si
     overall_si = overall_si,
     n_bins_use = n_bins_use,
     out = out)

  aniso_info = get_aniso_info(
    f_array = f_array,
    overall_si = overall_si,
    si_list = si_list,
    expected_ssqr_list = expected_ssqr_list,
    s_value_list = s_value_list,
    n_bins_use = n_bins_use,
    direction_vectors = direction_vectors,
    weights_para_list = weights_para_list,
    resolution = resolution,
    weight_by_variance = weight_by_variance,
    minimum_sd = minimum_sd,
    maximum_ratio = maximum_ratio,
    use_dv_weighting = use_dv_weighting,
    cc_b_cart = cc_b_cart,
    out = out)


  aniso_info = estimate_s_matrix(aniso_info)

  # Total correction
  print("\nD matrix (Uncertainty-based scaling correction factor)\n"+
       "(Negative means uncertainties increase more in this direction)\n " +
      "(%.3f, %.3f, %.3f, %.3f, %.3f, %.3f) " %(
     tuple(aniso_info.dd_b_cart)), file = out)

  print("\nS matrix (anisotropy correction to scale factor)\n"+
       "(Negative means amplitudes fall off more in this direction)\n " +
      "(%.3f, %.3f, %.3f, %.3f, %.3f, %.3f) " %(
     tuple(aniso_info.ss_b_cart)), file = out)

  print("\nU matrix (Overall anisotropic fall-off relative to ideal)\n"+
       "(Negative means amplitudes fall off more in this direction)\n " +
      "(%.3f, %.3f, %.3f, %.3f, %.3f, %.3f) " %(
     tuple(aniso_info.uu_b_cart)), file = out)

  print("\nS scale values by direction vector", file = out)
  print("  D-min  A-zero   E**2   ", file = out, end = "")

  display_scale_values(
    aniso_info = aniso_info,
    n_display=n_display,
    values_by_dv = aniso_info.ss_values_by_dv,
    out = out)

  print("\nS scale values (calculated) by direction vector", file = out)
  print("  D-min  A-zero   E**2   ", file = out, end = "")

  display_scale_values(
      aniso_info = aniso_info,
      n_display=n_display,
      values_by_dv = aniso_info.ss_calc_values_by_dv,
      out = out)

  # Get summary information

  # Use qq_values instead of target_scale_factors (very similar)
  if update_scale_values_to_match_qq_values:
    print("\nUpdating scale values with qq_values (new version)\n",file = out)
    aniso_info = update_scale_values_with_qq_values(
      aniso_info = aniso_info,
      out = out)

  # Update scale factors from overall values if desired
  if update_scale_values_to_match_s_matrix:
    print("\nRecalculating scale values from S matrix and overall scale",
       file = out)
    # Recalculate target_scale_factors for each direction vector and save in
    #   corresponding si
    aniso_info = update_scale_values(  # update with S matrix
      aniso_info = aniso_info,
      out = out)


  scale_factor_info = group_args(
    group_args_type = """
     scale_factor_info:
  overall_si:  si overall
  scaling_info_list: si (scaling_info) objects, one for each direction vector
    each si:  si.target_scale_factors   # scale factors vs sthol2
              si.target_sthol2 # sthol2 values  d = 1/s = 0.25/sthol2**0.5
  a_zero_values:  isotropic fall-off of the data
  fo_b_cart_as_u_cart: anisotropic fall-off of data
  aa_b_cart_as_u_cart: anisotropy of the data
  bb_b_cart_as_u_cart: anisotropy of the uncertainties
  ss_b_cart_as_u_cart: anisotropy of the scale factors
  uu_b_cart_as_u_cart: anisotropic fall-off of data relative to ideal
  overall_scale: radial part of overall correction factor


    """,
    scaling_info_list = aniso_info.si_list,
    overall_si = overall_si,
    a_zero_values = overall_si.a_zero_values,
    fo_b_cart_as_u_cart = adptbx.b_as_u(aniso_info.fo_b_cart),
    aa_b_cart_as_u_cart = adptbx.b_as_u(aniso_info.aa_b_cart),
    bb_b_cart_as_u_cart = adptbx.b_as_u(aniso_info.bb_b_cart),
    ss_b_cart_as_u_cart = adptbx.b_as_u(aniso_info.ss_b_cart),
    uu_b_cart_as_u_cart = adptbx.b_as_u(aniso_info.uu_b_cart),
    overall_scale = overall_si.target_scale_factors,
   )
  return scale_factor_info

def estimate_s_matrix(aniso_info):

  """Estimate errors and calculate A,B,D S and Q matrices"""

  aniso_info = get_starting_sd_info(
    aniso_info = aniso_info,)

  aniso_info.ss_b_cart = get_aniso_from_scale_values(
    aniso_info,
    aniso_info.ss_scale_values,
    aniso_info.ss_sd_values,
  )

  aniso_info.ss_calc_values_by_dv = get_calc_values(
      aniso_info = aniso_info,
      b_cart = aniso_info.ss_b_cart,
      apply_b = True)

  # Update errors and run again:

  aniso_info = update_sd_values(aniso_info)

  # Now real thing for S matrix
  aniso_info.ss_b_cart = get_aniso_from_scale_values(
    aniso_info,
    aniso_info.ss_scale_values,
    aniso_info.ss_sd_values,
    b_cart = aniso_info.ss_b_cart,
  )
  aniso_info.ss_calc_values_by_dv = get_calc_values(
      aniso_info = aniso_info,
      b_cart = aniso_info.ss_b_cart,
      apply_b = True)

  # Repeat for A matrix
  aniso_info.aa_b_cart = get_aniso_from_scale_values(
    aniso_info,
    aniso_info.aa_scale_values,
    aniso_info.ss_sd_values,  # using sigmas for ss_b_cart
  )

  # Repeat for B matrix
  aniso_info.bb_b_cart = get_aniso_from_scale_values(
    aniso_info,
    aniso_info.bb_scale_values,
    aniso_info.ss_sd_values,  # using sigmas for ss_b_cart
  )

  # Repeat for D matrix
  aniso_info.dd_b_cart = get_aniso_from_scale_values(
    aniso_info,
    aniso_info.dd_scale_values,
    aniso_info.ss_sd_values,  # using sigmas for ss_b_cart
  )

  # Repeat for F (anisotropy of data)
  aniso_info.fo_b_cart = get_aniso_from_scale_values(
    aniso_info,
    aniso_info.fo_scale_values,
    aniso_info.ss_sd_values,
  )

  # Repeat for U (anisotropy of data relative to ideal)
  aniso_info.uu_b_cart = get_aniso_from_scale_values(
    aniso_info,
    aniso_info.uu_scale_values,
    aniso_info.ss_sd_values,
  )


  return aniso_info

def get_overall_anisotropy(overall_si,
  n_bins_use,
  out = sys.stdout):
  """Estimate overall fall-off with resolution of data
    and corrections for it including uncertainties"""

  print("\nEstimating overall fall-off with resolution of data"+
    " and correction including uncertainties", file = out)

  overall_si.cc_list.set_selected(overall_si.cc_list<1.e-4,1.e-4)
  overall_si.rms_fo_list.set_selected(overall_si.rms_fo_list <= 1.e-10, 1.e-10)

  overall_si.ssqr_values = 2. * ( -1 + 1./overall_si.cc_list)  # cc* is cc_list

  correction_factor = 1./flex.sqrt(1. + 0.5*overall_si.ssqr_values)
       # == sqrt(overall_si.cc_list)

  # Ao(|s|) = ( rmsFobs(|s|)/rmsFc(|s|) ) / sqrt (1 + 0.5*ssqr(|s|)) == C(|s|)
  # D(|s|) = 1./sqrt(1 +  0.5* ssqr(|s|))

  # So Ao(|s|) = ( rmsFobs(|s|)/rmsFc(|s|) )  * D(|s|)

  overall_si.dd_values = correction_factor

  overall_si.a_zero_values = correction_factor *(
      overall_si.rms_fo_list/ overall_si.rms_fc_list)

  info = get_effective_b(
    values = overall_si.a_zero_values,
    sthol2_values = overall_si.target_sthol2,
    n_bins_use = n_bins_use)

  print("Overall approximate B-value: %.2f A**2 (Scale = %.4f  rms = %.4f)" %(
    info.effective_b,
    info.b_zero,
    info.rms,), file = out)

  cc_b_cart = (
    -info.effective_b,-info.effective_b,-info.effective_b,0,0,0)

  print (" D-min   Ao(|s|)   Calc", file = out)
  for i in range(info.sthol2_values.size()):
    dd = 0.5/info.sthol2_values[i]**0.5
    print ("%6.2f  %7.4f   %6.4f  " %(
      dd, overall_si.a_zero_values[i], info.calc_values[i]),file = out)
  return cc_b_cart

def get_aniso_info(
    f_array = None,
    overall_si = None,
    si_list = None,
    expected_ssqr_list = None,
    s_value_list = None,
    n_bins_use = None,
    direction_vectors = None,
    weights_para_list = None,
    resolution = None,  # nominal resolution
    weight_by_variance = None,
    minimum_sd = None,
    maximum_ratio = None,
    small_ssqr_for_maximum_ratio = 5.,
    use_dv_weighting= None,
    cc_b_cart = None,
    out = sys.stdout):

  """
  Calculate anisotropic information for an array.

  ssqr(s) = (1/CC_half(s) - 1)  ...along any direction s
  Ao(|s|)**2 = rmsFobs(s*)**2 * (1 + 0.5*ssqr(s*))    ...along s*

  A(s) = (rmsFo(s)/rmsFc(|s|))*sqrt((1 +  0.5* ssqr(|s|))/(1 +  0.5* ssqr(s)))
  B(s) = A(s) * sqrt (ssqr(s) /ssqr(|s|))
  Target scale factors:
  Q(s) = (rmsFc(|s|)/rmsFo(s)) * sqrt(1 + 0.5* ssqr(s))/ (1 + ssqr(s))

  S(s) =  Q(s)/Q(|s|) # anisotropy of target scale factors

  """

  print("\nUsing overall average fall-off as baseline", file = out)

  overall_si.ssqr_values = 2. * ( -1 + 1./overall_si.cc_list)  # cc* is in cc_list

  fo_scale_values = flex.double()
  fo_values_by_dv = []
  aa_scale_values = flex.double()
  aa_values_by_dv = []
  bb_scale_values = flex.double()
  bb_values_by_dv = []
  dd_scale_values = flex.double()
  dd_values_by_dv = []
  ss_scale_values = flex.double()
  ss_values_by_dv = []
  uu_scale_values = flex.double()
  uu_values_by_dv = []

  indices = flex.miller_index()
  indices_by_dv = []
  # Limit range of scale factors relative to overall if errors are not small
  overall_ssqr_values_are_large = (overall_si.ssqr_values > small_ssqr_for_maximum_ratio)

  # We are going to recalculate target_scale_factors here with slightly
  #   different formula and call them qq_values
  #  target_scale_factors: T = (rmsFc(s)/rmsFo(s)) * 1/sqrt(1+0.5*ssqr(s))
  #  qq_values = (rmsFc(|s|)/rmsFo(s)) * sqrt(1 + 0.5* ssqr(s))/ (1 + ssqr(s))
  #  Normalized these:  qq_values = qq_values/qq_values[0]

  #  These differ by the ratio:
  #   (1 +  0.5 * ssqr(s))**2/(1 + ssqr(s))
  #  ... which has a value of 1 for ssqr=0 and 1.04 for ssqr=5...

  #  dd_values = 1/sqrt(1+0.5*ssqr)

  overall_si.qq_values = (
     overall_si.rms_fc_list *
     flex.sqrt(1. + 0.5 * overall_si.ssqr_values)) / (
     overall_si.rms_fo_list *
     (1. + overall_si.ssqr_values))

  overall_normalization = overall_si.target_scale_factors.min_max_mean().mean/ \
         max(1.e-10,overall_si.qq_values.min_max_mean().mean)
  overall_si.target_scale_factors *= overall_normalization

  overall_si.target_scale_factors.set_selected(
    overall_si.target_scale_factors < 1.e-10, 1.e-10)
  overall_si.qq_values.set_selected(
    overall_si.qq_values< 1.e-10, 1.e-10)

  overall_si.aa_values = flex.double(overall_si.qq_values.size(),1)
  overall_si.bb_values = flex.double(overall_si.qq_values.size(),1)

  for dv,si in zip(direction_vectors, si_list):
    si.cc_list.set_selected(si.cc_list < 1.e-10,1.e-10)
    si.dd_values = flex.sqrt(si.cc_list)  # 1/(1. + 0.5 * ssqr)**0.5
    si.ssqr_values = 2. * ( -1 + 1./si.cc_list)  # cc* is in cc_list
    si.qq_values = (
     si.rms_fc_list *
     flex.sqrt(1. + 0.5 * si.ssqr_values)) / (
     si.rms_fo_list *
     (1. + si.ssqr_values))

    # Normalize to overall_si.target_scale_factors[0] if possible
    local_normalization=\
        overall_si.qq_values.min_max_mean().mean/max(1.e-10,
       si.qq_values.min_max_mean().mean)
    si.qq_values *= local_normalization

    if overall_si.qq_values[0] > 1.e-10 and \
       si.qq_values[0] > 1.e-10 and \
       si.qq_values[0]/overall_si.qq_values[0] > 0.1 and\
       si.qq_values[0]/overall_si.qq_values[0] < 10:
      local_normalization = \
         overall_si.qq_values[0]/si.qq_values[0]
      si.qq_values *= local_normalization

    # Anisotropy of the data: A
    si.aa_values = ( si.rms_fo_list * si.dd_values ) / (
      overall_si.rms_fo_list * overall_si.dd_values )

    # Anisotropy of the errors: B
    si.bb_values = si.aa_values * flex.sqrt(
       si.ssqr_values/overall_si.ssqr_values)

    # Anisotropy of scale factors S:
    #     S = ratio of target scale factors to overall values
    si.ss_values = si.qq_values/overall_si.qq_values
    si.ss_values.set_selected(
       (overall_ssqr_values_are_large) & (si.ss_values > maximum_ratio),
           maximum_ratio)
    si.ss_values.set_selected(
       (overall_ssqr_values_are_large) & (si.ss_values < 1/maximum_ratio),
           1/maximum_ratio)

    # Estimate of true fall-off of data relative to ideal U
    # U(s) = rmsFo(s) / (rmsFc(|s|) sqrt(1 +  0.5* ssqr(s)))
    si.uu_values = si.rms_fo_list * si.dd_values / overall_si.rms_fc_list

    info = get_calculated_scale_factors( # get the indices
          s_value_list=s_value_list,
          cc_list = flex.double(s_value_list.size(),1),
          dv = dv,
          uc = f_array.unit_cell(),
           )
    indices_by_dv.append(info.indices)
    indices.extend(info.indices)
    fo_values_by_dv.append(si.rms_fo_list)
    fo_scale_values.extend(si.rms_fo_list)
    aa_values_by_dv.append(si.aa_values)
    aa_scale_values.extend(si.aa_values)
    bb_values_by_dv.append(si.bb_values)
    bb_scale_values.extend(si.bb_values)
    dd_values_by_dv.append(si.dd_values)
    dd_scale_values.extend(si.dd_values)
    ss_values_by_dv.append(si.ss_values)
    ss_scale_values.extend(si.ss_values)
    uu_values_by_dv.append(si.uu_values)
    uu_scale_values.extend(si.uu_values)

  return group_args(
    f_array = f_array,
    direction_vectors = direction_vectors,
    weights_para_list = weights_para_list,
    resolution = resolution,
    ssqr_values = overall_si.ssqr_values,
    s_value_list = s_value_list,
    minimum_sd = minimum_sd,
    maximum_ratio = maximum_ratio,
    weight_by_variance = weight_by_variance,
    use_dv_weighting = use_dv_weighting,
    n_bins = overall_si.target_sthol2.size(),
    n_dv = direction_vectors.size(),
    si_list = si_list,
    overall_si = overall_si,
    n_bins_use = n_bins_use,
    cc_b_cart = cc_b_cart,
    indices = indices,
    indices_by_dv = indices_by_dv,
    fo_scale_values = fo_scale_values,
    fo_values_by_dv = fo_values_by_dv,
    aa_scale_values = aa_scale_values,
    aa_values_by_dv = aa_values_by_dv,
    bb_scale_values = bb_scale_values,
    bb_values_by_dv = bb_values_by_dv,
    dd_scale_values = dd_scale_values,
    dd_values_by_dv = dd_values_by_dv,
    ss_scale_values = ss_scale_values,
    ss_values_by_dv = ss_values_by_dv,
    uu_scale_values = uu_scale_values,
    uu_values_by_dv = uu_values_by_dv,
  )

def get_starting_sd_info(aniso_info = None):

  """Try to get variances of aa, bb values within each resolution bin"""
  sd_ss = flex.double()
  for i in range(aniso_info.n_bins):
    ss_in_bin = flex.double()
    for k in range(aniso_info.n_dv):
      ss_in_bin.append(aniso_info.ss_values_by_dv[k][i])
    sd_value = ss_in_bin.sample_standard_deviation()
    # Try to catch cases where the values are stopped by bounds
    if ss_in_bin.min_max_mean().mean >= 0.9* aniso_info.maximum_ratio:
      sd_value = aniso_info.maximum_ratio
    elif ss_in_bin.min_max_mean().mean <= 1.1/aniso_info.maximum_ratio:
      sd_value = 1
    sd_ss.append(sd_value)
  ss_sd_values = flex.double()
  for k in range(aniso_info.n_dv):
    ss_sd_values.extend(sd_ss)
  average_sd = ss_sd_values.min_max_mean().mean
  minimum_sd_use = max(1.e-5,aniso_info.minimum_sd * average_sd)
  ss_sd_values.set_selected(ss_sd_values < minimum_sd_use, minimum_sd_use)
  aniso_info.ss_sd_values = ss_sd_values
  return aniso_info

def get_calc_values_with_dv_weighting(
   aniso_info,
   b_cart,
   direction_vector_k_list = None,
   apply_b = None):
  """Calculate anisotropic scale factors in each direction dv
      that removes anisotropic b_cart from data, applying weighting
      depending on the direction dv"""

  if direction_vector_k_list is None:
    direction_vector_k_list = list(range(aniso_info.n_dv))

  # Calculate values from matrices but weight as in dv weighting
  calc_values= get_scale_from_aniso_b_cart(
      f_array = aniso_info.f_array,
      indices = aniso_info.f_array.indices(),
      b_cart = b_cart,
      apply_b = apply_b)
  calc_values_by_dv=[]

  if direction_vector_k_list is None:
    direction_vector_k_list = list(range(aniso_info.n_dv))

  for k in range(aniso_info.n_dv):
    calc_values_by_dv.append(flex.double())

  for i_bin in aniso_info.f_array.binner().range_used():
    sel       = aniso_info.f_array.binner().selection(i_bin)
    for k in direction_vector_k_list:
      weights_para = aniso_info.weights_para_list[k]
      weights_para_sel = weights_para.select(sel)
      mean_weight = max(1.e-10,weights_para_sel.min_max_mean().mean)

      calc_values_by_dv[k].append( (calc_values.select(sel) *
         weights_para_sel).min_max_mean().mean/mean_weight)
  return calc_values_by_dv

def get_calc_values(
   aniso_info,
   b_cart,
   use_dv_weighting = None,
   direction_vector_k_list = None,
   apply_b = None):
  """Calculate anisotropic scale factors in each direction dv
      that removes anisotropic b_cart from data"""
  if use_dv_weighting is None:
    use_dv_weighting = aniso_info.use_dv_weighting
  if use_dv_weighting:
   values = get_calc_values_with_dv_weighting(
     aniso_info,
     b_cart,
     direction_vector_k_list = direction_vector_k_list,
     apply_b = apply_b,
    )
   return values

  if direction_vector_k_list is None:
    direction_vector_k_list = list(range(aniso_info.n_dv))

  # Calculate values from matrices
  calc_values_by_dv=[]
  for k in range(aniso_info.n_dv):
    if k in direction_vector_k_list:
      calc_values_by_dv.append(get_scale_from_aniso_b_cart(
        f_array = aniso_info.f_array,
        indices = aniso_info.indices_by_dv[k],
        b_cart = b_cart,
        apply_b = apply_b))
    else:
      calc_values_by_dv.append(flex.double())  # empty
  return calc_values_by_dv

def update_sd_values(aniso_info):

  """Update error estimates using calculated values as reference
  Try to get variances of aa, bb values within each resolution bin"""

  delta_ss = flex.double(aniso_info.n_bins,0)
  for k in range(aniso_info.n_dv):
    delta_ss += flex.pow2(
       aniso_info.ss_values_by_dv[k] - aniso_info.ss_calc_values_by_dv[k])
  scale = max(1,aniso_info.n_dv - 1)
  ss_sd_vs_resolution= flex.sqrt(delta_ss/scale) # sd vs resolution

  # And create sd vector
  ss_sd_values = flex.double()
  for k in range(aniso_info.n_dv):
    ss_sd_values.extend(ss_sd_vs_resolution)

  aniso_info.ss_sd_values = ss_sd_values
  return aniso_info

def get_scale_from_aniso_b_cart(f_array = None,
    indices = None,
    b_cart = None,
    apply_b = False):
  """Calculate anisotropic scale factor that removes anisotropic b_cart from
  data"""
  scale_values_array = f_array.customized_copy(
    data = flex.double(indices.size(),1),
    indices = indices)
  from mmtbx.scaling import absolute_scaling
  scale_values_array.set_observation_type_xray_amplitude()

  # NOTE: this removes b_cart from data. To apply it,  use apply_b=True
  if apply_b:
    overall_u_cart_to_apply = adptbx.b_as_u(tuple(-flex.double(tuple(b_cart))))
  else:
    overall_u_cart_to_apply = adptbx.b_as_u(tuple( flex.double(tuple(b_cart))))
  u_star= adptbx.u_cart_as_u_star(
    scale_values_array.unit_cell(),
     tuple(matrix.col(overall_u_cart_to_apply)))
  scaled_f_array = absolute_scaling.anisotropic_correction(
          scale_values_array,0.0, u_star ,must_be_greater_than=-0.0001)
  return scaled_f_array.data()

def update_scale_values_with_qq_values(
    aniso_info = None,
    out = sys.stdout):
  '''
   Replace target_scale_factors with qq_values
  '''

  aniso_info.overall_si.target_scale_factors = aniso_info.overall_si.qq_values

  for k in range(aniso_info.direction_vectors.size()):
    si = aniso_info.si_list[k]
    original_target_scale_factors = si.target_scale_factors
    si.target_scale_factors = si.qq_values

    print("Target scale factor replacement with qq_values for direction_vector",
       k,file=out)
    for sthol2,o,t in zip(
      si.target_sthol2,original_target_scale_factors,si.target_scale_factors):
      dd = 0.5/sthol2**0.5
      print(" %.2f %.3f %.3f" %(dd,o,t), file = out)

  return aniso_info

def update_scale_values(
    aniso_info = None,
    out = sys.stdout):
  '''
   Update the values of target_scale_factors using the information in
   aniso_info and s_matrix and target_scale_factors from overall_si (overall)
  '''
  for k in range(aniso_info.direction_vectors.size()):
    si = aniso_info.si_list[k]
    original_target_scale_factors = si.target_scale_factors
    normalized_scale_factors = get_any_list_from_any(
      aniso_info = aniso_info,
      any_matrix = aniso_info.ss_b_cart,
      use_dv_weighting = True, # REQUIRED
      target_sthol2_list = si.target_sthol2,
      direction_vector_k = k)
    si.target_scale_factors = normalized_scale_factors * \
       aniso_info.overall_si.target_scale_factors

    print("Target scale factor replacement with S matrix for direction_vector ",
       k,file=out)
    for sthol2,o,t in zip(
      si.target_sthol2,original_target_scale_factors,si.target_scale_factors):
      dd = 0.5/sthol2**0.5
      print(" %.2f %.3f %.3f" %(dd,o,t), file = out)

  return aniso_info

def get_any_list_from_any(
   aniso_info = None,
   any_matrix = None,
   target_sthol2_list = None,
   use_dv_weighting = None,
   direction_vector_k = None):
  '''
  Calculate estimated amplitude fall-off along this direction vector vs sthol2
  Value of any_matrix = A(s) (any anisotropic tensor)
  '''

  direction_vector_k_list = [direction_vector_k]

  calc_values_by_dv = get_calc_values(
    aniso_info,
    any_matrix,
    use_dv_weighting,
    direction_vector_k_list,
    apply_b = True)

  return calc_values_by_dv[direction_vector_k]

def display_scale_values(
    aniso_info = None,
    n_display=None,
    values_by_dv = None,
    out = sys.stdout):
  """Display scale values as function of direction vectors"""
  for k in range(min(aniso_info.n_dv,n_display)):
    print("  %4s " %(k+1), file = out, end = "")
  print("", file = out)
  for i in range(aniso_info.n_bins):
    dd = 0.5/aniso_info.overall_si.target_sthol2[i]**0.5
    print ("%6.2f  %6.3f %8.3f   " %(dd,
      aniso_info.overall_si.a_zero_values[i],
         aniso_info.overall_si.ssqr_values[i]),
        file = out, end = "")
    for k in range(min(aniso_info.n_dv,n_display)):
      print (" %5.2f " %(values_by_dv[k][i]), file = out, end= "")
    print("", file=out)

def get_aniso_from_scale_values(
   aniso_info = None,
   scale_values = None,
   sd_values = None,
   b_cart = None,
   ):
  """Estimate anistropy from scale values"""
  # If nothing present yet,
  #   Get a first cut for aniso_obj.b_cart with analyze_aniso
  if not b_cart:
    scale_values_array = aniso_info.f_array.customized_copy(
        data = scale_values,
        indices = aniso_info.indices)
    scaled_array,aniso_obj=analyze_aniso(
        b_iso=0,
        f_array=scale_values_array,resolution=aniso_info.resolution,
        remove_aniso=True,out=null_out())
    if not aniso_obj:
      return None
    elif aniso_obj.b_cart:
      b_cart = aniso_obj.b_cart
    else:
      b_cart = (0,0,0,0,0,0)

  # Optimize this (with weighting if weight_by_variance)

  ar = aniso_refinery(
    aniso_info,
    b_cart,
    scale_values,
    sd_values,
    eps = .01)
  ar.run()
  b_cart = ar.get_b()

  # If we want overall scale factor, it is here:
  # overall_scale = ar.get_overall_scale()
  # resid=ar.residual(b_cart)

  return b_cart

class aniso_refinery:
  """Refine anisotropic parameters to match scale factors along direction
   vectors dv. Specific for tools in this file"""
  def __init__(self,
    aniso_info,
    b_cart,
    scale_values,
    sd_values,
    overall_scale = 1.0,
    eps=0.01,
    tol=1.e-6,
    max_iterations=20,
    start_with_grid_search = True,
    grid_delta = 10,
    grid_n = 2,
    get_overall_scale_factor = True,
    ):

    self.aniso_info = aniso_info
    self.b_cart=b_cart
    self.overall_scale=overall_scale
    self.scale_values=scale_values
    self.sd_values=sd_values

    self.tol=tol
    self.eps=eps
    self.max_iterations=max_iterations
    self.get_overall_scale_factor=get_overall_scale_factor

    self.x = flex.double(b_cart)
    self.start_with_grid_search = start_with_grid_search
    self.grid_delta = grid_delta
    self.grid_n = grid_n



  def run(self):

    if self.start_with_grid_search:
      self.grid_search()
      b = self.get_b()
      resid = self.residual(b)
      if resid < self.tol: # done
        return

    else:
      best_b = self.get_b(self.x)
      resid=self.residual(self.x)
      scitbx.lbfgs.run(target_evaluator=self,
      termination_params=scitbx.lbfgs.termination_parameters(
        traditional_convergence_test_eps=self.tol,
                     max_iterations=self.max_iterations,
       ))

  def grid_search(self):
    best_b = self.get_b()
    working_b = self.get_b()
    best_resid = self.residual(best_b)
    for i in range(-self.grid_n,self.grid_n+1):
      for j in range(-self.grid_n,self.grid_n+1):
        for k in range(-self.grid_n,self.grid_n+1):
          b = list(
          matrix.col(working_b) +
          matrix.col((i*self.grid_delta,
           j*self.grid_delta,k*self.grid_delta,0,0,0)
           ))
          resid = self.residual(b)
          if resid < best_resid:
            best_resid = resid
            best_b = b
    self.x = flex.double(best_b)

  def show_result(self,out=sys.stdout):

    b=self.get_b()
    value = self.residual(b)
    return value

  def compute_functional_and_gradients(self):
    b = self.get_b()
    f = self.residual(b)
    g = self.gradients(b)
    return f, g

  def calculate_overall_scale(self,calc_values):
     if self.get_overall_scale_factor:
       return max(0, self.scale_values.min_max_mean().mean/max(1.e-10,
         calc_values.min_max_mean().mean))
     else:
       return 1

  def residual(self,b):
    calc_values_by_dv = get_calc_values(
      aniso_info = self.aniso_info,
      b_cart = b,
      apply_b = True)

    calc_values = flex.double()
    for c in calc_values_by_dv:
      calc_values.extend(c)

    # Get overall scale
    self.overall_scale = self.calculate_overall_scale(calc_values)

    diffs = calc_values*self.overall_scale - self.scale_values
    if self.aniso_info.weight_by_variance:
      diffs = diffs/self.sd_values
    diffs_dc=diffs.deep_copy()
    sd_dc= self.sd_values.deep_copy()
    if self.aniso_info.n_bins_use:  # just take first n_bins_use of each group
      i_pos = 0
      n_bins = calc_values_by_dv[0].size()
      diffs_use = flex.double()
      for k in range(len(calc_values_by_dv)):
        diffs_use.extend(diffs[i_pos:i_pos+self.aniso_info.n_bins_use])
        i_pos += n_bins
      diffs = diffs_use
    resid = diffs.rms()
    return resid

  def gradients(self,b):

    result = flex.double()
    for i in range(len(list(b))):
      rs = []
      for signed_eps in [self.eps, -self.eps]:
        params_eps = deepcopy(b)
        params_eps[i] += signed_eps
        rs.append(self.residual(params_eps))
      result.append((rs[0]-rs[1])/(2*self.eps))
    return result

  def get_overall_scale(self):
    return self.overall_scale

  def get_b(self):
    return list(self.x)

  def callback_after_step(self, minimizer):
    pass # can do anything here

class shell_aniso_refinery(aniso_refinery):
  def __init__(self,
    f_array,
    power = 1.,
    b_cart = None,
    overall_scale = None,
    eps=.1,
    tol=1.e-6,
    max_iterations=20,
    ):

    if not b_cart:
      b_cart = (0,0,0,0,0,0)
    if not overall_scale:
      overall_scale = f_array.data().min_max_mean().mean

    self.f_array = f_array.deep_copy()

    self.power=power
    self.tol=tol
    self.eps=eps
    self.max_iterations=max_iterations

    self.x = self.get_x(b_cart,overall_scale)

    self.start_with_grid_search = False

  def get_b(self, x = None):
    if x is None:
      x = self.x
    return [x[0],x[1],-1.*(x[0]+x[1]),x[2],x[3],x[4]]

  def get_overall_scale(self, x = None):
    if x is None:
      x = self.x
    return x[5]


  def get_x(self, b_cart, overall_scale):
    return flex.double((b_cart[0],b_cart[1],b_cart[3],b_cart[4],b_cart[5],
      overall_scale))

  def compute_functional_and_gradients(self):
    x = self.x
    f = self.residual(x)
    g = self.gradients(x)
    return f, g

  def gradients(self,x):

    result = flex.double()
    for i in range(len(list(x))):
      rs = []
      for signed_eps in [self.eps, -self.eps]:
        params_eps = deepcopy(x)
        params_eps[i] += signed_eps
        rs.append(self.residual(params_eps))
      result.append((rs[0]-rs[1])/(2*self.eps))
    return result


  def get_calc_array(self, x,  array_with_indices = None):
    b = self.get_b(x)
    overall_scale = self.get_overall_scale(x)
    from mmtbx.scaling import absolute_scaling
    overall_u_cart_to_apply = adptbx.b_as_u(tuple(
       -1*self.power*flex.double(tuple(b))))

    if not array_with_indices:
      array_with_indices = self.f_array

    u_star= adptbx.u_cart_as_u_star(
      array_with_indices.unit_cell(),
       tuple(matrix.col(overall_u_cart_to_apply)))
    fit_array = array_with_indices.customized_copy(
       data=flex.double(array_with_indices.size(),overall_scale))
    fit_array.set_observation_type_xray_amplitude()
    calc_array = absolute_scaling.anisotropic_correction(
          fit_array,0.0, u_star ,must_be_greater_than=-0.0001)

    return calc_array

  def residual(self,x):
    calc_array = self.get_calc_array(x)
    if not calc_array or calc_array.size()==0:
       return 1.e+30
    diffs = calc_array.data()-self.f_array.data()
    residual = flex.pow2(diffs).min_max_mean().mean
    rms = residual**0.5
    return rms

class iso_refinery(aniso_refinery):
  def __init__(self,
    values,
    sthol2_values,
    n_bins_use = None,
    b_iso = None,
    eps=0.01,
    tol=1.e-6,
    max_iterations=20,
    start_with_grid_search = False,
    grid_delta = 50,
    grid_n = 10,
    ):

    if not b_iso:
      b_iso = 0

    self.values = values
    self.sthol2_values = sthol2_values
    self.n_bins_use = n_bins_use
    self.tol=tol
    self.eps=eps
    self.max_iterations=max_iterations

    self.x = flex.double((b_iso,))

    self.start_with_grid_search = start_with_grid_search
    self.grid_delta = grid_delta
    self.grid_n = grid_n

  def get_info(self):
    b = self.get_b()
    b_value = b[0]
    info=get_b_calc(b_value,
       self.sthol2_values,
       self.values,
       n_bins_use = self.n_bins_use)
    return info

  def grid_search(self):
    best_b = self.get_b()
    working_b = self.get_b()
    grid_delta = self.grid_delta
    for cycle in range(self.grid_n):
      best_resid = self.residual(best_b)
      for i in range(-self.grid_n,self.grid_n+1):
        b = [working_b[0] + i*grid_delta]
        resid = self.residual(b)
        if resid < best_resid:
          best_resid = resid
          best_b = b
      self.x = flex.double(best_b)
      grid_delta = grid_delta/(self.grid_n*2)
      if grid_delta < self.eps: break

  def residual(self,b):
    b_value = b[0]
    info=get_b_calc(b_value,
       self.sthol2_values,
       self.values,
       n_bins_use = self.n_bins_use)
    return info.rms

def calculate_kurtosis(ma,phases,b,resolution,n_real=None,
    d_min_ratio=None):
  """Calculate the kurtosis of a map"""
  map_data=get_sharpened_map(ma,phases,b,resolution,n_real=n_real,
  d_min_ratio=d_min_ratio)
  return get_kurtosis(map_data.as_1d())



def get_aniso_obj_from_direction_vectors(
        f_array = None,
        resolution = None,
        direction_vectors = None,
        si_list = None,
        s_value_list = None,
        expected_rms_fc_list = None,
        invert_in_scaling = False,
        ):
      """Calculate anisotropic information from direction vectors and
         a list of CC values or expected rms FC values """
      scale_values = flex.double()
      indices = flex.miller_index()
      extra_b = -15.  # Just so ml scaling gives about the right answer for
                # constant number that are not really reflection data
      if not si_list:
        si_list = []
        for dv in direction_vectors:
          si_list.append(group_args(
            effective_b = None,
            effective_b_f_obs = None,
            b_zero = 0,
            cc_list =expected_rms_fc_list,
           ))

      for dv,si in zip(direction_vectors,si_list):
        info = get_calculated_scale_factors(
          s_value_list=s_value_list,
          effective_b=(None if (si.effective_b is None) else (si.effective_b + extra_b)),  # so xtriage gives about right
          b_zero = si.b_zero,
          cc_list = si.cc_list,
          dv = dv,
          uc = f_array.unit_cell(),
           )
        indices.extend(info.indices)
        scale_values.extend(info.scale_values)

      if invert_in_scaling:
        scale_values = 1/scale_values
      scale_values_array = f_array.customized_copy(
        data = scale_values,
        indices = indices)

      scaled_array,aniso_obj=analyze_aniso(
        b_iso=0,
        invert = invert_in_scaling,
        f_array=scale_values_array,resolution=resolution,
        remove_aniso=True,out=null_out())
      return aniso_obj

def get_resolution_for_aniso(s_value_list=None, si_list=None,
    minimum_ratio = 0.05):
  """Estimate a reasonable resolution to use for anisotropic calculations"""
  highest_d_min = None
  for si in si_list:
    first_rms_fo = None
    for rms_fo,s_value in  zip(si.rms_fo_list,s_value_list):
      d = 1/s_value
      if first_rms_fo is None:
        first_rms_fo = rms_fo
      else:
        ratio = rms_fo/max(1.e-10, first_rms_fo)
        if ratio < minimum_ratio and (
            highest_d_min is None or d > highest_d_min):
          highest_d_min = d
  return highest_d_min

def remove_values_if_necessary(f_values,max_ratio=100, min_ratio=0.01):
  """Make sure values are within a factor of 100 of low_res ones...if they are
  not it was probably something like zero values or near-zero values"""
  f_values=flex.double(f_values)
  low_res = f_values[:3].min_max_mean().mean
  new_values=flex.double()
  last_value = low_res
  for x in f_values:
    if x > max_ratio * low_res or x < min_ratio * low_res:
      new_values.append(last_value)
    else:
      new_values.append(x)
      last_value = x
  return new_values

def smooth_values(cc_values, max_relative_rms=10, n_smooth = None,
    skip_first_frac = 0.1,
    overall_values = None,
    smooth = True,
    max_ratio = 2.,
    min_ratio = 0.,
      ): # normally do not smooth the very first ones
  '''  If overall_values are supplied, make sure all values are within
      min_ratio to max_ratio of those values'''


  if overall_values and (max_ratio is not None or min_ratio is not None):
    new_cc_values = flex.double()
    for cc,cc_overall in zip(cc_values,overall_values):
      new_cc_values.append(max(
        min_ratio*cc_overall, min(max_ratio*cc_overall, cc)))
    cc_values = new_cc_values

  if not smooth:
    return cc_values

  skip_first = max (1, int(0.5+skip_first_frac*cc_values.size()))

  if n_smooth:
    # smooth with window of n_smooth
    new_cc_values = flex.double()
    for i in range(cc_values.size()):
      if i < skip_first:
        new_cc_values.append(cc_values[i])
      else:
        sum=0.
        sum_n=0.
        for j in range(-n_smooth, n_smooth+1):
          weight = 1/(1+(abs(j)/n_smooth)) # just less as we go out
          k = i+j
          if k < 0 or k >= cc_values.size(): continue
          sum += cc_values[k] * weight
          sum_n += weight
        new_cc_values.append(sum/max(1.e-10,sum_n))
    return new_cc_values

  # Smooth values in cc_values  max_relative_rms is avg rms / avg delta
  if relative_rms(cc_values) <= max_relative_rms:
    return cc_values
  for i in range(1,cc_values.size()//2):
    smoothed_cc_values=smooth_values(cc_values,n_smooth=i)
    if relative_rms(smoothed_cc_values) <= max_relative_rms:
      return smoothed_cc_values
  smoothed_cc_values=smooth_values(cc_values,n_smooth=cc_values.size()//2)
  return smoothed_cc_values


def relative_rms(cc_values):
  """Calculate relative rms of CC values"""
  diffs = cc_values[:-1] - cc_values[1:]
  avg_delta = abs(diffs.min_max_mean().mean)
  rms = diffs.sample_standard_deviation()
  return rms/max(1.e-10,avg_delta)

def complete_cc_analysis(
       direction_vector,
       cc_list,
       rms_fc_list,
       rms_fo_list,
       ratio_list,
       scale_using_last,
       max_cc_for_rescale,
       optimize_b_eff,
       is_model_based,
       s_value_list,
       cc_cut,
       max_possible_cc,
       fraction_complete,
       min_fraction_complete,
       low_res_bins,
       si,
       b_eff,
       input_info,
       cutoff_after_last_high_point,
       expected_rms_fc_list,
       expected_ssqr_list,
       overall_si,
       out):
  """Analyze CC values as function of direction vectors. Return
    an si object with this analysis"""
  if scale_using_last: # rescale to give final value average==0
    cc_list,baseline=rescale_cc_list(
       cc_list=cc_list,scale_using_last=scale_using_last,
       max_cc_for_rescale=max_cc_for_rescale)
    if baseline is None: # don't use it
      scale_using_last=None

  if expected_ssqr_list and overall_si:
    # Replace with expected scaled by avg fo/fo; if
    #  this direction has small fo then errors are bigger
    directional_ssqr_list = expected_ssqr_list * flex.pow2(
       overall_si.rms_fo_list/rms_fo_list)
    cc_list = 1/(max(1.e-10,0.5*directional_ssqr_list + 1))

  original_cc_list=deepcopy(cc_list)
  if is_model_based: # jut smooth cc if nec
    fitted_cc=get_fitted_cc(
      cc_list=cc_list,s_value_list=s_value_list,cc_cut=cc_cut,
      scale_using_last=scale_using_last,
      cutoff_after_last_high_point = cutoff_after_last_high_point,)
    cc_list=fitted_cc
    text=" FIT "
  else:
    cc_list=estimate_cc_star(cc_list=cc_list,s_value_list=s_value_list,
      cc_cut=cc_cut,scale_using_last=scale_using_last)
    text=" CC* "


  if not max_possible_cc:
    max_possible_cc=0.01
  if si.target_scale_factors: # not using these
    max_possible_cc=1.
    fraction_complete=1.
  elif (not is_model_based):
    max_possible_cc=1.
    fraction_complete=1.
  else:
    # Define overall CC based on model completeness (CC=sqrt(fraction_complete))

    if fraction_complete is None:
      fraction_complete=max_possible_cc**2

      print(
     "Estimated fraction complete is %5.2f based on low_res CC of %5.2f" %(
          fraction_complete,max_possible_cc), file=out)
    else:
      print(
      "Using fraction complete value of %5.2f "  %(fraction_complete), file=out)
      max_possible_cc=fraction_complete**0.5

  if optimize_b_eff and is_model_based:
    ''' Find b_eff that maximizes expected map-model-cc to model with B=0'''
    best_b_eff = b_eff
    best_weighted_cc = get_target_scale_factors(
      cc_list=cc_list,
      rms_fo_list=rms_fo_list,
      ratio_list=ratio_list,
      b_eff=b_eff,
      max_possible_cc=max_possible_cc,
      **input_info()).weighted_cc
    for i in range(20):
      b_eff_working = 0.1 * i * b_eff
      weighted_cc=get_target_scale_factors(
         cc_list=cc_list,
         rms_fo_list=rms_fo_list,
         ratio_list=ratio_list,
         b_eff = b_eff_working,
         max_possible_cc=max_possible_cc,
         **input_info()).weighted_cc
      if weighted_cc > best_weighted_cc:
        best_b_eff = b_eff_working
        best_weighted_cc = weighted_cc
    print("Optimized effective B value: %.3f A**2 " %(best_b_eff),file=out)
    b_eff = best_b_eff

  info = get_target_scale_factors(
      cc_list=cc_list,
      rms_fo_list=rms_fo_list,
      ratio_list=ratio_list,
      b_eff=b_eff,
      max_possible_cc=max_possible_cc,
      **input_info())
  target_scale_factors = info.target_scale_factors

  if direction_vector:
    print ("\n Analysis for direction vector (%5.2f, %5.2f, %5.2f): "% (
      direction_vector), file = out)

  if info.effective_b:
    print("\nEffective B value for CC*: %.3f A**2 " %(
        info.effective_b),file=out)

  if fraction_complete < min_fraction_complete:
    print("\nFraction complete (%5.2f) is less than minimum (%5.2f)..." %(
      fraction_complete,min_fraction_complete) + "\nSkipping scaling", file=out)
    target_scale_factors=flex.double(target_scale_factors.size()*(1.0,))
  print ("\nAverage CC: %.3f" %(cc_list.min_max_mean().mean),file=out)
  print("\nScale factors vs resolution:", file=out)
  print("Note 1: CC* estimated from sqrt(2*CC/(1+CC))", file=out)
  print("Note 2: CC estimated by fitting (smoothing) for values < %s" %(cc_cut), file=out)
  print("Note 3: Scale = A  CC*  rmsFc/rmsFo (A is normalization)", file=out)
  print("  d_min     rmsFo       rmsFc    CC      %s  Scale " %(
      text), file=out)

  for sthol2,scale,rms_fo,cc,rms_fc,orig_cc in zip(
     input_info.target_sthol2,target_scale_factors,rms_fo_list,
      cc_list,rms_fc_list,
      original_cc_list):
     print("%7.2f  %9.1f  %9.1f %7.3f  %7.3f  %5.2f " %(
       0.5/sthol2**0.5,rms_fo,rms_fc,orig_cc,cc,scale),
        file=out)

  si.target_scale_factors=target_scale_factors
  si.target_sthol2=input_info.target_sthol2
  si.d_min_list=input_info.d_min_list
  si.original_cc_list=original_cc_list # this is CC(half-map1, half_map2)
  si.cc_list=cc_list
  si.rms_fo_list = rms_fo_list
  si.rms_fc_list = rms_fc_list
  si.low_res_cc = cc_list[:low_res_bins].min_max_mean().mean # low-res average
  si.effective_b = info.effective_b
  si.effective_b_f_obs = info.effective_b_f_obs
  si.b_zero = info.b_zero
  si.rms = info.rms
  si.expected_rms_fc_list = expected_rms_fc_list

  return si

def get_sel_para(f_array, direction_vector, minimum_dot = 0.70):
    """get selections based on |dot(normalized_indices, direction_vector)|"""
    u = f_array.unit_cell()
    rcvs = u.reciprocal_space_vector(f_array.indices())
    norms = rcvs.norms()
    norms.set_selected((norms == 0),1)
    index_directions = rcvs/norms
    sel = (flex.abs(index_directions.dot(direction_vector)) > minimum_dot)
    return sel

def get_normalized_weights_para(f_array,direction_vectors, dv,
    include_all_in_lowest_bin = None):
    """Get normalized weights parallel to direction vectors dv"""

    sum_weights = flex.double(f_array.size(),0)
    current_weights = None
    for direction_vector in direction_vectors:
      weights = get_weights_para(f_array, direction_vector,
        include_all_in_lowest_bin = include_all_in_lowest_bin)
      if direction_vector == dv:
        current_weights = weights
      sum_weights += weights
    sum_weights.set_selected((sum_weights <= 1.e-10), 1.e-10)
    return current_weights * (1/sum_weights)

def get_weights_para(f_array, direction_vector,
       weight_by_cos = True,
       min_dot = 0.7,
       very_high_dot = 0.9,
       pre_factor_scale= 10,
       include_all_in_lowest_bin = None):
    """Get weights parallel to direction vectors dv"""
    u = f_array.unit_cell()
    rcvs = u.reciprocal_space_vector(f_array.indices())
    norms = rcvs.norms()
    norms.set_selected((norms == 0),1)
    index_directions = rcvs/norms
    if weight_by_cos:

      weights = flex.abs(index_directions.dot(direction_vector))
      sel = (weights < min_dot)
      weights.set_selected(sel,0)

      weights += (1-very_high_dot)  # move very_high to 1.0
      sel = (weights > 1)
      weights.set_selected(sel,1) # now from (min_dot+(1-very_high_dot) to 1)

      weights = (weights - 1 ) * pre_factor_scale
      sel = (weights > -20)  &  (weights < 20)
      weights.set_selected(sel, flex.exp(weights.select(sel)))
      weights.set_selected(~sel,0)


    else:
      weights = flex.double(index_directions.size(),0)
      sel = (flex.abs(index_directions.dot(direction_vector)) > min_dot)
      weights.set_selected(sel,1)
    if include_all_in_lowest_bin:
      i_bin = 1
      sel       = f_array.binner().selection(i_bin)
      weights.set_selected(sel, 1.0)  # full weights on low-res in all directions
    return weights

def get_nearest_lattice_points(unit_cell, reciprocal_space_vectors):
  """Return lattice points nearest each of a set of reciprocal-space vectors"""
  lattice_points=flex.vec3_double()
  v = matrix.sqr(unit_cell.fractionalization_matrix()).inverse()
  for x in reciprocal_space_vectors:
    lattice_points.append(v*x)
  lattice_points=lattice_points.iround()
  return lattice_points

def get_target_scale_factors(
     f_array = None,
     ratio_list = None,
     rms_fo_list = None,
     cc_list = None,
     n_list = None,
     target_sthol2 = None,
     d_min_list = None,
     max_possible_cc = None,
     pseudo_likelihood = None,
     equalize_power = None,
     is_model_based = None,
     skip_scale_factor = None,
     maximum_scale_factor = None,
     b_eff = None,
     out = sys.stdout):
  """Estimate target scale factors and return a group_args object
   containing them"""

  weighted_cc = 0
  weighted = 0

  target_scale_factors=flex.double()
  sum_w=0.
  sum_w_scale=0.
  for i_bin in f_array.binner().range_used():
    index=i_bin-1
    ratio=ratio_list[index]
    cc=cc_list[index]
    sthol2=target_sthol2[index]
    d_min=d_min_list[index]


    corrected_cc=max(0.00001,min(1.,cc/max(1.e-10,max_possible_cc)))

    if (not is_model_based): # NOTE: cc is already converted to cc*
      scale_on_fo=ratio * corrected_cc  # CC* * rmsFc/rmsFo
    elif b_eff is not None:
      if pseudo_likelihood:
        scale_on_fo=(cc/max(0.001,1-cc**2))
      else: # usual
        scale_on_fo=ratio * min(1.,
          max(0.00001,corrected_cc) * math.exp(min(20.,sthol2*b_eff)) )
    else:
      scale_on_fo=ratio * min(1.,max(0.00001,corrected_cc))

    w = n_list[index]*(rms_fo_list[index])**2
    sum_w += w
    sum_w_scale += w * scale_on_fo**2
    target_scale_factors.append(scale_on_fo)
    weighted_cc += n_list[index]*rms_fo_list[index] * scale_on_fo * corrected_cc
    weighted += n_list[index]*rms_fo_list[index] * scale_on_fo

  weighted_cc = weighted_cc/max(1.e-10,weighted)
  if not pseudo_likelihood and not skip_scale_factor: # normalize
    avg_scale_on_fo = (sum_w_scale/max(1.e-10,sum_w))**0.5
    if equalize_power and avg_scale_on_fo>1.e-10:
      # XXX do not do this if only 1 bin has values > 0
      scale_factor = 1/avg_scale_on_fo
    else: # usual
      scale_factor=1./target_scale_factors.min_max_mean().max
    target_scale_factors=\
      target_scale_factors*scale_factor
  if maximum_scale_factor and \
     target_scale_factors.min_max_mean().max > maximum_scale_factor:
    truncated_scale_factors = flex.double()
    for x in target_scale_factors:
      truncated_scale_factors.append(min(maximum_scale_factor,x ))
    target_scale_factors = truncated_scale_factors

  # Get effective B for cc_list and target_sthol2
  info = get_effective_b(values = cc_list,
    sthol2_values = target_sthol2)
  effective_b = info.effective_b
  b_zero= info.b_zero
  rms= info.rms

  # Also get effective_b for amplitudes
  amplitude_info = get_effective_b(values = rms_fo_list/max(
     1.e-10,rms_fo_list[0]),
    sthol2_values = target_sthol2)
  effective_b_f_obs = amplitude_info.effective_b

  return group_args(
    target_scale_factors = target_scale_factors,
    weighted_cc = weighted_cc,
    effective_b = effective_b,
    effective_b_f_obs = effective_b_f_obs,
    b_zero = b_zero,
    rms = rms
  )

def get_effective_b(values = None,
      sthol2_values = None,
       n_bins_use = None):
  """Estimate effective B corresponding to falloff of a set of values at
   sin**2(theta)/lambda**2 values"""
  ir = iso_refinery(
    values,
    sthol2_values,
    n_bins_use,
    start_with_grid_search=True)
  ir.run()
  return ir.get_info()

  """
  info object looks like:
    group_args_type =
       'get_b_iso values calc_values = b_zero * exp(-b_value* sthol2)',
    effective_b = refined_b_value,
    b_zero =  scale_factor_b_zero,
    sthol2_values = sthol2_values,
    values = values,
    calc_values = calc_values,
    n_bins_use = n_bins_use,
    rms = rms
  """


def get_b_calc( b_value, sthol2_values, values, n_bins_use = None):
  '''Calculate values using b_value and sthol2_values, normalize to
   first element of values.
   If n_bins_use is set, just use that many for rms value'''
  import math
  sum= 0.
  sumx= 0.
  sumy= 0.
  sum2= 0.
  sumn=0.
  calc_values = flex.double()
  for sthol2, value in zip (sthol2_values, values):
    calc_values.append(math.exp(max(-20.,min(20.,
      - b_value* sthol2))))
  b_zero = values[0]/calc_values[0]
  calc_values *= b_zero
  from libtbx.test_utils import approx_equal
  assert approx_equal(values[0],calc_values[0])
  if n_bins_use is None:
    n_bins_use = values.size()
  delta = values[:n_bins_use]-calc_values[:n_bins_use]
  rms = delta.rms()
  return group_args(
    group_args_type = \
      'get_b_iso values calc_values = b_zero * exp(-b_value* sthol2)',
    effective_b = b_value,
    b_zero=b_zero,
    sthol2_values = sthol2_values,
    values=values,
    calc_values=calc_values,
    n_bins_use = n_bins_use,
    rms=rms)


def analyze_aniso(f_array=None,map_coeffs=None,b_iso=None,resolution=None,
     get_remove_aniso_object=True,
     invert = False,
     remove_aniso=None, aniso_obj=None, out=sys.stdout):
  """Analyze anisotropy in f_array or map_coeffs.
     Optionally remove anisotropy and set all directions to mean value.
  return array and analyze_aniso_object
  resolution can be None, b_iso can be None
  if remove_aniso is None, just analyze and return original array"""

  if map_coeffs:  # convert to f and apply
    from cctbx.maptbx.segment_and_split_map import map_coeffs_as_fp_phi
    f_local,phases_local=map_coeffs_as_fp_phi(map_coeffs)
    f_local,f_local_aa=analyze_aniso(f_array=f_local,
       aniso_obj=aniso_obj,
       get_remove_aniso_object=get_remove_aniso_object,
       remove_aniso=remove_aniso, resolution=resolution,out=out)
    return f_local.phase_transfer(phase_source=phases_local,deg=True),f_local_aa

  elif not get_remove_aniso_object:
    return f_array,aniso_obj # don't do anything

  else:  # have f_array and resolution
    if not aniso_obj:
      aniso_obj=analyze_aniso_object()
      aniso_obj.set_up_aniso_correction(f_array=f_array,d_min=resolution,
        b_iso=b_iso, invert = invert)

    if remove_aniso and aniso_obj and aniso_obj.b_cart:
      f_array=aniso_obj.apply_aniso_correction(f_array=f_array)
      print("Removing anisotropy with b_cart=(%7.2f,%7.2f,%7.2f)\n" %(
        aniso_obj.b_cart[:3]), file=out)
    return f_array,aniso_obj

def scale_amplitudes(model_map_coeffs=None,
    map_coeffs=None,
    external_map_coeffs=None,
    first_half_map_coeffs=None,
    second_half_map_coeffs=None,
    si=None,resolution=None,overall_b=None,
    fraction_complete=None,
    min_fraction_complete=0.05,
    map_calculation=True,
    verbose=False,
    out=sys.stdout):
  """Figure out resolution_dependent sharpening to optimally
  match map and model. Then apply it as usual.
  if second_half_map_coeffs instead of model,
  use second_half_map_coeffs same as
  normalized model map_coeffs, except that the target fall-off should be
  skipped (could use fall-off based on a dummy model...)"""

  if model_map_coeffs and (
      not first_half_map_coeffs or not second_half_map_coeffs):
    is_model_based=True
  elif si.target_scale_factors or (
       first_half_map_coeffs and second_half_map_coeffs) or (
        external_map_coeffs):
    is_model_based=False
  else:
    assert map_coeffs
    if si.is_model_sharpening():
      is_model_based=True
    else:
      is_model_based=False

  if si.verbose and not verbose:
    verbose=True

  # if si.target_scale_factors is set, just use those scale factors

  from cctbx.maptbx.segment_and_split_map import map_coeffs_as_fp_phi,get_b_iso

  f_array,phases=map_coeffs_as_fp_phi(map_coeffs)

  (d_max,d_min)=f_array.d_max_min(d_max_is_highest_defined_if_infinite=True)
  n_bins_use = si.n_bins
  while not f_array.binner():
    try:
      f_array.setup_binner(n_bins=si.n_bins,d_max=d_max,d_min=d_min)
      f_array.binner().require_all_bins_have_data(min_counts=1,
        error_string="Please use a lower value of n_bins")
    except Exception as e:
      if n_bins_use > 1:
        n_bins_use -= 1
      else:
        raise Exception(e)

  if resolution is None:
    resolution=si.resolution
  if resolution is None:
    raise Sorry("Need resolution for model sharpening")

  obs_b_iso=get_b_iso(f_array,d_min=resolution)
  print("\nEffective b_iso of observed data: %6.1f A**2" %(obs_b_iso), file=out)

  if not si.target_scale_factors: # get scale factors if don't already have them
    si=calculate_fsc(si=si,
      f_array=f_array,  # just used for binner
      map_coeffs=map_coeffs,
      model_map_coeffs=model_map_coeffs,
      first_half_map_coeffs=first_half_map_coeffs,
      second_half_map_coeffs=second_half_map_coeffs,
      external_map_coeffs=external_map_coeffs,
      resolution=resolution,
      fraction_complete=fraction_complete,
      min_fraction_complete=min_fraction_complete,
      is_model_based=is_model_based,
      cc_cut=si.cc_cut,
      scale_using_last=si.scale_using_last,
      max_cc_for_rescale=si.max_cc_for_rescale,
      pseudo_likelihood=si.pseudo_likelihood,
      verbose=verbose,
      out=out)
    # now si.target_scale_factors array are the scale factors

  # Now create resolution-dependent coefficients from the scale factors

  if not si.target_scale_factors: # nothing to do
    print("\nNo scaling applied", file=out)
    map_data=calculate_map(map_coeffs=map_coeffs,n_real=si.n_real)
    return map_and_b_object(map_data=map_data)
  elif not map_calculation:
    return map_and_b_object()
  else:  # apply scaling
    if si.pseudo_likelihood:
      print("Normalizing structure factors", file=out)
      f_array=quasi_normalize_structure_factors(f_array,set_to_minimum=0.01,
        pseudo_likelihood=si.pseudo_likelihood)
      f_array.setup_binner(n_bins=si.n_bins,d_max=d_max,d_min=d_min)
    map_and_b=apply_target_scale_factors(
      f_array=f_array,phases=phases,resolution=resolution,
      target_scale_factors=si.target_scale_factors,
      n_real=si.n_real,
      out=out)
    return map_and_b

def apply_target_scale_factors(f_array=None,phases=None,
   resolution=None,target_scale_factors=None,
   n_real=None,
   return_map_coeffs=None,out=sys.stdout):
    """Apply target_scale_factors to f_array.  Returns map_and_b object
    with map_data and starting and final b_iso values. """
    from cctbx.maptbx.segment_and_split_map import get_b_iso
    f_array_b_iso=get_b_iso(f_array,d_min=resolution)
    scale_array=f_array.binner().interpolate(
      target_scale_factors, 1) # d_star_power=1
    scaled_f_array=f_array.customized_copy(data=f_array.data()*scale_array)
    scaled_f_array_b_iso=get_b_iso(scaled_f_array,d_min=resolution)
    print("\nInitial b_iso for "+\
      "map: %5.1f A**2     After applying scaling: %5.1f A**2" %(
      f_array_b_iso,scaled_f_array_b_iso), file=out)
    new_map_coeffs=scaled_f_array.phase_transfer(phase_source=phases,deg=True)
    assert new_map_coeffs.size() == f_array.size()
    if return_map_coeffs:
      return new_map_coeffs

    map_data=calculate_map(map_coeffs=new_map_coeffs,n_real=n_real)
    return map_and_b_object(map_data=map_data,starting_b_iso=f_array_b_iso,
      final_b_iso=scaled_f_array_b_iso)
def calculate_map(map_coeffs=None,crystal_symmetry=None,n_real=None):
  """Calculate a map from map_coeffs and crystal_symmetry"""
  if crystal_symmetry is None: crystal_symmetry=map_coeffs.crystal_symmetry()
  from cctbx.development.create_models_or_maps import get_map_from_map_coeffs
  map_data=get_map_from_map_coeffs(
     map_coeffs=map_coeffs,crystal_symmetry=crystal_symmetry, n_real=n_real)
  return map_data

def get_sharpened_map(ma=None,phases=None,b=None,resolution=None,
    n_real=None,d_min_ratio=None):
  """Calculate a sharpened map from amplitudes in ma, the values in b,
   and phases"""
  assert n_real is not None
  sharpened_ma=adjust_amplitudes_linear(ma,b[0],b[1],b[2],resolution=resolution,
     d_min_ratio=d_min_ratio)
  new_map_coeffs=sharpened_ma.phase_transfer(phase_source=phases,deg=True)
  map_data=calculate_map(map_coeffs=new_map_coeffs,n_real=n_real)
  return map_data

def calculate_match(target_sthol2=None,target_scale_factors=None,b=None,resolution=None,d_min_ratio=None,rmsd=None,fraction_complete=None):
  """Calculate residual between target_scale_factors and those calculated
   using effective b values sthol2_1 2 and 3"""

  if fraction_complete is None:
    pass # XXX not implemented for fraction_complete

  if rmsd is None:
    rmsd=resolution/3.
    print("Setting rmsd to %5.1f A based on resolution of %5.1f A" %(
       rmsd,resolution), file=out)

  if rmsd is None:
    b_eff=None
  else:
    b_eff=8*3.14159*rmsd**2

  d_min=d_min_ratio*resolution
  sthol2_2=0.25/resolution**2
  sthol2_1=sthol2_2*0.5
  sthol2_3=0.25/d_min**2
  b0=0.0
  b1=b[0]
  b2=b[1]
  b3=b[2]
  b3_use=b3+b2

  resid=0.
  import math
  value_list=flex.double()
  scale_factor_list=flex.double()

  for sthol2,scale_factor in zip(target_sthol2,target_scale_factors):
    if sthol2 > sthol2_2:
      value=b2+(sthol2-sthol2_2)*(b3_use-b2)/(sthol2_3-sthol2_2)
    elif sthol2 > sthol2_1:
      value=b1+(sthol2-sthol2_1)*(b2-b1)/(sthol2_2-sthol2_1)
    else:
      value=b0+(sthol2-0.)*(b1-b0)/(sthol2_1-0.)

    value=math.exp(value)
    if b_eff is not None:
      value=value*math.exp(-sthol2*b_eff)
    value_list.append(value)
    scale_factor_list.append(scale_factor)
  mean_value=value_list.min_max_mean().mean
  mean_scale_factor=scale_factor_list.min_max_mean().mean
  ratio=mean_scale_factor/mean_value
  value_list=value_list*ratio
  delta_list=value_list-scale_factor_list
  delta_sq_list=delta_list*delta_list
  resid=delta_sq_list.min_max_mean().mean
  return resid

def calculate_adjusted_sa(ma,phases,b,
    resolution=None,
    d_min_ratio=None,
    solvent_fraction=None,
    region_weight=None,
    max_regions_to_test=None,
    sa_percent=None,
    fraction_occupied=None,
    wrapping=None,
    n_real=None):
  """Calculate adjusted surface area for a map"""
  map_data=get_sharpened_map(ma,phases,b,resolution,n_real=n_real,
    d_min_ratio=d_min_ratio)
  from cctbx.maptbx.segment_and_split_map import score_map

  si=score_map(
    map_data=map_data,
    solvent_fraction=solvent_fraction,
    fraction_occupied=fraction_occupied,
    wrapping=wrapping,
    sa_percent=sa_percent,
    region_weight=region_weight,
    max_regions_to_test=max_regions_to_test,
    out=null_out())
  return si.adjusted_sa

def get_kurtosis(data=None):
  """Calculate kurtosis of a list of values in a flex.double array"""
  mean=data.min_max_mean().mean
  sd=data.sample_standard_deviation()
  x=data-mean
  return (x**4).min_max_mean().mean/sd**4

class analyze_aniso_object:
  """ Object to hold information about anisotropy in structure factors"""
  def __init__(self):

    self.b_cart=None
    self.b_cart_aniso_removed=None
    self.b_iso=None

  def set_up_aniso_correction(self,f_array=None,b_iso=None,d_min=None,
     b_cart_to_remove = None, invert = False):

    assert f_array is not None
    if not d_min:
      (d_max,d_min)=f_array.d_max_min(d_max_is_highest_defined_if_infinite=True)

    if b_cart_to_remove and b_iso:
      self.b_cart=b_cart_to_remove
      self.b_cart_aniso_removed = [ -b_iso, -b_iso, -b_iso, 0, 0, 0] # change
      self.b_iso=b_iso
    else:
      from cctbx.maptbx.segment_and_split_map import get_b_iso
      b_mean,aniso_scale_and_b=get_b_iso(f_array,d_min=d_min,
        return_aniso_scale_and_b=True)
      if not aniso_scale_and_b or not aniso_scale_and_b.b_cart:
        return # failed

      if b_iso is None:
        b_iso=b_mean  # use mean
      self.b_iso=b_iso

      self.b_cart=aniso_scale_and_b.b_cart  # current
      self.b_cart_aniso_removed = [ -b_iso, -b_iso, -b_iso, 0, 0, 0] # change
      if invert:
        self.b_cart = tuple([-x for x in self.b_cart])

      # ready to apply

  def apply_aniso_correction(self,f_array=None):

    if self.b_cart is None or self.b_cart_aniso_removed is None:
      return f_array  # nothing to do

    from mmtbx.scaling import absolute_scaling

    u_star= adptbx.u_cart_as_u_star(
      f_array.unit_cell(), adptbx.b_as_u( self.b_cart) )

    f_array.set_observation_type_xray_amplitude()

    u_star_aniso_removed = adptbx.u_cart_as_u_star(
      f_array.unit_cell(), adptbx.b_as_u( self.b_cart_aniso_removed  ) )

    no_aniso_array = absolute_scaling.anisotropic_correction(
      f_array,0.0, u_star ,must_be_greater_than=-0.0001)

    no_aniso_array.set_observation_type_xray_amplitude()
    no_aniso_array = absolute_scaling.anisotropic_correction(
      no_aniso_array,0.0,u_star_aniso_removed,must_be_greater_than=-0.0001)

    no_aniso_array=no_aniso_array.set_observation_type( f_array)
    return no_aniso_array


class refinery:
  """Refine machinery specific for auto-sharpening"""
  def __init__(self,ma,phases,b,resolution,
    residual_target=None,
    solvent_fraction=None,
    region_weight=None,
    max_regions_to_test=None,
    sa_percent=None,
    fraction_occupied=None,
    wrapping=None,
    eps=0.01,
    tol=0.01,
    max_iterations=20,
    n_real=None,
    target_sthol2=None,
    target_scale_factors=None,
    d_min_ratio=None,
    rmsd=None,
    fraction_complete=None,
    dummy_run=False):

    self.ma=ma
    self.n_real=n_real
    self.phases=phases
    self.resolution=resolution
    self.d_min_ratio=d_min_ratio
    self.rmsd=rmsd
    self.fraction_complete=fraction_complete

    self.target_sthol2=target_sthol2
    self.target_scale_factors=target_scale_factors

    self.tol=tol
    self.eps=eps
    self.max_iterations=max_iterations

    self.solvent_fraction=solvent_fraction
    self.region_weight=region_weight
    self.max_regions_to_test=max_regions_to_test
    self.residual_target=residual_target
    self.sa_percent=sa_percent
    self.fraction_occupied=fraction_occupied
    self.wrapping=wrapping

    self.x = flex.double(b)

  def run(self):

    scitbx.lbfgs.run(target_evaluator=self,
      termination_params=scitbx.lbfgs.termination_parameters(
        traditional_convergence_test_eps=self.tol,
                     max_iterations=self.max_iterations,
       ))

  def show_result(self,out=sys.stdout):

    b=self.get_b()
    value = -1.*self.residual(b)
    print("Result: b1 %7.2f b2 %7.2f b3 %7.2f resolution %7.2f %s: %7.3f" %(
     b[0],b[1],b[2],self.resolution,self.residual_target,value), file=out)

    if self.ma:
      self.sharpened_ma=adjust_amplitudes_linear(
         self.ma,b[0],b[1],b[2],resolution=self.resolution,
         d_min_ratio=self.d_min_ratio)
    else:
      self.sharpened_ma=None
    return value

  def compute_functional_and_gradients(self):
    b = self.get_b()
    f = self.residual(b)
    g = self.gradients(b)
    return f, g

  def residual(self,b,restraint_weight=100.):

    if self.residual_target=='kurtosis':
      resid=-1.*calculate_kurtosis(self.ma,self.phases,b,self.resolution,
         n_real=self.n_real,d_min_ratio=self.d_min_ratio)

    elif self.residual_target=='adjusted_sa':
      resid=-1.*calculate_adjusted_sa(self.ma,self.phases,b,
        resolution=self.resolution,
        d_min_ratio=self.d_min_ratio,
        solvent_fraction=self.solvent_fraction,
        region_weight=self.region_weight,
        max_regions_to_test=self.max_regions_to_test,
        sa_percent=self.sa_percent,
        fraction_occupied=self.fraction_occupied,
        wrapping=self.wrapping,n_real=self.n_real)

    elif self.residual_target=='model':
      resid=calculate_match(target_sthol2=self.target_sthol2,
        target_scale_factors=self.target_scale_factors,
        b=b,
        resolution=self.resolution,
        d_min_ratio=self.d_min_ratio,
        emsd=self.rmsd,
        fraction_complete=self.complete)

    else:
      raise Sorry("residual_target must be kurtosis or adjusted_sa or match_target")

    # put in restraint so b[1] is not bigger than b[0]
    if b[1]>b[0]:  resid+=(b[1]-b[0])*restraint_weight
    # put in restraint so b[2] <=0
    if b[2]>0:  resid+=b[2]*restraint_weight
    return resid

  def gradients(self,b):

    result = flex.double()
    for i in range(len(list(b))):
      rs = []
      for signed_eps in [self.eps, -self.eps]:
        params_eps = deepcopy(b)
        params_eps[i] += signed_eps
        rs.append(self.residual(params_eps))
      result.append((rs[0]-rs[1])/(2*self.eps))
    return result

  def get_b(self):
    return list(self.x)

  def callback_after_step(self, minimizer):
    pass # can do anything here

def calculate_kurtosis(ma,phases,b,resolution,n_real=None,
    d_min_ratio=None):
  """Calculate kurtosis from amplitudes and phases for a map"""
  map_data=get_sharpened_map(ma,phases,b,resolution,n_real=n_real,
  d_min_ratio=d_min_ratio)
  return get_kurtosis(map_data.as_1d())

def run(map_coeffs=None,
  b=[0,0,0],
  sharpening_info_obj=None,
  resolution=None,
  residual_target=None,
  solvent_fraction=None,
  region_weight=None,
  max_regions_to_test=None,
  sa_percent=None,
  fraction_occupied=None,
  n_bins=None,
  eps=None,
  wrapping=False,
  n_real=False,
  target_sthol2=None,
  target_scale_factors=None,
  d_min_ratio=None,
  rmsd=None,
  fraction_complete=None,
  normalize_amplitudes_in_resdep=None,
  out=sys.stdout):

  """Run map auto-sharpening"""
  if sharpening_info_obj:
    solvent_fraction=sharpening_info_obj.solvent_fraction
    wrapping=sharpening_info_obj.wrapping
    n_real=sharpening_info_obj.n_real
    fraction_occupied=sharpening_info_obj.fraction_occupied
    sa_percent=sharpening_info_obj.sa_percent
    region_weight=sharpening_info_obj.region_weight
    max_regions_to_test=sharpening_info_obj.max_regions_to_test
    residual_target=sharpening_info_obj.residual_target
    resolution=sharpening_info_obj.resolution
    d_min_ratio=sharpening_info_obj.d_min_ratio
    rmsd=sharpening_info_obj.rmsd
    fraction_complete=sharpening_info_obj.fraction_complete
    eps=sharpening_info_obj.eps
    n_bins=sharpening_info_obj.n_bins
    normalize_amplitudes_in_resdep= \
    sharpening_info_obj.normalize_amplitudes_in_resdep
  else:
    from cctbx.maptbx.segment_and_split_map import sharpening_info
    sharpening_info_obj=sharpening_info()


  if map_coeffs:
    phases=map_coeffs.phases(deg=True)
    ma=map_coeffs.as_amplitude_array()
  else:
    phases=None
    ma=None

  # set some defaults
  if residual_target is None: residual_target='kurtosis'

  assert (solvent_fraction is not None ) or residual_target=='kurtosis'
  assert resolution is not None

  if residual_target=='adjusted_sa' and solvent_fraction is None:
    raise Sorry("Solvent fraction is required for residual_target=adjusted_sa")

  if eps is None and residual_target=='kurtosis':
    eps=0.01
  elif eps is None:
    eps=0.5

  if fraction_complete is None:
    pass # XXX not implemented

  if rmsd is None:
    rmsd=resolution/3.
    print("Setting rmsd to %5.1f A based on resolution of %5.1f A" %(
       rmsd,resolution), file=out)

  if fraction_occupied is None: fraction_occupied=0.20
  if region_weight is None: region_weight=20.
  if sa_percent is None: sa_percent=30.
  if n_bins is None: n_bins=20
  if max_regions_to_test is None: max_regions_to_test=30


  # Get initial value

  best_b=b
  print("Getting starting value ...",residual_target, file=out)
  refined = refinery(ma,phases,b,resolution,
    residual_target=residual_target,
    solvent_fraction=solvent_fraction,
    region_weight=region_weight,
    sa_percent=sa_percent,
    max_regions_to_test=max_regions_to_test,
    fraction_occupied=fraction_occupied,
    wrapping=wrapping,
    n_real=n_real,
    target_sthol2=target_sthol2,
    target_scale_factors=target_scale_factors,
    d_min_ratio=d_min_ratio,
    rmsd=rmsd,
    fraction_complete=fraction_complete,
    eps=eps)


  starting_result=refined.show_result(out=out)
  print("Starting value: %7.2f" %(starting_result), file=out)

  if ma:
    (d_max,d_min)=ma.d_max_min(d_max_is_highest_defined_if_infinite=True)
    ma.setup_binner(n_bins=n_bins,d_max=d_max,d_min=d_min)
    if normalize_amplitudes_in_resdep:
      print("Normalizing structure factors...", file=out)
      ma=quasi_normalize_structure_factors(ma,set_to_minimum=0.01)
  else:
    assert resolution is not None

  refined = refinery(ma,phases,b,resolution,
    residual_target=residual_target,
    solvent_fraction=solvent_fraction,
    region_weight=region_weight,
    max_regions_to_test=max_regions_to_test,
    sa_percent=sa_percent,
    fraction_occupied=fraction_occupied,
    wrapping=wrapping,
    n_real=n_real,
    target_sthol2=target_sthol2,
    target_scale_factors=target_scale_factors,
    d_min_ratio=d_min_ratio,
    rmsd=rmsd,
    fraction_complete=fraction_complete,
    eps=eps)

  starting_normalized_result=refined.show_result(out=out)
  print("Starting value after normalization: %7.2f" %(
     starting_normalized_result), file=out)
  best_sharpened_ma=ma
  best_result=starting_normalized_result
  best_b=refined.get_b()

  refined.run()

  final_result=refined.show_result(out=out)
  print("Final value: %7.2f" %(
     final_result), file=out)

  if final_result>best_result:
    best_sharpened_ma=refined.sharpened_ma
    best_result=final_result
    best_b=refined.get_b()
  print("Best overall result: %7.2f: " %(best_result), file=out)

  sharpening_info_obj.resolution_dependent_b=best_b
  return best_sharpened_ma,phases


if (__name__ == "__main__"):
  args=sys.argv[1:]
  residual_target='kurtosis'
  if 'adjusted_sa' in args:
    residual_target='adjusted_sa'
  resolution=2.9 # need to set this as nominal resolution
  # get data
  map_coeffs=get_amplitudes(args)

  new_map_coeffs=run(map_coeffs=map_coeffs,
    resolution=resolution,
    residual_target=residual_target)
  mtz_dataset=new_map_coeffs.as_mtz_dataset(column_root_label="FWT")
  mtz_dataset.mtz_object().write(file_name='sharpened.mtz')


 *******************************************************************************
