

 *******************************************************************************
xfel/xpp/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/xpp/isoform.py
from __future__ import absolute_import, division, print_function

class phil_validation:
  def __init__(self,param):

    self.param = param
    self.application_level_validation()

  def application_level_validation(self):
    pass

from xfel.merging.database.merging_database import manager
class application(manager):
  def __init__(self,params):
    self.params = params
    self.db = self.connection()
    self.cursor = self.db.cursor()
    self.insert_isoform()
    self.insert_hkl()
    self.db.commit()

  def connection(self):
    print()
    if self.params.db.password is None:
      import getpass
      password = getpass.getpass()
    else:
      password = self.params.db.password

    try:
      import MySQLdb
      db = MySQLdb.connect(passwd=password,
                           user = self.params.db.user,
                           host = self.params.db.host,
                           db = self.params.db.name,compress=False)
      cursor = db.cursor()
      cursor.execute("use %s;"%self.params.db.name)

      return db
    except Exception as e:
      print(e)
      raise RuntimeError("Couldn't connect to mysql database")

  def insert_isoform(self, **kwargs):

    kwargs["name"]=self.params.isoform.name
    cell = self.params.isoform.cell.parameters()
    kwargs["cell_a"]=cell[0]
    kwargs["cell_b"]=cell[1]
    kwargs["cell_c"]=cell[2]
    kwargs["cell_alpha"]=cell[3]
    kwargs["cell_beta"]=cell[4]
    kwargs["cell_gamma"]=cell[5]
    kwargs["lookup_symbol"]=self.params.isoform.lookup_symbol

    (sql, parameters) = self._insert(
      table='`%s_isoforms`' % self.params.experiment_tag,
      **kwargs)

    self.cursor.execute(sql, parameters[0])
    self._lastrowid = self.cursor.lastrowid
    print("isoformID=",self._lastrowid)

  def insert_hkl(self):
    kwargs = {}
    cell = self.params.isoform.cell
    from cctbx.crystal import symmetry

    cs = symmetry(unit_cell = cell,space_group_symbol=self.params.isoform.lookup_symbol)
    mset = cs.build_miller_set(anomalous_flag=False, d_min=self.params.isoform.resolution_limit)

    indices = mset.indices()

    from six.moves import cStringIO as StringIO
    query = StringIO()
    query.write("INSERT INTO `%s_hkls` (h,k,l,isoforms_isoform_id) VALUES "%self.params.experiment_tag)
    firstcomma = ""
    for item in indices:
      query.write(firstcomma); firstcomma=","
      query.write("('%d','%d','%d','%d')"%(item[0],item[1],item[2],self._lastrowid))
    self.cursor.execute( query.getvalue() )

    print("Inserted %d HKLs"%(len(indices)))


 *******************************************************************************


 *******************************************************************************
xfel/xpp/progress_support.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from cctbx.array_family import flex
from cctbx.miller import match_multi_indices
from cctbx.miller import set as mset
from scitbx import matrix

from xfel.merging.database.merging_database import manager
from six.moves import zip
class progress_manager(manager):
  def __init__(self,params,db_experiment_tag,trial,rungroup_id,run):
    self.params = params
    self.db_experiment_tag = db_experiment_tag
    self.trial = trial
    self.rungroup_id = rungroup_id
    self.run = run

  def get_trial_id(self, cursor):
    query = "SELECT trial_id FROM %s_trials WHERE %s_trials.trial = %d"%(self.db_experiment_tag, self.db_experiment_tag, self.trial)
    cursor.execute(query)
    assert cursor.rowcount == 1
    return int(cursor.fetchall()[0][0])

  def get_run_id(self, cursor):
    query = "SELECT run_id FROM %s_runs WHERE %s_runs.run = %d"%(self.db_experiment_tag, self.db_experiment_tag, self.run)
    cursor.execute(query)
    assert cursor.rowcount == 1
    return int(cursor.fetchall()[0][0])

  def get_HKL(self,cursor):
    name = self.db_experiment_tag
    query = '''SELECT H,K,L,%s_hkls.hkl_id from %s_hkls,%s_isoforms WHERE %s_hkls.isoforms_isoform_id = %s_isoforms.isoform_id AND %s_isoforms.name = "%s"'''%(
            name, name, name, name, name, name, self.params["identified_isoform"])
    cursor.execute(query)
    ALL = cursor.fetchall()
    indices = flex.miller_index([(a[0],a[1],a[2]) for a in ALL])
    miller_id = flex.int([a[3] for a in ALL])
    self.miller_set = mset(crystal_symmetry=self.params["observations"][0].crystal_symmetry(), indices=indices)
    self.miller_set_id = miller_id
    # might have to change this to isoform_id next iteration
    cursor.execute('SELECT isoform_id FROM %s_isoforms WHERE name = "%s"'%(
            name, self.params["identified_isoform"]))

    self.isoform_id = cursor.fetchall()[0][0]
    return indices,miller_id

  def connection(self):
    pass

  def scale_frame_detail(self,timestamp,cursor,do_inserts=True,result=None):#, file_name, db_mgr, out):
    if result is None:
      result = self.params

    # If the pickled integration file does not contain a wavelength,
    # fall back on the value given on the command line.  XXX The
    # wavelength parameter should probably be removed from master_phil
    # once all pickled integration files contain it.
    wavelength = result["wavelength"]
    assert (wavelength > 0)

    # Do not apply polarization correction here, as this requires knowledge of
    # pixel size at minimum, and full detector geometry in general.  The optimal
    # redesign would be to apply the polarization correction just after the integration
    # step in the integration code.
    print("Step 3. Correct for polarization.")
    observations = result["observations"][0]
    indexed_cell = observations.unit_cell()

    observations_original_index = observations.deep_copy()

    assert len(observations_original_index.indices()) == len(observations.indices())

    # Now manipulate the data to conform to unit cell, asu, and space group
    # of reference.  The resolution will be cut later.
    # Only works if there is NOT an indexing ambiguity!
    #observations = observations.customized_copy(
    #  anomalous_flag=not self.params.merge_anomalous,
    #  crystal_symmetry=self.miller_set.crystal_symmetry()
    #  ).map_to_asu()

    #observations_original_index = observations_original_index.customized_copy(
    #  anomalous_flag=not self.params.merge_anomalous,
    #  crystal_symmetry=self.miller_set.crystal_symmetry()
    #  )
    observations = observations.customized_copy(anomalous_flag=False).map_to_asu()
    print("Step 4. Filter on global resolution and map to asu")

    #observations.show_summary(f=out, prefix="  ")
    from rstbx.dials_core.integration_core import show_observations
    show_observations(observations)


    print("Step 6.  Match to reference intensities, filter by correlation, filter out negative intensities.")
    assert len(observations_original_index.indices()) \
      ==   len(observations.indices())

    # Ensure that match_multi_indices() will return identical results
    # when a frame's observations are matched against the
    # pre-generated Miller set, self.miller_set, and the reference
    # data set, self.i_model.  The implication is that the same match
    # can be used to map Miller indices to array indices for intensity
    # accumulation, and for determination of the correlation
    # coefficient in the presence of a scaling reference.
    self.miller_set.show_summary(prefix="mset ")

    matches = match_multi_indices(
      miller_indices_unique=self.miller_set.indices(),
      miller_indices=observations.indices())

    slope = 1.0
    offset = 0.0

    print(result.get("sa_parameters")[0])
    have_sa_params = ( type(result.get("sa_parameters")[0]) == type(dict()) )

    observations_original_index_indices = observations_original_index.indices()
    print(list(result.keys()))
    kwargs = {'wavelength': wavelength,
              'beam_x': result['xbeam'],
              'beam_y': result['ybeam'],
              'distance': result['distance'],
              'slope': slope,
              'offset': offset,
              'unique_file_name': timestamp,
              'eventstamp':timestamp,
              'sifoil': 0.0}

    trial_id = self.get_trial_id(cursor)
    run_id = self.get_run_id(cursor)
    kwargs["trials_id"] = trial_id
    kwargs["rungroups_id"] = self.rungroup_id
    kwargs["runs_run_id"] = run_id
    kwargs["isoforms_isoform_id"] = self.isoform_id
    res_ori_direct = matrix.sqr(
        observations.unit_cell().orthogonalization_matrix()).transpose().elems

    kwargs['res_ori_1'] = res_ori_direct[0]
    kwargs['res_ori_2'] = res_ori_direct[1]
    kwargs['res_ori_3'] = res_ori_direct[2]
    kwargs['res_ori_4'] = res_ori_direct[3]
    kwargs['res_ori_5'] = res_ori_direct[4]
    kwargs['res_ori_6'] = res_ori_direct[5]
    kwargs['res_ori_7'] = res_ori_direct[6]
    kwargs['res_ori_8'] = res_ori_direct[7]
    kwargs['res_ori_9'] = res_ori_direct[8]

    kwargs['mosaic_block_rotation'] = result.get("ML_half_mosaicity_deg",[float("NaN")])[0]
    kwargs['mosaic_block_size'] = result.get("ML_domain_size_ang",[float("NaN")])[0]
    kwargs['ewald_proximal_volume'] = result.get("ewald_proximal_volume",[float("NaN")])[0]


    sql, parameters = self._insert(
      table='`%s_frames`' % self.db_experiment_tag,
      **kwargs)
    print(sql)
    print(parameters)
    results = {'frame':[sql, parameters, kwargs]}
    if do_inserts:
      cursor.execute(sql, parameters[0])
      frame_id = cursor.lastrowid
    else:
      frame_id = None

    xypred = result["mapped_predictions"][0]
    indices = flex.size_t([pair[1] for pair in matches.pairs()])

    sel_observations = flex.intersection(
      size=observations.data().size(),
      iselections=[indices])
    set_original_hkl = observations_original_index_indices.select(
      flex.intersection(
        size=observations_original_index_indices.size(),
        iselections=[indices]))
    set_xypred = xypred.select(
      flex.intersection(
        size=xypred.size(),
        iselections=[indices]))
    ''' debugging printout
    print len(observations.data())
    print len(indices)
    print len(sel_observations)
    for x in range(len(observations.data())):
      print x,observations.indices().select(sel_observations)[x],
      print set_original_hkl[x],
      index_into_hkl_id = matches.pairs()[x][0]
      print index_into_hkl_id,
      print self.miller_set.indices()[index_into_hkl_id],
      cursor.execute('SELECT H,K,L FROM %s_hkls WHERE hkl_id = %d'%(
            self.db_experiment_tag, self.miller_set_id[index_into_hkl_id]))

      print cursor.fetchall()[0]
    '''
    print("Adding %d observations for this frame"%(len(sel_observations)))
    kwargs = {'hkls_id': self.miller_set_id.select(flex.size_t([pair[0] for pair in matches.pairs()])),
              'i': observations.data().select(sel_observations),
              'sigi': observations.sigmas().select(sel_observations),
              'detector_x_px': [xy[0] for xy in set_xypred],
              'detector_y_px': [xy[1] for xy in set_xypred],
              'frames_id': [frame_id] * len(matches.pairs()),
              'overload_flag': [0] * len(matches.pairs()),
              'original_h': [hkl[0] for hkl in set_original_hkl],
              'original_k': [hkl[1] for hkl in set_original_hkl],
              'original_l': [hkl[2] for hkl in set_original_hkl],
              'frames_rungroups_id': [self.rungroup_id] * len(matches.pairs()),
              'frames_trials_id': [trial_id] * len(matches.pairs()),
              'panel': [0] * len(matches.pairs())
    }
    if do_inserts:
      # For MySQLdb executemany() is six times slower than a single big
      # execute() unless the "values" keyword is given in lowercase
      # (http://sourceforge.net/p/mysql-python/bugs/305).
      #
      # See also merging_database_sqlite3._insert()
      query = ("INSERT INTO `%s_observations` (" % self.db_experiment_tag) \
              + ", ".join(kwargs) + ") values (" \
              + ", ".join(["%s"] * len(kwargs)) + ")"
      try:
        parameters = list(zip(*list(kwargs.values())))
      except TypeError:
        parameters = [list(kwargs.values())]
      cursor.executemany(query, parameters)
      #print "done execute many"
      #print cursor._last_executed
      results['observations'] = [query, parameters, kwargs]
    else:
      # since frame_id isn't valid in the query here, don't include a sql statement or parameters array in the results
      results['observations'] = [None, None, kwargs]

    return results

'''START HERE
scons ; cxi.xtc_process input.cfg=xppi6115/LI61-PSII-db.cfg input.experiment=xppi6115 input.run_num=137
'''


 *******************************************************************************


 *******************************************************************************
xfel/xpp/progress_utils.py
from __future__ import absolute_import, division, print_function
from cctbx.array_family import flex

from cctbx.miller import set as mset

from cctbx.uctbx import unit_cell
from cctbx.crystal import symmetry
import time

from libtbx.development.timers import Timer

def phil_validation(params):
  return True

def application(params, loop = True):
  from cxi_xdr_xes.cftbx.cspad_ana import db as cxidb
  dbobj = cxidb.dbconnect(params.db.host, params.db.name, params.db.user, params.db.password)
  cursor = dbobj.cursor()
  PM = progress_manager(params, cursor)
  PM.setup_isoforms(cursor)
  PM.setup_runtags(cursor)
  isoforms = PM.isoforms
  del dbobj

  while 1:
    dbobj = cxidb.dbconnect(params.db.host, params.db.name, params.db.user, params.db.password)
    cursor = dbobj.cursor()

    results = {}
    print("Looking for data...")

    for tag in params.run_tags.split(','):
      for isoform in isoforms:
        M = PM.get_HKL(cursor,isoform=isoform,run_tags=tag)
        cell = isoforms[isoform]['cell']
        miller_set = mset(anomalous_flag = False, crystal_symmetry=symmetry(unit_cell=cell, space_group_symbol=isoforms[isoform]['lookup_symbol']), indices=M)
        miller_set.show_comprehensive_summary()

        miller_set.setup_binner(d_min=params.resolution, n_bins=params.n_bins)
        given = miller_set.binner().counts_given()
        ccomplete = miller_set.binner().counts_complete()
        for i_bin in miller_set.binner().range_used():
            sel         = miller_set.binner().selection(i_bin)
            self_sel    = miller_set.select(sel)
            d_max,d_min = self_sel.d_max_min()
            compl       = self_sel.completeness(d_max = d_max)

            n_ref       = sel.count(True)
            if ccomplete[i_bin] == 0.:
              multiplicity = 0.
            else:
              res_highest   = d_min
              multiplicity  = given[i_bin]/ccomplete[i_bin]
            d_range     = miller_set.binner().bin_legend(
                   i_bin = i_bin, show_bin_number = False, show_counts = True)
            fmt = "%3d: %-24s %4.2f %6d mult=%4.2f"
            print(fmt % (i_bin,d_range,compl,n_ref,
                          multiplicity))
        print()
        if len(tag) > 0:
          key = "%s %s"%(tag, isoform)
        else:
          key = isoform
        given_used = flex.int(given).select(flex.size_t(miller_set.binner().range_used()))
        ccomplete_used = flex.int(ccomplete).select(flex.size_t(miller_set.binner().range_used()))
        results[key] = dict(
          multiplicity = flex.sum(given_used)/flex.sum(ccomplete_used),
          completeness = miller_set.completeness(),
          multiplicity_highest = multiplicity,
          completeness_highest = compl,
          resolution_highest = res_highest
        )
    del dbobj
    if not loop:
      return results
    time.sleep(10)

from xfel.merging.database.merging_database import manager
class progress_manager(manager):
  def __init__(self,params, cursor):
    self.params = params
    if params.experiment_tag is None:
      self.db_experiment_tag = params.experiment
    else:
      self.db_experiment_tag = params.experiment_tag

    query = "SELECT trial_id FROM %s_trials WHERE %s_trials.trial = %d"%(self.db_experiment_tag, self.db_experiment_tag, params.trial)
    cursor.execute(query)
    assert cursor.rowcount == 1
    self.trial_id = int(cursor.fetchall()[0][0])

  def setup_isoforms(self, cursor):
    isoform_ids = []
    query = "SELECT DISTINCT(isoforms_isoform_id) from %s_frames WHERE %s_frames.trials_id = %d"%(self.db_experiment_tag, self.db_experiment_tag, self.trial_id)
    cursor.execute(query)
    for entry in cursor.fetchall():
      isoform_ids.append(int(entry[0]))

    d = {}
    for isoform_id in isoform_ids:
      query = "SELECT * FROM %s_isoforms WHERE %s_isoforms.isoform_id = %d"%(self.db_experiment_tag, self.db_experiment_tag, isoform_id)
      cursor.execute(query)
      assert cursor.rowcount == 1
      if_id, name, cell_a, cell_b, cell_c, cell_alpha, cell_beta, cell_gamma, lookup_symbol = cursor.fetchall()[0]
      assert isoform_id == int(if_id)
      cell = unit_cell((cell_a, cell_b, cell_c, cell_alpha, cell_beta, cell_gamma))
      d[name] = dict(
        isoform_id = isoform_id,
        cell = cell,
        lookup_symbol = lookup_symbol)
    self.isoforms = d

  def setup_runtags(self, cursor):
    if self.params.run_tags is None:
      self.params.run_tags = ""

    if len(self.params.run_tags) > 0:
      return

    all_tags = []
    name = self.db_experiment_tag
    query = """ SELECT rg.startrun, rg.endrun
                  FROM %s_rungroups rg
                  JOIN %s_trial_rungroups trg
                    ON rg.rungroup_id = trg.rungroups_id
                    AND trg.trials_id = %d
                    AND trg.active
               """%(name, name, self.trial_id)
    cursor.execute(query)

    for entry in cursor.fetchall():
      startrun, endrun = entry
      query = """SELECT DISTINCT(runs.tags)
                   FROM %s_runs runs
                   WHERE runs.run_id >= %d
              """%(name, startrun)
      if endrun is not None:
        query += " AND runs.run_id <= %d"%endrun
      cursor.execute(query)

      for row in cursor.fetchall():
        tags = row[0]
        if tags is None: continue
        if len(tags) == 0: continue
        all_tags.extend(tags.split(','))
    self.params.run_tags = ','.join(set(all_tags))

    print("Found these run tags:", self.params.run_tags)

  def get_HKL(self,cursor,isoform,run_tags):
    name = self.db_experiment_tag
    if run_tags is not None:
      extrajoin = "JOIN %s_runs runs ON frames.runs_run_id = runs.run_id"%name
      for tag in run_tags.split():
        tag = tag.strip()
        extrajoin += " AND runs.tags LIKE '%%%s%%'"%tag
    else:
      extrajoin = ""
    if self.params.include_negatives:
      extrawhere = ""
    else:
      extrawhere = "WHERE obs.i >= 0"

    query = """SELECT hkls.h, hkls.k, hkls.l
               FROM %s_observations obs
               JOIN %s_hkls hkls ON obs.hkls_id = hkls.hkl_id
               JOIN %s_isoforms isos ON hkls.isoforms_isoform_id = isos.isoform_id
                 AND isos.isoform_id = %d
               JOIN %s_frames frames ON obs.frames_id = frames.frame_id
                 AND frames.trials_id = %d
               JOIN %s_trial_rungroups trg
                 ON trg.trials_id = frames.trials_id
                 AND trg.rungroups_id = frames.rungroups_id
                 AND trg.active
               %s
               %s"""%(
               name, name, name, self.isoforms[isoform]['isoform_id'], name, self.trial_id, name, extrajoin, extrawhere)

    #print query
    if len(run_tags) > 0:
      print("%s, isoform %s"%(run_tags, isoform))
    else:
      print("isoform %s"%isoform)
    T = Timer("Reading db...")
    cursor.execute(query)
    del T
    T = Timer("Getting results...")
    ALL = cursor.fetchall()
    del T
    T = Timer("Parsing results...")
    indices = flex.miller_index([(a[0],a[1],a[2]) for a in ALL])
    del T

    print("ISOFORM %s:"%(isoform),len(indices),"result")
    return indices

  def connection(self):
    pass


 *******************************************************************************


 *******************************************************************************
xfel/xpp/simulate.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import time

class phil_validation:
  def __init__(self,param):

    self.param = param
    self.application_level_validation()

  def application_level_validation(self):
    pass

class file_table:
  def __init__(self,param,query,enforce80=False,enforce81=False):
    from six.moves import urllib
    auth_handler = urllib.request.HTTPBasicAuthHandler()
    auth_handler.add_password(realm="Webservices Auth",
                          uri="https://pswww.slac.stanford.edu",
                          user=param.web.user,
                          passwd=param.web.password)
    opener = urllib.request.build_opener(auth_handler)
    # ...and install it globally so it can be used with urlopen.
    urllib.request.install_opener(opener)
    R = urllib.request.urlopen(query)
    if R.getcode() != 200:
      print("Status",R.getcode())
    import xml.etree.ElementTree
    X = xml.etree.ElementTree.XML(R.read())
    if X.tag == 'error':
      print('Error getting run list:', X.text)
    self.runs = []
    self.items = []
    self.times = []
    for child in X:
      if child.tag=="issued":
        for change in child:
          if change.tag == "change":
            for token in change:
              if token.tag == "run":
                #from IPython import embed; embed()
                self.runs.append(int(token.text))
              if token.tag == "item":
                self.items.append(token.text)
              if token.tag == "time":
                self.times.append(token.text)
    self.unixtimes = []
    for item in self.times:
      self.unixtimes.append(  time.mktime(time.strptime(item[:19],"%Y-%m-%dT%H:%M:%S"))  )
    self.rundict = {}
    for i in range(len(self.runs)):
      if enforce80: #assume the required FEE spectrometer data is in stream 80
        if self.items[i].find("-s80-") < 0: continue
      if enforce81: #assume the required FEE spectrometer data is in stream 81
        if self.items[i].find("-s81-") < 0: continue
      if self.rundict.get(self.runs[i],0)==0:
        self.rundict[self.runs[i]]=dict(items=[],unixtimes=[])
      self.rundict[self.runs[i]]["items"].append(self.items[i])
      self.rundict[self.runs[i]]["unixtimes"].append(self.unixtimes[i])

  def get_runs(self,filter=None):
    values = []
    for key in self.rundict:
      if filter is None or (filter[0]<=key and key<=filter[1]):
        values.append(dict(run=key,time=max(self.rundict[key]["unixtimes"])))
    return values

class application:
  def __init__(self,param):
    self.param = param
    query = self.get_query1()
    FT = file_table(param,query)
    runs = FT.get_runs(filter = self.param.runlimits)
    # finally sort them by time
    runs.sort ( key = lambda A : A["time"])

    # Now prepare for the simulation
    data_timespan = runs[-1]["time"] - runs[0]["time"]
    simulation_timespan = data_timespan / self.param.speedup.factor
    print("Simulation duration %5.2f sec"%simulation_timespan)
    import time
    begin = time.time()
    runptr = 0
    while runptr<len(runs):
      time.sleep(1)
      dataclock = runs[runptr]["time"] - runs[0]["time"]
      if dataclock < self.param.speedup.factor * ( time.time() - begin):
        print("Run %d, time %s"%(runs[runptr]["run"],time.asctime(time.localtime(runs[runptr]["time"]))))
        runptr+=1

  def get_query1(self):
    return "https://pswww.slac.stanford.edu/ws-auth/dataexport/placed?exp_name=%s"%(
      self.param.data[0])


 *******************************************************************************


 *******************************************************************************
xfel/xpp/split_pickles.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from libtbx import easy_pickle
from cctbx import miller # import dependency
import os

# jiffy script specifically for xppk4715, where some background effects require splitting integrated data into two halfs
# Run this script in a cctbx.xfel 'results' directory.  The script will search the directory for trials and rungroups,
# find integration pickles, and split them into left, right, middle and nomid (left + right, no middle), according to the
# key fuller_kapton_absorption_correction

for runroot in os.listdir("."):
  if not os.path.isdir(runroot):
    continue
  for rg in os.listdir(runroot):
    if not os.path.isdir(os.path.join(runroot,rg)):
      continue
    intpath = os.path.join(runroot, rg, "integration")
    destroot_l = os.path.join(intpath, "left")
    destroot_r = os.path.join(intpath, "right")
    destroot_m = os.path.join(intpath, "middle")
    destroot_n = os.path.join(intpath, "nomid")
    if not os.path.exists(destroot_l):
      os.makedirs(destroot_l)
    if not os.path.exists(destroot_r):
      os.makedirs(destroot_r)
    if not os.path.exists(destroot_m):
      os.makedirs(destroot_m)
    if not os.path.exists(destroot_n):
      os.makedirs(destroot_n)
    for picklename in os.listdir(intpath):
      if os.path.splitext(picklename)[1] != ".pickle":
        continue
      picklepath = os.path.join(intpath, picklename)
      print(picklepath)
      destpath_l = os.path.join(destroot_l, os.path.splitext(picklename)[0] + "_l.pickle")
      destpath_r = os.path.join(destroot_r, os.path.splitext(picklename)[0] + "_r.pickle")
      destpath_m = os.path.join(destroot_m, os.path.splitext(picklename)[0] + "_m.pickle")
      destpath_n = os.path.join(destroot_n, os.path.splitext(picklename)[0] + "_n.pickle")
      #if os.path.exists(destpath_l): continue
      try:
        data = easy_pickle.load(picklepath)
      except Exception as e:
        print("Pickle failed to load", picklepath)
        continue
      if not "fuller_kapton_absorption_correction" in data:
        continue

      corr = data["fuller_kapton_absorption_correction"]

      from xfel.cxi.cspad_ana.rayonix_tbx import get_rayonix_pixel_size
      from scitbx.array_family import flex
      pixel_size = get_rayonix_pixel_size(2)
      bx = data['xbeam'] / pixel_size
      by = data['ybeam'] / pixel_size

      preds = data['mapped_predictions']

      sel_l = []
      sel_r = []
      sel_mid = []
      sel_nomid = []

      all_good = True
      for i in range(len(preds)):
        # all preds left of the beam center
        p1_sel = preds[i].parts()[1] < bx
        # mostly will be preds right of the beam center, but includes a few to the left of middle strip
        p2_sel = (corr[i] != 1.0) & (corr[i] <= 1.5)
        # the rest
        mid_sel = (~p1_sel) & (~p2_sel)
        mid = preds[i].select(mid_sel)

        # p2 should really be all preds right of the middle strip, ignoring the odd ones to the left of it
        if len(mid) > 0:
          meanx = flex.mean(mid.parts()[1])
          p2_sel = p2_sel & (preds[i].parts()[1] > meanx)

        # the rest, this time including the odd bits from the right half
        mid_sel = (~p1_sel) & (~p2_sel)

        # preds without the middle strip
        nomid_sel = p1_sel | p2_sel

        a = p1_sel.count(True)
        b = p2_sel.count(True)
        c = mid_sel.count(True)
        all_good = a + b + c == len(preds[i])

        sel_l.append(p1_sel)
        sel_r.append(p2_sel)
        sel_mid.append(mid_sel)
        sel_nomid.append(nomid_sel)

      if not all_good:
        print("Weird one", picklepath)
        continue

      dl = {}
      dr = {}
      dm = {}
      dn = {}

      for key in data:
        if key == "correction_vectors":
          # drop correction_vectors as they aren't as easy to split up
          continue
        elif key in ["current_cb_op_to_primitive", "effective_tiling", "pointgroup", "identified_isoform"]:
          dl[key] = data[key]
          dr[key] = data[key]
          dm[key] = data[key]
          dn[key] = data[key]
          continue
        elif key == "current_orientation":
          from cctbx import crystal_orientation
          dl[key] = [crystal_orientation.crystal_orientation(c) for c in data[key]]
          dr[key] = [crystal_orientation.crystal_orientation(c) for c in data[key]]
          dm[key] = [crystal_orientation.crystal_orientation(c) for c in data[key]]
          dn[key] = [crystal_orientation.crystal_orientation(c) for c in data[key]]
          continue
        try:
          assert len(data[key]) == len(sel_l)
          islist = True
        except TypeError as e:
          islist = False

        if islist:
          val_l = []
          val_r = []
          val_m = []
          val_n = []
          for i, item in enumerate(data[key]):
            if hasattr(item, "select"):
              val_l.append(item.select(sel_l[i]))
              val_r.append(item.select(sel_r[i]))
              val_m.append(item.select(sel_mid[i]))
              val_n.append(item.select(sel_nomid[i]))
            else:
              val_l.append(item)
              val_r.append(item)
              val_m.append(item)
              val_n.append(item)
          dl[key] = val_l
          dr[key] = val_r
          dm[key] = val_m
          dn[key] = val_n
        else:
          dl[key] = data[key]
          dr[key] = data[key]
          dm[key] = data[key]
          dn[key] = data[key]
      easy_pickle.dump(destpath_l, dl)
      easy_pickle.dump(destpath_r, dr)
      easy_pickle.dump(destpath_m, dm)
      easy_pickle.dump(destpath_n, dn)


 *******************************************************************************
