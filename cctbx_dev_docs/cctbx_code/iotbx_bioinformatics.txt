

 *******************************************************************************
iotbx/bioinformatics/__init__.py
"""
Tools for bioinformatics (reading/writing/comparison of sequence files.)
"""
from __future__ import absolute_import, division, print_function
from operator import itemgetter
from libtbx.utils import Abort
from libtbx import group_args
from six.moves import cStringIO as StringIO
import re
import operator
import os.path
import sys
from functools import reduce
from six.moves import range
from six.moves import zip
from libtbx.utils import Sorry

# Wrap lines that are longer than 'width'
def wrap(text, width):
  """Wrap lines that are longer than 'width'"""

  return re.findall( "[^\n]{1,%d}" % width, text )


# Sequence headers
class ebi_pdb_header(object):
  """
  EBI-style parsed header
  """

  regex = re.compile(
    r"""
    ^ > \s* PDB \s* :
    ( [^_]+ ) _
    ( \w* ) \s*
    """,
    re.VERBOSE
    )

  format = ">PDB:XXXX_X"

  def __init__(self, identifier, chain):

    self.identifier = identifier
    self.chain = chain


  def __str__(self):

    return ">PDB:%s_%s" % ( self.identifier, self.chain )


  def parse(cls, data):

    match = cls.regex.search( data )

    if not match:
      raise ValueError( cls.format, str( data ))

    ( identifier, chain ) = match.groups()
    return cls( identifier = identifier.upper(), chain = chain.upper() )

  parse = classmethod( parse )


class generic_sequence(object):
  """
  Sequence
  """

  def __init__(self, header, body):

    self.header = header
    self.body = body


  def format(self, width):

    return "\n".join(
      [ str( self.header )[:width] ] + wrap( self.body, width )
      )


  def reinterpret_header(self, header):

    self.header = header.parse( data = str( self.header ) )


  def __len__(self):

    return len( self.body )


  def __str__(self):

    return self.format( 70 )


# Sequence formats
class sequence(object):
  """
  Sequence
  """

  def __init__(self, sequence, name = ""):

    self.name = name
    self.sequence = "".join(
      [ char for char in sequence if not char.isspace() ]
      )


  def format(self, width):

    return "\n".join(
      [ ( ">%s" % self.name )[:width] ]
      + wrap( self.sequence, width )
      )


  def __str__(self):

    return self.format( 70 )

  def __hash__(self):
    '''
    Return a UID (hash) for the instanciated object. This method is required
    to be implemented if __eq__ is implemented and if the object is immutable.
    Immutable objects can be then used as keys for dictionaries.
    '''

    #NOTE: To my knowledge objects of this class are used as keyword in a dictionary only
    #by Sculptor.

    #NOTE: id(self) is already unique but hash(id(self)) distributes better the hashed values
    #this means that two consecutive id will have very different hash(id(self)) making
    #hashtable searches more robust when using them as keys of dictionaries. Difference in
    #performance are irrelevant.

    return hash(id(self))

  def __eq__(self, other):
    '''
    This method is used any time the equality among two instances of this class must be tested.
    Two sequences are equal if their sequence attribute is equal. Although self and other are
    two different objects. In particular two sequences might have a different name but they are equal
    if they have the same sequence.
    '''

    return isinstance( other, sequence ) and self.sequence == other.sequence


  def __ne__(self, other): # for Python 2.2 compatibility

    return not self.__eq__(other)


  def __len__(self):

    return len( self.sequence )


  def __getitem__(self, index):

    return self.sequence[ index ]


class fasta_sequence(sequence):
  """
  Fasta sequence
  """

  def __init__(self, sequence, name = "", description = ""):

    super( fasta_sequence, self ).__init__( sequence, name )
    self.description = description


  def format(self, width):

    return "\n".join(
      [ ( ">%s %s" % ( self.name, self.description ) )[:width] ]
      + wrap( self.sequence, width )
      )


class pir_sequence(sequence):
  """
  Pir sequence
  """

  def __init__(self, sequence, name = "", type = "P1", description = ""):

    super( pir_sequence, self ).__init__( sequence, name )
    self.type = type
    self.description = description


  def format(self, width):

    return (
      ( ">%s;%s" % ( self.type, self.name ) )[:width] + "\n"
        + self.description[:width] + "\n"
        + "\n".join(
          [ "  " + line for line in wrap( self.sequence, width - 2 ) ]
          )
          + "*"
      )


class pdb_sequence(sequence):
  """
  PDB sequence
  """

  def __init__(self, sequence, name = "", chain = ""):

    super( pdb_sequence, self ).__init__( sequence, name )
    self.chain = chain


  def format(self, width):

    return (
      ( ">PDB:%s_%s" % ( self.name, self.chain ) )[:width] + "\n"
        + "\n".join(
          [ "  " + line for line in wrap( self.sequence, width - 2 ) ]
          )
      )


# Alignment middle/match line calculation
class midline(object):
  """
  Alignment midline
  """

  def __init__(self, identical = "*", conserved = ":", semi = ".", differ = " "):

    self.identical = identical
    self.conserved = conserved
    self.semi = semi
    self.differ = differ


  def compare(self, alignments, gap = "-"):

    result = []
    if (len(alignments) != 0):
      for equi in zip( *alignments ):
        if gap not in equi:
          result.append(self.conservation_code( equi ))
        else:
          result.append(self.differ)
    return "".join(result)


  def conservation_code(self, residues):

    for r in residues:
      if (residues[0] != r): return self.differ
    return self.identical


# Alignment formats
class alignment(object):
  """
  Alignment
  """

  def __init__(self, alignments, names, gap = "-"):

    # The number of names should match the number of alignments
    if len( alignments ) != len( names ):
      raise ValueError(
        "'alignments' and 'names' do not have the same length"
      )

    self._set_alignments( alignments = alignments )

    self.names = names
    self.alignments = alignments
    self.gap = gap


  def aligned_positions_count(self):

    result = 0

    for equi in zip( *self.alignments ):
      if self.gap not in equi:
        result += 1

    return result


  def identity_count(self):

    result = 0
    if (len(self.alignments) != 0):
      for equi in zip( *self.alignments ):
        if (self.gap in equi): continue
        for p in equi[1:]:
          if (p != equi[0]): break
        else:
          result += 1
    return result



  def shortest_seqlen(self): # returns the number of aligned residues

    if not self.alignments:
      return 0

    alignment_length = len( self.alignments[0] )
    shortest_sequence_length = min(
      [ alignment_length - seq.count( self.gap ) for seq in self.alignments ]
      )
    return shortest_sequence_length


  def identity_fraction(self):

    if not self.alignments:
      return 1.0

    shortest_sequence_length = self.shortest_seqlen()

    if shortest_sequence_length == 0:
      return 1.0

    return float( self.identity_count() ) / shortest_sequence_length


  def sequence_strings(self):

    return [ "".join( [ c for c in seq if c != self.gap ] )
        for seq in self.alignments ]


  def length(self):

    if self.alignments:
      return len( self.alignments[0] )
    return 0


  def multiplicity(self):

    return len( self.alignments )


  def format(self, width):

    return "\n\n".join( [ sequence( seq, name ).format( width )
      for ( seq, name ) in zip( self.alignments, self.names ) ] )


  def simple_format(self, width):

    return self.format( width = width )


  def extend(self, sequences):

    if len( sequences ) != self.multiplicity():
      raise ValueError("Inconsistent extension sequence set")

    pre_alis = []
    pre_gaps = []
    post_alis = []
    post_gaps = []

    for ( a, s ) in zip( self.alignments, sequences ):
      partial = "".join( [ c for c in a if c != self.gap ] )
      pos = s.find( partial )

      if pos == -1:
        raise ValueError("Alignment does not match sequence")

      assert 0 <= pos
      pre_alis.append( s[:pos] )
      pre_gaps.append( self.gap * pos )

      end = pos + len( partial )
      post_alis.append( s[ end : ] )
      post_gaps.append( self.gap * ( len( s ) - end ) )

    alis = []

    for ( index, a ) in enumerate( self.alignments ):
      alis.append(
        reduce(
          operator.add,
          pre_gaps[ : index ] + [ pre_alis[ index ] ] + pre_gaps[ index + 1 : ]
          )
        + a
        + reduce(
          operator.add,
          post_gaps[ : index ] + [ post_alis[ index ] ] + post_gaps[ index + 1 : ]
          )
        )

    self._set_alignments( alignments = alis )


  def copy(self, **kwd):

      keywords = {
          "alignments": self.alignments,
          "names": self.names,
          "gap": self.gap,
          }
      keywords.update( self.extra() )
      keywords.update( kwd )
      return self.__class__( **keywords )


  def extra(self):

      return {}


  def assign_as_target(self, index):

    if index < 0 or self.multiplicity() <= index:
      raise IndexError("Sequence not found in alignment")

    self.names = ( [ self.names[ index ] ] + self.names[ : index ]
      + self.names[ index + 1 : ] )
    self.alignments = ( [ self.alignments[ index ] ]
      + self.alignments[ : index ] + self.alignments[ index + 1 : ] )


  def _set_alignments(self, alignments):

    # All alignments should have the same length
    for o in alignments[1:]:
      if (len( alignments[0] ) != len( o )):
        raise ValueError("'alignments' do not have the same length")

    self.alignments = alignments


  def __str__(self):

    return self.format( 70 )


class fasta_alignment(alignment):
  """
  Fasta alignment
  """

  def __init__(self, alignments, names, descriptions, gap = "-"):

    # The number of names, types and description should be equal
    if len( names ) != len( descriptions ):
      raise ValueError(
        "Inconsistent 'alignments' and 'descriptions' attributes"
        )

    super( fasta_alignment, self ).__init__( alignments, names, gap )
    self.descriptions = descriptions


  def format(self, width):

    return "\n\n".join( [
      fasta_sequence( sequence, name, description ).format( width )
      for ( sequence, name, description )
      in zip( self.alignments, self.names, self.descriptions )
      ] )


  def simple_format(self, width):

    return self.format( width = width )


  def extra(self):

    return {
      "descriptions": self.descriptions,
      }


  def __str__(self):

    return self.format( 70 )


class pir_alignment(alignment):
  """
  Pir alignment
  """

  def __init__(self, alignments, names, types, descriptions, gap = "-"):

    # The number of names, types and description should be equal
    if ( len( names ) != len( types )
      or len( names ) != len( descriptions ) ):
      raise ValueError(
        "Inconsistent 'alignments', 'types' and 'descriptions' attributes"
        )

    super( pir_alignment, self ).__init__( alignments, names, gap )
    self.types = types
    self.descriptions = descriptions


  def format(self, width):

    return "\n\n".join( [
      pir_sequence( sequence, name, type, description ).format( width )
      for ( sequence, name, type, description )
      in zip( self.alignments, self.names, self.types, self.descriptions )
      ] )


  def simple_format(self, width):

    return self.format( width = width )


  def extra(self):

    return {
      "descriptions": self.descriptions,
      "types": self.types,
      }


  def __str__(self):

    return self.format( 70 )


class clustal_alignment(alignment):
  """
  Clustal alignment
  """

  def __init__(self, alignments, names, program = "CLUSTAL 2.0.9", gap = "-"):

    super( clustal_alignment, self ).__init__( alignments, names, gap )
    self.program = program

  def make_aln_info(self, caption, alignment, aln_width):

    running_total = 0
    aln_info = []

    for line in wrap( alignment, aln_width ):
      running_total += len( line ) - line.count( self.gap )
      aln_info.append( ( caption, line, running_total ) )

    return aln_info


  def format(self, aln_width, caption_width, number_width = None, middle_line=None):

    if not middle_line:
      middle_line = midline().compare(
        alignments = self.alignments,
        gap = self.gap
        )

    elif len( middle_line ) != self.length():
      raise ValueError("Incorrect midline length")

    if number_width is None:
      number_width = self.length_digits()

    # All alignments
    aln_infos = [
      self.make_aln_info(
        caption = name.ljust( caption_width )[:caption_width ],
        alignment = alignment,
        aln_width = aln_width
        )
      for ( name, alignment ) in zip( self.names, self.alignments )
      ]

    # Midline
    aln_infos.append(
      [ ( " " * caption_width, line, "" )
        for line in wrap( middle_line, aln_width ) ]
      )

    def fmt_num(num):
      if num: return " " + str( num ).rjust( number_width )
      return ""
    return (
      "%s multiple sequence alignment\n\n" % self.program
      + "\n\n".join(
        [
          "\n".join(
            [ "%s %s%s" % ( cap, ali, fmt_num(num) )
              for ( cap, ali, num ) in zipped_infos ]
            )
          for zipped_infos in zip( *aln_infos )
          ]
        )
      )


  def length_digits(self):

    import math
    return int( math.log10( self.length() ) ) + 1


  def simple_format(self, width):

    aln_width = int( 0.8 * width )
    num_width = min( self.length_digits(), width - aln_width )
    caption_width = max( width - ( aln_width + 1 ) - ( num_width + 1 ), 1 )
    return self.format(
      aln_width = aln_width,
      caption_width = caption_width,
      number_width = num_width,
      )


  def extra(self):

    return {
      "program": self.program,
      }


  def __str__(self):

    return self.format( 60, 15 )


# Sequence file parser
class generic_sequence_parser(object):
  """
  General-purpose sequence parser
  """

  def __init__(self, regex, type):

    self.regex = regex
    self.type = type


  def parse(self, text, **kwargs):

    objects = []
    non_compliant = []

    while True:
      match = self.regex.search( text )

      if match:
        i = text.find(match.group(0))
        assert i >= 0
        unknown, text = text[:i], text[i+len(match.group(0)):]

        if unknown and not unknown.isspace():
          non_compliant.append( unknown )

        objects.append(
          self.type( **dict( list(kwargs.items()) + list(match.groupdict().items()) ) )
          )

      else:
        break

    if text:
      non_compliant.append( text )

    return ( objects, non_compliant )


  def __call__(self, text, **kwargs):

    return self.parse( text, **kwargs )

# New-style sequence parsing
class SequenceFormat(object):
  """
  Bundle of independent parsing data
  """

  def __init__(self, regex, expected, create):

    self.regex = regex
    self.expected = expected
    self.create = create


st_separated_fasta = SequenceFormat(
  regex = re.compile(
    r"""
    ^
    ( > [^\n]* ) \n
    ( [^>^*]* )
    \*?\s*
    """,
    re.MULTILINE | re.VERBOSE
    ),
  expected = ">Header\nSEQUENCE",
  create = lambda headers, body: generic_sequence(
      header = headers[0],
      body  = body
      )
  )


def partition_string(data, regex):
  """
  Partition sequence files
  """

  position = 0

  for match in regex.finditer( data ):
    current = match.start()
    yield ( match.groups(), ( position, current, data[ position : current ] ) )
    position = match.end()

  remaining = data[ position : ].strip()

  if remaining:
    yield ( (), ( position, len( data ), remaining ) )

  raise StopIteration


def parse_sequence_str(data, format):
  """
  Generic purpose sequence header parser
  """

  partition = partition_string( data = data, regex = format.regex )

  for ( groups, ( n_start, n_end, n_data ) ) in partition:
    if n_data.strip():
      raise ValueError(
        "Uninterpretable block from %s to %s:\nExpected: %s\nFound: %s" % (
          n_start,
          n_end,
          format.expected,
          n_data,
          ))

    body = "".join( [ c for c in groups[-1] if not c.isspace() ] )
    yield format.create( headers = groups[:-1], body = body )


# Sequence parser instances that can be used as functions
seq_sequence_parse = generic_sequence_parser(
  regex = re.compile(
    r"""
    ^ > [ \t\v\f]*
    (?P<name> [^\n]* ) \n
    (?P<sequence> [^>^*]* )
    \*?\s*
    """,
    re.MULTILINE | re.VERBOSE
    ),
  type = sequence
  )

fasta_sequence_parse = generic_sequence_parser(
  regex = re.compile(
    r"""
    ^ > [ \t]*
    (?P<name> [^\s^:]+ ) [ :]? [ \t]*
    (?P<description> [^\n]* ) \n
    (?P<sequence> [^>]* )
    \s*
    """,
    re.MULTILINE | re.VERBOSE
    ),
  type = fasta_sequence
  )

pir_sequence_parse = generic_sequence_parser(
  regex = re.compile(
    r"""
    ^ >
    (?P<type> [PFDRN][13LC] ) ;
    (?P<name> \S* ) \n
    (?P<description> [^\n]* ) \n
    (?P<sequence> [^>^\*]* )
    \* \s*
    """,
    re.MULTILINE | re.VERBOSE
    ),
  type = pir_sequence
  )

tolerant_pir_sequence_parse = generic_sequence_parser(
  regex = re.compile(
    r"""
    ^ >
    (?P<name> [^\n]* ) \n
    (?P<description> [^\n]* ) \n
    (?P<sequence> [^>^\*]* )
    \*? \s*
    """,
    re.MULTILINE | re.VERBOSE
    ),
  type = pir_sequence
  )

lineseparated_sequence_parse = generic_sequence_parser(
  regex = re.compile(
    r"""
    (?P<sequence> [^>^*]*? )
    \*?\s*\n(?:\n|\Z)
    """,
    re.MULTILINE | re.VERBOSE
    ),
  type = sequence
  )

dbfetch_sequence_parse = generic_sequence_parser(
  regex = re.compile(
    r"""
    ^ > \s* PDB \s* :
    (?P<name> [^_]+ ) _
    (?P<chain> \w* ) \s* \n
    (?P<sequence> [^>]* )
    \s*
    """,
    re.MULTILINE | re.VERBOSE
    ),
  type = pdb_sequence
  )

_tf_split_regex = re.compile(
  r"""
  \A \s*
  > ( [^\n]* ) \n
  ( .* )
  \Z
  """,
  re.MULTILINE | re.VERBOSE | re.DOTALL
  )

def tf_sequence_parse(text):
  """
  plain text sequence parser used for SOLVE/RESOLVE files
  """

  match = _tf_split_regex.search( text )

  if not match:
    return ( [], [ text ] )

  ( name, data ) = match.groups()

  return lineseparated_sequence_parse.parse(
    text = data,
    name = name.strip()
    )


_implemented_sequence_parsers = {
  ".fasta": fasta_sequence_parse,
  ".fa": fasta_sequence_parse,
  ".faa" : fasta_sequence_parse,
  ".pir": pir_sequence_parse,
  ".seq": seq_sequence_parse,
  ".dat": seq_sequence_parse,
  }

def sequence_parser_for(file_name):
  """Identify sequence parser suitable for file_name"""
  ( name, extension ) = os.path.splitext( file_name )

  return _implemented_sequence_parsers.get( extension )


def sequence_parser_for_extension(extension):
  """Identify sequence parser suitable for extension"""

  return _implemented_sequence_parsers.get( extension )


def known_sequence_formats():
  """List known sequence formats"""
  return list(_implemented_sequence_parsers.keys())


def parse_sequence(data):
  """Parse a sequence"""
  parsers = [
    fasta_sequence_parse,
    dbfetch_sequence_parse,
    pir_sequence_parse,
    tolerant_pir_sequence_parse,
    seq_sequence_parse,
    lineseparated_sequence_parse,
    tf_sequence_parse,
    ]

  results = []

  for p in parsers:
    ( seqs, junk ) = p( text = data )

    if not junk:
      return ( seqs, junk )

    else:
      results.append( ( seqs, junk ) )

  return min( results, key = lambda p: len( p[1] ) )

# XXX test needed
def any_sequence_format(file_name, assign_name_if_not_defined=False,
    data=None):
  """Parse any sequence from any format"""
  format_parser = sequence_parser_for(file_name)
  if data is None:
    with open(file_name, "r") as f:
      data = f.read()
  seq_object = None
  if (format_parser is not None):
    try :
      objects, non_compliant = format_parser.parse(data)
      assert (len(objects) > 0)
      assert (objects[0].sequence != "")
    except Exception as e :
      pass
    else :
      if (assign_name_if_not_defined):
        base_name = os.path.splitext(os.path.basename(file_name))[0]
        for k, seq in enumerate(objects):
          if (seq.name == ""):
            seq.name = "%s_%d" % (base_name, k+1)
            print(seq.name)
      return objects, non_compliant
  for other_parser in [fasta_sequence_parse.parse, pir_sequence_parse.parse,
                       seq_sequence_parse.parse, tf_sequence_parse] :
    if (other_parser is not format_parser):
      try :
        objects, non_compliant = other_parser(data)
        assert (len(objects) > 0)
        assert (objects[0].sequence != "")
      except Exception as e :
        pass
      else :
        if (assign_name_if_not_defined):
          base_name = os.path.splitext(os.path.basename(file_name))[0]
          for k, seq in enumerate(objects):
            if (seq.name == ""):
              seq.name = "%s_%d" % (base_name, k+1)
              print(seq.name)
        return objects, non_compliant
  # fallback: unformatted
  data = re.sub(r"\s", "", data)
  if (re.search(r"[^a-zA-Z\*]", data) is None):
    seq = sequence(data)
    if (assign_name_if_not_defined ):
      seq.name = os.path.splitext(os.path.basename(file_name))[0]
    return [seq], []
  return None, None

def merge_sequence_files(file_names, output_file, sequences=(),
    include_non_compliant=False, call_back_on_error=None,
    force_new_file=False):
  """Merge a set of sequence files into a single file"""
  assert (len(file_names) > 0) or (len(sequences) > 0)
  if (len(file_names)==1) and (len(sequences)==0) and (not force_new_file):
    return file_names[0]
  seq_out = StringIO()
  for seq_file in file_names :
    objects, non_compliant = any_sequence_format(seq_file)
    if (objects is None):
      msg = ("The file '%s' could not be parsed as one of the "+
          "standard formats.  This could either be a problem with the file, "+
          "a non-standard format, or a bug in our code.  For further help, "+
          "please email the developers at help@phenix-online.org.") % seq_file
      if (hasattr(call_back_on_error, "__call__")):
        msg += "  (This is not a fatal error, but the combined sequence "+\
                "file will be incomplete.)"
        if (not call_back_on_error(msg)):
          raise Abort()
        continue
      else :
        raise RuntimeError(msg)
    for seq_record in objects :
      name = str(seq_record.name)
      if (name == ""):
        name = "none"
      seq_out.write("> %s\n" % name)
      seq_out.write("%s\n" % seq_record.sequence)
  if (len(sequences) > 0):
    for k, seq in enumerate(sequences):
      seq_out.write("> seq%d\n" % (k+1))
      seq_out.write("%s\n" % seq)
  f = open(output_file, "w")
  f.write(seq_out.getvalue())
  f.close()
  return output_file

# Alignment file parsers
class generic_alignment_parser(object):
  """
  General purpose alignment parser
  """

  def fail(self, text):

    return ( None, text )


  def extract(self, text):

    return [ match.groupdict() for match in self.regex.finditer( text ) ]


  def assess_parsing_results(self, data_dict, text):

    assert "alignments" in data_dict and "names" in data_dict

    # Remove whitespace from alignments
    alignments = [
      "".join( [ char for char in ali_str if not char.isspace() ] )
      for ali_str in data_dict[ "alignments" ]
      ]

    if not check_alignments_are_valid( alignments ):
      return ( None, text )

    data_dict[ "alignments" ] = alignments

    # Create alignment object
    return ( self.type( **data_dict ), "" )


  def __call__(self, text):

    return self.parse( text )


def check_alignments_are_valid(alignments):
  """Check for valid alignments"""

  if not alignments:
    return True

  first = len( alignments[0] )

  for line in alignments[1:]:
    if first != len( line ):
      return False

  return True


class sequential_alignment_parser(generic_alignment_parser):
  """
  Specific for sequential format alignments
  """

  def __init__(self, regex, type):

    self.regex = regex
    self.type = type


  def parse(self, text):

    data = self.extract( text )

    if text and not data:
      return self.fail( text )

    preprocessed_data = dict( [ ( name, [ info[ name ] for info in data ] )
      for name in self.regex.groupindex.keys() ] )

    return self.assess_parsing_results(
      data_dict = preprocessed_data,
      text = text
      )


CLUSTAL_BODY = re.compile(
  r"""
  ^
  (?P<name> .+? ) \s+
  (?P<alignment> [A-Z\-]+ )
  (?P<number> \s+ \d+ )? \s* $
  """,
  re.VERBOSE
  )
CLUSTAL_MIDLINE = re.compile( r"^ \s* [:.* ]+ \s* $", re.VERBOSE )
CLUSTAL_HEADER = re.compile( r"\A(.*) multiple sequence alignment$" )

def clustal_alignment_parse(text):
  """
  Specific for Clustal alignments
  """

  lines = list( reversed( text.splitlines() ) ) # create a stack

  if not lines:
    return ( None, text )

  assert 0 < len( lines )

  match = CLUSTAL_HEADER.search( lines.pop() )

  if not match:
    return ( None, text )

  program = match.group( 1 )

  # Read first block
  discard_clustal_empty_lines( lines )

  try:
    ( names, sequences ) = read_clustal_block( lines )

  except ValueError:
    return ( None, text )

  assert len( names ) == len( sequences )
  parts = [ sequences ]
  discard_clustal_empty_lines( lines )

  while lines:
    try:
      ( ns, sequences ) = read_clustal_block( lines )

    except ValueError:
      return ( None, text )

    assert len( ns ) == len( sequences )

    if names != ns:
      return ( None, text )

    assert len( names ) == len( sequences )

    parts.append( sequences )
    discard_clustal_empty_lines( lines )

  return (
    clustal_alignment(
      names = names,
      alignments = [
        "".join( seqs[ i ] for seqs in parts ) for i in range( len( names ) )
        ],
      program = program
      ),
    ""
    )


def read_clustal_block(lines):
  """Read a block from a clustal-format file"""

  names = []
  sequences = []

  while lines:
    if not lines[-1].strip():
      break

    # Get names and data
    match = CLUSTAL_BODY.search( lines[-1] )

    if match:
      info = match.groupdict()
      names.append( info[ "name" ] )
      sequences.append( info[ "alignment" ] )

    else:
      if not CLUSTAL_MIDLINE.search( lines[-1] ):
        raise ValueError("Line '%s' does not match expected body or midline formats" % lines[-1])

    lines.pop()

  return ( names, sequences )


def discard_clustal_empty_lines(lines):
  """Discard empty lines from clustal file"""
  while lines:
    if lines[ -1 ].strip():
      break

    lines.pop()


def hhalign_alignment_parse(text):
  """Parse an HHalign formatted file"""
  try:
    hhalign = hhalign_parser( output = text )

  except ValueError:
    return ( None, text )

  return ( hhalign.alignment(), "" )


# Alignment parser instances that can be used as functions
pir_alignment_parse = sequential_alignment_parser(
  regex = re.compile(
    r"""
    ^ >
    (?P<types> [PFDRN][13LC] ) ;
    (?P<names> \S+ ) \n
    (?P<descriptions> [^\n]* ) \n
    (?P<alignments> [^>]* )
    \* \s*
    """,
    re.MULTILINE | re.VERBOSE
    ),
  type = pir_alignment
  )
fasta_alignment_parse = sequential_alignment_parser(
  regex = re.compile(
    r"""
    ^ >
    (?P<names> \S+ ) \s+
    (?P<descriptions> [^\n]* ) \n
    (?P<alignments> [^>]* )
    \s*
    """,
    re.MULTILINE | re.VERBOSE
    ),
  type = fasta_alignment
  )
ali_alignment_parse = sequential_alignment_parser(
  regex = re.compile(
    r"""
    ^ > \s*
    (?P<names> [^\n]* ) \n
    (?P<alignments> [^>^*]* )
    \*?\s*
    """,
    re.MULTILINE | re.VERBOSE
    ),
  type = alignment
  )

_implemented_alignment_parsers = {
  ".pir": pir_alignment_parse,
  ".aln": clustal_alignment_parse,
  ".clustal": clustal_alignment_parse,
  ".fasta": fasta_alignment_parse,
  ".ali": ali_alignment_parse,
  ".fa": ali_alignment_parse,
  ".hhr": hhalign_alignment_parse,
  }

def alignment_parser_for(file_name):
  """Identify alignment parser for file_name"""

  ( name, extension ) = os.path.splitext( file_name )

  return _implemented_alignment_parsers.get( extension )


def alignment_parser_for_extension(extension):
  """Identify alignment parser for extension"""

  return _implemented_alignment_parsers.get( extension )


def known_alignment_formats():
  """List known alignment formats"""
  return list(_implemented_alignment_parsers.keys())

def any_alignment_file(file_name):
  """Parse any alignment file"""
  base, ext = os.path.splitext(file_name)
  with open(file_name) as f:
    data = f.read()
  parser1 = None
  if (ext != ".hhr") and (ext in _implemented_alignment_parsers):
    parser1 = _implemented_alignment_parsers.get(ext)
    try :
      aln = parser1(data)[0]
      assert (len(aln.names) != 0)
    except KeyboardInterrupt : raise
    except Exception as e : pass
    else :
      return aln
  for parser in [pir_alignment_parse, clustal_alignment_parse,
                 fasta_alignment_parse, ali_alignment_parse] :
    if (parser is parser1):
      continue
    try :
      aln = parser(data)[0]
      assert (len(aln.names) != 0)
    except KeyboardInterrupt : raise
    except Exception as e : pass
    else :
      return aln
  raise RuntimeError("Not a recognizeable sequence alignment format!")


def any_alignment_string(data, extension = None):
  """Parse any alignment string"""

  tried = None

  if extension == ".hhr":
    return hhalign_parser( output = data )

  elif extension is not None:
    parser = alignment_parser_for_extension( extension = extension )
    aln = parser( text = data )[0]
    tried = parser

    if aln and aln.multiplicity() != 0:
      return aln

  for parser in [pir_alignment_parse, clustal_alignment_parse,
                 fasta_alignment_parse, ali_alignment_parse] :
    if parser is tried:
      continue

    aln = parser( text = data)[0]

    if aln and aln.multiplicity() != 0:
      return aln

  raise RuntimeError("Not a recognizeable sequence alignment format!")


class homology_search_hit(object):
  """
  A hit from a homology search
  """

  def __init__(self, identifier, chain, annotation, alignment):

    self.identifier = identifier
    self.chain = chain
    self.annotation = annotation
    self.alignment = alignment


  def target_alignment_index(self):

    return 0


  def model_alignment_index(self):

    return 1


  def target_alignment_sequence(self):

    return self.alignment.alignments[ self.target_alignment_index() ]


  def model_alignment_sequence(self):

    return self.alignment.alignments[ self.model_alignment_index() ]


class hhpred_parser(object):
  """
  Parses .hhr files from HHPred
  """

  SPLIT = re.compile(
    r"""
    ( Query .*? ) ( \n | \r\n | \r ) \2
    ( \s+ No \s+ Hit .*? ) \2 \2
    ( .*? )
    (?= (?:Done!) | (?:\Z) )
    """,
    re.VERBOSE | re.DOTALL
    )

  HEADER = re.compile(
    r"""
    \A
    Query \s+ ( \S .* ) (?: \n | \r\n | \r )
    Match_columns \s+ ( \d+ ) (?: \n | \r\n | \r )
    No_of_seqs \s+ ( \d + ) \s out \s of \s ( \d+ ) (?: \n | \r\n | \r )
    Neff \s+ (\S[^\n^\r]*) (?: \n | \r\n | \r )
    Searched_HMMs \s+ ( \d+ ) (?: \n | \r\n | \r )
    Date \s+ (\S[^\n^\r]*) (?: \n | \r\n | \r )
    Command \s+ (\S.*)
    \Z
    """,
    re.VERBOSE
    )


  def __init__(self, output):

    res = self.SPLIT.search( output )

    if not res:
      raise ValueError("Incorrect file format")

    ( header, linefeed, summary, hits ) = res.groups()
    self.process_header( header = header )
    self.process_hits( hits = hits )


  def process_header(self, header):

    res = self.HEADER.search( header )

    if not res:
      raise ValueError("Incorrect header format")

    self.query = res.group( 1 )
    self.match_columns = int( res.group( 2 ) )
    self.no_of_sequences = ( int( res.group( 3 ) ), int( res.group( 4 ) ) )
    self.neff = res.group( 5 )
    self.searched_hmms = int( res.group( 6 ) )
    self.date = res.group( 7 )
    self.command = res.group( 8 )


  def process_hits(self, hits):

    self.setup_data_arrays()
    start = 0

    for m in self.HITS.finditer( hits ):
      if m.start() != start:
        assert start < m.start()

        unknown = hits[ start : m.start() ].strip()

        if unknown:
          raise ValueError("Uninterpretable block: %s" % repr( unknown ))

      start = m.end()

      self.add_match_to_hit_header_results( match = m )
      self.process_blocks( blocks = m.groupdict()[ "blocks" ] )

    remaining = hits[ start: ].strip()

    if remaining:
      raise ValueError("Uninterpretable tail: %s" % repr( remaining ))


  def process_blocks(self, blocks):

    matches = []
    start = 0

    for match in self.BLOCKS.finditer( blocks ):
      if match.start() != start:
        assert start < match.start()

        unknown = blocks[ start : match.start() ].strip()

        if unknown:
          raise ValueError("Uninterpretable: %s" % repr( unknown ))

      start = match.end()
      matches.append( match.groups() )

    remaining = blocks[ start: ].strip()

    if remaining:
      raise ValueError("Uninterpretable: %s" % repr( remaining ))

    if not matches:
      raise ValueError("Empty homology block")

    assert all([len( matches[0] ) == len( a ) for a in matches[1:]])

    return self.merge_and_process_block_hits( matches = matches )


  def merge_sequence_numbers(self, starts, ends, others):

    for ( s, e ) in zip( starts[1:], ends[:-1] ):
      if s != e + 1:
        raise ValueError("Incorrect sequence indices")

      if not all([others[0] == o for o in others[1:]]):
        raise ValueError("Incorrect sequence indices")

    return ( starts[0], ends[-1], others[0] )


class hhsearch_parser(hhpred_parser):
  """
  Specific for hhsearch output
  """

  HITS = re.compile(
    r"""
    No \s ( \d+) \s* (?: \n | \r\n | \r )
    >( [\w]{4} ) _  ( [\w]+ ) \s ( [^\n]* )(?: \n | \r\n | \r )
    Probab = ( [+-]? \d+ \. \d* ) \s+
    E-value = ( \d+ \.? \d* )( e[+-]? \d+ )? \s+
    Score = ( [+-]? \d+\.\d+ ) \s+
    (?: Aligned_cols | Aligned_columns ) = ( \d+ ) \s+
    Identities = ( \d+ ) %
    [^\n]*
    (?: \n | \r\n | \r )
    (?P<blocks> .*? )(?= (?:^No) | (?:\Z) )
    """,
    re.VERBOSE | re.DOTALL | re.MULTILINE
    )
  BLOCKS = re.compile(
    r"""
    (?: Q .* (?: \n | \r\n | \r ) )*
    Q \s+ [\w:\.|-]* \s+ ( \d+ ) \s+ ( [\w\.-]+ ) \s+ ( \d+ ) \s+ \( ( \d+ ) \) \s* (?: \n | \r\n | \r )
    Q \s+ Consensus \s+ \d+ \s+ [\w\.~-]+ \s+ \d+ \s+ \( \d+ \) \s* (?: \n | \r\n | \r )
    \s* [ \.\-+|=]* (?: \n | \r\n | \r )
    T \s+ Consensus \s+ \d+ \s+ [\w\.~-]+ \s+ \d+ \s+ \( \d+ \) \s* (?: \n | \r\n | \r )
    T \s+ \w+ \s+ ( \d+ ) \s+ ( [\w\.-]+ ) \s+ ( \d+ ) \s+ \( ( \d+ ) \) \s* (?: \n | \r\n | \r )
    (?: T .* (?: \n | \r\n | \r ) )*
    (?: Confidence .* (?: \n | \r\n | \r ))?
    """,
    re.VERBOSE
    )

  def setup_data_arrays(self):

    self.indices = []
    self.pdbs = []
    self.chains = []
    self.annotations = []
    self.probabs = []
    self.e_values = []
    self.scores = []
    self.aligned_cols = []
    self.identities = []

    self.query_starts = []
    self.query_ends = []
    self.query_others = []
    self.query_alignments = []

    self.hit_starts = []
    self.hit_ends = []
    self.hit_others = []
    self.hit_alignments = []


  def restrict(self, max_count):

    self.indices = self.indices[:max_count]
    self.pdbs = self.pdbs[:max_count]
    self.chains = self.chains[:max_count]
    self.annotations = self.annotations[:max_count]
    self.probabs = self.probabs[:max_count]
    self.e_values = self.e_values[:max_count]
    self.scores = self.scores[:max_count]
    self.aligned_cols = self.aligned_cols[:max_count]
    self.identities = self.identities[:max_count]

    self.query_starts = self.query_starts[:max_count]
    self.query_ends = self.query_ends[:max_count]
    self.query_others = self.query_others[:max_count]
    self.query_alignments = self.query_alignments[:max_count]

    self.hit_starts = self.hit_starts[:max_count]
    self.hit_ends = self.hit_ends[:max_count]
    self.hit_others = self.hit_others[:max_count]
    self.hit_alignments = self.hit_alignments[:max_count]


  def add_match_to_hit_header_results(self, match):

    self.indices.append( int( match.group( 1 ) ) )
    self.pdbs.append( match.group( 2 ).upper() )
    self.chains.append( match.group( 3 ) )
    self.annotations.append( match.group( 4 ) )
    self.probabs.append( float( match.group( 5 ) ) )
    number = match.group( 6 )
    if match.group( 7 ): number += match.group( 7 )
    self.e_values.append( float( number ) )
    self.scores.append( float( match.group( 8 ) ) )
    self.aligned_cols.append( int( match.group( 9 ) ) )
    self.identities.append( float( match.group( 10 ) ) )


  def merge_and_process_block_hits(self, matches):

    data = list(zip( *matches ))
    assert len( data ) == 8

    sequences = [ data[1], data[5] ]

    for index in range( len( matches ) ):
      if len( sequences[0][ index ] ) != len( sequences[1][ index ] ):
        raise ValueError("Incorrect alignments")

    mergeds = [ "".join( a ) for a in sequences ]

    q_indices = self.merge_sequence_numbers(
      starts = [ int( d ) for d in data[0] ],
      ends = [ int( d ) for d in data[2] ],
      others = [ int( d ) for d in data[3] ]
      )

    t_indices = self.merge_sequence_numbers(
      starts = [ int( d ) for d in data[4] ],
      ends = [ int( d ) for d in data[6] ],
      others = [ int( d ) for d in data[7] ]
      )

    self.query_starts.append( q_indices[0] )
    self.query_ends.append( q_indices[1] )
    self.query_others.append( q_indices[2] )
    self.query_alignments.append( mergeds[0] )

    self.hit_starts.append( t_indices[0] )
    self.hit_ends.append( t_indices[1] )
    self.hit_others.append( t_indices[2] )
    self.hit_alignments.append( mergeds[1] )


  def hits(self):

    data = list(zip(
      self.pdbs,
      self.chains,
      self.annotations,
      self.query_alignments,
      self.hit_alignments,
      ))

    for ( pdb, chain, annotation, query, hit ) in data:
      alignment = clustal_alignment(
        names = [ "target", "%s_%s" % ( pdb, chain ) ],
        alignments = [ query, hit ],
        program = "HHPred"
        )

      yield homology_search_hit(
        identifier = pdb,
        chain = chain,
        annotation = annotation,
        alignment = alignment
        )


  def __len__(self):

    return len( self.pdbs )


class hhalign_parser(hhpred_parser):
  """
  Specific for hhalign output
  """

  HITS = re.compile(
    r"""
    No \s ( \d+) \s* (?: \n | \r\n | \r )
    > \s* ( [^\n]* ) (?: \n | \r\n | \r )
    Probab = ( [+-]? \d+ \. \d* ) \s+
    E-value = ( \d+ \.? \d* )( e[+-]? \d+ )? \s+
    Score = ( \d+\.\d+ ) \s+
    (?: Aligned_cols | Aligned_columns ) = ( \d+ ) \s+
    Identities = ( \d+ ) %
    [^\n]*
    (?: \n | \r\n | \r )
    (?P<blocks> .*? )(?= (?:^No) | (?:\Z) )
    """,
    re.VERBOSE | re.DOTALL | re.MULTILINE
    )
  BLOCKS = re.compile(
    r"""
    Q \s+ ss_pred \s+ ( [\w-]+ ) \s* (?: \n | \r\n | \r )
    Q \s+ ss_conf \s+ ( [\w-]+ ) \s* (?: \n | \r\n | \r )
    Q \s+ [\w:\.]+ \s+ ( \d+ ) \s+ ( [\w-]+ ) \s+ ( \d+ ) \s+ \( ( \d+ ) \) \s* (?: \n | \r\n | \r )
    Q \s+ Consensus \s+ ( \d+ ) \s+ ( [\w~-]+ ) \s+ ( \d+ ) \s+ \( ( \d+ ) \) \s* (?: \n | \r\n | \r )
    \s+ ( [ \.\-+|=]+ ) (?: \n | \r\n | \r )
    T \s+ Consensus \s+ ( \d+ ) \s+ ( [\w~-]+ ) \s+ ( \d+ ) \s+ \( ( \d+ ) \) \s* (?: \n | \r\n | \r )
    T \s+ [\w\.]+ \s+ ( \d+ ) \s+ ( [\w-]+ ) \s+ ( \d+ ) \s+ \( ( \d+ ) \) \s* (?: \n | \r\n | \r )
    T \s+ ss_pred \s+ ( [\w-]+ ) \s* (?: \n | \r\n | \r )
    T \s+ ss_conf \s+ ( [\w-]+ ) \s* (?: \n | \r\n | \r )
    """,
    re.VERBOSE
    )

  def setup_data_arrays(self):

    self.indices = []
    self.annotations = []
    self.probabs = []
    self.e_values = []
    self.scores = []
    self.aligned_cols = []
    self.identities = []

    self.query_starts = []
    self.query_ends = []
    self.query_others = []
    self.query_alignments = []
    self.query_consensi = []
    self.query_ss_preds = []
    self.query_ss_confs = []

    self.midlines = []

    self.hit_starts = []
    self.hit_ends = []
    self.hit_others = []
    self.hit_alignments = []
    self.hit_consensi = []
    self.hit_ss_preds = []
    self.hit_ss_confs = []


  def add_match_to_hit_header_results(self, match):

    self.indices.append( int( match.group( 1 ) ) )
    self.annotations.append( match.group( 2 ) )
    self.probabs.append( float( match.group( 3 ) ) )
    number = match.group( 4 )
    if match.group( 5 ):
      number += match.group( 5 )
    self.e_values.append( float( number ) )
    self.scores.append( float( match.group( 6 ) ) )
    self.aligned_cols.append( int( match.group( 7 ) ) )
    self.identities.append( float( match.group( 8 ) ) )


  def merge_and_process_block_hits(self, matches):

    data = list(zip( *matches ))
    assert len( data ) == 21

    sequences = [
        data[0], data[1], data[3], data[7],
        data[12], data[16], data[19], data[20] ]
    midlines = []

    for ( index , alis) in enumerate( zip( *sequences ) ):
      count = len( alis[0] )

      if not all([count == len( c ) for c in alis[1:]]):
        raise ValueError("Incorrect alignments")

      midlines.append(
        " " * ( count - len( data[10][ index ] ) ) + data[10][ index ]
        )

    merged = [ reduce( operator.add, a ) for a in sequences ]
    assert len( midlines ) == len( matches )
    midline = reduce( operator.add, midlines )

    # Comment out consistency check
    # if data[2] != data[6] or data[4] != data[8] or data[5] != data[9]:
    #  raise ValueError, "Inconsistent query numbering"

    q_indices = self.merge_sequence_numbers(
      starts = [ int( d ) for d in data[2] ],
      ends = [ int( d ) for d in data[4] ],
      others = [ int( d ) for d in data[5] ]
      )

    # Comment out consistency check
    # if data[11] != data[15] or data[13] != data[17] or data[14] != data[18]:
    #  raise ValueError, "Inconsistent target numbering"

    t_indices = self.merge_sequence_numbers(
      starts = [ int( d ) for d in data[11] ],
      ends = [ int( d ) for d in data[13] ],
      others = [ int( d ) for d in data[18] ]
      )

    self.query_starts.append( q_indices[0] )
    self.query_ends.append( q_indices[1] )
    self.query_others.append( q_indices[2] )
    self.query_ss_preds.append( merged[0] )
    self.query_ss_confs.append( merged[1] )
    self.query_alignments.append( merged[2] )
    self.query_consensi.append( merged[3] )

    self.midlines.append( midline )

    self.hit_starts.append( t_indices[0] )
    self.hit_ends.append( t_indices[1] )
    self.hit_others.append( t_indices[2] )
    self.hit_consensi.append( merged[4] )
    self.hit_alignments.append( merged[5] )
    self.hit_ss_preds.append( merged[6] )
    self.hit_ss_confs.append( merged[7] )


  def alignment(self):

    assert len( self.annotations ) == 1
    return clustal_alignment(
      names = [ "target", self.annotations[0] ],
      alignments = [ self.query_alignments[0], self.hit_alignments[0] ],
      program = "HHPred"
      )

def any_hh_file(file_name):
  """Parse any HHalign file"""
  with open(file_name) as f:
    data = f.read()
  for parser in [hhalign_parser, hhsearch_parser] :
    try :
      p = parser(data)
    except KeyboardInterrupt : raise
    except Exception as e :
      pass
    else :
      return p
  raise RuntimeError("Not an HHpred/HHalign/HHsearch file!")

def any_a3m_file(file_name):
  """Parse any a3m file"""
  with open(file_name) as f:
    data = f.read()
  try:
    a3m_info = read_a3m(data)
  except KeyboardInterrupt : raise
  except Exception as e :
    pass
  else :
    return a3m_info
  raise RuntimeError("Not an a3m file")

def read_a3m(text):
  """ Read text as a3m format
      Ignore any lines starting with # at top
      Lines starting with > or blank lines are separators and are ignored
      Text between separators are sequence info.
        First line is base sequence.  It cannot contain any lowercase or "-"
        or "."
        All other lines contain upper case characters (sequence matches),
        lower case characters (insertions), and "-" or "." (gaps).
      The total number of characters in base sequence must equal the
        number of upper case characters plus number of gap characters, minus
        the number of insertion characters in each other line.
  """

  # Get text as utf-8
  if hasattr(text,'decode'):
    text = text.decode(encoding='utf-8')

  # Read in line representing each sequence
  sequences = []
  new_line = ""
  for line in text.splitlines():
    line = line.strip()
    if not sequences and line.startswith("#"): continue # skip leading # lines
    if not line or line.startswith(">"):  # separator
      if new_line:
        sequences.append(new_line)
        new_line = ""
      continue
    new_line += line.strip().replace(" ","")
  if new_line:
    sequences.append(new_line)


  # Check for illegal characters and length of lines
  base_sequence = sequences[0]
  n = len(base_sequence)
  import re
  for s in sequences:
    if not ok_a3m_sequence(s, n = n, base_sequence = base_sequence):
      return None

  from libtbx import group_args
  a3m_info = group_args(group_args_type = 'a3m_info',
    base_sequence = base_sequence,
    sequence_length = len(base_sequence),
    sequences = sequences,
     )
  return a3m_info

def ok_a3m_sequence(s, n = None, base_sequence = None):
  """ Check a sequence and make sure it matches expectations for an a3m line
  """
  # Remove blanks/linefeeds and convert . to -
  s = s.replace(" ","").replace("\r","").replace("\n","")
  s = s.replace(".","-")

  n_gap_chars = s.count("-")
  s_all = s

  # Get upper and lowercase
  s = s.replace("-","")
  # count lowercase/uppercase
  n_upper = 0
  n_lower = 0
  n_other = 0
  s_upper = ""
  for c in s:
    if c >="A" and c <= "Z":
      n_upper += 1
      s_upper += c
    elif c >="a" and c <= "z":
      n_lower += 1
    else:
      n_other += 1

  if n_other > 0:
    return False
  if n_upper + n_gap_chars == n:
    return True
  else:
    return False
def composition_from_sequence_file(file_name, log=None):
  """Get composition from a sequence file"""
  if (log is None):
    log = sys.stdout
  try :
    seq_file, non_compliant = any_sequence_format(file_name)
    if (seq_file is None):
      raise ValueError("Could not parse %s" % file_name)
  except Exception as e :
    print(str(e), file=log)
    return None
  else :
    if (len(non_compliant) > 0):
      print("Warning: non-compliant entries in sequence file", file=log)
      for nc in non_compliant :
        print("  " + str(nc), file=log)
    n_residues = n_bases = 0
    for seq_entry in seq_file :
      (n_res_seq, n_base_seq) = composition_from_sequence(seq_entry.sequence)
      n_residues += n_res_seq
      n_bases += n_base_seq
    return group_args(n_residues=n_residues, n_bases=n_bases)

def composition_from_sequence(sequence):
  """Get composition from a sequence"""
  seq = sequence.upper()
  n_residues = n_bases = 0
  n_valid = len(seq) - seq.count("X")
  n_na = seq.count("A") + seq.count("U") + seq.count("T") + \
    seq.count("G") + seq.count("C")
  if (n_na >= int(n_valid * 0.9)):
    n_bases += len(seq)
  else :
    n_residues += len(seq)
  return n_residues, n_bases

def duplicate_multiple_chains(text):
  """make chains that are marked with >7WZE_1|Chains A, B[Auth X]| or similar
  N times that many chains"""
  new_groups = []
  next_lines = []
  unused_lines = []
  previous_line = None
  for line in text.splitlines():
    line = line.strip()
    if line.startswith(">"):
      next_lines = [line]
      new_groups.append(next_lines)
    elif not line.strip():
      next_lines = [">"]
      new_groups.append(next_lines)
    elif next_lines:
      next_lines.append(line)
    else:
      unused_lines.append(line)
    previous_line = line
  if unused_lines and new_groups:
    return text # could not do anything with this

  elif unused_lines:
    return text # nothing to do

  else: # usual
    new_lines = []
    for new_group in new_groups:
      if not new_group or not new_group[0]:
        continue
      first_line = new_group[0]
      n = get_number_of_dups(first_line)
      lines_in_group= new_group[1:]
      for i in range(n):
        new_lines.append(first_line)
        new_lines.append("".join(lines_in_group))
    text = "\n".join(new_lines)
    text = text.replace(">\n\n>",">")
    text = text.replace(">\n\n>",">")
    return text

def remove_inside_brackets(text):
  """ remove everything enclosed in []"""
  if not text.find("[")>-1:
    return text
  new_text = ""
  found_lb = False
  for c in text:
    if c=="[":
      found_lb = True
    elif c == "]":
      found_lb = False
    elif found_lb:
      pass # skip it
    else:
      new_text += c
  return new_text

def get_number_of_dups(line):
  """ Get number of duplicate chains: looks like >7WZE_1|Chains A, B[Auth X]| or similar """
  if not line.startswith(">"):
    return 1
  spl = line.split("|")
  if len(spl) < 2:
    return 1
  text = spl[1].strip()
  text = remove_inside_brackets(text)
  if not text.startswith("Chain"):
    return 1
  text = text.replace("Chains","").replace("Chain","").replace("=",""
       ).replace(",","")
  values = text.split()
  return max(1, len(values))

def clear_empty_lines(text, apply_duplicate_multiple_chains = False,
    keep_labels = False):
  """First duplicate any multiple chains, then clear empty lines."""
  if apply_duplicate_multiple_chains:
    text = duplicate_multiple_chains(text)
  # make empty lines just a blank line.  Includes >>> etc.
  # If keep_labels, make the starting line for each group start with >

  new_lines=[]
  prev_line = ""
  label_line = ""
  for line in text.splitlines():
    if keep_labels and line.startswith(">"):
      label_line = line
    if not line.replace(">","").replace(" ",""):
       line=""
    elif line.startswith(">"):
       line=""
    line=line.replace("?","")
    if (not line) and (not prev_line):
      continue # skip blanks if dup or at beginning
    if line and label_line:
      new_lines.append(label_line)
      label_line = ""
    new_lines.append(line)
    prev_line = line
  return "\n".join(new_lines)+"\n"

def get_sequences(file_name=None,text=None,remove_duplicates=None,
     apply_duplicate_multiple_chains = False,
     remove_unknowns = False,
     return_sequences_with_labels = False):
  """return simple list of sequences in this file. duplicates included
  unless remove_duplicates=True.
  remove unknowns (X) if requested.
  If return_sequences_with_labels,  return sequence objects with labels"""
  if not text:
    if not file_name:
      raise Sorry("Missing file for get_sequences: %s" %(
        file_name))
    with open(file_name) as f:
      text = f.read()
  # clear any lines that have only > and nothing else
  text=clear_empty_lines(text, apply_duplicate_multiple_chains,
    keep_labels = return_sequences_with_labels)

  ( sequences, unknowns ) = parse_sequence( text )

  simple_sequence_list=[]
  sequence_object_list = []
  for sequence in sequences:
    if remove_duplicates and sequence.sequence in simple_sequence_list:
      continue # it is a duplicate
    elif remove_unknowns: # remove any X and take it
      sequence.sequence = sequence.sequence.upper().replace("X","")
      simple_sequence_list.append(sequence.sequence)
      sequence_object_list.append(sequence)
    else: # take it
      sequence.sequence = sequence.sequence.upper()
      simple_sequence_list.append(sequence.sequence)
      sequence_object_list.append(sequence)
  if return_sequences_with_labels:
    return sequence_object_list
  else:
    return simple_sequence_list

#####################################################################
####   Methods to try and guess chain types from sequences ##########
#####################################################################
def guess_chain_types_from_sequences(file_name=None,text=None,
    return_as_dict=False,minimum_fraction=None,
    likely_chain_types=None):
  """Guess what chain types are in this sequence file"""
  if not text:
    if not file_name:
      from libtbx.utils import Sorry
      raise Sorry("Missing file for guess_chain_types_from_sequences: %s" %(
        file_name))
    with open(file_name) as f:
      text = f.read()
  # clear any lines that have only > and nothing else
  text=clear_empty_lines(text)

  chain_types=[]
  ( sequences, unknowns ) = parse_sequence( text )
  dd={}
  dd_n={}
  total_residues=0
  for sequence in sequences:
    chain_type,n_residues=chain_type_and_residues(text=sequence.sequence,
      likely_chain_types=likely_chain_types)
    if chain_type is None and n_residues is None:
      continue
    if chain_type and not chain_type in chain_types:
      chain_types.append(chain_type)
      dd[chain_type]=[]
      dd_n[chain_type]=0
    if chain_type:
      dd[chain_type].append(sequence)
      dd_n[chain_type]+=len(sequence.sequence)
      total_residues+=len(sequence.sequence)
  if minimum_fraction and len(chain_types)>1 and total_residues>1:
    # remove anything < minimum_fraction
    new_chain_types=[]
    new_dd={}
    new_dd_n={}
    for chain_type in chain_types:
      if dd_n[chain_type]/total_residues >= minimum_fraction:
        new_chain_types.append(chain_type)
        new_dd[chain_type]=dd[chain_type]
        new_dd_n[chain_type]=dd_n[chain_type]
    chain_types=new_chain_types
    dd=new_dd
    dd_n=new_dd_n
  if return_as_dict:
    return dd # dict of chain_types and sequences for each chain_type
  else:
    chain_types.sort()
    return chain_types

def text_from_chains_matching_chain_type(file_name=None,text=None,
    chain_type=None,width=80):
  """Get sequence text from all the chains that match chain_type"""
  dd=guess_chain_types_from_sequences(file_name=file_name,
    text=text,return_as_dict=True,likely_chain_types=[chain_type])
  sequence_text=""
  for ct in dd.keys():
    if chain_type is None or ct==chain_type:
      for seq in dd[ct]:
        sequence_text+="""
%s
 """ %(seq.format(width=width))
  return sequence_text

def count_letters(letters="",text="",only_count_non_allowed=None):
  """Count letter in text"""
  n=0
  if only_count_non_allowed: # count the ones that are not there
    for t in text:
      if not t in letters:
        n+=1

  else: # usual
    for let in letters:
      n+=text.count(let)
  return n

def chain_type_and_residues(text=None,chain_type=None,likely_chain_types=None):
  """guess the type of chain from text string containing 1-letter codes
  and count residues.
  If chain_type is specified, just use it.
  If likely_chain_types are specified, use them if possible.

   Assumptions:
    1. few or no letters that are not part of the correct dict (there
      may be a few like X or other unknowns)
    2. if all letters are are A is is because poly-ala and poly-gly
       are common for unknowns.
    3. otherwise if it can be DNA or protein it is DNA (because chance is
       very high that if it were protein there would be a non-DNA letter)
   Method:  Choose the chain-type that matches the most letters in text.
    If a tie, take the chain type that has the fewest letters.
    If likely_chain_types are specified, use one from there first

  """

  text=text.replace(" ","").replace("\n","").lower()
  if not text:
    return None,None
  letter_dict={
    'PROTEIN':"acdefghiklmnpqrstvwy",
    'DNA':"gact",
    'RNA':"gacu",}
  if chain_type not in [None,'None']:
    for key in list(letter_dict.keys()):
      if key != chain_type:
        del letter_dict[key]
  # Get all allowed letters
  all_letters=[]
  for chain_type in letter_dict.keys():
    for let in letter_dict[chain_type]:
      if not let in all_letters: all_letters.append(let)

  # remove non-allowed letters from text
  new_text=""
  for let in text:
    if let in all_letters:
      new_text+=let
  text=new_text
  if not text:
    return None,None
  # See which chain_type matches best
  count_dict={}
  non_allowed_count_dict={}
  for chain_type in letter_dict.keys():
    count_dict[chain_type]=count_letters(letters=letter_dict[chain_type],
      text=text)
    non_allowed_count_dict[chain_type]=count_letters(
      letters=letter_dict[chain_type],
      text=text,only_count_non_allowed=True)
  # Take max count_dict. If tie, take minimum non-allowed. If tie take the one
  #  with fewer letters (i.e., DNA instead of protein if matches both except
  #  poly-ala not poly-A)
  score_list=[]
  for chain_type in letter_dict.keys():
    score_list.append([count_dict[chain_type],chain_type])
  score_list.sort(key=itemgetter(0))
  score_list.reverse()
  ok_list=[]
  best_score=score_list[0][0]
  for score,chain_type in score_list:
    if score==best_score:
      ok_list.append(chain_type)
  residues=best_score
  if len(ok_list)<1: return None,None
  if len(ok_list)==1: return ok_list[0],residues

  # take one from the likely list
  if likely_chain_types:
    likely_results=[]
    for lct in likely_chain_types:
      if lct in ok_list:
        likely_results.append(lct)

    if len(likely_results)==1:
      return likely_results[0],residues

  # decide which of the ones with the most matches is best..
  score_list=[]
  for chain_type in ok_list:
    score_list.append([non_allowed_count_dict[chain_type],chain_type])
  score_list.sort(key=itemgetter(0))
  ok_list=[]
  best_score=score_list[0][0]
  for score,chain_type in score_list:
    if score==best_score:
      ok_list.append(chain_type)
  if len(ok_list)<1: return None,None
  if len(ok_list)==1: return ok_list[0],residues

  if text.replace("g","a")==residues*"a" and "PROTEIN" in letter_dict.keys():
    # special case, all Adenine or Ala
    return "PROTEIN",residues

  score_list=[]
  for chain_type in ok_list:
    score_list.append([len(letter_dict[chain_type]),chain_type])
  score_list.sort(key=itemgetter(0))
  ok_list=[]
  best_score=score_list[0][0]
  for score,chain_type in score_list:
    if score==best_score:
      ok_list.append(chain_type)
  if len(ok_list)<1:
    return None,None
  else:
    return ok_list[0],residues

#####################################################################
####   END OF methods to try and guess chain types from sequences ###
#####################################################################

def random_sequence(n_residues=None,residue_basket=None,
   chain_type = 'PROTEIN'):
  """Return n_residues random residues"""
  assert n_residues and (residue_basket or chain_type)
  if not residue_basket:
    chain_type = chain_type.upper()
    if chain_type == "PROTEIN":
        # Approximate eukaryotic frequencies using W as basic unit
        residue_basket = "AAAAAACCCEEEEDDDDDGGGGGG"+\
          "FFFIIIHHKKKKKKMLLLLLLNNNQQQPPPPSSSSSSRRRTTTTTWVVVVVYYY"
    elif chain_type == "DNA":
        residue_basket = "GATC"
    elif chain_type == "RNA":
        residue_basket = "GAUC"
    else:
        raise Sorry("Chain type needs to be RNA/DNA/PROTEIN")

  import random
  s=""
  nn=len(residue_basket)-1
  for i in range(n_residues):
    id=random.randint(0,nn)
    s+=residue_basket[id]
  return s


 *******************************************************************************


 *******************************************************************************
iotbx/bioinformatics/alignment_as_hsearch.py
"""Generate information for homology search from a sequence file"""

from __future__ import absolute_import, division, print_function

from iotbx import bioinformatics
from six.moves import zip

def get_extension(filename):

  import os.path
  return os.path.splitext( filename )[1]


def any_alignment(source):

  if isinstance( source, str ):
    infile = open( source )
    data = infile.read()
    infile.close()
    extension = get_extension( filename = source )

  else:
    data = source.read()

    try:
      name = source.name

    except AttributeError:
      extension = None

    else:
      extension = get_extension( filename = name )

  return bioinformatics.any_alignment_string( data = data, extension = extension )


class Result(object):
  """
  Homology search from alignment file
  """

  def __init__(self, alignment):

    self.alignment = alignment
    self.max_count = None


  def restrict(self, max_count):

    if max_count is None:
      self.max_count = None

    else:
      self.max_count = max_count + 1


  def hits(self):

    names = self.alignment.names[ 1 : self.max_count ]
    alignments = self.alignment.alignments[ 1 : self.max_count ]

    try:
      descriptions = self.alignment.descriptions[ 1 : self.max_count ]

    except AttributeError:
      descriptions = alignments

    base = self.alignment.alignments[0]

    for ( n, seq, desc ) in zip( names, alignments, descriptions ):
      pieces = n.split( "_" )
      assert 0 < len( pieces )

      if len( pieces ) == 1:
        pdb = pieces[0]
        chain = ""

      else:
        ( pdb, chain ) = pieces[:2]

      alignment = bioinformatics.clustal_alignment(
        names = [ "target", "%s_%s" % ( pdb, chain ) ],
        alignments = [ base, seq ],
        program = "<unknown>"
        )
      yield bioinformatics.homology_search_hit(
        identifier = pdb,
        chain = chain,
        annotation = desc,
        alignment = alignment
        )


  def __len__(self):

    if self.max_count is None:
      return self.alignment.multiplicity() - 1

    else:
      return self.max_count - 1


def parse(source):

  alignment = any_alignment( source = source )
  return Result( alignment = alignment )



 *******************************************************************************


 *******************************************************************************
iotbx/bioinformatics/ebi_wu_blast_xml.py
"""Carry out homology searach by WU-BLAST at EBI"""

from __future__ import absolute_import, division, print_function

from iotbx.bioinformatics.xmlbuild import Single, Multiple, DataAttribute
from iotbx.bioinformatics.xmlbuild import Text, TextWithDefault, Attribute, AttributeWithDefault, Value
from iotbx.bioinformatics.xmlbuild import Parser

class Result(object):
  """
  Homology search by WU-BLAST at EBI
  """

  def __init__(self, root):

    self.root = root


  def restrict(self, max_count):

    del self.root.hits[max_count:]


  def hits(self):

    from iotbx import bioinformatics

    for h in self.root.hits:
      pieces = h.identifier.split( "_" )
      assert 0 < len( pieces )

      if len( pieces ) == 1:
        pdb = pieces[0]
        chain = ""

      else:
        ( pdb, chain ) = pieces[:2]

      alignment = bioinformatics.clustal_alignment(
        names = [ "target", "%s_%s" % ( pdb, chain ) ],
        alignments = [ h.alignments[0].query.seq, h.alignments[0].match.seq ],
        program = "WU-BLAST"
        )
      yield bioinformatics.homology_search_hit(
        identifier = pdb,
        chain = chain,
        annotation = h.description,
        alignment = alignment
        )

  def __len__(self):

    return len( self.root.hits )


from xml.etree.cElementTree import QName

def ebi_qualified(tag):

  return QName( "http://www.ebi.ac.uk/schema", tag ).text


BUILDER = Single(
  child_data_tagged = {
    ebi_qualified( tag = "Header" ): (
      "header",
      Single(
        child_data_tagged = {
          ebi_qualified( tag = "program" ): (
            "program",
            Single(
              extractions = [
                Attribute( attribute = "name", name = "name" ),
                Attribute( attribute = "version", name = "version" ),
                AttributeWithDefault(
                  attribute = "citation",
                  name = "citation",
                  default = "",
                  ),
                ],
              ),
            ),
          ebi_qualified( tag = "commandLine" ): ( "command", DataAttribute( name = "command" ) ),
          ebi_qualified( tag = "parameters" ): (
            "parameters",
            Single(
              extractions = [
                TextWithDefault(
                  attribute = "scores",
                  tag = ebi_qualified( "scores" ),
                  conversion = float,
                  default = 0.0,
                  ),
                TextWithDefault(
                  attribute = "alignments",
                  tag = ebi_qualified( "alignments" ),
                  conversion = int,
                  default = 0,
                  ),
                TextWithDefault(
                  attribute = "matrix",
                  tag = ebi_qualified( "matrix" ),
                  default = "",
                  ),
                TextWithDefault(
                  attribute = "expectation_upper",
                  tag = ebi_qualified( "expectationUpper" ),
                  conversion = float,
                  default = 0.0,
                  ),
                TextWithDefault(
                  attribute = "statistics",
                  tag = ebi_qualified( "statistics" ),
                  default = "",
                  ),
                TextWithDefault(
                  attribute = "filter",
                  tag = ebi_qualified( "filter" ),
                  default = "",
                  ),
                ],
              child_data_tagged = {
                ebi_qualified( tag = "sequences" ): (
                  "sequences",
                  Multiple(
                    tag = ebi_qualified( tag = "sequence" ),
                    processor = Single(
                      extractions = [
                        Attribute( attribute = "number", name = "number", conversion = int ),
                        Attribute( attribute = "name", name = "name" ),
                        Attribute( attribute = "type", name = "type" ),
                        Attribute( attribute = "length", name = "length", conversion = int ),
                        ]
                      ),
                    ),
                  ),
                ebi_qualified( tag = "databases" ): (
                  "databases",
                  Multiple(
                    tag = ebi_qualified( tag = "database" ),
                    processor = Single(
                      extractions = [
                        Attribute( attribute = "number", name = "number", conversion = int ),
                        Attribute( attribute = "name", name = "name" ),
                        Attribute( attribute = "type", name = "type" ),
                        AttributeWithDefault( attribute = "created", name = "created", default = "" ),
                        ]
                      ),
                    ),
                  ),
                },
              ),
            ),
          ebi_qualified( tag = "timeInfo" ): (
            "time",
            Single(
              extractions = [
                Attribute( attribute = "start", name = "start" ),
                Attribute( attribute = "end", name = "end" ),
                Attribute( attribute = "search", name = "search" ),
                ],
              ),
            ),
          },
        )
      ),
    ebi_qualified( "hits" ): (
      "hits",
      Multiple(
        tag = ebi_qualified( tag = "hit" ),
        processor = Single(
          extractions = [
            Attribute( attribute = "number", name = "number", conversion = int ),
            Attribute( attribute = "database", name = "database" ),
            Attribute( attribute = "identifier", name = "id" ),
            Attribute( attribute = "length", name = "length", conversion = int ),
            Attribute( attribute = "description", name = "description" ),
            ],
          child_data_tagged = {
            ebi_qualified( tag = "alignments" ): (
              "alignments",
              Multiple(
                tag = ebi_qualified( tag = "alignment" ),
                processor = Single(
                  extractions = [
                    Attribute( attribute = "number", name = "number", conversion = int ),
                    Text(
                      attribute = "score",
                      tag = ebi_qualified( tag = "score" ),
                      conversion = float,
                      ),
                    Text(
                      attribute = "bits",
                      tag = ebi_qualified( tag = "bits" ),
                      conversion = float,
                      ),
                    Text(
                      attribute = "expectation",
                      tag = ebi_qualified( tag = "expectation" ),
                      conversion = float,
                      ),
                    TextWithDefault(
                      attribute = "probability",
                      tag = ebi_qualified( tag = "probability" ),
                      conversion = float,
                      default = 0.0,
                      ),
                    Text(
                      attribute = "identity",
                      tag = ebi_qualified( tag = "identity" ),
                      conversion = float,
                      ),
                    Text(
                      attribute = "positives",
                      tag = ebi_qualified( tag = "positives" ),
                      conversion = float,
                      ),
                    Text( attribute = "pattern", tag = ebi_qualified( tag = "pattern" ) ),
                    ],
                  child_data_tagged = {
                    ebi_qualified( tag = "querySeq" ): (
                      "query",
                      Single(
                        extractions = [
                          Attribute( attribute = "start", name = "start", conversion = int ),
                          Attribute( attribute = "end", name = "end", conversion = int ),
                          Value( attribute = "seq" ),
                          ]
                        )
                      ),
                    ebi_qualified( tag = "matchSeq" ): (
                      "match",
                      Single(
                        extractions = [
                          Attribute( attribute = "start", name = "start", conversion = int ),
                          Attribute( attribute = "end", name = "end", conversion = int ),
                          Value( attribute = "seq" ),
                          ]
                        )
                      ),
                    },
                  ),
                ),
              ),
            }
          ),
        ),
      ),
    },
  )

parse = Parser(
  tag = ebi_qualified( tag = "EBIApplicationResult" ),
  builder = BUILDER,
  restype = Result,
  cElementTree = True,
  )



 *******************************************************************************


 *******************************************************************************
iotbx/bioinformatics/homologues.py
"""BLAST search and fetch models"""

from __future__ import absolute_import, division, print_function

import mmtbx.model
import sys
import iotbx.pdb

import iotbx.bioinformatics.pdb_info
from iotbx.bioinformatics import local_blast
from iotbx.bioinformatics.structure import summarize_blast_output
from iotbx.pdb.fetch import fetch
import six

def get_best_homologues(model, chain_ids=None):
  """
  Do local BLAST search for homologues, pick the best resolution, fetch models
  chain_ids - list of chains to search. If none - do for all of them.
  As usual, multi-model is not supported.
  Returns dictionary of chain_id: model
  """

  # Load data
  pdb_info = iotbx.bioinformatics.pdb_info.pdb_info_local()

  h = model.get_hierarchy()
  result = {}
  for chain in h.only_model().chains():
    print("Working with chain '%s'" % chain.id)
    if not chain.is_protein():
      print("  skipping, not protein. Maybe water or NA")
      continue
    if chain_ids is not None and chain.id not in chain_ids():
      continue
    sequence = chain.as_padded_sequence()
    l_blast = local_blast.pdbaa(seq=sequence) # why sequence in init???
    blast_xml_result = l_blast.run()
    # Probably alignment info is lost in this call,
    # but we have tools to do alignment, see mmtbx/alignment/__init__.py
    blast_summary = summarize_blast_output("\n".join(blast_xml_result))
    pdb_ids_to_study = {}
    for hit in blast_summary:
      # print dir(hit)
      hit.show(out=sys.stdout)
      # Don't have clear idea what these values mean right now,
      # but it would be reasonable to filter somehow.
      if hit.identity < 70:
        continue
      pdb_ids_to_study[hit.pdb_id] = hit.chain_id # can add more info
    #
    info_list = pdb_info.get_info_list(list(pdb_ids_to_study.keys()))
    # It would be good to merge info_list with hits in blast_summary in
    # one data structure here and do better filtering.
    # Let's go without it for now.

    # Sort by resolution in place
    info_list.sort(key=lambda tup: tup[1])
    best_pdb_id = info_list[0][0]
    best_pdb_chain = pdb_ids_to_study[info_list[0][0]]
    print("Best pdb:", info_list[0], "chain:", pdb_ids_to_study[info_list[0][0]])
    # print info_list

    # Get actual selected model from PDB.
    data = fetch(id=best_pdb_id)
    m = mmtbx.model.manager(
        model_input=iotbx.pdb.input(source_info=None, lines=data.readlines()))
    sel = m.selection("chain '%s'" % best_pdb_chain)
    result[chain.id] = m.select(sel)
  return result

def run(args):
  # prepare dict. DO IT ONCE, THEN COMMENT OUT!!! This will fetch data from RCSB.
  # iotbx.bioinformatics.pdb_info.get_all_experimental_pdb_info_to_pkl()
  #------------------------


  model = mmtbx.model.manager(model_input=iotbx.pdb.input("1ucs.pdb"))
  r = get_best_homologues(model)
  print(r)
  for chain, model in six.iteritems(r):
    model.pdb_or_mmcif_string_info(
        target_format='pdb',
        target_filename="chain_%s.pdb" % chain,
        write_file=True)


if __name__ == '__main__':
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
iotbx/bioinformatics/local_blast.py
"""
This is a tool to run BLAST locally against selected databased such
as PDBaa ...etc.  The executables and databases are all distributed
with Phenix so no extra installation is required. PDBaa has been
implemted.  More classes, e.g. ScopE, PDBstructure ... will be
added later.
LWH 4/12/19

Useage:
>>>from iotbx.bioinformatics import local_blast
>>>myxml=local_blast.pdbaa(seq=myseq).run()

where
myseq is the query protein sequence string. You can use 'X' to fill gaps.
myxml is the stdout_lines object of the blast XML output.
"""

from __future__ import absolute_import, division, print_function
from libtbx import easy_run
import os,sys,libtbx.load_env

#setup lib dir
phenixpath=os.getenv('PHENIX')
ligand_lib_dir = libtbx.env.find_in_repositories(
  relative_path=os.path.join("chem_data","ligand_lib"),
  test=os.path.isdir)
cwd=os.getcwd()

#make sure blast exists.  Never had a problem but probably won't hurt.
def checkblast(binary="blastp"):
  phenix_blast_exe=''
  systype=sys.platform #linux2,darwin,win32
  sysname='Linux'
  phenix_blast_exe='%s_%s'%(binary,systype)
  if systype=='win32':
    phenix_blast_exe='%s_%s.exe'%(binary,systype)
    sysname='Windows'
  elif systype=='darwin':
    phenix_blast_exe='%s_%s'%(binary,systype)
    sysname='OSX'
  elif systype.startswith('linux') and sys.version_info.major == 3:
    systype = 'linux2'
    phenix_blast_exe='%s_%s'%(binary,systype)
  else:
    pass
  phenix_blast=os.path.join(ligand_lib_dir, phenix_blast_exe)
  blastexe=None
  blastpath=None
  if os.path.exists(phenix_blast):
    blastpath=phenix_blast
    blastexe=phenix_blast_exe
    #print('%s version is running...\n'%sysname)
  else:
    print('BLAST executable does not exist. please check your Phenix installation.')
    sys.exit(0)
  return blastpath



class pdbaa(object):
  def __init__(self, workdir=None, prefix=None, seq=None, output=None):
    self.workdir=workdir
    self.prefix=prefix
    self.seq=seq
    self.output=output

  def run(self, debug=False, binary="blastp"):
    blastpath=checkblast(binary)
    curdir=os.getcwd()
    if self.workdir is None:
      self.workdir=curdir
    elif os.path.exists(self.workdir) is False:
      print("Input working directory does not exist. Try to work in current directory instead.")
      self.workdir=curdir
    fasta_file='myprotein.fasta'
    fasta_path=os.path.join(self.workdir,fasta_file)
    if self.prefix is None:
       self.prefix="myprotein"
    fastaline=">%s\n%s\n"%(self.prefix,self.seq)
    f=open(fasta_path,'w').writelines(fastaline)
    dbname="pdbaa.00"
    outfmt="-outfmt 5" #xml_out
    blastdb=os.path.join(ligand_lib_dir,dbname)
    if binary=="blastall":
      blastrun_seq=" -p blastp -i %s -a 8 -F F -W 3 -G 11 -E 2 \
          -V F -e 1E-3 -m 7 -d %s"%(fasta_path, blastdb)
    else:
      blastrun_seq= "-query %s -matrix BLOSUM62 -num_threads 8 -word_size 3 -gapopen 11\
        -gapextend 2 -evalue 1E-3 %s -db %s"%(fasta_path, outfmt, blastdb)

    cmds="%s %s"%(blastpath,blastrun_seq)
    #print(cmds)
    try:
      result = easy_run.fully_buffered(
        command=cmds,
        stdin_lines='')
    except KeyboardInterrupt :
      raise KeyboardInterrupt
    else :
      if debug:
        output='myprotein.xml'
        open(output, "w").write("\n".join(result.stdout_lines))
      return result.stdout_lines


 *******************************************************************************


 *******************************************************************************
iotbx/bioinformatics/ncbi_blast_xml.py
"""
  Homology search by BLAST, XML output
"""
from __future__ import absolute_import, division, print_function

from iotbx.bioinformatics.xmlbuild import Single, Multiple
from iotbx.bioinformatics.xmlbuild import Text, TextWithDefault, Group
from iotbx.bioinformatics.xmlbuild import Parser

class Result(object):
  """
  Homology search by BLAST, XML output
  """

  def __init__(self, root):

    self.root = root


  def restrict(self, max_count):

    if self.root.iterations:
      del self.root.iterations[-1].hits[max_count:]


  def hits(self):

    if not self.root.iterations:
      raise StopIteration

    from iotbx import bioinformatics

    for h in self.root.iterations[-1].hits:
      pieces = h.accession.split( "_" )
      assert 0 < len( pieces )

      if len( pieces ) == 1:
        pdb = pieces[0]
        chain = ""

      else:
        ( pdb, chain ) = pieces[:2]

      alignment = bioinformatics.clustal_alignment(
        names = [ "target", "%s_%s" % ( pdb, chain ) ],
        alignments = [ h.hsps[0].query.seq, h.hsps[0].hit.seq ],
        program = "NCBI-BLAST"
        )
      yield bioinformatics.homology_search_hit(
        identifier = pdb,
        chain = chain,
        annotation = h.annotation,
        alignment = alignment
        )


  def __len__(self):

    if not self.root.iterations:
      return 0

    return len( self.root.iterations[-1].hits )


_alternatives = frozenset( "-" )

def substitute_underscore(data):

  return "".join( "_" if c in _alternatives else c for c in data )


BUILDER = Single(
  extractions = [
    Text( attribute = "program", tag = "BlastOutput_program" ),
    Text( attribute = "version", tag = "BlastOutput_version" ),
    Text( attribute = "reference", tag = "BlastOutput_reference" ),
    Text( attribute = "db", tag = "BlastOutput_db" ),
    Text( attribute = "query_id", tag = "BlastOutput_query-ID" ),
    Text( attribute = "query_def", tag = "BlastOutput_query-def" ),
    Text( attribute = "query_len", tag = "BlastOutput_query-len", conversion = int ),
    ],
  child_data_tagged = {
    "BlastOutput_param": (
      "parameters",
      Single(
        extractions = [
          Text( attribute = "matrix", tag = "Parameters/Parameters_matrix" ),
          Text( attribute = "expect", tag = "Parameters/Parameters_expect", conversion = float ),
          Text( attribute = "gap_open", tag = "Parameters/Parameters_gap-open", conversion = float ),
          Text( attribute = "gap_extend", tag = "Parameters/Parameters_gap-extend", conversion = float ),
          Text( attribute = "filter", tag = "Parameters/Parameters_filter" ),
          ],
        ),
      ),
    "BlastOutput_iterations": (
      "iterations",
      Multiple(
        tag = "Iteration",
        processor = Single(
          extractions = [
            Text( attribute = "num", tag = "Iteration_iter-num", conversion = int ),
            TextWithDefault( attribute = "query_id", tag = "Iteration_query-ID", default = "" ),
            TextWithDefault( attribute = "query_def", tag = "Iteration_query-def", default = "" ),
            TextWithDefault(
              attribute = "query_len",
              tag = "Iteration_query-len",
              conversion = int,
              default = 0,
              ),
            ],
          child_data_tagged = {
            "Iteration_hits": (
              "hits",
              Multiple(
                tag = "Hit",
                processor = Single(
                  extractions = [
                    Text( attribute = "num", tag = "Hit_num", conversion = int ),
                    Text( attribute = "identifier", tag = "Hit_id" ),
                    Text( attribute = "annotation", tag = "Hit_def" ),
                    Text( attribute = "accession", tag = "Hit_accession", conversion = substitute_underscore ),
                    Text( attribute = "length", tag = "Hit_len", conversion = int ),
                    ],
                  child_data_tagged = {
                    "Hit_hsps": (
                      "hsps",
                      Multiple(
                        tag = "Hsp",
                        processor = Single(
                          extractions = [
                            Text( attribute = "num", tag = "Hsp_num", conversion = int ),
                            Text( attribute = "bit_score", tag = "Hsp_bit-score", conversion = float ),
                            Text( attribute = "score", tag = "Hsp_score", conversion = float ),
                            Text( attribute = "evalue", tag = "Hsp_evalue", conversion = float ),
                            Group(
                              attribute = "query",
                              extractions = [
                                Text( attribute = "start", tag = "Hsp_query-from", conversion = int ),
                                Text( attribute = "end", tag = "Hsp_query-to", conversion = int ),
                                TextWithDefault(
                                  attribute = "frame",
                                  tag = "Hsp_query-frame",
                                  conversion = int,
                                  default = 0,
                                  ),
                                Text( attribute = "seq", tag = "Hsp_qseq" ),
                                ],
                              ),
                            Group(
                              attribute = "hit",
                              extractions = [
                                Text( attribute = "start", tag = "Hsp_hit-from", conversion = int ),
                                Text( attribute = "end", tag = "Hsp_hit-to", conversion = int ),
                                TextWithDefault(
                                  attribute = "frame",
                                  tag = "Hsp_hit-frame",
                                  conversion = int,
                                  default = 0,
                                  ),
                                Text( attribute = "seq", tag = "Hsp_hseq" ),
                                ],
                              ),
                            Text( attribute = "identity", tag = "Hsp_identity", conversion = int ),
                            Text( attribute = "positive", tag = "Hsp_positive", conversion = int ),
                            TextWithDefault( attribute = "gaps", tag = "Hsp_gaps", conversion = int, default = 0 ),
                            Text( attribute = "length", tag = "Hsp_align-len", conversion = int ),
                            Text( attribute = "midline", tag = "Hsp_midline" ),
                            ],
                          ),
                        ),
                      ),
                    },
                  ),
                ),
              ),
            "Iteration_stat": (
              "statistics",
              Single(
                extractions = [
                  Text( attribute = "db_num", tag = "Statistics/Statistics_db-num", conversion = int ),
                  Text( attribute = "db_len", tag = "Statistics/Statistics_db-len", conversion = int ),
                  Text( attribute = "hsp_len", tag = "Statistics/Statistics_hsp-len", conversion = int ),
                  Text(
                    attribute = "eff_space",
                    tag = "Statistics/Statistics_eff-space",
                    conversion = float,
                    ),
                  Text( attribute = "kappa", tag = "Statistics/Statistics_kappa", conversion = float ),
                  Text( attribute = "lambdav", tag = "Statistics/Statistics_lambda", conversion = float ),
                  Text( attribute = "entropy", tag = "Statistics/Statistics_entropy", conversion = float ),
                  ],
                ),
              ),
            },
          ),
        ),
      ),
    },
  )

parse = Parser( tag = "BlastOutput", builder = BUILDER, restype = Result, cElementTree = True )


 *******************************************************************************


 *******************************************************************************
iotbx/bioinformatics/pdb_info.py
"""Get experimental information summary for PDB IDS from the PDB"""

from __future__ import absolute_import, division, print_function

import sys
from libtbx import easy_pickle
from libtbx.utils import Sorry
import libtbx.load_env
from libtbx import smart_open
import os
import csv
import requests

web_urls = {"rcsb": (
  "https://data.rcsb.org/graphql", """
  {{
    entries(entry_ids: {pdb_list} )
    {{
      rcsb_id
      refine {{
        ls_R_factor_R_free
        ls_R_factor_R_work
        ls_R_factor_obs
        ls_d_res_high
      }}
    }}
  }}"""
)}

def get_experimental_pdb_info(pdbids, site="rcsb"):
  """
  returns list of tuples (pdb_id, resolution, rwork, rfree) and dict
  pdbid: (resolution, rwork, rfree)


  OBSOLETED. New functionality is mmtbx.wwpdb.rcsb_entry_request.get_info
  """
  rlist = []
  rdict = {}
  assert site in ["rcsb"]
  url = web_urls[site][0]
  request = web_urls[site][1]

  pdb_list = "%s" % pdbids
  pdb_list = pdb_list.replace("'", '"')
  request = request.format(pdb_list=pdb_list)
  r = requests.post(url, json={"query":request}, timeout=10)
  r_json = r.json()
  for res in r_json["data"]["entries"]:
    pdb_id = str(res["rcsb_id"])
    resolution, rwork, rfree = (None, None, None)
    if res["refine"] is not None:
      resolution = None if res["refine"][0]["ls_d_res_high"] is None else float(res["refine"][0]["ls_d_res_high"])
      rwork = None if res["refine"][0]["ls_R_factor_R_work"] is None else float(res["refine"][0]["ls_R_factor_R_work"])
      rfree = None if res["refine"][0]["ls_R_factor_R_free"] is None else float(res["refine"][0]["ls_R_factor_R_free"])
      if rwork is None:
        rwork = None if res["refine"][0]["ls_R_factor_obs"] is None else float(res["refine"][0]["ls_R_factor_obs"])
    tup = (pdb_id, resolution, rwork, rfree)
    rlist.append(tup)
    rdict[pdb_id] = tup[1:]
  return rlist, rdict

class pdb_info_local(object):
  def __init__(self):
    """
    Loads pickle with data. Path is temporary in current work dir.
    Should be centralized somewhere else upon going to production.
    """
    db_dict = {}
    pdb_info_file = libtbx.env.find_in_repositories(
      relative_path="cctbx_project/iotbx/bioinformatics/pdb_info.csv.gz",
      test=os.path.isfile)
    csv_file = smart_open.for_reading(file_name=pdb_info_file, gzip_mode="rt")
    csv_reader = csv.reader(csv_file,delimiter=";")
    for row in csv_reader:
      db_dict[row[0]] = (row[1],row[2],row[3],row[4],row[5])
    self.db_dict = db_dict

  def _get_info(self, pdbid, skip_none=True, raise_if_absent=False):
    info = self.db_dict.get(pdbid.upper(), None)
    if info is None and raise_if_absent:
      raise Sorry("Not in database")
    if skip_none and info is not None and info[0] is None:
      info = None
    return info

  def get_info_list(self, pdbids, skip_none=True, raise_if_absent=False):
    """
    Get info about pdbids (list of strings) in form of list of tuples
    (pdbid, resolution, rwork, rfree). Easy to sort.
    """
    result = []
    for pdbid in pdbids:
      info = self._get_info(pdbid, raise_if_absent=raise_if_absent)
      if info is not None:
        result.append( (pdbid,) + info)
    return result

  def get_info_dict(self, pdbids, skip_none=True, raise_if_absent=False):
    """
    Get info about pdbids (list of strings) in form of dict
    pdbid: (resolution, rwork, rfree). Easy to lookup.
    """
    result = {}
    for pdbid in pdbids:
      info = self._get_info(pdbid, raise_if_absent=raise_if_absent)
      if info is not None:
        result[pdbid] = info
    return result

def get_all_experimental_pdb_info_to_pkl():
  """
  Get info (resolution, rwork, rfree) for all PDB from RCSB and dump into
  pickle file:
  pdb_dict 5.6 Mb.
  Takes ~1 minute from home.
  Use only xray diffraction.
  """

  base_url = "https://search.rcsb.org/rcsbsearch/v1/query?json="
  q = {
  "query": {
    "type": "terminal",
    "service": "text",
    "parameters": {
      "attribute": "exptl.method",
      "operator": "exact_match",
      "value": "X-RAY DIFFRACTION"
    }
  },
  "request_options": {
    "return_all_hits": True,
  },
  "return_type": "entry"
  }

  # First get all x-ray pdb ids
  r1 = requests.post(base_url, json=q)
  r1_json = r1.json()
  print ('Total:', r1_json["total_count"])
  res_ids = []
  for res in r1_json["result_set"]:
    res_ids.append(str(res["identifier"]))
  print ('total resids', len(res_ids))
  # Now get the info:
  rlist, rdict = get_experimental_pdb_info(res_ids)
  n_bad = 0
  for tup in rlist:
    if tup.count(None) > 0:
      print(tup)
      n_bad += 1
  print("Total bad records", n_bad)
  easy_pickle.dump(file_name='pdb_dict.pickle', obj=rdict)

def tst_pdb_info_local():
  # Enable before running.
  # get_all_experimental_pdb_info_to_pkl()

  # I don't know why there are 5 values now in the table for each PDB.
  # info_local = pdb_info_local()
  # ans_dict_1 = {'1yjp': (1.8, 0.181, 0.19), '1ucs': (0.62, 0.133, 0.155)}
  # ans_list_1 = [('1ucs', 0.62, 0.133, 0.155), ('1yjp', 1.8, 0.181, 0.19)]
  # assert info_local.get_info_dict(["1ucs", "1yjp"]) == ans_dict_1,
  # assert info_local.get_info_list(["1ucs", "1yjp"]) == ans_list_1
  ans_dict_2 = {'1YJP': (1.8, 0.18086, 0.19014), '1UCS': (0.62, 0.133, 0.155)}
  ans_list_2 = [('1UCS', 0.62, 0.133, 0.155), ('1YJP', 1.8, 0.18086, 0.19014)]
  try:
    rlist, rdict = get_experimental_pdb_info(["1ucs", "1yjp"])
  except requests.exceptions.ReadTimeout:
    print("Skipped test: transient read timeout, can't run test right now")
    return
  assert rlist == ans_list_2, rlist
  assert rdict == ans_dict_2, rdict


def run(args):
  tst_pdb_info_local()

if __name__ == '__main__':
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
iotbx/bioinformatics/pubmed.py
"""Get pubmed reference"""
from __future__ import absolute_import, division, print_function
from libtbx.utils import Sorry
import libtbx.utils
from libtbx import slots_getstate_setstate
from xml.dom.minidom import parseString
from six.moves import urllib

def get_node_data(xml_node, node_name):
  child_nodes = xml_node.getElementsByTagName(node_name)
  return child_nodes[0].childNodes[0].data

def get_pubmed_xml(pmid):
  if isinstance(pmid, str):
    try :
      pmid = int(pmid)
    except ValueError :
      raise Sorry("PubMed IDs must be integers.")
  url = "http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
  params = urllib.parse.urlencode({
    "id" : str(pmid),
    "rettype" : "full",
    "db" : "PubMed",
    "retmode" : "xml",
  })
  result = libtbx.utils.urlopen(url, params).read()
  xmlrec = parseString(result)
  articles = xmlrec.getElementsByTagName("PubmedArticle")
  if (len(articles) == 0):
    raise Sorry("No article with PMID %s found.")
  assert len(articles) == 1
  return article(articles[0])

class author(slots_getstate_setstate):
  __slots__ = ["last_name", "initials"]
  def __init__(self, xml_node):
    self.last_name = get_node_data(xml_node, "LastName").encode("utf-8")
    self.initials = get_node_data(xml_node, "Initials").encode("utf-8")

  def __str__(self):
    return ("%s %s" % (self.last_name, self.initials)).strip()

  def bibtex(self):
    initials = ""
    for i in list(self.initials):
      initials += "%s." % i
    return ("%s, %s" % (self.last_name, initials)).strip()

class article(slots_getstate_setstate):
  __slots__ = ["authors", "title", "year", "journal", "volume", "pages",
               "pmid", "doi"]
  def __init__(self, xmlrec):
    authors = xmlrec.getElementsByTagName("Author")
    self.authors = []
    for author_xml_node in authors :
      self.authors.append(author(author_xml_node))
    self.title = get_node_data(xmlrec, "ArticleTitle").encode("utf-8")
    self.year = get_node_data(xmlrec, "Year").encode("utf-8")
    self.journal = get_node_data(xmlrec, "MedlineTA").encode("utf-8")
    self.volume = get_node_data(xmlrec, "Volume").encode("utf-8")
    self.pages = get_node_data(xmlrec, "MedlinePgn").encode("utf-8")
    self.pmid = get_node_data(xmlrec, "PMID").encode("utf-8")
    self.doi = None
    article_ids = xmlrec.getElementsByTagName("ArticleId")
    for xml_node in article_ids :
      id_type = xml_node.getAttribute("IdType")
      if (id_type == "doi"):
        self.doi = xml_node.childNodes[0].data.encode("utf-8")

  def as_phenix_citation(self):
    doi = "None"
    if (self.doi is not None):
      doi = "\"%s\"" % self.doi
    return "\n".join([
      "citation {",
      "  article_id = %s" % self.pmid,
      "  authors = %s" % ", ".join([ str(a) for a in self.authors ]),
      "  title = %s" % self.title,
      "  journal = %s" % self.journal,
      "  volume = %s" % self.volume,
      "  pages = %s" % self.pages,
      "  year = %s" % self.year,
      "  doi_id = %s" % doi,
      "  pmid = %s" % self.pmid,
      "}"])

  def as_bibtex_citation(self):
    return "\n".join([
      "@Article{%s," % self.pmid,
      "  Author = {%s}," % " and ".join([ a.bibtex() for a in self.authors ]),
      "  Year = {%s}," % self.year,
      "  Title = {%s}," % self.title,
      "  Journal = {%s}," % self.journal,
      "  Volume = {%s}," % self.volume,
      "  Pages = {%s}," % self.pages,
      "}"])


 *******************************************************************************


 *******************************************************************************
iotbx/bioinformatics/structure.py
"""Tools to run BLAST searches"""

from __future__ import absolute_import, division, print_function
from libtbx.utils import Sorry
from libtbx import slots_getstate_setstate_default_initializer
import libtbx.utils
from six.moves import urllib
from six.moves import cStringIO as StringIO
import time, os

def get_ncbi_pdb_blast(sequence, file_name=None, blast_type="blastp",
    expect=0.01):
  """
  Run BLAST against the PDB sequence database on the NCBI's web server.
  Basically just a frontend to a BioPython module.

  :param sequence: plaintext amino-acid or nucleotide sequence
  :param file_name: optional output file name
  :param blast_type: program to run ("blastp" or "blastn")
  :param expect: BLAST e-value cutoff

  :returns: XML string
  """
  assert (blast_type in ["blastp", "blastn"])
  if (sequence[-1] == '*'):
    sequence = sequence[:-1]
  if (not sequence.isalpha()):
    raise Sorry("The sequence contains non-alphabetical characters; in "+
      "addition to A-Z, only an asterisk denoting a stop codon is permitted.")
  assert (expect >= 0)
  try :
    from Bio.Blast import NCBIWWW
  except ImportError :
    raise Sorry("You need to have BioPython installed to use this function.")
  # FIXME will this use the HTTP proxy if defined?
  blast = NCBIWWW.qblast(blast_type, "pdb", sequence, expect=expect)
  blast_out = blast.read()
  if (file_name is not None):
    f = open(file_name, "w")
    f.write(blast_out)
    f.close()
  return blast_out

class blast_hit(slots_getstate_setstate_default_initializer):
  __slots__ = [ "hit_num", "pdb_id", "chain_id", "evalue", "length",
      "identity", "positives", "all_ids", ]

  def show(self, out=None):
    if (out is None) : out = sys.stdout
    print("%3s  %1s   %12g  %6d  %6.2f  %6.2f  %4d" % (self.pdb_id,
      self.chain_id, self.evalue, self.length, self.identity, self.positives,
      len(self.all_ids)), file=out)

def summarize_blast_output(blast_out=None, blast_file=None,
    min_identity=None, expect=None, stop_if_no_alignment=True):
  """
  Parse NCBI BLAST XML output and convert to a list of simple summary
  objects.  Note that this is very specific to searching the PDB, and returns
  incomplete information (suitable for summarizing in a flat table).
  """
  assert ([blast_out, blast_file].count(None) == 1)
  from Bio.Blast import NCBIXML
  import iotbx.pdb.fetch
  if (blast_out is not None):
    blast_in = StringIO(blast_out)
  else :
    assert os.path.isfile(blast_file)
    blast_in = open(blast_file)
  parsed = NCBIXML.parse(blast_in)
  blast = next(parsed)
  if (len(blast.alignments) == 0):
    if stop_if_no_alignment:
      raise Sorry("No matching sequences!")
    else: return list()
  results = []
  for i_hit, hit in enumerate(blast.alignments):
    pdb_chain_id = str(hit.accession)
    #hit.accession may only have pdb_id, e.g. 1EMB
    if len(pdb_chain_id.split("_")) > 1:
      pdb_id, chain_id = pdb_chain_id.split("_")
    else:
      pdb_id = pdb_chain_id
      chain_id = None
    #
    hsp = hit.hsps[0]
    assert (hsp.align_length > 0)
    identity = 100 * hsp.identities / hsp.align_length
    if (min_identity is not None) and (identity < min_identity):
      continue
    # XXX this is really appalling, but the NCBI groups together identical
    # sequences in its BLAST output, so I need to parse the accession code
    # strings to extract the individual PDB IDs
    hit_def_fields = hit.hit_def.split("|")
    all_ids = []
    all_ids.append([pdb_id,chain_id])
    for i_field, field in enumerate(hit_def_fields):
      if (field == "pdb") and (i_field < len(hit_def_fields) -1):
        next_pdb_id = hit_def_fields[i_field + 1]
        if "Chain" in hit_def_fields[i_field + 2]:
          next_chain_id = hit_def_fields[i_field + 2].split()[0]
        else:
          next_chain_id = None
        if (iotbx.pdb.fetch.valid_pdb_id(next_pdb_id)):
          all_ids.append([next_pdb_id,next_chain_id])
    summary = blast_hit(
      hit_num=i_hit+1,
      pdb_id=pdb_id,
      chain_id=chain_id,
      evalue=hsp.expect,
      length=hsp.align_length,
      identity=identity,
      positives=100*hsp.positives/hsp.align_length,
      hsp = hsp,
      all_ids=all_ids)
    results.append(summary)
  return results

def get_all_matching_pdb_ids(sequence, min_identity):
  blast_out = get_ncbi_pdb_blast(sequence=sequence)
  results = summarize_blast_output(blast_out=blast_out,
    min_identity=min_identity)
  ids = []
  for result in results :
    ids.extend([ pdb_chain_id[0].lower() for pdb_chain_id in result.all_ids ])
  return ids

def get_ebi_pdb_wublast(sequence, email, file_name=None, blast_type="blastp",
    sequence_type="protein", exp="1e-3"):
  """
  Run WU-BLAST against the PDB sequence database on the EBI's web server.
  Somewhat more complicated than the NCBI BLAST service, because of two-step
  process (submission and retrieval).  An email address is required to submit
  a job.

  :param sequence: plaintext amino-acid or nucleotide sequence
  :param email: user email address
  :param file_name: optional output file name
  :param blast_type: program to run ("blastp" or "blastn")
  :param sequence_type: currently ignored
  :param exp: BLAST e-value cutoff

  :returns: XML string
  """
  assert (email is not None)
  url = "http://www.ebi.ac.uk/Tools/services/rest/wublast/run/"
  params = urllib.parse.urlencode({
    'sequence': sequence,
    'program' : program,
    'email'   : email,
    'exp'     : exp,
    'database': 'pdb',
    'stype'   : 'protein',
  })
  job_id = libtbx.utils.urlopen(url, params).read()
  while (True):
    time.sleep(1)
    url = "http://www.ebi.ac.uk/Tools/services/rest/wublast/status/%s" % job_id
    status = libtbx.utils.urlopen(url).read()
    if (status == "RUNNING"):
      continue
    elif (status == "FINISHED"):
      url = "http://www.ebi.ac.uk/Tools/services/rest/wublast/result/%s/xml" %\
        job_id
      result = libtbx.utils.urlopen(url).read()
      return result
    elif (status == "ERROR"):
      raise RuntimeError("The EBI server reported an error.")
    elif (status == "FAILURE"):
      raise Sorry("Search failed!")
    elif (status == "NOT_FOUND"):
      raise RuntimeError("The EBI server can't find the job!")
    else :
      raise RuntimeError("Unknown status %s" % status)


 *******************************************************************************


 *******************************************************************************
iotbx/bioinformatics/test/tst_alignment_as_hsearch.py
from __future__ import absolute_import, division, print_function

from iotbx.bioinformatics import alignment_as_hsearch

import unittest


class TestParse(unittest.TestCase):

  HSEARCH = None

  def setUp(self):

    if self.__class__.HSEARCH is None:
      import libtbx.load_env
      root = libtbx.env.under_dist( "iotbx", "bioinformatics" )

      import os.path
      self.__class__.HSEARCH = alignment_as_hsearch.parse(
        source = open( os.path.join( root, "test", "alignment.pir" ) )
        )

  def test_len(self):

    self.assertEqual( len( self.HSEARCH ), 2 )


  def test_hits(self):

    hits = list( self.HSEARCH.hits() )

    self.assertEqual( len( hits ), 2 )

    self.assertEqual( hits[0].identifier, "3ADK" )
    self.assertEqual( hits[0].chain, "A" )
    self.assertEqual( hits[0].annotation, "First hit" )
    self.assertEqual( hits[0].alignment.multiplicity(), 2 )
    self.assertEqual( hits[0].alignment.names[0], "target" )
    self.assertEqual( hits[0].alignment.names[1], "3ADK_A" )
    self.assertEqual(
      hits[0].alignment.alignments[0],
      "MEEKLKKSKIIFVVGGPGSGKGTQCEKIVQKYGYTHLSTGDLLRAEVSSGSARGKMLSEIMEKGQLVPLETVLDMLRDAM"
      + "VAKVDTSKGFLIDGYPREVKQGEEFERKIGQPTLLLYVDAGPETMTKRLLKRGETSGRVDDNEETIKKRLETYYKATEPV"
      + "IAFYEKRGIVRKVNAEGSVDDVFSQVCTHLDTLK",
      )
    self.assertEqual(
      hits[0].alignment.alignments[1],
      "MEEKLKKSKIIFVVGGPGSGKGTQCEKIVQKYGYTHLSTGDLLRAEVSSGSARGKMLSEIMEKGQLVPLETVLDMLRDAM"
      + "VAKVDTSKGFLIDGYPREVKQGEEFERKIGQPTLLLYVDAGPETMTKRLLKRGETSGRVDDNEETIKKRLETYYKATEPV"
      + "IAFYEKRGIVRKVNAEGSVDDVFSQVCTHLDTLK",
      )

    self.assertEqual( hits[1].identifier, "1Z83" )
    self.assertEqual( hits[1].chain, "A" )
    self.assertEqual( hits[1].annotation, "Second hit" )
    self.assertEqual( hits[1].alignment.multiplicity(), 2 )
    self.assertEqual( hits[1].alignment.names[0], "target" )
    self.assertEqual( hits[1].alignment.names[1], "1Z83_A" )
    self.assertEqual(
      hits[1].alignment.alignments[0],
      "MEEKLKKSKIIFVVGGPGSGKGTQCEKIVQKYGYTHLSTGDLLRAEVSSGSARGKMLSEIMEKGQLVPLETVLDMLRDAM"
      + "VAKVDTSKGFLIDGYPREVKQGEEFERKIGQPTLLLYVDAGPETMTKRLLKRGETSGRVDDNEETIKKRLETYYKATEPV"
      + "IAFYEKRGIVRKVNAEGSVDDVFSQVCTHLDTLK",
      )
    self.assertEqual(
      hits[1].alignment.alignments[1],
      "MEEKLKKTNIIFVVGGPGSGKGTQCEKIVQKYGYTHLSTGDLLRSEVSSGSARGKKLSEIMEKGQLVPLETVLDMLRDAM"
      + "VAKVNTSKGFLIDGYPREVQQGEEFERRIGQPTLLLYVDAGPETMTQRLLKRGETSGRVDDNEETIKKRLETYYKATEPV"
      + "IAFYEKRGIVRKVNAEGSVDSVFSQVCTHLDAL-"
      )


suite_parse = unittest.TestLoader().loadTestsFromTestCase( TestParse )

alltests = unittest.TestSuite(
  [
    suite_parse,
    ]
  )


def load_tests(loader, tests, pattern):

  return alltests


if __name__ == "__main__":
  unittest.TextTestRunner( verbosity = 2 ).run( alltests )



 *******************************************************************************


 *******************************************************************************
iotbx/bioinformatics/test/tst_ebi_wu_blast_xml.py
from __future__ import absolute_import, division, print_function

from iotbx.bioinformatics import ebi_wu_blast_xml

import unittest


class TestParse(unittest.TestCase):

  HSEARCH = None

  def setUp(self):

    if self.__class__.HSEARCH is None:
      import libtbx.load_env
      root = libtbx.env.under_dist( "iotbx", "bioinformatics" )

      import os.path
      import gzip
      self.__class__.HSEARCH = ebi_wu_blast_xml.parse(
        source = gzip.open( os.path.join( root, "test", "ebi_blast.xml.gz" ) )
        )


  def testAlignment(self):

    ali = self.HSEARCH.root.hits[6].alignments[0]

    self.assertEqual( ali.number, 1 )
    self.assertEqual( ali.score, 605 )
    self.assertAlmostEqual( ali.bits, 218.0, 1 )
    self.assertAlmostEqual( ali.expectation, 2.0e-59, 10 )
    self.assertAlmostEqual( ali.probability, 2.0e-59, 10 )
    self.assertEqual( ali.identity, 58 )
    self.assertEqual( ali.positives, 79 )
    self.assertEqual( ali.query.start, 4 )
    self.assertEqual( ali.query.end, 194 )
    self.assertEqual(
      ali.query.seq,
      "EKLKKSKIIFVVGGPGSGKGTQCEKIVQKYGYTHLSTGDLLRAEVSSGSARGKMLSEIMEKGQLVP"
      + "LETVLDMLRDAMVAKVDTSKGFLIDGYPREVKQGEEFERKIGQPTLLLYVDAGPETMTKRLLKR"
      + "GETSGRVDDNEETIKKRLETYYKATEPVIAFYEKRGIVRKVNAEGSVDDVFSQVCTHLDTL"
      )
    self.assertEqual( ali.match.start, 7 )
    self.assertEqual( ali.match.end, 197 )
    self.assertEqual(
      ali.match.seq,
      "EDLRKCKIIFIIGGPGSGKGTQCEKLVEKYGFTHLSTGELLREELASESERSKLIRDIMERGDLVP"
      + "SGIVLELLKEAMVASLGDTRGFLIDGYPREVKQGEEFGRRIGDPQLVICMDCSADTMTNRLLQM"
      + "SRSSLPVDDTTKTIAKRLEAYYRASIPVIAYYETKTQLHKINAEGTPEDVFLQLCTAIDSI"
      )
    self.assertEqual(
      ali.pattern,
      "E L+K KIIF++GGPGSGKGTQCEK+V+KYG+THLSTG+LLR E++S S R K++ +IME+G LVP"
      + "   VL++L++AMVA +  ++GFLIDGYPREVKQGEEF R+IG P L++ +D   +TMT RLL+ "
      + "  +S  VDD  +TI KRLE YY+A+ PVIA+YE +  + K+NAEG+ +DVF Q+CT +D++"
      )


  def testHit(self):

    hit = self.HSEARCH.root.hits[0]

    self.assertEqual( hit.number, 1 )
    self.assertEqual( hit.database, "pdb" )
    self.assertEqual( hit.identifier, "3ADK_A" )
    self.assertEqual( hit.length, 195 )
    self.assertEqual( hit.description, "mol:protein length:195  ADENYLATE KINASE" )
    self.assertEqual( len( hit.alignments ), 1 )


  def testRoot(self):

    header = self.HSEARCH.root.header

    self.assertEqual( header.program.name, "WU-blastp" )
    self.assertEqual( header.program.version, "2.0MP-WashU [04-May-2006]" )
    self.assertEqual( header.program.citation, "PMID:12824421" )
    self.assertEqual(
      header.command,
      "/ebi/extserv/bin/wu-blast/blastp \"pdb\" /ebi/extserv/blast-work/interactive/blast-20090331-16013"
      + "83158.input E=10 B=50 V=100 -mformat=1 -matrix BLOSUM62 -sump  -filter seg -cpus 8 -sort_by_pvalue -putenv='"
      + "WUBLASTMAT=/ebi/extserv/bin/wu-blast/matrix' -putenv=\"WUBLASTDB=$IDATA_CURRENT/blastdb\" -putenv="
      + "'WUBLASTFILTER=/ebi/extserv/bin/wu-blast/filter' "
      )
    self.assertEqual( header.time.start, "2009-03-31T16:01:44+01:00" )
    self.assertEqual( header.time.end, "2009-03-31T16:01:45+01:00" )
    self.assertEqual( header.time.search, "PT01S" )
    self.assertEqual( len( header.parameters.sequences ), 1 )

    seq = header.parameters.sequences[0]
    self.assertEqual( seq.number, 1 )
    self.assertEqual( seq.name, "Sequence" )
    self.assertEqual( seq.type, "p" )
    self.assertEqual( seq.length, 195 )
    self.assertEqual( len( header.parameters.databases ), 1 )

    db = header.parameters.databases[0]
    self.assertEqual( db.number, 1 )
    self.assertEqual( db.name, "pdb" )
    self.assertEqual( db.type, "p" )
    self.assertEqual( db.created, "2009-03-27T00:00:07+01:00" )

    self.assertEqual( header.parameters.scores, 100.0 )
    self.assertEqual( header.parameters.alignments, 50 )
    self.assertEqual( header.parameters.matrix, "BLOSUM62" )
    self.assertEqual( header.parameters.expectation_upper, 10.0 )
    self.assertEqual( header.parameters.statistics, "sump" )
    self.assertEqual( header.parameters.filter, "seg" )

    hits = self.HSEARCH.root.hits
    self.assertEqual( len( hits ), 50 )


suite_parse = unittest.TestLoader().loadTestsFromTestCase( TestParse )

alltests = unittest.TestSuite(
  [
    suite_parse,
    ]
  )


def load_tests(loader, tests, pattern):

  return alltests


if __name__ == "__main__":
  unittest.TextTestRunner( verbosity = 2 ).run( alltests )



 *******************************************************************************


 *******************************************************************************
iotbx/bioinformatics/test/tst_local_blast.py
from __future__ import absolute_import, division, print_function
from iotbx.bioinformatics import local_blast

#Note:
#I've used "X" to fill gaps and N-term positions to keep resnum consistant with
#blast output.  It seems to work fine. If you keep track of your sequences
#seperately, you probably can do it differently.   LWH 4/12/19

seq="XXXXNKLHVIDLHKRYGGHEVLKGVSLQARAGDVISIIGSSGSGKSTFLRCINFLEKPSEGAIIVNGQNINLVR\
DKDGQLKVADKNQLRLLRTRLTMVFQHFNLWSHMTVLENVMEAPIQVLGLSKHDARERALKYLAKVGIDERAQGKYPVH\
LSGGQQQRVSIARALAMEPDVLLFDEPTSALDPELVGEVLRIMQQLAEEGKTMVVVTHEMGFARHVSSHVIFLHQGKIE\
EEGDPEQVFGNPQSPRLQQFLKGSLKKLEH"

if __name__=="__main__":
  a=local_blast.pdbaa(seq=seq).run()
  xmldata="\n".join(a)
  hit="1B0U"
  assert hit in xmldata,"XML output not as expected. Pdbaa test failed."
  print("OK")


 *******************************************************************************


 *******************************************************************************
iotbx/bioinformatics/test/tst_ncbi_blast_xml.py
from __future__ import absolute_import, division, print_function

from iotbx.bioinformatics import ncbi_blast_xml

import unittest


class TestParse(unittest.TestCase):

  HSEARCH = None

  def setUp(self):

    if self.__class__.HSEARCH is None:
      import libtbx.load_env
      root = libtbx.env.under_dist( "iotbx", "bioinformatics" )

      import os.path
      import gzip
      self.__class__.HSEARCH = ncbi_blast_xml.parse(
        source = gzip.open( os.path.join( root, "test", "ncbi_blast.xml.gz" ) )
        )


  def testHsps(self):

    hsp = self.HSEARCH.root.iterations[0].hits[4].hsps[0]

    self.assertEqual( hsp.num, 1 )
    self.assertAlmostEqual( hsp.bit_score, 159.073, 3 )
    self.assertEqual( hsp.score, 401 )
    self.assertAlmostEqual( hsp.evalue, 6.95488e-40, 8 )
    self.assertEqual( hsp.query.start, 8 )
    self.assertEqual( hsp.query.end, 192 )
    self.assertEqual( hsp.query.frame, 0 )
    self.assertEqual( hsp.hit.start, 2 )
    self.assertEqual( hsp.hit.end, 193 )
    self.assertEqual( hsp.hit.frame, 0 )
    self.assertEqual( hsp.identity, 78 )
    self.assertEqual( hsp.positive, 127 )
    self.assertEqual( hsp.gaps, 7 )
    self.assertEqual( hsp.length, 192 )
    self.assertEqual(
      hsp.query.seq,
      "KSKIIFVVGGPGSGKGTQCEKIVQKYGYTHLSTGDLLRAEVSS-GSARGKMLSEIMEKGQLVPLET"
      + "VLDMLR---DAMVAKVDTSKGFLIDGYPREVKQGEEFERKI---GQPTLLLYVDAGPETMTKRL"
      + "LKRGETSGRVDDNEETIKKRLETYYKATEPVIAFYEKRGIVRKVNAEGSVDDVFSQVCTHLD"
      )
    self.assertEqual(
      hsp.hit.seq,
      "KPLVVFVLGGPGAGKGTQCARIVEKYGYTHLSAGELLRDERKNPDSQYGELIEKYIKEGKIVPVEI"
      + "TISLLKREMDQTMAANAQKNKFLIDGFPRNQDNLQGWNKTMDGKADVSFVLFFDCNNEICIERC"
      + "LERGKSSGRSDDNRESLEKRIQTYLQSTKPIIDLYEEMGKVKKIDASKSVDEVFDEVVQIFD"
      )
    self.assertEqual(
      hsp.midline,
      "K  ++FV+GGPG+GKGTQC +IV+KYGYTHLS G+LLR E  +  S  G+++ + +++G++VP+E "
      + " + +L+   D  +A       FLIDG+PR     + + + +      + +L+ D   E   +R "
      + "L+RG++SGR DDN E+++KR++TY ++T+P+I  YE+ G V+K++A  SVD+VF +V    D"
      )


  def testHits(self):

    hit = self.HSEARCH.root.iterations[0].hits[4]

    self.assertEqual( hit.num, 5 )
    self.assertEqual( hit.identifier, "gi|50513735|pdb|1TEV|A" )
    self.assertEqual(
      hit.annotation,
      "Chain A, Crystal Structure Of The Human UmpCMP KINASE IN OPEN Conformation"
      )
    self.assertEqual( hit.accession, "1TEV_A" )
    self.assertEqual( hit.length, 196 )
    self.assertEqual( len( hit.hsps ), 1 )


  def testIterations(self):

    iteration = self.HSEARCH.root.iterations[0]

    self.assertEqual( iteration.num, 1 )
    self.assertEqual( iteration.query_id, "77636" )
    self.assertEqual( iteration.query_def, "3ADK:A|PDBID|CHAIN|SEQUENCE" )
    self.assertEqual( iteration.query_len, 195 )
    self.assertEqual( len( iteration.hits ), 68 )
    self.assertEqual( iteration.statistics.db_num, 40680 )
    self.assertEqual( iteration.statistics.db_len, 9289891 )
    self.assertEqual( iteration.statistics.hsp_len, 0 )
    self.assertEqual( iteration.statistics.eff_space, 0 )
    self.assertEqual( iteration.statistics.kappa, 0.041 )
    self.assertEqual( iteration.statistics.lambdav, 0.267 )
    self.assertEqual( iteration.statistics.entropy, 0.14 )


  def testRoot(self):

    output = self.HSEARCH.root

    self.assertEqual( output.program, "blastp" )
    self.assertEqual( output.version, "BLASTP 2.2.20+" )
    self.assertEqual(
      output.reference,
      'Alejandro A. Sch&auml;ffer, L. Aravind, Thomas L. Madden, '
      + 'Sergei Shavirin, John L. Spouge, Yuri I. Wolf, Eugene V. Koonin, '
      + 'and Stephen F. Altschul (2001), '
      +'"Improving the accuracy of PSI-BLAST protein database searches with '
      + 'composition-based statistics and other refinements", '
      + 'Nucleic Acids Res. 29:2994-3005.'
      )
    self.assertEqual( output.db, "pdb" )
    self.assertEqual( output.query_id, "77636" )
    self.assertEqual( output.query_def, "3ADK:A|PDBID|CHAIN|SEQUENCE" )
    self.assertEqual( output.query_len, 195 )
    self.assertEqual( len( output.iterations ), 1 )
    self.assertEqual( output.parameters.matrix, "BLOSUM62" )
    self.assertEqual( output.parameters.expect, 10 )
    self.assertEqual( output.parameters.gap_open, 11 )
    self.assertEqual( output.parameters.gap_extend, 1 )
    self.assertEqual( output.parameters.filter, "F" )


suite_parse = unittest.TestLoader().loadTestsFromTestCase( TestParse )

alltests = unittest.TestSuite(
  [
    suite_parse,
    ]
  )


def load_tests(loader, tests, pattern):

  return alltests


if __name__ == "__main__":
  unittest.TextTestRunner( verbosity = 2 ).run( alltests )



 *******************************************************************************


 *******************************************************************************
iotbx/bioinformatics/xmlbuild.py
"""
Tools for working with xml
"""

from __future__ import absolute_import, division, print_function

class Node(object):
  """
  A node that allows setting arbitrary attributes
  """

def dummy(value):

  return value

# Extractions
class Value(object):
  """
  Extract text from elem
  """

  def __init__(self, attribute, conversion = dummy):

    self.attribute = attribute
    self.conversion = conversion


  def __call__(self, elem, node):

    setattr( node, self.attribute, self.conversion( elem.text ) )


class Text(object):
  """
  Extract text
  """

  def __init__(self, attribute, tag, conversion = dummy):

    self.attribute = attribute
    self.tag = tag
    self.conversion = conversion


  def __call__(self, elem, node):

    child = elem.find( self.tag )

    if child is None:
      raise RuntimeError("Element '%s' has no child '%s'" % (elem.tag, self.tag ))

    setattr( node, self.attribute, self.conversion( child.text ) )


class TextWithDefault(object):
  """
  Extract text or fill in default if not present
  """

  def __init__(self, attribute, tag, default = None, conversion = dummy):

    self.attribute = attribute
    self.tag = tag
    self.default = default
    self.conversion = conversion


  def __call__(self, elem, node):

    child = elem.find( self.tag )

    if child is None:
      value = self.default

    else:
      value = self.conversion( child.text )

    setattr( node, self.attribute, value )


class Attribute(object):
  """
  Extract an attribute
  """

  def __init__(self, attribute, name, conversion = dummy):

    self.attribute = attribute
    self.name = name
    self.conversion = conversion


  def __call__(self, elem, node):

    res = elem.get( self.name )

    if res is None:
      raise RuntimeError("Element '%s' has no attribute '%s'" % (
        elem.tag,
        self.name,
        ))

    setattr( node, self.attribute, self.conversion( res ) )


class AttributeWithDefault(object):
  """
  Extract an attribute or fill in a default value
  """

  def __init__(self, attribute, name, default, conversion = dummy):

    self.attribute = attribute
    self.name = name
    self.default = default
    self.conversion = conversion


  def __call__(self, elem, node):

    res = elem.get( self.name, self.default )
    setattr( node, self.attribute, self.conversion( res ) )


class Group(object):
  """
  Adds extra node to group items with a logical connection
  """

  def __init__(self, attribute, extractions):

    self.attribute = attribute
    self.extractions = extractions


  def __call__(self, elem, node):

    child = Node()
    setattr( node, self.attribute, child )

    for processing in self.extractions:
      processing( elem = elem, node = child )


# Nodes
class Single(object):
  """
  Extract data from an ElementTree Element into a Node
  """

  def __init__(self, child_data_tagged = {}, extractions = []):

    self.child_data_tagged = child_data_tagged
    self.extractions = extractions


  def __call__(self, iterstream, endtag):

    unseen = set( self.child_data_tagged )
    node = Node()

    for ( event, elem ) in iterstream:
      if elem.tag == endtag:
        assert event == "end"

        for processing in self.extractions:
          processing( elem = elem, node = node )

        elem.clear()
        break

      elif event == "start" and elem.tag in self.child_data_tagged:
        unseen.remove( elem.tag )
        ( attribute, processor ) = self.child_data_tagged[ elem.tag ]
        child = processor( iterstream = iterstream, endtag = elem.tag )
        setattr( node, attribute, child )

    if unseen:
      raise RuntimeError("Missing children for element '%s': %s" % (
        endtag,
        ", ".join( "'%s'" % t for t in unseen ),
        ))

    return node


class Multiple(object):
  """
  Extract data from an ElementTree Element into a child Node
  """

  def __init__(self, tag, processor):

    self.tag = tag
    self.processor = processor


  def __call__(self, iterstream, endtag):

    array = []

    for ( event, elem ) in iterstream:
      if event == "end":
        assert elem.tag == endtag
        elem.clear()
        break

      elif event == "start":
        assert elem.tag == self.tag
        child = self.processor( iterstream = iterstream, endtag = self.tag )
        array.append( child )

    return array


class DataAttribute(object):
  """
  A node that is converted directly to data, i.e. no children
  """

  def __init__(self, name, conversion = dummy):

    self.name = name
    self.conversion = conversion


  def __call__(self, iterstream, endtag):

    for ( event, elem ) in iterstream:
      if event == "end" and elem.tag == endtag:
        raw = elem.get( self.name )

        if raw is None:
          raise RuntimeError("%s has no %s attribute" % ( elem.tag, self.name ))

        result = self.conversion( raw )
        elem.clear()
        return result


# Parser
class Parser(object):

  def __init__(self, tag, builder, restype, cElementTree = False):

    self.tag = tag
    self.builder = builder
    self.restype = restype

    if cElementTree:
      import xml.etree.cElementTree
      self.module = xml.etree.cElementTree

    else:
      import xml.etree.ElementTree
      self.module = xml.etree.ElementTree


  def __call__(self, source):

    iterstream = self.module.iterparse( source, events = ( "start", "end" ) )

    for ( event, elem ) in iterstream:
      if elem.tag == self.tag and event == "start":
        return self.restype(
          root = self.builder( iterstream = iterstream, endtag = self.tag )
          )

    else:
      raise RuntimeError("Start tag %s not found" % self.tag)



 *******************************************************************************
