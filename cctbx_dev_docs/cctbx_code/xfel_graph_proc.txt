

 *******************************************************************************
xfel/graph_proc/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/graph_proc/components.py
""" Module for treating images as the vertices of a graph. Includes both Edge and Vertex (ImageNode) classes. """
from __future__ import absolute_import, division, print_function
from scitbx.matrix import sqr
from cctbx.uctbx import unit_cell
import numpy as np
import math
import logging
from cctbx.array_family import flex
from xfel.clustering.singleframe import SingleFrame

class ImageNode(SingleFrame):
  """ Extends a SingleFrame object to make it a vertex in a graph. Adds a list
   of edges.
  """

  def __init__(self, *args, **kwargs):
    """ Constructor is same as for SingleFrame object, but has additional
    kwargs:
    :param kwargs['scale']: default True. Specifies if the images should be scaled upon creation. Mainly switched off for testing.
    :param kwargs['use_b']: default True. If false, only initialise the scale factor, not the B factor.
    """
    SingleFrame.__init__(self, *args, **kwargs)
    if hasattr(self, 'miller_array'):  # i.e. if the above worked.
      self.use_scales = kwargs.get('scale', True)
      self.use_b = kwargs.get('use_b', True)
      self.edges = []  # this is populated when the Graph is made.
      self.partialities = self.calc_partiality(self.get_x0(),
                                               update_wilson=self.use_scales)
      self.scales = self.calc_scales(self.get_x0())
      self.label = None  # to be used for classification after instantiation
      self.source = None  # for testing, when we know the 'source' of the image
      self.params = self.get_x0()


  def calc_partiality(self, params_in, update_wilson=False):
    """ Reuturn a partiality vector for all the reflections in self, using the
    parameters defined in the array params.

    :params: a tuple of the form appropriate for the crystal symetry, such as
    one produced by get_x0()
    :return: a list of partialities for the miller indicies in self.
    """
    from xfel.cxi.postrefine.mod_leastsqr import prep_output, \
      get_crystal_orientation
    from xfel.cxi.postrefine.mod_partiality \
      import partiality_handler as PartialityHandler

    # Expand to triclinic params, and unpack
    params_all = prep_output(params_in, self.crystal_system)
    G, B, rotx, roty, ry, rz, re, a, b, c, alpha, beta, gamma = params_all

    # Create a unit cell based on params, not the initialised values.
    try:
      uc = unit_cell((a, b, c, alpha, beta, gamma))
    except RuntimeError:
      return  None
      logging.warning("Could not create unit cell with ({}, {}, {}, {}, {}, {})"
                      ". Resetting unit cell to original value."
                      .format(a, b, c, alpha, beta, gamma))
      uc = self.orientation.unit_cell()

    ortho_matrix = uc.orthogonalization_matrix()
    orientation_matrix = self.orientation.crystal_rotation_matrix()

    crystal_init_orientation = get_crystal_orientation(ortho_matrix,
                                                       orientation_matrix)
    crystal_orientation_model = crystal_init_orientation \
      .rotate_thru((1, 0, 0), rotx).rotate_thru((0, 1, 0), roty)
    a_star = sqr(crystal_orientation_model.reciprocal_matrix())

    # set up variables
    miller_indices = self.miller_array.indices()
    ph = PartialityHandler(self.wavelength, None)
    bragg_angle_set = self.miller_array.two_theta(wavelength=self.wavelength) \
      .data()
    mm_predictions = self.pixel_size * self.mapped_predictions
    alpha_angle_obs = flex.double([math.atan(abs(pred[0] - self.xbeam) /
                                             abs(pred[1] - self.ybeam))
                                   for pred in mm_predictions])
    assert len(alpha_angle_obs) == len(self.miller_array.indices()), \
      'Size of alpha angles and observations are not equal %6.0f, %6.0f' \
      % (len(alpha_angle_obs), len(self.miller_array.indices()))

    partiality = ph.calc_partiality_anisotropy_set(a_star, miller_indices, ry,
                                                   rz, re, bragg_angle_set,
                                                   alpha_angle_obs)
    if update_wilson:
      self.minus_2B, self.G, self.log_i, self.sinsqtheta_over_lambda_sq, \
      self.wilson_err = self.init_calc_wilson(use_b_factor=self.use_b,
                                              i_corrections=(1 / partiality))
    return partiality

  def calc_scales(self, params_in):
    """ Calculate an array of scales based on scale, B and wavelength using the
    equation $scale * exp(-2*B*(sin(theta)/wavelength)^2)$

    Reuturn a scale vector for all the reflections in self, using the
    parameters defined in the array params.

    :params: a tuple of the form appropriate for the crystal symetry, such as
    one produced by get_x0(). This method only uses params[0] (scale) and
    params[1] (B)

    :return: a list of scales for all the miller indicies in self
    """
    if self.use_scales:
      scale = params_in[0]
      B = params_in[1]
      sin_sq_theta = self.miller_array.two_theta(wavelength=self.wavelength) \
        .sin_theta_over_lambda_sq().data()

      scales = scale * self.miller_array.data()
      exp_arg = flex.double(-2 * B * sin_sq_theta)
      return flex.double(flex.double(scales) * flex.exp(exp_arg))
    else:
      # Horrible way to get vector of ones...
      return flex.double(self.miller_array.data()/self.miller_array.data())


  def get_x0(self):
    """
    Return the initial estimages of parameters to be refined for this frame
    :return: inital (G, B, rotx, roty, ry, rz, re, a, b, c, alpha, beta, gamma)
    for lowest symetry, up to (G, B, rotx, roty, ry, rz, re, a) for cubic.
    values, using:

      - G: initial guess from linear regression
      - B: initial guess from linear regression
      - rotx: 0
      - roty: 0
      - ry: spot radius initial guess
      - rz: spot radius initial guess
      - re: 0.0026 (From Winkler et al. paper)
      - [a, b, c, alpha, beta, gamma]: from initial indexing solution, dependent on crystal system:

            Triclinic: (G, B, rotx, roty, ry, rz, re, a)
            Monoclinic: (G, B, rotx, roty, ry, rz, re,a,b,c,beta])
            Orthorhombic: (G, B, rotx, roty, ry, rz, re,a,b,c)
            Tetragonal: (G, B, rotx, roty, ry, rz, re,a,c)
            Trigonal or Hexagonal: (G, B, rotx, roty, ry, rz, re,a,c)
            Cubic: (G, B, rotx, roty, ry, rz, re,a)
    """
    from xfel.cxi.postrefine.mod_leastsqr import prep_input
    from xfel.cxi.postrefine.test_rs import calc_spot_radius

    a_star = sqr(self.orientation.reciprocal_matrix())
    miller_indices = self.miller_array.indices()
    spot_radius = calc_spot_radius(a_star, miller_indices, self.wavelength)
    x_init = [self.G, - 1 * self.minus_2B / 2, 0, 0,
              spot_radius, spot_radius, 0.0026]
    x_init.extend(self.uc)
    x0_all = np.array(x_init)
    x0 = prep_input(x0_all, self.crystal_system)
    return x0


class Edge:
  """
  .. note::
    Developmental code. Do not use without contacting zeldin@stanford.edu

  Defines an undirected edge in a graph. Contains the connecting vertices, and a weight.
  """

  def __init__(self, vertex_a, vertex_b, weight):
    self.vertex_a = vertex_a
    self.vertex_b = vertex_b
    self.weight = weight
    self.intra = None  # True means we assume it connects two edges from the
                       # same class, False means it is an iter-class edge.

  def mean_residual(self):
    """
    :return: the mean residual of the absolute value of the all the weigts on this edge.
    """
    return np.mean(np.abs(self.residuals()))

  def other_vertex(self, vertex):
    """
    Simple method to get the other vertex along and edge.

    :param vertex: a vertex that is on one end of this edge
    :return: the vertex at the other end of the edge
    """
    assert vertex == self.vertex_a or vertex == self.vertex_b
    if vertex is self.vertex_a:
      return self.vertex_b
    elif vertex is self.vertex_b:
      return self.vertex_a

  @staticmethod
  def _calc_residuals(va, vb, pa, pb, sa, sb):

    mtch_indcs = va.miller_array.match_indices(vb.miller_array,
                                               assert_is_similar_symmetry=False)

    va_selection = mtch_indcs.pair_selection(0)
    vb_selection = mtch_indcs.pair_selection(1)

    sp_a = pa.select(va_selection) * sa.select(va_selection)
    sp_b = pb.select(vb_selection) * sb.select(vb_selection)

    ia_over_ib = va.miller_array.data().select(va_selection) / \
                 vb.miller_array.data().select(vb_selection)

    residuals = (flex.log(sp_a) - flex.log(sp_b) - flex.log(ia_over_ib))
    residuals = residuals.as_numpy_array()
    #logging.debug("Mean Residual: {}".format(np.mean(residuals)))
    return residuals[~np.isnan(residuals)]

  def residuals(self):
    """
    Calculates the edge residual, as defined as the sum over all common miller indices of:
      log(scale * partiality of a) - log(scale * partiality of b) - log(I_a/I_b)

    :return: the residual score for this edge
    """
    # 1. Create flex selection array
    # 2. Trim these so that they only contain common reflections
    # 3. Calculate residual

    partialities_a = self.vertex_a.partialities
    partialities_b = self.vertex_b.partialities
    scales_a = self.vertex_a.scales
    scales_b = self.vertex_b.scales

    return Edge._calc_residuals(self.vertex_a, self.vertex_b,
                           partialities_a, partialities_b,
                           scales_a, scales_b)


 *******************************************************************************


 *******************************************************************************
xfel/graph_proc/graph.py
""" This module is designed to provide tools to deal with groups of serial
crystallography images, treated as a graph problem.

**Author:**   Oliver Zeldin <zeldin@stanford.edu>
"""
from __future__ import absolute_import, division, print_function
from functools import reduce
from six.moves import range
from six.moves import zip

__author__ = 'zeldin'

import os
import logging
from xfel.graph_proc.components import ImageNode, Edge
from xfel.clustering.cluster import Cluster
import numpy as np
import matplotlib.pyplot as plt
import random

class Graph(Cluster):
  """
  .. note::
    Developmental code. Do not use without contacting zeldin@stanford.edu

  Class for treating a graph of serial still XFEL shots as a graph.
  """
  def __init__(self, vertices, min_common_reflections=10):
    """
    Extends the constructor from cluster.Cluster to describe the cluster as a
    graph.

    :param min_common_reflections: number of reflections two images must have in
    common for an edge to be created.
    """
    Cluster.__init__(self, vertices, "Graph cluster", "made as a graph")
    self.common_miller_threshold = min_common_reflections
    self.edges = self._make_edges()


  def _make_edges(self):
    all_edges = []
    for a, vertex1 in enumerate(self.members):
      for b, vertex2 in enumerate(self.members[:a]):
        miller1 = vertex1.miller_array
        miller2 = vertex2.miller_array
        edge_weight = miller1.common_set(miller2,
                                      assert_is_similar_symmetry=False).size()
        if edge_weight >= self.common_miller_threshold:
          logging.debug("Making edge: ({}, {}) {}, {}"
                        .format(a, b, vertex1.name, vertex2.name))
          this_edge = Edge(vertex1, vertex2, edge_weight)
          logging.debug("Edge weight: {}".format(edge_weight))
          all_edges.append(this_edge)
          vertex1.edges.append(this_edge)
          vertex2.edges.append(this_edge)

    assert len(all_edges) == sum([len(v.edges) for v in self.members])/2
    return all_edges


  @classmethod
  def from_directories(cls, folder, **kwargs):
    """
    Creates a Graph class instance from a directory of files (recursively)

    :param folder: the root directory containing integration pickles.
    :return: A Graph instance.
    """
    def add_frame(all_vertices, dirpath, filename, **kwargs):
      path = os.path.join(dirpath, filename)
      this_frame = ImageNode(path, filename, **kwargs)
      if hasattr(this_frame, 'miller_array'):
        all_vertices.append(this_frame)
      else:
        logging.info('skipping file {}'.format(filename))

    nmax = kwargs.get('nmax', None)

    all_vertices = []
    for arg in folder:
      for (dirpath, dirnames, filenames) in os.walk(arg):
        for filename in filenames:
          if not nmax:
            add_frame(all_vertices, dirpath, filename, **kwargs)
          else:
            if len(all_vertices) <= nmax:
              add_frame(all_vertices, dirpath, filename)
    return cls(all_vertices)

  def make_nx_graph(self, saveas=None, show_singletons=True, force_res=False,
                    **kwargs):
    """
    Create a networkx Graph, and visualise it.

    :param saveas: optional filename to save the graph as a pdf as. Displays graph if not specified.
    :param pos: an optional networkx position dictionairy for plotting the graph
    :param force_res: if True, use the edge residual field to color them. Otherwise use edge labels if available.
    :return: the position dictionairy, so that mutiple plots can be made using the same set of positions.
    """
    import networkx as nx
    import brewer2mpl
    cols = brewer2mpl.get_map('BrBG', 'Diverging', 3).mpl_colors

    nx_graph = nx.Graph()
    # Add vertices to graph
    if show_singletons:
      nx_graph.add_nodes_from(self.members)
    else:
      nx_graph.add_nodes_from([vert for vert in self.members if vert.edges])
    logging.debug("Vertices added to graph.")

    # Add edges to graph
    for edge in self.edges:
      nx_graph.add_edge(edge.vertex_a, edge.vertex_b,
                        {'n_conn': edge.weight,
                         'residual': np.mean(np.abs(edge.residuals())),
                         'intra': edge.intra})
    logging.debug("Edges added to graph.")

    # Position vertices
    if 'pos' not in kwargs :
      kwargs['pos'] = nx.spring_layout(nx_graph, iterations=1000)

    # Organise Edge colors
    if all([e.intra is None for e in self.edges]) or force_res:
      # color by residual
      e_cols = [edge[2]["residual"]
                     for edge in nx_graph.edges_iter(data=True)]
      e_cmap = plt.get_cmap('jet')
      e_vmax = max(e_cols)
      e_vmin = 0
    else:
      # color by edge
      e_cmap = None
      e_vmin = None
      e_vmax = None
      e_cols = []
      for edge in nx_graph.edges_iter(data=True):
        intra = edge[2]["intra"]
        if intra is None:
          e_cols.append('0.5')
        elif intra is True:
          e_cols.append('black')
        elif intra is False:
          e_cols.append('red')
        else:
          raise Exception("something bad happened here")

    # Organise Vertex colors
    for v in nx_graph:
      if v.label is None:
        v.col = 0.7
      elif v.label == 0:
        v.col = cols[0]
      elif v.label == 1:
        v.col = cols[2]

    if all([v.source is not None for v in nx_graph]):
      nxlabels = {v: v.source for v in nx_graph}
    else:
      nxlabels = None

    fig = plt.figure(figsize=(12, 9))
    im_vertices = nx.draw_networkx_nodes(nx_graph, with_labels=False,
                                         node_color=[v.col for v in nx_graph],
                                         **kwargs)

    im_edges = nx.draw_networkx_edges(nx_graph,
                                      vmin=e_vmin,
                                      vmax=e_vmax,
                                      edge_color=e_cols,
                                      edge_cmap=e_cmap,
                                      **kwargs)

    # Organize labels
    if all([v.source is not None for v in nx_graph]):
      nx.draw_networkx_labels(nx_graph,labels=nxlabels, **kwargs)

    if e_cmap:
      cb = plt.colorbar(im_edges)
      cmin, cmax = cb.get_clim()
      ticks = np.linspace(cmin, cmax, 7)
      cb.set_ticks(ticks)
      cb.set_ticklabels(['{:.3f}'.format(t) for t in ticks])
      cb.set_label("Edge Residual")

    plt.axis('off')
    plt.tight_layout()
    plt.title("Network Processing\nThickness shows number of connections")
    if saveas is not None:
      plt.savefig(saveas, bbox_inches='tight')
    else:
      plt.show()
    return kwargs['pos']

  def merge_dict(self, estimate_fullies=True):
    """ Make a dict of Miller indices with  ([list of intensities], resolution)
    value tuples for each miller index. Use the fully-corrected equivalent.

    :param estimate_fullies: Boolean for if the partialities and scales should be used.
    """
    intensity_miller = {}
    for vertex in self.members:
      if vertex.edges:
        miller_array = vertex.miller_array
        if estimate_fullies:
          miller_array = miller_array / (vertex.partialities
                                         * vertex.scales)
        #miller_array = miller_array.map_to_asu()
        d_spacings = list(miller_array.d_spacings().data())
        miller_indeces = list(miller_array.indices())
        miller_intensities = list(miller_array.data())

        for observation in zip(miller_indeces, miller_intensities, d_spacings):
          try:
            intensity_miller[observation[0]][0].append(observation[1])
          except KeyError:
            intensity_miller[observation[0]] = ([observation[1]],
                                                observation[2])
    return intensity_miller

  def cc_half(self, nbins=10, estimate_fullies=True):
    """
    Calculate the correlation coefficients for the vertices of the graph.

    :param nbins: number of bins to use for binned correlations.
    :param estimate_fullies: if True, use current orientation, spot profile and scale to estimate the 'full' intensity of each reflection.

    :return cc_half, p_value: overall CC1/2 for all reflections in the cluster.
    :return pretty_string: string with the CC by bin. Lowest resolution bin is extended if data cannot be split into n_bins evenly.
    """
    from scipy.stats import pearsonr
    import operator

    intensity_miller = self.merge_dict(estimate_fullies=estimate_fullies)

    # Remove miller indices that are only measured once.
    multiple_millers = [values for values in intensity_miller.values()
                        if len(values[0]) > 1]

    # Sort, since we will be binning later
    multiple_millers.sort(key=lambda x: x[1])

    # Avoid corner case where number of millers goes exactly into the bins
    if len(multiple_millers) % nbins == 0:
      nbins += 1

    # Figure out bin sizes:
    bin_size = int(len(multiple_millers) / nbins)
    bin_edges = [multiple_millers[i][1] for i in range(0, len(multiple_millers),
                                                       bin_size)]

    # Extend the last bin does not cover the whole resolution range
    bin_edges[-1] = multiple_millers[-1][1]

    assert bin_edges[0] == multiple_millers[0][1]
    assert bin_edges[-1] == multiple_millers[-1][1]
    assert len(bin_edges) == nbins + 1

    # For bins of size bin_size, split each miller index into two, and add half
    # the observations to first_half and half to second_half
    first_half = [[] for _ in range(nbins)]
    second_half = [[] for _ in range(nbins)]
    for indx, intensities in enumerate(multiple_millers):
      # Extending the last bin if necesarry
      bin_id = indx // bin_size if indx // bin_size < nbins else nbins - 1
      obs = intensities[0]
      random.shuffle(obs)
      middle = len(obs) // 2
      first_half[bin_id].append(np.mean(obs[:middle]))
      second_half[bin_id].append(np.mean(obs[middle:]))

    # Calculate the CC for the two halves of each resolution bin.
    logging.info("Calculating CC1/2 by resolution bin. "
                 "{} miller indices per bin".format(bin_size))
    pretty_string = ''
    for bin_id in reversed(range(nbins)):
      bin_cc, bin_p = pearsonr(first_half[bin_id], second_half[bin_id])
      bin_str = "{:6.3f} - {:6.3f}: {:4.3f}".format(bin_edges[bin_id + 1],
                                                    bin_edges[bin_id],
                                                    bin_cc)
      logging.info(bin_str)
      pretty_string += bin_str + "\n"

    # Combine all the split millers, and take the CC of everything
    first_half_all = reduce(operator.add, first_half)
    second_half_all = reduce(operator.add, second_half)
    cc_half, p_value = pearsonr(first_half_all, second_half_all)
    logging.info("CC 1/2 calculated: {:4.3f}, p-value: {}".format(cc_half,
                                                                  p_value))

    return cc_half, p_value, pretty_string


  def k_means_edges(self):
    """
    Preforms k-means clustering on the edge residuals, updating the Edge.intra attribute depending on if it is in the 1st (edge.intra = True) or 2nd (edge.intra=False) cluster.
    """
    from scipy.cluster.vq import kmeans2

    # 0. Make an array of edge residuals
    edge_residuals = np.array([e.mean_residual() for e in self.edges])

    # 1. split into in-cluster and between-cluster using k-means
    labels = kmeans2(edge_residuals, 2)

    # 2. find which cluster corresponds to intra-group edges (ie. is smallest)
    if labels[0][0] < labels[0][1]:
      intra = 0
    else:
      intra = 1

    # 2. assign these labels to the edges
    for edge, group in zip(self.edges, labels[1]):
      if group == intra:
        edge.intra = True
      else:
        edge.intra = False

    logging.info(('initial edge assignment complete: {} intra edges, '
                  '{} inter edges').format(
                                  sum([e.intra == True for e in self.edges]),
                                  sum([e.intra == False for e in self.edges])))


  def label_vertices(self, max_iter=100):
    """
    Labels the vertices in the graph as members of cluster 0 or 1.
    Currently just does a BFS to get through the graph, and updates each node
    the first time it comes across is.

    Next step would be to take a majority vote for each vertex, update, and
    repeat this process until convergence.

    :param max_iter: Maximum number of rounds of 'majority vote' updates for
    vertex labeling.

    """
    from collections import deque

    if not all([e.intra is not None for e in self.edges]):
       logging.warning("k_means_edges has not been run -- edges are not \
           labled. Running this now.")
       self.k_means_edges()
    assert all([e.intra is not None for e in self.edges])

    def majority_vote(vertex):
      """ Reasign the vertex id based on the nearest neighbours.
      """
      from scipy.stats import mode
      votes = []
      for e in vertex.edges:
        other_class = e.other_vertex(vertex).label
        if e.intra:
          votes.append(other_class)
        else:
          if other_class == 0:
            votes.append(1)
          elif vertex.label == 1:
            votes.append(0)

      new_label, frequency = mode(votes)
      if new_label != vertex.label:
        logging.debug("updating vertex to {}, with {}/{} votes" \
                              .format(new_label[0], frequency[0], len(votes)))
        vertex.label = int(new_label)

    # 0. Get the most connected node.
    most_conn = max(self.members, key=lambda x: len(x.edges))
    most_conn.label = 0
    q = deque([most_conn])

    # 1. BFS, calling the near_edges each time a node is popped.
    # ToDo: when we move to a mixed model for edges, use a priority queue to
    # pick the best edge to move along.
    for v in self.members:
      v.visited = False
    curr_node = most_conn
    curr_node.visited = True
    while q:
      curr_node = q.popleft()  # Take a node of the stack
      curr_node.visited = True
      """ Take a vertex, and updates the labels of all the
      vertices around it using the argument vertex's edge labels. If a vertex
      at the other end of the edge already has a label, skip it.
      """
      assert curr_node.label is not None, "Vertex must already have a label."
      for e in curr_node.edges:
        other_v = e.other_vertex(curr_node)
        # Is the curr_node on the other end already assigned? if yes, skip
        if other_v.label is not None:
          continue
        if e.intra:
          other_v.label = curr_node.label
        elif not e.intra:
          if curr_node.label == 0:
            other_v.label = 1
          elif curr_node.label == 1:
            other_v.label = 0
      # add this node's connections to the stack
      q.extend([e.other_vertex(curr_node) for e in curr_node.edges
                if not e.other_vertex(curr_node).visited])

    logging.info(('initial vertex assignment complete: {} vertices in group 0, '
                  '{} in group 1').format(
                                    sum([v.label == 0 for v in self.members]),
                                    sum([v.label == 1 for v in self.members])))

    if logging.Logger.root.level <= logging.DEBUG:  # Extreme debug!
      self.make_nx_graph()

    # 2. Majority vote
    original_labels = [v.label for v in self.members]
    current_labels = [v.label for v in self.members]
    v_iter = 0
    while v_iter <= max_iter:
      v_iter += 1
      for v in self.members:
        majority_vote(v)
      new_labels = [v.label for v in self.members]
      if current_labels == new_labels:
        logging.info("Majority vote converged after {} iterations. {} "
                     "vertices out of {} have been changed." \
                         .format(v_iter, sum([new_labels[i] != original_labels[i]
                                              for i in range(len(new_labels))]),
                               len(new_labels)))
        break
      if iter == max_iter:
        logging.info("Majority voting did no make vertex labels "
                     "converge. final state: {}/{} different" \
                     .format(sum([new_labels[i] != original_labels[i]
                                  for i in range(len(new_labels))]),
                             len(new_labels)))

      current_labels = new_labels


 *******************************************************************************


 *******************************************************************************
xfel/graph_proc/multiproc_temp_parame_opt.py
""" Tools for optimizing the parameters of the images in a graph to minimize the overall residual.
**Author:**   Oliver Zeldin <zeldin@stanford.edu>
"""
from __future__ import absolute_import, division, print_function
__author__ = 'zeldin'
import logging
import numpy as np
from scipy.optimize import fmin_l_bfgs_b as lbfgs

def loss_func(x):
  """ return the contribution to the loss function of residual x. Placeholder for rebust methods to come """
  return x**2


def total_score(params, *args):
  """ This is the function to be minimized by graph processing.

  It returns the total residual given params, which is a tuple containing the parameters to refine for one image. Formated for use with the scipy minimization routines. There are thus between 5 and 12,

  :param params: a list of the parameters for the minimisation as a tuple, only including vertices that have edges. See ImageNode.get_x0() for details of parameters.
  :param *args:  a 2-tuple of the vertex to be minimised, cross_val. cross_val is a list of edge to leave out of the optimisation for cross-validation purposes.

  :return: the total squared residual of the graph minimisation
  """
  from xfel.graph_proc.components import Edge
  vertex = args[0]
  cross_val = args[1]


  assert vertex.partialities != vertex.calc_partiality(params), "params have not changed the partialities"
  pa = vertex.calc_partiality(params)
  sa = vertex.calc_scales(params)

  # 2. Calculate all new residuals
  total_sum = 0
  for edge in vertex.edges:
    if edge not in cross_val:
      other_v = edge.other_vertex(vertex)
      residuals = Edge._calc_residuals(vertex, other_v,
                  pa, other_v.partialities,
                  sa, other_v.scales)
      total_sum += np.sum([loss_func(res) for res in residuals])


  logging.debug("Total Score: {}".format(total_sum))
  return total_sum


def _calc_residuals(work_edges, test_edges):
  """ Internal function to quickly calculate the sum of residuals for two lists of edges: work edges, and test edges. """

  work_residual = 0
  test_residual = 0
  # Calculate total graph residual
  for e in work_edges:
      for r in e.residuals():
        work_residual += loss_func(r)

  for e in test_edges:
      for r in e.residuals():
        test_residual += loss_func(r)

  return work_residual, test_residual


def multiproc_wrapper(stuff):
  """ Trivial wrapper for python multiprocessing """
  args, kwargs = stuff
  #print str(args)  + "\n" +  str(kwargs) + "\n----\n"
  params, _, result = lbfgs(*args, **kwargs)
  if result['warnflag'] != 0:
    logging.warning('lbfgs failed')
  return kwargs['args'][0], params


def global_minimise(graph, nsteps=10, eta=10, cross_val=[None], nproc=None):
  """
  Perform a global minimisation on the total squared residuals on all the edges, as calculated by Edge.residuals(). Uses the L-BFGS algorithm.

  Done by repetedly locally optimizing each node, and repeating this until convergence.

  :param graph: the graph object to minimize.
  :param nsteps: max number of iterations.
  :param cross_val: a list of edge to leave out of the optimisation for cross-validation purposes.

  :return: (end_residual, starting_residual) for the overall graph
  """
  if nproc != 1:
    from multiprocessing import Pool
    p = Pool(nproc)

  # make test/work set
  work_edges = []
  test_edges = []
  for e in graph.edges:
    if e in cross_val:
      test_edges.append(e)
    else:
      work_edges.append(e)

  init_work_residual, init_test_residual = _calc_residuals(work_edges,
                                                           test_edges)
  logging.info("Starting work/test residual is {}/{}".format(init_work_residual,
                                                            init_test_residual))

  old_residual = float("inf")
  new_residual = init_work_residual
  n_int = 0
  work_residuals = [init_work_residual]
  test_residuals = [init_test_residual]
  param_history = [[v.params for v in graph.members]]
  while abs(old_residual - new_residual) > eta and n_int < nsteps:
    all_args = []
    if nproc != 1:
      for v in graph.members:
        all_args.append(((total_score, list(v.params)),  # args
                         {'approx_grad':True, 'args':[v, cross_val], 'm': 10,
                         'factr':1e7, 'epsilon':1e-8, 'iprint':0}))  # kwargs
      new_params = p.map(multiproc_wrapper, all_args)
    else:
      new_params = []
      for v in graph.members:
        new_params.append(multiproc_wrapper((total_score, list(v.params)),  # args
                         {'approx_grad':True, 'args':[v, cross_val], 'm': 10,
                         'factr':1e7, 'epsilon':1e-8, 'iprint':0}))  # kwargs


    # Update scales and partialities for each node that has edges.
    for v, final_params in new_params:
      v.params = final_params
      v.partialties = v.calc_partiality(final_params)
      v.scales = v.calc_scales(final_params)
      v.G = final_params[0]
      v.B = final_params[1]
      if any(v.partialities == v.calc_partiality(v.get_x0())):
        pchange = False
      else:
        pchange = True
      if any(v.scales == v.calc_scales(v.get_x0())):
        schange = False
      else:
        schange = True
      logging.info("partialityi/scales changed: {}/{}".format(pchange,schange))
    work_residual, test_residual = _calc_residuals(work_edges,
                                                   test_edges)
    work_residuals.append(work_residual)
    test_residuals.append(test_residual)
    n_int += 1
    logging.info("Iteration {}: work/test residual is {}/{}".format(n_int,
                                                                  work_residual,
                                                                  test_residual))
    if n_int == nsteps:
      logging.warning("Reached max iterations")

    old_residual = new_residual
    new_residual = work_residual

  p.terminate()
  return work_residuals, test_residuals, param_history


 *******************************************************************************


 *******************************************************************************
xfel/graph_proc/parameter_optimizer.py
""" Tools for optimizing the parameters of the images in a graph to minimize the overall residual.
**Author:**   Oliver Zeldin <zeldin@stanford.edu>
"""
from __future__ import absolute_import, division, print_function
__author__ = 'zeldin'
import logging
import numpy as np
from scipy.optimize import fmin_l_bfgs_b as lbfgs

def loss_func(x):
  """ return the contribution to the loss function of residual x. Placeholder for rebust methods to come """
  return x**2


def total_score(params, *args):
  """ This is the function to be minimized by graph processing.

  It returns the total residual given params, which is a tuple containing the parameters to refine for one image. Formated for use with the scipy minimization routines. There are thus between 5 and 12,

  :param params: a list of the parameters for the minimisation as a tuple, only including vertices that have edges. See ImageNode.get_x0() for details of parameters.
  :param *args:  a 2-tuple of the vertex to be minimised, cross_val. cross_val is a list of edge to leave out of the optimisation for cross-validation purposes.

  :return: the total squared residual of the graph minimisation
  """
  from xfel.graph_proc.components import Edge
  vertex = args[0]
  cross_val = args[1]


  assert vertex.partialities != vertex.calc_partiality(params), "params have not changed the partialities"
  pa = vertex.calc_partiality(params, update_wilson=False)
  sa = vertex.calc_scales(params)

  # 2. Calculate all new residuals
  total_sum = 0
  for edge in vertex.edges:
    if edge not in cross_val:
      other_v = edge.other_vertex(vertex)
      residuals = Edge._calc_residuals(vertex, other_v,
                                       pa, other_v.partialities,
                                       sa, other_v.scales)
      total_sum += np.sum([loss_func(res) for res in residuals])


  logging.debug("Total Score: {}".format(total_sum))
  return total_sum


def _calc_residuals(work_edges, test_edges):
  """ Internal function to quickly calculate the sum of residuals for two lists of edges: work edges, and test edges. """

  work_residual = 0
  test_residual = 0
  # Calculate total graph residual
  for e in work_edges:
      for r in e.residuals():
        work_residual += loss_func(r)

  for e in test_edges:
      for r in e.residuals():
        test_residual += loss_func(r)

  return work_residual, test_residual


def multiproc_wrapper(stuff):
  """ Trivial wrapper for python multiprocessing """
  args, kwargs = stuff
  #print str(args)  + "\n" +  str(kwargs) + "\n----\n"
  return lbfgs(*args, **kwargs)
  #params, _, result = lbfgs(*args, **kwargs)
  #if result['warnflag'] != 0:
  #  logging.warning('lbfgs failed')
  #return kwargs['args'][0], params


def global_minimise(graph, nsteps=10, eta=10, cross_val=[None], nproc=None):
  """
  Perform a global minimisation on the total squared residuals on all the edges, as calculated by Edge.residuals(). Uses the L-BFGS algorithm.

  Done by repetedly locally optimizing each node, and repeating this until convergence.

  :param graph: the graph object to minimize.
  :param nsteps: max number of iterations.
  :param cross_val: a list of edge to leave out of the optimisation for cross-validation purposes.

  :return: (end_residual, starting_residual) for the overall graph
  """

  # make test/work set
  work_edges = []
  test_edges = []
  for e in graph.edges:
    if e in cross_val:
      test_edges.append(e)
    else:
      work_edges.append(e)

  init_work_residual, init_test_residual = _calc_residuals(work_edges,
                                                           test_edges)
  logging.info("Starting work/test residual is {}/{}".format(init_work_residual,
                                                            init_test_residual))

  old_residual = float("inf")
  new_residual = init_work_residual
  n_int = 0
  work_residuals = [init_work_residual]
  test_residuals = [init_test_residual]
  param_history = [[v.params for v in graph.members]]
  while abs(old_residual - new_residual) > eta and n_int < nsteps:
    new_params = []
    for v in graph.members:

      all_args = ((total_score, list(v.params)),  # args
                  {'approx_grad':True, 'args':[v, cross_val],
                   'epsilon': 1e-8, 'factr': 10**12, 'iprint': 0})
      final_params, min_total_res, info_dict = multiproc_wrapper(all_args)

      new_params.append((v, final_params))

    # Update scales and partialities for each node that has edges.
    for v, final_params in new_params:
      v.params = final_params
      v.partialities = v.calc_partiality(final_params)
      v.scales = v.calc_scales(final_params)
      v.G = final_params[0]
      v.B = final_params[1]
      if any(v.partialities.as_numpy_array() \
             == v.calc_partiality(v.get_x0()).as_numpy_array()):
        pchange = False
      else:
        pchange = True
      if any(v.scales.as_numpy_array() \
             == v.calc_scales(v.get_x0()).as_numpy_array()):
        schange = False
      else:
        schange = True
      if not schange or not pchange:
        logging.info("partialityi/scales changed: {}/{}".format(pchange,schange))

    param_history.append([v.params for v in graph.members])
    work_residual, test_residual = _calc_residuals(work_edges,
                                                   test_edges)
    work_residuals.append(work_residual)
    test_residuals.append(test_residual)
    n_int += 1
    logging.info("Iteration {}: work/test residual is {}/{}".format(n_int,
                                                                 work_residual,
                                                                 test_residual))
    if n_int == nsteps:
      logging.warning("Reached max iterations")

    old_residual = new_residual
    new_residual = work_residual

  return work_residuals, test_residuals, param_history


 *******************************************************************************
