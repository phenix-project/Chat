

 *******************************************************************************
xfel/merging/__init__.py
from __future__ import absolute_import, division, print_function
import boost_adaptbx.boost.python as bp
ext = bp.import_ext("sx_merging_ext")
from sx_merging_ext import *


 *******************************************************************************


 *******************************************************************************
xfel/merging/absorption.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from cctbx.array_family import flex

class show_observations:
  def __init__(self,obs,unobstructed,params,out=None, n_bins=12):
    if out==None:
      import sys
      out = sys.stdout
    from libtbx.str_utils import format_value
    self.params = params
    self.out = out
    self.obs = obs

    obs.setup_binner(n_bins = n_bins)
    attenuated = obs.select(~unobstructed)
    unattenuated = obs.select(unobstructed)

    attenuated.use_binning_of(obs)
    unattenuated.use_binning_of(obs)

    self.result = []
    counts_given = obs.binner().counts_given()
    counts_complete = obs.binner().counts_complete()

    counts_given_attenuated = attenuated.binner().counts_given()
    counts_given_unattenuated = unattenuated.binner().counts_given()

    for i_bin in obs.binner().range_used():
      sel_w = obs.binner().selection(i_bin)
      sel_fo_all = obs.select(sel_w)
      d_max_,d_min_ = sel_fo_all.d_max_min()
      d_range = obs.binner().bin_legend(
        i_bin=i_bin, show_bin_number=False, show_counts=True)
      sel_data = obs.select(sel_w).data()
      sel_sig = obs.select(sel_w).sigmas()

      sel_unatten_w = unattenuated.binner().selection(i_bin)
      sel_unatten_data = unattenuated.select(sel_unatten_w).data()
      sel_unatten_sig = unattenuated.select(sel_unatten_w).sigmas()

      sel_atten_w = attenuated.binner().selection(i_bin)
      sel_atten_data = attenuated.select(sel_atten_w).data()
      sel_atten_sig = attenuated.select(sel_atten_w).sigmas()

      if len(sel_unatten_data)>0:
        unatten_mean_I = flex.mean(sel_unatten_data)
        unatten_mean_I_sigI  = flex.mean(sel_unatten_data/sel_unatten_sig)
      else:
        unatten_mean_I = 0
        unatten_mean_I_sigI  = 0

      if len(sel_atten_data)>0:
        atten_mean_I = flex.mean(sel_atten_data)
        atten_mean_I_sigI  = flex.mean(sel_atten_data/sel_atten_sig)
      else:
        atten_mean_I = 0
        atten_mean_I_sigI  = 0

      if(sel_data.size() > 0):
        bin = resolution_bin(
          i_bin        = i_bin,
          d_range      = d_range,
          mean_I       = flex.mean(sel_data),
          n_work       = sel_data.size(),
          mean_I_sigI  = flex.mean(sel_data/sel_sig),
          d_max_min    = (d_max_, d_min_),
          completeness = (counts_given[i_bin], counts_complete[i_bin]),
          given_unatten = counts_given_unattenuated[i_bin],
          unatten_mean_I       = unatten_mean_I,
          unatten_mean_I_sigI  = unatten_mean_I_sigI,
          given_atten  = counts_given_attenuated[i_bin],
          atten_mean_I       = atten_mean_I,
          atten_mean_I_sigI  = atten_mean_I_sigI,
        )
        self.result.append(bin)
    self.set_limits(unobstructed)
    print("\n Bin  Resolution Range  Compl.         <I>     <I/sig(I)> Unobstructed <I>     <I/sig(I)> Obstructed <I>     <I/sig(I)>", file=out)
    for bin in self.result:
      fmt = " %s %s    %s  %s%s   %s   %s  %s%s   %s  %s   %s%s"
      print(fmt%(
        format_value("%3d",   bin.i_bin),
        format_value("%-17s", bin.d_range),
        format_value("%8.1f", bin.mean_I),
        format_value("%8.2f", bin.mean_I_sigI),
        format_value("%1s",   getattr(bin,"limit"," ")),
        format_value("%6d",   bin.given_unatten),
        format_value("%8.1f", bin.unatten_mean_I),
        format_value("%8.2f", bin.unatten_mean_I_sigI),
        format_value("%1s",   getattr(bin,"unatten_limit"," ")),
        format_value("%6d",   bin.given_atten),
        format_value("%8.1f", bin.atten_mean_I),
        format_value("%8.2f", bin.atten_mean_I_sigI),
        format_value("%1s",   getattr(bin,"atten_limit"," ")),
        ), file=out)

  def set_limits(self, unobstructed):
    acceptable_resolution_bins = [
      bin.mean_I_sigI > self.params.significance_filter.sigma for bin in self.result]
    acceptable_nested_bin_sequences = [i for i in range(len(acceptable_resolution_bins))
                                       if False not in acceptable_resolution_bins[:i+1]]

    N_acceptable_bins = max(acceptable_nested_bin_sequences)
    self.result[N_acceptable_bins].limit="*"

    unatten_acceptable = N_acceptable_bins
    for x in range(N_acceptable_bins,len(self.result)):
      if self.result[x].unatten_mean_I_sigI > self.params.significance_filter.sigma:
        unatten_acceptable = x
      else:
        break

    self.result[unatten_acceptable].unatten_limit="*"

    atten_acceptable = N_acceptable_bins
    for x in range(N_acceptable_bins,1,-1):
      if self.result[x].atten_mean_I_sigI < self.params.significance_filter.sigma:
        atten_acceptable = x - 1

    self.result[atten_acceptable].atten_limit="*"

    # Now compute the appropriate selections
    unattenuated_res_limit = float(self.result[unatten_acceptable].d_range.split()[2])
    attenuated_res_limit = float(self.result[atten_acceptable].d_range.split()[2])
    print("New combination resolution filter at %7.2f and %7.2f"%(unattenuated_res_limit,
    attenuated_res_limit), file=self.out)

    unattenuated_res_selection = self.obs.resolution_filter_selection(
            d_min=unattenuated_res_limit)
    attenuated_res_selection = self.obs.resolution_filter_selection(
            d_min=attenuated_res_limit)

    self.master_selection = (unobstructed & unattenuated_res_selection) | (~unobstructed & attenuated_res_selection)


class resolution_bin(object):
  def __init__(self,
               **kwargs):
    for key in kwargs.keys():
      assert not hasattr(self.__dict__, key)
      self.__dict__[key] = kwargs[key]
    #similar funtionality: from libtbx import adopt_init_args
    # XXX not sure if this introduces a memory leak.  Keep an eye on it.


 *******************************************************************************


 *******************************************************************************
xfel/merging/algorithms/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/algorithms/error_model/__init__.py
from __future__ import absolute_import, division, print_function
import boost_adaptbx.boost.python as bp
ext = bp.import_ext("xfel_sdfac_refine_ext")
from xfel_sdfac_refine_ext import *


 *******************************************************************************


 *******************************************************************************
xfel/merging/algorithms/error_model/error_modeler_base.py
from __future__ import absolute_import, division, print_function

"""
Base class for error modeling. Used after merging to adjust the errors.
"""
class error_modeler_base(object):
  def __init__(self, scaler):
    self.scaler = scaler
    self.log = scaler.log

  def adjust_errors(self):
    raise NotImplementedError("Override!")


 *******************************************************************************


 *******************************************************************************
xfel/merging/algorithms/error_model/errors_from_residuals.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from scitbx.array_family import flex

from xfel.merging.algorithms.error_model.error_modeler_base import error_modeler_base

class errors_from_residuals(error_modeler_base):
  def adjust_errors(self):
    """
    Use the distribution of intensities in a given miller index to compute the error for each merged reflection
    """
    print("Computing error estimates from sample residuals", file=self.log)
    self.scaler.summed_weight= flex.double(self.scaler.n_refl, 0.)
    self.scaler.summed_wt_I  = flex.double(self.scaler.n_refl, 0.)

    for hkl_id in range(self.scaler.n_refl):
      hkl = self.scaler.miller_set.indices()[hkl_id]
      if hkl not in self.scaler.ISIGI: continue

      n = len(self.scaler.ISIGI[hkl])
      if n <= 1: continue
      x = flex.double([self.scaler.ISIGI[hkl][i][0] for i in range(n)])
      if self.scaler.params.raw_data.error_models.errors_from_sample_residuals.biased:
        m = flex.mean(x)
        variance = flex.sum((x-m)**2)/n
      else:
        variance = flex.mean_and_variance(x).unweighted_sample_variance() # flex.sum((x-m)**2)/(n-1)

      for i in range(n):
        Intensity = self.scaler.ISIGI[hkl][i][0] # scaled intensity
        self.scaler.summed_wt_I[hkl_id] += Intensity / variance
        self.scaler.summed_weight[hkl_id] += 1 / variance
    print("Done computing error estimates", file=self.log)

class errors_from_residuals_refltable(error_modeler_base):
  def adjust_errors(self):
    """
    Use the distribution of intensities in a given miller index to compute the error for each merged reflection
    """
    print("Computing error estimates from sample residuals", file=self.log)
    self.scaler.summed_weight= flex.double(self.scaler.n_refl, 0.)
    self.scaler.summed_wt_I  = flex.double(self.scaler.n_refl, 0.)

    # 3-pass variance
    print("Variance step 1 of 3...", file=self.log)
    hkl_sumI = flex.double(self.scaler.n_refl, 0)
    n_obs = flex.double(self.scaler.n_refl, 0)

    for i in range(len(self.scaler.ISIGI)):
      idx = self.scaler.ISIGI['miller_id'][i]
      hkl_sumI[idx] += self.scaler.ISIGI['scaled_intensity'][i]

      n_obs[idx] += 1

    hkl_mean = flex.double(self.scaler.n_refl, 0)
    sel = n_obs > 0
    hkl_mean.set_selected(sel, hkl_sumI.select(sel)/n_obs.select(sel))

    print("Variance step 2 of 3...", file=self.log)
    hkl_sumsq = flex.double(self.scaler.n_refl, 0)
    for i in range(len(self.scaler.ISIGI)):
      idx = self.scaler.ISIGI['miller_id'][i]
      hkl_sumsq[idx] += (self.scaler.ISIGI['scaled_intensity'][i] - hkl_mean[idx])**2

    print("Variance step 3 of 3...", file=self.log)
    for i in range(len(self.scaler.ISIGI)):
      idx = self.scaler.ISIGI['miller_id'][i]
      n = n_obs[idx]
      if n <= 1:
        continue
      Intensity = self.scaler.ISIGI['scaled_intensity'][i]
      variance = hkl_sumsq[idx] / (n-1)

      self.scaler.summed_wt_I[idx] += Intensity / variance
      self.scaler.summed_weight[idx] += 1 / variance
    print("Done computing error estimates", file=self.log)


 *******************************************************************************


 *******************************************************************************
xfel/merging/algorithms/error_model/reduced_chi_squared.py
from __future__ import division
from six.moves import range
from scitbx.array_family import flex

from xfel.merging.algorithms.error_model.error_modeler_base import error_modeler_base

class reduced_chi_squared(error_modeler_base):
  def compute(self):
    """
    Computes the reduced chi squared parameter. Can be used to
    correct for under-estimation of experimental errors.
    See
    https://en.wikipedia.org/wiki/Weighted_arithmetic_mean#Correcting_for_over-_or_under-dispersion
    """
    print >> self.log, "Computing reduced chi squared"
    self.scaler.reduced_chi_squared = flex.double(self.scaler.n_refl, 1.)

    for hkl_id in range(self.scaler.n_refl):
      hkl = self.scaler.miller_set.indices()[hkl_id]
      if hkl not in self.scaler.ISIGI: continue

      n = len(self.scaler.ISIGI[hkl])
      if n <= 1:
        continue

      i = self.scaler.summed_wt_I[hkl_id] / self.scaler.summed_weight[hkl_id]
      x = flex.double([self.scaler.ISIGI[hkl][i][0] for i in range(n)])
      v = (x / flex.double([self.scaler.ISIGI[hkl][i][1] for i in range(n)]))**2

      self.scaler.reduced_chi_squared[hkl_id] = 1/(n-1) * flex.sum((x-i)**2/v)
    sel = self.scaler.reduced_chi_squared > 0

    print >> self.log, "Done computing reduced chi squared", flex.mean(self.scaler.reduced_chi_squared.select(sel))

    if False:
      from matplotlib import pyplot as plt
      plt.hist(self.scaler.reduced_chi_squared.select(sel).as_numpy_array(),
        bins = 100, range=(0,10))
      plt.show()


 *******************************************************************************


 *******************************************************************************
xfel/merging/algorithms/error_model/sdfac_propagate.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from dials.array_family import flex
import math
from rstbx.symmetry.constraints.parameter_reduction \
    import symmetrize_reduce_enlarge
from scitbx.matrix import sqr, col

from xfel.merging.algorithms.error_model.error_modeler_base import error_modeler_base
from xfel.merging.algorithms.error_model.sdfac_refine_lbfgs import finite_difference

from libtbx import group_args
from six.moves import zip

"""
Classes to support propagating erros after postrefinement in cxi.merge
"""

def r2d(radians):
  return 180*radians/math.pi

# Bucket to hold refinable error terms
class error_terms(group_args):
  @staticmethod
  def from_x(x):
    return error_terms(sigma_thetax  = x[0],
                       sigma_thetay  = x[1],
                       sigma_lambda  = x[2],
                       sigma_deff    = x[3],
                       sigma_gstar   = x[4:])

  def to_x(self):
    x = flex.double([self.sigma_thetax,
                     self.sigma_thetay,
                     self.sigma_lambda,
                     self.sigma_deff])
    x.extend(self.sigma_gstar)
    return x

class sdfac_propagate(error_modeler_base):
  def __init__(self, scaler, error_terms = None, verbose = True):
    super(sdfac_propagate, self).__init__(scaler)

    # Note, since the rs algorithm doesn't explicitly refine eta and deff separately, but insteads refines RS,
    # assume rs only incorporates information from deff and set eta to zero.
    ct = scaler.crystal_table
    ct['deff'] = 1/ct['RS']
    ct['eta'] = flex.double(len(ct), 0)

    self.error_terms = error_terms
    self.verbose = verbose

  def finite_difference(self, parameter_name, table, DELTA = 1.E-7):
    """ Compute finite difference given a parameter name """
    refls = self.scaler.ISIGI

    def target():
      r = self.compute_intensity_parameters()
      return refls['iobs'] / r['D']

    functional = target()

    if parameter_name.startswith('c'):
      # Handle the crystal parameters
      from scitbx.matrix import sqr
      cryst_param = int(parameter_name.lstrip('c'))
      parameter_name = 'b_matrix'
      current = table[parameter_name]*1 # make a copy

      sre = symmetrize_reduce_enlarge(self.scaler.params.target_space_group.group())
      for i in range(len(table)):
        sre.set_orientation(orientation=table['b_matrix'][i])
        vals = list(sre.forward_independent_parameters())
        vals[cryst_param] += DELTA
        newB = sqr(sre.backward_orientation(vals).reciprocal_matrix())
        table['b_matrix'][i] = newB
    else:
      current = table[parameter_name]
      table[parameter_name] = current + DELTA

    dfunctional = target()
    #calculate by finite_difference
    finite_g = (dfunctional-functional )/DELTA

    table[parameter_name] = current

    return finite_g

  def compute_intensity_parameters(self):
    """ Create a new reflection table with all the derived parameters needed
    to apply corrections from RS postrefinement """
    refls = self.scaler.ISIGI
    ct = self.scaler.crystal_table

    rx = flex.mat3_double() # crystal rotation around x
    ry = flex.mat3_double() # crystal rotation around y
    u = flex.mat3_double()  # U matrix (orientation)
    b = flex.mat3_double()  # B matrix (cell parameters)
    wavelength = flex.double()
    G = flex.double()       # scaling gfactor
    B = flex.double()       # wilson B factor
    s0 = flex.vec3_double() # beam vector
    deff = flex.double()    # effective domain size
    eta = flex.double()     # effective mosaic domain misorientation angle

    ex = col((1,0,0))       # crystal rotation x axis
    ey = col((0,1,0))       # crystal rotation y axis

    for i in range(len(ct)):
      # Need to copy crystal specific terms for each reflection. Equivalent to a JOIN in SQL.
      n_refl = ct['n_refl'][i]
      rx.extend(flex.mat3_double(n_refl, ex.axis_and_angle_as_r3_rotation_matrix(ct['thetax'][i])))
      ry.extend(flex.mat3_double(n_refl, ey.axis_and_angle_as_r3_rotation_matrix(ct['thetay'][i])))
      u.extend(flex.mat3_double(n_refl, ct['u_matrix'][i]))
      b.extend(flex.mat3_double(n_refl, ct['b_matrix'][i]))
      wavelength.extend(flex.double(n_refl, ct['wavelength'][i]))
      G.extend(flex.double(n_refl, ct['G'][i]))
      B.extend(flex.double(n_refl, ct['B'][i]))
      s0.extend(flex.vec3_double(n_refl, (0,0,-1)) * (1/ct['wavelength'][i]))
      deff.extend(flex.double(n_refl, ct['deff'][i]))
      eta.extend(flex.double(n_refl, ct['eta'][i]))

    h          = refls['miller_index_original'].as_vec3_double()
    q          = ry * rx * u * b * h                  # vector pointing from origin of reciprocal space to RLP
    qlen       = q.norms()                            # length of q
    d          = 1/q.norms()                          # resolution
    #rs         = (1/deff)+(eta/(2*d))                # proper formulation of RS
    rs         = 1/deff                               # assumes eta is zero
    rs_sq      = rs*rs                                # square of rs
    s          = (s0+q)                               # vector from center of Ewald sphere to RLP
    slen       = s.norms()                            # length of s
    rh         = slen-(1/wavelength)                  # distance from RLP to Ewald sphere
    p_n        = rs_sq                                # numerator of partiality lorenzian expression
    p_d        = (2. * (rh * rh)) + rs_sq             # denominator of partiality lorenzian expression
    partiality = p_n/p_d
    theta      = flex.asin(wavelength/(2*d))
    epsilon    = -8*B*(flex.sin(theta)/wavelength)**2 # exponential term in partiality
    eepsilon   = flex.exp(epsilon)                    # e^epsilon
    D          = partiality * G * eepsilon            # denominator of partiality lorenzian expression
    thetah     = flex.asin(wavelength/(2*d))          # reflecting angle
    sinthetah  = flex.sin(thetah)
    er         = sinthetah/wavelength                 # ratio term in epsilon

    # save all the columns
    r = flex.reflection_table()
    r['rx']         = rx
    r['ry']         = ry
    r['u']          = u
    r['b']          = b
    r['h']          = h
    r['q']          = q
    r['qlen']       = qlen
    r['D']          = D
    r['rs']         = rs
    r['eta']        = eta
    r['deff']       = deff
    r['d']          = d
    r['s']          = s
    r['slen']       = slen
    r['wavelength'] = wavelength
    r['p_n']        = p_n
    r['p_d']        = p_d
    r['partiality'] = partiality
    r['G']          = G
    r['B']          = B
    r['eepsilon']   = eepsilon
    r['thetah']     = thetah
    r['sinthetah']  = sinthetah
    r['er']         = er

    return r

  def initial_estimates(self):
    # Compute errors by examining distributions of parameters
    ct = self.scaler.crystal_table
    stats_thetax = flex.mean_and_variance(ct['thetax'])
    stats_thetay = flex.mean_and_variance(ct['thetay'])
    stats_lambda = flex.mean_and_variance(ct['wavelength'])
    stats_deff   = flex.mean_and_variance(ct['deff'])
    stats_rs     = flex.mean_and_variance(ct['RS'])
    sigma_thetax = stats_thetax.unweighted_sample_standard_deviation()
    sigma_thetay = stats_thetay.unweighted_sample_standard_deviation()
    sigma_lambda = stats_lambda.unweighted_sample_standard_deviation()
    sigma_deff   = stats_deff.unweighted_standard_error_of_mean()
    sigma_rs     = stats_rs.unweighted_sample_standard_deviation()
    print("ThetaX %.4f +/- %.4f"    %(r2d(stats_thetax.mean()), r2d(sigma_thetax)), file=self.log)
    print("Thetay %.4f +/- %.4f"    %(r2d(stats_thetay.mean()), r2d(sigma_thetay)), file=self.log)
    print("Wavelength %.4f +/- %.4f"%(    stats_lambda.mean(),      sigma_lambda), file=self.log)
    print("DEFF %.4f +/- %.4f"      %(    stats_deff.mean(),        sigma_deff), file=self.log)
    print("RS %.6f +/- %.6f"        %(    stats_rs.mean(),          sigma_rs), file=self.log)

    sre = symmetrize_reduce_enlarge(self.scaler.params.target_space_group.group())
    c_gstar_params = None

    for i in range(len(ct)):
      # Get the G* unit cell parameters from cctbx
      sre.set_orientation(orientation=ct['b_matrix'][i])
      p = sre.forward_independent_parameters()
      if c_gstar_params is None:
        c_gstar_params = [flex.double() for j in range(len(p))]

      for j in range(len(p)):
        c_gstar_params[j].append(p[j])

    # Compute the error in the unit cell terms from the distribution of unit cell parameters provided
    print("Free G* parameters", file=self.log)
    sigma_gstar = flex.double()
    for j in range(len(c_gstar_params)):
      stats  = flex.mean_and_variance(c_gstar_params[j])
      print("G* %d %.4f *1e-5 +/- %.4f *1e-5"%(j, stats.mean()*1e5, stats.unweighted_sample_standard_deviation()*1e5), file=self.log)
      sigma_gstar.append(stats.unweighted_sample_standard_deviation())

    self.error_terms = error_terms(sigma_thetax  = sigma_thetax,
                                   sigma_thetay  = sigma_thetay,
                                   sigma_lambda  = sigma_lambda,
                                   sigma_deff    = sigma_deff,
                                   sigma_gstar   = sigma_gstar)

  def dI_derrorterms(self):
    refls = self.scaler.ISIGI
    ct = self.scaler.crystal_table

    # notation: dP1_dP2 is derivative of parameter 1 with respect to parameter 2. Here,
    # for example, is the derivative of rx wrt thetax
    drx_dthetax = flex.mat3_double()
    dry_dthetay = flex.mat3_double()
    s0hat = flex.vec3_double(len(refls), (0,0,-1))

    ex = col((1,0,0))
    ey = col((0,1,0))

    # Compute derivatives
    sre = symmetrize_reduce_enlarge(self.scaler.params.target_space_group.group())
    gstar_params = None
    gstar_derivatives = None

    for i in range(len(ct)):
      n_refl = ct['n_refl'][i]

      # Derivatives of rx/y wrt thetax/y come from cctbx
      drx_dthetax.extend(flex.mat3_double(n_refl, ex.axis_and_angle_as_r3_derivative_wrt_angle(ct['thetax'][i])))
      dry_dthetay.extend(flex.mat3_double(n_refl, ey.axis_and_angle_as_r3_derivative_wrt_angle(ct['thetay'][i])))

      # Derivatives of the B matrix wrt to the unit cell parameters also come from cctbx
      sre.set_orientation(orientation=ct['b_matrix'][i])
      p = sre.forward_independent_parameters()
      dB_dp = sre.forward_gradients()
      if gstar_params is None:
        assert gstar_derivatives is None
        gstar_params = [flex.double() for j in range(len(p))]
        gstar_derivatives = [flex.mat3_double() for j in range(len(p))]
      assert len(p) == len(dB_dp) == len(gstar_params) == len(gstar_derivatives)
      for j in range(len(p)):
        gstar_params[j].extend(flex.double(n_refl, p[j]))
        gstar_derivatives[j].extend(flex.mat3_double(n_refl, tuple(dB_dp[j])))

    # Compute the scalar terms used while computing derivatives
    self.r = r = self.compute_intensity_parameters()

    # Begin computing derivatives
    sigma_Iobs = refls['scaled_intensity']/refls['isigi']
    dI_dIobs = 1/r['D']

    def compute_dI_dp(dq_dp):
      """ Deriviatives of the scaled intensity I wrt to thetax, thetay and the unit cell parameters
      are computed the same, starting with the deriviatives of those parameters wrt to q """
      dqlen_dp = r['q'].dot(dq_dp)/r['qlen']
      dd_dp    = -(1/(r['qlen']**2)) * dqlen_dp
      drs_dp   = -(r['eta']/(2 * r['d']**2)) * dd_dp
      dslen_dp = r['s'].dot(dq_dp)/r['slen']
      drhsq_dp = 2 * (r['slen'] - (1/r['wavelength'])) * dslen_dp
      dPn_dp   = 2 * r['rs'] * drs_dp
      dPd_dp   = 2 * ((r['rs'] * drs_dp) + drhsq_dp)
      dP_dp    = ((r['p_d'] * dPn_dp)-(r['p_n'] * dPd_dp))/(r['p_d']**2)
      dI_dp    = -(refls['iobs']/(r['partiality']**2 * r['G'] * r['eepsilon'])) * dP_dp
      return dI_dp

    # Derivatives wrt the unit cell parameters
    dI_dgstar = []
    for j in range(len(gstar_params)):
      dI_dgstar.append(compute_dI_dp(r['ry'] * r['rx'] * r['u'] * gstar_derivatives[j] * r['h']))

    # Derivatives wrt the crystal orientation
    dI_dthetax = compute_dI_dp(r['ry'] * drx_dthetax * r['u'] * r['b'] * r['h'])
    dI_dthetay = compute_dI_dp(dry_dthetay * r['rx'] * r['u'] * r['b'] * r['h'])

    # Derivatives wrt to the wavelength
    dthetah_dlambda  = 1/(flex.sqrt(1 - ((r['wavelength']/(2 * r['d']))**2)) * 2 * r['d'])
    den_dlambda      = flex.cos(r['thetah']) * dthetah_dlambda
    der_dlambda      = ((r['wavelength'] * den_dlambda) - r['sinthetah'])/r['wavelength']**2
    depsilon_dlambda = -16 * r['B'] * r['er'] * der_dlambda
    ds0_dlambda      = s0hat*(-1/r['wavelength']**2)
    dslen_dlambda    = r['s'].dot(ds0_dlambda)/r['slen']
    drhsq_dlambda    = 2*(r['slen']-(1/r['wavelength']))*(dslen_dlambda+(1/r['wavelength']**2))
    dP_dlambda       = -2*(r['p_n']/r['p_d']**2) * drhsq_dlambda
    dD_dlambda       = (r['G'] * r['eepsilon'] * dP_dlambda) + (r['partiality'] * r['G'] * r['eepsilon'] * depsilon_dlambda)
    dI_dlambda       = -(refls['iobs']/r['D']**2) * dD_dlambda

    # Derivatives wrt to the deff
    drs_deff = -1/(r['deff']**2)
    dPn_deff = 2 * r['rs'] * drs_deff
    dPd_deff = 2 * r['rs'] * drs_deff
    dP_deff  = ((r['p_d'] * dPn_deff)-(r['p_n'] * dPd_deff))/(r['p_d']**2)
    dI_deff  = -(refls['iobs']/(r['partiality']**2 * r['G'] * r['eepsilon'])) * dP_deff

    # Derivatives wrt to eta (unused for RS refinement)
    # drs_deta = 1/(2*r['d'])
    # dPn_deta = 2 * r['rs'] * drs_deta
    # dPd_deta = 2 * r['rs'] * drs_deta
    # dP_deta  = ((r['p_d']*dPn_deta)-(r['p_n']*dPd_deta))/(r['p_d']**2)
    # dI_deta  = -(refls['iobs']/(r['partiality']**2 * r['G'] * r['eepsilon'])) * dP_deta

    if self.verbose:
      # Show comparisons to finite differences
      n_cryst_params = sre.constraints.n_independent_params()
      print("Showing finite differences and derivatives for each parameter (first few reflections only)")
      for parameter_name, table, derivatives, delta, in zip(['iobs', 'thetax', 'thetay', 'wavelength', 'deff'] + ['c%d'%cp for cp in range(n_cryst_params)],
                                                    [refls, ct, ct, ct, ct] + [ct]*n_cryst_params,
                                                    [dI_dIobs, dI_dthetax, dI_dthetay, dI_dlambda, dI_deff] + dI_dgstar,
                                                    [1e-7]*5 + [1e-11]*n_cryst_params):
        finite_g = self.finite_difference(parameter_name, table, delta)
        print(parameter_name)
        for refl_id in range(min(10, len(refls))):
          print("%d % 21.1f % 21.1f"%(refl_id, finite_g[refl_id], derivatives[refl_id]))
        stats = flex.mean_and_variance(finite_g-derivatives)
        stats_finite = flex.mean_and_variance(finite_g)
        percent = 0 if stats_finite.mean() == 0 else 100*stats.mean()/stats_finite.mean()
        print("Mean difference between finite and analytical: % 24.4f +/- % 24.4f (%8.3f%% of finite d.)"%( \
            stats.mean(), stats.unweighted_sample_standard_deviation(), percent))
        print()

    return [dI_dIobs, dI_dthetax, dI_dthetay, dI_dlambda, dI_deff] + dI_dgstar

  def adjust_errors(self, dI_derrorterms = None, compute_sums = True):
    """ Propagate errors to the scaled and merged intensity errors based on statistical error propagation.
    This uses 1) and estimate of the errors in the post-refined parametes from the observed population
    and 2) partial derivatives of the scaled intensity with respect to each of the post-refined parameters.
    """
    assert self.scaler.params.postrefinement.algorithm == 'rs'

    refls = self.scaler.ISIGI
    ct = self.scaler.crystal_table

    if self.error_terms is None:
      self.initial_estimates()

    if dI_derrorterms is None:
      dI_derrorterms = self.dI_derrorterms()
    dI_dIobs, dI_dthetax, dI_dthetay, dI_dlambda, dI_deff = dI_derrorterms[0:5]
    dI_dgstar = dI_derrorterms[5:]
    sigma_Iobs = refls['scaled_intensity']/refls['isigi']
    r = self.r

    # Propagate errors
    refls['isigi'] = refls['scaled_intensity'] / \
                     flex.sqrt((sigma_Iobs**2 * dI_dIobs**2) +
                     sum([self.error_terms.sigma_gstar[j]**2 * dI_dgstar[j]**2 for j in range(len(self.error_terms.sigma_gstar))]) +
                     (self.error_terms.sigma_thetax**2 * dI_dthetax**2) +
                     (self.error_terms.sigma_thetay**2 * dI_dthetay**2) +
                     (self.error_terms.sigma_lambda**2 * dI_dlambda**2) +
                     (self.error_terms.sigma_deff**2 * dI_deff**2))
    if self.verbose:
      # Show results of propagation
      from scitbx.math import five_number_summary
      all_data = [(refls['iobs'], "Iobs"),
                  (sigma_Iobs, "Original errors"),
                  (1/r['D'], "Total scale factor"),
                  (refls['iobs']/r['D'], "Inflated intensities"),
                  (refls['scaled_intensity']/refls['isigi'], "Propagated errors"),
                  (flex.sqrt(sigma_Iobs**2 * dI_dIobs**2), "Iobs term"),
                  (flex.sqrt(self.error_terms.sigma_thetax**2 * dI_dthetax**2), "Thetax term"),
                  (flex.sqrt(self.error_terms.sigma_thetay**2 * dI_dthetay**2), "Thetay term"),
                  (flex.sqrt(self.error_terms.sigma_lambda**2 * dI_dlambda**2), "Wavelength term"),
                  (flex.sqrt(self.error_terms.sigma_deff**2 * dI_deff**2), "Deff term")] + \
                 [(flex.sqrt(self.error_terms.sigma_gstar[j]**2 * dI_dgstar[j]**2), "Gstar term %d"%j) for j in range(len(self.error_terms.sigma_gstar))]
      print("%20s % 20s % 20s % 20s"%("Data name","Quartile 1", "Median", "Quartile 3"), file=self.log)
      for data, title in all_data:
        fns = five_number_summary(data)
        print("%20s % 20d % 20d % 20d"%(title, fns[1], fns[2], fns[3]), file=self.log)

    if compute_sums:
      # Final terms for cxi.merge
      self.scaler.summed_weight= flex.double(self.scaler.n_refl, 0.)
      self.scaler.summed_wt_I  = flex.double(self.scaler.n_refl, 0.)

      Intensity = refls['scaled_intensity']
      sigma = Intensity / refls['isigi']
      variance = sigma * sigma

      for i in range(len(refls)):
        j = refls['miller_id'][i]
        self.scaler.summed_wt_I[j] += Intensity[i] / variance[i]
        self.scaler.summed_weight[j] += 1 / variance[i]


 *******************************************************************************


 *******************************************************************************
xfel/merging/algorithms/error_model/sdfac_propagate_and_refine.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from xfel.merging.algorithms.error_model.sdfac_refine_lbfgs import sdfac_refinery, sdfac_refine_refltable_lbfgs, lbfgs_minimizer
from xfel.merging.algorithms.error_model.sdfac_propagate import sdfac_propagate, error_terms, r2d
from scitbx.array_family import flex
import math

from xfel.cxi.postrefinement_legacy_rs import unpack_base
from six.moves import zip
class sdfac_propagate_parameterization(unpack_base):
  def __getattr__(YY,item):
    if item=="SDFAC" : return YY.reference[0]
    if item=="SDB"   : return YY.reference[1]
    if item=="SDADD" : return YY.reference[2]
    if item=="SDFACSQ" : return YY.reference[0]**2
    if item=="SDBSQ"   : return YY.reference[1]**2
    if item=="SDADDSQ" : return YY.reference[2]**2
    if item=="sigma_thetax" : return YY.reference[3]
    if item=="sigma_thetay" : return YY.reference[4]
    if item=="sigma_lambda" : return YY.reference[5]
    if item=="sigma_deff"   : return YY.reference[6]
    if item=="sigma_gstar"  : return YY.reference[7:]
    if item=="sd_terms"        : return YY.reference[0:3]
    if item=="propagate_terms" : return YY.reference[3:]
    raise AttributeError(item)

  def show(YY, out):
    print("sdfac: %20.10f, sdb: %20.10f, sdadd: %20.10f"%(YY.SDFAC, YY.SDB, YY.SDADD), file=out)
    for item in "sigma_thetax", "sigma_thetay", "sigma_lambda", "sigma_deff":
      if 'theta' in item:
        print("%s: %20.10f"%(item, r2d(getattr(YY, item))), end=' ', file=out)
      else:
        print("%s: %20.10f"%(item, getattr(YY, item)), end=' ', file=out)
    print(file=out)
    for v_id, v in enumerate(YY.sigma_gstar):
      print("sigma_gstar_%d: %20.10f * 1e-5"%(v_id, v*1e5), end=' ', file=out)
    print(file=out)

class sdfac_propagate_refinery(sdfac_refinery):
  def __init__(self, scaler, modeler, indices, bins, log):
    super(sdfac_propagate_refinery, self).__init__(scaler, modeler, indices, bins, log)
    self.propagator = sdfac_propagate(self.scaler, verbose=False)

    # Get derivatives from error propagation
    self.dI_derrorterms = self.propagator.dI_derrorterms()

  def fvec_callable(self, values):
    """ Compute the functional by first propagating errors from postrefinement and then
    applying the current values for the sd parameters to the input data, then computing
    the complete set of normalized deviations and finally using those normalized
    deviations to compute the functional."""

    # Restore original sigmas
    refls = self.ISIGI
    refls['isigi'] = refls['scaled_intensity']/refls['original_sigmas']

    # Propagate errors from postrefinement
    self.propagator.error_terms = error_terms.from_x(values.propagate_terms)
    self.propagator.adjust_errors(dI_derrorterms=self.dI_derrorterms, compute_sums=False)

    return super(sdfac_propagate_refinery, self).fvec_callable(values)

  def jacobian_callable(self, values):
    # Restore original sigmas
    refls = self.scaler.ISIGI
    refls['isigi'] = refls['scaled_intensity']/refls['original_sigmas']

    # Propagate errors from postrefinement
    self.propagator.error_terms = error_terms.from_x(values.propagate_terms)
    self.propagator.adjust_errors(dI_derrorterms=self.dI_derrorterms, compute_sums=False)

    all_sigmas_normalized, sigma_prime = self.get_normalized_sigmas(values)

    # et: error term
    df_derrorterms = []
    for et, dI_det in zip(values.propagate_terms, self.dI_derrorterms[1:]): # don't need dI wrt iobs
      dsigmasq_det = 2 * et
      dsigmasq_detsq = dI_det**2 * dsigmasq_det
      dsigprimesq_detsq = values.SDFACSQ * dsigmasq_detsq
      df_detsq = self.df_dpsq(all_sigmas_normalized, sigma_prime, dsigprimesq_detsq)
      df_derrorterms.append(df_detsq)

    sdfac_derivatives = super(sdfac_propagate_refinery, self).jacobian_callable(values)

    return sdfac_derivatives + df_derrorterms

class sdfac_propagate_and_refine(sdfac_refine_refltable_lbfgs):
  def __init__(self, scaler):
    sdfac_refine_refltable_lbfgs.__init__(self, scaler)
    self.parameterization = sdfac_propagate_parameterization

  def run_minimzer(self, values, sels, **kwargs):
    refinery = sdfac_propagate_refinery(self.scaler, self, self.scaler.miller_set.indices(), sels, self.log)
    return lbfgs_minimizer(values.reference, self.parameterization, refinery, self.log,
      show_finite_differences = self.scaler.params.raw_data.error_models.sdfac_refine.show_finite_differences)

  def adjust_errors(self):
    print("Starting adjust_errors", file=self.log)

    # Save original sigmas
    refls = self.scaler.ISIGI
    refls['original_sigmas'] = refls['scaled_intensity']/refls['isigi']

    print("Computing initial estimates of parameters", file=self.log)
    propagator = sdfac_propagate(self.scaler, verbose=False)
    propagator.initial_estimates()
    propagator.adjust_errors(compute_sums=False)
    init_params = flex.double(self.get_initial_sdparams_estimates())
    init_params.extend(propagator.error_terms.to_x())
    values = self.parameterization(init_params)

    print("Initial estimates:", end=' ', file=self.log)
    values.show(self.log)
    print("Refining error correction parameters", file=self.log)
    sels, binned_intensities = self.get_binned_intensities()
    minimizer = self.run_minimzer(values, sels)
    values = minimizer.get_refined_params()
    print("Final", end=' ', file=self.log)
    values.show(self.log)

    print("Applying sdfac/sdb/sdadd 1", file=self.log)
    # Restore original sigmas
    refls['isigi'] = refls['scaled_intensity']/refls['original_sigmas']

    # Propagate refined errors from postrefinement
    propagator.error_terms = error_terms.from_x(values.propagate_terms)
    propagator.adjust_errors()
    minimizer.apply_sd_error_params(self.scaler.ISIGI, values)

    self.scaler.summed_weight= flex.double(self.scaler.n_refl, 0.)
    self.scaler.summed_wt_I  = flex.double(self.scaler.n_refl, 0.)

    print("Applying sdfac/sdb/sdadd 2", file=self.log)
    for i in range(len(self.scaler.ISIGI)):
      hkl_id = self.scaler.ISIGI['miller_id'][i]
      Intensity = self.scaler.ISIGI['scaled_intensity'][i] # scaled intensity
      sigma = Intensity / self.scaler.ISIGI['isigi'][i] # corrected sigma
      variance = sigma * sigma
      self.scaler.summed_wt_I[hkl_id] += Intensity / variance
      self.scaler.summed_weight[hkl_id] += 1 / variance

    if self.scaler.params.raw_data.error_models.sdfac_refine.plot_refinement_steps:
      from matplotlib.pyplot import cm
      from matplotlib import pyplot as plt
      import numpy as np
      for i in range(2):
        f = plt.figure(i)
        lines = plt.gca().get_lines()
        color=cm.rainbow(np.linspace(0,1,len(lines)))
        for line, c in zip(reversed(lines), color):
          line.set_color(c)
      plt.ioff()
      plt.show()

    if False:
      # validate using http://ccp4wiki.org/~ccp4wiki/wiki/index.php?title=Symmetry%2C_Scale%2C_Merge#Analysis_of_Standard_Deviations
      print("Validating", file=self.log)
      from matplotlib import pyplot as plt
      all_sigmas_normalized = self.compute_normalized_deviations(self.scaler.ISIGI, self.scaler.miller_set.indices())
      plt.hist(all_sigmas_normalized, bins=100)
      plt.figure()

      binned_rms_normalized_sigmas = []

      for i, sel in enumerate(sels):
        binned_rms_normalized_sigmas.append(math.sqrt(flex.mean(all_sigmas_normalized.select(sel)*all_sigmas_normalized.select(sel))))

      plt.plot(binned_intensities, binned_rms_normalized_sigmas, 'o')
      plt.show()

      all_sigmas_normalized = all_sigmas_normalized.select(all_sigmas_normalized != 0)
      self.normal_probability_plot(all_sigmas_normalized, (-0.5, 0.5), plot = True)

class sdfac_propagate_and_refine_levmar(sdfac_propagate_and_refine):
  def run_minimzer(self, values, sels, **kwargs):
    from xfel.merging.algorithms.error_model.sdfac_refine_levmar import sdfac_helper
    from scitbx.lstbx import normal_eqns_solving

    self.refinery = sdfac_propagate_refinery(self.scaler, self, self.scaler.miller_set.indices(), sels, self.log)
    self.helper = sdfac_helper(current_x = values.reference,
                               parameterization = self.parameterization, refinery = self.refinery,
                               out = self.log )
    self.iterations = normal_eqns_solving.levenberg_marquardt_iterations(
      non_linear_ls = self.helper,
      track_all=True,
      gradient_threshold=1e-08,
      step_threshold=1e-08,
      tau=1e-08,
      n_max_iterations=200)
    return self

  def get_refined_params(self):
    return self.parameterization(self.helper.x)

  def apply_sd_error_params(self, data, values):
    self.refinery.apply_sd_error_params(data, values)


 *******************************************************************************


 *******************************************************************************
xfel/merging/algorithms/error_model/sdfac_refine.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from scitbx.array_family import flex
import math, sys

from xfel.merging.algorithms.error_model.error_modeler_base import error_modeler_base
from xfel.merging.algorithms.error_model import setup_isigi_stats

from xfel.cxi.postrefinement_legacy_rs import unpack_base
from six.moves import zip
class sdfac_parameterization(unpack_base):
  def __getattr__(YY,item):
    if item=="SDFAC" : return YY.reference[0]
    if item=="SDB"   : return YY.reference[1]
    if item=="SDADD" : return YY.reference[2]
    if item=="SDFACSQ" : return YY.reference[0]**2
    if item=="SDBSQ"   : return YY.reference[1]**2
    if item=="SDADDSQ" : return YY.reference[2]**2
    raise AttributeError(item)

  def show(YY, out):
    print("sdfac: %8.5f, sdb: %8.5f, sdadd: %8.5f"%(YY.SDFAC, YY.SDB, YY.SDADD), file=out)

from scitbx.simplex import simplex_opt
class simplex_minimizer(object):
  """Class for refining sdfac, sdb and sdadd"""
  def __init__(self, values, parameterization, data, indices, bins, seed = None, log=None):
    """
    @param values parameterization of the SD terms
    @param data ISIGI dictionary of unmerged intensities
    @param indices array of miller indices to refine against
    @param bins array of flex.bool object specifying the bins to use to calculate the functional
    @param log Log to print to (none for stdout)
    """
    if log is None:
      log = sys.stdout
    self.log = log
    self.data = data
    self.intensity_bin_selections = bins
    self.indices = indices
    self.parameterization = parameterization
    self.n = 3
    self.x = flex.double([values.SDFAC, values.SDB, values.SDADD])
    self.starting_simplex = []
    if seed is None:
      random_func = flex.random_double
    else:
      print("Using random seed %d"%seed, file=self.log)
      mt = flex.mersenne_twister(seed)
      random_func = mt.random_double

    for i in range(self.n+1):
      self.starting_simplex.append(random_func(self.n))

    self.optimizer = simplex_opt( dimension = self.n,
                                  matrix    = self.starting_simplex,
                                  evaluator = self,
                                  tolerance = 1e-1)
    self.x = self.optimizer.get_solution()

  def target(self, vector):
    """ Compute the functional by first applying the current values for the sd parameters
    to the input data, then computing the complete set of normalized deviations and finally
    using those normalized deviations to compute the functional."""
    values = self.parameterization(vector)

    if values.SDFAC < 0 or values.SDB < 0 or values.SDADD < 0:
      f = 1e6
    else:
      data = self.apply_sd_error_params(self.data, values)
      all_sigmas_normalized = self.compute_normalized_deviations(data, self.indices)

      f = 0
      for bin in self.intensity_bin_selections:
        binned_normalized_sigmas = all_sigmas_normalized.select(bin)
        n = len(binned_normalized_sigmas)
        if n == 0: continue
        # weighting scheme from Evans, 2011
        w = math.sqrt(n)
        # functional is weight * (1-rms(normalized_sigmas))^s summed over all intensitiy bins
        f += w * ((1-math.sqrt(flex.mean(binned_normalized_sigmas*binned_normalized_sigmas)))**2)

    print("f: % 12.1f,"%f, end=' ', file=self.log)
    values.show(self.log)
    return f

  def compute_normalized_deviations(self, data, indices):
    from xfel import compute_normalized_deviations
    return compute_normalized_deviations(data, indices)

  def apply_sd_error_params(self, data, values):
    from xfel import apply_sd_error_params
    return apply_sd_error_params(data, values.SDFAC, values.SDB, values.SDADD)

  def get_refined_params(self):
    return self.parameterization(self.x)

class sdfac_refine(error_modeler_base):
  def __init__(self, scaler):
    error_modeler_base.__init__(self, scaler)
    self.parameterization = sdfac_parameterization

  def get_overall_correlation_flex (self, data_a, data_b) :
    """
    Correlate any two sets of data.
    @param data_a data set a
    @param data_b data set b
    @return tuple containing correlation coefficent, slope and offset.
    """
    import math

    assert len(data_a) == len(data_b)
    corr = 0
    slope = 0
    offset = 0
    try:
      sum_xx = 0
      sum_xy = 0
      sum_yy = 0
      sum_x  = 0
      sum_y  = 0
      N      = 0
      for i in range(len(data_a)):
        I_r       = data_a[i]
        I_o       = data_b[i]
        N      += 1
        sum_xx += I_r**2
        sum_yy += I_o**2
        sum_xy += I_r * I_o
        sum_x  += I_r
        sum_y  += I_o
      slope = (N * sum_xy - sum_x * sum_y) / (N * sum_xx - sum_x**2)
      offset = (sum_xx * sum_y - sum_x * sum_xy) / (N * sum_xx - sum_x**2)
      corr  = (N * sum_xy - sum_x * sum_y) / (math.sqrt(N * sum_xx - sum_x**2) *
                 math.sqrt(N * sum_yy - sum_y**2))
    except ZeroDivisionError:
      pass

    return corr, slope, offset

  def normal_probability_plot(self, data, rankits_sel=None, plot=False):
    """ Use normal probability analysis to determine if a set of data is normally distributed
    See https://en.wikipedia.org/wiki/Normal_probability_plot.
    Rankits are computed in the same way as qqnorm does in R.
    @param data flex array
    @param rankits_sel only use the rankits in a certain range. Useful for outlier rejection. Should be
    a tuple such as (-0.5,0.5).
    @param plot whether to show the normal probabilty plot
    """
    from scitbx.math import distributions
    import numpy as np
    norm = distributions.normal_distribution()

    n = len(data)
    if n <= 10:
      a = 3/8
    else:
      a = 0.5

    sorted_data = flex.sorted(data)
    rankits = flex.double([norm.quantile((i+1-a)/(n+1-(2*a))) for i in range(n)])

    if rankits_sel is None:
      corr, slope, offset = self.get_overall_correlation_flex(sorted_data, rankits)
    else:
      sel = (rankits >= rankits_sel[0]) & (rankits <= rankits_sel[1])
      corr, slope, offset = self.get_overall_correlation_flex(sorted_data.select(sel), rankits.select(sel))

    if plot:
      from matplotlib import pyplot as plt
      f = plt.figure(0)
      lim = -5, 5
      x = np.linspace(lim[0],lim[1],100) # 100 linearly spaced numbers
      y = slope * x + offset
      plt.plot(sorted_data, rankits, '-')
      #plt.plot(x,y)
      plt.title("CC: %.3f Slope: %.3f Offset: %.3f"%(corr, slope, offset))
      plt.xlabel("Sorted data")
      plt.ylabel("Rankits")
      plt.xlim(lim); plt.ylim(lim)
      plt.axes().set_aspect('equal')

      f = plt.figure(1)
      h = flex.histogram(sorted_data, n_slots=100, data_min = lim[0], data_max = lim[1])
      stats = flex.mean_and_variance(sorted_data)
      plt.plot(h.slot_centers().as_numpy_array(), h.slots().as_numpy_array(), '-')
      plt.xlim(lim)
      plt.xlabel("Sorted data")
      plt.ylabel("Count")
      plt.title("Normalized data mean: %.3f +/- %.3f"%(stats.mean(), stats.unweighted_sample_standard_deviation()))

      if self.scaler.params.raw_data.error_models.sdfac_refine.plot_refinement_steps:
        plt.ion()
        plt.pause(0.05)

    return corr, slope, offset

  def compute_normalized_deviations(self, data, indices):
   from xfel import compute_normalized_deviations
   return compute_normalized_deviations(data, indices)

  def get_initial_sdparams_estimates(self):
    """
    Use normal probability analysis to compute intial sdfac and sdadd parameters.
    """
    all_sigmas_normalized = self.compute_normalized_deviations(self.scaler.ISIGI, self.scaler.miller_set.indices())
    assert ((all_sigmas_normalized > 0) | (all_sigmas_normalized <= 0)).count(True) == len(all_sigmas_normalized) # no nans allowed

    # remove zeros (miller indices with only one observation will have a normalized deviation of 0 which shouldn't contribute to
    # the normal probability plot analysis and initial parameter estimation
    all_sigmas_normalized = all_sigmas_normalized.select(all_sigmas_normalized != 0)

    corr, slope, offset = self.normal_probability_plot(all_sigmas_normalized, (-0.5, 0.5))
    sdfac = 1/slope
    sdadd = offset
    #sdadd = -offset/slope
    sdb = math.sqrt(sdadd)

    return sdfac, sdb, sdadd


  def get_binned_intensities(self, n_bins=100):
    """
    Using self.ISIGI, bin the intensities using the following procedure:
    1) Find the minimum and maximum intensity values.
    2) Divide max-min by n_bins. This is the bin step size
    The effect is
    @param n_bins number of bins to use.
    @return a tuple with an array of selections for each bin and an array of median
    intensity values for each bin.
    """
    print("Computing intensity bins.", end=' ', file=self.log)
    all_mean_Is = flex.double()
    only_means = flex.double()
    for hkl_id in range(self.scaler.n_refl):
      hkl = self.scaler.miller_set.indices()[hkl_id]
      if hkl not in self.scaler.ISIGI: continue
      n = len(self.scaler.ISIGI[hkl])
      # get scaled intensities
      intensities = flex.double([self.scaler.ISIGI[hkl][i][0] for i in range(n)])
      meanI = flex.mean(intensities)
      only_means.append(meanI)
      all_mean_Is.extend(flex.double([meanI]*n))
    step = (flex.max(only_means)-flex.min(only_means))/n_bins
    print("Bin size:", step, file=self.log)

    sels = []
    binned_intensities = []
    min_all_mean_Is = flex.min(all_mean_Is)
    for i in range(n_bins):
      sel = (all_mean_Is > (min_all_mean_Is + step * i)) & (all_mean_Is < (min_all_mean_Is + step * (i+1)))
      if sel.all_eq(False): continue
      sels.append(sel)
      binned_intensities.append((step/2 + step*i)+min(only_means))

    for i, (sel, intensity) in enumerate(zip(sels, binned_intensities)):
      print("Bin %02d, number of observations: % 10d, midpoint intensity: %f"%(i, sel.count(True), intensity), file=self.log)

    return sels, binned_intensities

  def run_minimzer(self, values, sels, **kwargs):
    return simplex_minimizer(values, self.parameterization, self.scaler.ISIGI, self.scaler.miller_set.indices(), sels, kwargs['seed'], self.log)

  def adjust_errors(self):
    """
    Adjust sigmas according to Evans, 2011 Acta D and Evans and Murshudov, 2013 Acta D
    """
    print("Starting adjust_errors", file=self.log)
    print("Computing initial estimates of sdfac, sdb and sdadd", file=self.log)
    values = self.parameterization(flex.double(self.get_initial_sdparams_estimates()))

    print("Initial estimates:", end=' ', file=self.log)
    values.show(self.log)
    print("Refining error correction parameters sdfac, sdb, and sdadd", file=self.log)
    sels, binned_intensities = self.get_binned_intensities()
    seed = self.scaler.params.raw_data.error_models.sdfac_refine.random_seed
    minimizer = self.run_minimzer(values, sels, seed=seed)
    values = minimizer.get_refined_params()
    print("Final", end=' ', file=self.log)
    values.show(self.log)

    print("Applying sdfac/sdb/sdadd 1", file=self.log)
    self.scaler.ISIGI = minimizer.apply_sd_error_params(self.scaler.ISIGI, values)

    self.scaler.summed_weight= flex.double(self.scaler.n_refl, 0.)
    self.scaler.summed_wt_I  = flex.double(self.scaler.n_refl, 0.)

    print("Applying sdfac/sdb/sdadd 2", file=self.log)
    for hkl_id in range(self.scaler.n_refl):
      hkl = self.scaler.miller_set.indices()[hkl_id]
      if hkl not in self.scaler.ISIGI: continue

      n = len(self.scaler.ISIGI[hkl])

      for i in range(n):
        Intensity = self.scaler.ISIGI[hkl][i][0] # scaled intensity
        sigma = Intensity / self.scaler.ISIGI[hkl][i][1] # corrected sigma
        variance = sigma * sigma
        self.scaler.summed_wt_I[hkl_id] += Intensity / variance
        self.scaler.summed_weight[hkl_id] += 1 / variance

    if False:
      # validate using http://ccp4wiki.org/~ccp4wiki/wiki/index.php?title=Symmetry%2C_Scale%2C_Merge#Analysis_of_Standard_Deviations
      print("Validating", file=self.log)
      from matplotlib import pyplot as plt
      all_sigmas_normalized = self.compute_normalized_deviations(self.scaler.ISIGI, self.scaler.miller_set.indices())

      plt.hist(all_sigmas_normalized, bins=100)
      plt.figure()

      binned_rms_normalized_sigmas = []

      for i, sel in enumerate(sels):
        binned_rms_normalized_sigmas.append(math.sqrt(flex.mean(all_sigmas_normalized.select(sel)*all_sigmas_normalized.select(sel))))

      plt.plot(binned_intensities, binned_rms_normalized_sigmas, 'o')
      plt.show()

      all_sigmas_normalized = all_sigmas_normalized.select(all_sigmas_normalized != 0)
      self.normal_probability_plot(all_sigmas_normalized, (-0.5, 0.5), plot = True)

class simplex_minimizer_refltable(simplex_minimizer):
  """Class for refining sdfac, sdb and sdadd"""
  def compute_normalized_deviations(self, data, indices):
   from xfel.merging.algorithms.error_model import compute_normalized_deviations
   return compute_normalized_deviations(data, indices)

  def target(self, vector):
    """ Compute the functional by first applying the current values for the sd parameters
    to the input data, then computing the complete set of normalized deviations and finally
    using those normalized deviations to compute the functional."""
    values = self.parameterization(vector)

    if values.SDFAC < 0 or values.SDB < 0 or values.SDADD < 0:
      f = 1e6
    else:
      orig_isigi = self.data['isigi'] * 1

      self.apply_sd_error_params(self.data, values)
      all_sigmas_normalized = self.compute_normalized_deviations(self.data, self.indices)
      self.data['isigi'] = orig_isigi

      f = 0
      for bin in self.intensity_bin_selections:
        binned_normalized_sigmas = all_sigmas_normalized.select(bin)
        n = len(binned_normalized_sigmas)
        if n == 0: continue
        # weighting scheme from Evans, 2011
        w = math.sqrt(n)
        # functional is weight * (1-rms(normalized_sigmas))^s summed over all intensitiy bins
        f += w * ((1-math.sqrt(flex.mean(binned_normalized_sigmas*binned_normalized_sigmas)))**2)

    print("f: % 12.1f,"%f, end=' ', file=self.log)
    values.show(self.log)
    return f

  def apply_sd_error_params(self, data, values):
    from xfel.merging.algorithms.error_model import apply_sd_error_params
    apply_sd_error_params(data, values.SDFAC, values.SDB, values.SDADD, False)

class sdfac_refine_refltable(sdfac_refine):
  def __init__(self, scaler):
    super(sdfac_refine_refltable, self).__init__(scaler)

    if isinstance(scaler.ISIGI, dict):
      # work around for cxi.xmerge which doesn't make a reflection table
      from xfel.merging import isigi_dict_to_reflection_table
      scaler.ISIGI = isigi_dict_to_reflection_table(scaler.miller_set.indices(), scaler.ISIGI);
    setup_isigi_stats(scaler.ISIGI, self.scaler.miller_set.indices())

  def compute_normalized_deviations(self, data, indices):
   from xfel.merging.algorithms.error_model import compute_normalized_deviations
   return compute_normalized_deviations(data, indices)

  def get_binned_intensities(self, n_bins=100):
    """
    Using self.ISIGI, bin the intensities using the following procedure:
    1) Find the minimum and maximum intensity values.
    2) Divide max-min by n_bins. This is the bin step size
    The effect is
    @param n_bins number of bins to use.
    @return a tuple with an array of selections for each bin and an array of median
    intensity values for each bin.
    """
    print("Computing intensity bins.", end=' ', file=self.log)
    ISIGI = self.scaler.ISIGI
    meanI = ISIGI['mean_scaled_intensity']

    sels = []
    binned_intensities = []

    if True:
      # intensity range per bin is the same
      min_meanI = flex.min(meanI)
      step = (flex.max(meanI)-min_meanI)/n_bins
      print("Bin size:", step, file=self.log)
      self.bin_indices = flex.int(len(ISIGI), -1)

      for i in range(n_bins):
        if i+1 == n_bins:
          sel = (meanI >= (min_meanI + step * i))
        else:
          sel = (meanI >= (min_meanI + step * i)) & (meanI < (min_meanI + step * (i+1)))
        if sel.all_eq(False): continue
        sels.append(sel)
        self.bin_indices.set_selected(sel, len(sels)-1)
        binned_intensities.append((step/2 + step*i)+min_meanI)
      assert(self.bin_indices == -1).count(True) == False
    else:
      # n obs per bin is the same
      sorted_meanI = meanI.select(flex.sort_permutation(meanI))
      bin_size = len(meanI)/n_bins
      for i in range(n_bins):
        bin_min = sorted_meanI[int(i*bin_size)]
        sel = meanI >= bin_min
        if i+1 == n_bins:
          bin_max = sorted_meanI[-1]
        else:
          bin_max = sorted_meanI[int((i+1)*bin_size)]
          sel &= meanI < bin_max
        sels.append(sel)
        binned_intensities.append(bin_min + ((bin_max-bin_min)/2))

    for i, (sel, intensity) in enumerate(zip(sels, binned_intensities)):
      print("Bin %02d, number of observations: % 10d, midpoint intensity: %f"%(i, sel.count(True), intensity), file=self.log)

    return sels, binned_intensities

  def run_minimzer(self, values, sels, **kwargs):
   return simplex_minimizer_refltable(values, self.parameterization, self.scaler.ISIGI, self.scaler.miller_set.indices(), sels, kwargs['seed'], self.log)

  def adjust_errors(self):
    """
    Adjust sigmas according to Evans, 2011 Acta D and Evans and Murshudov, 2013 Acta D
    """
    if self.scaler.params.raw_data.error_models.sdfac_refine.apply_to_I_only:
      import copy
      tmp_ISIGI = copy.deepcopy(self.scaler.ISIGI)
      self.scaler.summed_weight_uncorrected = flex.double(self.scaler.n_refl, 0.)

    print("Starting adjust_errors", file=self.log)
    print("Computing initial estimates of sdfac, sdb and sdadd", file=self.log)
    values = self.parameterization(flex.double(self.get_initial_sdparams_estimates()))

    print("Initial estimates:", end=' ', file=self.log)
    values.show(self.log)
    print("Refining error correction parameters sdfac, sdb, and sdadd", file=self.log)
    sels, binned_intensities = self.get_binned_intensities()
    seed = self.scaler.params.raw_data.error_models.sdfac_refine.random_seed
    minimizer = self.run_minimzer(values, sels, seed=seed)
    values = minimizer.get_refined_params()
    print("Final", end=' ', file=self.log)
    values.show(self.log)

    print("Applying sdfac/sdb/sdadd 1", file=self.log)
    minimizer.apply_sd_error_params(self.scaler.ISIGI, values)

    self.scaler.summed_weight= flex.double(self.scaler.n_refl, 0.)
    self.scaler.summed_wt_I  = flex.double(self.scaler.n_refl, 0.)

    print("Applying sdfac/sdb/sdadd 2", file=self.log)
    for i in range(len(self.scaler.ISIGI)):
      hkl_id = self.scaler.ISIGI['miller_id'][i]
      Intensity = self.scaler.ISIGI['scaled_intensity'][i] # scaled intensity
      sigma = Intensity / self.scaler.ISIGI['isigi'][i] # corrected sigma
      variance = sigma * sigma
      self.scaler.summed_wt_I[hkl_id] += Intensity / variance
      self.scaler.summed_weight[hkl_id] += 1 / variance

      if self.scaler.params.raw_data.error_models.sdfac_refine.apply_to_I_only:
        sigma = Intensity / tmp_ISIGI['isigi'][i] # uncorrected sigma
        variance = sigma * sigma
        self.scaler.summed_weight_uncorrected[hkl_id] += 1 / variance

    if self.scaler.params.raw_data.error_models.sdfac_refine.plot_refinement_steps:
      from matplotlib.pyplot import cm
      from matplotlib import pyplot as plt
      import numpy as np
      for i in range(2):
        f = plt.figure(i)
        lines = plt.gca().get_lines()
        color=cm.rainbow(np.linspace(0,1,len(lines)))
        for line, c in zip(reversed(lines), color):
          line.set_color(c)
      plt.ioff()
      plt.show()

    if False:
      # validate using http://ccp4wiki.org/~ccp4wiki/wiki/index.php?title=Symmetry%2C_Scale%2C_Merge#Analysis_of_Standard_Deviations
      print("Validating", file=self.log)
      from matplotlib import pyplot as plt
      all_sigmas_normalized = self.compute_normalized_deviations(self.scaler.ISIGI, self.scaler.miller_set.indices())
      plt.hist(all_sigmas_normalized, bins=100)
      plt.figure()

      binned_rms_normalized_sigmas = []

      for i, sel in enumerate(sels):
        binned_rms_normalized_sigmas.append(math.sqrt(flex.mean(all_sigmas_normalized.select(sel)*all_sigmas_normalized.select(sel))))

      plt.plot(binned_intensities, binned_rms_normalized_sigmas, 'o')
      plt.show()

      all_sigmas_normalized = all_sigmas_normalized.select(all_sigmas_normalized != 0)
      self.normal_probability_plot(all_sigmas_normalized, (-0.5, 0.5), plot = True)


 *******************************************************************************


 *******************************************************************************
xfel/merging/algorithms/error_model/sdfac_refine_lbfgs.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from scitbx.array_family import flex
import math

from xfel.merging.algorithms.error_model.sdfac_refine import sdfac_refine_refltable
from xfel.merging.algorithms.error_model import compute_normalized_deviations, apply_sd_error_params, df_dpsq as df_dpsq_cpp

def finite_difference(target, values, p, DELTA = 1.e-7):
  """ Compute finite difference given a target function """
  import copy
  functional = target(values)
  tempvals = copy.deepcopy(values)
  tempvals.reference[p] += DELTA

  dfunctional = target(tempvals)
  #calculate by finite_difference
  finite_g = (dfunctional-functional )/DELTA
  return finite_g

class sdfac_refine_refltable_lbfgs(sdfac_refine_refltable):
  def get_initial_sdparams_estimates(self):
    sdfac, sdb, sdadd = super(sdfac_refine_refltable_lbfgs, self).get_initial_sdparams_estimates()
    sdadd = math.sqrt(sdadd)
    sdb = math.sqrt(sdadd)
    return sdfac, sdb, sdadd

  def run_minimzer(self, values, sels, **kwargs):
    refinery = sdfac_refinery(self.scaler, self, self.scaler.miller_set.indices(), sels, self.log)
    return lbfgs_minimizer(values.reference, self.parameterization, refinery, self.log,
      show_finite_differences = self.scaler.params.raw_data.error_models.sdfac_refine.show_finite_differences)

from libtbx import adopt_init_args
class sdfac_refinery(object):
  def __init__(self, scaler, modeler, indices, bins, log):
    adopt_init_args(self, locals())
    self.ISIGI = self.scaler.ISIGI

    self.weights = flex.double()
    for bin in bins:
      # weighting scheme from Evans, 2011
      n = bin.count(True)
      self.weights.append(math.sqrt(n))

  def fvec_callable(self, values):
    """ Compute the functional by first applying the current values for the sd parameters
    to the input data, then computing the complete set of normalized deviations and finally
    using those normalized deviations to compute the functional."""

    all_sigmas_normalized, _ = self.get_normalized_sigmas(values)
    f = flex.double()
    for i, bin in enumerate(self.bins):
      binned_normalized_sigmas = all_sigmas_normalized.select(bin)
      n = len(binned_normalized_sigmas)
      if n == 0:
        f.append(0)
        continue
      # functional is weight * (1-rms(normalized_sigmas))^s summed over all intensitiy bins
      f.append(1-math.sqrt(flex.mean(binned_normalized_sigmas*binned_normalized_sigmas)))

    if self.scaler.params.raw_data.error_models.sdfac_refine.plot_refinement_steps:
      all_sigmas_normalized = all_sigmas_normalized.select(all_sigmas_normalized != 0)
      print(self.modeler.normal_probability_plot(all_sigmas_normalized, (-0.5, 0.5), plot = True))
    return f

  def functional(self, fvec):
    return flex.sum(self.weights * fvec**2)

  def jacobian_callable(self, values):
    all_sigmas_normalized, sigma_prime = self.get_normalized_sigmas(values)
    df_dsdfacsq = self.df_dsdfacsq(values, all_sigmas_normalized, sigma_prime)
    df_dsdbsq   = self.df_dsdbsq(values, all_sigmas_normalized, sigma_prime)
    df_dsaddbsq = self.df_dsaddbsq(values, all_sigmas_normalized, sigma_prime)
    return [df_dsdfacsq, df_dsdbsq, df_dsaddbsq]

  def gradients(self, values):
    g = flex.double()
    for d in self.jacobian_callable(values):
      g.append(flex.sum(self.weights * d))
    return g

  def apply_sd_error_params(self, data, values):
    apply_sd_error_params(data, values.SDFACSQ, values.SDBSQ, values.SDADDSQ, True)

  def get_normalized_sigmas(self, values):
    orig_isigi = self.ISIGI['isigi'] * 1
    # time split between these two functions
    self.apply_sd_error_params(self.ISIGI, values)
    all_sigmas_normalized = compute_normalized_deviations(self.ISIGI, self.indices)

    sigma_prime = self.ISIGI['scaled_intensity'] / self.ISIGI['isigi']

    self.ISIGI['isigi'] = orig_isigi
    return all_sigmas_normalized, sigma_prime

  def df_dpsq(self, all_sigmas_normalized, sigma_prime, dsigmasq_dpsq):
    g = df_dpsq_cpp(all_sigmas_normalized, sigma_prime, dsigmasq_dpsq, self.ISIGI, self.modeler.bin_indices, len(self.bins))
    return g

  def df_dsdfacsq(self, values, all_sigmas_normalized, sigma_prime):
    sigma = self.ISIGI['scaled_intensity'] / self.ISIGI['isigi']
    imean = self.ISIGI['mean_scaled_intensity']

    dsigmasq_dsdfac = 2 * values.SDFAC
    dsigmasq_dsdfacsq = (sigma**2 + values.SDBSQ * imean + values.SDADDSQ * imean**2) * dsigmasq_dsdfac

    return self.df_dpsq(all_sigmas_normalized, sigma_prime, dsigmasq_dsdfacsq)

  def df_dsdbsq(self, values, all_sigmas_normalized, sigma_prime):
    imean = self.ISIGI['mean_scaled_intensity']

    dsigmasq_dsdb = 2 * values.SDB
    dsigmasq_dsdbsq = values.SDFACSQ * imean * dsigmasq_dsdb

    return self.df_dpsq(all_sigmas_normalized, sigma_prime, dsigmasq_dsdbsq)

  def df_dsaddbsq(self, values, all_sigmas_normalized, sigma_prime):
    imean = self.ISIGI['mean_scaled_intensity']

    dsigmasq_dsdadd = 2 * values.SDADD
    dsigmasq_dsdaddsq = values.SDFACSQ * imean**2 * dsigmasq_dsdadd

    return self.df_dpsq(all_sigmas_normalized, sigma_prime, dsigmasq_dsdaddsq)

class lbfgs_minimizer(object):
  def __init__(self, current_x=None, parameterization=None, refinery=None,
               ISIGI = None, indices = None, bins = None, out=None,
               min_iterations=0, max_calls=1000, max_drop_eps=1.e-10,
               show_finite_differences = False):
    adopt_init_args(self, locals())
    self.n = current_x.size()
    self.x = current_x

    if False:
      self.diag_mode = "always"
      from scitbx import lbfgs
      self.minimizer = lbfgs.run(
        target_evaluator=self,
        termination_params=lbfgs.termination_parameters(
          traditional_convergence_test=False,
          drop_convergence_test_max_drop_eps=max_drop_eps,
          min_iterations=min_iterations,
          max_iterations = None,
          max_calls=max_calls),
        exception_handling_params=lbfgs.exception_handling_parameters(
           ignore_line_search_failed_rounding_errors=True,
           ignore_line_search_failed_step_at_lower_bound=True,#the only change from default
           ignore_line_search_failed_step_at_upper_bound=False,
           ignore_line_search_failed_maxfev=False,
           ignore_line_search_failed_xtol=False,
           ignore_search_direction_not_descent=False)
        )
    else:
      from scitbx import lbfgsb
      l = flex.double(self.n, 1e-8)

      if len(l) > 3:
        for p in range(7,len(l)):
          l[p] = 1e-15 # g*

      self.minimizer = lbfgsb.minimizer(
        n = self.n,
        l = l,
        u = flex.double(self.n, 0),
        nbd = flex.int(self.n, 1),
      )
      while True:
        self.compute_functional_and_gradients()
        if self.minimizer.process(self.x, self.f, self.g):
          pass
        elif self.minimizer.is_terminated():
          break

  def compute_functional_and_gradients(self):
    values = self.parameterization(self.x)
    fvec = self.refinery.fvec_callable(values)
    self.f = functional = self.refinery.functional(fvec)
    self.g = self.refinery.gradients(values)

    if self.show_finite_differences:
      finite_g = flex.double()
      for x in range(self.n):
        finite_g.append(finite_difference(
          lambda v: self.refinery.functional(self.refinery.fvec_callable(v)),
          values, x))

      for x in range(self.n):
        print("p%d finite % 20.10f analytical % 20.10f"%(x, finite_g[x], self.g[x]), file=self.out)

    print("functional value % 20.3f"%functional, end=' ', file=self.out)
    values.show(self.out)
    return self.f, self.g

  def compute_functional_gradients_diag(self):
    values = self.parameterization(self.x)
    fvec = self.refinery.fvec_callable(values)
    self.f = functional = self.refinery.functional(fvec)

    self.g = flex.double()
    self.c = flex.double()
    for d in self.refinery.jacobian_callable(values):
      self.g.append(flex.sum(self.refinery.weights * d))
      self.c.append(flex.sum(self.refinery.weights * d * d))

    diag = 1/self.c

    if self.show_finite_differences:
      finite_g = flex.double()
      for x in range(self.n):
        finite_g.append(finite_difference(
          lambda v: self.refinery.functional(self.refinery.fvec_callable(v)),
          values, x))

      for x in range(self.n):
        print("p%d finite % 20.10f analytical % 20.10f"%(x, finite_g[x], self.g[x]), file=self.out)

    print("functional value % 20.3f"%functional, end=' ', file=self.out)
    values.show(self.out)
    return self.f, self.g, diag


  def get_refined_params(self):
    return self.parameterization(self.x)

  def apply_sd_error_params(self, data, values):
    self.refinery.apply_sd_error_params(data, values)

  def __del__(self):
    values = self.parameterization(self.x)
    print("FINALMODEL", end=' ', file=self.out)
    print("functional value % 20.3f"%self.f, end=' ', file=self.out)
    values.show(self.out)


 *******************************************************************************


 *******************************************************************************
xfel/merging/algorithms/error_model/sdfac_refine_levmar.py
from __future__ import absolute_import, division, print_function

from xfel.cxi.postrefinement_hybrid_rs import per_frame_helper
from scitbx.lstbx import normal_eqns_solving
from scitbx.array_family import flex
import math

from xfel.merging.algorithms.error_model.sdfac_refine_lbfgs import \
  sdfac_refinery, sdfac_refine_refltable_lbfgs

class sdfac_helper(per_frame_helper):
  def build_up(pfh, objective_only=False):
    values = pfh.parameterization(pfh.x)
    residuals = pfh.refinery.fvec_callable(values)
    pfh.reset()
    if objective_only:
      pfh.add_residuals(residuals, weights=pfh.refinery.weights)
    else:
      grad_r = pfh.refinery.jacobian_callable(values)
      jacobian = flex.double(
        flex.grid(len(pfh.refinery.bins), pfh.n_parameters))
      for j, der_r in enumerate(grad_r):
        jacobian.matrix_paste_column_in_place(der_r,j)
        #print >> pfh.out, "COL",j, list(der_r)
      # note the minus sign on the jacobian
      pfh.add_equations(residuals, -jacobian, weights=pfh.refinery.weights)
    #print >> pfh.out, "rms %10.3f"%math.sqrt(flex.mean(pfh.refinery.weights*residuals*residuals)),
    print("functional value % 20.3f"%pfh.refinery.functional(residuals), end=' ', file=pfh.out)
    values.show(pfh.out)

class sdfac_refine_refltable_levmar(sdfac_refine_refltable_lbfgs):
  def run_minimzer(self, values, sels, **kwargs):
    self.refinery = sdfac_refinery(self.scaler, self, self.scaler.miller_set.indices(), sels, self.log)
    self.helper = sdfac_helper(current_x = values.reference,
                               parameterization = self.parameterization, refinery = self.refinery,
                               out = self.log )
    self.iterations = normal_eqns_solving.levenberg_marquardt_iterations(
      non_linear_ls = self.helper,
      track_all=True,
      gradient_threshold=1e-08,
      step_threshold=1e-08,
      tau=1e-08,
      n_max_iterations=200)
    return self

  def get_refined_params(self):
    return self.parameterization(self.helper.x)

  def apply_sd_error_params(self, data, values):
    self.refinery.apply_sd_error_params(data, values)


 *******************************************************************************


 *******************************************************************************
xfel/merging/algorithms/postrefine/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/algorithms/statistics/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/balance/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/balance/factory.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.worker import factory as factory_base
from xfel.merging.application.balance.load_balancer import load_balancer

class factory(factory_base):
  """Factory class for balancing input data load."""
  @staticmethod
  def from_parameters(params, additional_info=None, mpi_helper=None, mpi_logger=None):
    if params.input.parallel_file_load.balance != None:
      return [load_balancer(params, mpi_helper, mpi_logger)]
    else:
      return []


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/balance/load_balancer.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.worker import worker
from dials.array_family import flex
from dxtbx.model.experiment_list import ExperimentList
from xfel.merging.application.reflection_table_utils import reflection_table_utils
import math

class load_balancer(worker):
  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(load_balancer, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Balance input data load'

  def reflection_table_stub(self, reflections):
    '''Return an empty reflection table with the same format as the reflection table input to this class'''
    table = flex.reflection_table()
    for key in reflections:
      table[key] = type(reflections[key])()
    return table

  def divide_list_into_chunks(self, list_, n): # n - number of chunks
    return [list_[start::n] for start in range(n)]

  def run(self, experiments, reflections):
    self.logger.log("Rebalancing input load -- %s method..."%self.params.input.parallel_file_load.balance)
    if self.mpi_helper.rank == 0:
      self.logger.main_log("Rebalancing input load -- %s method..."%self.params.input.parallel_file_load.balance)
    if self.params.input.parallel_file_load.balance == "global2":
      # get status BEFORE balancing and print to main_log for EACH RANK if we have balance_verbose set
      from xfel.merging.application.utils.data_counter import data_counter
      if self.params.input.parallel_file_load.balance_verbose and self.mpi_helper.rank == 0:
        self.logger.main_log("Data distribution before load balancing:")
      expt_counts_by_rank, _, _ = data_counter(self.params).count_each(experiments, reflections, verbose=self.params.input.parallel_file_load.balance_verbose)
    else:
      expt_counts_by_rank = None

    if self.params.input.parallel_file_load.balance == "global1":
      new_experiments, new_reflections = self.distribute_over_ranks_shuffle(experiments, reflections, self.mpi_helper.comm, self.mpi_helper.size)
    elif self.params.input.parallel_file_load.balance == "global2":
      new_experiments, new_reflections = self.distribute_over_ranks_minimalist(experiments, reflections, self.mpi_helper.comm, self.mpi_helper.size, expt_counts_by_rank)
    elif self.params.input.parallel_file_load.balance == "per_node":
      mpi_color = int(self.mpi_helper.rank / self.params.input.parallel_file_load.ranks_per_node)
      mpi_new_rank = self.mpi_helper.rank % self.params.input.parallel_file_load.ranks_per_node
      mpi_split_comm = self.mpi_helper.comm.Split(mpi_color, mpi_new_rank)
      new_experiments, new_reflections = self.distribute_over_ranks(experiments, reflections, mpi_split_comm, self.params.input.parallel_file_load.ranks_per_node)

    if self.params.input.parallel_file_load.balance == "global2":
      # get status again AFTER balancing and report back number of experiments on EACH RANK in the main_log if balance_verbose is set
      if self.params.input.parallel_file_load.balance_verbose and self.mpi_helper.rank == 0:
        self.logger.main_log("Data distribution after load balancing:")
        data_counter(self.params).count_each(experiments, reflections, verbose=True)

    # Do we have any data?
    from xfel.merging.application.utils.data_counter import data_counter
    data_counter(self.params).count(new_experiments, new_reflections)

    return new_experiments, new_reflections

  def distribute_over_ranks_shuffle(self, experiments, reflections, mpi_communicator, number_of_mpi_ranks):
    self.logger.log_step_time("LB_SPLIT_LIST")
    self.split_experiments = self.divide_list_into_chunks(experiments, number_of_mpi_ranks)
    self.logger.log_step_time("LB_SPLIT_LIST", True)

    # If some (but not all!) chunks are empty, we want those empty chunks to be randomly distributed.
    # Otherwise, after alltoall, high-index ranks will get no data.
    self.logger.log_step_time("LB_SHUFFLE")
    number_of_empty_chunks = [len(self.split_experiments[i]) for i in range(len(self.split_experiments))].count(0)
    if number_of_empty_chunks > 0 and len(experiments) != 0:
      import random
      #random.seed(8)
      random.shuffle(self.split_experiments)
    self.logger.log_step_time("LB_SHUFFLE", True)

    '''
    self.logger.log("Split experiment list into %d chunks"%len(self.split_experiments))
    for chunk in self.split_experiments:
      self.logger.log(len(chunk))
    '''

    # Distribute reflections over experiment chunks
    self.logger.log_step_time("LB_REF_DISTR")
    self.distribute_reflections_over_experiment_chunks_cpp(reflections)
    reflections.clear()
    del experiments
    self.logger.log_step_time("LB_REF_DISTR", True)

    # Run alltoall on experiments
    new_experiments = self.exchange_experiments_by_alltoall(mpi_communicator)

    # Run alltoall on reflections
    if self.params.input.parallel_file_load.balance_mpi_alltoall_slices == 1:
      new_reflections = self.exchange_reflections_by_alltoall(mpi_communicator)
    else:
      new_reflections = self.exchange_reflections_by_alltoall_sliced(mpi_communicator, self.params.input.parallel_file_load.balance_mpi_alltoall_slices)

    return new_experiments, new_reflections

  def exchange_experiments_by_alltoall(self, mpi_communicator):
    self.logger.log_step_time("LB_EXPTS_ALL_TO_ALL")
    new_split_experiments = mpi_communicator.alltoall(self.split_experiments)
    del self.split_experiments
    self.logger.log_step_time("LB_EXPTS_ALL_TO_ALL", True)

    self.logger.log_step_time("LB_EXPTS_CONSOLIDATE")
    self.logger.log("Consolidating experiments after all-to-all...")
    new_experiments = ExperimentList()
    for entry in new_split_experiments:
      new_experiments.extend(entry)
    del new_split_experiments
    self.logger.log_step_time("LB_EXPTS_CONSOLIDATE", True)

    return new_experiments

  def exchange_reflections_by_alltoall(self, mpi_communicator):
    ''' Run all-to-all and return a new reflection table'''
    self.logger.log_step_time("LB_REFLS_ALL_TO_ALL")
    new_split_reflections = mpi_communicator.alltoall(self.split_reflections)
    del self.split_reflections
    self.logger.log_step_time("LB_REFLS_ALL_TO_ALL", True)

    self.logger.log_step_time("LB_REFLS_CONSOLIDATE")
    self.logger.log("Consolidating reflections after all-to-all...")
    new_reflections = flex.reflection_table.concat(new_split_reflections)
    del new_split_reflections
    self.logger.log_step_time("LB_REFLS_CONSOLIDATE", True)

    return new_reflections

  def exchange_reflections_by_alltoall_sliced(self, mpi_communicator, number_of_slices):
    '''Split each hkl chunk into N slices. This is needed to address the MPI alltoall memory problem'''
    list_of_sliced_reflection_chunks = [] # if the self.split_reflections list contains chunks: [A,B,C...], it will be sliced like: [[A1,A2,...,An], [B1,B2,...,Bn], [C1,C2,...,Cn], ...], where n is the number of chunk slices
    for i in range(len(self.split_reflections)):
      reflection_chunk_slices = []
      for chunk_slice in reflection_table_utils.get_next_reflection_table_slice(self.split_reflections[i], number_of_slices, self.reflection_table_stub):
        reflection_chunk_slices.append(chunk_slice)
      list_of_sliced_reflection_chunks.append(reflection_chunk_slices)

    for j in range(number_of_slices):
      reflection_chunks_for_alltoall = list()
      for i in range(len(self.split_reflections)):
        reflection_chunks_for_alltoall.append(list_of_sliced_reflection_chunks[i][j]) # [Aj,Bj,Cj...]

      self.logger.log_step_time("ALL-TO-ALL")
      received_reflection_chunks = mpi_communicator.alltoall(reflection_chunks_for_alltoall)
      self.logger.log("After all-to-all received %d reflection chunks" %len(received_reflection_chunks))
      self.logger.log_step_time("ALL-TO-ALL", True)

      self.logger.log_step_time("CONSOLIDATE")
      self.logger.log("Consolidating reflection tables...")
      if j == 0:
        all_received = received_reflection_chunks
      else:
        for i in range(len(self.split_reflections)):
          # extend here because we are working within a chunk, and the ids are consistent within that chunk
          all_received[i].extend(received_reflection_chunks[i])
      self.logger.log_step_time("CONSOLIDATE", True)
    # now concatenate all the chunks, resetting the ids to be contiguous
    result_reflections = flex.reflection_table.concat(all_received)

    return result_reflections

  def distribute_reflections_over_experiment_chunks_python(self, reflections):
    self.split_reflections = []

    for i in range(len(self.split_experiments)):
      sel = flex.bool(len(reflections), False)
      for expt_id, experiment in enumerate(self.split_experiments[i]):
        sel |= reflections['id'] == expt_id
      self.split_reflections.append(reflections.select(sel))

  def distribute_reflections_over_experiment_chunks_cpp(self, reflections):
    '''Distribute reflections over experiment chunks according to experiment identifiers, '''
    reflection_count = reflections.size()
    distributed_reflection_count = 0
    # initialize a list of reflection chunks
    self.split_reflections = []
    for i in range(len(self.split_experiments)):
      self.split_reflections.append(self.reflection_table_stub(reflections))

    if reflection_count > 0:
      # set up two lists to be passed to the C++ extension: experiment ids and chunk ids. It's basically a hash table to look up chunk ids by experiment identifier
      exp_id_list = flex.std_string()
      chunk_id_list = flex.int()
      for i in range(len(self.split_experiments)):
        for exp in self.split_experiments[i]:
          exp_id_list.append(exp.identifier)
          chunk_id_list.append(i)

      # distribute reflections over the experiment chunks using a C++ extension
      from xfel.merging import split_reflections_by_experiment_chunks_cpp
      split_reflections_by_experiment_chunks_cpp(reflections, exp_id_list, chunk_id_list, self.split_reflections)

      for ref_table in self.split_reflections:
        distributed_reflection_count += ref_table.size()

        for expt_id in set(ref_table['id']):
          ref_table.experiment_identifiers()[expt_id] = reflections.experiment_identifiers()[expt_id]

    self.logger.log("Distributed %d out of %d reflections"%(distributed_reflection_count, reflection_count))

  def distribute_over_ranks_minimalist(self, experiments, reflections, mpi_communicator, number_of_mpi_ranks, current_counts_by_rank):
    self.logger.log_step_time("LB_SPLIT_LIST")
    if self.mpi_helper.rank == 0:
      def first(lst, test):
        for i in range(len(lst)):
          if test(lst[i]):
            return i

      # quota: max number of counts that should end up on one rank once balanced
      # difference: how unbalanced each rank is currently
      # send_tuples: instructions for redistributing load, a list [L1,L2,L3...Li] where i is the rank
      # sending data and Li describes where to send it

      quota = int(math.ceil(sum(current_counts_by_rank)/len(current_counts_by_rank)))
      difference = [count - quota for count in current_counts_by_rank]
      send_tuples = [[] for i in range(len(current_counts_by_rank))]

      # algorithm (deterministic):
      # - for each rank, if we are overburdened, find the first rank that is underburdened
      # - send it as much of the load as it can take before hitting quota (denoted by a tuple of the target
      #   rank and the number of counts to send)
      # - repeat until no rank is overburdened (assert this to be the case)
      # - it may still be the case that ranks are underburdened unequally (e.g. [10,10,10,8,8])
      # - to address this, for each rank that is underburdened by more than 1, find the first rank at quota
      # - request one count from it
      # - assert all ranks are now within one count of each other

      for i in range(len(current_counts_by_rank)):
        while difference[i] > 0:
          j = first(difference, lambda count: count<0)
          send = min(difference[i], -1 * difference[j])
          send_tuples[i].append((j, send))
          difference[i] -= send
          difference[j] += send
      assert max(difference) == 0
      for i in range(len(current_counts_by_rank)):
        while difference[i] < -1:
          j = first(difference, lambda count: count==0)
          send_tuples[j].append((i, 1))
          difference[i] += 1
          difference[j] -= 1
      assert max(difference) == 0
      assert min(difference) >= -1

    else:
      send_tuples = None # not sure if we need this?

    # broadcast instructions
    send_tuples = mpi_communicator.bcast(send_tuples, root=0)

    self.logger.log_step_time("LB_SPLIT_LIST", True)
    self.logger.log_step_time("LB_EXPTS_AND_REFLS_ALLTOALL")

    # carry out load balancing with all-to-all mpi communication
    send_instructions = send_tuples[self.mpi_helper.rank]

    # pare down balanced_experiments and balanced_reflections as we separate off what to send out
    send_data = [(None, None) for j in range(len(send_tuples))]
    recv_data = [(None, None) for j in range(len(send_tuples))]
    emap = reflections.experiment_identifiers()
    for (j, count) in send_instructions:
      send_expt_j = experiments[-count:]
      experiments = experiments[:-count]
      send_refl_j = self.reflection_table_stub(reflections)
      for k, e in enumerate(send_expt_j):
        sel = emap.values() == e.identifier
        assert sel.count(True) == 1
        e_id = emap.keys().select(sel)[0]
        r = reflections.select(reflections['id'] == e_id) # select matching reflections to send
        send_refl_j.extend(r)
        reflections = reflections.select(reflections['id'] != e_id) # remove from this rank's reflections
      send_refl_j.reset_ids()
      send_data[j] = (send_expt_j, send_refl_j)
    recv_data = mpi_communicator.alltoall(send_data)

    # tack on only what was targeted to be received by the current rank
    for (received_expt_i, received_refl_i) in recv_data:
      if received_expt_i is None: continue
      current_ids = set([e.identifier for e in experiments])
      recv_ids = set([e.identifier for e in received_expt_i])
      assert current_ids.isdisjoint(recv_ids)
      experiments.extend(received_expt_i)
      reflections = flex.reflection_table.concat([reflections, received_refl_i])

    self.logger.log_step_time("LB_EXPTS_AND_REFLS_ALLTOALL", True)

    return experiments, reflections

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(load_balancer)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/errors/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/errors/error_modifier_ev11.py
from __future__ import absolute_import, division, print_function
from dials.array_family import flex
from xfel.merging.application.worker import worker
from xfel.merging.application.reflection_table_utils import reflection_table_utils
import math
import numpy as np
import sys

number_of_intensity_bins = 100

class error_modifier_ev11(worker):

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(error_modifier_ev11, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Adjust intensity errors -- ev11'

  def run(self, experiments, reflections):
    '''Modify intensity errors according to EV11 -- Brewster2019'''
    assert self.params.merging.error.model == "ev11"

    self.logger.log_step_time("ERROR_MODIFIER_EV11")
    self.logger.log("Modifying intensity errors -- ev11 method (starting with %d reflections)"%(len(reflections)))
    reflections = self.modify_errors(reflections)
    self.logger.log_step_time("ERROR_MODIFIER_EV11", True)

    return experiments, reflections

  def calculate_intensity_bin_limits(self):
    '''Calculate the minimum and maximum values of the mean intensities for each HKL'''
    count = self.work_table.size()
    mean_intensity_min = flex.min(self.work_table['biased_mean']) if count > 0 else float('inf')
    mean_intensity_max = flex.max(self.work_table['biased_mean']) if count > 0 else float('-inf')
    if count > 0:
      self.logger.log("Using %d multiply-measured HKLs; mean intensity (min,max): (%f,%f)"%(count, mean_intensity_min, mean_intensity_max))
    else:
      self.logger.log("No multiply-measured HKLs available")

    comm = self.mpi_helper.comm
    MPI = self.mpi_helper.MPI
    global_mean_intensity_min = comm.allreduce(mean_intensity_min, MPI.MIN)
    global_mean_intensity_max = comm.allreduce(mean_intensity_max, MPI.MAX)
    self.logger.log("Global mean intensity (min,max): (%f,%f)"%(global_mean_intensity_min, global_mean_intensity_max))

    self.intensity_bin_limits = np.linspace(global_mean_intensity_min, global_mean_intensity_max, number_of_intensity_bins + 1)
    self.intensity_bin_limits[0] = float('-inf')
    self.intensity_bin_limits[len(self.intensity_bin_limits) - 1] = float('inf')

  def setup_work_arrays(self, reflections):
    '''Select multiply-measured HKLs. Calculate and cache reflection deltas, deltas squared, and HKL means for every reflection'''
    self.deltas     = flex.double()
    self.work_table = flex.reflection_table()
    delta_sq        = flex.double()
    mean            = flex.double() # mean = <I'_hj>
    biased_mean     = flex.double() # biased_mean = <I_h>, so dont leave out any reflection
    var             = flex.double()
    all_biased_mean = flex.double()

    for refls in reflection_table_utils.get_next_hkl_reflection_table(reflections):
      number_of_measurements = refls.size()
      if number_of_measurements == 0: # if the returned "refls" list is empty, it's the end of the input "reflections" list
        break
      refls_biased_mean = flex.double(len(refls), flex.mean(refls['intensity.sum.value']))
      all_biased_mean.extend(refls_biased_mean)

      if number_of_measurements > self.params.merging.minimum_multiplicity:
        nn_factor_sqrt = math.sqrt((number_of_measurements - 1) / number_of_measurements)
        i_sum = flex.double(number_of_measurements, flex.sum(refls['intensity.sum.value']))
        i_sum_minus_val = i_sum - refls['intensity.sum.value']
        mean_without_val = i_sum_minus_val/(number_of_measurements-1)
        delta = nn_factor_sqrt * (refls['intensity.sum.value'] - mean_without_val)
        self.deltas.extend(delta/flex.sqrt(refls['intensity.sum.variance'])) # Please be careful about where to put the var
        delta_sq.extend(delta**2)
        mean.extend(mean_without_val)
        biased_mean.extend(refls_biased_mean)
        var.extend(refls['intensity.sum.variance'])

    self.work_table["delta_sq"]    = delta_sq
    self.work_table["mean"]        = mean
    self.work_table["biased_mean"] = biased_mean
    self.work_table["var"]         = var
    reflections['biased_mean'] = all_biased_mean
    self.logger.log("Number of work reflections selected: %d"%self.deltas.size())
    return reflections

  def calculate_functional_ev11(self):
    # Results of calculation (on rank 0):
    func           = 0
    der_wrt_sfac   = 0
    der_wrt_sb     = 0
    der_wrt_sadd   = 0

    comm = self.mpi_helper.comm
    MPI = self.mpi_helper.MPI

    for reflections in self.intensity_bins:
      number_of_reflections_in_bin = reflections.size()

      sum_of_delta_squared_in_bin = flex.double(number_of_reflections_in_bin, 0)
      sum_of_der_wrt_sfac_in_bin  = flex.double(number_of_reflections_in_bin, 0)
      sum_of_der_wrt_sb_in_bin    = flex.double(number_of_reflections_in_bin, 0)
      sum_of_der_wrt_sadd_in_bin  = flex.double(number_of_reflections_in_bin, 0)

      if number_of_reflections_in_bin > 0:
        variance = reflections['var']
        mean_intensity = reflections['biased_mean'] # the mean intensity of the sample of HKLs which includes this reflection

        var_ev11               = self.sfac**2*(variance + self.sb**2*mean_intensity + self.sadd**2*mean_intensity**2)
        var_ev11_der_over_sfac = 2*self.sfac * (variance + self.sb**2 * mean_intensity + self.sadd**2 * mean_intensity**2)
        var_ev11_der_over_sb   = self.sfac**2 * 2*self.sb   * mean_intensity
        var_ev11_der_over_sadd = self.sfac**2 * 2*self.sadd * mean_intensity**2

        sum_of_delta_squared_in_bin += reflections['delta_sq'] / var_ev11

        sum_of_der_wrt_sfac_in_bin  -= reflections['delta_sq'] / var_ev11**2 * var_ev11_der_over_sfac
        sum_of_der_wrt_sb_in_bin    -= reflections['delta_sq'] / var_ev11**2 * var_ev11_der_over_sb
        sum_of_der_wrt_sadd_in_bin  -= reflections['delta_sq'] / var_ev11**2 * var_ev11_der_over_sadd

      global_number_of_reflections_in_bin   = comm.reduce(number_of_reflections_in_bin, MPI.SUM, root=0)
      global_sum_of_delta_squared_in_bin    = comm.reduce(flex.sum(sum_of_delta_squared_in_bin),  MPI.SUM, root=0)
      global_sum_of_der_wrt_sfac_in_bin     = comm.reduce(flex.sum(sum_of_der_wrt_sfac_in_bin),   MPI.SUM, root=0)
      global_sum_of_der_wrt_sb_in_bin       = comm.reduce(flex.sum(sum_of_der_wrt_sb_in_bin),     MPI.SUM, root=0)
      global_sum_of_der_wrt_sadd_in_bin     = comm.reduce(flex.sum(sum_of_der_wrt_sadd_in_bin),   MPI.SUM, root=0)

      if self.mpi_helper.rank == 0:
        if global_number_of_reflections_in_bin > 0 and global_sum_of_delta_squared_in_bin > 0:

          global_weight_for_bin = math.sqrt(global_number_of_reflections_in_bin)

          func += global_weight_for_bin * (1 - math.sqrt(global_sum_of_delta_squared_in_bin/global_number_of_reflections_in_bin))**2

          #if global_sum_of_delta_squared_in_bin == 0:
          #  from IPython import embed;embed()

          der_temp = global_weight_for_bin * (1 / math.sqrt(global_sum_of_delta_squared_in_bin/global_number_of_reflections_in_bin) - 1)  / global_number_of_reflections_in_bin

          der_wrt_sfac  -= der_temp * global_sum_of_der_wrt_sfac_in_bin
          der_wrt_sb    -= der_temp * global_sum_of_der_wrt_sb_in_bin
          der_wrt_sadd  -= der_temp * global_sum_of_der_wrt_sadd_in_bin


    #if self.mpi_helper.rank==0:
    #  self.functional = functional
    #  self.der_wrt_sfac = der_wrt_sfac
    #  self.der_wrt_sb = der_wrt_sb
    #  self.der_wrt_sadd = der_wrt_sadd

    # Broadcast these derivates and functional values to all ranks
    self.functional = comm.bcast(func, root=0)
    self.der_wrt_sfac = comm.bcast(der_wrt_sfac, root=0)
    self.der_wrt_sb = comm.bcast(der_wrt_sb, root=0)
    self.der_wrt_sadd = comm.bcast(der_wrt_sadd, root=0)

    #return (functional, der_wrt_sfac, der_wrt_sb, der_wrt_sadd)

  def calculate_delta_statistics(self):
    '''Calculate min, max, mean, and stddev for the normalized deltas'''
    delta_min = flex.min(self.deltas) if self.deltas.size() > 0 else float('inf')
    delta_max = flex.max(self.deltas) if self.deltas.size() > 0 else float('-inf')
    delta_sum = flex.sum(self.deltas) if self.deltas.size() > 0 else 0.0

    comm = self.mpi_helper.comm
    MPI = self.mpi_helper.MPI

    # global min and max
    self.global_delta_min = comm.allreduce(delta_min, MPI.MIN)
    self.global_delta_max = comm.allreduce(delta_max, MPI.MAX)

    # global mean
    self.global_delta_count = comm.allreduce(self.deltas.size(), MPI.SUM)
    if self.global_delta_count < 20:
      raise ValueError("Too few reflections available for ev11 algorithm")
    global_delta_sum = comm.allreduce(delta_sum, MPI.SUM)
    self.global_delta_mean = global_delta_sum / self.global_delta_count

    # global standard deviation
    array_of_global_delta_means = flex.double(self.deltas.size(), self.global_delta_mean)
    array_of_diffs = self.deltas - array_of_global_delta_means
    array_of_square_diffs = array_of_diffs * array_of_diffs
    sum_of_square_diffs = flex.sum(array_of_square_diffs)
    global_sum_of_square_diffs = comm.allreduce(sum_of_square_diffs, MPI.SUM)
    self.global_delta_stddev = math.sqrt(global_sum_of_square_diffs / (self.global_delta_count - 1))
    if self.mpi_helper.rank == 0:
      self.logger.main_log("Global delta statistics (count,min,max,mean,stddev): (%d,%f,%f,%f,%f)"%(self.global_delta_count, self.global_delta_min, self.global_delta_max, self.global_delta_mean, self.global_delta_stddev))

  def calculate_delta_bin_limits(self):
    '''Divide the delta (min,max) range into "number of ranks" bins. For a balanced rank load, bin limits should be
       chosen so that the bins are equally populated by the deltas. Assuming the normal distribution of deltas,
       we use the probability density function for the bin calculations.'''
    from scipy.stats import norm
    import numpy as np
    cdf_min = norm.cdf(self.global_delta_min, loc=self.global_delta_mean, scale=self.global_delta_stddev)
    cdf_max = norm.cdf(self.global_delta_max, loc=self.global_delta_mean, scale=self.global_delta_stddev)
    self.delta_bin_limits = flex.double()
    for val in np.linspace(cdf_min, cdf_max, self.mpi_helper.size + 1):
      self.delta_bin_limits.append(norm.ppf(val, loc=self.global_delta_mean, scale=self.global_delta_stddev))
    # To fool-proof the binning, set the first and last bin limits to infinity
    self.delta_bin_limits[0]                                = float('-inf')
    self.delta_bin_limits[self.delta_bin_limits.size() - 1] = float('inf')

  def distribute_deltas_over_bins(self):
    '''Have each rank distribute its deltas over the global delta bins'''
    self.logger.log("Delta count: %d"%self.deltas.size())
    self.delta_bins = []
    for bin_begin in range(self.delta_bin_limits.size() - 1):
      test_1 = self.deltas >= flex.double(self.deltas.size(), self.delta_bin_limits[bin_begin])
      test_2 = self.deltas < flex.double(self.deltas.size(), self.delta_bin_limits[bin_begin + 1])
      d = self.deltas.select(test_1 & test_2)
      self.delta_bins.append(d)

    total_deltas_distributed = 0
    for delta_bin in self.delta_bins:
      total_deltas_distributed += delta_bin.size()
    self.logger.log("Total deltas distributed: %d"%total_deltas_distributed)

  def distribute_deltas_over_ranks(self):
    '''Use alltoall to accumulate all deltas of one delta bin at a single rank'''
    new_delta_bins = self.mpi_helper.comm.alltoall(self.delta_bins)

    self.deltas = flex.double()
    for delta_bin in new_delta_bins:
      self.deltas.extend(delta_bin)

    self.deltas = flex.sorted(self.deltas)

    self.logger.log("New deltas count: %d"%self.deltas.size())

  def calculate_delta_rankits(self):
    '''Implement expression (12) of Brewster2019'''
    # Get the base global index for this rank's deltas. Example: if rank 0 has 10 deltas, the first delta on rank 1 will be the 10th global delta.
    delta_count_per_rank = self.mpi_helper.comm.allreduce([self.deltas.size()])
    base_delta_index = sum(delta_count_per_rank[0:self.mpi_helper.rank])
    self.logger.log("Delta base index: %d"%base_delta_index)

    from scitbx.math import distributions
    import numpy as np
    norm = distributions.normal_distribution()

    a = 3./8. if self.global_delta_count < 10. else 0.5

    self.rankits = flex.double()
    for i in range(self.deltas.size()):
      global_delta_index = base_delta_index + i
      rankit = norm.quantile((global_delta_index+1-a)/(self.global_delta_count+1-(2*a)))
      self.rankits.append(rankit)

  def get_overall_correlation_flex(self, data_a, data_b) :
    """
    Correlate any two sets of data.
    @param data_a - references
    @param data_b - observations
    @return tuple containing correlation coefficent, slope and offset.
    """
    import math

    assert len(data_a) == len(data_b)
    corr = 0
    slope = 0
    offset = 0
    try:
      sum_xx = 0
      sum_xy = 0
      sum_yy = 0
      sum_x  = 0
      sum_y  = 0
      N      = 0
      for i in range(len(data_a)):
        I_r       = data_a[i]
        I_o       = data_b[i]
        N      += 1
        sum_xx += I_r**2
        sum_yy += I_o**2
        sum_xy += I_r * I_o
        sum_x  += I_r
        sum_y  += I_o
      slope = (N * sum_xy - sum_x * sum_y) / (N * sum_xx - sum_x**2)
      offset = (sum_xx * sum_y - sum_x * sum_xy) / (N * sum_xx - sum_x**2)
      corr  = (N * sum_xy - sum_x * sum_y) / (math.sqrt(N * sum_xx - sum_x**2) *
                 math.sqrt(N * sum_yy - sum_y**2))
    except ZeroDivisionError:
      pass

    return corr, slope, offset

  def calculate_initial_ev11_parameters(self):
    '''Do a global LS fit of deltas to rankits. Work only in the [0.5,0.5] range of rankits'''
    sum_xx = 0
    sum_yy = 0
    sum_xy = 0
    sum_x  = 0
    sum_y  = 0
    count = 0
    for delta,rankit in zip(self.deltas, self.rankits):
      if rankit >= -0.5 and rankit <= 0.5:
        sum_xx += delta ** 2
        sum_yy += rankit ** 2
        sum_xy += delta * rankit
        sum_x  += delta
        sum_y  += rankit
        count += 1

    comm = self.mpi_helper.comm
    MPI = self.mpi_helper.MPI
    global_sum_xx = comm.reduce(sum_xx, MPI.SUM, root =0)
    global_sum_yy = comm.reduce(sum_yy, MPI.SUM, root =0)
    global_sum_xy = comm.reduce(sum_xy, MPI.SUM, root =0)
    global_sum_x  = comm.reduce(sum_x,  MPI.SUM, root =0)
    global_sum_y  = comm.reduce(sum_y,  MPI.SUM, root =0)
    global_count  = comm.reduce(count,  MPI.SUM, root =0)

    if self.mpi_helper.rank == 0:
      slope = 0
      offset = 0
      corr = 0
      try:
        DELTA = global_count * global_sum_xx - global_sum_x**2 # see p. 105 in Bevington & Robinson
        #assert abs(DELTA) > sys.float_info.epsilon, "Cannot initialize ev11 parameters"
        slope = (global_count * global_sum_xy - global_sum_x * global_sum_y) / DELTA
        offset = (global_sum_xx * global_sum_y - global_sum_x * global_sum_xy) / DELTA
      except ZeroDivisionError:
        pass
      self.logger.main_log("SLOPE: %f; OFFSET: %f"%(slope,offset))

      # Calculate initial EV11 parameters
      self.sfac = 1/slope
      self.sadd = offset
      self.sb = math.sqrt(self.sadd) if self.sadd > 0 else 0

      '''
      if True:
        from matplotlib import pyplot as plt
        import numpy as np
        f = plt.figure(0)
        lim = -5, 5
        x = np.linspace(lim[0],lim[1],100) # 100 linearly spaced numbers
        y = slope * x + offset
        plt.plot(self.deltas, self.rankits, '-')

        #plt.plot(x,y)
        plt.title("CC: %.3f Slope: %.3f Offset: %.3f"%(corr, slope, offset))
        plt.xlabel("Sorted data")
        plt.ylabel("Rankits")
        plt.xlim(lim); plt.ylim(lim)
        plt.axes().set_aspect('equal')

        f = plt.figure(1)
        h = flex.histogram(self.deltas, n_slots=100, data_min = lim[0], data_max = lim[1])
        stats = flex.mean_and_variance(self.deltas)
        plt.plot(h.slot_centers().as_numpy_array(), h.slots().as_numpy_array(), '-')
        plt.xlim(lim)
        plt.xlabel("Sorted data")
        plt.ylabel("Count")
        plt.title("Normalized data mean: %.3f +/- %.3f"%(stats.mean(), stats.unweighted_sample_standard_deviation()))

        #plt.show()

        if True:
          plt.ion()
          plt.pause(0.05)
      '''

      initial_params = (self.sfac, self.sadd, self.sb)
    else:
      initial_params = None

    initial_params = self.mpi_helper.comm.bcast(initial_params, root=0)
    self.sfac  = initial_params[0]
    self.sadd  = initial_params[1]
    self.sb    = initial_params[2]

  def distribute_reflections_over_intensity_bins(self):
    self.intensity_bins = []
    count = self.work_table.size()

    for bin_begin in range(number_of_intensity_bins):
      self.intensity_bins.append(flex.reflection_table())

      test_1 = self.work_table['biased_mean'] >= flex.double(count, self.intensity_bin_limits[bin_begin])
      test_2 = self.work_table['biased_mean'] < flex.double(count, self.intensity_bin_limits[bin_begin + 1])

      sel = self.work_table.select(test_1 & test_2)

      self.intensity_bins[bin_begin].extend(sel)

    # for debugging
    number_of_refls_distributed = 0
    for intensity_bin in self.intensity_bins:
      number_of_refls_distributed += intensity_bin.size()
    self.logger.log("Distributed over intensity bins %d out of %d reflections"%(number_of_refls_distributed, count))

  def run_minimizer(self):
    comm = self.mpi_helper.comm
    MPI = self.mpi_helper.MPI
    size = self.mpi_helper.size
    self.n = 3
    self.x = flex.double([self.sfac, self.sb, self.sadd])
    self.logger.main_log('Initial Parameter Estimates = sdfac: %.2f  sdb: %.2f  sdadd: %.2f'%(self.sfac, self.sb, self.sadd))
    if True:
      from scitbx import lbfgsb
      l = flex.double(self.n, 1e-8)

      if len(l) > 3:
        for p in range(7,len(l)):
          l[p] = 1e-15 # g*

      if self.mpi_helper.rank == 0:
        self.minimizer = lbfgsb.minimizer(
          n = self.n,
          l = l,
          u = flex.double(self.n, 0),
          nbd = flex.int(self.n, 1),
        )
      while True:
        self.compute_functional_and_gradients()
        status=-1
        if self.mpi_helper.rank == 0:
          if self.minimizer.process(self.x, self.f, self.g):
            self.logger.main_log('intermediate minimization results = functional: %.2f  sdfac: %.2f sdb: %.2f sdadd: %.2f' %(self.f,self.x[0],self.x[1], self.x[2]))
            status=1
            self.sfac = self.x[0]
            self.sb = self.x[1]
            self.sadd = self.x[2]
          elif self.minimizer.is_terminated():
            status=0

        comm.barrier()
        status=comm.bcast(status, root=0)
        if status==1:
          self.sfac=comm.bcast(self.sfac, root=0)
          self.sb=comm.bcast(self.sb, root=0)
          self.sadd=comm.bcast(self.sadd, root=0)
          pass
        if status==0:
          break
    if self.mpi_helper.rank == 0:
      self.logger.main_log('FINAL SDFAC VALUES = functional: %.2f  sdfac: %.2f sdb: %.2f sdadd: %.2f' %(self.f,self.x[0],self.x[1], self.x[2]))

  def compute_functional_and_gradients(self):
    self.calculate_functional_ev11()
    self.f = self.functional
    self.g = flex.double([self.der_wrt_sfac, self.der_wrt_sb, self.der_wrt_sadd])
    return self.f, self.g

  def modify_errors(self, reflections):

    # First set up a reflection table to do work downstream. Needed to calculate delta_sq and bin reflections
    reflections = self.setup_work_arrays(reflections)
    # Make sure every rank knows the global mean/stdev for deltas and use them to get the bin limits
    self.calculate_delta_statistics()
    self.calculate_delta_bin_limits()
    # assign deltas for each reflection to appropriate bin
    self.distribute_deltas_over_bins()
    # Each rank gets its own bin. Make sure all deltas in that bin are on that rank and sorted.
    self.distribute_deltas_over_ranks()
    # calculate rankits, each rank does its own rankits calculation
    self.calculate_delta_rankits()
    # initial ev11 params using slope and offset of fit to rankits
    self.calculate_initial_ev11_parameters()
    # Now moving to intensities, find the bin limits using global min/max of the means of each reflection
    self.calculate_intensity_bin_limits()
    # Once bin limits are determined, assign intensities on each rank to appropriate bin limits
    self.distribute_reflections_over_intensity_bins()
    # Run LBFGSB minimizer -- only rank0 does minimization but gradients/functionals are calculated using all rank
    self.run_minimizer()
    # Finally update the variances of each reflection as per Eq (10) in Brewster et. al (2019)
    reflections['intensity.sum.variance'] = (self.sfac**2)*(reflections['intensity.sum.variance'] +
                                                            self.sb*self.sb*reflections['biased_mean'] +
                                                            self.sadd*self.sadd*reflections['biased_mean']**2)

    del reflections['biased_mean']

    return reflections

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(error_modifier)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/errors/error_modifier_ha14.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from dials.array_family import flex
from xfel.merging.application.worker import worker

class error_modifier_ha14(worker):

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(error_modifier_ha14, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Adjust intensity errors -- ha14 method'

  def run(self, experiments, reflections):
    '''Modify intensity errors according to the ha14 error model'''
    assert self.params.merging.error.model == "ha14"

    new_reflections = flex.reflection_table()
    self.logger.log_step_time("ERROR_MODIFIER_HA14")
    if len(reflections) > 0:
      self.logger.log("Modifying intensity errors -- ha14 method...")
      for expt_id in range(len(experiments)):
        refls = reflections.select(reflections['id'] == expt_id)
        refls = self.modify_errors(refls)
        new_reflections.extend(refls)
    self.logger.log_step_time("ERROR_MODIFIER_HA14", True)

    return experiments, new_reflections

  def modify_errors(self, reflections):
    '''Formerly sdfac_auto, ha14 method applies sdfac to each-image data assuming negative intensities are normally distributed noise'''
    assert 0 == (reflections['intensity.sum.variance'] <= 0.0).count(True)
    I_over_sig = reflections['intensity.sum.value'] / flex.sqrt(reflections['intensity.sum.variance'])
    negative_I_over_sig = I_over_sig.select(I_over_sig < 0.)
    #assert that at least a few I/sigmas are less than zero
    negative_I_over_sig_count = I_over_sig.select(I_over_sig < 0.).size()
    if negative_I_over_sig_count > 2:
      # get a rough estimate for the SDFAC, assuming that negative measurements
      # represent false predictions and therefore normally distributed noise.
      no_signal = negative_I_over_sig
      for xns in range(len(no_signal)):
        no_signal.append(-no_signal[xns])
      Stats = flex.mean_and_variance(no_signal)
      SDFAC = Stats.unweighted_sample_standard_deviation()
    else:
      SDFAC = 1.
    reflections['intensity.sum.variance'] *= (SDFAC**2)
    if self.params.output.log_level == 0:
      self.logger.log("The applied SDFAC is %7.4f"%SDFAC)
    return reflections

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(error_modifier)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/errors/error_modifier_mm24.py
from __future__ import absolute_import, division, print_function
from dials.array_family import flex
import math
import numpy as np
import os
import scipy.optimize
from scipy.special import gamma
from scipy.special import polygamma
import scipy.stats
from xfel.merging.application.worker import worker
from xfel.merging.application.reflection_table_utils import reflection_table_utils

class error_modifier_mm24(worker):
  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(error_modifier_mm24, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)
    if not self.params.merging.error.mm24.expected_gain is None:
      self.expected_sf = math.sqrt(self.params.merging.error.mm24.expected_gain)
    else:
      self.expected_sf = None
    if self.params.merging.error.mm24.n_max_differences is None:
      self.limit_differences = True
    else:
      self.limit_differences = False

    self.tuning_param = self.params.merging.error.mm24.tuning_param

    # Currently the use of intensity bins is in evaluation. Historically this algorithm has used
    # 100 intensity bins solely because Phil Evans did in the Evans 2011 paper. Recent evaluations
    # showed that removing binning does not harm the data processing. If anything, it might lead to
    # a tiny improvement.
    # To use the method without binning, set params.merging.error.mm24.number_of_intensity_bins = 0
    # In the future, if binning is decided to be removed, delete the methods:
    #   self.calculate_functional_binning
    #   self.calculate_intensity_bin_limits()
    #   self.distribute_differences_over_intensity_bins()
    # To keep the binning, delete the methods:
    #   self.calculate_functional_no_bining
    self.number_of_intensity_bins = self.params.merging.error.mm24.number_of_intensity_bins
    if self.number_of_intensity_bins > 0:
      self.calculate_functional = self.calculate_functional_binning
    else:
      self.calculate_functional = self.calculate_functional_no_bining
    if self.params.merging.error.mm24.constant_sadd:
      self.cc_key = None
    else:
      if self.params.merging.error.mm24.cc_after_pr:
        self.cc_key = 'correlation_after_post'
      else:
        self.cc_key = 'correlation'

  def __repr__(self):
    return 'Adjust intensity errors -- mm24'

  def run(self, experiments, reflections):
    '''Modify intensity errors according to Mittan-Moreau 202X'''
    assert self.params.merging.error.model == "mm24"
    self.logger.log_step_time("ERROR_MODIFIER_MM24")
    self.logger.log("Modifying intensity errors -- mm24 method (starting with %d reflections)"%(len(reflections)))
    reflections = self.modify_errors(reflections)
    self.logger.log_step_time("ERROR_MODIFIER_MM24", True)
    return experiments, reflections

  def modify_errors(self, reflections):
    # First set up a reflection table to do work downstream.
    reflections = self.setup_work_arrays(reflections)
    # Now moving to intensities, find the bin limits using global min/max of the means of each reflection
    # Once bin limits are determined, assign intensities on each rank to appropriate bin limits
    if self.number_of_intensity_bins > 0:
      self.calculate_intensity_bin_limits()
      self.distribute_differences_over_intensity_bins()
    # Intialize the s_fac and s_add parameters
    self.initialize_mm24_params()
    # Run LBFGSB minimizer
    #  -- only rank0 does minimization but gradients/functionals are calculated using all rank
    self.run_minimizer()

    if self.params.merging.error.mm24.do_diagnostics:
      self.plot_diagnostics(reflections)
    # Finally update the variances of each reflection as per Eq (10) in Brewster et. al (2019)
    if self.cc_key:
      correlation = reflections[self.cc_key]
    else:
      correlation = None
    reflections['intensity.sum.variance'] = self._get_var_mm24(
      reflections['intensity.sum.variance'],
      reflections['biased_mean'],
      correlation
      )
    del reflections['biased_mean']
    return reflections

  def setup_work_arrays(self, reflections):
    def pairing(k1, k2):
      return int((k1 + k2) * (k1 + k2 + 1) / 2 + k2)

    self.work_table = flex.reflection_table()
    self.refl_biased_means = []
    biased_mean = flex.double() # Go with the pairwise differences in self.work_table
    biased_mean_to_reflections = flex.double() # Put into the original reflection table
    self.biased_mean_count = flex.double() # Used to calculate the number of reflections in each intensity bin
    pairwise_differences = flex.double()
    pairwise_differences_normalized = flex.double()
    counting_stats_var_i = flex.double()
    counting_stats_var_j = flex.double()
    if self.cc_key:
      correlation_i = flex.double()
      correlation_j = flex.double()
    number_of_reflections = 0

    for refls in reflection_table_utils.get_next_hkl_reflection_table(reflections):
      number_of_measurements = refls.size()
      # if the returned "refls" list is empty, it's the end of the input "reflections" list
      if number_of_measurements == 0:
        break
      refls_biased_mean = flex.double(len(refls), flex.mean(refls['intensity.sum.value']))
      biased_mean_to_reflections.extend(refls_biased_mean)
      self.refl_biased_means.append(refls_biased_mean[0])
      if number_of_measurements > self.params.merging.minimum_multiplicity:
        I = refls['intensity.sum.value'].as_numpy_array()
        var_cs = refls['intensity.sum.variance'].as_numpy_array()
        if self.cc_key:
          correlation = refls[self.cc_key].as_numpy_array()
        number_of_reflections += I.size
        self.biased_mean_count.extend(flex.double(I.size, refls_biased_mean[0]))
        indices = np.triu_indices(n=I.size, k=1)
        N = indices[0].size
        if self.limit_differences == False:
          if N > self.params.merging.error.mm24.n_max_differences:
            # random number generation needs to be consistent between symmetry related reflections
            # for reproducibility
            hkl = refls[0]['miller_index_asymmetric']
            # Convert hkl to a hash with Cantor's pairing function.
            # Add 1000 to keep inputs positive.
            hkl_hash = pairing(pairing(hkl[0]+1000, hkl[1]+1000), hkl[2]+1000)
            rng = np.random.default_rng(seed=hkl_hash + self.params.merging.error.mm24.random_seed)
            # Reflections are in different order when run with different numbers of ranks
            sort_indices = np.argsort(I)
            rng.shuffle(sort_indices)
            I = I[sort_indices]
            var_cs = var_cs[sort_indices]
            if self.cc_key:
              correlation = correlation[sort_indices]
            # this option is for performance trade-offs
            if N > 1000:
              subset_indices = rng.choice(
                N,
                size=self.params.merging.error.mm24.n_max_differences,
                replace=False,
                shuffle=False
                )
            else:
              subset_indices = rng.permutation(N)[:self.params.merging.error.mm24.n_max_differences]
            indices = (indices[0][subset_indices], indices[1][subset_indices])
            N = self.params.merging.error.mm24.n_max_differences
        differences = flex.double(np.abs(I[indices[0]] - I[indices[1]]))
        pairwise_differences.extend(differences)
        biased_mean.extend(flex.double(N, refls_biased_mean[0]))
        counting_stats_var_i.extend(flex.double(var_cs[indices[0]]))
        counting_stats_var_j.extend(flex.double(var_cs[indices[1]]))
        if self.cc_key:
          correlation_i.extend(flex.double(correlation[indices[0]]))
          correlation_j.extend(flex.double(correlation[indices[1]]))

    self.work_table['pairwise_differences'] = pairwise_differences
    self.work_table['biased_mean'] = biased_mean
    self.work_table['counting_stats_var_i'] = counting_stats_var_i
    self.work_table['counting_stats_var_j'] = counting_stats_var_j
    if self.cc_key:
      self.work_table['correlation_i'] = correlation_i
      self.work_table['correlation_j'] = correlation_j
    reflections['biased_mean'] = biased_mean_to_reflections

    self.logger.log(f"Number of work reflections selected: {number_of_reflections}")
    return reflections

  def calculate_intensity_bin_limits(self):
    '''Calculate the intensity bins between the 0.5 and 99.5 percentiles'''
    all_biased_means = self.mpi_helper.gather_variable_length_numpy_arrays(
      np.array(self.refl_biased_means, dtype=float), root=0, dtype=float
      )
    if self.mpi_helper.rank == 0:
      all_biased_means = np.sort(all_biased_means)
      lower_percentile = 0.005
      upper_percentile = 0.995
      n = all_biased_means.size
      lower = all_biased_means[int(lower_percentile * n)]
      upper = all_biased_means[int(upper_percentile * n)]
      self.intensity_bin_limits = np.linspace(lower, upper, self.number_of_intensity_bins + 1)
    else:
      self.intensity_bin_limits = np.empty(self.number_of_intensity_bins + 1)
    self.mpi_helper.comm.Bcast(self.intensity_bin_limits, root=0)

  def distribute_differences_over_intensity_bins(self):
    self.intensity_bins = [flex.reflection_table() for i in range(self.number_of_intensity_bins)]
    self.n_differences_in_bin = flex.double(self.number_of_intensity_bins, 0)
    self.n_refls_in_bin = flex.double(self.number_of_intensity_bins, 0)
    self.bin_weighting = flex.double(self.number_of_intensity_bins, 0)
    count = self.work_table.size()
    for bin_index in range(self.number_of_intensity_bins):
      subset_work_table = self.work_table.select(
        (self.work_table['biased_mean'] >= self.intensity_bin_limits[bin_index])
        & (self.work_table['biased_mean'] < self.intensity_bin_limits[bin_index + 1])
        )
      self.intensity_bins[bin_index].extend(subset_work_table)
      self.n_differences_in_bin[bin_index] = self.mpi_helper.comm.allreduce(
        len(subset_work_table), self.mpi_helper.MPI.SUM
        )

      subset_biased_mean = self.biased_mean_count.select(
        (self.biased_mean_count >= self.intensity_bin_limits[bin_index])
        & (self.biased_mean_count < self.intensity_bin_limits[bin_index + 1])
        )
      self.n_refls_in_bin[bin_index] = self.mpi_helper.comm.allreduce(
        len(subset_biased_mean), self.mpi_helper.MPI.SUM
        )
      if self.n_differences_in_bin[bin_index] > 0:
        self.bin_weighting[bin_index] = math.sqrt(self.n_refls_in_bin[bin_index]) / self.n_differences_in_bin[bin_index]

    # for debugging
    number_of_differences_distributed = 0
    for intensity_bin in self.intensity_bins:
      number_of_differences_distributed += intensity_bin.size()
    self.logger.log(
      "Distributed over intensity bins %d out of %d differences"
      % (number_of_differences_distributed, count)
      )

  def initialize_mm24_params(self):
    if len(self.refl_biased_means) == 0:
      maximum_intensity = 0
    else:
      maximum_intensity = max(self.refl_biased_means)
    upper = self.mpi_helper.comm.reduce(
        maximum_intensity,
        op=self.mpi_helper.MPI.MAX,
        root=0
        )
    n_bins = 100
    if self.mpi_helper.rank == 0:
      intensity_bins = np.linspace(0, 0.1*upper, n_bins + 1)
      bin_centers = (intensity_bins[1:] + intensity_bins[:-1]) / 2
    else:
      intensity_bins = np.zeros(n_bins + 1)
    self.mpi_helper.comm.Bcast(intensity_bins, root=0)
    if len(self.refl_biased_means) == 0:
      summation_rank = np.zeros(n_bins)
      counts_rank = np.zeros(n_bins)
    else:
      biased_mean_rank = self.work_table['biased_mean'].as_numpy_array()
      pairwise_differences_rank = self.work_table['pairwise_differences'].as_numpy_array()
      summation_rank, _ = np.histogram(
        biased_mean_rank,
        bins=intensity_bins,
        weights=pairwise_differences_rank
      )
      counts_rank, _ = np.histogram(
        biased_mean_rank,
        bins=intensity_bins
      )
    summation = np.zeros(n_bins)
    counts = np.zeros(n_bins, dtype=int)
    self.mpi_helper.comm.Reduce(summation_rank, summation, op=self.mpi_helper.MPI.SUM, root=0)
    self.mpi_helper.comm.Reduce(counts_rank, counts, op=self.mpi_helper.MPI.SUM, root=0)

    if self.mpi_helper.rank == 0:
      def fitting_equation(params, bin_centers, mean_differences_0, return_jac):
        sf = params[0]
        sadd = params[1]
        prefactor = 2 / np.sqrt(np.pi)
        arg = sf**2 * (bin_centers + sadd**2 * bin_centers**2)
        curve = prefactor * np.sqrt(arg) + mean_differences_0
        if return_jac:
          darg_dsf = 2 * sf * (bin_centers + sadd**2 * bin_centers**2)
          darg_dsadd = 2 * sf**2 * sadd * bin_centers**2
          dcurve_darg = 1/2 * prefactor/np.sqrt(arg)
          dcurve_dsf = dcurve_darg * darg_dsf
          dcurve_dsadd = dcurve_darg * darg_dsadd
          return curve, dcurve_dsf, dcurve_dsadd
        else:
          return curve

      def target_fun_bfgs(params, bin_centers, mean_differences):
        curve, dcurve_dsf, dcurve_dsadd = fitting_equation(params, bin_centers, mean_differences[0], True)
        arg = curve - mean_differences
        loss = 0.5 * np.sum(arg**2)
        dloss_dsf = np.sum(arg * dcurve_dsf)
        dloss_dsadd = np.sum(arg * dcurve_dsadd)
        return loss, (dloss_dsf, dloss_dsadd)

      def target_fun_scalar(sadd, bin_centers, mean_differences):
        curve = fitting_equation([self.expected_sf, sadd], bin_centers, mean_differences[0], False)
        arg = curve - mean_differences
        loss = 0.5 * np.sum(arg**2)
        return loss

      good_indices = counts > 0
      mean_differences = summation[good_indices] / counts[good_indices]
      bin_centers = bin_centers[good_indices]

      if self.cc_key:
        self.sadd = [0, 0.001, 0.001]
      else:
        self.sadd = [0]
      if self.expected_sf is None:
        results = scipy.optimize.minimize(
          target_fun_bfgs,
          x0=(1, 1),
          args=(bin_centers, mean_differences),
          jac=True,
          method='BFGS'
          )
        self.sfac = abs(float(results.x[0]))
        self.sadd[0] = math.sqrt(abs(float(results.x[1])))
        fit_curve = fitting_equation(results.x, bin_centers, mean_differences[0], False)
      else:
        results = scipy.optimize.minimize_scalar(
          target_fun_scalar,
          bounds=(0, 10),
          args=(bin_centers, mean_differences),
          )
        self.sfac = self.expected_sf
        self.sadd[0] = math.sqrt(abs(float(results.x)))

      if self.params.merging.error.mm24.do_diagnostics:
        import matplotlib.pyplot as plt
        fit_curve = fitting_equation([self.sfac, self.sadd[0]], bin_centers, mean_differences[0], False)
        fig, axes = plt.subplots(1, 1, figsize=(5, 3))
        axes.plot(
          bin_centers, mean_differences,
          linestyle='none', marker='.', color=[0, 0, 0], label='Data'
          )
        axes.plot(bin_centers, fit_curve, color=[0, 0.8, 0], label='Initialization')
        axes.legend()
        fig.tight_layout()
        fig.savefig(os.path.join(
          self.params.output.output_dir,
          self.params.output.prefix + '_initial_differences.png'
          ))
        plt.close()

    else:
      self.sfac = 0
      if self.cc_key:
        self.sadd = [0, 0, 0]
      else:
        self.sadd = [0]
    self.sfac = self.mpi_helper.comm.bcast(self.sfac, root=0)
    self.sadd = self.mpi_helper.comm.bcast(self.sadd, root=0)

  def run_minimizer(self):
    from scitbx import lbfgsb

    comm = self.mpi_helper.comm
    MPI = self.mpi_helper.MPI
    size = self.mpi_helper.size

    if self.params.merging.error.mm24.tuning_param_opt:
      param_shift = 1
      self.x = flex.double([self.tuning_param, self.sfac, *self.sadd])
    else:
      param_shift = 0
      self.x = flex.double([self.sfac, *self.sadd])
    n_parameters = len(self.x)
    l = flex.double(n_parameters, 1e-8)
    u = flex.double(n_parameters, 0)
    if self.params.merging.error.mm24.tuning_param_opt:
      # normalization for the truncated t-distribution is numerically unstable for nu < 2
      l[0] = 2.5
      if self.x[0] < 2.5:
        self.x[0] = 2.5
    for degree_index in range(param_shift, n_parameters):
      l[degree_index] = -1000
    if self.mpi_helper.rank == 0:
      self.minimizer = lbfgsb.minimizer(
        n = n_parameters,
        l = l,
        u = u,
        nbd = flex.int(n_parameters, 1),
      )
    if self.mpi_helper.rank == 0:
      self.logger.main_log(
        'Initial Parameter Estimates = '
        + f'sfac: {self.sfac} '
        + f'sadd: {self.sadd[0]} '
        + f'nu: {self.tuning_param} '
      )
    while True:
      self.compute_functional_and_gradients()
      status = -1
      if self.mpi_helper.rank == 0:
        if self.minimizer.process(self.x, self.L, self.g):
          if self.params.merging.error.mm24.tuning_param_opt:
            self.tuning_param = self.x[0]
            tuning_param = f'{self.tuning_param:0.3f}'
          self.sfac = self.x[0 + param_shift]
          self.sadd = self.x[1 + param_shift:]
          log_out = 'intermediate minimization results = '\
            + f'loss: {self.L:.2f} '\
            + f'sfac: {self.sfac:0.3f} '\
            + f'sadd: {[f"{value:0.3f}" for value in self.sadd]} '
          if self.params.merging.error.mm24.tuning_param_opt:
            log_out += f'nu: {tuning_param}'
          self.logger.main_log(log_out)
          status = 1
        elif self.minimizer.is_terminated():
          status=0

      comm.Barrier()
      status = comm.bcast(status, root=0)
      if status == 1:
        self.tuning_param = comm.bcast(self.tuning_param, root=0)
        self.sfac = comm.bcast(self.sfac, root=0)
        self.sadd = comm.bcast(self.sadd, root=0)
        pass
      if status==0:
        break

    if self.mpi_helper.rank == 0:
      tuning_param = f'{self.tuning_param:0.3f}'
      log_out = 'FINAL mm24 VALUES = '\
        + f'loss: {self.L:.2f} '\
        + f'sfac: {self.sfac:0.3f} '\
        + f'sadd: {[f"{value:0.3f}" for value in self.sadd]} '
      if self.params.merging.error.mm24.tuning_param_opt:
        log_out += f'nu: {tuning_param}'
      self.logger.main_log(log_out)

  def compute_functional_and_gradients(self):
    self.calculate_functional()
    if self.mpi_helper.rank == 0:
      if self.params.merging.error.mm24.tuning_param_opt:
        self.g = flex.double([self.dL_dnu, self.dL_dsfac, *self.dL_dsadd])
      else:
        self.g = flex.double([self.dL_dsfac, *self.dL_dsadd])

  def verify_derivatives(self):
    shift = 0.000001
    import copy

    self.calculate_functional()
    sfac = copy.copy(self.sfac)
    sadd = copy.copy(self.sadd)
    tuning_param = copy.copy(self.tuning_param)
    if self.mpi_helper.rank == 0:
      TF = copy.copy(self.L)
      der_wrt_sfac = copy.copy(self.dL_dsfac)
      der_wrt_sadd = copy.copy(self.dL_dsadd)
      if self.params.merging.error.mm24.tuning_param_opt:
        der_wrt_nu = copy.copy(self.dL_dnu)

    # Tuning param
    if self.params.merging.error.mm24.tuning_param_opt:
      self.tuning_param = tuning_param * (1 + shift)
      self.calculate_functional()
      if self.mpi_helper.rank == 0:
        TF_p = copy.copy(self.L)
      self.tuning_param = tuning_param * (1 - shift)
      self.calculate_functional()
      if self.mpi_helper.rank == 0:
        TF_m = copy.copy(self.L)
      self.tuning_param = tuning_param
      if self.mpi_helper.rank == 0:
        check_der_wrt_nu = (TF_p - TF_m) / (2 * shift * tuning_param)
        print(f'der_wrt_nu numerical: {check_der_wrt_nu} analytical {der_wrt_nu}')

    # sfac
    self.sfac = sfac * (1 + shift)
    self.calculate_functional()
    if self.mpi_helper.rank == 0:
      TF_p = copy.copy(self.L)
    self.sfac = sfac * (1 - shift)
    self.calculate_functional()
    if self.mpi_helper.rank == 0:
      TF_m = copy.copy(self.L)
    self.sfac = sfac
    if self.mpi_helper.rank == 0:
      check_der_wrt_sfac = (TF_p - TF_m) / (2 * shift * sfac)
      print(f'der_wrt_sfac numerical: {check_der_wrt_sfac} analytical {der_wrt_sfac}')

    # sadd:
    for degree_index in range(len(self.sadd)):
      if sadd[degree_index] == 0:
        self.sadd[degree_index] = shift
      else:
        self.sadd[degree_index] = sadd[degree_index] * (1 + shift)
      self.calculate_functional()
      if self.mpi_helper.rank == 0:
        TF_p = copy.copy(self.L)
      if sadd[degree_index] == 0:
        self.sadd[degree_index] = -shift
      else:
        self.sadd[degree_index] = sadd[degree_index] * (1 - shift)
      self.calculate_functional()
      if self.mpi_helper.rank == 0:
        TF_m = copy.copy(self.L)
      self.sadd[degree_index] = sadd[degree_index]
      if self.mpi_helper.rank == 0:
        if sadd[degree_index] == 0:
          check_der_wrt_sadd = (TF_p - TF_m) / (2 * shift)
        else:
          check_der_wrt_sadd = (TF_p - TF_m) / (2 * shift * sadd[degree_index])
        print(
          f'der_wrt_sadd - degree {degree_index} '
          + f'numerical: {check_der_wrt_sadd} '
          + f'analytical {der_wrt_sadd[degree_index]}'
          )

  def _loss_function_normal(self, differences, var_i, var_j):
    var = var_i + var_j
    z = differences / flex.sqrt(var)
    dz_dvar = -differences / (2 * var**(3/2))
    L1 = 1/2*flex.log(var)
    dL1_dvar = 1/2 * 1/var
    L2 = 1/2 * z**2
    dL2_dz = z
    L = L1 + L2
    dL_dvar_x = dL1_dvar + dL2_dz * dz_dvar
    return L, dL_dvar_x

  def _loss_function_t(self, differences, var_i, var_j):
    v = self.tuning_param
    var = var_i + var_j
    z = differences / flex.sqrt(var)
    dz_dvar = -differences / (2 * var**(3/2))
    arg = 1 + 1/v * z**2
    darg_dz = 2*z/v

    L1 = 1/2 * flex.log(var)
    dL1_dvar = 1/2 * 1/var
    L2 = (v+1)/2 * flex.log(arg)
    dL2_darg = (v+1)/2 * 1/arg
    dL2_dvar = dL2_darg * darg_dz * dz_dvar
    L = L1 + L2
    dL_dvar_x = dL1_dvar + dL2_dvar
    return L, dL_dvar_x

  def _loss_function_t_v_opt(self, differences, var_i, var_j):
    v = self.tuning_param
    var = var_i + var_j
    z = differences / flex.sqrt(var)
    dz_dvar = -differences / (2 * var**(3/2))
    arg = 1 + 1/v * z**2
    darg_dz = 2*z/v
    darg_dv = -z**2 / v**2
    darg_dvar = darg_dz * dz_dvar

    L0 = -math.log(gamma((v+1)/2))
    dL0_dv = -float(polygamma(0, (v+1)/2) * 1/2)

    L1 = 1/2 * math.log(np.pi)

    L2 = 1/2 * math.log(v)
    dL2_dv = 1 / (2*v)

    L3 = math.log(gamma(v/2))
    dL3_dv = float(polygamma(0, v/2) * 1/2)

    L4 = 1/2 * flex.log(var)
    dL4_dvar = 1/2 * 1/var

    L5 = (v+1)/2 * flex.log(arg)
    dL5_dvar = (v+1)/2 * 1/arg * darg_dvar
    dL5_dv = 1/2 * flex.log(arg) + (v+1)/2 * 1/arg * darg_dv

    L = L0 + L1 + L2 + L3 + L4 + L5
    dL_dvar = dL4_dvar + dL5_dvar
    dL_dv = dL0_dv + dL2_dv + dL3_dv + dL5_dv
    return L, dL_dvar, dL_dv

  def _get_sadd2(self, correlation):
    if correlation:
      term1 = flex.exp(-self.sadd[1] * correlation)
      sadd2 = self.sadd[0]**2 * term1 + self.sadd[2]**2
      dsadd2_dsaddi = [
        2 * self.sadd[0] * term1,
        -correlation * self.sadd[0]**2 * term1,
        2 * self.sadd[2] * flex.double(len(correlation), 1)
        ]
    else:
      sadd2 = self.sadd[0]**2
      dsadd2_dsaddi = [2 * self.sadd[0]]
    return sadd2, dsadd2_dsaddi

  def _get_var_mm24(self, counting_err, biased_mean, correlation, return_der=False):
    sadd2, dsadd2_dsaddi = self._get_sadd2(correlation)
    var = self.sfac**2 * (counting_err + sadd2 * biased_mean**2)
    if return_der:
      dvar_dsfac = 2 * self.sfac * (counting_err + sadd2 * biased_mean**2)
      dvar_dsadd2 = self.sfac**2 * biased_mean**2
      return var, dvar_dsfac, dvar_dsadd2, dsadd2_dsaddi
    else:
      return var

  def calculate_functional_no_bining(self):
    comm = self.mpi_helper.comm
    MPI = self.mpi_helper.MPI

    if self.cc_key:
      correlation_i = self.work_table['correlation_i']
      correlation_j = self.work_table['correlation_j']
    else:
      correlation_i = None
      correlation_j = None
    var_i, dvar_i_dsfac, dvar_i_dsadd2, dsadd2_i_dsaddi = self._get_var_mm24(
      self.work_table['counting_stats_var_i'],
      self.work_table['biased_mean'],
      correlation_i,
      return_der=True
      )
    var_j, dvar_j_dsfac, dvar_j_dsadd2, dsadd2_j_dsaddi = self._get_var_mm24(
      self.work_table['counting_stats_var_j'],
      self.work_table['biased_mean'],
      correlation_j,
      return_der=True
      )

    if self.params.merging.error.mm24.likelihood == 'normal':
      L_rank, dL_dvar_x = self._loss_function_normal(
        self.work_table['pairwise_differences'], var_i, var_j
        )
    elif self.params.merging.error.mm24.likelihood == 't-dist':
      if self.params.merging.error.mm24.tuning_param_opt:
        L_rank, dL_dvar_x, dL_dnu = self._loss_function_t_v_opt(
          self.work_table['pairwise_differences'], var_i, var_j
          )
        dL_dnu_rank = flex.sum(dL_dnu)
      else:
        L_rank, dL_dvar_x = self._loss_function_t(
          self.work_table['pairwise_differences'], var_i, var_j
          )

    dL_dsfac_rank = flex.sum(dL_dvar_x * (dvar_i_dsfac + dvar_j_dsfac))
    dL_dsadd_rank = [None for _ in range(len(self.sadd))]
    for degree_index in range(len(self.sadd)):
      dL_dsadd_rank[degree_index] = flex.sum(dL_dvar_x * (
        dvar_i_dsadd2 * dsadd2_i_dsaddi[degree_index] + dvar_j_dsadd2 * dsadd2_j_dsaddi[degree_index]
        ))

    self.L = comm.reduce(flex.sum(L_rank), MPI.SUM, root=0)
    self.dL_dsfac = comm.reduce(dL_dsfac_rank, MPI.SUM, root=0)
    self.dL_dsadd = [None for _ in range(len(self.sadd))]
    for degree_index in range(len(self.sadd)):
      self.dL_dsadd[degree_index] = comm.reduce(dL_dsadd_rank[degree_index], MPI.SUM, root=0)
    if self.params.merging.error.mm24.tuning_param_opt:
      self.dL_dnu = comm.reduce(dL_dnu_rank, MPI.SUM, root=0)

  def calculate_functional_binning(self):
    comm = self.mpi_helper.comm
    MPI = self.mpi_helper.MPI
    L_bin_rank = flex.double(self.number_of_intensity_bins, 0)
    dL_dsfac_bin_rank = flex.double(self.number_of_intensity_bins, 0)
    dL_dsadd_bin_rank = [flex.double(self.number_of_intensity_bins, 0) for i in range(3)]
    if self.params.merging.error.mm24.tuning_param_opt:
      dL_dnu_bin_rank = flex.double(self.number_of_intensity_bins, 0)

    for bin_index, differences in enumerate(self.intensity_bins):
      if len(differences) > 0:
        var_i, dvar_i_dsfac, dvar_i_dsadd2, dsadd2_i_dsaddi = self._get_var_mm24(
          differences['counting_stats_var_i'],
          differences['biased_mean'],
          differences['correlation_i'],
          return_der=True
          )
        var_j, dvar_j_dsfac, dvar_j_dsadd2, dsadd2_j_dsaddi = self._get_var_mm24(
          differences['counting_stats_var_j'],
          differences['biased_mean'],
          differences['correlation_j'],
          return_der=True
          )

        if self.params.merging.error.mm24.likelihood == 'normal':
          L_in_bin, dL_dvar_x = self._loss_function_normal(
            differences['pairwise_differences'], var_i, var_j
            )
        elif self.params.merging.error.mm24.likelihood == 't-dist':
          if self.params.merging.error.mm24.tuning_param_opt:
            L_in_bin, dL_dvar_x, dL_dnu = self._loss_function_t_v_opt(
              differences['pairwise_differences'], var_i, var_j
              )
            dL_dnu_bin_rank[bin_index] = flex.sum(dL_dnu)
          else:
            L_in_bin, dL_dvar_x = self._loss_function_t(
              differences['pairwise_differences'], var_i, var_j
              )

        L_bin_rank[bin_index] = flex.sum(L_in_bin)
        dL_dsfac_bin_rank[bin_index] = flex.sum(dL_dvar_x * (dvar_i_dsfac + dvar_j_dsfac))
        for degree_index in range(3):
          dL_dsadd_bin_rank[degree_index][bin_index] = flex.sum(dL_dvar_x * (
            dvar_i_dsadd2 * dsadd2_i_dsaddi[degree_index] + dvar_j_dsadd2 * dsadd2_j_dsaddi[degree_index]
            ))

    L_bin = comm.reduce(L_bin_rank, MPI.SUM, root=0)
    dL_dsfac_bin = comm.reduce(dL_dsfac_bin_rank, MPI.SUM, root=0)
    dL_dsadd_bin = [None for i in range(3)]
    for degree_index in range(3):
      dL_dsadd_bin[degree_index] = comm.reduce(dL_dsadd_bin_rank[degree_index], MPI.SUM, root=0)
    if self.params.merging.error.mm24.tuning_param_opt:
      dL_dnu_bin = comm.reduce(dL_dnu_bin_rank, MPI.SUM, root=0)

    if self.mpi_helper.rank == 0:
      self.L = flex.sum(self.bin_weighting * L_bin)
      self.dL_dsfac = flex.sum(self.bin_weighting * dL_dsfac_bin)
      self.dL_dsadd = [0 for i in range(3)]
      for degree_index in range(3):
        self.dL_dsadd[degree_index] = flex.sum(self.bin_weighting * dL_dsadd_bin[degree_index])
      if self.params.merging.error.mm24.tuning_param_opt:
        self.dL_dnu = flex.sum(self.bin_weighting * dL_dnu_bin)

  def plot_diagnostics(self, reflections):
    def get_rankits(n, down_sample, distribution):
      prob_level = (np.arange(1, n+1) - 0.5) / n
      if distribution == 'half normal':
        return scipy.stats.halfnorm.ppf(prob_level[::down_sample])
      elif distribution == 'half t-dist':
        prob_level = (prob_level + 1) / 2
        return scipy.stats.t.ppf(prob_level[::down_sample], df=self.tuning_param)

    # Get all the pairwise differences onto rank 0 for plotting
    pairwise_differences = []
    if self.cc_key:
      correlation_i = self.work_table['correlation_i']
      correlation_j = self.work_table['correlation_j']
    else:
      correlation_i = None
      correlation_j = None
    var_i = self._get_var_mm24(
      self.work_table['counting_stats_var_i'],
      self.work_table['biased_mean'],
      correlation_i,
      return_der=False
      )
    var_j = self._get_var_mm24(
      self.work_table['counting_stats_var_j'],
      self.work_table['biased_mean'],
      correlation_j,
      return_der=False
      )
    normalized_differences = self.work_table['pairwise_differences'] / flex.sqrt(var_i + var_j)
    pairwise_differences = normalized_differences.as_numpy_array()
    all_pairwise_differences = self.mpi_helper.gather_variable_length_numpy_arrays(
      pairwise_differences, root=0, dtype=float
      )

    if self.mpi_helper.rank == 0:
      import matplotlib.pyplot as plt
      sorted_pairwise_differences = np.sort(all_pairwise_differences)
      lim = 5
      downsample = 10000
      grey1 = np.array([99, 102, 106]) / 255
      grey2 = np.array([177, 179, 179]) / 255

      pairwise_differences_bins = np.linspace(0, lim, 101)
      pairwise_differences_db = pairwise_differences_bins[1] - pairwise_differences_bins[0]
      pairwise_differences_centers = (pairwise_differences_bins[1:] + pairwise_differences_bins[:-1]) / 2
      pairwise_differences_hist, _ = np.histogram(
        sorted_pairwise_differences, bins=pairwise_differences_bins, density=True
        )

      fig, axes = plt.subplots(1, 2, figsize=(6, 3))
      axes[0].bar(
        pairwise_differences_centers, pairwise_differences_hist,
        width=pairwise_differences_db, label=r'$\omega_{hkl}$'
        )
      axes[0].plot(
        pairwise_differences_centers,
        scipy.stats.halfnorm.pdf(pairwise_differences_centers),
        color=grey1, label='Normal'
        )
      if self.params.merging.error.mm24.likelihood in ['t-dist']:
        axes[0].plot(
          pairwise_differences_centers,
          2*scipy.stats.t.pdf(pairwise_differences_centers, df=self.tuning_param),
          color=grey2, label=r't-dist $\nu$: ' + f'{self.tuning_param:0.1f}'
          )

      axes[0].legend(frameon=False, fontsize=8, handlelength=1)
      axes[0].set_ylabel(r'Distribution of $\omega_{hbk}$')
      axes[0].set_xlabel(r'Normalized PD ($\omega_{hbk}$)')
      axes[0].set_xlim([0, 4.5])
      axes[0].set_xticks([0, 1, 2, 3, 4])

      axes[1].plot([0, lim], [0, lim], color=[0, 0, 0], linewidth=1, linestyle=':')
      axes[1].plot(
        sorted_pairwise_differences[::downsample],
        get_rankits(sorted_pairwise_differences.size, downsample, 'half normal'),
        color=grey1
        )
      if self.params.merging.error.mm24.likelihood in ['t-dist']:
        axes[1].plot(
          sorted_pairwise_differences[::downsample],
          get_rankits(sorted_pairwise_differences.size, downsample, 'half t-dist'),
          color=grey2
          )

      axes[1].set_ylim([0, lim])
      axes[1].set_ylabel('Rankits')
      axes[1].set_xlabel(r'Sorted Normalized PD ($\omega_{hbk}$)')
      axes[1].set_box_aspect(1)
      axes[1].set_xticks([0, 1, 2, 3, 4])
      axes[1].set_yticks([0, 1, 2, 3, 4])
      axes[1].set_xlim([0, 4.5])
      axes[1].set_ylim([0, 4.5])
      fig.tight_layout()
      fig.savefig(os.path.join(
        self.params.output.output_dir,
        self.params.output.prefix + '_PairwiseDifferences.png'
        ))
      plt.close()

    # Get the correlations for later plotting
    if self.cc_key:
      cc_all = self.mpi_helper.gather_variable_length_numpy_arrays(
          np.unique(self.work_table['correlation_i'].as_numpy_array()), root=0, dtype=float
          )
      if self.mpi_helper.rank == 0:
        # CC & sadd plots #
        bins = np.linspace(cc_all.min(), cc_all.max(), 101)
        dbin = bins[1] - bins[0]
        centers = (bins[1:] + bins[:-1]) / 2
        hist_all, _ = np.histogram(cc_all, bins=bins)

        hist_color = np.array([0, 49, 60]) / 256
        line_color = np.array([213, 120, 0]) / 256
        sadd2, _ = self._get_sadd2(flex.double(centers))
        fig, axes_hist = plt.subplots(1, 1, figsize=(5, 3))
        axes_sadd = axes_hist.twinx()
        axes_hist.bar(centers, hist_all / 1000, width=dbin, color=hist_color)
        axes_sadd.plot(centers, self.sfac**2 * sadd2, color=line_color)
        axes_hist.set_xlabel('Correlation Coefficient')
        axes_hist.set_ylabel('Lattices (x1,000)')
        axes_sadd.set_ylabel(r'$s_{\mathrm{fac}}^2 \times s_{\mathrm{add}}^2$')
        fig.tight_layout()
        fig.savefig(os.path.join(
          self.params.output.output_dir,
          self.params.output.prefix + '_sadd.png'
          ))
        plt.close()


if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(error_modifier)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/errors/error_modifier_sr.py
from __future__ import absolute_import, division, print_function
from dials.array_family import flex
from xfel.merging.application.worker import worker
from xfel.merging.application.reflection_table_utils import reflection_table_utils

class error_modifier_sr(worker):

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(error_modifier_sr, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Adjust intensity errors -- sample residuals method'

  def run(self, experiments, reflections):
    '''Modify intensity errors according to HKL sample residuals'''
    assert self.params.merging.error.model == "errors_from_sample_residuals"
    self.logger.log_step_time("ERROR_MODIFIER_SR")
    if len(reflections) > 0:
      self.logger.log("Modifying intensity errors -- sample residuals method...")
      reflections = self.modify_errors(reflections)
    self.logger.log_step_time("ERROR_MODIFIER_SR", True)

    return experiments, reflections

  def modify_errors(self, reflections):
    '''For each asu hkl replace the variances of individual measured intensities with the variance of the intensity distribution'''
    new_reflections = flex.reflection_table()
    number_of_hkls = 0
    number_of_multiply_measured_hkls = 0

    for refls in reflection_table_utils.get_next_hkl_reflection_table(reflections):
      if refls.size() == 0:
        break # unless the input "reflections" list is empty, generated "refls" lists cannot be empty
      number_of_hkls += 1
      number_of_measurements = len(refls)
      if number_of_measurements > 1:
        stats = flex.mean_and_variance(refls['intensity.sum.value'])
        refls['intensity.sum.variance'] = flex.double(number_of_measurements, stats.unweighted_sample_variance())
        number_of_multiply_measured_hkls += 1
      new_reflections.extend(refls)

    self.logger.log("Modified errors for (multiply-measured) %d symmetry-reduced HKLs out of %d symmetry-reduced HKLs"%(number_of_multiply_measured_hkls, number_of_hkls))

    return new_reflections

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(error_modifier)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/errors/factory.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.errors.error_modifier_sr import error_modifier_sr
from xfel.merging.application.errors.error_modifier_ev11 import error_modifier_ev11
from xfel.merging.application.errors.error_modifier_ha14 import error_modifier_ha14
from xfel.merging.application.errors.error_modifier_mm24 import error_modifier_mm24
from xfel.merging.application.worker import factory as factory_base

class factory(factory_base):
  '''Factory class for modifying errors of measured intensities'''
  @staticmethod
  def from_parameters(params, additional_info=[], mpi_helper=None, mpi_logger=None):
    assert len(additional_info) > 0
    assert additional_info[0] in ["premerge", "merge"]
    if additional_info[0] == "premerge":
      if params.merging.error.model == "ha14":
        return [error_modifier_ha14(params, mpi_helper, mpi_logger)]
    elif additional_info[0] == "merge":
      if params.merging.error.model == "errors_from_sample_residuals":
        return [error_modifier_sr(params, mpi_helper, mpi_logger)]
      elif params.merging.error.model == "ev11":
        return [error_modifier_ev11(params, mpi_helper, mpi_logger)]
      elif params.merging.error.model == "mm24":
        return [error_modifier_mm24(params, mpi_helper, mpi_logger)]
    return []


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/filter/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/filter/experiment_filter.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.worker import worker
from cctbx.crystal import symmetry
from libtbx import Auto

class experiment_filter(worker):
  '''Reject experiments based on various criteria'''

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(experiment_filter, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def validate(self):
    filter_by_unit_cell = 'unit_cell' in self.params.filter.algorithm
    filter_by_n_obs = 'n_obs' in self.params.filter.algorithm
    filter_by_resolution = 'resolution' in self.params.filter.algorithm
    if filter_by_unit_cell:
      assert self.params.filter.unit_cell.value.target_space_group is not None, \
        'Space group is required for unit cell filtering'
    if filter_by_n_obs:
      check0 = self.params.filter.n_obs.min is not None
      check1 = self.params.filter.n_obs.max is not None
      assert check0 or check1, \
        'Either min or max is required for n_obs filtering'
      if check0 and check1:
        assert self.params.filter.n_obs.min < self.params.filter.n_obs.max, \
        'filter.n_obs.min must be less than filter.n_obs.max'
    if filter_by_resolution:
      assert self.params.filter.resolution.d_min is not None, \
        'd_min is required for resolution filtering'

  def __repr__(self):
    return 'Filter experiments'

  def check_unit_cell(self, experiment):
    experiment_unit_cell = experiment.crystal.get_unit_cell()
    is_ok = experiment_unit_cell.is_similar_to(self.params.filter.unit_cell.value.target_unit_cell,
                                               self.params.filter.unit_cell.value.relative_length_tolerance,
                                               self.params.filter.unit_cell.value.absolute_angle_tolerance)
    return is_ok

  def check_space_group(self, experiment):
    # build patterson group from the target space group
    target_unit_cell = self.params.filter.unit_cell.value.target_unit_cell
    target_space_group_info = self.params.filter.unit_cell.value.target_space_group
    target_symmetry = symmetry(unit_cell=target_unit_cell, space_group_info=target_space_group_info)
    target_space_group = target_symmetry.space_group()
    target_patterson_group_sn = target_space_group.build_derived_patterson_group().info().symbol_and_number()

    # build patterson group from the experiment space group
    experiment_space_group = experiment.crystal.get_space_group()
    experiment_patterson_group_sn = experiment_space_group.build_derived_patterson_group().info().symbol_and_number()

    is_ok = (target_patterson_group_sn == experiment_patterson_group_sn)

    return is_ok

  @staticmethod
  def remove_experiments(experiments, reflections, experiment_ids_to_remove):
    '''Remove specified experiments from the experiment list. Remove corresponding reflections from the reflection table'''
    experiments.select_on_experiment_identifiers([i for i in experiments.identifiers() if i not in experiment_ids_to_remove])
    reflections.remove_on_experiment_identifiers(experiment_ids_to_remove)
    reflections.reset_ids()
    return experiments, reflections

  def check_cluster(self, experiment):
    import numpy as np
    from math import sqrt
    experiment_unit_cell = experiment.crystal.get_unit_cell()
    P = experiment_unit_cell.parameters()
    features = [P[idx] for idx in self.cluster_data["idxs"]]
    features = np.array(features).reshape(1,-1)
    cov=self.cluster_data["populations"].fit_components[self.params.filter.unit_cell.cluster.covariance.component]
    m_distance = sqrt(cov.mahalanobis(features))
    for other in self.params.filter.unit_cell.cluster.covariance.skip_component:
      skip_cov = self.cluster_data["populations"].fit_components[other]
      skip_distance = sqrt(skip_cov.mahalanobis(features))
      if skip_distance < self.params.filter.unit_cell.cluster.covariance.skip_mahalanobis:
        return False
    return m_distance < self.params.filter.unit_cell.cluster.covariance.mahalanobis

  def run(self, experiments, reflections):
    filter_by_unit_cell = 'unit_cell' in self.params.filter.algorithm
    filter_by_n_obs = 'n_obs' in self.params.filter.algorithm
    filter_by_resolution = 'resolution' in self.params.filter.algorithm
    # only "unit_cell" "n_obs" and "resolution" algorithms are supported
    if (not filter_by_unit_cell) and (not filter_by_n_obs) and (not filter_by_resolution):
      return experiments, reflections
    self.logger.log_step_time("FILTER_EXPERIMENTS")

    # BEGIN BY-VALUE FILTER
    experiment_ids_to_remove = []
    if filter_by_unit_cell:
      experiment_ids_to_remove_unit_cell, removed_for_unit_cell, removed_for_space_group = self.run_filter_by_unit_cell(experiments, reflections)
      experiment_ids_to_remove += experiment_ids_to_remove_unit_cell
    else:
      removed_for_unit_cell = 0
      removed_for_space_group = 0
    if filter_by_n_obs:
      experiment_ids_to_remove_n_obs, removed_for_n_obs = self.run_filter_by_n_obs(experiments, reflections)
      experiment_ids_to_remove += experiment_ids_to_remove_n_obs
    else:
      removed_for_n_obs = 0
    if filter_by_resolution:
      experiment_ids_to_remove_resolution, removed_for_resolution = self.run_filter_by_resolution(experiments, reflections)
      experiment_ids_to_remove += experiment_ids_to_remove_resolution
    else:
      removed_for_resolution = 0
    experiment_ids_to_remove = list(set(experiment_ids_to_remove))

    input_len_expts = len(experiments)
    input_len_refls = len(reflections)
    new_experiments, new_reflections = experiment_filter.remove_experiments(experiments, reflections, experiment_ids_to_remove)

    removed_reflections = input_len_refls - len(new_reflections)
    assert len(experiment_ids_to_remove) == input_len_expts - len(new_experiments)

    self.logger.log("Experiments rejected because of unit cell dimensions: %d"%removed_for_unit_cell)
    self.logger.log("Experiments rejected because of space group %d"%removed_for_space_group)
    self.logger.log("Experiments rejected because of n_obs %d"%removed_for_n_obs)
    self.logger.log("Experiments rejected because of resolution %d"%removed_for_resolution)
    self.logger.log("Reflections rejected because of rejected experiments: %d"%removed_reflections)

    # MPI-reduce total counts
    comm = self.mpi_helper.comm
    MPI = self.mpi_helper.MPI
    total_removed_for_unit_cell = comm.reduce(removed_for_unit_cell, MPI.SUM, 0)
    total_removed_for_space_group = comm.reduce(removed_for_space_group, MPI.SUM, 0)
    total_removed_for_n_obs = comm.reduce(removed_for_n_obs, MPI.SUM, 0)
    total_removed_for_resolution = comm.reduce(removed_for_resolution, MPI.SUM, 0)
    total_reflections_removed = comm.reduce(removed_reflections, MPI.SUM, 0)

    # rank 0: log total counts
    if self.mpi_helper.rank == 0:
      self.logger.main_log("Total experiments rejected because of unit cell dimensions: %d"%total_removed_for_unit_cell)
      self.logger.main_log("Total experiments rejected because of space group %d"%total_removed_for_space_group)
      self.logger.main_log("Total experiments rejected because of n_obs %d"%total_removed_for_n_obs)
      self.logger.main_log("Total experiments rejected because of resolution %d"%total_removed_for_resolution)
      self.logger.main_log("Total reflections rejected because of rejected experiments %d"%total_reflections_removed)

    self.logger.log_step_time("FILTER_EXPERIMENTS", True)

    # Do we have any data left?
    from xfel.merging.application.utils.data_counter import data_counter
    data_counter(self.params).count(new_experiments, new_reflections)
    return new_experiments, new_reflections

  def run_filter_by_unit_cell(self, experiments, reflections):
    experiment_ids_to_remove = []
    removed_for_unit_cell = 0
    removed_for_space_group = 0
    if self.params.filter.unit_cell.algorithm == "value":
      # If the filter unit cell and/or space group params are Auto, use the corresponding scaling targets.
      if self.params.filter.unit_cell.value.target_unit_cell in (Auto, None):
        if self.params.scaling.unit_cell is None:
          try:
            self.params.filter.unit_cell.value.target_unit_cell = self.params.statistics.average_unit_cell
          except AttributeError:
            pass
        else:
          self.params.filter.unit_cell.value.target_unit_cell = self.params.scaling.unit_cell
      if self.params.filter.unit_cell.value.target_space_group in (Auto, None):
        self.params.filter.unit_cell.value.target_space_group = self.params.scaling.space_group

      self.logger.log("Using filter target unit cell: %s"%str(self.params.filter.unit_cell.value.target_unit_cell))
      self.logger.log("Using filter target space group: %s"%str(self.params.filter.unit_cell.value.target_space_group))

      for experiment in experiments:
        if not self.check_space_group(experiment):
          experiment_ids_to_remove.append(experiment.identifier)
          removed_for_space_group += 1
        elif not self.check_unit_cell(experiment):
          experiment_ids_to_remove.append(experiment.identifier)
          removed_for_unit_cell += 1
    # END BY-VALUE FILTER
    elif self.params.filter.unit_cell.algorithm == "cluster":
      from uc_metrics.clustering.util import get_population_permutation # implicit import
      import pickle
      class Empty: pass
      if self.mpi_helper.rank == 0:
        with open(self.params.filter.unit_cell.cluster.covariance.file,'rb') as F:
          data = pickle.load(F)
          E=Empty()
          E.features_ = data["features"]
          E.sample_name = data["sample"]
          E.output_info = data["info"]
          pop=data["populations"]
          self.logger.main_log("Focusing on cluster component %d from previous analysis of %d cells"%(
            self.params.filter.unit_cell.cluster.covariance.component, len(pop.labels)))
          self.logger.main_log("%s noise %d order %s"%(pop.populations, pop.n_noise_, pop.main_components))

          legend = pop.basic_covariance_compact_report(feature_vectors=E).getvalue()
          self.logger.main_log(legend)
          self.logger.main_log("Applying Mahalanobis cutoff of %.3f"%(self.params.filter.unit_cell.cluster.covariance.mahalanobis))
        transmitted = data
      else:
        transmitted = None
      # distribute cluster information to all ranks
      self.cluster_data = self.mpi_helper.comm.bcast(transmitted, root=0)
      # pull out the index numbers of the unit cell parameters to be used for covariance matrix
      self.cluster_data["idxs"]=[["a","b","c","alpha","beta","gamma"].index(F) for F in self.cluster_data["features"]]

      for experiment in experiments:
        if not self.check_cluster(experiment):
          experiment_ids_to_remove.append(experiment.identifier)
          removed_for_unit_cell += 1
    # END OF COVARIANCE FILTER
    return experiment_ids_to_remove, removed_for_unit_cell, removed_for_space_group

  def run_filter_by_n_obs(self, experiments, reflections):
    experiment_ids_to_remove = []
    removed_for_n_obs = 0
    for expt_index, experiment in enumerate(experiments):
      refls_expt = reflections.select(reflections['id'] == expt_index)
      if self.params.filter.n_obs.max and len(refls_expt) > self.params.filter.n_obs.max:
        experiment_ids_to_remove.append(experiment.identifier)
        removed_for_n_obs += 1
      if self.params.filter.n_obs.min and len(refls_expt) < self.params.filter.n_obs.min:
        experiment_ids_to_remove.append(experiment.identifier)
        removed_for_n_obs += 1
    return experiment_ids_to_remove, removed_for_n_obs

  def run_filter_by_resolution(self, experiments, reflections):
    experiment_ids_to_remove = []
    removed_for_resolution = 0
    resolution = reflections.compute_d(experiments)
    start = 0
    for expt_index, expt in enumerate(experiments):
      refls_expt = reflections.select(reflections['id'] == expt_index)
      n_refls = refls_expt.size()
      max_resolution = min(resolution[start: start + n_refls])
      start += n_refls
      if max_resolution > self.params.filter.resolution.d_min:
        experiment_ids_to_remove.append(expt.identifier)
        removed_for_resolution += 1
    return experiment_ids_to_remove, removed_for_resolution

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(experiment_filter)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/filter/factory.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.filter.experiment_filter import experiment_filter
from xfel.merging.application.filter.reflection_filter import reflection_filter
from xfel.merging.application.filter.global_filter import GlobalFilter
from xfel.merging.application.worker import factory as factory_base

class factory(factory_base):
  """ Factory class for filtering experiments. """
  @staticmethod
  def from_parameters(params, additional_info=None, mpi_helper=None, mpi_logger=None):
    """ """
    workers = []
    if additional_info and additional_info[0] == 'global':
      workers.append(GlobalFilter(params, mpi_helper, mpi_logger))
    elif additional_info:
      raise KeyError('Unknown worker: filter_' + '_'.join(additional_info))
    else:  # if not additional_info
      if params.filter.algorithm is not None:
        workers.append(experiment_filter(params, mpi_helper, mpi_logger))
      if params.select.algorithm is not None:
        workers.append(reflection_filter(params, mpi_helper, mpi_logger))
    return workers


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/filter/global_filter.py
from __future__ import division

from collections import Counter
from enum import Enum
from itertools import chain

import numpy as np

from dials.array_family import flex
from dxtbx.model import ExperimentList
from xfel.merging.application.utils.data_counter import data_counter
from xfel.merging.application.worker import worker


def flat_array(iterable):
  """Return the contents of all input iterables flattened into a 1d np.array"""
  return np.array(list(chain.from_iterable(iterable)))


def uniques(*iterables):
  """Return a set of unique elements across all input iterables"""
  return set(sum([list(i) for i in iterables], []))


class FilterReasons(Enum):
  """Enumerator documenting all possible reasons for filtering expts/refls"""
  intensity_extremum_iqr_dist = "Intensity extremum outside IQR dist threshold"
  # add subsequent global filtering reasons here (step 1/3)

  @classmethod
  def max_reason_len(cls):
    return max(len(r.value) for r in cls)

  @classmethod
  def report_line(cls, reason, filtered_expts, filtered_refls):
    """Return a line for the report listing all input in nice format"""
    fmt = '- {:' + str(cls.max_reason_len() + 1) + '} {:6d} expts, {:9d} refls'
    return fmt.format(str(reason) + ':', filtered_expts, filtered_refls)


class GlobalFilter(worker):
  """Filter experiments & reflections based on their aggregated statistics"""

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    self.expt_filter_reasons = Counter()
    self.refl_filter_reasons = Counter()
    super(GlobalFilter, self).__init__(params=params, mpi_helper=mpi_helper,
                                       mpi_logger=mpi_logger)

  def __repr__(self):
    return """Filter expts & refls based on their aggregated statistics"""

  def filter_intensity_extrema(self, expts, refls):
    """Filter expts whose refls' intensity extrema don't fit the population"""
    iqr_lim = self.params.filter_global.intensity_extrema_iqr_dist_threshold
    maxima = []
    minima = []
    refl_list = []
    for iid, expt in enumerate(expts):
      refl = refls.select(refls["id"] == iid)
      maxima.append(max(refl['intensity.sum.value']))
      minima.append(min(refl['intensity.sum.value']))
      refl_list.append(refl)
    all_maxima = flat_array(self.mpi_helper.comm.allgather(maxima))
    all_minima = flat_array(self.mpi_helper.comm.allgather(minima))
    maxima_quartiles = np.nanpercentile(all_maxima, [25, 50, 75])
    minima_quartiles = np.nanpercentile(all_minima, [25, 50, 75])
    maxima_iqr = maxima_quartiles[2] - maxima_quartiles[0]
    minima_iqr = minima_quartiles[2] - minima_quartiles[0]
    maxima_upper_lim = maxima_quartiles[1] + iqr_lim * maxima_iqr
    minima_lower_lim = minima_quartiles[1] - iqr_lim * minima_iqr
    filtered_expts = ExperimentList()
    filtered_refls = flex.reflection_table()
    for expt, refl, minimum, maximum in zip(expts, refl_list, minima, maxima):
      if minimum < minima_lower_lim or maximum > maxima_upper_lim:
        filter_reason = FilterReasons.intensity_extremum_iqr_dist
        self.expt_filter_reasons[filter_reason] += 1
        self.refl_filter_reasons[filter_reason] += refl.size()
      else:
        filtered_expts.append(expt)
        filtered_refls.extend(refl)
    return filtered_expts, filtered_refls

  # implement subsequent global filtering algorithms here (step 2/3)

  def report_filter_reasons(self):
    self.logger.log('Experiments/reflections filtered on this rank due to:')
    for r in uniques(self.expt_filter_reasons, self.refl_filter_reasons):
      te = self.expt_filter_reasons[r]
      tr = self.refl_filter_reasons[r]
      self.logger.log(FilterReasons.report_line(r.value, te, tr))
    te = sum(self.expt_filter_reasons.values())
    tr = sum(self.refl_filter_reasons.values())
    self.logger.log(FilterReasons.report_line('TOTAL', te, tr))
    all_expt_filter_reasons = self.mpi_helper.count(self.expt_filter_reasons)
    all_refl_filter_reasons = self.mpi_helper.count(self.refl_filter_reasons)
    if self.mpi_helper.rank == 0:
      self.logger.main_log('Experiments/reflections filtered due to:')
      for r in uniques(all_expt_filter_reasons, all_refl_filter_reasons):
        te = all_expt_filter_reasons[r]
        tr = all_refl_filter_reasons[r]
        self.logger.main_log(FilterReasons.report_line(r.value, te, tr))
      te = sum(all_expt_filter_reasons.values())
      tr = sum(all_refl_filter_reasons.values())
      self.logger.main_log(FilterReasons.report_line('TOTAL', te, tr))

  def run(self, experiments, reflections):
    expts, refls = self.filter_intensity_extrema(experiments, reflections)
    # call subsequent global filtering algorithms here (step 3/3)
    self.report_filter_reasons()
    data_counter(self.params).count(expts, refls)
    return expts, refls


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/filter/reflection_filter.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from xfel.merging.application.worker import worker
from dials.array_family import flex
from dxtbx.model.experiment_list import ExperimentList
from rstbx.dials_core.integration_core import show_observations
from cctbx import miller
from cctbx.crystal import symmetry
from six.moves import cStringIO as StringIO
import numpy as np


class reflection_filter(worker):
  '''Reject individual reflections based on various criteria'''

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(reflection_filter, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Filter reflections'

  def validate(self):
    filter_by_significance = 'significance_filter' in self.params.select.algorithm
    filter_by_isolation_forest = 'isolation_forest' in self.params.select.algorithm
    if filter_by_isolation_forest:
      check0 = self.params.select.reflection_filter.tail_percentile > 0
      check1 = self.params.select.reflection_filter.tail_percentile < 1
      assert check0 and check1, \
        'tail_percentile must be between 0 and 1'

      check0 = self.params.select.reflection_filter.contamination_lower > 0
      check1 = self.params.select.reflection_filter.contamination_lower < 1
      check2 = self.params.select.reflection_filter.contamination_upper > 0
      check3 = self.params.select.reflection_filter.contamination_upper < 1
      assert check0 and check1 and check2 and check3, \
        'contamination must be between 0 and 1'
    if filter_by_isolation_forest:
      check0 = self.params.select.reflection_filter.sampling_fraction > 0
      check1 = self.params.select.reflection_filter.sampling_fraction < 1
      assert check0 and check1, \
        'sampling_fraction must be between 0 and 1'

  def plot_reflections(self, experiments, reflections, tag):
    if reflections:
      correct_info = 'miller_index' in reflections.keys()
      if correct_info == False:
        reflections['miller_index'] = reflections['miller_index_asymmetric']
      q2_rank = 1 / reflections.compute_d(experiments).as_numpy_array()**2
      if correct_info == False:
        del reflections['miller_index']
      intensity_rank = reflections['intensity.sum.value'].as_numpy_array()
    else:
      q2_rank = np.zeros(0)
      intensity_rank = np.zeros(0)
    q2 = self.mpi_helper.comm.gather(q2_rank, root=0)
    intensity = self.mpi_helper.comm.gather(intensity_rank, root=0)
    if self.mpi_helper.rank == 0:
      import matplotlib.pyplot as plt
      import os
      q2 = np.concatenate(q2)
      intensity = np.concatenate(intensity)
      fig, axes = plt.subplots(1, 1, figsize=(8, 3))
      axes.scatter(q2, intensity, s=1, color=[0, 0, 0], marker='.')
      axes.set_ylabel('Intensity')
      axes.set_title(tag)
      axes.set_xlabel(r'Resolution ($\mathrm{\AA}$)')
      xticks = axes.get_xticks()
      xticks = xticks[xticks > 0]
      xticklabels = [f'{l:0.2f}' for l in 1 / np.sqrt(xticks)]
      axes.set_xticks(xticks)
      axes.set_xticklabels(xticklabels)
      fig.tight_layout()
      fig.savefig(os.path.join(
        self.params.output.output_dir,
        self.params.output.prefix + f'_Iobs_{tag}.png'
        ).replace(' ', '_'))
      plt.close()

  def run(self, experiments, reflections):
    filter_by_significance = 'significance_filter' in self.params.select.algorithm
    filter_by_isolation_forest = 'isolation_forest' in self.params.select.algorithm
    # only "unit_cell" "n_obs" and "resolution" algorithms are supported
    if (not filter_by_significance) and (not filter_by_isolation_forest):
      return experiments, reflections

    n_reflections_initial = len(reflections)
    n_experiments_initial = len(experiments)
    if self.params.select.reflection_filter.do_diagnostics:
      self.plot_reflections(experiments, reflections, 'Initial')
    if filter_by_significance:
      experiments, reflections = self.apply_significance_filter(experiments, reflections)
      removed_reflections_significance_rank = n_reflections_initial - len(reflections)
      removed_experiments_significance_rank = n_experiments_initial - len(experiments)
      removed_reflections_significance = self.mpi_helper.comm.reduce(
        removed_reflections_significance_rank, self.mpi_helper.MPI.SUM, root=0
        )
      removed_experiments_significance = self.mpi_helper.comm.reduce(
        removed_experiments_significance_rank, self.mpi_helper.MPI.SUM, root=0
        )
      self.logger.log(f"Reflections rejected because of significant: {removed_reflections_significance_rank}")
      self.logger.log(f"Experiments rejected because of significant: {removed_experiments_significance_rank}")
      if self.mpi_helper.rank == 0:
        self.logger.main_log(f"Total reflections rejected because of significant: {removed_reflections_significance}")
        self.logger.main_log(f"Total experiments rejected because of significant: {removed_experiments_significance}")
      if self.params.select.reflection_filter.do_diagnostics:
        self.plot_reflections(experiments, reflections, 'After Significance Filter')
    if filter_by_isolation_forest:
      experiments, reflections = self.apply_isolation_forest(experiments, reflections)
      filter_type = 'Isolation Forest'
      if self.params.select.reflection_filter.do_diagnostics:
        self.plot_reflections(experiments, reflections, 'After Isolation Forest')

    if filter_by_isolation_forest:
      removed_reflections_filter_rank = n_reflections_initial - len(reflections)
      removed_experiments_filter_rank = n_experiments_initial - len(experiments)
      if filter_by_significance:
        removed_reflections_filter_rank -= removed_reflections_significance_rank
        removed_experiments_filter_rank -= removed_experiments_significance_rank
      removed_reflections_filter = self.mpi_helper.comm.reduce(
        removed_reflections_filter_rank, self.mpi_helper.MPI.SUM, root=0
        )
      removed_experiments_filter = self.mpi_helper.comm.reduce(
        removed_experiments_filter_rank, self.mpi_helper.MPI.SUM, root=0
        )
      self.logger.log(f"Reflections rejected because of {filter_type}: {removed_reflections_filter_rank}")
      self.logger.log(f"Experiments rejected because of {filter_type}: {removed_experiments_filter_rank}")
      if self.mpi_helper.rank == 0:
        self.logger.main_log(f"Total reflections rejected because of {filter_type}: {removed_reflections_filter}")
        self.logger.main_log(f"Total experiments rejected because of {filter_type}: {removed_experiments_filter}")

    from xfel.merging.application.utils.data_counter import data_counter
    data_counter(self.params).count(experiments, reflections)
    return experiments, reflections

  def apply_significance_filter(self, experiments, reflections):
    self.logger.log_step_time("SIGNIFICANCE_FILTER")

    # Apply an I/sigma filter ... accept resolution bins only if they
    #   have significant signal; tends to screen out higher resolution observations
    #   if the integration model doesn't quite fit
    unit_cell = self.params.scaling.unit_cell
    if unit_cell is None:
      try:
        unit_cell = self.params.statistics.average_unit_cell
      except AttributeError:
        pass
    target_symm = symmetry(unit_cell = unit_cell, space_group_info = self.params.scaling.space_group)

    new_experiments = ExperimentList()
    new_reflections = flex.reflection_table()

    kap = 'kapton_absorption_correction' in reflections
    for expt_id, experiment in enumerate(experiments):
      exp_reflections = reflections.select(reflections['id'] == expt_id)
      if not len(exp_reflections): continue

      N_obs_pre_filter = exp_reflections.size()

      N_bins_small_set = N_obs_pre_filter // self.params.select.significance_filter.min_ct
      N_bins_large_set = N_obs_pre_filter // self.params.select.significance_filter.max_ct

      # Ensure there is at least one bin.
      N_bins = max([min([self.params.select.significance_filter.n_bins,N_bins_small_set]), N_bins_large_set, 1])
      if kap:
        unattenuated = exp_reflections['kapton_absorption_correction'] == 1.0
        iterable = [exp_reflections.select(unattenuated), exp_reflections.select(~unattenuated)]
        exp_miller_indices = miller.set(target_symm, exp_reflections['miller_index'], True)
        exp_observations = miller.array(exp_miller_indices, exp_reflections['intensity.sum.value'], flex.sqrt(exp_reflections['intensity.sum.variance']))
        binner = exp_observations.setup_binner(n_bins = N_bins)
        N_bins = None
      else:
        iterable = [exp_reflections]

      #print ("\nN_obs_pre_filter %d"%N_obs_pre_filter)
      #print >> out, "Total obs %d Choose n bins = %d"%(N_obs_pre_filter,N_bins)
      #if indices_to_edge is not None:
      #  print >> out, "Total preds %d to edge of detector"%indices_to_edge.size()

      new_exp_reflections = flex.reflection_table()
      for refls in iterable:
        # Build a miller array for the experiment reflections
        exp_miller_indices = miller.set(target_symm, refls['miller_index'], True)
        exp_observations = miller.array(exp_miller_indices, refls['intensity.sum.value'], flex.sqrt(refls['intensity.sum.variance']))
        if kap:
          exp_observations.use_binning(binner)

        assert exp_observations.size() == refls.size()

        out = StringIO()
        ### !!! CRITICAL BOTTLENECK !!! ###
        bin_results = show_observations(exp_observations, out=out, n_bins=N_bins)

        if self.params.output.log_level == 0:
          self.logger.log(out.getvalue())

        acceptable_resolution_bins = [bin.mean_I_sigI > self.params.select.significance_filter.sigma for bin in bin_results]

        acceptable_nested_bin_sequences = [i for i in range(len(acceptable_resolution_bins)) if False not in acceptable_resolution_bins[:i+1]]

        if len(acceptable_nested_bin_sequences) == 0:
          continue
        else:
          N_acceptable_bins = max(acceptable_nested_bin_sequences) + 1

          imposed_res_filter = float(bin_results[N_acceptable_bins-1].d_range.split()[2])
          if self.params.output.log_level == 0:
            ident = experiment.identifier
            self.logger.log(
              "Experiment id %d, resolution cutoff %f, experiment identifier %s\n"
              %(expt_id, imposed_res_filter, ident)
            )
          else:
            self.logger.log(
              "Experiment id %d, resolution cutoff %f\n"
              %(expt_id, imposed_res_filter)
            )

          if self.params.select.significance_filter.d_min and imposed_res_filter > self.params.select.significance_filter.d_min:
            self.logger.log("Resolution below %f, rejecting"%self.params.select.significance_filter.d_min)
            continue
          imposed_res_sel = exp_observations.resolution_filter_selection(d_min=imposed_res_filter)

          assert imposed_res_sel.size() == refls.size()

          new_exp_reflections.extend(refls.select(imposed_res_sel))

      if new_exp_reflections.size() > 0:
        new_experiments.append(experiment)
        new_reflections.extend(new_exp_reflections)

      #self.logger.log("N acceptable bins %d"%N_acceptable_bins)
      #self.logger.log("Old n_obs: %d, new n_obs: %d"%(N_obs_pre_filter, exp_observations.size()))
      #if indices_to_edge is not None:
      #  print >> out, "Total preds %d to edge of detector"%indices_to_edge.size()

    self.logger.log_step_time("SIGNIFICANCE_FILTER", True)
    new_reflections.reset_ids()
    return new_experiments, new_reflections

  def _common_initial(self, experiments, reflections):
    if reflections:
      correct_info = 'miller_index' in reflections.keys()
      if correct_info == False:
        reflections['miller_index'] = reflections['miller_index_asymmetric']
      resolution = reflections.compute_d(experiments)
      if correct_info == False:
        del reflections['miller_index']
      reflections['q2'] = 1 / resolution**2
      q2_rank = reflections['q2'].as_numpy_array()
      intensity_rank = reflections['intensity.sum.value'].as_numpy_array()
    else:
      q2_rank = np.zeros(0)
      intensity_rank = np.zeros(0)

    # get bin edges in q2
    n_bins = self.params.select.reflection_filter.n_bins
    q2_min = self.mpi_helper.comm.reduce(q2_rank.min() if reflections else np.inf, op=self.mpi_helper.MPI.MIN, root=0)
    q2_max = self.mpi_helper.comm.reduce(q2_rank.max() if reflections else -np.inf, op=self.mpi_helper.MPI.MAX, root=0)
    if self.mpi_helper.rank == 0:
      q2_bins = np.linspace(q2_min, q2_max, n_bins + 1)
    else:
      q2_bins = np.zeros(n_bins + 1)
    self.mpi_helper.comm.Bcast(q2_bins, root=0)

    # Get the mean intensity binned in q2 using a histogram method
    intensity_summation_rank, _ = np.histogram(q2_rank, bins=q2_bins, weights=intensity_rank)
    intensity_counts_rank, _ = np.histogram(q2_rank, bins=q2_bins)
    intensity_summation = np.zeros(n_bins, dtype=intensity_summation_rank.dtype)
    intensity_counts = np.zeros(n_bins, dtype=intensity_counts_rank.dtype)
    self.mpi_helper.comm.Reduce(
      intensity_summation_rank, intensity_summation, op=self.mpi_helper.MPI.SUM, root=0
      )
    self.mpi_helper.comm.Reduce(
      intensity_counts_rank, intensity_counts, op=self.mpi_helper.MPI.SUM, root=0
      )
    if self.mpi_helper.rank == 0:
      binned_mean = intensity_summation / intensity_counts
    else:
      binned_mean = np.zeros(n_bins)
    self.mpi_helper.comm.Bcast(binned_mean, root=0)

    # Normalize intensities on each rank
    intensity_normalized_rank = np.zeros(len(reflections))
    for bin_index in range(n_bins):
      indices = np.logical_and(
        q2_rank >= q2_bins[bin_index],
        q2_rank < q2_bins[bin_index + 1]
        )
      intensity_normalized_rank[indices] = intensity_rank[indices] / binned_mean[bin_index]
    reflections['intensity_normalized'] = flex.double(intensity_normalized_rank)

    # Find the lower and upper thresholds for the data's tails
    percentile = self.params.select.reflection_filter.tail_percentile
    # These flags are used to identify which reflections are in the tails. They evently will be
    # added to the reflection table to simplify logistical management
    lower_tail_flag_rank = np.zeros(len(reflections), dtype=bool)
    upper_tail_flag_rank = np.zeros(len(reflections), dtype=bool)
    lower_tail = []
    upper_tail = []
    for bin_index in range(n_bins):
      indices = np.logical_and(
        q2_rank >= q2_bins[bin_index],
        q2_rank < q2_bins[bin_index + 1]
        )
      intensity_normalized_rank_bin = intensity_normalized_rank[indices]
      q2_rank_bin = q2_rank[indices]

      bin_sizes = self.mpi_helper.comm.gather(q2_rank_bin.size, root=0)
      if self.mpi_helper.rank == 0:
        q2_bin = np.zeros(intensity_counts[bin_index])
        intensity_normalized_bin = np.zeros(intensity_counts[bin_index])
      else:
        q2_bin = None
        intensity_normalized_bin = None
      self.mpi_helper.comm.Gatherv(
        sendbuf=q2_rank_bin,
        recvbuf=[q2_bin, bin_sizes],
        root=0
        )
      self.mpi_helper.comm.Gatherv(
        sendbuf=intensity_normalized_rank_bin,
        recvbuf=[intensity_normalized_bin, bin_sizes],
        root=0
        )

      if self.mpi_helper.rank == 0:
        sort_indices = np.argsort(intensity_normalized_bin)
        intensity_normalized_bin = intensity_normalized_bin[sort_indices]
        q2_bin = q2_bin[sort_indices]

        lower = int(percentile * intensity_normalized_bin.size)
        upper = int((1 - percentile) * intensity_normalized_bin.size)

        thresholds = np.array([intensity_normalized_bin[lower], intensity_normalized_bin[upper]])
        lower_tail.append(np.column_stack((
          intensity_normalized_bin[:lower],
          q2_bin[:lower]
          )))
        upper_tail.append(np.column_stack((
          intensity_normalized_bin[upper:],
          q2_bin[upper:]
          )))
      else:
        thresholds = np.zeros(2)
      self.mpi_helper.comm.Bcast(thresholds, root=0)

      lower_tail_flag_rank[indices] = intensity_normalized_rank_bin <= thresholds[0]
      upper_tail_flag_rank[indices] = intensity_normalized_rank_bin >= thresholds[1]
    reflections['lower_tail_flag'] = flex.bool(lower_tail_flag_rank)
    reflections['upper_tail_flag'] = flex.bool(upper_tail_flag_rank)
    if self.mpi_helper.rank == 0:
      lower_tail = np.concatenate(lower_tail, axis=0)
      upper_tail = np.concatenate(upper_tail, axis=0)
    else:
      lower_tail = None
      upper_tail = None

    return reflections, upper_tail, lower_tail

  def do_diagnostics(self, reflections, model_upper, upper_tail, model_lower, lower_tail):
    def plot_outliers(I_normalized, q2, Y, tag, model):
      inlier_indices = Y == 1
      outlier_indices = Y == -1
      fig, axes = plt.subplots(1, 1, figsize=(8, 3), sharex=True)
      axes.scatter(
        q2[inlier_indices], I_normalized[inlier_indices],
        s=1, color=[0, 0, 0], marker='.', alpha=0.5, label='Inliers'
        )
      axes.scatter(
        q2[outlier_indices], I_normalized[outlier_indices],
        s=20, color=[0.8, 0, 0], marker='.', alpha=1, label='Outliers'
        )
      xx, yy = np.meshgrid(
        np.linspace(I_normalized.min(), I_normalized.max(), 150),
        np.linspace(q2.min(), q2.max(), 150)
        )
      Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
      Z = Z.reshape(xx.shape)
      # https://github.com/matplotlib/matplotlib/issues/23303
      # The label for the contour does not appear in the legend. Must add manually.
      contour = axes.contour(yy, xx, Z, levels=[0], linewidths=2, colors="green")
      contour_handle, _ = contour.legend_elements()
      handles, labels = axes.get_legend_handles_labels()
      handles += contour_handle
      labels += ['Decision Boundary']
      axes.set_xlabel(r'Resolution ($\mathrm{\AA}$)')
      xticks = axes.get_xticks()
      xticks = xticks[xticks > 0]
      xticklabels = [f'{l:0.2f}' for l in 1 / np.sqrt(xticks)]
      axes.set_xticks(xticks)
      axes.set_xticklabels(xticklabels)
      axes.set_ylabel('Normalized Intensity')
      if tag == 'upper':
        loc = 'upper right'
      elif tag == 'lower':
        loc = 'lower right'
      axes.legend(handles, labels, loc=loc, frameon=False)
      fig.tight_layout()
      fig.savefig(os.path.join(
        self.params.output.output_dir,
        self.params.output.prefix + f'_model_pred_{tag}.png'
        ).replace(' ', '_'))
      plt.close()

    intensity_normalized = self.mpi_helper.comm.gather(
      reflections['intensity_normalized'].as_numpy_array(), root=0
      )
    q2 = self.mpi_helper.comm.gather(reflections['q2'].as_numpy_array() if reflections else np.zeros(0), root=0)
    if self.mpi_helper.rank == 0:
      import matplotlib.pyplot as plt
      import os
      plot_outliers(upper_tail[:, 0], upper_tail[:, 1], model_upper.predict(upper_tail), 'upper', model_upper)
      plot_outliers(lower_tail[:, 0], lower_tail[:, 1], model_lower.predict(lower_tail), 'lower', model_lower)

      fig, axes = plt.subplots(3, 1, figsize=(8, 6), sharex=True)
      axes[0].scatter(
        np.concatenate(q2), np.concatenate(intensity_normalized),
        s=1, color=[0, 0, 0], marker='.', alpha=0.5
        )
      axes[1].scatter(
        upper_tail[:, 1], upper_tail[:, 0],
        s=1, color=[0, 0, 0], marker='.', alpha=0.5
        )
      axes[2].scatter(
        lower_tail[:, 1], lower_tail[:, 0],
        s=1, color=[0, 0, 0], marker='.', alpha=0.5
        )
      axes[2].set_xlabel(r'$q^2$ = 1/$d^2$ (1/$\mathrm{\AA^2}$)')
      for i in range(3):
        axes[i].set_ylabel('Normalized Intensity')
      fig.tight_layout()
      fig.savefig(os.path.join(
        self.params.output.output_dir,
        self.params.output.prefix + '_normalized_I.png'
        ).replace(' ', '_'))
      plt.close()

  def apply_isolation_forest(self, experiments, reflections):
    self.logger.log_step_time("ISOLATION_FOREST")
    from sklearn.ensemble import IsolationForest

    reflections, upper_tail, lower_tail = self._common_initial(experiments, reflections)
    if self.mpi_helper.rank == 0:
      sampling_fraction = self.params.select.reflection_filter.sampling_fraction
      model_lower = IsolationForest(
        n_estimators=self.params.select.reflection_filter.n_estimators,
        contamination=self.params.select.reflection_filter.contamination_lower,
        max_features=2,
        max_samples=int(sampling_fraction*lower_tail.shape[0]),
        random_state=self.params.select.reflection_filter.random_seed
        )
      model_lower.fit(lower_tail)
      model_upper = IsolationForest(
        n_estimators=self.params.select.reflection_filter.n_estimators,
        contamination=self.params.select.reflection_filter.contamination_upper,
        max_features=2,
        max_samples=int(sampling_fraction*upper_tail.shape[0]),
        random_state=self.params.select.reflection_filter.random_seed
        )
      model_upper.fit(upper_tail)
    else:
      model_lower = None
      model_upper = None

    if self.params.select.reflection_filter.do_diagnostics:
      self.do_diagnostics(reflections, model_upper, upper_tail, model_lower, lower_tail)
    new_experiments, new_reflections = self._common_final(
      experiments, reflections, model_lower, model_upper, 'isolation forest'
      )
    self.logger.log_step_time("ISOLATION_FOREST", True)
    return new_experiments, new_reflections

  def _common_final(self, experiments, reflections, model_lower, model_upper, filter_type):
    if self.mpi_helper.rank == 0:
        for dest_rank in range(1, self.mpi_helper.size):
            self.mpi_helper.comm.send(model_lower, dest=dest_rank)
            self.mpi_helper.comm.send(model_upper, dest=dest_rank)
    else:
        model_lower = self.mpi_helper.comm.recv(source=0)
        model_upper = self.mpi_helper.comm.recv(source=0)

    if not reflections: return experiments, reflections

    lower_tail_indices = reflections['lower_tail_flag'].as_numpy_array()
    upper_tail_indices = reflections['upper_tail_flag'].as_numpy_array()
    lower_tail_reflections = reflections.select(reflections['lower_tail_flag'])
    upper_tail_reflections = reflections.select(reflections['upper_tail_flag'])
    lower_outliers = model_lower.predict(np.column_stack((
      lower_tail_reflections['intensity_normalized'].as_numpy_array(),
      lower_tail_reflections['q2'].as_numpy_array()
      ))) if lower_tail_reflections else np.zeros(0)
    upper_outliers = model_upper.predict(np.column_stack((
      upper_tail_reflections['intensity_normalized'].as_numpy_array(),
      upper_tail_reflections['q2'].as_numpy_array()
      ))) if upper_tail_reflections else np.zeros(0)

    inlier = np.ones(len(reflections), dtype=bool)
    if self.params.select.reflection_filter.apply_lower:
      if np.sum(lower_outliers == -1) > 0:
        indices = np.argwhere(lower_tail_indices)[:, 0]
        inlier[indices[lower_outliers == -1]] = False
    if self.params.select.reflection_filter.apply_upper:
      if np.sum(upper_outliers == -1) > 0:
        indices = np.argwhere(upper_tail_indices)[:, 0]
        inlier[indices[upper_outliers == -1]] = False
    reflections['inlier'] = flex.bool(inlier)

    new_experiments = ExperimentList()
    new_reflections = flex.reflection_table()
    for expt_id, experiment in enumerate(experiments):
      exp_reflections = reflections.select(reflections['id'] == expt_id)
      if not len(exp_reflections): continue

      new_exp_reflections = flex.reflection_table()
      new_exp_reflections.extend(exp_reflections.select(exp_reflections['inlier']))
      if new_exp_reflections.size() > 0:
        new_experiments.append(experiment)
        new_reflections.extend(new_exp_reflections)

    new_reflections.reset_ids()
    del new_reflections['q2']
    del new_reflections['intensity_normalized']
    del new_reflections['lower_tail_flag']
    del new_reflections['upper_tail_flag']
    del new_reflections['inlier']
    return new_experiments, new_reflections


if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(reflection_filter)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/group/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/group/factory.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.group.group_reflections import hkl_group
from xfel.merging.application.worker import factory as factory_base

class factory(factory_base):
  '''Factory class for grouping all measurements of an asu hkl from all ranks at a single rank'''
  @staticmethod
  def from_parameters(params, additional_info=None, mpi_helper=None, mpi_logger=None):
    """ """
    return [hkl_group(params, mpi_helper, mpi_logger)]


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/group/group_reflections.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from xfel.merging.application.worker import worker
from dials.array_family import flex
from xfel.merging.application.reflection_table_utils import reflection_table_utils
from xfel.merging.application.utils.memory_usage import get_memory_usage

class hkl_group(worker):
  '''For each asu hkl, gather all of its measurements from all ranks at a single rank, while trying to evenly distribute asu HKLs over the ranks.'''

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(hkl_group, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return "Group symmetry-reduced HKLs"

  def distribute_reflection_table(self, reflections):
    '''Create a reflection table for storing reflections distributed over hkl chunks'''
    table = flex.reflection_table()
    for key in reflections:
      table[key] = type(reflections[key])()
    return table

  def run(self, experiments, reflections):

    self.logger.log_step_time("GROUP")

    reflections = reflection_table_utils.prune_reflection_table_keys(reflections=reflections,
                        keys_to_keep=['intensity.sum.value', 'intensity.sum.variance', 'miller_index_asymmetric', \
                                      'id', 'intensity.sum.value.unmodified', 'intensity.sum.variance.unmodified'],
                        keys_to_ignore=self.params.input.persistent_refl_cols)

    # set up hkl chunks to be used for all-to-all; every avialable rank participates in all-to-all, even a rank that doesn't load any data
    self.logger.log_step_time("SETUP_CHUNKS")
    self.setup_hkl_chunks(reflections)
    self.logger.log_step_time("SETUP_CHUNKS", True)

    # for the ranks, which have loaded the data, distribute the reflections over the hkl chunks
    self.logger.log_step_time("DISTRIBUTE_OVER_CHUNKS")
    self.distribute_reflections_over_hkl_chunks(reflections=reflections)
    self.logger.log_step_time("DISTRIBUTE_OVER_CHUNKS", True)

    # run all-to-all
    if self.params.parallel.a2a == 1: # 1 means: the number of slices in each chunk is 1, i.e. alltoall is done on the whole chunks
      alltoall_reflections = self.get_reflections_from_alltoall()
    else: # do alltoall on chunk slices - useful if the run-time memory is not sufficient to do alltoall on the whole chunks
      alltoall_reflections = self.get_reflections_from_alltoall_sliced(number_of_slices=self.params.parallel.a2a)

    self.logger.log_step_time("SORT")
    self.logger.log("Sorting consolidated reflection table...")
    alltoall_reflections.sort('miller_index_asymmetric')
    self.logger.log_step_time("SORT", True)

    self.logger.log_step_time("GROUP", True)

    return None, alltoall_reflections

  def setup_hkl_chunks(self, reflections):
    '''Set up a list of reflection tables, or chunks, for distributing reflections'''
    # split the full miller set into chunks; the number of chunks is equal to the number of ranks
    import numpy as np
    self.hkl_split_set = np.array_split(self.params.scaling.miller_set.indices(), self.mpi_helper.size)

    # initialize a list of hkl chunks - reflection tables to store distributed reflections
    self.hkl_chunks = []
    for i in range(len(self.hkl_split_set)):
      self.hkl_chunks.append(self.distribute_reflection_table(reflections))

  def distribute_reflections_over_hkl_chunks(self, reflections):
    '''Distribute reflections, according to their HKLs, over pre-set HKL chunks'''
    total_reflection_count = reflections.size()
    total_distributed_reflection_count = 0

    if total_reflection_count > 0:
      # set up two lists to be passed to the C++ extension: HKLs and chunk ids. It's basically a hash table to look up chunk ids by HKLs
      hkl_list = flex.miller_index()
      chunk_id_list = flex.int()

      for i in range(len(self.hkl_split_set)):
        for j in range(len(self.hkl_split_set[i])):
          hkl = (int(self.hkl_split_set[i][j][0]), int(self.hkl_split_set[i][j][1]), int(self.hkl_split_set[i][j][2]))
          hkl_list.append(hkl)
          chunk_id_list.append(i)

      # distribute reflections over hkl chunks, using a C++ extension
      from xfel.merging import get_hkl_chunks_cpp
      get_hkl_chunks_cpp(reflections, hkl_list, chunk_id_list, self.hkl_chunks)
      for chunk in self.hkl_chunks:
        total_distributed_reflection_count += len(chunk)

        # identifiers are dropped so re-add them
        for expt_id in set(chunk['id']):
          chunk.experiment_identifiers()[expt_id] = reflections.experiment_identifiers()[expt_id]
        #chunk.reset_ids() # don't reset them, from this point on, ids are not guaranteed to be contiguous

    self.logger.log("Distributed %d out of %d reflections"%(total_distributed_reflection_count, total_reflection_count))
    self.logger.log("Memory usage: %d MB"%get_memory_usage())

    reflections.clear()

  def get_reflections_from_alltoall(self):
    '''Use MPI alltoall method to gather all reflections with the same asu hkl from all ranks at a single rank'''
    self.logger.log_step_time("ALL-TO-ALL")
    self.logger.log("Executing MPI all-to-all...")

    received_hkl_chunks = self.mpi_helper.comm.alltoall(self.hkl_chunks)

    self.logger.log("Received %d hkl chunks after all-to-all"%len(received_hkl_chunks))
    self.logger.log_step_time("ALL-TO-ALL", True)

    self.logger.log_step_time("CONSOLIDATE")
    self.logger.log("Consolidating reflection tables...")

    result_reflections = flex.reflection_table.concat(received_hkl_chunks)

    self.logger.log_step_time("CONSOLIDATE", True)

    return result_reflections

  def get_reflections_from_alltoall_sliced(self, number_of_slices):
    '''Split each hkl chunk into N slices. This is needed to address the MPI alltoall memory problem'''

    result_reflections = self.distribute_reflection_table() # the total reflection table, which this rank will receive after all slices of alltoall

    list_of_sliced_hkl_chunks = [] # if self.hkl_chunks is [A,B,C...], this list will be [[A1,A2,...,An], [B1,B2,...,Bn], [C1,C2,...,Cn], ...], where n is the number of chunk slices
    for i in range(len(self.hkl_chunks)):
      hkl_chunk_slices = []
      for chunk_slice in reflection_table_utils.get_next_reflection_table_slice(self.hkl_chunks[i], number_of_slices, self.distribute_reflection_table):
        hkl_chunk_slices.append(chunk_slice)
      list_of_sliced_hkl_chunks.append(hkl_chunk_slices)

    self.logger.log("Ready for all-to-all...")
    self.logger.log("Memory usage: %d MB"%get_memory_usage())

    for j in range(number_of_slices):
      hkl_chunks_for_alltoall = list()
      for i in range(len(self.hkl_chunks)):
        hkl_chunks_for_alltoall.append(list_of_sliced_hkl_chunks[i][j]) # [Aj,Bj,Cj...]

      self.logger.log_step_time("ALL-TO-ALL")
      self.logger.log("Executing MPI all-to-all...")
      self.logger.log("Memory usage: %d MB"%get_memory_usage())

      received_hkl_chunks = comm.alltoall(hkl_chunks_for_alltoall)

      self.logger.log("After all-to-all received %d hkl chunks" %len(received_hkl_chunks))
      self.logger.log_step_time("ALL-TO-ALL", True)

      self.logger.log_step_time("CONSOLIDATE")
      self.logger.log("Consolidating reflection tables...")

      for chunk in received_hkl_chunks:
        result_reflections.extend(chunk)

      self.logger.log_step_time("CONSOLIDATE", True)

    return result_reflections

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(hkl_group)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/input/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/input/factory.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.input.file_loader import simple_file_loader
from xfel.merging.application.worker import factory as factory_base

""" Factory class for file loading """

class factory(factory_base):
  @staticmethod
  def from_parameters(params, additional_info=None, mpi_helper=None, mpi_logger=None):
    """ Only one kind of loading supported at present, so construct a simple file loader """
    return [simple_file_loader(params, mpi_helper, mpi_logger)]


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/input/file_lister.py
from __future__ import absolute_import, division, print_function

from collections import namedtuple
import glob
import os
from six.moves import UserDict

import numpy as np
from ordered_set import OrderedSet


class StemLocator(UserDict):
  """Subclass of dict which raises an error when overwriting existing value"""

  class StemExistsError(FileExistsError):
    MSG = 'Multiple files with stem "{}" detected. Matching expts/refls ' \
          'must EITHER be pairwise located in the same directory ' \
          'OR have unique names across all input directories.'

    @classmethod
    def from_stem(cls, stem, *args, **kwargs):
      return cls(cls.MSG.format(stem), *args, **kwargs)  # noqa

  def __setitem__(self, key, value):
    if key in self.data:
      if value != self.data.__getitem__(key):
        raise self.StemExistsError.from_stem(key)
    else:
      self.data.__setitem__(key, value)


class PathPair(namedtuple('PathPair', ['expt_path', 'refl_path'])):
  """Named 2-el. tuple of expt & refl file paths with convenience methods"""
  __slots__ = ()

  expt_suffix = ''
  refl_suffix = ''

  @classmethod
  def from_dir_expt_name(cls, base_path, expt_filename):
    expt_path = os.path.join(base_path, expt_filename)
    refl_path = expt_path.split(cls.expt_suffix, 1)[0] + cls.refl_suffix
    refl_path = refl_path if os.path.exists(refl_path) else None
    return cls(expt_path, refl_path)

  @classmethod
  def from_dir_refl_name(cls, base_path, refl_filename):
    refl_path = os.path.join(base_path, refl_filename)
    expt_path = refl_path.split(cls.refl_suffix, 1)[0] + cls.expt_suffix
    expt_path = expt_path if os.path.exists(expt_path) else None
    return cls(expt_path, refl_path)

  @property
  def expt_stem(self):
    return os.path.basename(self.expt_path).split(self.expt_suffix, 1)[0]

  @property
  def refl_stem(self):
    return os.path.basename(self.refl_path).split(self.refl_suffix, 1)[0]


def list_input_pairs(params):
  """Create a list of expt/refl `PathPair`s on rank 0 based on `params`."""

  # Load alist of files/directories to be exclusively kept or specifically
  # rejected and define all associated alist functions
  alist = set()
  alist_files = params.input.alist.file
  if alist_files is not None:
    for f in alist_files:
      lines = np.loadtxt(f, str, ndmin=1)
      alist = alist.union(lines)
    if params.input.alist.type == "files":
      assert all(os.path.isabs(f) for f in alist)

  def is_on_alist(expt_path):
    if params.input.alist.type == "tags":
      return any(tag in expt_path for tag in alist)
    else:  # alist.type == "files"
      return os.path.abspath(expt_path) in alist

  if len(alist) == 0:
    def is_accepted_expt(*_): return True
  elif params.input.alist.op == "reject":
    def is_accepted_expt(expt_path): return not is_on_alist(expt_path)
  else:  # if params.input.alist.op == "keep"
    def is_accepted_expt(expt_path): return is_on_alist(expt_path)

  # Load paths to the input experiments and reflections. All expt/refl file
  # pairs must have the same filename stem and EITHER be in the same directory
  # OR have unique filename stems across input; otherwise raise StemExistsError
  path_pairs = []
  PathPair.expt_suffix = params.input.experiments_suffix
  PathPair.refl_suffix = params.input.reflections_suffix

  def load_path_if_expt_or_refl(path_, filename_):
    if filename.endswith(params.input.experiments_suffix):
      path_pairs.append(PathPair.from_dir_expt_name(path_, filename_))
    if filename.endswith(params.input.reflections_suffix):
      path_pairs.append(PathPair.from_dir_refl_name(path_, filename_))

  for pathstring in params.input.path:
    for path in glob.glob(pathstring):
      if os.path.isdir(path):
        for filename in os.listdir(path):
          load_path_if_expt_or_refl(path, filename)
      else:
        dir_name, filename = os.path.split(path)
        load_path_if_expt_or_refl(dir_name, filename)
  accepted_pairs = [pp for pp in OrderedSet(path_pairs)
                    if is_accepted_expt(pp.expt_path) or not pp.expt_path]

  # Merge every matching pair of PathPair(expt, None) + PathPair(None, refl)
  # in path_pairs with common stub name into a single PathPair(expt, refl)
  matched_pairs, expt_singlets, refl_singlets = [], StemLocator(), StemLocator()
  for path_pair in accepted_pairs:
    if path_pair.expt_path and path_pair.refl_path:
      matched_pairs.append(path_pair)
    elif path_pair.expt_path and not path_pair.refl_path:
      expt_singlets[path_pair.expt_stem] = path_pair.expt_path
    elif path_pair.refl_path and not path_pair.expt_path:
      refl_singlets[path_pair.refl_stem] = path_pair.refl_path
  common_stems = OrderedSet(expt_singlets).intersection(OrderedSet(refl_singlets))
  new = [PathPair(expt_singlets[c], refl_singlets[c]) for c in common_stems]
  return matched_pairs + new


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/input/file_load_calculator.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import sys
import os
from xfel.merging.application.input.file_lister import list_input_pairs

debug=False # set debug=True, if you want to see a per-rank experiments/reflections file pair list generated by this calculator
available_rank_count = 0  # used only when running this script stand-alone:
                          # if input.parallel_file_load.method=uniform, set this to the available rank count
                          # if input.parallel_file_load.method=node_memory, leave it at zero and the calculator will determine how many nodes you need

b2GB = 1024 * 1024 * 1024 # byte to GB

class file_load_calculator(object):
  def __init__(self, params, file_list, logger=None):
    self.params = params
    self.file_list = file_list
    self.logger = logger
    global debug
    if debug:
      self.debug_log_path = os.path.join(self.params.output.output_dir, 'file_load_calculator.out')
      assert not os.path.exists(self.debug_log_path), "\n\nFile %s already exists. Please either remove this file or set debug=False in file_load_calculator.py and try again.\n"%self.debug_log_path
    else:
      self.debug_log_path = None

  def debug_log_write(self, string):
    if not debug:
      return
    debug_log_file_handle = open(self.debug_log_path, 'a')
    debug_log_file_handle.write(string)
    debug_log_file_handle.close()

  def calculate_file_load(self, available_rank_count=0):
    '''Calculate a load and build a dictionary {rank:file_list} for the input number of ranks'''
    if self.logger:
      self.logger.log_step_time("CALCULATE_FILE_LOAD")
    rank_files = {}
    if self.params.input.parallel_file_load.method == "uniform":
      rank_files = self.calculate_file_load_simple(available_rank_count)
    elif self.params.input.parallel_file_load.method == "node_memory":
      rank_files = self.calculate_file_load_node_memory_based(available_rank_count)

    if debug:
      for rank in range(len(rank_files)):
        self.debug_log_write("\nRank %d"%rank)
        for file_pair in rank_files[rank]:
          self.debug_log_write("\n%s"%str(file_pair))

    total_file_pairs = 0
    for key, value in rank_files.items():
      total_file_pairs += len(value)

    if self.logger:
      self.logger.log("Generated a list of %d file items for %d ranks"%(total_file_pairs, len(rank_files)))
      self.logger.log_step_time("CALCULATE_FILE_LOAD", True)

    return rank_files

  def calculate_file_load_simple(self, available_rank_count):
    '''Uniformly distribute experiments/reflections file pairs over the input number of ranks. Return a dictionary {rank:filepair_list}'''
    assert available_rank_count > 0, "Available rank count has to be greater than zero."
    rank_files = {} #{rank:[file_pair1, file_pair2, ...]}
    for rank in range(0, available_rank_count):
      rank_files[rank] = self.file_list[rank::available_rank_count]

    return rank_files

  def calculate_file_load_node_memory_based(self, available_rank_count):
    '''Assign experiments/reflections file pairs to nodes taking into account the node memory limit. Then distribute node-assigned file pairs over the ranks within each node. Return a dictionary {rank:file_list}'''
    # get sizes of all files
    file_sizes = {} # {file_pair:file_pair_size_GB}
    for index in range(len(self.file_list)):
      file_sizes[self.file_list[index]] = os.stat(self.file_list[index][1]).st_size / b2GB # [1] means: use only the reflection file size for now

    # assign files to the anticipated nodes - based on the file sizes and the node memory limit
    node_files = {} # {node:[file_pair1, file_pair2,...]}
    node = 0
    node_files[node] = []
    anticipated_memory_usage_GB = 0
    for file_pair in file_sizes:
      anticipated_memory_usage_GB += (file_sizes[file_pair] * self.params.input.parallel_file_load.node_memory.pickle_to_memory)
      if anticipated_memory_usage_GB < self.params.input.parallel_file_load.node_memory.limit: # keep appending the files as long as the total anticipated memory doesn't exceed the node memory limit
        node_files[node].append(file_pair)
      else:
        node += 1
        node_files[node] = []
        node_files[node].append(file_pair)
        anticipated_memory_usage_GB = (file_sizes[file_pair] * self.params.input.parallel_file_load.node_memory.pickle_to_memory)

    # now we know how many nodes are required
    required_number_of_nodes = len(node_files)
    print("\nMinimum required number of nodes for mpi.merge: %d\n"%required_number_of_nodes)

    # for each node evenly distribute the files over the ranks
    rank_files = {} #{rank:[file_pair1, file_pair2, ...]}
    rank_base = 0 # the first rank of a node
    required_number_of_ranks = 0 # on all nodes
    for node in range(required_number_of_nodes):
      rank_base = (node * self.params.input.parallel_file_load.ranks_per_node)
      for rank in range(rank_base, rank_base + self.params.input.parallel_file_load.ranks_per_node):
        rank_files[rank] = node_files[node][rank - rank_base::self.params.input.parallel_file_load.ranks_per_node]
        if len(rank_files[rank]) > 0:
          required_number_of_ranks += 1

    if available_rank_count > 0: # if the caller has provided the available rank count, assert that we have enough ranks
      assert required_number_of_ranks <= available_rank_count, "Not enough ranks to load the reflection files: available %d rank(s), required %d rank(s)"%(available_rank_count, required_number_of_ranks)

    # optionally print out the anticipated memory load per node
    if debug:
      for node in range(required_number_of_nodes):
        anticipated_memory_usage_GB = 0
        for file_pair in node_files[node]:
          anticipated_memory_usage_GB += os.stat(file_pair[1]).st_size / b2GB * self.params.input.parallel_file_load.node_memory.pickle_to_memory
        self.debug_log_write("\nNode %d: anticipated memory usage %f GB"%(node, anticipated_memory_usage_GB))

    if debug:
      print ("File load calculator output file: ", self.debug_log_path)

    return rank_files

from xfel.merging.application.phil.phil import Script as Script_Base
class Script(Script_Base):
  '''A class for running the script.'''

  def get_file_list(self):
    '''Get experiments/reflections file list'''
    file_list = list_input_pairs(self.params)
    print("Built an input list of %d experiments/reflections file pairs"%len(file_list))
    print("To view the list, set debug=True in file_load_calculator.py")
    return file_list

  def run(self, available_rank_count=0):
    # Read and parse phil
    self.initialize()
    self.validate()

    # Calculate file load
    load_calculator = file_load_calculator(self.params, self.get_file_list())
    rank_files = load_calculator.calculate_file_load(available_rank_count)

    print ("OK")
    return

  def validate(self):
    """ Override to perform any validation of the input parameters """
    pass

if __name__ == '__main__':
  script = Script()
  result = script.run(available_rank_count)
  if result is None:
    sys.exit(1)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/input/file_loader.py
from __future__ import absolute_import, division, print_function
import os
from dxtbx.model.experiment_list import ExperimentListFactory
from dials.array_family import flex
from six.moves import range
import json
from xfel.merging.application.input.file_lister import list_input_pairs
from xfel.merging.application.input.file_load_calculator import file_load_calculator
from xfel.merging.application.utils.memory_usage import get_memory_usage

"""
Utility functions used for reading input data
"""

def create_experiment_identifier(experiment, experiment_file_path, experiment_id):
  'Create a hashed experiment identifier based on the experiment file path, experiment index in the file, and experiment features'
  import hashlib
  exp_identifier_str = os.path.basename(experiment_file_path + \
                       str(experiment_id) + \
                       str(experiment.beam) + \
                       str(experiment.crystal) + \
                       str(experiment.detector) + \
                       ''.join([os.path.basename(p) for p in experiment.imageset.paths()]))
  hash_obj = hashlib.md5(exp_identifier_str.encode('utf-8'))
  return hash_obj.hexdigest()


def preGen_experiment_identifiers(experiments, exp_filename):
  """
  Label experiments according to image number (for multi-image files), lattice number, H
  where H is a unique per-experiment hash
  This information will be preserved in the reflection files that are optionally output
  if output.save_experiments_and_reflections=True.
  """
  done_expts = {}
  for i_exp, expt in enumerate(experiments):
    exp_hash = create_experiment_identifier(expt, exp_filename, i_exp)
    iset = expt.imageset
    path = iset.paths()[0]
    single_file_index = iset.indices()[0]
    key = "%s_%s" % (path, single_file_index)
    n_hits = done_expts.setdefault(key, 0)
    done_expts[key] = n_hits + 1
    ident = "I%s_L%s_%s" % (single_file_index, n_hits, exp_hash)
    expt.identifier = ident

#for integration pickles:
allowable_basename_endings = ["_00000.pickle",
                              ".pickle",
                              ".refl",
                              "_refined_experiments.json",
                              "_refined.expt",
                              "_experiments.json",
                              "_indexed.expt"
                             ]
def is_odd_numbered(file_name, use_hash = False):
  if use_hash:
    import hashlib
    hash_object = hashlib.md5(file_name.encode('utf-8'))
    return int(hash_object.hexdigest(), 16) % 2 == 0
  for allowable in allowable_basename_endings:
    if (file_name.endswith(allowable)):
      try:
        return int(os.path.basename(file_name).split(allowable)[-2][-1])%2==1
      except ValueError:
        file_name = os.path.basename(file_name).split(allowable)[0]
        break
  #can not find standard filename extension, instead find the last digit:
  for idx in range(1,len(file_name)+1):
    if file_name[-idx].isdigit():
      return int(file_name[-idx])%2==1
  raise ValueError
#if __name__=="__main__":
#  print is_odd_numbered("int_fake_19989.img")


def identifiers_void(*identifiers):
  """True only if all identifiers evaluate to False (eg. '', None)"""
  return not any(identifiers)


def identifiers_match(*identifiers):
  """True only if all identifiers match"""
  return len(set(identifiers)) <= 1


from xfel.merging.application.worker import worker
class simple_file_loader(worker):
  '''A class for running the script.'''

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(simple_file_loader, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Read experiments and data'

  def run(self, all_experiments, all_reflections):
    """ Load all the data using MPI """
    from dxtbx.model.experiment_list import ExperimentList
    from dials.array_family import flex

    # Both must be none or not none
    test = [all_experiments is None, all_reflections is None].count(True)
    assert test in [0,2]
    if test == 2:
      all_experiments = ExperimentList()
      all_reflections = flex.reflection_table()
      starting_expts_count = starting_refls_count = 0
    else:
      starting_expts_count = len(all_experiments)
      starting_refls_count = len(all_reflections)
    self.logger.log("Initial number of experiments: %d; Initial number of reflections: %d"%(starting_expts_count, starting_refls_count))

    # Generate and send a list of file paths to each worker
    if self.mpi_helper.rank == 0:
      file_list = list_input_pairs(self.params)
      self.logger.log("Built an input list of %d json/pickle file pairs"%(len(file_list)))

      # optionally write a file list mapping to disk, useful in post processing if save_experiments_and_reflections=True
      file_id_from_names = None
      if self.params.output.expanded_bookkeeping:
        apath = lambda x: os.path.abspath(x)
        file_names_from_id = {i_f: tuple(map(apath, exp_ref_pair)) for i_f, exp_ref_pair in enumerate(file_list)}
        with open(os.path.join(self.params.output.output_dir, "file_list_map.json"), "w") as o:
          json.dump(file_names_from_id, o)
        file_id_from_names = {tuple(map(apath, exp_ref_pair)): i_f for i_f, exp_ref_pair in enumerate(file_list)}

      per_rank_file_list = file_load_calculator(self.params, file_list, self.logger).\
                              calculate_file_load(available_rank_count = self.mpi_helper.size)
      self.logger.log('Transmitting a list of %d lists of json/pickle file pairs'%(len(per_rank_file_list)))
      transmitted = per_rank_file_list, file_id_from_names
    else:
      transmitted = None

    self.logger.log_step_time("BROADCAST_FILE_LIST")
    new_file_list, file_names_mapping = self.mpi_helper.comm.bcast(transmitted, root = 0)
    new_file_list = new_file_list[self.mpi_helper.rank] if self.mpi_helper.rank < len(new_file_list) else None
    self.logger.log_step_time("BROADCAST_FILE_LIST", True)

    # Load the data
    self.logger.log_step_time("LOAD")
    if new_file_list is not None:
      self.logger.log("Received a list of %d json/pickle file pairs"%len(new_file_list))
      for experiments_filename, reflections_filename in new_file_list:
        self.logger.log("Reading %s %s"%(experiments_filename, reflections_filename))
        experiments = ExperimentListFactory.from_json_file(experiments_filename, check_format = self.params.input.read_image_headers)
        reflections = flex.reflection_table.from_file(reflections_filename)
        if self.params.output.expanded_bookkeeping:
          # NOTE: these are un-prunable
          reflections["input_refl_index"] = flex.int(
            list(range(len(reflections))))
          reflections["original_id"] = reflections['id']
          assert file_names_mapping is not None
          exp_ref_pair = os.path.abspath(experiments_filename), os.path.abspath(reflections_filename)
          this_refl_fileMappings = [file_names_mapping[exp_ref_pair]]*len(reflections)
          reflections["file_list_mapping"] = flex.int(this_refl_fileMappings)
        self.logger.log("Data read, prepping")

        if 'intensity.sum.value' in reflections:
          reflections['intensity.sum.value.unmodified'] = reflections['intensity.sum.value'] * 1
        if 'intensity.sum.variance' in reflections:
          reflections['intensity.sum.variance.unmodified'] = reflections['intensity.sum.variance'] * 1

        new_ids = flex.int(len(reflections), -1)
        eid = reflections.experiment_identifiers()
        eid_copy = dict(eid)
        for k in eid.keys():
          del eid[k]

        if self.params.output.expanded_bookkeeping:
          preGen_experiment_identifiers(experiments, experiments_filename)
        for experiment_id, experiment in enumerate(experiments):
          # select reflections of the current experiment
          refls_sel = reflections['id'] == experiment_id

          if refls_sel.count(True) == 0: continue

          refls_identifier = eid_copy.get(experiment_id, '')
          if identifiers_void(experiment.identifier, refls_identifier) \
                  or self.params.input.override_identifiers:
            new_identifier = create_experiment_identifier(
              experiment, experiments_filename, experiment_id)
            experiment.identifier = new_identifier
          elif not identifiers_match(experiment.identifier, refls_identifier):
            m = 'Expt and refl identifier mismatch: "{}" in {} vs "{}" in {}'
            raise KeyError(m.format(experiment.identifier, experiments_filename,
                                    refls_identifier, reflections_filename))

          if not self.params.input.keep_imagesets:
            experiment.imageset = None
          all_experiments.append(experiment)

          # Reflection 'id' is unique within this rank; experiment.identifier is unique globally
          new_id = len(all_experiments)-1
          eid[new_id] = experiment.identifier
          new_ids.set_selected(refls_sel, new_id)
        assert (new_ids < 0).count(True) == 0, "Not all reflections accounted for"
        reflections['id'] = new_ids
        all_reflections.extend(reflections)
    else:
      self.logger.log("Received a list of 0 json/pickle file pairs")
    self.logger.log_step_time("LOAD", True)

    self.logger.log('Read %d experiments consisting of %d reflections'%(len(all_experiments)-starting_expts_count, len(all_reflections)-starting_refls_count))
    self.logger.log("Memory usage: %d MB"%get_memory_usage())

    all_reflections = self.prune_reflection_table_keys(all_reflections)

    # Do we have any data?
    from xfel.merging.application.utils.data_counter import data_counter
    data_counter(self.params).count(all_experiments, all_reflections)
    return all_experiments, all_reflections

  def prune_reflection_table_keys(self, reflections):
    from xfel.merging.application.reflection_table_utils import reflection_table_utils
    reflections = reflection_table_utils.prune_reflection_table_keys(reflections=reflections,
                    keys_to_keep=['id', 'intensity.sum.value', 'intensity.sum.variance', 'miller_index', 'miller_index_asymmetric', \
                                  's1', 'intensity.sum.value.unmodified', 'intensity.sum.variance.unmodified',
                                  'kapton_absorption_correction', 'flags'],
                    keys_to_ignore=self.params.input.persistent_refl_cols)
    self.logger.log("Pruned reflection table")
    self.logger.log("Memory usage: %d MB"%get_memory_usage())
    return reflections

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(simple_file_loader)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/integrate/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/integrate/factory.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.integrate.integrate import integrate
from xfel.merging.application.worker import factory as factory_base

class factory(factory_base):
  """ Factory class for modification of intensites. """
  @staticmethod
  def from_parameters(params, additional_info=None, mpi_helper=None, mpi_logger=None):
    return [integrate(params, mpi_helper, mpi_logger)]


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/integrate/integrate.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.worker import worker
from dxtbx.imageset import ImageSetFactory
from dxtbx.model.experiment_list import ExperimentList
from dials.array_family import flex
import os

from dials.command_line.stills_process import Processor
class integrate_only_processor(Processor):
  def __init__(self, params):
    self.params = params

class integrate(worker):
  """
  Calls the stills process version of dials.integrate
  """
  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(integrate, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Integrate reflections'

  def run(self, experiments, reflections):
    from dials.util import log
    self.logger.log_step_time("INTEGRATE")

    logfile = os.path.splitext(self.logger.rank_log_file_path)[0] + "_integrate.log"
    log.config(logfile=logfile)
    processor = integrate_only_processor(self.params)

    # Re-generate the image sets using their format classes so we can read the raw data
    # Integrate the experiments one at a time to not use up memory
    all_integrated_expts = ExperimentList()
    all_integrated_refls = None
    current_imageset = None
    current_imageset_path = None
    for expt_id, expt in enumerate(experiments):
      assert len(expt.imageset.paths()) == 1 and len(expt.imageset) == 1
      self.logger.log("Starting integration experiment %d"%expt_id)
      refls = reflections.select(reflections['id'] == expt_id)
      if expt.imageset.paths()[0] != current_imageset_path:
        current_imageset_path = expt.imageset.paths()[0]
        current_imageset = ImageSetFactory.make_imageset(expt.imageset.paths())
      idx = expt.imageset.indices()[0]
      expt.imageset = current_imageset[idx:idx+1]
      idents = refls.experiment_identifiers()
      del idents[expt_id]
      idents[0] = expt.identifier
      refls['id'] = flex.int(len(refls), 0)

      try:
        integrated = processor.integrate(experiments[expt_id:expt_id+1], refls)
      except RuntimeError:
        self.logger.log("Error integrating expt %d"%expt_id)
        continue

      all_integrated_expts.append(expt)
      if all_integrated_refls:
        all_integrated_refls = flex.reflection_table.concat([all_integrated_refls, integrated])
      else:
        all_integrated_refls = integrated

    if all_integrated_refls is None:
      all_integrated_refls = flex.reflection_table()
    self.logger.log("Integration done, %d experiments, %d reflections" %
                    (len(all_integrated_expts), len(all_integrated_refls)))
    return all_integrated_expts, all_integrated_refls


if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(integrate)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/lunus/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/lunus/factory.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.lunus.lunus import lunus
from xfel.merging.application.worker import factory as factory_base

class factory(factory_base):
  """ Factory class for modification of intensites. """
  @staticmethod
  def from_parameters(params, additional_info=None, mpi_helper=None, mpi_logger=None):
      return [lunus(params, mpi_helper, mpi_logger)]


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/lunus/lunus.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.worker import worker
from dxtbx.model.experiment_list import ExperimentList
from dxtbx.imageset import ImageSetFactory
from dials.array_family import flex
from lunus.command_line.process import get_experiment_params, get_experiment_xvectors
import lunus as lunus_processor
from scitbx import matrix
import numpy as np

class lunus(worker):
  """
  Calls into the Lunus library to do diffuse scatter integration

  See DOI: 10.1007/978-1-59745-483-4_17
  """
  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(lunus, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

    self.current_path = None
    self.current_imageset = None
#    mpi_init()

  def __repr__(self):
    return 'Process diffuse scattering using Lunus, yielding a 3D dataset'

  def filter_by_n_laticces(self, experiments, reflections, n = 1):
    # filter out experiments with more than one lattice
    image_list = {}
    for expt in experiments:
      assert len(expt.imageset.paths()) == 1
      path = expt.imageset.paths()[0]
      if path not in image_list:
        image_list[path] = {}
      assert len(expt.imageset.indices()) == 1
      index = expt.imageset.indices()[0]
      if index in image_list[path]:
        image_list[path][index] += 1
      else:
        image_list[path][index] = 1
    all_image_lists = self.mpi_helper.comm.gather(image_list)
    if self.mpi_helper.rank == 0:
      all_image_list = {}
      for ilist in all_image_lists:
        for path in ilist:
          if path not in all_image_list:
            all_image_list[path] = {}
          for index in ilist[path]:
            if index in all_image_list[path]:
              all_image_list[path][index] += ilist[path][index]
            else:
              all_image_list[path][index] = ilist[path][index]
    else:
      all_image_list = None
    image_list = self.mpi_helper.comm.bcast(all_image_list, root=0)

    filtered_expts = ExperimentList()
    refls_sel = flex.bool(len(reflections), False)
    for expt_id, expt in enumerate(experiments):
      path = expt.imageset.paths()[0]
      index = expt.imageset.indices()[0]
      if image_list[path][index] == n:
        filtered_expts.append(expt)
        refls_sel |= reflections['id'] == expt_id
    filtered_refls = reflections.select(refls_sel)
    filtered_refls.reset_ids()
    self.logger.log("Filtered out %d experiments with more than %d lattices out of %d"%((len(experiments)-len(filtered_expts), n, len(experiments))))
    return filtered_expts, filtered_refls

  def run(self, experiments, reflections):

    self.logger.log_step_time("LUNUS")

    experiments, reflections = self.filter_by_n_laticces(experiments, reflections)

    if self.mpi_helper.rank == 0:
      self.reference_experiments = experiments[0:1]
      self.logger.log("LUNUS: Using reference experiment %s with image %s %d" % (self.reference_experiments[0].identifier,self.reference_experiments[0].imageset.paths()[0],self.reference_experiments[0].imageset.indices()[0]))
    else:
      self.reference_experiments = None

    self.reference_experiments = self.mpi_helper.comm.bcast(self.reference_experiments, root=0)

    reference_experiment_params = get_experiment_params(self.reference_experiments[0:1])
    self.processor = lunus_processor.Process(len(reference_experiment_params))

    if self.mpi_helper.rank == 0:
      with open(self.params.lunus.deck_file) as f:
        self.deck = f.read()
    else:
      self.deck = None

    self.deck = self.mpi_helper.comm.bcast(self.deck, root=0)

    deck_and_extras = self.deck+reference_experiment_params[0]
    self.processor.LunusSetparamslt(deck_and_extras)

    self.lunus_integrate(self.reference_experiments, is_reference = True)

    for i in range(len(experiments)):
      self.lunus_integrate(experiments[i:i+1])

    self.finalize()

    self.logger.log_step_time("LUNUS", True)

    return experiments, reflections

  def lunus_integrate(self, experiments, is_reference = False):
    assert len(experiments) == 1

    experiment_params = get_experiment_params(experiments)
    p = self.processor

#    self.logger.log("LUNUS_INTEGRATE: Passed %s %d" % (experiments[0].imageset.paths()[0],experiments[0].imageset.indices()[0]))

    if self.current_path != experiments[0].imageset.paths()[0]:
      self.current_imageset = ImageSetFactory.make_imageset(experiments[0].imageset.paths())
    idx = experiments[0].imageset.indices()[0]
    experiments[0].imageset = self.current_imageset[idx:idx+1]

    self.logger.log("LUNUS_INTEGRATE: Processing image %s %d" % (experiments[0].imageset.paths()[0],experiments[0].imageset.indices()[0]))

    data = experiments[0].imageset[0]
    if not isinstance(data, tuple):
      data = data,
    for panel_idx, panel in enumerate(data):
      self.processor.set_image(panel_idx, panel)
#      self.logger.log("LUNUS_INTEGRATE: file %s panel_idx %d panel[0:10] = %s " % (experiments[0].imageset.paths()[0],panel_idx,str(list(panel[0:10]))))

    for pidx in range(len(experiment_params)):
      deck_and_extras = self.deck+experiment_params[pidx]
      p.LunusSetparamsim(pidx,deck_and_extras)

#    self.logger.log("LUNUS: Processing image")

    if is_reference:
      x = get_experiment_xvectors(experiments)
      for pidx in range(len(x)):
        p.set_xvectors(pidx,x[pidx])

      # We need an amatrix for the next call, to set up the lattice size
      if self.params.merging.set_average_unit_cell:
        assert 'average_unit_cell' in (self.params.statistics).__dict__
        uc = self.params.statistics.__phil_get__('average_unit_cell')
      else:
        uc = self.params.scaling.unit_cell

      assert uc is not None, "Lunus needs a target unit cell"
      A_matrix = matrix.sqr(uc.orthogonalization_matrix())
      At_flex = A_matrix.transpose().as_flex_double_matrix()

      p.set_amatrix(At_flex)

      p.LunusProcimlt(0)
    else:
      crystal = experiments[0].crystal
      A_matrix = matrix.sqr(crystal.get_A()).inverse()
      At_flex = A_matrix.transpose().as_flex_double_matrix()

      p.set_amatrix(At_flex)

      p.LunusProcimlt(1)

  def finalize(self):
    self.mpi_helper.comm.Barrier() # Need to synchronize at this point so that all the server/client ranks finish

    self.logger.log("LUNUS: STARTING FINALIZE, rank %d, size %d"%(self.mpi_helper.rank, self.mpi_helper.size))

    p = self.processor
    p = self.mpi_reduce_p(p)

    if self.mpi_helper.rank == 0:
      p.divide_by_counts()
      p.write_as_hkl('results.hkl')
      p.write_as_vtk('results.vtk')

  def mpi_reduce_p(self,p, root=0):
    l = p.get_lattice().as_numpy_array()
    c = p.get_counts().as_numpy_array()

    if self.mpi_helper.rank == root:
      lt = np.zeros_like(l)
      ct = np.zeros_like(c)
    else:
      lt = None
      ct = None

    self.mpi_helper.comm.Reduce(l,lt,op=self.mpi_helper.MPI.SUM,root=root)
    self.mpi_helper.comm.Reduce(c,ct,op=self.mpi_helper.MPI.SUM,root=root)

    if self.mpi_helper.rank == root:
      p.set_lattice(flex.double(lt))
      p.set_counts(flex.int(ct))

    return p

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(lunus)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/merge/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/merge/factory.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.merge.merger import merger
from xfel.merging.application.worker import factory as factory_base

class factory(factory_base):
  """Factory class for calculating averaged intensities of symmetry-reduced HKLs."""
  @staticmethod
  def from_parameters(params, additional_info=None, mpi_helper=None, mpi_logger=None):
    return [merger(params, mpi_helper, mpi_logger)]


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/merge/merger.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.worker import worker
from xfel.merging.application.reflection_table_utils import \
    reflection_table_utils as rt_util
from cctbx.crystal import symmetry
from cctbx import miller
import os
from six.moves import cStringIO as StringIO
import numpy as np
from scitbx.array_family import flex

class merger(worker):
  """
  Merges multiple measurements of symmetry-reduced HKLs.
  """
  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(merger, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return "Merge multiple measurements of symmetry-reduced HKLs"

  def run(self, experiments, reflections):

    sel_col = "id"
    if self.params.statistics.shuffle_ids:
      refl_ids = list(set(reflections["id"]))
      new_ids = np.random.permutation(refl_ids).astype(np.int32)
      new_id_map = {old_id: new_id for old_id, new_id in zip(refl_ids, new_ids)}
      new_id_col = [new_id_map[i] for i in reflections["id"]]
      reflections["shuffled_id"] = flex.int(new_id_col)
      sel_col = "shuffled_id"

    # select, merge and output odd reflections
    odd_reflections = rt_util.select_odd_experiment_reflections(reflections, sel_col)
    odd_reflections_merged = rt_util.merge_reflections(
        odd_reflections,
        self.params.merging.minimum_multiplicity,
        thresh=self.params.filter.outlier.mad_thresh
    )
    self.gather_and_output_reflections(odd_reflections_merged, 'odd')

    # select, merge and output even reflections
    even_reflections = rt_util.select_even_experiment_reflections(reflections, sel_col)
    even_reflections_merged = rt_util.merge_reflections(
        even_reflections,
        self.params.merging.minimum_multiplicity,
        thresh=self.params.filter.outlier.mad_thresh
    )
    self.gather_and_output_reflections(even_reflections_merged, 'even')

    # merge and output all reflections
    name = "merged_good_refls2/rank%d" % self.mpi_helper.comm.rank
    all_reflections_merged = rt_util.merge_reflections(
        reflections,
        self.params.merging.minimum_multiplicity,
        nameprefix=name,
        thresh=self.params.filter.outlier.mad_thresh
    )
    self.gather_and_output_reflections(all_reflections_merged, 'all')

    return None, reflections

  def gather_and_output_reflections(self, reflections, selection_name):
    # gather merged HKLs from all ranks
    self.logger.log_step_time("GATHER")
    self.logger.log("Running MPI-gather on merged %s HKLs..."%selection_name)
    all_merged_reflection_tables = self.mpi_helper.comm.gather(reflections, root = 0)
    self.logger.log_step_time("GATHER", True)

    # concatenate all merged HKLs
    if self.mpi_helper.rank == 0:
      self.logger.log_step_time("MERGE")
      final_merged_reflection_table = rt_util.merged_reflection_table()
      self.logger.log("Concatenating merged %s HKLs at rank 0..."%selection_name)
      for table in all_merged_reflection_tables:
        final_merged_reflection_table.extend(table)
      self.logger.main_log("Total (not limited by resolution) merged %s HKLs: %d"%(selection_name, final_merged_reflection_table.size()))
      self.logger.log_step_time("MERGE", True)

      # output as mtz
      if len(final_merged_reflection_table) > 0:
        self.output_reflections_mtz(final_merged_reflection_table, selection_name)

      # free the memory
      del all_merged_reflection_tables
      del final_merged_reflection_table

  def output_reflections_mtz(self, reflections, filename_postfix):
    if self.params.merging.set_average_unit_cell:
      assert 'average_unit_cell' in (self.params.statistics).__dict__
      unit_cell = self.params.statistics.__phil_get__('average_unit_cell')
    else:
      unit_cell = self.params.scaling.unit_cell

    final_symm = symmetry(
                          unit_cell=unit_cell,
                          space_group_info = self.params.scaling.space_group)

    assert 'average_wavelength' in (self.params.statistics).__dict__
    wavelength = self.params.statistics.__phil_get__('average_wavelength')

    if self.params.merging.d_max is None:
      self.logger.main_log("Output merged HKLs limited by %f A resolution"%(self.params.merging.d_min))
    else:
      self.logger.main_log("Output merged HKLs limited by (%f - %f) A resolution range"%(self.params.merging.d_max, self.params.merging.d_min))

    all_obs = miller.array(
      miller_set=miller.set(final_symm, reflections['miller_index'], not self.params.merging.merge_anomalous),
      data=reflections['intensity'],
      sigmas=reflections['sigma']).resolution_filter(
                                   d_min=self.params.merging.d_min,
                                   d_max=self.params.merging.d_max).set_observation_type_xray_intensity()

    mtz_file = os.path.join(self.params.output.output_dir, "%s_%s.mtz"%(self.params.output.prefix, filename_postfix))

    mtz_out = all_obs.as_mtz_dataset(
      column_root_label="Iobs",
      title=self.params.output.title,
      wavelength=wavelength)
    mtz_out.add_miller_array(
      miller_array=all_obs.average_bijvoet_mates(),
      column_root_label="IMEAN")

    if self.params.merging.include_multiplicity_column:
      all_mult = miller.array(
        miller_set=miller.set(final_symm, reflections['miller_index'], not self.params.merging.merge_anomalous),
        data=reflections['multiplicity']).resolution_filter(
                                          d_min=self.params.merging.d_min,
                                          d_max=self.params.merging.d_max).set_observation_type_xray_intensity()
      mtz_out.add_miller_array(
        miller_array=all_mult,
        column_root_label='multiplicity')

    mtz_obj = mtz_out.mtz_object()

    # Create formatted timestamp with both local and GMT time
    import time
    date_str = time.strftime("%Y-%m-%d at %H:%M:%S %Z")

    # Add history record with version and timestamp
    from dials.util.version import dials_version
    mtz_obj.add_history(f"From cctbx.xfel.merge {dials_version()}, {date_str}")

    mtz_obj.write(mtz_file)
    self.logger.main_log("Output anomalous and mean data:\n    %s" %os.path.abspath(mtz_file))
    self.logger.main_log("Output data summary:")
    out = StringIO()
    all_obs.show_summary(prefix="  ", f=out)
    self.logger.main_log(out.getvalue())

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(merge)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/model/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/model/crystal_model.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.worker import worker
import mmtbx.programs.fmodel
import mmtbx.utils
from cctbx import miller
from cctbx.crystal import symmetry
import iotbx.pdb
import mmtbx.model
from six.moves import cStringIO as StringIO
import os

class crystal_model(worker):

  def __repr__(self):
    return 'Build crystal model'

  def __init__(self, params, purpose, mpi_helper=None, mpi_logger=None):
    super(crystal_model, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)
    self.purpose = purpose

  def run(self, experiments, reflections):
    '''Create a model for the crystal'''
    self.logger.log_step_time("CREATE_CRYSTAL_MODEL")

    # perform some run-time validation
    assert self.purpose in ["scaling", "statistics", "cosym"]
    if self.purpose == "statistics" and self.params.merging.set_average_unit_cell: # without this flag the average unit cell wouldn't have been generated
      assert 'average_unit_cell' in (self.params.statistics).__dict__ # the worker, tasked with averaging the unit cell, would put it there

    # Generate the reference, or model, intensities
    i_model = None
    if self.purpose in ["scaling","cosym"]:
      model_file_path = str(self.params.scaling.model)
      if model_file_path is not None:
        self.logger.log("Scaling model: " + model_file_path)
        if self.mpi_helper.rank == 0:
          self.logger.main_log("Scaling model: " + model_file_path)
      else:
        self.logger.log("No scaling model has been provided")
        if self.mpi_helper.rank == 0:
          self.logger.main_log("No scaling model has been provided")
    elif self.purpose == "statistics":
      model_file_path = str(self.params.statistics.cciso.mtz_file)
      if model_file_path is not None:
        self.logger.log("Reference for statistics: " + model_file_path)
        if self.mpi_helper.rank == 0:
          self.logger.main_log("Reference for statistics: " + model_file_path)
      else:
        self.logger.log("No reference for statistics has been provided")
        if self.mpi_helper.rank == 0:
          self.logger.main_log("No reference for statistics has been provided")

    if model_file_path is not None:
      if model_file_path.endswith(".mtz") or model_file_path.endswith("sf.cif"):
        i_model = self.create_model_from_mtz(model_file_path)
      elif model_file_path.endswith(".pdb"):
        i_model = self.create_model_from_pdb(model_file_path)
      elif model_file_path.endswith(".cif"):
        i_model = self.create_model_from_structure_file(model_file_path)

    if self.purpose == "cosym":
      return i_model

    # Generate a full miller set. If the purpose is scaling and i_model is available, then the miller set has to be consistent with the model
    miller_set, i_model = self.consistent_set_and_model(i_model=i_model)

    # Save i_model and miller_set to the parameters
    if i_model is not None:
      if not 'i_model' in self.params.scaling.__dict__:
        self.params.scaling.__inject__('i_model', i_model)
      else:
        self.params.scaling.__setattr__('i_model', i_model)

    if not 'miller_set' in self.params.scaling.__dict__:
      self.params.scaling.__inject__('miller_set', miller_set)
    else:
      self.params.scaling.__setattr__('miller_set', miller_set)

    self.logger.log_step_time("CREATE_CRYSTAL_MODEL", True)

    # Add asymmetric unit indexes to the reflection table
    if self.purpose == "scaling":
      self.logger.log_step_time("ADD_ASU_HKL_COLUMN")
      self.add_asu_miller_indices_column(reflections)
      self.logger.log_step_time("ADD_ASU_HKL_COLUMN", True)

    return experiments, reflections

  def create_model_from_pdb(self, model_file_path):
      return self.create_model_from_structure_file(model_file_path)

  def create_model_from_structure_file(self, model_file_path):

    if self.mpi_helper.rank == 0:
      model_ext = os.path.splitext(model_file_path)[-1].lower()
      assert model_ext in [".pdb", ".cif"]
      if model_ext == ".pdb":
        import iotbx.pdb
        pdb_in = iotbx.pdb.input(model_file_path)
        xray_structure = pdb_in.xray_structure_simple()
      elif model_ext == ".cif":
        from libtbx.utils import Sorry
        try:
          from cctbx.xray import structure
          xs_dict = structure.from_cif(file_path=model_file_path)
          assert len(xs_dict) == 1, "CIF should contain only one xray structure"
          xray_structure = list(xs_dict.values())[0]
        except Sorry:
          inp = iotbx.pdb.input(model_file_path)
          model = mmtbx.model.manager(model_input=inp)
          xray_structure = model.get_xray_structure()
      out = StringIO()
      xray_structure.show_summary(f=out)
      self.logger.main_log(out.getvalue())
      space_group = xray_structure.crystal_symmetry().space_group().info()
      unit_cell = xray_structure.crystal_symmetry().unit_cell()
    else:
      xray_structure = space_group = unit_cell = None
    if self.purpose != "cosym":
      xray_structure, space_group, unit_cell = self.mpi_helper.comm.bcast((xray_structure, space_group, unit_cell), root=0)
    if self.purpose == "scaling":
      # save space group and unit cell as scaling targets
      self.params.scaling.space_group = space_group
      self.params.scaling.unit_cell = unit_cell

    # prepare phil parameters to generate model intensities
    phil2 = mmtbx.programs.fmodel.master_phil
    params2 = phil2.extract()

    '''
    #
    # RB: for later
    #
    # We need to set the model resolution limits here, which will determine the set of asu HKLs,
    # for which the intensities will be calculated. So our input resolution limits need to be adjusted
    # to account for the difference between the model unit cell and the variations of the unit cells in the experiments.
    # Ideally, we should calculate the model intensity for _every_ observed HKL in every experiment, which might be a time-wasteful calculation.
    # So we use a shortcut: a ratio of the corresponding unt cell volumes as well as a fudge factor - "resolution scalar".

    model_unit_cell_volume = xray_structure.crystal_symmetry().unit_cell().volume()

    min_exp_unit_cell_volume, max_exp_unit_cell_volume = self.get_min_max_experiment_unit_cell_volume(self.experiments)

    assert self.params.scaling.resolution_scalar > 0 and self.params.scaling.resolution_scalar < 1.0 # the way this scalar is used, it should be less than unity

    params2.high_resolution = self.params.merging.d_min / math.pow(max_exp_unit_cell_volume/model_unit_cell_volume, 1./3.) * self.params.scaling.resolution_scalar
    if self.params.merging.d_max is not None:
      params2.low_resolution = self.params.merging.d_max / math.pow(min_exp_unit_cell_volume/model_unit_cell_volume, 1./3.) / self.params.scaling.resolution_scalar
    '''

    #
    # RB: for now we do it the way it's done in cxi.merge. There is a problem with that approach though,
    # because the "filter unit cell" may be different from the "model unit cell",
    # so the cell variations we are trying to account for here might be for a different unit cell.
    #
    # from cxi.merge:
    # adjust the cutoff of the generated intensities to assure that
    # statistics will be reported to the desired high-resolution limit
    # even if the observed unit cell differs slightly from the reference.
    params2.high_resolution = self.params.merging.d_min * self.params.scaling.resolution_scalar
    if self.params.merging.d_max is not None:
      params2.low_resolution = self.params.merging.d_max / self.params.scaling.resolution_scalar

    params2.output.type = "real"

    if self.params.scaling.pdb.include_bulk_solvent:
      params2.fmodel.k_sol = self.params.scaling.pdb.k_sol
      params2.fmodel.b_sol = self.params.scaling.pdb.b_sol

    # vvv These params restore the "legacy" solvent mask generation before
    # vvv cctbx commit 2243cc9a
    if self.params.scaling.pdb.solvent_algorithm == "flat":
      params2.mask.Fmask_res_high = 0
      params2.mask.grid_step_factor = 4
      params2.mask.solvent_radius = 1.11
      params2.mask.use_resolution_based_gridding = True
    # ^^^

    # Build an array of the model intensities according to the input parameters
    f_model = mmtbx.utils.fmodel_from_xray_structure(xray_structure = xray_structure,
                                                     f_obs          = None,
                                                     add_sigmas     = True,
                                                     params         = params2).f_model
    if not self.params.merging.merge_anomalous:
      f_model = f_model.generate_bijvoet_mates()

    return f_model.as_intensity_array().change_basis(self.params.scaling.model_reindex_op).map_to_asu()

  def create_model_from_mtz(self, model_file_path):

    if self.mpi_helper.rank == 0:
      assert model_file_path.endswith("mtz") or model_file_path.endswith("sf.cif")
      # support both old-style *.mtz and structure factor *-sf.cif
      from iotbx import reflection_file_reader
      arrays = reflection_file_reader.any_reflection_file(file_name = model_file_path).as_miller_arrays()
      space_group = arrays[0].space_group().info()
      unit_cell   = arrays[0].unit_cell()
    else:
      arrays = space_group = unit_cell = None

    if self.purpose != "cosym":
      arrays, space_group, unit_cell = self.mpi_helper.comm.bcast((arrays, space_group, unit_cell), root=0)

    # save space group and unit cell as scaling targets
    if self.purpose == "scaling":
      self.params.scaling.space_group = space_group
      self.params.scaling.unit_cell   = unit_cell

    if self.purpose in ["scaling", "cosym"]:
      mtz_column_F = str(self.params.scaling.mtz.mtz_column_F.lower())
    elif self.purpose == "statistics":
      mtz_column_F = str(self.params.statistics.cciso.mtz_column_F.lower())

    for array in arrays:
      this_label = array.info().label_string().lower()
      if True not in [tag in this_label for tag in ["iobs","imean", mtz_column_F]]:
        continue

      return array.as_intensity_array().change_basis(self.params.scaling.model_reindex_op).map_to_asu()

    raise Exception("mtz did not contain expected label Iobs or Imean")

  def consistent_set_and_model(self, i_model=None):
    assert self.params.scaling.space_group, "Space group must be specified in the input parameters or a reference file must be present"
    # which unit cell are we using?
    if self.purpose == "scaling":
      assert self.params.scaling.unit_cell is not None, "Unit cell must be specified in the input parameters or a reference file must be present"
      unit_cell = self.params.scaling.unit_cell
      self.logger.log("Using target unit cell: " + str(unit_cell))
      if self.mpi_helper.rank == 0:
        self.logger.main_log("Using target unit cell: " + str(unit_cell))
    elif self.purpose == "statistics":
      if self.params.merging.set_average_unit_cell:
        assert self.params.statistics.average_unit_cell is not None, "Average unit cell hasn't been calculated"
        unit_cell = self.params.statistics.average_unit_cell
        unit_cell_formatted = "(%.6f, %.6f, %.6f, %.3f, %.3f, %.3f)"\
                          %(unit_cell.parameters()[0], unit_cell.parameters()[1], unit_cell.parameters()[2], \
                            unit_cell.parameters()[3], unit_cell.parameters()[4], unit_cell.parameters()[5])
        self.logger.log("Using average unit cell: " + unit_cell_formatted)
        if self.mpi_helper.rank == 0:
          self.logger.main_log("Using average unit cell: " + unit_cell_formatted)
      else:
        assert self.params.scaling.unit_cell is not None, "Unit cell must be specified in the input parameters or a reference file must be present"
        unit_cell = self.params.scaling.unit_cell
        self.logger.log("Using target unit cell: " + str(unit_cell))
        if self.mpi_helper.rank == 0:
          self.logger.main_log("Using target unit cell: " + str(unit_cell))

    # create symmetry for the full miller set
    symm = symmetry(unit_cell=unit_cell, space_group_info = self.params.scaling.space_group)

    # Adjust the minimum d-spacing of the generated Miller set to assure
    # that the desired high-resolution limit is included even if the
    # observed unit cell differs slightly from the target.  Use the same
    # expansion formula as used in merging/general_fcalc.py, to assure consistency.
    # If a reference model is present, ensure that Miller indices are ordered
    # identically.

    # set up the resolution limits
    d_max = 100000 # a default like in cxi-merge
    if self.params.merging.d_max != None:
      d_max = self.params.merging.d_max
    # RB: for later
    #d_max /= self.params.scaling.resolution_scalar
    d_min = self.params.merging.d_min * self.params.scaling.resolution_scalar

    # build the full miller set
    miller_set = symm.build_miller_set(anomalous_flag=(not self.params.merging.merge_anomalous), d_max=d_max, d_min=d_min)
    miller_set = miller_set.change_basis(self.params.scaling.model_reindex_op).map_to_asu()

    # Handle the case where model is anomalous=False but the requested merging is anomalous=True
    if i_model is not None:
      if i_model.anomalous_flag() is False and miller_set.anomalous_flag() is True:
        i_model = i_model.generate_bijvoet_mates()

      # manage the sizes of arrays. General_fcalc assures that
      # N(i_model) >= N(miller_set) since it fills non-matches with invalid structure factors
      # However, if N(i_model) > N(miller_set), it's because this run of cxi.merge requested
      # a smaller resolution range.  Must prune off the reference model.

      #RB 10/07/2019 The old cxi.merge comment regarding N(i_model) vs. N(miller_set) - see above - refers to pdb references only.
      #Now applying the same approach to all cases when the reference is mtz.

      if self.purpose == "scaling":
        is_mtz = str(self.params.scaling.model).endswith(".mtz")
        if i_model.indices().size() > miller_set.indices().size() or is_mtz:
          matches = miller.match_indices(i_model.indices(), miller_set.indices())
          pairs = matches.pairs()
          i_model = i_model.select(pairs.column(0))
        matches = miller.match_indices(i_model.indices(), miller_set.indices())
        if is_mtz:
          assert matches.pairs().size() >= self.params.scaling.mtz.minimum_common_hkls, "Number of common HKLs between mtz reference and data (%d) is less than required (%d)."\
                 %(matches.pairs().size(), self.params.scaling.mtz.minimum_common_hkls)
        miller_set = miller_set.select(matches.permutation())

    return miller_set, i_model

  def add_asu_miller_indices_column(self, reflections):
    '''Add a "symmetry-reduced hkl" column to the reflection table'''
    if reflections.size() == 0:
      return

    import copy

    # Build target symmetry. The exact experiment unit cell values don't matter for converting HKLs to asu HKLs.
    target_unit_cell = self.params.scaling.unit_cell
    target_space_group_info = self.params.scaling.space_group
    target_symmetry = symmetry(unit_cell=target_unit_cell, space_group_info=target_space_group_info)
    target_space_group = target_symmetry.space_group()

    # generate and add an asu hkl column
    if 'miller_index_asymmetric' not in reflections:
      reflections['miller_index_asymmetric'] = copy.deepcopy(reflections['miller_index'])
      miller.map_to_asu(target_space_group.type(),
                        not self.params.merging.merge_anomalous,
                        reflections['miller_index_asymmetric'])
  '''
  def get_min_max_experiment_unit_cell_volume(self, experiments):

    vols = []
    for experiment in experiments:
      vols.append(experiment.crystal.get_crystal_symmetry().unit_cell().volume())

    return min(vols),max(vols)
  '''

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(crystal_model)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/model/factory.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.model.crystal_model import crystal_model
from xfel.merging.application.model.resolution_binner import resolution_binner
from xfel.merging.application.worker import factory as factory_base

class factory(factory_base):
  '''Factory class for creating crystal model and resolution binner'''
  @staticmethod
  def from_parameters(params, additional_info=[], mpi_helper=None, mpi_logger=None):
    assert len(additional_info) > 0
    return [crystal_model(params, additional_info[0], mpi_helper, mpi_logger), resolution_binner(params, mpi_helper, mpi_logger)]


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/model/resolution_binner.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.worker import worker
from xfel.merging.application.utils.memory_usage import get_memory_usage

class resolution_binner(worker):

  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(resolution_binner, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Set up resolution bins'

  def run(self, experiments, reflections):
    '''Set up resolution bins; assign bin number to each hkl in the full miller set'''

    '''
    # for debugging purposes on Cori
    memory_MB = get_memory_usage()
    self.logger.log("Starting resolution_binner with memory usage: %d"%memory_MB)
    comm = self.mpi_helper.comm
    MPI = self.mpi_helper.MPI
    max_memory_usage_MB = comm.reduce(memory_MB, MPI.MAX)
    if self.mpi_helper.rank == 0:
      self.logger.main_log("Maximum memory usage per process: %d MB"%max_memory_usage_MB)
    '''

    # Create resolution binner
    full_miller_set = self.params.scaling.miller_set
    full_miller_set.setup_binner(d_max=100000, d_min=self.params.merging.d_min, n_bins=self.params.statistics.n_bins)
    resolution_binner = full_miller_set.binner()

    # Save resolution binner to the parameters
    if not 'resolution_binner' in (self.params.statistics).__dict__:
      self.params.statistics.__inject__('resolution_binner', resolution_binner)
    else:
      self.params.statistics.__setattr__('resolution_binner', resolution_binner)

    # Provide resolution bin number for each asu hkl in the full miller set
    hkl_resolution_bins = {} # hkl vs resolution bin number
    hkls_with_assigned_bin = 0
    for i_bin in resolution_binner.range_used():
      bin_hkl_selection = resolution_binner.selection(i_bin)
      bin_hkls = full_miller_set.select(bin_hkl_selection)
      for hkl in bin_hkls.indices():
        assert not hkl in hkl_resolution_bins # each hkl should be assigned a bin number only once
        hkl_resolution_bins[hkl] = i_bin
        hkls_with_assigned_bin += 1
    self.logger.log("Provided resolution bin number for %d asu hkls"%(hkls_with_assigned_bin))

    # Save hkl bin asignments to the parameters
    if not 'hkl_resolution_bins' in (self.params.statistics).__dict__:
      self.params.statistics.__inject__('hkl_resolution_bins', hkl_resolution_bins)
    else:
      self.params.statistics.__setattr__('hkl_resolution_bins', hkl_resolution_bins)

    return experiments, reflections

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(resolution_binner)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/modify/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/modify/aux_cosym.py
from __future__ import absolute_import, division, print_function
import os
from scitbx.array_family import flex
from cctbx.array_family import flex as cctbx_flex
from cctbx import sgtbx, miller
from libtbx import easy_mp, Auto
from scipy import sparse
import numpy as np
from ordered_set import OrderedSet
import copy

# Specialization, run only a subset of cosym steps and include plot
from dials.algorithms.symmetry.cosym import CosymAnalysis as BaseClass
from dials.util.multi_dataset_handling import select_datasets_on_identifiers

from dials.algorithms.symmetry.cosym.target import Target

from cctbx.sgtbx.literal_description import literal_description

class CosymAnalysis(BaseClass):

  def __init__(self, *args, **kwargs):
    self.do_plot = kwargs.pop('do_plot', False)
    self.i_plot = kwargs.pop('i_plot', None)
    self.plot_fname = kwargs.pop('plot_fname', None)
    self.plot_format = kwargs.pop('plot_format', None)
    self.output_dir = kwargs.pop('output_dir', None)
    self.cb_op = kwargs.pop('cb_op', None)
    super(CosymAnalysis, self).__init__(*args, **kwargs)

  def run(self):
    super(CosymAnalysis, self).run()
    if self.do_plot: self.plot_after_cluster_analysis()
    # we have the xy embedded coords at this point.

  def plot_after_optimize(self):
          print ("optimized coordinates", self.coords.shape)
          xx = []
          yy = []
          for item in range(self.coords.shape[0]):
            xx.append(self.coords[(item,0)])
            yy.append(self.coords[(item,1)])
          from matplotlib import pyplot as plt
          plt.plot(xx,yy,"r.")
          # denominator of 12 is specific to the use case of P6 (# symops in the metric superlattice)
          plt.plot(xx[::len(xx)//12],yy[::len(yy)//12],"b.")
          plt.plot(xx[:1],yy[:1],"g.")
          plt.axes().set_aspect("equal")
          circle = plt.Circle((0,0),1,fill=False,edgecolor="b")
          ax = plt.gca()
          ax.add_artist(circle)
          plt.show()

  def plot_after_cluster_analysis(self):
      if self.coords.shape[1] == 2: # one twining operator, most merohedry problems
          xx = flex.double()
          yy = flex.double()
          for item in range(self.coords.shape[0]):
            xx.append(self.coords[(item,0)])
            yy.append(self.coords[(item,1)])
          from matplotlib import pyplot as plt
          plt.plot(xx, yy, 'r.')
          plt.plot(xx[0:1], yy[0:1], 'k.')
          plt.plot([0,0],[-0.01,0.01],"k-")
          plt.plot([-0.01,0.01],[0,0],"k-")
          plt.xlim(-0.2,1.0)
          plt.ylim(-0.2,1.0)
          plt.title("$<w_{ij}>$=%.1f"%(np.mean(self.target.wij_matrix)))
          ax = plt.gca()
          ax.set_aspect("equal")
          circle = plt.Circle((0,0),1,fill=False,edgecolor="b")
          ax.add_artist(circle)
      elif self.coords.shape[1] == 4: # special case of P3 having 4 cosets
          # cast the data into two different shaped arrays.
          # one for me
          xyzt = [flex.double(),flex.double(),flex.double(),flex.double()]
          datasize = self.coords.shape[0]
          for item in range(datasize):
            for icoset in range(4):
              xyzt[icoset].append(self.coords[(item,icoset)])
          # and one for sklearn
          xsrc = []
          for i in range(datasize):
            pt = []
            for j in range(4):
              pt.append(xyzt[j][i])
            xsrc.append(pt)
          X = np.array(xsrc)

          from sklearn.cluster import DBSCAN
          db = DBSCAN(eps=0.1, min_samples=10).fit(X)
          labels = db.labels_

          # Number of clusters in labels, ignoring noise if present.
          n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
          n_noise_ = list(labels).count(-1)

          print("Estimated number of clusters: %d" % n_clusters_)
          print("Estimated number of noise points: %d" % n_noise_)

          coset_keys = list(set(self.reindexing_ops))
          assert len(coset_keys)==4 # result from dials cosym nearest neighbor clustering
          assert n_clusters_ == 4 # result from dbscan

          assert len(labels)==datasize
          print("unique labels",set(labels))

          from matplotlib import pyplot as plt
          colordict = {-1:"black",0:"green",1:"red",2:"magenta",3:"blue"}
          colors = [colordict[item] for item in labels]

          import matplotlib.gridspec as gridspec
          fig = plt.figure(figsize=(8,7))
          gs = gridspec.GridSpec(nrows=2, ncols=2, height_ratios=[1, 1])
          label={0:"$x$",1:"$y$",2:"$z$",3:"$t$"}
          for axp, axf in zip([(0,0),(0,1),(1,0),(1,1)],[(0,1),(3,1),(0,2),(3,2)]):
            axspec = fig.add_subplot(gs[axp[0],axp[1]])
            axspec.scatter(xyzt[axf[0]], xyzt[axf[1]], c=colors, marker='.')

            axspec.plot([0,0],[-0.01,0.01],"k-")
            axspec.plot([-0.01,0.01],[0,0],"k-")
            axspec.set_xlim(-0.2,1.0)
            axspec.set_ylim(-0.2,1.0)
            axspec.set_xlabel(label[axf[0]])
            axspec.set_ylabel(label[axf[1]])
            axspec.set_aspect("equal")
            plt.title("$<w_{ij}>$=%.1f"%(np.mean(self.target.wij_matrix)))
            if axp[0]==1: plt.title("")
            ax = plt.gca()
            circle = plt.Circle((0,0),1,fill=False,edgecolor="b")
            ax.add_artist(circle)

      if self.plot_fname is None:
            plt.show()
      else:
            plot_path = os.path.join(self.output_dir, self.plot_fname)
            plot_fname = "{}_{}.{}".format(
                plot_path, self.i_plot, self.plot_format)
            plt.savefig(plot_fname)

  def _intialise_target(self):
      if self.params.dimensions is Auto:
          dimensions = None
      else:
          dimensions = self.params.dimensions
      if self.params.lattice_group is not None:
          self.lattice_group = (
              self.params.lattice_group.group()
              .build_derived_patterson_group()
              .info()
              .primitive_setting()
              .group()
          )
      if self.params.twin_axis:
        twin_axis = [tuple(x) for x in self.params.twin_axis]
        twin_rotation = self.params.twin_rotation
      else:
        twin_axis = None
        twin_rotation = None
      self.target = TargetWithCustomSymops(
          self.intensities,
          self.dataset_ids.as_numpy_array(),
          min_pairs=self.params.min_pairs,
          lattice_group=self.lattice_group,
          dimensions=dimensions,
          weights=self.params.weights,
          twin_axes=twin_axis,
          twin_angles=twin_rotation,
          cb_op=self.cb_op
      )



from dials.command_line.cosym import logger
from dials.command_line.cosym import cosym as dials_cl_cosym_wrapper
from dials.util.exclude_images import get_selection_for_valid_image_ranges
from dials.command_line.symmetry import (
    apply_change_of_basis_ops,
    change_of_basis_ops_to_minimum_cell,
    eliminate_sys_absent,
)
from dials.util.filter_reflections import filtered_arrays_from_experiments_reflections

class dials_cl_cosym_subclass (dials_cl_cosym_wrapper):
    def __init__(self, experiments, reflections, uuid_cache_in, params=None,
            do_plot=False, i_plot=None, output_dir=None):
        super(dials_cl_cosym_wrapper, self).__init__(
            events=["run_cosym", "performed_unit_cell_clustering"]
        )
        if params is None:
            params = phil_scope.extract()
        self.params = params

        self._reflections = []
        for refl, expt in zip(reflections, experiments):
            sel = get_selection_for_valid_image_ranges(refl, expt)
            self._reflections.append(refl.select(sel))

        self._experiments, self._reflections = self._filter_min_reflections(
            experiments, self._reflections, uuid_cache_in
        )
        self.ids_to_identifiers_map = {}
        for table in self._reflections:
            self.ids_to_identifiers_map.update(table.experiment_identifiers())
        self.identifiers_to_ids_map = {
            value: key for key, value in self.ids_to_identifiers_map.items()
        }

        if len(self._experiments) > 1:
            # perform unit cell clustering
            identifiers = self._unit_cell_clustering(self._experiments)
            if len(identifiers) < len(self._experiments):
                logger.info(
                    "Selecting subset of %i datasets for cosym analysis: %s"
                    % (len(identifiers), str(identifiers))
                )
                self._experiments, self._reflections = select_datasets_on_identifiers(
                    self._experiments, self._reflections, use_datasets=identifiers
                )
                self.uuid_cache = [self.uuid_cache[int(id)] for id in identifiers]

        # Map experiments and reflections to minimum cell
        cb_ops = change_of_basis_ops_to_minimum_cell(
            self._experiments,
            params.lattice_symmetry_max_delta,
            params.relative_length_tolerance,
            params.absolute_angle_tolerance,
        )
        in_cb_ops = len(cb_ops)
        exclude = [
            expt.identifier
            for expt, cb_op in zip(self._experiments, cb_ops)
            if not cb_op
        ]
        if len(exclude):
            logger.info(
                "Rejecting {} datasets from cosym analysis "\
                "(couldn't determine consistent cb_op to minimum cell):\n"\
                "{}".format(len(exclude), exclude)
            )
            self._experiments, self._reflections = select_datasets_on_identifiers(
                self._experiments, self._reflections, exclude_datasets=exclude
            )
            assert len(cb_ops) == len(self.uuid_cache)
            self.uuid_cache = [
                x for i, x in enumerate(self.uuid_cache)
                if cb_ops[i] is not None
            ]
            cb_ops = list(filter(None, cb_ops))

        ex_cb_ops = len(cb_ops)

        #Normally we expect that all the cb_ops are the same (applicable for PSI with P63)
        assertion_dict = {}
        for cb_op in cb_ops:
          key_ = cb_op.as_hkl()
          assertion_dict[key_] = assertion_dict.get(key_, 0)
          assertion_dict[key_] += 1
        if len(assertion_dict) != 1:
          # unexpected, there is normally only 1 cb operator to minimum cell
          from libtbx.mpi4py import MPI
          mpi_rank = MPI.COMM_WORLD.Get_rank()
          mpi_size = MPI.COMM_WORLD.Get_size()
          print ("RANK %02d, # experiments %d, after exclusion %d, unexpectedly there are %d unique cb_ops: %s"%(
            mpi_rank, in_cb_ops, ex_cb_ops, len(assertion_dict),
            ", ".join(["%s:%d"%(key,assertion_dict[key]) for key in assertion_dict])))
          # revisit with different use cases later

          # In fact we need all cb_ops to match because the user might supply
          # a custom reindexing operator and we need to consistently tranform
          # it from the conventional basis into the minimum basis. Therefore,
          # force them all to match, but make sure user is aware.
          if not params.single_cb_op_to_minimum:
            raise RuntimeError('There are >1 different cb_ops to minimum and \
cosym.single_cb_op_to_minimum is not True')
          else:
            best_cb_op_str = max(assertion_dict, key=assertion_dict.get)
            best_cb_op = None
            for cb_op in cb_ops:
              if cb_op.as_hkl() == best_cb_op_str:
                best_cb_op = cb_op
                break
            assert best_cb_op is not None
            cb_ops = [best_cb_op] * len(cb_ops)

        self.cb_op_to_minimum = cb_ops



        # Eliminate reflections that are systematically absent due to centring
        # of the lattice, otherwise they would lead to non-integer miller indices
        # when reindexing to a primitive setting
        self._reflections = eliminate_sys_absent(self._experiments, self._reflections)

        self._experiments, self._reflections = apply_change_of_basis_ops(
            self._experiments, self._reflections, cb_ops
        )

        # transform models into miller arrays
        datasets = filtered_arrays_from_experiments_reflections(
            self.experiments,
            self.reflections,
            outlier_rejection_after_filter=False,
            partiality_threshold=params.partiality_threshold,
        )

        datasets = [
            ma.as_anomalous_array().merge_equivalents().array() for ma in datasets
        ]
        # opportunity here to subclass as defined above, instead of the dials-implemented version
        self.params.min_reflections = 0 # avoid any further filtering implemented in https://github.com/dials/dials/pull/2741
        self.cosym_analysis = CosymAnalysis(
            datasets,
            self.params,
            do_plot=do_plot,
            i_plot=i_plot,
            plot_fname=self.params.plot.filename,
            plot_format=self.params.plot.format,
            output_dir=output_dir,
            cb_op=cb_ops[0]
            )

    def _filter_min_reflections(self, experiments, reflections, uuid_cache_in):
        identifiers = []
        self.uuid_cache = []
        for expt, refl, uuid in zip(experiments, reflections, uuid_cache_in):
            if len(refl) >= self.params.min_reflections:
                identifiers.append(expt.identifier)
                self.uuid_cache.append(uuid)

        return select_datasets_on_identifiers(
            experiments, reflections, use_datasets=identifiers
        )


class TargetWithFastRij(Target):
  def __init__(self, *args, **kwargs):

    # nproc is an init arg that was removed from class
    # dials.algorithms.symmetry.cosym.target.Target in dials commit 1cd5afe4
    self._nproc = kwargs.pop('nproc', 1)

    # if test_data_path is provided, we are constructing this for a unit test
    test_data_path = kwargs.pop('test_data_path', None)
    if test_data_path is None:
      super(TargetWithFastRij, self).__init__(*args, **kwargs)
      return
    else:
      # This is only for unit testing
      import pickle
      import numpy as np
      self._nproc = 1
      with open(test_data_path, 'rb') as f:
        self._lattices = np.array(pickle.load(f))
        self.sym_ops = pickle.load(f)
        self._weights = pickle.load(f)
        self._data = pickle.load(f)
        self._patterson_group = pickle.load(f)
        self._min_pairs = 3 # minimum number of mutual miller indices for a match

        # truncate the input data to save time
        # self._lattices is the list of start index of each dataset
        self._lattices = self._lattices[:11]

        i_last = self._lattices[-1]
        self._data = self._data[:i_last]

        # now remove the last index in self._lattices as this was just used
        # to determine the start index of the next dataset (which does not
        # exist in the cut set)
        self._lattices = self._lattices[:-1]

  def _lattice_lower_upper_index(self, lattice_id):
       lower_index = int(self._lattices[lattice_id])
       upper_index = None
       if lattice_id < len(self._lattices) - 1:
           upper_index = int(self._lattices[lattice_id + 1])
       else:
           assert lattice_id == len(self._lattices) - 1
       return lower_index, upper_index

  def compute_gradients(self, x):
    grad = super(TargetWithFastRij, self).compute_gradients(x)
    return grad.A1

  def _compute_rij_wij(self, use_cache=True, use_super=False):

    if use_super:
      # for testing
      return super()._compute_rij_wij(use_cache=use_cache)

    def _compute_one_row(args):
      """
      Call the compute_one_row method of CC, which is a compute_rij_wij_detail
      instance. Args is a tuple that we unpack because easy_mp.parallel_map
      can only pass a single argument.
      """
      CC, i, NN = args
      rij_row, rij_col, rij_data, wij_row, wij_col, wij_data = [
          list(x) for x in CC.compute_one_row(self._lattices.size, i)
      ]
      rij = sparse.coo_matrix((rij_data, (rij_row, rij_col)), shape=(NN, NN))
      wij = sparse.coo_matrix((wij_data, (wij_row, wij_col)), shape=(NN, NN))
      return rij, wij

    n_lattices = self._lattices.size
    n_sym_ops = len(self.sym_ops)
    NN = n_lattices * n_sym_ops
    lower_i = flex.int()
    upper_i = flex.int()
    for lidx in range(self._lattices.size):
      LL,UU = self._lattice_lower_upper_index(lidx)
      lower_i.append(int(LL))
      if UU is None:  UU = self._data.data().size()
      upper_i.append(int(UU))
    indices = {}
    space_group_type = self._data.space_group().type()
    from xfel.merging import compute_rij_wij_detail
    CC = compute_rij_wij_detail(
        lower_i,
        upper_i,
        self._data.data(),
        self._min_pairs)
    for sym_op in self.sym_ops:
        cb_op = sgtbx.change_of_basis_op(sym_op)
        indices_reindexed = cb_op.apply(self._data.indices())
        miller.map_to_asu(space_group_type, False, indices_reindexed)
        indices[cb_op.as_xyz()] = indices_reindexed
        CC.set_indices(cb_op, indices_reindexed)

    assert self._nproc==1
    results = map(
        _compute_one_row,
        [(CC, i, NN) for i in range(n_lattices)]
    )

    rij_matrix = None
    wij_matrix = None
    for (rij, wij) in results:
        if rij_matrix is None:
            rij_matrix = rij
            wij_matrix = wij
        else:
            rij_matrix += rij
            wij_matrix += wij
    self.rij_matrix = rij_matrix.todense().astype(np.float64)
    self.wij_matrix = wij_matrix.todense().astype(np.float64)
    return self.rij_matrix, self.wij_matrix

class TargetWithCustomSymops(TargetWithFastRij):
  def __init__(
      self,
      intensities,
      lattice_ids,
      weights=None,
      min_pairs=3,
      lattice_group=None,
      dimensions=None,
      nproc=None,
      twin_axes=None,
      twin_angles=None,
      cb_op=None,
  ):
    '''
    A couple extra init arguments permit testing user-defined reindexing ops.
    twin_axes is a list of tuples, e.g. [(0,1,0)] means the twin axis is b.
    twin_angles is a corresponding list to define the rotations; 2 is a twofold
        rotation etc.
    cb_op is the previously determined transformation from the input cells to
        the minimum cell. The data have already been transformed by this
        operator, so we transform the twin operators before testing them.
    '''

    if nproc is not None:
      warnings.warn("nproc is deprecated", DeprecationWarning)
    self._nproc = 1

    if weights is not None:
      assert weights in ("count", "standard_error")
    self._weights = weights
    self._min_pairs = min_pairs

    data = intensities.customized_copy(anomalous_flag=False)
    cb_op_to_primitive = data.change_of_basis_op_to_primitive_setting()
    data = data.change_basis(cb_op_to_primitive).map_to_asu()

    # Convert to uint64 avoids crashes on Windows when later constructing
    # flex.size_t (https://github.com/cctbx/cctbx_project/issues/591)
    order = lattice_ids.argsort().astype(np.uint64)
    sorted_data = data.data().select(flex.size_t(order))
    sorted_indices = data.indices().select(flex.size_t(order))
    self._lattice_ids = lattice_ids[order]
    self._data = data.customized_copy(indices=sorted_indices, data=sorted_data)
    assert isinstance(self._data.indices(), type(cctbx_flex.miller_index()))
    assert isinstance(self._data.data(), type(cctbx_flex.double()))

    # construct a lookup for the separate lattices
    self._lattices = np.array(
        [
            np.where(self._lattice_ids == i)[0][0]
            for i in np.unique(self._lattice_ids)
        ]
    )

    self.sym_ops = OrderedSet(["x,y,z"])
    self._lattice_group = lattice_group
    auto_sym_ops = self._generate_twin_operators()
    if twin_axes is not None:
      assert len(twin_axes) == len(twin_angles)
      lds = [literal_description(cb_op.inverse().apply(op)) for op in auto_sym_ops]
      ld_tuples = [(ld.r_info.ev(), ld.r_info.type()) for ld in lds]
      i_symops_to_keep = []
      for i, (axis, angle) in enumerate(ld_tuples):
        if axis in twin_axes and angle in twin_angles:
          i_symops_to_keep.append(i)
      assert len(i_symops_to_keep) == len(twin_axes)
      sym_ops = [auto_sym_ops[i] for i in i_symops_to_keep]
    else:
      sym_ops = auto_sym_ops
    self.sym_ops.update(op.as_xyz() for op in sym_ops)
    if dimensions is None:
      dimensions = max(2, len(self.sym_ops))
    self.set_dimensions(dimensions)

    self._lattice_group = copy.deepcopy(self._data.space_group())
    for sym_op in self.sym_ops:
      self._lattice_group.expand_smx(sym_op)
    self._patterson_group = self._lattice_group.build_derived_patterson_group()

    logger.debug(
        "Lattice group: %s (%i symops)",
        self._lattice_group.info().symbol_and_number(),
        len(self._lattice_group),
    )
    logger.debug(
        "Patterson group: %s", self._patterson_group.info().symbol_and_number()
    )

    self.rij_matrix, self.wij_matrix = self._compute_rij_wij()


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/modify/compare_results.py
from __future__ import absolute_import, division, print_function
import pandas as pd

def run(reference_file, test_file):
  refdata = pd.read_pickle(reference_file)
  cosdata = pd.read_pickle(test_file)

  #inner join
  merged_inner = pd.merge(left=refdata, right=cosdata, left_on='experiment', right_on='experiment')

  same = []
  isame = 0
  for idx in range(len(merged_inner["experiment"])):
    is_same = merged_inner['coset_x'][idx] == merged_inner['coset_y'][idx]
    if is_same: isame +=1
    same.append(is_same)
  merged_inner['same'] = same

  pd.set_option('display.max_rows', 300)
  # What's the size of the output data?
  print(merged_inner.shape)
  print(merged_inner)

  print("File %s, %d aligned experiments"%(reference_file, len(refdata["experiment"])))
  print("File %s, %d aligned experiments"%(test_file, len(cosdata["experiment"])))
  print("The common experiment set consists of %d"%len(merged_inner["experiment"]))
  A = len(merged_inner["same"])
  B = len(merged_inner[merged_inner["same"]==True])
  print("%d of %d common alignments agree on coset assignment, %0.2f%%"%(B,A,100.*B/A))

  all_expt = len(set(list(refdata["experiment"])).union(set(list(cosdata["experiment"]))))
  print("%d matches out of a total of %d experiments, or %.2f%%"%(B, all_expt, 100.*B/all_expt))
  return A, B/A

if __name__=="__main__":
  import sys
  reference_file = sys.argv[1]
  test_file = sys.argv[2]
  run(reference_file, test_file)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/modify/cosym.py
from __future__ import absolute_import, division, print_function
from xfel.merging.application.worker import worker
from xfel.merging.application.utils.memory_usage import get_memory_usage
from cctbx.array_family import flex
from cctbx import sgtbx
from cctbx.sgtbx import change_of_basis_op
from dxtbx.model.crystal import MosaicCrystalSauter2014
from dxtbx.model.experiment_list import ExperimentList
#from libtbx.development.timers import Profiler, Timer


class cosym(worker):
  """
  Resolve indexing ambiguity using dials.cosym
  """
  def __init__(self, params, mpi_helper=None, mpi_logger=None):
    super(cosym, self).__init__(params=params, mpi_helper=mpi_helper, mpi_logger=mpi_logger)

  def __repr__(self):
    return 'Resolve indexing ambiguity using dials.cosym'


  @staticmethod
  def experiment_id_detail(expts, refls, exp_reflections): # function arguments are modified by the function
        simple_experiment_id = len(expts) - 1
        #experiment.identifier = "%d"%simple_experiment_id
        expts[-1].identifier = "%d"%simple_experiment_id
        # experiment identifier must be a string according to *.h file
        # the identifier is changed on the _for_cosym Experiment list, not the master experiments for through analysis

        exp_reflections['id'] = flex.int(len(exp_reflections), simple_experiment_id)
        # register the integer id as a new column in the per-experiment reflection table

        exp_reflections.experiment_identifiers()[simple_experiment_id] = expts[-1].identifier
        #apparently the reflection table holds a map from integer id (reflection table) to string id (experiment)

        refls.append(exp_reflections)

  @staticmethod
  def task_a(params):
      # add an anchor
      sampling_experiments_for_cosym = ExperimentList()
      sampling_reflections_for_cosym = []
      if params.modify.cosym.anchor:
        from xfel.merging.application.model.crystal_model import crystal_model
        #P = Timer("construct the anchor reference model")
        XM = crystal_model(params = params, purpose="cosym")
        model_intensities = XM.run([],[])
        #del P
        from dxtbx.model import Experiment, Crystal
        from scitbx.matrix import sqr
        O = sqr(model_intensities.unit_cell().orthogonalization_matrix()).transpose().elems
        real_a = (O[0],O[1],O[2])
        real_b = (O[3],O[4],O[5])
        real_c = (O[6],O[7],O[8])
        nc = Crystal(real_a,real_b,real_c, model_intensities.space_group())
        sampling_experiments_for_cosym.append(Experiment(crystal=nc)) # prepends the reference model to the cosym E-list
        from dials.array_family import flex

        exp_reflections = flex.reflection_table()
        exp_reflections['intensity.sum.value'] = model_intensities.data()
        exp_reflections['intensity.sum.variance'] = flex.pow(model_intensities.sigmas(),2)
        exp_reflections['miller_index'] = model_intensities.indices()
        exp_reflections['miller_index_asymmetric'] = model_intensities.indices()
        exp_reflections['flags'] = flex.size_t(model_intensities.size(), flex.reflection_table.flags.integrated_sum)

        # prepare individual reflection tables for each experiment
        cosym.experiment_id_detail(sampling_experiments_for_cosym, sampling_reflections_for_cosym, exp_reflections)
      return sampling_experiments_for_cosym, sampling_reflections_for_cosym

  @staticmethod
  def task_c(params, mpi_helper, logger, tokens,
      sampling_experiments_for_cosym, sampling_reflections_for_cosym, uuid_starting=[], communicator_size=1, do_plot=False):
      # Purpose: assemble a composite tranch from src inputs, and instantiate the COSYM class
      uuid_cache = uuid_starting

      for tranch_experiments, tranch_reflections in tokens:
          for expt_id, experiment in enumerate(tranch_experiments):
            sampling_experiments_for_cosym.append(experiment)
            uuid_cache.append(experiment.identifier)

            exp_reflections = tranch_reflections.select(tranch_reflections['id'] == expt_id)
            # prepare individual reflection tables for each experiment

            cosym.experiment_id_detail(sampling_experiments_for_cosym, sampling_reflections_for_cosym, exp_reflections)

      from dials.command_line import cosym as cosym_module
      cosym_module.logger = logger

      i_plot = mpi_helper.rank
      from xfel.merging.application.modify.aux_cosym import dials_cl_cosym_subclass as dials_cl_cosym_wrapper
      COSYM = dials_cl_cosym_wrapper(
                sampling_experiments_for_cosym, sampling_reflections_for_cosym,
                uuid_cache, params=params.modify.cosym,
                output_dir=params.output.output_dir, do_plot=do_plot,
                i_plot=i_plot)
      return COSYM


  def run(self, input_experiments, input_reflections):
    from collections import OrderedDict
    if self.mpi_helper.rank == 0:
      print("Starting cosym worker")
      #Overall = Profiler("Cosym total time")

    #  Evenly distribute all experiments from mpi_helper ranks
    reports = self.mpi_helper.comm.gather((len(input_experiments)),root=0) # report from all ranks on experiment count
    if self.mpi_helper.rank == 0:
      from xfel.merging.application.modify.token_passing_left_right import construct_src_to_dst_plan
      plan = construct_src_to_dst_plan(flex.int(reports), self.params.modify.cosym.tranch_size, self.mpi_helper.comm)
    else:
      plan = 0
    plan = self.mpi_helper.comm.bcast(plan, root = 0)
    dst_offset = 1 if self.mpi_helper.size>1 else 0 # decision whether to reserve rank 0 for parallel anchor determination
                                                    # FIXME XXX probably need to look at plan size to decide dst_offset or not
    from xfel.merging.application.modify.token_passing_left_right import apply_all_to_all
    tokens = apply_all_to_all(plan=plan, dst_offset=dst_offset,
                   value=(input_experiments, input_reflections), comm = self.mpi_helper.comm)

    if self.params.modify.cosym.anchor:
      if self.mpi_helper.rank == 0:
        MIN_ANCHOR = 20
        from xfel.merging.application.modify.token_passing_left_right import construct_anchor_src_to_dst_plan
        anchor_plan = construct_anchor_src_to_dst_plan(MIN_ANCHOR, flex.int(reports), self.params.modify.cosym.tranch_size, self.mpi_helper.comm)
      else:
        anchor_plan = 0
      anchor_plan = self.mpi_helper.comm.bcast(anchor_plan, root = 0)
    self.logger.log_step_time("COSYM")

    if self.params.modify.cosym.plot.interactive:
      self.params.modify.cosym.plot.filename = None

    has_tokens = len(tokens) > 0
    all_has_tokens = self.mpi_helper.comm.allgather(has_tokens)
    ranks_with_tokens = [i for (i, val) in enumerate(all_has_tokens) if val]
    ranks_to_plot = ranks_with_tokens[:self.params.modify.cosym.plot.n_max]
    do_plot = (self.params.modify.cosym.plot.do_plot
        and self.mpi_helper.rank in ranks_to_plot)

    if len(tokens) > 0: # Only select ranks that have been assigned tranch data, for mutual coset determination
      # because cosym has a problem with hashed identifiers, use simple experiment identifiers
      sampling_experiments_for_cosym = ExperimentList()
      sampling_reflections_for_cosym = [] # is a list of flex.reflection_table
      COSYM = self.task_c(self.params, self.mpi_helper, self.logger, tokens,
        sampling_experiments_for_cosym, sampling_reflections_for_cosym,
        communicator_size=self.mpi_helper.size, do_plot=do_plot)
      self.uuid_cache = COSYM.uuid_cache # reformed uuid list after n_refls filter


      rank_N_refl=flex.double([r.size() for r in COSYM.reflections])
      message = """Task 1. Prepare the data for cosym
    change_of_basis_ops_to_minimum_cell
    eliminate_sys_absent
    transform models into Miller arrays, putting data in primitive triclinic reduced cell
    There are %d experiments with %d reflections, averaging %.1f reflections/experiment"""%(
      len(COSYM.experiments), flex.sum(rank_N_refl), flex.mean(rank_N_refl))
      self.logger.log(message)
      if self.mpi_helper.rank == 1: print(message) #; P = Timer("COSYM.run")
      COSYM.run()
      #if self.mpi_helper.rank == 1: del P

      keyval = [("experiment", []), ("reindex_op", []), ("coset", [])]
      raw = OrderedDict(keyval)

      if self.mpi_helper.rank == 0: print("Rank",self.mpi_helper.rank,"experiments:",len(sampling_experiments_for_cosym))

      for sidx in range(len(self.uuid_cache)):
        raw["experiment"].append(self.uuid_cache[sidx])

        sidx_plus = sidx

        try:
          minimum_to_input = COSYM.cb_op_to_minimum[sidx_plus].inverse()
        except Exception as e:
          print ("raising",e,sidx_plus, len(COSYM.cb_op_to_minimum))
          raise e

        reindex_op = minimum_to_input * \
                     sgtbx.change_of_basis_op(COSYM.cosym_analysis.reindexing_ops[sidx_plus]) * \
                     COSYM.cb_op_to_minimum[sidx_plus]

        # Keep this block even though not currently used; need for future assertions:
        LG = COSYM.cosym_analysis.target._lattice_group
        LGINP = LG.change_basis(COSYM.cosym_analysis.cb_op_inp_min.inverse()).change_basis(minimum_to_input)
        SG = COSYM.cosym_analysis.input_space_group
        SGINP = SG.change_basis(COSYM.cosym_analysis.cb_op_inp_min.inverse()).change_basis(minimum_to_input)
        CO = sgtbx.cosets.left_decomposition(LGINP, SGINP)
        partitions = CO.partitions
        this_reindex_op = reindex_op.as_hkl()
        this_coset = None
        for p_no, partition in enumerate(partitions):
          partition_ops = [change_of_basis_op(ip).as_hkl() for ip in partition]
          if this_reindex_op in partition_ops:
            this_coset = p_no; break
        assert this_coset is not None
        raw["coset"].append(this_coset)
        raw["reindex_op"].append(this_reindex_op)

      keys = list(raw.keys())
      from pandas import DataFrame as df
      data = df(raw)
      # major assumption is that all the coset decompositions "CO" are the same.  NOT sure if a test is needed.
      reports = self.mpi_helper.comm.gather((data, CO),root=0)
    else:
      reports = self.mpi_helper.comm.gather(None,root=0)
    if self.mpi_helper.rank == 0:
      # report back to rank==0 and reconcile all coset assignments
      while None in reports:
        reports.pop(reports.index(None))
      # global CO
      global_coset_decomposition = reports[0][1] # again, assuming here they are all the same XXX
    else:
      global_coset_decomposition = 0
    global_coset_decomposition = self.mpi_helper.comm.bcast(global_coset_decomposition, root=0)
    partitions = global_coset_decomposition.partitions
    self.mpi_helper.comm.barrier()
    # end of distributed embedding

    if self.params.modify.cosym.anchor:
        anchor_tokens = apply_all_to_all(plan=anchor_plan, dst_offset=0,
        value=(input_experiments, input_reflections), comm = self.mpi_helper.comm)

    if self.mpi_helper.rank == 0:
        from xfel.merging.application.modify.df_cosym import reconcile_cosym_reports
        REC = reconcile_cosym_reports(reports)
        results = REC.composite_tranch_merge(voting_method="consensus")

        # at this point we have the opportunity to reconcile the results with an anchor
        # recycle the data structures for anchor determination
        if self.params.modify.cosym.anchor:
          sampling_experiments_for_cosym, sampling_reflections_for_cosym = self.task_a(self.params)
          ANCHOR = self.task_c(self.params, self.mpi_helper, self.logger, anchor_tokens,
            sampling_experiments_for_cosym, sampling_reflections_for_cosym,
            uuid_starting=["anchor structure"], communicator_size=1) # only run on the rank==0 tranch.
          self.uuid_cache = ANCHOR.uuid_cache # reformed uuid list after n_refls filter
          #P = Timer("ANCHOR.run")
          ANCHOR.run() # Future redesign XXX FIXME do this in rank 0 in parallel with distributed composite tranches
          #del P

          keyval = [("experiment", []), ("coset", [])]
          raw = OrderedDict(keyval)
          print("Anchor","experiments:",len(sampling_experiments_for_cosym))

          anchor_op = ANCHOR.cb_op_to_minimum[0].inverse() * \
                     sgtbx.change_of_basis_op(ANCHOR.cosym_analysis.reindexing_ops[0]) * \
                     ANCHOR.cb_op_to_minimum[0]
          anchor_coset = None
          for p_no, partition in enumerate(partitions):
              partition_ops = [change_of_basis_op(ip).as_hkl() for ip in partition]
              if anchor_op.as_hkl() in partition_ops:
                anchor_coset = p_no; break
          assert anchor_coset is not None
          print("The consensus for the anchor is",anchor_op.as_hkl()," anchor coset", anchor_coset)

          raw["experiment"].append("anchor structure"); raw["coset"].append(anchor_coset)
          for sidx in range(1,len(self.uuid_cache)):
            raw["experiment"].append(self.uuid_cache[sidx])

            sidx_plus = sidx

            minimum_to_input = ANCHOR.cb_op_to_minimum[sidx_plus].inverse()
            reindex_op = minimum_to_input * \
                     sgtbx.change_of_basis_op(ANCHOR.cosym_analysis.reindexing_ops[sidx_plus]) * \
                     ANCHOR.cb_op_to_minimum[sidx_plus]
            this_reindex_op = reindex_op.as_hkl()
            this_coset = None
            for p_no, partition in enumerate(partitions):
              partition_ops = [change_of_basis_op(ip).as_hkl() for ip in partition]
              if this_reindex_op in partition_ops:
                this_coset = p_no; break
            assert this_coset is not None
            raw["coset"].append(this_coset)

          from pandas import DataFrame as df
          anchor_data = df(raw)
          REC.reconcile_with_anchor(results, anchor_data, anchor_op)
          # no need for return value; results dataframe is modified in place

        if self.params.modify.cosym.dataframe:
          import os
          results.to_pickle(path = os.path.join(self.params.output.output_dir,self.params.modify.cosym.dataframe))
        transmitted = results
    else:
        transmitted = 0
    self.mpi_helper.comm.barrier()
    transmitted = self.mpi_helper.comm.bcast(transmitted, root = 0)
    # "transmitted" holds the global coset assignments

    #subselect expt and refl on the successful coset assignments
    # output:  experiments-->result_experiments_for_cosym; reflections-->reflections (modified in place)
    result_experiments_for_cosym = ExperimentList()
    good_refls = flex.bool(len(input_reflections), False)
    good_expt_id = list(transmitted["experiment"])
    good_coset = list(transmitted["coset"]) # would like to understand how to use pandas rather than Python list
    for iexpt in range(len(input_experiments)):
        iexpt_id = input_experiments[iexpt].identifier
        keepit = iexpt_id in good_expt_id
        if keepit:
          this_coset = good_coset[ good_expt_id.index(iexpt_id) ]
          this_cb_op = change_of_basis_op(global_coset_decomposition.partitions[this_coset][0])
          accepted_expt = input_experiments[iexpt]
          if this_coset > 0:
            accepted_expt.crystal = MosaicCrystalSauter2014( accepted_expt.crystal.change_basis(this_cb_op) )
                                    # need to use wrapper because of cctbx/dxtbx#5
          result_experiments_for_cosym.append(accepted_expt)
          good_refls |= input_reflections["id"] == iexpt
    selected_reflections = input_reflections.select(good_refls)
    selected_reflections.reset_ids()
    self.mpi_helper.comm.barrier()

    # still have to reindex the reflection table, but try to do it efficiently
    from xfel.merging.application.modify.reindex_cosym import reindex_refl_by_coset
    if (len(result_experiments_for_cosym) > 0):
      reindex_refl_by_coset(refl = selected_reflections,
                          data = transmitted,
                          symms=[E.crystal.get_crystal_symmetry() for E in result_experiments_for_cosym],
                          uuids=[E.identifier for E in result_experiments_for_cosym],
                          co=global_coset_decomposition,
                          anomalous_flag = self.params.merging.merge_anomalous==False,
                          verbose=False)
    # this should have re-indexed the refls in place, no need for return value

    self.mpi_helper.comm.barrier()
    # Note:  this handles the simple case of lattice ambiguity (P63 in P/mmm lattice group)
    # in this use case we assume all inputs and outputs are in P63.
    # more complex use cases would have to reset the space group in the crystal, and recalculate
    # the ASU "miller_indicies" in the reflections table.

    self.logger.log_step_time("COSYM", True)
    self.logger.log("Memory usage: %d MB"%get_memory_usage())

    from xfel.merging.application.utils.data_counter import data_counter
    data_counter(self.params).count(result_experiments_for_cosym, selected_reflections)
    return result_experiments_for_cosym, selected_reflections

if __name__ == '__main__':
  from xfel.merging.application.worker import exercise_worker
  exercise_worker(reindex_to_reference)


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/modify/cosym_tranch_align.py
from __future__ import absolute_import, division, print_function
from itertools import permutations
import copy
from cctbx.array_family import flex
import numpy as np

"""Details of tranch alignment."""

def get_proposal_score(reports_as_lists):
    n_proposal_score = 0
    NN = len(reports_as_lists) # = n_tranches x n_permutations
    rij = flex.double(flex.grid(NN,NN),0.)
    wij = flex.double(flex.grid(NN,NN),1.)

    for ix in range(len(reports_as_lists)):
      base_report = reports_as_lists[ix]
      # compute the unions
      base_all = set(base_report[0])
      for icoset in range(1, len(base_report)):
        base_all = base_all.union(set(base_report[icoset]))

      for iy in range(len(reports_as_lists)):
        test_report = reports_as_lists[iy]
        matches = 0
        # compute the unions
        test_all = set(test_report[0])
        for icoset in range(1, len(test_report)):
          test_all = test_all.union(set(test_report[icoset]))
        # total overlap between base and test irrespective of cosets;
        total_overlay = len(base_all.intersection(test_all))

        for icoset in range(len(test_report)):
          matches += len(set(base_report[icoset]).intersection(set(test_report[icoset])))
        print ('%3d/%3d'%(matches, total_overlay),end=' ')
        rij [(ix,iy)] = matches/total_overlay if total_overlay>0 else 0.
        wij [(ix,iy)] = total_overlay if total_overlay>0 else 1.
        n_proposal_score += matches
      print()
    return rij, wij

def alignment_by_embedding(reports,plot=False):
  #from IPython import embed; embed()
  """reports is a list of tranch results, one list item per composite tranch
     Each item is a list, over cosets, e.g. two elements for a merohedral twinning op
     Each element is itself a list of uuid's assigned to that coset.
  """
  n_tranches = len(reports)
  n_operators = len(reports[0])
  reports = copy.deepcopy(reports)
  # will now amend the reports-list so that it has a new section for each permutation of cosets.

  for itranch in range(len(reports)):
    cache_permutations = list(permutations(reports[itranch]))
    assert len(reports[itranch]) == n_operators
    for jperm in range(1,len(cache_permutations)):
      reports.append(list(cache_permutations[jperm]))
    # Code is only valid for single twin operator, see alignment_by_seqential_trial() below

  rij, wij = get_proposal_score(reports)
  mt = flex.mersenne_twister(seed=0)
  #from IPython import embed; embed()
  NN = len(reports)
  xcoord = mt.random_double (size=NN)
  ycoord = mt.random_double (size=NN)
  if plot:
    from matplotlib import pyplot as plt
    plt.title("Initial spread before refinement")
    plt.plot(xcoord, ycoord, "g.")
    plt.show()

  from cctbx.merging.brehm_diederichs import minimize as mz, minimize_divide
  M = mz(xcoord,ycoord,rij,wij,verbose=True)
  coord_x = M.x[0:NN]
  coord_y = M.x[NN:2*NN]
  if plot:
    plt.title("Raw refinement")
    plt.plot(coord_x,coord_y,"r.", markersize=1.)
    plt.show()
  P = minimize_divide(coord_x, coord_y)
  selection = P.plus_minus()
  if plot:
    plt.title("After refinement")
    plt.plot(coord_x.select(selection),coord_y.select(selection),"r.", markersize=2.)
    plt.plot(coord_x.select(~selection),coord_y.select(~selection),"k.", markersize=3.)
    plt.show()

  print (list(selection))
  reformed_reports = [[] for i in range(n_tranches)] # output should have as many reports as tranches
  n_permutations = NN // n_tranches
  for iflag, select_flag in enumerate(selection):
    if select_flag:
      itranch = iflag % n_tranches
      #print( itranch, iflag, n_permutations)
      reformed_reports[itranch] = reports[iflag]

  assert [] not in reformed_reports # all tranches must have coset assignments
  return reformed_reports

def counts_for_sequential_trial(reports, active_subset):
  """Once the tranches are aligned
         tranche0 tranche1 tranche2
   coset0    A       C        E
   coset1    B       D        F
   The total number of datasets is union (A,....,F)
   This should also be equal to union (A,C,E) + union (B,D,F)
   Not true prior to alignment.
  """
  n_tranches = len(reports)
  n_cosets = len(reports[0])
  coset_totals = [set() for c in range(n_cosets)]
  total = set()
  for itranche in range(n_tranches):
    if not active_subset[itranche]: continue
    for icoset in range(n_cosets):
      cut = set(reports[itranche][icoset])
      total = total.union(cut)
      coset_totals[icoset] = coset_totals[icoset].union(cut)
  test = 0
  for icoset in range(n_cosets):
    test += len(coset_totals[icoset])
  print("Aligned total is",test,"grand total is",len(total), "delta",test-len(total))
  return test-len(total)

def grand_total_for_sequential_trial(reports, active_subset):
  n_tranches = len(reports)
  n_cosets = len(reports[0])
  total = set()
  for itranche in range(n_tranches):
    if not active_subset[itranche]: continue
    for icoset in range(n_cosets):
      cut = set(reports[itranche][icoset])
      total = total.union(cut)
  return len(total)

def alignment_by_sequential_trial(reports, plot=False):
  reports = copy.deepcopy(reports)
  active_subset_base = [False for i in reports]
  active_subset_base[0]=True # always start with tranche 0

  while active_subset_base.count(True)<len(active_subset_base):
    print("BEGIN LOOP with",active_subset_base.count(True),"aligned tranches")

    if False: # shortcut, fails
      selected_index = active_subset_base.index(False)
    else:
      #try to find the max overlapping tranche candidate (smallest grand total)
      base_total = grand_total_for_sequential_trial(reports, active_subset_base)
      candidates = np.array([np.iinfo(np.int64).max]*len(reports)) # all have max values
      for itranche in range(len(reports)):
        if not active_subset_base[itranche]:
          active_subset = copy.deepcopy(active_subset_base)
          active_subset[itranche]=True
          candidates[itranche]=grand_total_for_sequential_trial(reports, active_subset)
          print(itranche,base_total,candidates[itranche])
      selected_index = np.argmin(candidates)
    print("The selected tranche is",selected_index)

    # now having identified the candidate
    active_subset_base[selected_index]=True
    cache_permutations = list(permutations(reports[selected_index]))
    n_perm = len(cache_permutations)
    candidates = np.array([np.iinfo(np.int64).max]*n_perm) # all have max values
    for iperm in range(n_perm):
      reformed = copy.deepcopy(reports)
      reformed[selected_index] = list(cache_permutations[iperm])
      candidates[iperm] = counts_for_sequential_trial(reformed, active_subset_base)
    selected_perm = np.argmin(candidates)
    print ("The selected perm is",selected_perm)
    reports[selected_index] = list(cache_permutations[selected_perm])
    print("END LOOP with",active_subset_base.count(True),"aligned tranches")
  return reports


 *******************************************************************************


 *******************************************************************************
xfel/merging/application/modify/df_cosym.py
from __future__ import absolute_import, division, print_function
from pandas import DataFrame as df
import pandas as pd
from scitbx.array_family import flex
from collections import OrderedDict
from itertools import permutations

class reconcile_cosym_reports:
  def __init__(self, reports):
    self.reports = reports
    # assume a list of reports, each report has (dataframe, coset_object)
    print("Processing reports from %d ranks"%(len(reports)))
    ireport = 0
    self.anchor_dataframe = reports[0][0]
    self.reference_partitions = reports[0][1].partitions
    self.result_dataframe = None
    self.n_cosets = len(self.reference_partitions)

  def simple_merge(self, voting_method="consensus"):
    print("simple merge")
    pd.set_option('display.max_rows', 300)
    pd.set_option('display.max_columns', 8)
    pd.set_option("display.width", None)
    # create as reports-as-lists data structure.
    # list { one item per rank }
    # item { one experiment-list per coset }
    reports_as_lists = []
    for idx, item in enumerate(self.reports):
      item_df = item[0]
      del(item_df["reindex_op"]) # do not need this column, coset is enough
      one_report = [[] for x in range(self.n_cosets)] # one sublist for every coset
      for lidx in range(len(item_df["experiment"])):
        one_report[item_df["coset"][lidx]].append(item_df["experiment"][lidx])
      reports_as_lists.append(one_report)

    print("There are %d reports"%len(reports_as_lists))
    print([(len(reports_as_lists[i][0]),len(reports_as_lists[i][1])) for i in range(len(reports_as_lists))])

    # now modify the reports-as-list structure so ranks line up with respect to their coset assignments
    for idx in range(1, len(reports_as_lists)):
      base_report = reports_as_lists[idx-1]
      matches_vs_perm = []
      cache_permutations = list(permutations(reports_as_lists[idx]))
      for iperm,perm in enumerate(cache_permutations):
        matches_vs_perm.append(0)
        for icoset in range(len(perm)):
          imatches = len(set(base_report[icoset]).intersection(set(perm[icoset])))
          matches_vs_perm[-1] += imatches
          print("Rank %d perm %d coset %d matches %d"%(idx,iperm,icoset,imatches))
      print("matches for all permutations",matches_vs_perm, "choose", matches_vs_perm.index(max(matches_vs_perm)))
      # now choose the correct permutation and put it into reports-as-list
      correct_iperm = matches_vs_perm.index(max(matches_vs_perm))
      correct_perm = cache_permutations[correct_iperm]
      reports_as_lists[idx] = correct_perm

    # merge everything together into experiments plus votes
    experiment_lookup = dict()
    for idx in range(1, len(reports_as_lists)):
      for icoset in range(len(reports_as_lists[idx])):
        for uuid_expt in reports_as_lists[idx][icoset]:
          experiment_lookup[uuid_expt] = experiment_lookup.get( uuid_expt, len(experiment_lookup) )
    # a unique integer has now been assigned to every experiment uuid
    coset_assign = flex.size_t(flex.grid((len(experiment_lookup),3)))
    vote_matrix = flex.size_t(flex.grid((len(experiment_lookup),3)))
    vote_population = flex.size_t(len(experiment_lookup), 0)
    for idx in range(len(reports_as_lists)):
      for icoset in range(len(reports_as_lists[idx])):
        for uuid_expt in reports_as_lists[idx][icoset]:
          ekey = experiment_lookup[uuid_expt]
          coset_assign[ ekey, vote_population[ekey] ] = icoset
          vote_matrix[ ekey, vote_population[ekey] ] = idx
          vote_population[ekey]+=1

    #print out the matrices:
    if False:
      for item in experiment_lookup:
        ekey = experiment_lookup[item]
        print ("%4d"%ekey, item, " ", coset_assign[ekey,0], coset_assign[ekey,1], coset_assign[ekey,2],
                                 " ", vote_matrix[ekey,0], vote_matrix[ekey,1], vote_matrix[ekey,2])

    # tally the votes first.
    vote_tallies = flex.size_t(flex.grid((len(experiment_lookup),self.n_cosets)))
    for item in experiment_lookup:
      ekey = experiment_lookup[item]
      for ivote in range(3):
        vote_tallies[ ekey, coset_assign[ekey,ivote] ] += 1
      #print ("tallies %4d"%ekey, item, vote_tallies[ekey,0], vote_tallies[ekey,1]) #printout for two-coset case

    # choose winners, either by consensus or majority
    uuid_consensus = []
    uuid_majority = []
    coset_consensus = []
    coset_majority = []
    for item in experiment_lookup:
      ekey = experiment_lookup[item]
      for icoset in range(self.n_cosets):
        if vote_tallies[ekey,icoset]>=2:
          uuid_majority.append(item)
          coset_majority.append(icoset)
          if vote_tallies[ekey,icoset]==3: #hardcoded as it is assumed we always cast three votes
            uuid_consensus.append(item)
            coset_consensus.append(icoset)
          break
    print("Of %d experiments, majority cosets were chosen for %d and consensus for %d"%(
           len(experiment_lookup),len(coset_majority),len(coset_consensus)
    ))
    if voting_method == "majority":
      keyval = [("experiment", uuid_majority), ("coset", coset_majority)]
    elif voting_method == "consensus":
      keyval = [("experiment", uuid_consensus), ("coset", coset_consensus)]
    raw = OrderedDict(keyval)
    return df(raw)

  @staticmethod
  def get_proposal_score(reports_as_lists):
    n_proposal_score = 0
    for ix in range(len(reports_as_lists)):
      base_report = reports_as_lists[ix]
      # compute the unions
      base_all = set(base_report[0])
      for icoset in range(1, len(base_report)):
        base_all = base_all.union(set(base_report[icoset]))

      for iy in range(len(reports_as_lists)):
        test_report = reports_as_lists[iy]
        matches = 0
        # compute the unions
        test_all = set(test_report[0])
        for icoset in range(1, len(test_report)):
          test_all = test_all.union(set(test_report[icoset]))
        # total overlap between base and test irrespective of cosets;
        total_overlay = len(base_all.intersection(test_all))

        for icoset in range(len(test_report)):
          matches += len(set(base_report[icoset]).intersection(set(test_report[icoset])))
        print ('%3d/%3d'%(matches, total_overlay),end=' ')
        n_proposal_score += matches
      print()
    print("score",n_proposal_score)
    return n_proposal_score

  def composite_tranch_merge(self, voting_method="consensus"):
    print("composite tranch merge")
    pd.set_option('display.max_rows', 300)
    pd.set_option('display.max_columns', 8)
    pd.set_option("display.width", None)
    # create as reports-as-lists data structure.
    # list { one item per rank }
    # item { one experiment-list per coset }
    reports_as_lists = []
    for idx, item in enumerate(self.reports):
      item_df = item[0]
      del(item_df["reindex_op"]) # do not need this column, coset is enough
      one_report = [[] for x in range(self.n_cosets)] # one sublist for every coset
      for lidx in range(len(item_df["experiment"])):
        one_report[item_df["coset"][lidx]].append(item_df["experiment"][lidx])
      reports_as_lists.append(one_report)

    print("There are %d reports"%len(reports_as_lists))
    print([
        tuple( [
           len(reports_as_lists[i][j]) for j in range(len(reports_as_lists[i]))
        ] )
      for i in range(len(reports_as_lists))
      ])
    #Bypass reconciliation and voting if there is only one tranch
    if len(reports_as_lists)==1 : return self.reports[0][0] # dataframe with one tranch

    # now modify the reports-as-list structure so ranks line up with respect to their coset assignments
    # entire function is same as simple_merge except in this block, where attention is paid to whether
    # there are sufficient overlap with base tranch.
    import pickle
    if self.n_cosets == 2:
      from xfel.merging.application.modify.cosym_tranch_align import alignment_by_embedding
      reports_as_lists = alignment_by_embedding(reports_as_lists)
    else: # for more than 1 twin law
      from xfel.merging.application.modify.cosym_tranch_align import alignment_by_sequential_trial
      reports_as_lists = alignment_by_sequential_trial(reports_as_lists)

    # merge everything together into experiments plus votes
    experiment_lookup = dict()
    for idx in range(1, len(reports_as_lists)):
      for icoset in range(len(reports_as_lists[idx])):
        for uuid_expt in reports_as_lists[idx][icoset]:
          experiment_lookup[uuid_expt] = experiment_lookup.get( uuid_expt, len(experiment_lookup) )
    # a unique integer has now been assigned to every experiment uuid
    coset_assign = flex.size_t(flex.grid((len(experiment_lookup),3)))
    vote_matrix = flex.size_t(flex.grid((len(experiment_lookup),3)))
    vote_population = flex.size_t(len(experiment_lookup), 0)
    for idx in range(len(reports_as_lists)):
      for icoset in range(len(reports_as_lists[idx])):
        for uuid_expt in reports_as_lists[idx][icoset]:
          ekey = experiment_lookup[uuid_expt]
          coset_assign[ ekey, vote_population[ekey] ] = icoset
          vote_matrix[ ekey, vote_population[ekey] ] = idx
          vote_population[ekey]+=1

    #print out the matrices:
    if False:
      for item in experiment_lookup:
        ekey = experiment_lookup[item]
        print ("%4d"%ekey, item, " ", coset_assign[ekey,0], coset_assign[ekey,1], coset_assign[ekey,2],
                                 " ", vote_matrix[ekey,0], vote_matrix[ekey,1], vote_matrix[ekey,2])

    # tally the votes first.
    vote_tallies = flex.size_t(flex.grid((len(experiment_lookup),self.n_cosets)))
    for item in experiment_lookup:
      ekey = experiment_lookup[item]
      for ivote in range(3):
        vote_tallies[ ekey, coset_assign[ekey,ivote] ] += 1
      #print ("tallies %4d"%ekey, item, vote_tallies[ekey,0], vote_tallies[ekey,1]) #printout for two-coset case

    # choose winners, either by consensus or majority
    uuid_consensus = []
    uuid_majority = []
    coset_consensus = []
    coset_majority = []
    for item in experiment_lookup:
      ekey = experiment_lookup[item]
      for icoset in range(self.n_cosets):
        if vote_tallies[ekey,icoset]>=2:
          uuid_majority.append(item)
          coset_majority.append(icoset)
          if vote_tallies[ekey,icoset]==3: #hardcoded as it is assumed we always cast three votes
            uuid_consensus.append(item)
            coset_consensus.append(icoset)
          break
    print("Of %d experiments, majority cosets were chosen for %d and consensus for %d"%(
           len(experiment_lookup),len(coset_majority),len(coset_consensus)
    ))
    if voting_method == "majority":
      keyval = [("experiment", uuid_majority), ("coset", coset_majority)]
    elif voting_method == "consensus":
      keyval = [("experiment", uuid_consensus), ("coset", coset_consensus)]
    raw = OrderedDict(keyval)
    return df(raw)

  def reconcile_with_anchor(self, global_dataframe, anchor_dataframe, anchor_operator, verbose=False):
    if verbose: print (anchor_dataframe)
    merged_inner = pd.merge(left=global_dataframe, right=anchor_dataframe, left_on='experiment', right_on='experiment')
    if verbose: print (merged_inner)

    #1) cyclic icoset permutation of the anchor till the key==0 icoset is 0
    # XXX fix me.  Actually not sure if cyclic permutation works for more than 2 cosets.  Good guess, but need to test
    working_uuid = list(anchor_dataframe["experiment"]); assert working_uuid[0]=="anchor structure"
    working_cyclic = list(anchor_dataframe["coset"])
    while working_cyclic[0] != 0:
      working_cyclic = [(icoset+1)%self.n_cosets for icoset in working_cyclic]
    anchor_dataframe["coset"] = working_cyclic

    merged_inner = pd.merge(left=global_dataframe, right=anchor_dataframe, left_on='experiment', right_on='experiment')
    if verbose: print (merged_inner)


    #2) all permutations of the global till the global matches the anchor, then return this
    reference_coset = list(merged_inner["coset_y"])
    coset_permutations = list(permutations(list(range(self.n_cosets))))
    for trial_permutation in coset_permutations:
      working_coset = list(merged_inner["coset_x"])
      working_coset = [trial_permutation[i] for i in working_coset]
      n_matches = 0
      for iexpt in range(len(reference_coset)):
        if working_coset[iexpt] == reference_coset[iexpt]:
          n_matches += 1
      # be somewhat clever about success level
      if (n_matches / len(reference_coset)) > (self.n_cosets-0.5)/self.n_cosets: break

    #3) apply the successful permutation to the global dataframe
    merged_inner["coset_x"] = working_coset
    if verbose: print (merged_inner)
    global_dataframe["coset"] = [trial_permutation[x] for x in list(global_dataframe["coset"])]
    return global_dataframe

def tst_gt():
  import pickle
  reports = pickle.load(open("GT_runold.pickle","rb"))
  reports = pickle.load(open("gt_2258.pickle","rb"))
  REC = reconcile_cosym_reports(reports)
  REC.simple_merge()
def tst_new():
  import pickle
  reports = pickle.load(open("try_runnew.pickle","rb"))
  reports = pickle.load(open("try_2258.pickle","rb"))
  REC = reconcile_cosym_reports(reports)
  REC.composite_tranch_merge()
def tst_new_succeed():
  import pickle
  reports = pickle.load(open("2218_succeeded.pickle","rb"))
  REC = reconcile_cosym_reports(reports)
  REC.composite_tranch_merge()
def tst_new_fail():
  import pickle
  reports = pickle.load(open("1655_failed.pickle","rb"))
  REC = reconcile_cosym_reports(reports)
  REC.composite_tranch_merge()

if __name__=="__main__":
  #tst_gt()
  tst_new()
  tst_new_succeed()
  tst_new_fail()


 *******************************************************************************
