

 *******************************************************************************
rstbx/dps_core/__init__.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from cctbx.array_family import flex # import dependency

import boost_adaptbx.boost.python as bp
bp.import_ext("rstbx_ext")
from rstbx_ext import *
import rstbx_ext as ext

import math

from cctbx.crystal_orientation import basis_type
from cctbx.crystal_orientation import ext as coext

@bp.inject_into(coext.crystal_orientation)
class _():

  def constrain(self,constraints):

    #algorithm 1.  Use pre-defined crystal_systems to give hard-coded restraints.
    # dps_core.constrainment.s_minimizer uses LBFGS minimizer to adapt
    # all 9 components of the orientation matrix.   This gives the best-fit
    # to the starting matrix (better than algorithm #2), but the disadvantage
    # is that it is keyed to the crystal_system descriptors.  It is therefore
    # not adapted to all small-molecule space groups (monoclinics),
    # and will not take into account non-standard settings.

    if constraints in ["triclinic","monoclinic",'orthorhombic','tetragonal',
                       "cubic","rhombohedral",'hexagonal']:

      from rstbx.dps_core.constrainment import s_minimizer
      S = s_minimizer(self,constraint=constraints)
      return S.newOrientation()

    #algorithm 2:  Tensor_rank_2 symmetrization
    # Advantages:  constraints are calculated directly from the space
    # group, so will account for non-standard settings.
    # Disadvantages:  drift away from starting orientation is greater than
    # for algorithm #1.

    from cctbx.sgtbx import space_group
    if isinstance(constraints,space_group):

      from rstbx.symmetry.constraints import AGconvert
      converter = AGconvert()
      converter.forward(self)
      average_cell = constraints.average_unit_cell(self.unit_cell())
      converter.validate_and_setG( average_cell.reciprocal().metrical_matrix() )
      return Orientation(converter.back(),basis_type.reciprocal)

    #Future plans:  a hybrid approach.  Use algorithm 2 to do the
    # symmetrization, as it is clearly the best approach for 1) conciseness of
    # code, 2) supporting non-standard settings.  Then use an LBFGS
    # minimizer to minimize the psi, phi and theta offsets to the
    # original orientation.

class Orientation(coext.crystal_orientation):

  def __init__(self,either_matrix, basis_type_flag=basis_type.reciprocal):
    if isinstance(either_matrix,Orientation) or \
       isinstance(either_matrix,coext.crystal_orientation):
      coext.crystal_orientation.__init__(self,either_matrix)
    else:
      coext.crystal_orientation.__init__(self,either_matrix,basis_type_flag)

def combocmp(a,b):
  #gives -1,0,1 depending on closeness of combo to (0,0,0)
  a_measure = a[0]+a[1]+a[2]
  b_measure = b[0]+b[1]+b[2]
  if a_measure<b_measure: return -1
  if a_measure==b_measure: return 0
  return 1

def directional_show(direction,message):
  print(message,"%.4f %8.2f %8.2f kmax=%2d kval=%5.1f kval2=%5.1f kval3=%5.1f"%(
    direction.real,180*direction.psi/math.pi, 180.*direction.phi/math.pi,
    direction.kmax, direction.kval,direction.kval2,direction.kval3))

@bp.inject_into(ext.dps_core)
class _():

  def combos(self,basis=10):
    """All interesting combinations of the directional candidates.
    Parameter MAXINDEX is the maximum id number considered when choosing
    combos.  With combos sorted before return, as below, I feel more
    comfortable increasing this parameter if needed to index a difficult case"""
    nc = self.n_candidates()
    MAXINDEX=basis
    bases = range(min(MAXINDEX,nc))
    comb=[]
    for pp in range(len(bases)-2):
      for qq in range(pp+1,len(bases)-1):
        for rr in range(qq+1, len(bases)):
          comb.append((bases[pp],bases[qq],bases[rr]))
    from functools import cmp_to_key
    comb.sort(key = cmp_to_key(combocmp))
    return comb

  def niggli(self, cutoff = 25.):
    from rstbx.dps_core.zuoreduction import rwgk_niggli as support_niggli
    ni = support_niggli(self.getOrientation(),cutoff=cutoff) # orientation of reduced cell
    self.setOrientation(ni)

# Boost python injector (not double inheritance) is required so that classes that inherit ext.dps_core
#   will be able to both execute the injected methods (combos and niggli) as well as the
#   ext.dps_core C++ class methods


 *******************************************************************************


 *******************************************************************************
rstbx/dps_core/basis_choice.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import math
from scitbx import matrix
from cctbx.uctbx.reduction_base import iteration_limit_exceeded as KGerror
from rstbx.dps_core.cell_assessment import unit_cell_too_small,SmallUnitCellVolume
from rstbx.dps_core import directional_show
from rstbx_ext import Direction
from six.moves import zip

diagnostic = False

class AbsenceHandler:
  def __init__(self):
    self.recursion_limit=8

  def absence_detected(self,hkllist):
    self.hkl = hkllist
    self.N   = self.hkl.size()
    self.flag = None

    from cctbx.sgtbx.sub_lattice_tools import generate_matrix
    allGenerators=[]
    # include identity for on_means calculation
    for mod in [6,5,4,3,2]:
        for matS in generate_matrix(mod):
          allGenerators.append(matS)
    self.allGenerators = allGenerators

    for idx,matS in enumerate(allGenerators):
        invS = matS.inverse()

        idx_possible = True
        for miller in [matrix.row(i) for i in hkllist]:
          transformed_miller = miller*invS.transpose()
          #print transformed_miller
          for element in (transformed_miller).elems:
            if element.denominator() > 1:
              idx_possible = False

        #print "transformation",invS.transpose().elems,{True:"is",False:"not"}[idx_possible],"possible"
        if idx_possible:
          print("There are systematic absences. Applying transformation",invS.transpose().elems)
          self.cb_op = invS.transpose().inverse()  # not sure if transpose.inverse or just inverse
          self.flag = True
          return 1

    return 0

  def correct(self,orientation):
    print("before", orientation.unit_cell(),orientation.unit_cell().volume())
    print([float(i) for i in self.cb_op.elems])
    corrected = orientation.change_basis([float(i) for i in self.cb_op.elems])
    print("after", corrected.unit_cell(),orientation.unit_cell().volume())
    unit_cell_too_small(corrected.unit_cell(),cutoff=25.)
    return corrected

class FewSpots(Exception): pass

class SolutionTracker:
  def __init__(self):
    self.all_solutions=[]
    self.volume_filtered=[]
  def append(self,item):
    item['serial']=len(self.all_solutions)
    self.all_solutions.append(item)
    self.update_analysis()
  def update_analysis(self):
    self.best_likelihood = max([i['model_likelihood'] for i in self.all_solutions])
    self.close_solutions = [
      i for i in self.all_solutions
        if i['model_likelihood']>=0.9*self.best_likelihood ]
    self.min_volume = min([i['volume'] for i in self.close_solutions])
      # new method, Jan 2005.  Rely on likelihood, except filter out solutions
      #  where the volume is significantly higher than the minimum:
    self.volume_filtered = [i for i in self.close_solutions if i['volume']<1.25*self.min_volume]
    self.best_volume_filtered_likelihood = max([i['model_likelihood'] for i in self.volume_filtered])
  def best_combo(self):
    #print "There are %d combos"%(len(self.all_solutions))
    if len(self.all_solutions)==0: return None
    combo = [i for i in self.volume_filtered if i['model_likelihood']==
            self.best_volume_filtered_likelihood][0]
    #print "In this round the best combo was",combo,"with likelihood",self.best_volume_filtered_likelihood
    return combo
  def halts(self):
    return len(self.volume_filtered)>=20

unphysical_cell_cutoff_small_molecule_regime = 25. # Angstrom^3

class HandleCombo:
  def __init__(self,ai,combo,cutoff=unphysical_cell_cutoff_small_molecule_regime):
    self.ai = ai
    self.combo = combo
    self.cutoff = cutoff
    solns=[ai[combo[i]] for i in [0,1,2]]
    self.setA(solns)
    unit_cell_too_small(ai.getOrientation().unit_cell(),cutoff=self.cutoff)
    ai.niggli() # reduce cell

  def handle_absences(self):
      Abs = AbsenceHandler()
      if Abs.absence_detected(self.ai.hklobserved()):
        newmat = Abs.correct(self.ai.getOrientation())
        self.ai.setOrientation(newmat)
        self.ai.niggli(cutoff=self.cutoff)

  def setA(self,solns):
    from scitbx import matrix as vector # to clarify role of column vector
    # set the orientation matrix based on list of three rstbx basis Directions
    assert type(solns) == list
    assert not 0 in [isinstance(x,Direction) for x in solns]
    self.ai.combo_state = solns # for derived feature in LABELIT
    realaxis=[]
    for i in range(3):
      realaxis.append(  vector.col(solns[i].dvec) * solns[i].real )
    matA = [  realaxis[0].elems[0],realaxis[0].elems[1],realaxis[0].elems[2],
              realaxis[1].elems[0],realaxis[1].elems[1],realaxis[1].elems[2],
              realaxis[2].elems[0],realaxis[2].elems[1],realaxis[2].elems[2]  ]
    self.ai.set_orientation_direct_matrix(matA)


def select_best_combo_of(ai,better_than=0.15,candidates=20,basis=15):
  """Take the first few candidates of ai.combos().  Search for all combos
     with hkl obs-calc better than a certain fraction limit, then
     handle absences, and return the best one"""
  best_combo = None

  base_likelihood = 0.30
  best_likelihood = base_likelihood
  C = ai.combos(basis)
  maxtry = min(candidates,len(C))
  try_counter = 0
  solutions = SolutionTracker()
  if diagnostic:
    for x in range(ai.n_candidates()):
      directional_show(ai[x],message="BC%d"%x)

  for combo in C:
    #print "COMBO: (%d,%d,%d)"%(combo[0],combo[1],combo[2])
    try:
      HC = HandleCombo(ai,combo)
      #HC.handle_absences()  #might need to add this in later
      if ai.rmsdev() < better_than:
        model_likelihood = 1. - ai.rmsdev() # provisional expression for likelihood
        if model_likelihood > best_likelihood:
          best_likelihood = model_likelihood
        this_solution = {'combo':combo,'model_likelihood':model_likelihood,
                          'volume':ai.getOrientation().unit_cell().volume()}
        solutions.append(this_solution)
      try_counter+=1
    except (FewSpots) as f:
      #print "COMBO: (%d,%d,%d) rejected on too few spots"%(combo[0],combo[1],combo[2])
      #printcombo(ai,combo)
      continue
    except (SmallUnitCellVolume) as f:
      #print "Small COMBO: (%d,%d,%d) rejected on small cell volume"%(combo[0],combo[1],combo[2])
      continue
    except (KGerror) as f:
      #print "KG COMBO: (%d,%d,%d) rejected on Krivy-Gruber iterations"%(combo[0],combo[1],combo[2])
      #printcombo(ai,combo)
      continue
    except ValueError as f :
      if str(f).find("Corrupt metrical matrix")>=0: continue # colinear or coplanar
    except (RuntimeError) as f:
      if str(f).find("Iteration limit exceeded")>0: continue
      if str(f).find("Matrix is not invertible")>=0: continue# colinear or coplanar
      print("Report this problem to LABELIT developers:")
      print("COMBO: (%d,%d,%d) rejected on C++ runtime error"%(combo[0],combo[1],combo[2]))
      #printcombo(ai,combo)
      continue
    except Exception:
      raise

    if solutions.halts():
      return solutions
    if try_counter == maxtry: break
  return solutions

class SelectBasisMetaprocedure:
  def __init__(self,input_index_engine):
    self.input_index_engine = input_index_engine

    #initial search
    all_sol = select_best_combo_of(input_index_engine,
                                   better_than=0.36,
                                   candidates=25)
    best_combo = all_sol.best_combo()
    #print "Best combo",best_combo
    if best_combo!=None:
      self.evaluate_combo(best_combo)
      return

    raise Exception("AutoIndexing Failed to Select Basis")

  def evaluate_combo(self,best_combo,verbose=False):
    #print "best combo",best_combo
    HC = HandleCombo(self.input_index_engine,best_combo['combo'])
    HC.handle_absences()

  def show_rms(self):
    print("+++++++++++++++++++++++++++++++")
    cell = self.input_index_engine.getOrientation().unit_cell()
    print("cell=%s volume(A^3)=%.3f"%(cell,cell.volume()))
    print("RMSDEV: %5.3f"%self.input_index_engine.rmsdev())
    print("-------------------------------")

    for hkl,obs in zip(self.input_index_engine.hklobserved(),self.input_index_engine.observed()):
      displace = matrix.col(hkl) - matrix.col(obs)
      diff = math.sqrt(displace.dot(displace))
      print("%-15s %5.3f"%(hkl,diff))


 *******************************************************************************


 *******************************************************************************
rstbx/dps_core/cell_assessment.py
from __future__ import absolute_import, division, print_function

class SmallUnitCellVolume(Exception): pass

# cutoff=25.0 Angstrom-cubed is the smallest conceivable unit cell for any
# crystal.  Cells with smaller volume are assumed to have two parallel
# basis vectors and are rejected.  Cutoff value of 100.0 can be used
# for macromolecular work.

def unit_cell_too_small(uc,cutoff=25.):
    abc = uc.parameters()
    if abc[0]*abc[1]*abc[2]/cutoff > uc.volume() or uc.volume() < cutoff:
      raise SmallUnitCellVolume


 *******************************************************************************


 *******************************************************************************
rstbx/dps_core/constrainment.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from scitbx import lbfgs
from libtbx import adopt_init_args

from cctbx.array_family import flex

class NoCrystalSystem(Exception): pass

class s_minimizer:

  def __init__(self, orient, constraint='triclinic',
               min_iterations=25, max_calls=1000):
    self.constraint=constraint
    adopt_init_args(self, locals())
    self.n = 9
    self.x = flex.double(orient.direct_matrix())
    self.minimizer = lbfgs.run(
      target_evaluator=self,
      termination_params=lbfgs.termination_parameters(
        traditional_convergence_test=00000,
        min_iterations=min_iterations,
        max_calls=max_calls))
    del self.g

  def compute_functional_and_gradients(self):
    self.internals()
    return self.f,self.g

  def internals(self):
    O = self.newOrientation()
    A = O.A
    B = O.B
    C = O.C
    D = O.D
    E = O.E
    F = O.F
    Aij = self.x
    dA_dAij = (2.*Aij[0],2.*Aij[1],2.*Aij[2],0,0,0,0,0,0)
    dB_dAij = (0,0,0,2.*Aij[3],2.*Aij[4],2.*Aij[5],0,0,0)
    dC_dAij = (0,0,0,0,0,0,2.*Aij[6],2.*Aij[7],2.*Aij[8])
    dD_dAij = (0,0,0,Aij[6],Aij[7],Aij[8],Aij[3],Aij[4],Aij[5])
    dE_dAij = (Aij[6],Aij[7],Aij[8],0,0,0,Aij[0],Aij[1],Aij[2])
    dF_dAij = (Aij[3],Aij[4],Aij[5],Aij[0],Aij[1],Aij[2],0,0,0)
    self.g = flex.double()

    if self.constraint == 'monoclinic':
      self.f = D*D + F*F
      for x in range(self.n):
        self.g.append(2.*D*dD_dAij[x] + 2.*F*dF_dAij[x])

    elif self.constraint == 'orthorhombic':
      self.f = D*D + E*E + F*F
      for x in range(self.n):
        self.g.append(2.*D*dD_dAij[x] + 2.*E*dE_dAij[x] + 2.*F*dF_dAij[x])

    elif self.constraint == 'tetragonal':
      self.f = D*D + E*E + F*F + (A-B)*(A-B)
      for x in range(self.n):
        self.g.append(2.*D*dD_dAij[x] + 2.*E*dE_dAij[x] + 2.*F*dF_dAij[x] +
                      2.*(A-B)*(dA_dAij[x]-dB_dAij[x]))

    elif self.constraint == 'cubic':
      self.f = D*D + E*E + F*F + (A-B)*(A-B) + (A-C)*(A-C)
      for x in range(self.n):
        self.g.append(2.*D*dD_dAij[x] + 2.*E*dE_dAij[x] + 2.*F*dF_dAij[x] +
                      2.*(A-B)*(dA_dAij[x]-dB_dAij[x]) +
                      2.*(A-C)*(dA_dAij[x]-dC_dAij[x]))

    elif self.constraint == 'rhombohedral':
      self.f = (A-B)*(A-B) + D*D + E*E + (F+A/2.)*(F+A/2.)
      for x in range(self.n):
        self.g.append(2.*(A-B)*(dA_dAij[x]-dB_dAij[x]) +
                      2.*D*dD_dAij[x] + 2.*E*dE_dAij[x] +
                      2.*(F+A/2.)*(dF_dAij[x]+dA_dAij[x]/2.) )

    elif self.constraint == 'hexagonal':
      self.f = D*D + E*E + (A-B)*(A-B) + (F+A/2.)*(F+A/2.)
      for x in range(self.n):
        self.g.append(2.*D*dD_dAij[x] + 2.*E*dE_dAij[x] +
                      2.*(A-B)*(dA_dAij[x]-dB_dAij[x])  +
                      2.*(F+A/2.)*(dF_dAij[x]+dA_dAij[x]/2.))

    elif self.constraint == 'triclinic':
      self.f = 0.0
      for x in range(self.n):
        self.g.append(0.0)

    else:
      raise NoCrystalSystem

  def newOrientation(self):
    #trick to instantiate Orientation given self.x, the direct space matrix
    from rstbx.dps_core import Orientation
    from cctbx.crystal_orientation import basis_type
    return Orientation(tuple(self.x),basis_type.direct)


 *******************************************************************************


 *******************************************************************************
rstbx/dps_core/lepage.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import scitbx.math
from scitbx import matrix
from scitbx.array_family import flex
from cctbx import sgtbx, crystal
from rstbx.symmetry.subgroup import metric_subgroups, MetricSubgroup
from cctbx.sgtbx.bravais_types import bravais_lattice

def echelon_constraints(group,reciprocal_space = 1):
  #    direct space : XT G X  = G
  #reciprocal space : X G* XT = G*
  #as tuple: g = g00,g01,g02,g11,g12,g22 = A F E B D C
  '''  G = g00 g01 g02 = A F E
           g01 g11 g12   F B D
           g02 g12 g22   E D C'''
  n0 = 6*len(group)
  n1 = 6
  m = flex.int(flex.grid(n0,n1))
  i = 0
  for x in group:
    if reciprocal_space==1:
      Rm = matrix.sqr(x.as_double_array()[0:9])
    else:
      Rm = matrix.sqr(x.as_double_array()[0:9]).transpose()
    R = Rm.elems
    Rt = Rm.transpose()
    e = (R[0]*R[0]-1, 2*R[0]*R[1],            2*R[0]*R[2],            R[1]*R[1],2*R[1]*R[2],           R[2]*R[2])
    for x in range(6): m[i] = int(e[x]); i+=1
    e = (R[0]*R[3],     R[1]*R[3]+R[0]*R[4]-1,  R[2]*R[3]+R[0]*R[5],  R[1]*R[4],  R[2]*R[4]+R[1]*R[5], R[2]*R[5])
    for x in range(6): m[i] = int(e[x]); i+=1
    e = (R[0]*R[6],     R[1]*R[6]+R[0]*R[7],    R[2]*R[6]+R[0]*R[8]-1,R[1]*R[7],  R[2]*R[7]+R[1]*R[8], R[2]*R[8])
    for x in range(6): m[i] = int(e[x]); i+=1
    e = (R[3]*R[3],   2*R[3]*R[4],            2*R[3]*R[5],            R[4]*R[4]-1,2*R[4]*R[5],         R[5]*R[5])
    for x in range(6): m[i] = int(e[x]); i+=1
    e = (R[3]*R[6],     R[4]*R[6]+R[3]*R[7],    R[5]*R[6]+R[3]*R[8],  R[4]*R[7],  R[5]*R[7]+R[4]*R[8]-1,R[5]*R[8])
    for x in range(6): m[i] = int(e[x]); i+=1
    e = (R[6]*R[6],   2*R[6]*R[7],            2*R[6]*R[8],            R[7]*R[7],2*R[7]*R[8],         R[8]*R[8]-1)
    for x in range(6): m[i] = int(e[x]); i+=1
  mnew = scitbx.math.row_echelon_form(m)
  i = 0
  #Rearrange row echelon changing coefficient order AFEBDC to ABCDEF
  n0 = mnew
  i=0
  C = flex.int(flex.grid(n0,n1))
  for x in range(mnew):
    C[i]=m[i]; C[i+1]=m[i+3]; C[i+2]=m[i+5]; C[i+3]=m[i+4]; C[i+4]=m[i+2]; C[i+5]=m[i+1]
    i+=6
  i=0
  return C

def equal(A,B,tolerance=0.99):
  return abs(A-B) < 1.- tolerance

def bestcmp(a,b):
  if equal(a['max_angular_difference'], b['max_angular_difference']):
    if a.number() > b.number(): return -1
    if a.number() == b.number(): return 0
    else: return 1
  if a['max_angular_difference'] > b['max_angular_difference']: return -1
  return 1

class iotbx_converter(metric_subgroups,list):

 def __init__(self,unit_cell,max_delta,bravais_types_only=True,
    space_group_symbol="P 1",force_minimum=False,best_monoclinic_beta=True,
    interest_focus="metric_symmetry",sort=True):
    # with regard to "force_minimum": when autoindexing, the orientation
    # matrix may be derived from comparison to a previously indexed case;
    # the setting may be non-standard; therefore we do not want to
    # convert to the reduced cell when calculating metric subgroups.
  if interest_focus=="metric_symmetry":
    input_symmetry = crystal.symmetry(unit_cell=unit_cell,
    space_group_symbol=space_group_symbol)
  elif interest_focus=="input_symmetry":
    input_symmetry = crystal.symmetry(unit_cell=unit_cell,
    space_group_symbol=space_group_symbol,
    force_compatible_unit_cell=False)

  metric_subgroups.__init__(self,input_symmetry,max_delta,
                       enforce_max_delta_for_generated_two_folds=True,
                       bravais_types_only=bravais_types_only,
                       force_minimum=force_minimum,
                       best_monoclinic_beta=best_monoclinic_beta,
                       interest_focus=interest_focus)

  for subgroup in self.result_groups:
    # required keys for subgroup:
    #   max_angular_difference
    #   subsym: the centrosymmetric group, referred to the input_cell basis
    #   cb_op_inp_best:  change of basis from the input cell to the best reference cell

    # derived keys added in other frames:
    #   orient: the orientation matrix, in the reference setting

    # methods:
    #   to_reference_setting_as_double_array_transpose (formerly 'matrix')
    #   number: the group number of subsym

    # other attributes:
    #   reduced_group: the acentric group, expressed in input_cell basis
    #   supersym: acentric metric supergroup, input_cell basis

    group_classification = bravais_lattice(sgtbx.space_group_info(
          group=subgroup['supersym'].space_group()).type().number())
    subgroup['bravais'] = str(group_classification)
    subgroup['system'] = group_classification.crystal_system.lower()

    # ad-hoc fix to support the s_minimizer; remove this when
    # Orientation.constrain() is re-implemented.
    if subgroup['bravais']=="hR" and subgroup['system']=="trigonal":
      subgroup['system']="rhombohedral"
    if subgroup['bravais']=="hP" and subgroup['system']=="trigonal":
      subgroup['system']="hexagonal"
    #end of ad-hoc section

    subgroup['best_group']=subgroup['best_subsym'].space_group()
    #special procedure to get non-centrosymmetric group
    subgroup['reduced_group']=\
      subgroup['subsym'].space_group().build_derived_acentric_group()
    subgroup['constraints']=echelon_constraints(subgroup['reduced_group'])
    self.append(MetricSubgroup().import_iotbx_style(subgroup))
  if (sort):
    from functools import cmp_to_key
    self.sort(key=cmp_to_key(bestcmp))


 *******************************************************************************


 *******************************************************************************
rstbx/dps_core/sampling.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import math
import os
from libtbx.test_utils import approx_equal
from rstbx.array_family import flex
from rstbx.dps_core import Direction, directional_show, SimpleSamplerTool

# sampling algorithm to cover the directional hemisphere

diagnostic=False

def test_simple_sampler():
  # SimpleSamplerTool migrated from Python to C++.
  SST = SimpleSamplerTool(0.014)
  SST.construct_hemisphere_grid(SST.incr)
  result = approx_equal(SST.incr, 0.014)
  result &= (len(SST.angles) == 32227)
  reference_psi = [0.0, 0.014024967203525862, 0.028049934407051724, 0.042074901610577586, 0.056099868814103448, 0.070124836017629311, 0.084149803221155173, 0.098174770424681035, 0.1121997376282069, 0.12622470483173276, 0.14024967203525862, 0.15427463923878448, 0.16829960644231035, 0.18232457364583621, 0.19634954084936207, 0.21037450805288793, 0.22439947525641379, 0.23842444245993966, 0.25244940966346552, 0.26647437686699138, 0.28049934407051724, 0.2945243112740431, 0.30854927847756897, 0.32257424568109483, 0.33659921288462069, 0.35062418008814655, 0.36464914729167242, 0.37867411449519828, 0.39269908169872414, 0.40672404890225, 0.42074901610577586, 0.43477398330930173, 0.44879895051282759, 0.46282391771635345, 0.47684888491987931, 0.49087385212340517, 0.50489881932693104, 0.5189237865304569, 0.53294875373398276, 0.54697372093750862, 0.56099868814103448, 0.57502365534456035, 0.58904862254808621, 0.60307358975161207, 0.61709855695513793, 0.6311235241586638, 0.64514849136218966, 0.65917345856571552, 0.67319842576924138, 0.68722339297276724, 0.70124836017629311, 0.71527332737981897, 0.72929829458334483, 0.74332326178687069, 0.75734822899039655, 0.77137319619392242, 0.78539816339744828, 0.79942313060097414, 0.8134480978045, 0.82747306500802587, 0.84149803221155173, 0.85552299941507759, 0.86954796661860345, 0.88357293382212931, 0.89759790102565518, 0.91162286822918104, 0.9256478354327069, 0.93967280263623276, 0.95369776983975862, 0.96772273704328449, 0.98174770424681035, 0.99577267145033621, 1.0097976386538621, 1.0238226058573878, 1.0378475730609138, 1.0518725402644398, 1.0658975074679655, 1.0799224746714913, 1.0939474418750172, 1.1079724090785432, 1.121997376282069, 1.1360223434855947, 1.1500473106891207, 1.1640722778926467, 1.1780972450961724, 1.1921222122996982, 1.2061471795032241, 1.2201721467067501, 1.2341971139102759, 1.2482220811138016, 1.2622470483173276, 1.2762720155208536, 1.2902969827243793, 1.3043219499279051, 1.318346917131431, 1.332371884334957, 1.3463968515384828, 1.3604218187420085, 1.3744467859455345, 1.3884717531490605, 1.4024967203525862, 1.416521687556112, 1.4305466547596379, 1.4445716219631639, 1.4585965891666897, 1.4726215563702154, 1.4866465235737414, 1.5006714907772674, 1.5146964579807931, 1.5287214251843189, 1.5427463923878448, 1.5567713595913708, 1.5707963267948966]
  all_psi = []
  for i in SST.angles:
    if i.psi not in all_psi:  all_psi.append(i.psi)
  result &= approx_equal(all_psi, reference_psi)
  with open(os.devnull, 'w') as devnull:
    one_slice_phi = [i.phi for i in SST.angles if approx_equal(i.psi, 0.014024967203525862, out=devnull)]
  reference_phi = [0.0, 1.0471975511965976, 2.0943951023931953, 3.1415926535897931, 4.1887902047863905, 5.2359877559829879]
  result &= approx_equal(one_slice_phi, reference_phi)
  return result

class HemisphereSamplerBase(SimpleSamplerTool):
  def __init__(self,characteristic_grid,max_cell):
    SimpleSamplerTool.__init__(self,characteristic_grid)
    self.construct_hemisphere_grid(self.incr)
    self.max_cell=max_cell
  def get_top_solutions(self,ai,input_directions,size,cutoff_divisor,grid):

    kval_cutoff = ai.getXyzSize()/cutoff_divisor;

    hemisphere_solutions = flex.Direction();
    hemisphere_solutions.reserve(size);
    #print "# of input directions", len(input_directions)
    for i in range(len(input_directions)):
      D = sampled_direction = ai.fft_result(input_directions[i])

      #hardcoded parameter in for silicon example.  Not sure at this point how
      #robust the algorithm is when this value is relaxed.

      if D.real < self.max_cell and sampled_direction.kval > kval_cutoff:
        if diagnostic:  directional_show(D, "%5d"%i)
        hemisphere_solutions.append(sampled_direction)
    if (hemisphere_solutions.size()<3):
      return hemisphere_solutions
    kvals = flex.double([
  hemisphere_solutions[x].kval for x in range(len(hemisphere_solutions))])

    perm = flex.sort_permutation(kvals,True)

    #  need to be more clever than just taking the top 30.
    #  one huge cluster around a strong basis direction could dominate the
    #  whole hemisphere map, preventing the discovery of three basis vectors

    perm_idx = 0
    unique_clusters = 0
    hemisphere_solutions_sort = flex.Direction()
    while perm_idx < len(perm) and \
          unique_clusters < size:
      test_item = hemisphere_solutions[perm[perm_idx]]
      direction_ok = True
      for list_item in hemisphere_solutions_sort:
        distance = math.sqrt(math.pow(list_item.dvec[0]-test_item.dvec[0],2) +
                     math.pow(list_item.dvec[1]-test_item.dvec[1],2) +
                     math.pow(list_item.dvec[2]-test_item.dvec[2],2)
                     )
        if distance < 0.087: #i.e., 5 degrees radius for clustering analysis
           direction_ok=False
           break
      if direction_ok:
        unique_clusters+=1
      hemisphere_solutions_sort.append(test_item)
      perm_idx+=1

    return hemisphere_solutions_sort;

  def hemisphere(self,ai,size = 30,cutoff_divisor = 4.,verbose=True):

    unrefined_basis_vectors = self.get_top_solutions(ai,self.angles,size,
      cutoff_divisor,grid=self.incr)

    if verbose:
      for i in range(len(unrefined_basis_vectors)):
        D = unrefined_basis_vectors[i];
        if diagnostic:  directional_show(D, "SE%5d"%i)

    ai.setSolutions(unrefined_basis_vectors)

def hemisphere_shortcut(ai,characteristic_sampling,max_cell):
    H = HemisphereSamplerBase(characteristic_grid = characteristic_sampling,max_cell=max_cell)
    H.hemisphere(ai,size=480,cutoff_divisor=1.5)
    #1.0/cutoff divisor is the height requirement (as a fraction of total Bragg spots)
    #required to consider a direction as a candidate basis vector.  4.0 good for
    #macromolecular work, 1.5 ok for small molecules; probably 1.1 would do.


 *******************************************************************************


 *******************************************************************************
rstbx/dps_core/tst_iotbx_converter.py
from __future__ import absolute_import, division, print_function
from six.moves import cPickle as pickle
from six.moves import cStringIO as StringIO
from cctbx import crystal,sgtbx,uctbx
from cctbx.sgtbx import lattice_symmetry
from rstbx.dps_core.lepage import iotbx_converter

class subgroup_comparator:

  def __init__(self,symmetry):
    self.symmetry = symmetry

  def get_input_symmetry_subgroups(self):
    cb_op_input_to_primitive = self.symmetry.change_of_basis_op_to_primitive_setting()
    primitive_symmetry = self.symmetry.primitive_setting()
    cb_op_input_to_minimum = primitive_symmetry.change_of_basis_op_to_minimum_cell() * cb_op_input_to_primitive

    subgroup_list = iotbx_converter(
      self.symmetry.change_basis(cb_op_input_to_minimum).unit_cell(),max_delta=3.0,
      bravais_types_only=False,
      space_group_symbol= str(self.symmetry.change_basis(cb_op_input_to_minimum).space_group_info()),
      force_minimum=False,interest_focus="input_symmetry")

    return subgroup_list

  def get_metric_symmetry_subgroups(self):
    subgroup_list = lattice_symmetry.metric_subgroups(
      self.symmetry,3.0,bravais_types_only=False,
      best_monoclinic_beta=False).result_groups

    return subgroup_list

  def show_list(self,any_subgroup_list):
    for subgroup in any_subgroup_list:
      subgroup['best_subsym'].show_summary()
      print()

  def get_best_subsym_list(self, any_subgroup_list):
    return [subgroup['best_subsym'] for subgroup in any_subgroup_list]

  def get_list_as_string(self,any_subgroup_list):
    S = StringIO()
    pickle.dump( [subgroup['best_subsym'] for subgroup in any_subgroup_list], S)
    return S.getvalue()

  def get_string_as_list(self,any_subgroup_string):
    from io import BytesIO
    S = BytesIO(any_subgroup_string.encode("ascii"))
    subgroups = pickle.load( S )
    return subgroups

  def compare_lists(self,a,b):
    assert len(a) == len(b)
    all_comparisons=[]
    for item in a:
      all_comparisons.append(False)
      for ritem in b:
        if item.is_similar_symmetry(ritem):
          all_comparisons[-1]=True
    assert False not in all_comparisons



def get_one_example():
  return subgroup_comparator(symmetry =
  # symmetry from 3ged
   crystal.symmetry(
    unit_cell=uctbx.unit_cell((124.287,124.287,162.608,90.0,90.0,90.0)),
    space_group=sgtbx.space_group_info('I 41 2 2').group())
  )

expected_output = """Unit cell: (124.287, 124.287, 162.608, 90, 90, 90)
Space group: I 41 2 2 (No. 98)

Unit cell: (124.287, 124.287, 162.608, 90, 90, 90)
Space group: I 41 (No. 80)

Unit cell: (124.287, 124.287, 162.608, 90, 90, 90)
Space group: I 21 21 21 (No. 24)

Unit cell: (162.608, 175.768, 175.768, 90, 90, 90)
Space group: F 2 2 2 (No. 22)

Unit cell: (124.287, 162.608, 124.287, 90, 90, 90)
Space group: I 1 2 1 (No. 5)

Unit cell: (119.725, 175.768, 119.725, 90, 94.4545, 90)
Space group: I 1 2 1 (No. 5)

Unit cell: (124.287, 124.287, 162.608, 90, 90, 90)
Space group: I 1 2 1 (No. 5)

Unit cell: (124.287, 124.287, 162.608, 90, 90, 90)
Space group: I 1 2 1 (No. 5)

Unit cell: (119.725, 175.768, 119.725, 90, 94.4545, 90)
Space group: I 1 2 1 (No. 5)

Unit cell: (119.725, 119.725, 119.725, 94.4545, 117.462, 117.462)
Space group: P 1 (No. 1)

Unit cell: (124.287, 124.287, 162.608, 90, 90, 90)
Space group: I 4/m m m (No. 139)

Unit cell: (124.287, 124.287, 162.608, 90, 90, 90)
Space group: I 4/m (No. 87)

Unit cell: (124.287, 124.287, 162.608, 90, 90, 90)
Space group: I m m m (No. 71)

Unit cell: (162.608, 175.768, 175.768, 90, 90, 90)
Space group: F m m m (No. 69)

Unit cell: (175.768, 162.608, 124.287, 90, 135, 90)
Space group: C 1 2/m 1 (No. 12)

Unit cell: (162.608, 175.768, 119.725, 90, 132.773, 90)
Space group: C 1 2/m 1 (No. 12)

Unit cell: (204.667, 124.287, 124.287, 90, 127.392, 90)
Space group: C 1 2/m 1 (No. 12)

Unit cell: (204.667, 124.287, 124.287, 90, 127.392, 90)
Space group: C 1 2/m 1 (No. 12)

Unit cell: (162.608, 175.768, 119.725, 90, 132.773, 90)
Space group: C 1 2/m 1 (No. 12)

Unit cell: (119.725, 119.725, 119.725, 117.462, 117.462, 94.4545)
Space group: P -1 (No. 2)

"""

expected_subgroup_lists=[
"""(lp0
ccopy_reg
_reconstructor
p1
(ccctbx.crystal
symmetry
p2
c__builtin__
object
p3
Ntp4
Rp5
(dp6
S'_unit_cell'
p7
ccctbx_uctbx_ext
unit_cell
p8
((F124.28700000000005
F124.28700000000005
F162.608
F90.0
F90.0
F90.0
tp9
tp10
Rp11
sS'_space_group_info'
p12
g1
(ccctbx.sgtbx
space_group_info
p13
g3
Ntp14
Rp15
(ccctbx_sgtbx_ext
space_group
p16
(S' I 4bw 2bw'
p17
tp18
Rp19
tp20
bsbag1
(g2
g3
Ntp21
Rp22
(dp23
g7
g8
((F124.28700000000005
F124.28700000000005
F162.608
F90.0
F90.0
F90.0
tp24
tp25
Rp26
sg12
g1
(g13
g3
Ntp27
Rp28
(g16
(S' I 4bw'
p29
tp30
Rp31
tp32
bsbag1
(g2
g3
Ntp33
Rp34
(dp35
g7
g8
((F124.28700000000003
F124.28700000000005
F162.60800000000003
F90.0
F90.0
F90.0
tp36
tp37
Rp38
sg12
g1
(g13
g3
Ntp39
Rp40
(g16
(S' I 2b 2c'
p41
tp42
Rp43
tp44
bsbag1
(g2
g3
Ntp45
Rp46
(dp47
g7
g8
((F162.608
F175.76836102666491
F175.76836102666496
F90.0
F90.0
F90.0
tp48
tp49
Rp50
sg12
g1
(g13
g3
Ntp51
Rp52
(g16
(S' F 2 2'
p53
tp54
Rp55
tp56
bsbag1
(g2
g3
Ntp57
Rp58
(dp59
g7
g8
((F124.28700000000003
F162.608
F124.28700000000005
F90.0
F90.000000000000014
F90.0
tp60
tp61
Rp62
sg12
g1
(g13
g3
Ntp63
Rp64
(g16
(S' C 2y (x,y,-x+z)'
p65
tp66
Rp67
tp68
bsbag1
(g2
g3
Ntp69
Rp70
(dp71
g7
g8
((F119.72455721571909
F175.76836102666491
F119.72455721571913
F90.0
F94.454526861950484
F90.0
tp72
tp73
Rp74
sg12
g1
(g13
g3
Ntp75
Rp76
(g16
(S' C 2y (x,y,-x+z)'
p77
tp78
Rp79
tp80
bsbag1
(g2
g3
Ntp81
Rp82
(dp83
g7
g8
((F124.28700000000005
F124.28700000000003
F162.60800000000003
F90.0
F90.000000000000014
F90.0
tp84
tp85
Rp86
sg12
g1
(g13
g3
Ntp87
Rp88
(g16
(S' C 2y (x,y,-x+z)'
p89
tp90
Rp91
tp92
bsbag1
(g2
g3
Ntp93
Rp94
(dp95
g7
g8
((F124.28700000000011
F124.28700000000003
F162.60800000000003
F90.0
F90.000000000000057
F90.0
tp96
tp97
Rp98
sg12
g1
(g13
g3
Ntp99
Rp100
(g16
(S' C 2y (x,y,-x+z)'
p101
tp102
Rp103
tp104
bsbag1
(g2
g3
Ntp105
Rp106
(dp107
g7
g8
((F119.72455721571912
F175.76836102666496
F119.72455721571913
F90.0
F94.454526861950498
F90.0
tp108
tp109
Rp110
sg12
g1
(g13
g3
Ntp111
Rp112
(g16
(S' C 2y (x,y,-x+z)'
p113
tp114
Rp115
tp116
bsbag1
(g2
g3
Ntp117
Rp118
(dp119
g7
g8
((F119.72455721571913
F119.72455721571913
F119.72455721571913
F94.454526861950498
F117.4623774424071
F117.4623774424071
tp120
tp121
Rp122
sg12
g1
(g13
g3
Ntp123
Rp124
(g16
(S' P 1'
p125
tp126
Rp127
tp128
bsba.""",
"""(lp0
ccopy_reg
_reconstructor
p1
(ccctbx.crystal
symmetry
p2
c__builtin__
object
p3
Ntp4
Rp5
(dp6
S'_unit_cell'
p7
ccctbx_uctbx_ext
unit_cell
p8
((F124.28700000000005
F124.28700000000005
F162.608
F90.0
F90.0
F90.0
tp9
tp10
Rp11
sS'_space_group_info'
p12
g1
(ccctbx.sgtbx
space_group_info
p13
g3
Ntp14
Rp15
(ccctbx_sgtbx_ext
space_group
p16
(S'-I 4 2'
p17
tp18
Rp19
tp20
bsbag1
(g2
g3
Ntp21
Rp22
(dp23
g7
g8
((F124.28700000000005
F124.28700000000005
F162.608
F90.0
F90.0
F90.0
tp24
tp25
Rp26
sg12
g1
(g13
g3
Ntp27
Rp28
(g16
(S'-I 4'
p29
tp30
Rp31
tp32
bsbag1
(g2
g3
Ntp33
Rp34
(dp35
g7
g8
((F124.28700000000003
F124.28700000000005
F162.60800000000003
F90.0
F90.0
F90.0
tp36
tp37
Rp38
sg12
g1
(g13
g3
Ntp39
Rp40
(g16
(S'-I 2 2'
p41
tp42
Rp43
tp44
bsbag1
(g2
g3
Ntp45
Rp46
(dp47
g7
g8
((F162.608
F175.76836102666491
F175.76836102666496
F90.0
F90.0
F90.0
tp48
tp49
Rp50
sg12
g1
(g13
g3
Ntp51
Rp52
(g16
(S'-F 2 2'
p53
tp54
Rp55
tp56
bsbag1
(g2
g3
Ntp57
Rp58
(dp59
g7
g8
((F175.76836102666491
F162.608
F124.28700000000003
F90.0
F135.0
F90.0
tp60
tp61
Rp62
sg12
g1
(g13
g3
Ntp63
Rp64
(g16
(S'-C 2y'
p65
tp66
Rp67
tp68
bsbag1
(g2
g3
Ntp69
Rp70
(dp71
g7
g8
((F162.608
F175.76836102666491
F119.72455721571909
F90.0
F132.77273656902477
F90.0
tp72
tp73
Rp74
sg12
g1
(g13
g3
Ntp75
Rp76
(g16
(S'-C 2y'
p77
tp78
Rp79
tp80
bsbag1
(g2
g3
Ntp81
Rp82
(dp83
g7
g8
((F204.66709562848644
F124.28700000000003
F124.28700000000005
F90.0
F127.39194857784936
F90.0
tp84
tp85
Rp86
sg12
g1
(g13
g3
Ntp87
Rp88
(g16
(S'-C 2y'
p89
tp90
Rp91
tp92
bsbag1
(g2
g3
Ntp93
Rp94
(dp95
g7
g8
((F204.66709562848644
F124.28700000000003
F124.28700000000005
F90.0
F127.39194857784936
F90.0
tp96
tp97
Rp98
sg12
g1
(g13
g3
Ntp99
Rp100
(g16
(S'-C 2y'
p101
tp102
Rp103
tp104
bsbag1
(g2
g3
Ntp105
Rp106
(dp107
g7
g8
((F162.608
F175.76836102666496
F119.72455721571912
F90.0
F132.77273656902477
F90.0
tp108
tp109
Rp110
sg12
g1
(g13
g3
Ntp111
Rp112
(g16
(S'-C 2y'
p113
tp114
Rp115
tp116
bsbag1
(g2
g3
Ntp117
Rp118
(dp119
g7
g8
((F119.72455721571913
F119.72455721571913
F119.72455721571913
F117.4623774424071
F117.4623774424071
F94.454526861950498
tp120
tp121
Rp122
sg12
g1
(g13
g3
Ntp123
Rp124
(g16
(S'-P 1'
p125
tp126
Rp127
tp128
bsba."""]

if __name__=="__main__":
  EX = get_one_example()
  input_sym_subgroups = EX.get_input_symmetry_subgroups()
  metric_sym_subgroups = EX.get_metric_symmetry_subgroups()

  #EX.show_list(input_sym_subgroups)
  #EX.show_list(metric_sym_subgroups)

  #print  EX.get_list_as_string(input_sym_subgroups)
  #print  EX.get_list_as_string(metric_sym_subgroups)

  # list the subgroups of a particular space group & compare to expected reference
  input_sym = expected_subgroup_lists[0]
  input_sym_list = EX.get_string_as_list(input_sym)
  EX.compare_lists(input_sym_list, EX.get_best_subsym_list(input_sym_subgroups))

  # list the subgroups of a metric symmetry & compare to expected reference
  metric_sym = expected_subgroup_lists[1]
  metric_sym_list = EX.get_string_as_list(metric_sym)
  EX.compare_lists(metric_sym_list, EX.get_best_subsym_list(metric_sym_subgroups))

  print("OK")


 *******************************************************************************


 *******************************************************************************
rstbx/dps_core/zuoreduction.py
from __future__ import absolute_import, division, print_function
from cctbx.uctbx import unit_cell,fast_minimum_reduction
from rstbx.dps_core.cell_assessment import unit_cell_too_small
from scitbx import matrix
from cctbx import crystal_orientation

def rwgk_niggli(UC,epsilon=None,cutoff=100.):

  '''Reference:
  R.W. Grosse-Kunstleve, N.K. Sauter and P.D. Adams.
  Numerically stable algorithms for the computation of reduced unit cells.
  Acta Cryst. A60, 1-6 (2004)
  '''

  if isinstance(UC,unit_cell):
    #return UC.niggli_cell(epsilon)
    return fast_minimum_reduction(UC).as_unit_cell()
  elif isinstance(UC,crystal_orientation.crystal_orientation):
    uc = UC.unit_cell()
    unit_cell_too_small(uc,cutoff=cutoff)
    #R = uc.niggli_reduction(epsilon)
    R = fast_minimum_reduction(uc)
    #minimum reduction gets r_inv as a tuple instead of scitbx.matrix
    rinverse = matrix.sqr( R.r_inv() )
    #NIG = R.as_unit_cell().parameters()
    #MIN = fast_minimum_reduction(uc).as_unit_cell().parameters()
    return UC.change_basis(rinverse.transpose().inverse().elems)

def test_reduction():
  from libtbx.test_utils import approx_equal
  uc = unit_cell((10,20,30,90,90,90))
  reference = uc.parameters()
  assert approx_equal (rwgk_niggli(uc).parameters(), reference)
  CO = crystal_orientation.crystal_orientation(uc.fractionalization_matrix(),True)
  assert approx_equal ( CO.unit_cell().parameters(), reference)
  assert approx_equal ( rwgk_niggli(CO).unit_cell().parameters(), reference)
  return True

if __name__=='__main__':
  test_reduction()
  print("OK")


 *******************************************************************************


 *******************************************************************************
rstbx/indexing/__init__.py


 *******************************************************************************


 *******************************************************************************
rstbx/indexing/do_index.py
from __future__ import absolute_import, division, print_function
from scitbx import matrix
from rstbx.dps_core import dps_core
from rstbx.dps_core.sampling import hemisphere_shortcut
from rstbx.dps_core.basis_choice import SelectBasisMetaprocedure

class algorithm_parameters:
  max_cell_edge_for_dps_fft = 100.0 # in Angstroms.
                                    # 100 Angstrom should be suitable for small molecular work
                                    # use Bragg spot spacing to estimate the maximum
                                    # cell edge for macromolecular cases
  directional_sampling_granularity = 0.029 # in radians; 0.029 radian granularity
                                           # is both fast enough and granular enough
                                           # to sample the hemisphere for small molecular work.
                                           # Tradeoff between performance and coverage occurs
                                           # for macromolecular work; DPS paper uses 0.029
  max_cell_edge_basis_choice = 8.0  # in Angstroms. This input parameter is important.
                                    # choice of basis vector is extremely sensitive to
                                    # this parameter--for silicon, must choose a value less
                                    # than twice the cell edge
def do_index(reciprocal_space_vectors, verbose=True):
  D = dps_core()
  D.setMaxcell(algorithm_parameters.max_cell_edge_for_dps_fft)
  D.setXyzData(reciprocal_space_vectors)
  hemisphere_shortcut(ai = D,
    characteristic_sampling = algorithm_parameters.directional_sampling_granularity,
    max_cell = algorithm_parameters.max_cell_edge_basis_choice)
  M = SelectBasisMetaprocedure(D)

  from rstbx.dps_core.lepage import iotbx_converter
  L = iotbx_converter(D.getOrientation().unit_cell().minimum_cell(),5.0)
  supergroup = L[0]

  triclinic = D.getOrientation().unit_cell()

  cb_op = supergroup['cb_op_inp_best'].c().as_double_array()[0:9]
  orient = D.getOrientation()
  orient_best = orient.change_basis(matrix.sqr(cb_op).transpose())
  constrain_orient = orient_best.constrain(supergroup['system'])
  D.setOrientation(constrain_orient)

  if verbose:
    for subgroup in L:
      print(subgroup.short_digest())
    print("\ntriclinic cell=%s volume(A^3)=%.3f"%(triclinic,triclinic.volume()))
    print("\nafter symmetrizing to %s:"%supergroup.reference_lookup_symbol())
    M.show_rms()
  return D,L


 *******************************************************************************


 *******************************************************************************
rstbx/indexing/tst_auto_monoscan.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import math
import libtbx.load_env
from libtbx.test_utils import approx_equal
from scitbx import matrix
from rstbx.array_family import flex
from rstbx.indexing.do_index import do_index

def get_token(name,buf):
  token_positions = {'dspacing':[16,26],'qx':[102,112],'qy':[112,122],'qz':[122,132],}
  return float(buf[token_positions[name][0]:token_positions[name][1]])

def parse_input(filename):
  with open(filename,"r") as G:
    lines = G.readlines()
  reciprocal_vectors = flex.vec3_double()
  qvec = ('qx','qy','qz')
  for line in lines[1:len(lines)]:
    l_buffer = line.rstrip()
    assert len(l_buffer)==132
    extended_q_nm = matrix.col(
     [get_token(qvec[i],l_buffer) for i in range(3)]
    )
    checklength = math.sqrt(extended_q_nm.dot(extended_q_nm))
    assert approx_equal(1./checklength, get_token('dspacing',l_buffer), eps=0.0001)
    #convert from nanometers to Angstroms
    extended_q_Angstrom = 0.1 * extended_q_nm
    reciprocal_vectors.append(extended_q_Angstrom.elems)
  return reciprocal_vectors

def test_automatic_monoscan(verbose=True):
  R = parse_input(libtbx.env.under_dist('rstbx', 'indexing/results2.dat'))
  return do_index(R,verbose)

if __name__=='__main__':
  test_automatic_monoscan()


 *******************************************************************************


 *******************************************************************************
rstbx/indexing/tst_dataset1.py
from __future__ import absolute_import, division, print_function
import math
import libtbx.load_env
from libtbx.test_utils import approx_equal
from scitbx import matrix
from rstbx.array_family import flex
from rstbx.indexing.do_index import do_index

# reciprocal space vectors expressed internally in units of inverse Angstroms:
# eV per KeV *
# Joules per eV /
# Joule-seconds (Planck's const) /
# meters/second (speed of light) /
# Angstroms per meter
keV_to_inv_Angstrom = 1E3             * \
                      1.602176487E-19 / \
                      6.62606896E-34  / \
                      299792458.E0    / \
                      1E10

def parse_input(filename):
  with open(filename,"r") as G:
    lines = G.readlines()
  reciprocal_vectors = flex.vec3_double()
  for line in lines[0:len(lines)-0]:
    tokens = line.strip().split('\t')
    assert len(tokens)==7
    q_vector = matrix.col([float(i) for i in tokens[4:7]])
    checklength = q_vector.dot(q_vector)
    assert approx_equal(checklength, 1.0, eps=0.001)
    energy_keV = float(tokens[2])
    theta = (math.pi/180.) * float(tokens[3])
    inv_lambda_Angstrom = keV_to_inv_Angstrom * energy_keV
    inv_d_Angstrom = 2. * math.sin(theta) * inv_lambda_Angstrom
    extended_q_Angstrom = inv_d_Angstrom * q_vector
    E=extended_q_Angstrom
    reciprocal_vectors.append(extended_q_Angstrom.elems)
  return reciprocal_vectors

def parse_synthetic(filename):
  with open(filename,"r") as G:
    lines = G.readlines()
  reciprocal_vectors = flex.vec3_double()
  #Example of Silicon F d 3-bar m unit cell oriented along lab axes
  A_mat = matrix.sqr((5.43,0.,0.,0.,5.43,0.,0.,0.,5.43))
  A_star = A_mat.inverse()
  for line in lines[1:]:
    tokens = line.strip().split('\t')
    assert len(tokens)==5
    miller = matrix.col([float(i) for i in tokens[0:3]])
    reciprocal_vector = A_star*miller
    reciprocal_vectors.append(reciprocal_vector.elems)
  return reciprocal_vectors

def test_case_obs_data(verbose=True):
  R = parse_input(libtbx.env.under_dist("rstbx", "indexing/si_brief.dat"))
  return do_index(R,verbose)

def test_case_synthetic_data(verbose=True):
  R = parse_synthetic(libtbx.env.under_dist("rstbx", "indexing/si_synthetic.dat"))
  return do_index(R,verbose)

if __name__=='__main__':
  print("Test autoindexing on synthetic reciprocal space positions")
  test_case_synthetic_data()
  print("Test autoindexing on monoscan data")
  test_case_obs_data()


 *******************************************************************************


 *******************************************************************************
rstbx/indexing_api/__init__.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import boost_adaptbx.boost.python as bp
import rstbx.dps_core # import dependency
bp.import_ext("rstbx_indexing_api_ext")
from rstbx_indexing_api_ext import *
import rstbx_indexing_api_ext as ext
from rstbx.array_family import flex
from scitbx.matrix import col
from rstbx_ext import * # gets us SpotClass

@bp.inject_into(ext.dps_extended)
class _():

  def set_beam_vector(self,beam):
    # input vector "beam" points from crystal to source.
    # S0 = -beam
    self.S0_vector = -beam
    self.inv_wave = self.S0_vector.length() # will be deprecated soon XXX

  def set_rotation_axis(self,axis):
    self.axis = axis
    assert axis.length() == 1.0

  def set_detector(self,input_detector):
    self.detector = input_detector

  @staticmethod
  def raw_spot_positions_mm_to_S1_vector( raw_spot_input, # as vec3_double
      detector, inverse_wave,
      panelID=None
      ):

    if panelID is None:
      panelID = flex.int(len(raw_spot_input),0)

    reciprocal_space_vectors = flex.vec3_double()

    # tile surface to laboratory transformation
    for n in range(len(raw_spot_input)):
      pid = panelID[n]
      lab_direct = col(detector[pid].get_lab_coord(raw_spot_input[n][0:2]))

    # laboratory direct to reciprocal space xyz transformation
      lab_recip = (lab_direct.normalize() * inverse_wave)

      reciprocal_space_vectors.append ( lab_recip )
    return reciprocal_space_vectors

  @staticmethod
  def raw_spot_positions_mm_to_reciprocal_space( raw_spot_input, # as vec3_double
      detector, inverse_wave, beam, axis, # beam, axis as scitbx.matrix.col
      panelID=None
      ):

    if panelID is None:
      panelID = flex.int(len(raw_spot_input),0)

    if axis is None:
      return raw_spot_positions_mm_to_reciprocal_space_xyz (
          raw_spot_input, detector, inverse_wave, beam, panelID )
    else:
      return raw_spot_positions_mm_to_reciprocal_space_xyz (
          raw_spot_input, detector, inverse_wave, beam, axis, panelID )

    """Assumptions:
    1) the raw_spot_input is in the same units of measure as the origin vector (mm).
       they are given in physical length, not pixel units
    2) the raw_spot centers of mass are given with the same corner/center convention
       as the origin vector.  E.g., spotfinder assumes that the mm scale starts in
       the middle of the lower-corner pixel.
    """

    reciprocal_space_vectors = flex.vec3_double()

    # tile surface to laboratory transformation
    for n in range(len(raw_spot_input)):
      pid = panelID[n]
      lab_direct = col(detector[pid].get_lab_coord(raw_spot_input[n][0:2]))

    # laboratory direct to reciprocal space xyz transformation
      lab_recip = (lab_direct.normalize() * inverse_wave) - beam

      reciprocal_space_vectors.append ( lab_recip.rotate_around_origin(
        axis=axis, angle=raw_spot_input[n][2], deg=True)
        )
    return reciprocal_space_vectors

  def model_likelihood(self,separation_mm):
    TOLERANCE = 0.5
    fraction_properly_predicted = 0.0

    #help(self.detector)
    #print self.detector[0]
    #help(self.detector[0])
    panel = self.detector[0]
    from scitbx import matrix
    Astar = matrix.sqr(self.getOrientation().reciprocal_matrix())

    import math
    xyz = self.getXyzData()

    # step 1.  Deduce fractional HKL values from the XyzData.  start with x = A* h
    #          solve for h:  h = (A*^-1) x
    Astarinv = Astar.inverse()
    Hint = flex.vec3_double()
    for x in xyz:
      H = Astarinv * x
      #print "%7.3f %7.3f %7.3f"%(H[0],H[1],H[2])
      Hint.append((round(H[0],0), round(H[1],0), round(H[2],0)))
    xyz_miller = flex.vec3_double()
    from rstbx.diffraction import rotation_angles
    # XXX limiting shell of 1.0 angstroms probably needs to be changed/ removed.  How?
    ra = rotation_angles(limiting_resolution=1.0,orientation = Astar,
                         wavelength = 1./self.inv_wave, axial_direction = self.axis)
    for ij,hkl in enumerate(Hint):
      xyz_miller.append( Astar * hkl ) # figure out how to do this efficiently on vector data
      if ra(hkl):
        omegas = ra.get_intersection_angles()
        rotational_diffs = [ abs((-omegas[omegaidx] * 180./math.pi)-self.raw_spot_input[ij][2])
                             for omegaidx in [0,1] ]
        min_diff = min(rotational_diffs)
        min_index = rotational_diffs.index(min_diff)
        omega = omegas[min_index]
        rot_mat = self.axis.axis_and_angle_as_r3_rotation_matrix(omega)

        Svec = (rot_mat * Astar) * hkl + self.S0_vector
#        print panel.get_ray_intersection(Svec), self.raw_spot_input[ij]
        if self.panelID is not None: panel = self.detector[ self.panelID[ij] ]
        calc = matrix.col(panel.get_ray_intersection(Svec))
        pred = matrix.col(self.raw_spot_input[ij][0:2])
#        print (calc-pred).length(), separation_mm * TOLERANCE
        if ((calc-pred).length() < separation_mm * TOLERANCE):
          fraction_properly_predicted += 1./ self.raw_spot_input.size()
    #print "fraction properly predicted",fraction_properly_predicted,"with spot sep (mm)",separation_mm
    return fraction_properly_predicted

  def get_predicted_spot_positions_and_status(self, old_status=None): #similar to above function; above can be refactored
    panel = self.detector[0]
    from scitbx import matrix
    Astar = matrix.sqr(self.getOrientation().reciprocal_matrix())
    # must have set the basis in order to generate Astar matrix.  How to assert this has been done???

    import math,copy
    xyz = self.getXyzData()
    if old_status is None:
      spot_status = [SpotClass.GOOD]*len(xyz)
    else:
      assert len(old_status)==len(xyz)
      spot_status = copy.copy( old_status ) # valid way of copying enums w/o reference
    self.assigned_hkl= [(0,0,0)]*len(xyz)

    # step 1.  Deduce fractional HKL values from the XyzData.  start with x = A* h
    #          solve for h:  h = (A*^-1) x
    Astarinv = Astar.inverse()
    Hint = flex.vec3_double()
    results = flex.vec3_double()
    for x in xyz:
      H = Astarinv * x
      Hint.append((round(H[0],0), round(H[1],0), round(H[2],0)))
    xyz_miller = flex.vec3_double()
    from rstbx.diffraction import rotation_angles
    ra = rotation_angles(limiting_resolution=1.0,orientation = Astar,
                         wavelength = 1./self.inv_wave, axial_direction = self.axis)
    for ij,hkl in enumerate(Hint):
      xyz_miller.append( Astar * hkl ) # figure out how to do this efficiently on vector data
      if ra(hkl):
        omegas = ra.get_intersection_angles()
        rotational_diffs = [ abs((-omegas[omegaidx] * 180./math.pi)-self.raw_spot_input[ij][2])
                             for omegaidx in [0,1] ]
        min_diff = min(rotational_diffs)
        min_index = rotational_diffs.index(min_diff)
        omega = omegas[min_index]
        rot_mat = self.axis.axis_and_angle_as_r3_rotation_matrix(omega)

        Svec = (rot_mat * Astar) * hkl + self.S0_vector
        if self.panelID is not None: panel = self.detector[ self.panelID[ij] ]
        xy = panel.get_ray_intersection(Svec)
        results.append((xy[0],xy[1],0.0))
        self.assigned_hkl[ij]=hkl
      else:
        results.append((0.0,0.0,0.0))
        spot_status[ij]=SpotClass.NONE
    return results,spot_status

  def get_hkl(self,idx):
    return self.assigned_hkl[idx]


 *******************************************************************************


 *******************************************************************************
rstbx/indexing_api/basis_choice.py
from __future__ import absolute_import, division, print_function
import exceptions
from libtbx.utils import Sorry

from cctbx.uctbx.reduction_base import iteration_limit_exceeded as KGerror
from rstbx.dps_core.cell_assessment import SmallUnitCellVolume
from rstbx.dps_core.basis_choice import HandleCombo as HandleComboBase
from rstbx.indexing_api.tools import AbsenceHandler

class FewSpots(exceptions.Exception): pass

unphysical_cell_cutoff_macromolecular_regime = 100. # Angstrom^3

class HandleCombo(HandleComboBase):
  def __init__(self,ai,combo):
    HandleComboBase.__init__(self,ai,combo,unphysical_cell_cutoff_macromolecular_regime)

  def handle_absences(self):
      Abs = AbsenceHandler()
      while Abs.absence_detected(self.ai.hklobserved()):
        newmat = Abs.correct(self.ai.getOrientation())
        self.ai.setOrientation(newmat)
        self.ai.niggli()

def select_best_combo_of(ai,better_than=0.36,candidates=20,basis=15,spot_sep=0.4,opt_inputs=None):
  """Take the first few candidates of ai.combos().  Search for all combos
     with hkl obs-calc better than a certain fraction limit, then
     handle absences, and return the best one"""
  best_combo = None
  #Establish a minimum acceptable likelihood requirement for the best_combo.
  # This parameter used to be 0.0; but was adjusted upward to 0.30 when preparing
  # figure 4, to force a wider search for good indexing solutions.
  # Explanation:
  #    The ai.model_likelihood function computes what percentage of observed
  #    spots have corresponding predicted spots within 0.5 x random rmsd.
  #    This will tend to be lower than 100% because:
  #      a) the default model mosaicity (0.1deg) is too low
  # If indexing fails on a good image either the best_likelihood must
  #  be lowered or the mosaicity must be increased.  Not yet decided how
  #  to do this for automated operations.--10/09/2003

  base_likelihood = 0.30
  best_likelihood = base_likelihood

  C = ai.combos(basis)
  maxtry = min(candidates,len(C))
  try_counter = 0
  solutions = SolutionTracker()

  for combo in C:
    #print "COMBO: (%d,%d,%d)"%(combo[0],combo[1],combo[2])
    try:
      HC = HandleCombo(ai,combo)

      dev = ai.rmsdev()
      if dev < better_than:
        HC.handle_absences()

        model_likelihood = ai.model_likelihood(spot_sep)
        #printcombo(ai,combo,model_likelihood)

        #  XXXXXX come back to this later
        #while ((best_likelihood<=base_likelihood) and
        #       ai.getMosaicity()<1.5 and
        #       model_likelihood <= best_likelihood):
        #         ai.setMosaicity(ai.getMosaicity() + 0.1)
        #         model_likelihood = ai.model_likelihood(spot_sep)
        #  XXXXXX

        if model_likelihood > best_likelihood:
          best_likelihood = model_likelihood
        this_solution = {'combo':combo,'model_likelihood':model_likelihood,
                          'volume':ai.getOrientation().unit_cell().volume(),"rmsdev":dev,
                          'lattice_likelihood':0.}

        solutions.append(this_solution)
      try_counter+=1
    except (FewSpots) as f:
      #print "COMBO: (%d,%d,%d) rejected on too few spots"%(combo[0],combo[1],combo[2])
      #printcombo(ai,combo)
      continue
    except (SmallUnitCellVolume) as f:
      #print "COMBO: (%d,%d,%d) rejected on small cell volume"%(combo[0],combo[1],combo[2])
      continue
    except (KGerror) as f:
      #print "COMBO: (%d,%d,%d) rejected on Krivy-Gruber iterations"%(combo[0],combo[1],combo[2])
      #printcombo(ai,combo)
      continue
    except (RuntimeError) as f:
      if str(f).find("Iteration limit exceeded")>0: continue
      print("Report this problem to DIALS developers:")
      print("COMBO: (%d,%d,%d) rejected on C++ runtime error"%(combo[0],combo[1],combo[2]))
      #printcombo(ai,combo)
      continue
    except Exception:
      raise
    if solutions.halts():
      return solutions
    if try_counter == maxtry: break
  return solutions

class SolutionTracker:
  def __init__(self):
    self.all_solutions=[]
    self.volume_filtered=[]
  def append(self,item):
    item['serial']=len(self.all_solutions)
    self.all_solutions.append(item)
    self.update_analysis()
  def update_analysis(self):
    self.best_likelihood = max([i['model_likelihood'] for i in self.all_solutions])
    self.close_solutions = [
      i for i in self.all_solutions
        if i['model_likelihood']>=0.9*self.best_likelihood ]
    '''added the following heuristic to accomodate poor diffraction such as
    19198 & 19199:  combos tend to form a big cluster in terms of
    model_likelihood.  In this top cluster, choose a good one that has the
    least primitive unit cell volume.  The other primitiveness test
    (in the paper) doesn't always work if there are too many close spots'''
    self.min_volume = min([i['volume'] for i in self.close_solutions])
      # new method, Jan 2005.  Rely on likelihood, except filter out solutions
      #  where the volume is significantly higher than the minimum:
    self.volume_filtered = [i for i in self.close_solutions if i['volume']<1.25*self.min_volume]
    self.best_volume_filtered_likelihood = max([i['model_likelihood'] for i in self.volume_filtered])
  def best_combo(self):
    if len(self.all_solutions)==0: return None
    combo = [i for i in self.volume_filtered if i['model_likelihood']==
            self.best_volume_filtered_likelihood][0]
    #print "In this round the best combo was",combo,"with likelihood",self.best_volume_filtered_likelihood
    return combo
  def best_lattice(self):
    if len(self.volume_filtered)==0: return None
    max_lattice = max([ele['lattice_likelihood'] for ele in self.volume_filtered])
    combo = [i for i in self.volume_filtered if i['lattice_likelihood']==
            max_lattice][0]
    return combo
  def halts(self):
    return len(self.volume_filtered)>=5

def increase_mosaicity(pd,ai,verbose=1):
  '''the parameter dictionary must be revised if the combo search determined
      that the minimum necessary mosaicity needed to be raised'''
  if 'mosaicity' in pd:
    old_mosaicity = float(pd['mosaicity'])
    new_mosaicity = ai.getMosaicity()
    if new_mosaicity>old_mosaicity:
      if verbose:
        print('input mosaicity %4.1f; new value %4.1f'%(old_mosaicity,new_mosaicity))
      pd['mosaicity']='%f'%new_mosaicity


### Limitations of this quick approach
# 1) local optimization of direct beam; no search of nearby local minima
# 2) no optimization of mosaicity
# 3) no rejection of second lattice
# 4) no check against the raw data for likelihood
# 5)  doesn't allow opt_inputs
# 6) target cell not allowed at present (FIXED)
# 7) no quick refinement of Direction vectors; requires laborious reindexing at present
# 8) no analytical approximation of fringe functions; could use LBFGS if this could be achieved
# 9) tied to refinement of beam vector instead of d0 origin vector (FIXED)
# 10) current supports single panel only (FIXED)

class SelectBasisMetaprocedure:
  def __init__(self,input_index_engine,input_dictionary,horizon_phil,
    opt_rawframes=None,opt_target=False,reduce_target=True):

    from libtbx import adopt_init_args
    adopt_init_args(self,locals())

    if self.horizon_phil.target_cell!=None:

      # change target to primitive centering type
      from cctbx import crystal
      input_symmetry = crystal.symmetry(
        unit_cell=self.horizon_phil.target_cell,
        space_group_symbol="Hall: %s 1" % self.horizon_phil.target_cell_centring_type)
      from cctbx.sgtbx.lattice_symmetry import metric_subgroups
      groups = metric_subgroups(input_symmetry, 0.0,
        enforce_max_delta_for_generated_two_folds=True)
      #groups.show()
      primitive_target_cell = groups.result_groups[-1]["best_subsym"].unit_cell()

      from rstbx.indexing_api.force_cell import force_cell
      best = force_cell(self.input_index_engine,primitive_target_cell)
      try:
        #print "Best score %.1f, triangle %12s"%(best["score"],str(best["triangle"])),best["orientation"].unit_cell()
        self.input_index_engine.setOrientation(best["orientation"])
      except Exception:
        raise Sorry("""Cannot index with the target_cell.  It is possible the target cell is wrong; try indexing
       without one.  It may be necessary to change the beam position, distance, or two theta angle on the
       command line.  See the http://cci.lbl.gov/labelit usage primer.""")
      # originally implemented with default conversion to niggli cell (reduce_target=True).
      # Added this as a configurable option in the context of indexing for sparse
      # nanocrystal stills, since we want to restrain to the originally-input target
      # setting, not necessarily the reduced cell:
      if reduce_target: self.input_index_engine.niggli()
      return

    #initial search
    all_sol = select_best_combo_of(input_index_engine,
                                      better_than=0.36,
                                      candidates=25,
                                      opt_inputs=(self.input_dictionary,opt_rawframes))

    best_combo = all_sol.best_combo()
    if best_combo!=None:
      self.evaluate_combo(best_combo)
      # XXX revisit the question of codecamp maxcell and whether this test should be functional
      #if self.horizon_phil.codecamp.maxcell != None: return
      if opt_rawframes == None or best_combo['lattice_likelihood'] > 3.0:
        return # quick abort to test out indexing 10/16/13

    raise Exception("""No autoindexing solution.
           Possible: incorrect beam center; multiple lattices; too few spots.""")

  def evaluate_combo(self,best_combo):
    increase_mosaicity(self.input_dictionary,self.input_index_engine,verbose=0)
    #print "best combo",best_combo
    HC = HandleCombo(self.input_index_engine,best_combo['combo'])
    HC.handle_absences()


 *******************************************************************************


 *******************************************************************************
rstbx/indexing_api/force_cell.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import math
from scitbx.matrix import col,sqr
from scitbx.math import unimodular_generator
from cctbx.uctbx import unit_cell
from cctbx import sgtbx
from rstbx.dps_core import Orientation

def is_length_match(a,b):
    return ( abs(a-b) < 0.02 * (a+b) )

def is_angle_match(a,b):
    return ( abs(a-b) < 3.0 )

def generate_unimodular_cells(cell):
    Amat = sqr(cell.orthogonalization_matrix()).transpose()

    for m in unimodular_generator(range=1).all():
      c_inv = sgtbx.rt_mx(sgtbx.rot_mx(m))
      orientation_similarity_cb_op = sgtbx.change_of_basis_op(c_inv).inverse()
      new_cell = cell.change_basis(orientation_similarity_cb_op)
      yield new_cell,orientation_similarity_cb_op

def get_triangle(lines):
  gamma = lines["gamma"]["match"]
  alpha = lines["alpha"]["match"]
  beta = lines["beta"]["match"]
  for acell,bcell in gamma:
    for al_bcell,al_ccell in alpha:
      if al_bcell != bcell: continue
      for be_ccell, be_acell in beta:
        if be_ccell != al_ccell: continue
        if be_acell != acell: continue
        return acell, al_bcell, be_ccell
  return None

def force_cell(index_engine,target_cell,verbose=False):

    if verbose:
      print("N candidates:",index_engine.n_candidates())
      from rstbx.dps_core import directional_show
      for x in range(index_engine.n_candidates()):
        directional_show( index_engine[x], "vector %d:"%x )
    Ns = index_engine.n_candidates()
    best = {"score":1.E100}
    orth = target_cell.orthogonalization_matrix()
    for working_cell,cb_op in generate_unimodular_cells(target_cell):
      #print "---->",working_cell, working_cell.volume()

      vectors = {"a":{"match":[],"length":working_cell.parameters()[0]},
                 "b":{"match":[],"length":working_cell.parameters()[1]},
                 "c":{"match":[],"length":working_cell.parameters()[2]},
                }
      for key in vectors.keys():
        for ns in range(Ns):
          if is_length_match(vectors[key]["length"],index_engine[ns].real):
            vectors[key]["match"].append(ns)
        #print key, vectors[key]

      lines = {"alpha":{"match":[],"points":("b","c"),"angle":working_cell.parameters()[3],"hit":False},
                "beta":{"match":[],"points":("c","a"),"angle":working_cell.parameters()[4],"hit":False},
               "gamma":{"match":[],"points":("a","b"),"angle":working_cell.parameters()[5],"hit":False},
              }
      for key in lines.keys():
        xmatch = len(vectors[lines[key]["points"][0]]["match"])
        ymatch = len(vectors[lines[key]["points"][1]]["match"])
        for xi in range(xmatch):
          for yi in range(ymatch):
            xkey = vectors[lines[key]["points"][0]]["match"][abs(xi)]
            ykey = vectors[lines[key]["points"][1]]["match"][abs(yi)]
            #print key,xkey,ykey,

            xvector = col(index_engine[xkey].dvec)
            yvector = col(index_engine[ykey].dvec)

            #print xvector.dot(yvector),
            costheta = xvector.dot(yvector)/math.sqrt(xvector.dot(xvector)*yvector.dot(yvector))
            angle = math.acos(costheta)*180./math.pi
            #print "angle %.2f"%angle,
            if is_angle_match(angle, lines[key]["angle"]):
              #print "*****",;
              lines[key]["hit"]=True
              lines[key]["match"].append((xkey,ykey))

      if lines["alpha"]["hit"] and lines["beta"]["hit"] and lines["gamma"]["hit"]:
        #print "HELLO HIT"
        #for key in lines.keys():
        #  print key, lines[key]
        tri = get_triangle(lines)
        #print "Triangle:",tri
        if tri==None: continue
        direct_matrix = sqr((
          index_engine[tri[0]].bvec()[0],index_engine[tri[0]].bvec()[1],index_engine[tri[0]].bvec()[2],
          index_engine[tri[1]].bvec()[0],index_engine[tri[1]].bvec()[1],index_engine[tri[1]].bvec()[2],
          index_engine[tri[2]].bvec()[0],index_engine[tri[2]].bvec()[1],index_engine[tri[2]].bvec()[2],
           ))
        ori = Orientation(direct_matrix,False)
        #print "Found unit cell",ori.unit_cell()
        #print "compatible",ori.unit_cell().change_basis(cb_op.inverse()),
        modo = ori.unit_cell().change_basis(cb_op.inverse()).orthogonalization_matrix()
        diff = ( modo[0]-orth[0],modo[1]-orth[1],modo[2]-orth[2],modo[4]-orth[4],modo[5]-orth[5],modo[8]-orth[8])
        score = math.sqrt(diff[0]*diff[0]+diff[1]*diff[1]+diff[2]*diff[2]+diff[3]*diff[3]+diff[4]*diff[4]+diff[5]*diff[5])
        #print "score %.1f"%score,tri
        if score<best["score"]:
          best = {"score":score,"triangle":tri,"orientation":ori.change_basis(sqr(cb_op.inverse().c().r().as_double()).transpose())}
    if verbose: print("Best score %.1f, triangle %12s"%(best["score"],str(best["triangle"])),best["orientation"].unit_cell())
    return best


 *******************************************************************************


 *******************************************************************************
rstbx/indexing_api/lattice.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from rstbx.array_family import flex
from rstbx.indexing_api import dps_extended
from rstbx.indexing_api.sampling import hemisphere_shortcut
from rstbx.dps_core import Directional_FFT
import math,cmath
from scitbx import matrix
from libtbx.test_utils import approx_equal
import boost_adaptbx.boost.python as bp

@bp.inject_into(dps_extended)
class _():

  def index(self, raw_spot_input=None, reciprocal_space_vectors=None,
            panel_addresses=None):
    assert [raw_spot_input, reciprocal_space_vectors].count(None) == 1
    self.raw_spot_input = raw_spot_input # deprecate record
    # must be x, y, phi in degrees, as a vec3_double

    if raw_spot_input is not None:
      if len(self.detector) > 1:
        assert len(raw_spot_input) == len(panel_addresses)

      # some hard protection against floating point error
      assert len(raw_spot_input) > 7 # no chance of 1DFFT indexing with 7 or fewer spots

      self.panelID = panel_addresses
      reciprocal_space_vectors = self.raw_spot_positions_mm_to_reciprocal_space(
        self.raw_spot_input, self.detector, self.inv_wave, self.S0_vector, self.axis,
        self.panelID)
    else:
      # some hard protection against floating point error
      assert len(reciprocal_space_vectors) > 7 # no chance of 1DFFT indexing with 7 or fewer spots

    if self.max_cell is None:
      from rstbx.indexing_api.nearest_neighbor import neighbor_analysis
      NN = neighbor_analysis(reciprocal_space_vectors)
      self.max_cell = NN.max_cell

    if self.recommended_grid_sampling_rad is None:
      rossmann_suggestion = 0.029 # radians; value used in Steller (1997)
      norms = reciprocal_space_vectors.norms()
      naive_obs_highest_resolution = 1./flex.max(norms)
      characteristic_grid = naive_obs_highest_resolution / self.max_cell
      # purely heuristic for now, figure out details later
      new_suggestion = 2. * characteristic_grid
      self.recommended_grid_sampling_rad = min(rossmann_suggestion,
                                               new_suggestion)

    self.setMaxcell(self.max_cell)

    self.setXyzData(reciprocal_space_vectors) # extended API

    hemisphere_shortcut(ai = self, # extended API
        characteristic_sampling = self.recommended_grid_sampling_rad,
        max_cell = self.max_cell
      )

  def sum_score_detail(self,reciprocal_space_vectors):
    """Evaluates the probability that the trial value of ( S0_vector | origin_offset ) is correct,
       given the current estimate and the observations.  The trial value comes through the
       reciprocal space vectors, and the current estimate comes through the short list of
       DPS solutions. Actual return value is a sum of NH terms, one for each DPS solution, each ranging
       from -1.0 to 1.0"""
    nh = min ( self.getSolutions().size(), 20) # extended API
    solutions = self.getSolutions() #extended API
    sum_score = 0.0
    for t in range(nh):
      #if t!=unique:continue
      dfft = Directional_FFT(angle = solutions[t], xyzdata = reciprocal_space_vectors,
                            granularity = self.granularity, amax = self.amax, # extended API
                            F0_cutoff = 11)
      kval = dfft.kval();
      kmax = dfft.kmax();
      kval_cutoff = self.raw_spot_input.size()/4.0; # deprecate record
      if ( kval > kval_cutoff ):
        ff=dfft.fft_result;
        kbeam = ((-dfft.pmin)/dfft.delta_p) + 0.5;
        Tkmax = cmath.phase(ff[kmax]);
        backmax = math.cos(Tkmax+(2*math.pi*kmax*kbeam/(2*ff.size()-1)) );
        ### Here it should be possible to calculate a gradient.
        ### Then minimize with respect to two coordinates.  Use lbfgs?  Have second derivatives?
        ### can I do something local to model the cosine wave?
        ### direction of wave travel.  Period. phase.
        sum_score += backmax;
      #if t == unique:
      #  print t, kmax, dfft.pmin, dfft.delta_p, Tkmax,(2*math.pi*kmax*kbeam/(2*ff.size()-1))
    return sum_score

  def get_S0_vector_score(self,trial_beam,unique):
    trial_beam = matrix.col(trial_beam)
    reciprocal_space_vectors = self.raw_spot_positions_mm_to_reciprocal_space(
      self.raw_spot_input, self.detector, self.inv_wave, trial_beam, self.axis,
      self.panelID)

    return self.sum_score_detail(reciprocal_space_vectors)

  def optimize_S0_local_scope(self):
      """Local scope: find the optimal S0 vector closest to the input S0 vector
         (local minimum, simple minimization)"""

      ############  Implement a direct beam check right here #########################
      unique=0
      # construct two vectors that are perpendicular to the beam.  Gives a basis for refining beam
      beamr0 = self.S0_vector.cross(self.axis).normalize()
      beamr1 = beamr0.cross(self.S0_vector).normalize()
      beamr2 = beamr1.cross(self.S0_vector).normalize()

      assert approx_equal(self.S0_vector.dot(beamr1), 0.)
      assert approx_equal(self.S0_vector.dot(beamr2), 0.)
      assert approx_equal(beamr2.dot(beamr1), 0.)
      # so the orthonormal vectors are self.S0_vector, beamr1 and beamr2

      grid = 10

      # DO A SIMPLEX MINIMIZATION
      from scitbx.simplex import simplex_opt
      class test_simplex_method(object):
        def __init__(selfOO):
          selfOO.starting_simplex=[]
          selfOO.n = 2
          for ii in range(selfOO.n+1):
            selfOO.starting_simplex.append(flex.random_double(selfOO.n))
          selfOO.optimizer = simplex_opt( dimension=selfOO.n,
                                        matrix  = selfOO.starting_simplex,
                                        evaluator = selfOO,
                                        tolerance=1e-7)
          selfOO.x = selfOO.optimizer.get_solution()

        def target(selfOO, vector):
          newvec = matrix.col(self.S0_vector) + vector[0]*0.0002*beamr1 + vector[1]*0.0002*beamr2
          normal = newvec.normalize() * self.inv_wave
          return -self.get_S0_vector_score(normal,unique) # extended API

      MIN = test_simplex_method()
      #MIN = test_cma_es()
      print("MINIMUM=",list(MIN.x))
      newvec = matrix.col(self.S0_vector) + MIN.x[0]*0.0002*beamr1 + MIN.x[1]*0.0002*beamr2
      new_S0_vector = newvec.normalize() * self.inv_wave

      print("old S0:",list(self.S0_vector.elems))
      print("new S0",list(new_S0_vector.elems))

      plot = False
      if plot:
        scores = flex.double()
        for x in range(-grid,grid+1):
         for y in range(-grid,grid+1):
          ref = matrix.col(self.S0_vector)
          newvec = ref + x*0.0002*beamr1 + y*0.0002*beamr2
          normal = newvec.normalize() * self.inv_wave
          scores.append( self.get_S0_vector_score(normal,unique) ) # extended API

        def show_plot(grid,excursi):
          excursi.reshape(flex.grid(grid, grid))

          from matplotlib import pyplot as plt
          plt.figure()
          CS = plt.contour([i*0.2 for i in range(grid)],[i*0.2 for i in range(grid)], excursi.as_numpy_array())
          plt.clabel(CS, inline=1, fontsize=10, fmt="%6.3f")
          plt.title("Score as to beam likelihood")
          plt.scatter([0.1*(grid-1)],[0.1*(grid-1)],color='g',marker='o')
          plt.scatter([0.1*(grid-1)+0.2*MIN.x[0]] , [0.1*(grid-1)+0.2*MIN.x[1]],color='r',marker='*')
          plt.axes().set_aspect("equal")
          plt.show()

        show_plot(2 * grid + 1, scores)

      return new_S0_vector

  @staticmethod
  def get_new_detector(old_detector,origin_offset):
    import copy
    new_detector = copy.deepcopy(old_detector)

    if len(new_detector) > 1 and len(new_detector.hierarchy()) > 1:
      h = new_detector.hierarchy()
      h.set_local_frame(fast_axis=h.get_fast_axis(),
                        slow_axis=h.get_slow_axis(),
                        origin=matrix.col(h.get_origin()) + origin_offset)
    else:
      for panel in new_detector:
        panel.set_local_frame(fast_axis=panel.get_fast_axis(),
                              slow_axis=panel.get_slow_axis(),
                              origin=matrix.col(panel.get_origin()) + origin_offset)

    return new_detector

  def get_origin_offset_score(self,trial_origin_offset):
    trial_detector = dps_extended.get_new_detector(self.detector,trial_origin_offset)

    reciprocal_space_vectors = self.raw_spot_positions_mm_to_reciprocal_space(
      self.raw_spot_input, trial_detector, self.inv_wave, self.S0_vector, self.axis,
      self.panelID)

    return self.sum_score_detail(reciprocal_space_vectors)

  def optimize_origin_offset_local_scope(self):
      """Local scope: find the optimal origin-offset closest to the current overall detector position
         (local minimum, simple minimization)"""
      # construct two vectors that are perpendicular to the beam.  Gives a basis for refining beam
      if self.axis is None:
        beamr0 = self.S0_vector.cross(matrix.col((1,0,0))).normalize()
      else:
        beamr0 = self.S0_vector.cross(self.axis).normalize()
      beamr1 = beamr0.cross(self.S0_vector).normalize()
      beamr2 = beamr1.cross(self.S0_vector).normalize()

      assert approx_equal(self.S0_vector.dot(beamr1), 0.)
      assert approx_equal(self.S0_vector.dot(beamr2), 0.)
      assert approx_equal(beamr2.dot(beamr1), 0.)
      # so the orthonormal vectors are self.S0_vector, beamr1 and beamr2

      # DO A SIMPLEX MINIMIZATION
      from scitbx.simplex import simplex_opt
      class test_simplex_method(object):
        def __init__(selfOO):
          selfOO.starting_simplex=[]
          selfOO.n = 2
          for ii in range(selfOO.n+1):
            selfOO.starting_simplex.append(flex.random_double(selfOO.n))
          selfOO.optimizer = simplex_opt( dimension=selfOO.n,
                                        matrix  = selfOO.starting_simplex,
                                        evaluator = selfOO,
                                        tolerance=1e-7)
          selfOO.x = selfOO.optimizer.get_solution()

        def target(selfOO, vector):
          trial_origin_offset = vector[0]*0.2*beamr1 + vector[1]*0.2*beamr2
          return -self.get_origin_offset_score(trial_origin_offset)

      MIN = test_simplex_method()
      trial_origin_offset =  MIN.x[0]*0.2*beamr1 + MIN.x[1]*0.2*beamr2
      #print "The Origin Offset best score is",self.get_origin_offset_score(trial_origin_offset)

      if self.horizon_phil.indexing.plot_search_scope:
        scope = self.horizon_phil.indexing.mm_search_scope
        plot_px_sz = self.detector[0].get_pixel_size()[0]
        grid = max(1,int(scope/plot_px_sz))
        scores = flex.double()
        for y in range(-grid,grid+1):
         for x in range(-grid,grid+1):
          new_origin_offset = x*plot_px_sz*beamr1 + y*plot_px_sz*beamr2
          scores.append( self.get_origin_offset_score(new_origin_offset) )

        def show_plot(widegrid,excursi):
          excursi.reshape(flex.grid(widegrid, widegrid))

          def igrid(x): return x - (widegrid//2)
          from matplotlib import pyplot as plt
          plt.figure()
          CS = plt.contour([igrid(i)*plot_px_sz for i in range(widegrid)],
                           [igrid(i)*plot_px_sz for i in range(widegrid)], excursi.as_numpy_array())
          plt.clabel(CS, inline=1, fontsize=10, fmt="%6.3f")
          plt.title("Wide scope search for detector origin offset")
          plt.scatter([0.0],[0.0],color='g',marker='o')
          plt.scatter([0.2*MIN.x[0]] , [0.2*MIN.x[1]],color='r',marker='*')
          plt.axes().set_aspect("equal")
          plt.xlabel("offset (mm) along beamr1 vector")
          plt.ylabel("offset (mm) along beamr2 vector")
          plt.show()

        show_plot(widegrid = 2 * grid + 1, excursi = scores)

      return dps_extended.get_new_detector(self.detector, trial_origin_offset)

  def get_basis_general(self):
    """
    In this function, self requires the following abstract interface:
       n_candidates() = number of candidate basis solutions presented
       __getitem__(i) = return the ith candidate basis vector of type rstbx_ext.Direction
       setOrientation(orientation) where orientation is a cctbx.crystal_orientation object.
          must represent the primitive setting.
       getOrientation()
       niggli() adjusts the stored orientation to the niggli setting
       getMosaicity() = mosaicity in degrees, from labelit, will be removed from interface
       hklobserved()
       combos()
       rmsdev()
       model_likelihood()

    """

    """side-effect: sets orientation matrix"""
    from rstbx.indexing_api.basis_choice import SelectBasisMetaprocedure as SBM
    pd = {}
    M = SBM(input_index_engine = self,input_dictionary = pd, horizon_phil = self.horizon_phil) # extended API

    print("Finished SELECT BASIS with solution M",M)

    from rstbx.dps_core.lepage import iotbx_converter
    L = iotbx_converter(self.getOrientation().unit_cell().minimum_cell(),5.0) # extended API
    supergroup = L[0]

    triclinic = self.getOrientation().unit_cell() # extended API

    cb_op = supergroup['cb_op_inp_best'].c().as_double_array()[0:9]
    orient = self.getOrientation() # extended API
    orient_best = orient.change_basis(matrix.sqr(cb_op).transpose())
    constrain_orient = orient_best.constrain(supergroup['system'])
    self.setOrientation(constrain_orient) # extended API
    L[-1]["orient"] = orient

    if True:
      for subgroup in L:
        print(subgroup.short_digest())
      print("\ntriclinic cell=%s volume(A^3)=%.3f"%(triclinic,triclinic.volume()))
      print("\nafter symmetrizing to %s:"%supergroup.reference_lookup_symbol())
      #M.show_rms()
    return L

class DPS_primitive_lattice(dps_extended):
  def __init__(self, max_cell, recommended_grid_sampling_rad, horizon_phil):
    from libtbx import adopt_init_args
    adopt_init_args(self,locals())
    dps_extended.__init__(self)

class basis_choice_adapter(dps_extended):
  def __init__(self):
    from libtbx import adopt_init_args
    adopt_init_args(self,locals())
    dps_extended.__init__(self)

#start here.
#0) rationalize the L class
#) fully document.  Have a map from here to there.  Implement Richard's fix
#P) figure out where the ".constrain()" code is
#X) symmetry
#1) encapsulate the parameter refinement
#2) encapsulate the outlier rejection.  Do not pass the autoindexengine to it
#3) encapsulate the get_basis_general command so it takes the canonical objects. not autoindexengine.


 *******************************************************************************


 *******************************************************************************
rstbx/indexing_api/nearest_neighbor.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import math

class neighbor_analysis(object):
  def __init__(self, rs_vectors, percentile=0.05):
    from scitbx.array_family import flex
    NEAR = 10
    self.NNBIN = 5 # target number of neighbors per histogram bin

    # nearest neighbor analysis
    from annlib_ext import AnnAdaptor
    query = flex.double()
    for spot in rs_vectors: # spots, in reciprocal space xyz
      query.append(spot[0])
      query.append(spot[1])
      query.append(spot[2])

    assert len(rs_vectors)>NEAR # Can't do nearest neighbor with too few spots

    IS_adapt = AnnAdaptor(data=query,dim=3,k=1)
    IS_adapt.query(query)

    direct = flex.double()
    for i in range(len(rs_vectors)):
       direct.append(1.0/math.sqrt(IS_adapt.distances[i]))

    # determine the most probable nearest neighbor distance (direct space)
    hst = flex.histogram(direct, n_slots=int(len(rs_vectors)/self.NNBIN))
    centers = hst.slot_centers()
    islot = hst.slots()
    highest_bin_height = flex.max(islot)
    most_probable_neighbor = centers[list(islot).index(highest_bin_height)]

    if False:  # to print out the histogramming analysis
      smin, smax = flex.min(direct), flex.max(direct)
      stats = flex.mean_and_variance(direct)
      import sys
      out = sys.stdout
      print("     range:     %6.2f - %.2f" % (smin, smax), file=out)
      print("     mean:      %6.2f +/- %6.2f on N = %d" % (
        stats.mean(), stats.unweighted_sample_standard_deviation(), direct.size()), file=out)
      hst.show(f=out, prefix="    ", format_cutoffs="%6.2f")
      print("", file=out)

    # determine the 5th-percentile direct-space distance
    perm = flex.sort_permutation(direct, reverse=True)
    percentile = direct[perm[int(percentile * len(rs_vectors))]]

    MAXTOL = 1.5 # Margin of error for max unit cell estimate
    self.max_cell = max( MAXTOL * most_probable_neighbor,
                         MAXTOL * percentile)

    if False:
      self.plot(direct)

  def plot(self,val):
    import numpy as np

    hist,bins = np.histogram(val,bins=int(len(val)/self.NNBIN))
    width = 0.7*(bins[1]-bins[0])
    center = (bins[:-1]+bins[1:])/2
    import matplotlib.pyplot as plt
    plt.bar(center, hist, align="center", width=width)
    plt.show()


 *******************************************************************************


 *******************************************************************************
rstbx/indexing_api/outlier_detection.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import math
import scitbx.math
from scitbx.array_family import flex
from rstbx.outlier_spots.fit_distribution import fit_cdf
from rstbx.indexing_api import rayleigh_cpp
from rstbx_ext import * # gets us SpotClass
from six.moves import zip

def format_data(x_data=None,y_data=None):
  """
  =============================================================================
  Function converts a list of separate x and y coordinates to a format suitable
  for plotting in ReportLab

  Arguments:
    x_data - a list of x coordinates (or any object that can be indexed)
    y_data - a list of y coordinates (or any object that can be indexed)

  Returns:
    A tuple of tuples containing the paired data

  Notes:
    The output from this function should be in a list for plotting in ReportLab.
    Multiple items in the list represent multiple data sets
  -----------------------------------------------------------------------------
  """
  assert(x_data is not None)
  assert(y_data is not None)
  assert(len(x_data) == len(y_data))

  # store data
  data = []
  for x,y in zip(x_data,y_data):
    data.append((x,y))
  data = tuple(data)

  return data

def make_histogram_data(data=None,n_bins=25):
  """
  =============================================================================
  Function generates a histogram given the data

  Arguments:
    data - data to be counted
    n_bins - number of bins to use in the histogram

  Returns:
    Two tuples are returned: upper limit of each bin, count in each bin

  Notes:
    data should already be sorted (minimum to maximum)
    the returned tuples will be of length n_bins
  -----------------------------------------------------------------------------
  """
  # determine binning
  min_value = float(math.floor(data[0]))
  max_value = 1.0
  d_bins = (max_value - min_value)/float(n_bins)
  bins = [0.0 for i in range(n_bins)]
  bins[0] = min_value + d_bins
  for i in range(1,n_bins):
    bins[i] = bins[i-1] + d_bins
  bins.append(float(math.ceil(data[-1])))
  n_bins = n_bins + 1
  # sort data into bins
  current_bin = 0
  histogram_data = [0 for i in range(n_bins)]
  for i in range(len(data)):
    if (data[i] < bins[current_bin]):
      histogram_data[current_bin] = histogram_data[current_bin] + 1
    else:
      while(data[i] > bins[current_bin]):
        current_bin = current_bin + 1

      histogram_data[current_bin] = histogram_data[current_bin] + 1
  if ((data[-1] < bins[-1]) and (data[-1] > bins[-2])):
    histogram_data[-1] = histogram_data[-1] + 1

  # fractionalize output
  for i in range(len(histogram_data)):
    histogram_data[i] = float(histogram_data[i])/float(len(data))

  return bins,histogram_data

class find_outliers:

  def __init__(self,ai=None,fraction=0.4,verbose=True,horizon_phil=None):

    # initialize variables
    self.fraction = fraction
    self.observed_spots = None
    self.predicted_spots = None
    self.fraction_spot_indices = None
    self.good = None

    self.sorted_observed_spots = None
    self.dr = None
    self.x = None
    self.not_good_dr = None
    self.dx = None
    self.dy = None

    self.plot_dxdy_data = None
    self.plot_cdf_data = None
    self.plot_pdf_data = None
    self.verbose = verbose
    self.cache_status = self.update(horizon_phil,ai,status_with_marked_outliers=None)

  def update(self,horizon_phil,ai,status_with_marked_outliers,verbose=False):
    # first time through: status with marked outliers is None; no input information
    # second time through after re-refinement: status is a list of SpotClass instances
    first_time_through = status_with_marked_outliers is None

    # update spot positions
    self.observed_spots = ai.raw_spot_input
    self.predicted_spots,current_status = ai.get_predicted_spot_positions_and_status(
                                          old_status = status_with_marked_outliers )

    # store Miller indices
    self.hkl = [0 for i in range(len(self.observed_spots))]
    for i in range(len(self.observed_spots)):
      self.hkl[i] = ai.get_hkl(i)
    return self.update_detail(horizon_phil,current_status,first_time_through,verbose)

  def update_detail(self,horizon_phil,current_status,first_time_through,verbose):
    assert(len(self.observed_spots) == len(self.predicted_spots))

    if horizon_phil.indexing.outlier_detection.verbose:
      classes=[str(current_status[i]) for i in range(len(self.observed_spots))]
      class_types = set(classes)
      class_counts = dict([[item,classes.count(item)] for item in class_types])
      flex_counts = flex.int(class_counts.values())
      assert flex.sum(flex_counts) == len(self.observed_spots)
      #for pair in class_counts.items():
      #  print "%10s %6d"%pair
      #print "%10s %6d"%("TOTAL",len(self.observed_spots))
      if status_with_marked_outliers == None:
        # status_with_marked_outliers==None is shorthand for identifying the first run through
        print("""After indexing on a subset of %d spots (from all images), %d were reclassified as
      either lying on the spindle, or potential overlapped spots or ice rings."""%(
      len(self.observed_spots),len(self.observed_spots)-class_counts["GOOD"]))
      else:
        print("""Rerefinement on just the well-fit spots followed by spot reclassification
      leaves %d good spots on which to calculate a triclinic rmsd."""%(class_counts["GOOD"]))

    # check good spots
    if (self.good is not None):
      match = 0
      for i in range(len(self.observed_spots)):
        if ((current_status[i] == SpotClass.GOOD) and self.good[i]):
          match = match + 1
      if self.verbose:print("Number of GOOD spots matched with previous model =",match)

    # calculate differences for all spots
    self.sorted_observed_spots = {}
    self.dr = flex.double()
    self.not_good_dr = flex.double()
    self.dx = [0.0 for i in range(len(self.observed_spots))]
    self.dy = [0.0 for i in range(len(self.observed_spots))]
    for i in range(len(self.observed_spots)):
      o = self.observed_spots[i]
      p = self.predicted_spots[i]
      self.dx[i] = o[0] - p[0]
      self.dy[i] = o[1] - p[1]
      self.sorted_observed_spots[
        math.sqrt(self.dx[i]*self.dx[i] + self.dy[i]*self.dy[i])] = i

    # separate GOOD spots
    spotclasses = {SpotClass.GOOD:0,SpotClass.SPINDLE:0,SpotClass.OVERLAP:0,SpotClass.ICE:0,SpotClass.OUTLIER:0,SpotClass.NONE:0}
    for key in sorted(self.sorted_observed_spots.keys()):
      spotclass = current_status[self.sorted_observed_spots[key]]
      spotclasses[spotclass]+=1
      if (current_status[self.sorted_observed_spots[key]] == SpotClass.GOOD):
        self.dr.append(key)
      else:
        self.not_good_dr.append(key)
    if verbose: print(", ".join(["=".join([str(i[0]),"%d"%i[1]]) for i in spotclasses.items()]), end=' ')
    totalsp = sum(spotclasses.values())
    if verbose: print("Total=%d"%(totalsp),"# observed spots",len(self.observed_spots))
    assert totalsp == len(self.observed_spots), "Some spot pairs have the same predicted-observed distances. Do you have duplicated images?"

    self.x = flex.double(len(self.dr))
    for i in range(len(self.x)):
      self.x[i] = float(i)/float(len(self.x))

    limit = int(self.fraction*len(self.dr))
    if limit < 4: return # Basic sanity check, need at least a few good spots to fit the distribution
    fitted_rayleigh = fit_cdf(x_data=self.dr[0:limit],
                              y_data=self.x[0:limit],distribution=rayleigh_cpp)
    if False:
        y_data=self.x[0:limit]
        inv_cdf = [fitted_rayleigh.distribution.inv_cdf(cdf) for cdf in y_data]
        from matplotlib import pyplot as plt
        plt.plot(self.dr[0:limit],self.x[0:limit],"r+")
        plt.plot(inv_cdf,y_data,"b.")
        plt.show()

    # store indices for spots used for fitting
    self.fraction_spot_indices = []
    for dr in self.dr[0:limit]:
      self.fraction_spot_indices.append(self.sorted_observed_spots[dr])

    # generate points for fitted distributions
    rayleigh_cdf_x = flex.double(range(500))
    rayleigh_cdf_x /= float(len(rayleigh_cdf_x))
    rayleigh_cdf = fitted_rayleigh.distribution.cdf(x=rayleigh_cdf_x)

    # generate points for pdf
    dr_bins,dr_histogram = make_histogram_data(data=self.dr,n_bins=100)
    rayleigh_pdf = flex.double(len(dr_bins))
    for i in range(len(dr_bins)):
      rayleigh_pdf[i] = fitted_rayleigh.distribution.pdf(x=dr_bins[i])
    rayleigh_pdf = rayleigh_pdf/flex.sum(rayleigh_pdf)
    dr_bins = flex.double(dr_bins)
    dr_histogram = flex.double(dr_histogram)

    # standard deviation for cdf
    sd = math.sqrt((4.0-math.pi)/(2.0)*
                   fitted_rayleigh.x[0]*fitted_rayleigh.x[0])
    if self.verbose:print('Standard deviation of Rayleigh fit = %4.3f'%sd)
    sd_data = None
    radius_outlier_index = None
    limit_outlier = None
    # --- Quoted code superceeded by extension module call to find_green_bar
    """
    for i in range(len(rayleigh_cdf_x)):
      mx = rayleigh_cdf_x[i]
      my = rayleigh_cdf[i]
      for j in range(1,len(self.dr)):
        upper_x = self.dr[j]
        upper_y = self.x[j]
        lower_x = self.dr[j-1]
        lower_y = self.x[j-1]
        if ((my >= lower_y) and (my < upper_y)):
          if ((sd <= (upper_x - mx)) and ((lower_x - mx) > 0.0)):
            sd_data = ((mx,my),(lower_x,lower_y))
            radius_outlier_index = j-1
            limit_outlier = lower_x
            if self.verbose:print "Width of green bar = %4.3f"%(lower_x - mx)
            break
        if (sd_data is not None):
          break
    """
    from rstbx.indexing_api import find_green_bar
    green = find_green_bar(rayleigh_cdf_x = rayleigh_cdf_x,
                           rayleigh_cdf = rayleigh_cdf,
                           dr = self.dr, x = self.x, sd = sd)
    if green.is_set:
      #assert radius_outlier_index == green.radius_outlier_index
      #assert limit_outlier == green.limit_outlier
      #assert sd_data[0][0] == green.sd_mx
      #assert sd_data[0][1] == green.sd_my
      #assert sd_data[1][0] == green.sd_lower_x
      #assert sd_data[1][1] == green.sd_lower_y
      if self.verbose:print("Width of green bar = %4.3f"%(green.sd_lower_x - green.sd_mx))
      radius_outlier_index = green.radius_outlier_index
      limit_outlier = green.limit_outlier
      sd_data = ((green.sd_mx, green.sd_my), (green.sd_lower_x, green.sd_lower_y))

    if (radius_outlier_index is None):
      radius_outlier_index = len(self.dr)
    if (limit_outlier is None):
      limit_outlier = self.dr[-1]
    radius_95 = None
    for i in range(len(rayleigh_cdf)):
      if (rayleigh_cdf[i] >= 0.95):
        radius_95 = rayleigh_cdf_x[i]
        break
    if (radius_95 is None):
      radius_95 = rayleigh_cdf_x[-1]
    upper_circle = []
    lower_circle = []
    d_radius = 2.0*radius_95/100.0
    x = -radius_95
    r2 = radius_95*radius_95
    for i in range(100):
      y = math.sqrt(r2 - x*x)
      upper_circle.append((x,y))
      lower_circle.append((x,-y))
      x = x + d_radius
    y = 0.0
    upper_circle.append((x,y))
    lower_circle.append((x,-y))
    self.sqrtr2 = math.sqrt(r2)

    # color code dx dy
    dxdy_fraction = []
    dxdy_inliers = []
    dxdy_outliers = []

    limit = self.dr[int(self.fraction*len(self.dr))]

    trifold = dict(fraction=0,inlier=0,outlier=0,total=0)
    for key in self.dr:
      trifold["total"]+=1
      i = self.sorted_observed_spots[key]
      if (key < limit):
        trifold["fraction"]+=1
        if (not ((self.dx[i] > 1.0) or (self.dx[i] < -1.0) or
                 (self.dy[i] > 1.0) or (self.dy[i] < -1.0))):
          dxdy_fraction.append((self.dx[i],self.dy[i]))
      elif (key < limit_outlier):
        trifold["inlier"]+=1
        if (not ((self.dx[i] > 1.0) or (self.dx[i] < -1.0) or
                 (self.dy[i] > 1.0) or (self.dy[i] < -1.0))):
          dxdy_inliers.append((self.dx[i],self.dy[i]))
      else:
        trifold["outlier"]+=1
        if (not ((self.dx[i] > 1.0) or (self.dx[i] < -1.0) or
                 (self.dy[i] > 1.0) or (self.dy[i] < -1.0))):
          dxdy_outliers.append((self.dx[i],self.dy[i]))
    if verbose: print(", ".join(["=".join([str(i[0]),"%d"%i[1]]) for i in trifold.items()]))

    # color code observed fractions
    o_fraction = []
    o_inliers = []
    o_outliers = []
    mr = format_data(x_data=rayleigh_cdf_x,y_data=rayleigh_cdf)
    limit = int(self.fraction*len(self.dr))
    for i in range(len(self.dr)):
      if (self.dr[i] <= 1.0):
        if (i < limit):
          o_fraction.append((self.dr[i],self.x[i]))
        elif (i < radius_outlier_index):
          o_inliers.append((self.dr[i],self.x[i]))
        else:
          o_outliers.append((self.dr[i],self.x[i]))
    if horizon_phil.indexing.outlier_detection.verbose:
      o_outliers_for_severity = []
      for i in range(radius_outlier_index, len(self.dr)):
        o_outliers_for_severity.append((self.dr[i],self.x[i]))

    # limit data range
    for i in range(len(dr_bins)):
      if (dr_bins[i] > 1.0):
        dr_bins.resize(i)
        dr_histogram.resize(i)
        rayleigh_pdf.resize(i)
        break
    ho = format_data(x_data=dr_bins,y_data=dr_histogram)
    hr = format_data(x_data=dr_bins,y_data=rayleigh_pdf)

    # format data for graphing
    self.plot_dxdy_data = [dxdy_fraction,dxdy_inliers,dxdy_outliers,
                           [(0.0,0.0)],[],[],[],[]]

    self.framework = {4:dict(status=SpotClass.SPINDLE),
                      5:dict(status=SpotClass.OVERLAP),
                      6:dict(status=SpotClass.OUTLIER),
                      7:dict(status=SpotClass.ICE),
    }
    for key in self.not_good_dr:
      i = self.sorted_observed_spots[key]
      status = current_status[i]
      if (not ((self.dx[i] > 1.0) or (self.dx[i] < -1.0) or
               (self.dy[i] > 1.0) or (self.dy[i] < -1.0))):
        statuskey = [k for k in self.framework.keys() if self.framework[k]["status"]==status][0]
        self.plot_dxdy_data[statuskey].append((self.dx[i],self.dy[i]))

    self.plot_cdf_data = [mr,o_fraction,o_inliers,o_outliers]
    if (sd_data is not None):
      self.plot_cdf_data.append(sd_data)
    self.plot_pdf_data = [ho,hr]

    # mark outliers
    if (first_time_through): #i.e., first time through the update() method
      if (radius_outlier_index < len(self.dr)):
        for i in range(radius_outlier_index,len(self.dr)):
          current_status[self.sorted_observed_spots[self.dr[i]]] = SpotClass.OUTLIER

    # reset good spots
    self.good = [False for i in range(len(self.observed_spots))]
    for i in range(len(self.observed_spots)):
      if (current_status[i] == SpotClass.GOOD):
        self.good[i] = True

    count_outlier = 0
    count_good = 0
    for i in range(len(self.observed_spots)):
      if (current_status[i] == SpotClass.OUTLIER):
        count_outlier = count_outlier + 1
      elif (current_status[i] == SpotClass.GOOD):
        count_good = count_good + 1
    if self.verbose:print('Old GOOD =', len(self.dr),\
          'OUTLIER =', count_outlier,\
          'New GOOD =', count_good)
    if horizon_phil.indexing.outlier_detection.verbose and status_with_marked_outliers is None:
      print("\nOf the remaining %d spots, %.1f%% were lattice outliers, leaving %d well-fit spots"%(
       len(self.dr),100.*count_outlier/len(self.dr), count_good ))
      if count_outlier==0:return
      #width of green bar is sd
      delta_spread = o_outliers_for_severity[1][1]-o_outliers_for_severity[0][1]
      severity = 0.
      for item in o_outliers_for_severity:
        delta_r = item[0] # obs - predicted deviation in mm
        spread = item[1] # order of observed deviation on a scale from 0 to 1
        # now invert the cdf to find expected delta r:
        expected_delta_r = fitted_rayleigh.distribution.sigma * math.sqrt(
          -2.* math.log(1.-spread) )
        #print item, expected_delta_r, (delta_r - expected_delta_r) / sd
        severity += ((delta_r - expected_delta_r) / sd)
      severity *= delta_spread
      print("The outlier severity is %.2f sigma [defined in J Appl Cryst (2010) 43, p.611 sec. 4].\n"%severity)
    return current_status


  # make plots
  def make_graphs(self,canvas=None,left_margin=None):#text=None):
    from reportlab.graphics import renderPDF
    from reportlab.lib.pagesizes import letter
    from reportlab.graphics.shapes import Drawing,String
    from reportlab.graphics.charts.legends import Legend
    from reportlab.graphics.charts.lineplots import LinePlot
    from reportlab.graphics.widgets.markers import makeMarker
    from reportlab.lib import colors
    from reportlab.lib.units import inch
    #help(colors)

    self.framework = {4:dict(status=SpotClass.SPINDLE,color=colors.black),
                      5:dict(status=SpotClass.OVERLAP,color=colors.limegreen),
                      6:dict(status=SpotClass.OUTLIER,color=colors.greenyellow),
                      7:dict(status=SpotClass.ICE,color=colors.skyblue),
    }


    # set size and position
    width,height = letter
    #letter_landscape = (width,height)
    plot_dim = 3.0*inch

    # construct scatter plot
    plot_dxdy_pos = (left_margin*inch,height - plot_dim - 0.5*inch)
    plot_dxdy = LinePlot()
    plot_dxdy.data = self.plot_dxdy_data

    std_colors = {0:colors.darkred, 1:colors.red, 2:colors.salmon}
    for key in std_colors.keys():
      plot_dxdy.lines[key].strokeColor = None
      plot_dxdy.lines[key].symbol = makeMarker('Circle')
      plot_dxdy.lines[key].symbol.strokeColor = None
      plot_dxdy.lines[key].symbol.fillColor = std_colors[key]
      plot_dxdy.lines[key].symbol.size = 1.2

    for key in self.framework.keys():
      plot_dxdy.lines[key].strokeColor = None
      plot_dxdy.lines[key].symbol = makeMarker('Circle')
      plot_dxdy.lines[key].symbol.strokeColor = None
      plot_dxdy.lines[key].symbol.fillColor = self.framework[key]["color"]
      plot_dxdy.lines[key].symbol.size = 1.2

    plot_dxdy.lines[3].strokeColor = None
    plot_dxdy.lines[3].symbol = makeMarker('Circle')
    plot_dxdy.lines[3].symbol.strokeColor = colors.blue
    plot_dxdy.lines[3].symbol.fillColor = None
    plot_dxdy.lines[3].symbol.strokeWidth = 0.6
    plot_dxdy.lines[3].symbol.size = plot_dim*(self.sqrtr2)
    #print plot_dxdy.lines[3].symbol.getProperties()
    plot_dxdy.width = plot_dim
    plot_dxdy.height = plot_dim
    plot_dxdy.xValueAxis.valueMax = 1.0
    plot_dxdy.xValueAxis.valueMin = -1.0
    plot_dxdy.xValueAxis.joinAxis = plot_dxdy.yValueAxis
    plot_dxdy.xValueAxis.joinAxisMode = 'value'
    plot_dxdy.xValueAxis.joinAxisPos = -1.0
    plot_dxdy.yValueAxis.valueMax = 1.0
    plot_dxdy.yValueAxis.valueMin = -1.0
    d_dxdy = Drawing(plot_dim,plot_dim)
    d_dxdy.add(plot_dxdy)

    # construct cdf plot
    plot_cdf_pos = (left_margin*inch, height - 2.0*(plot_dim + 0.5*inch))
    plot_cdf = LinePlot()
    plot_cdf.data = self.plot_cdf_data
    plot_cdf.lines[0].strokeColor = colors.blue

    for key in std_colors.keys():
      plot_cdf.lines[key+1].strokeColor = None
      plot_cdf.lines[key+1].symbol = makeMarker('Circle')
      plot_cdf.lines[key+1].symbol.strokeColor = None
      plot_cdf.lines[key+1].symbol.fillColor = std_colors[key]
      plot_cdf.lines[key+1].symbol.size = 1.2

    if (len(self.plot_cdf_data) == 5):
      plot_cdf.lines[4].strokeColor = colors.green
    plot_cdf.width = plot_dim
    plot_cdf.height = plot_dim
    plot_cdf.xValueAxis.valueMax = 1.0
    plot_cdf.xValueAxis.valueMin = 0.0
    plot_cdf.yValueAxis.valueMax = 1.0
    plot_cdf.yValueAxis.valueMin = 0.0
    d_cdf = Drawing(plot_dim,plot_dim)
    d_cdf.add(plot_cdf)

    # construct pdf plot
    plot_pdf_pos = (left_margin*inch, height - 3.0*(plot_dim + 0.5*inch))
    plot_pdf = LinePlot()
    plot_pdf.data = self.plot_pdf_data
    plot_pdf.lines[1].strokeColor = colors.blue
    plot_pdf.lines[0].strokeColor = None
    plot_pdf.lines[0].symbol = makeMarker('Circle')
    plot_pdf.lines[0].symbol.strokeColor = colors.red
    plot_pdf.lines[0].symbol.size = 1
    plot_pdf.width = plot_dim
    plot_pdf.height = plot_dim
    plot_pdf.xValueAxis.valueMax = 1.0
    plot_pdf.xValueAxis.valueMin = 0.0
    d_pdf = Drawing(2*plot_dim,plot_dim)
    d_pdf.add(plot_pdf)

    # add legend
    legend = Legend()
    legend.alignment = 'right'
    legend.colorNamePairs = [(std_colors[0],'Inliers (%d'%int(self.fraction*100.0) + '% used for fit)'),
                             (std_colors[1],'Other inliers'),
                             (std_colors[2],'Outliers, reject next round'),]
    for key in self.framework.keys():
      legend.colorNamePairs.append(  (self.framework[key]["color"], "%s"%self.framework[key]["status"]  )  )

    legend.x = plot_dim - 1.0*inch
    legend.y = plot_dim
    legend.columnMaximum = 8
    d_pdf.add(legend)

    # add titles
    title_pos = (plot_dim/2.0,plot_dim + 0.25*inch)
    title_dxdy = String(title_pos[0],title_pos[1],'dx vs. dy (all)')
    title_dxdy.fontSize = 15
    title_dxdy.textAnchor = 'middle'
    d_dxdy.add(title_dxdy)
    title_cdf = String(title_pos[0],title_pos[1],'cdf (good)')
    title_cdf.fontSize = 15
    title_cdf.textAnchor = 'middle'
    d_cdf.add(title_cdf)
    title_pdf = String(title_pos[0],title_pos[1],'pdf (good)')
    title_pdf.fontSize = 15
    title_pdf.textAnchor = 'middle'
    d_pdf.add(title_pdf)

    # draw everything
    renderPDF.draw(d_dxdy,canvas,plot_dxdy_pos[0],plot_dxdy_pos[1])
    renderPDF.draw(d_cdf,canvas,plot_cdf_pos[0],plot_cdf_pos[1])
    renderPDF.draw(d_pdf,canvas,plot_pdf_pos[0],plot_pdf_pos[1])

class find_outliers_from_matches(find_outliers):
  """Specifically rewritten ('RRR') to take dials refinery reflection manager matches.
     May have to be refactored again depending on what input list is finally chosen."""

  def get_new_status(self, old_status = None ):
    if old_status == None:
      return ( [SpotClass.GOOD] * len(self.predicted_spots) )
    else:
      return old_status

  def get_cache_status(self):
    value = flex.bool()
    if self.cache_status is None:
      raise Exception('No reflections left after removing outliers!')
    else:
      for status in self.cache_status:
        value.append( status==SpotClass.GOOD )
      return value

  def update(self,horizon_phil,matches,status_with_marked_outliers,verbose=False):
    # first time through: status with marked outliers is None; no input information
    # second time through after re-refinement: status is a list of SpotClass instances
    first_time_through = status_with_marked_outliers is None

    # update spot positions
    self.observed_spots = flex.vec2_double([(m.x_obs,m.y_obs) for m in matches])
    self.predicted_spots = flex.vec2_double([(m.x_calc,m.y_calc) for m in matches])

    # store Miller indices
    self.hkl = [m.miller_index for m in matches]

    current_status = self.get_new_status( old_status = status_with_marked_outliers )
    return self.update_detail(horizon_phil,current_status,first_time_through,verbose)


 *******************************************************************************


 *******************************************************************************
rstbx/indexing_api/outlier_procedure.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from rstbx_ext import SpotClass
from scitbx.array_family import flex
from rstbx.indexing_api import outlier_detection

class Graph:

  def __init__(self,fileout):
    from reportlab.pdfgen.canvas import Canvas
    from reportlab.lib.pagesizes import letter
    self.c = Canvas(fileout,pagesize=letter)

class OutlierPlotPDF:
  def __init__(self,filename):
    self.R = Graph(filename)

  def setTitle(self,title):
    self.title=title

  def labelit_screen_output(self,lines):
    # outlier detection output
    from reportlab.lib.pagesizes import letter
    from reportlab.lib.units import inch

    margin = 0.25
    block_text = self.R.c.beginText(margin*inch,letter[1] - margin*inch)
    block_text.setFont('Courier',7)
    for line in lines.split("\n"):
      block_text.textLine(line.rstrip())
    self.R.c.drawText(block_text)
    self.R.c.showPage()

def main_go(index_engine,verbose=False,phil_set=None):
    # first round of minimization
    if phil_set.indexing.outlier_detection.pdf is not None:
      phil_set.__inject__("writer",OutlierPlotPDF(phil_set.indexing.outlier_detection.pdf))

    # do some 12G parameter refinement here

    if verbose: print("Before outlier rejection, triclinic rmsd %.3f on %d spots"%(
      index_engine.residual(), index_engine.count_GOOD()))

    # outlier detection
    od = outlier_detection.find_outliers(ai=index_engine,verbose=verbose,
                                         horizon_phil=phil_set)
    if phil_set.indexing.outlier_detection.pdf is not None:
      od.make_graphs(canvas=phil_set.writer.R.c,left_margin=0.5)

    # do some 12G parameter refinement here, using the od.cache_status flags

    if verbose: print("-After outlier rejection, triclinic rmsd %.3f on %d spots"%(
      index_engine.residual(), index_engine.count_GOOD()))

    # update outlier graphs
    od.update(phil_set,ai=index_engine,status_with_marked_outliers=od.cache_status)

    if phil_set.indexing.outlier_detection.pdf is not None:
      od.make_graphs(canvas=phil_set.writer.R.c,left_margin=4.5)
      phil_set.writer.R.c.showPage()
      phil_set.writer.R.c.save()

    # estimate unit cell error

    if phil_set.indexing.outlier_detection.switch==True:
      # placeholder implementation XXX return to this later
      raw_spot_input = flex.vec3_double()
      assert len(process_dictionary['indexing'])==len(index_engine.get_observed_spot_positions(False))
      aipos = index_engine.get_observed_spot_positions(False)
      for ij in range(len(process_dictionary['indexing'])):
        spot = process_dictionary["indexing"][ij]
        if index_engine.get_status(ij)==SpotClass.OUTLIER:
          raw_spot_input.append((spot[0],spot[1],spot[2]))
      process_dictionary["indexing"] = raw_spot_input
      if len(raw_spot_input) < phil_set.distl_minimum_number_spots_for_indexing:
        message = "The remaining %d spots are less than the minimum %d set for indexing."%(
          len(raw_spot_input) , phil_set.distl_minimum_number_spots_for_indexing)
        print("Raising exception",message)
        raise Exception(message)
      print("Reindexing on the %d outlying spots to hunt for a second lattice"%len(raw_spot_input))
      index_engine = AutoIndexOne(process_dictionary,opt_rawframes = opt_frames,
                                  horizon_phil=phil_set)
      # do some 3G parameter refinement here
      # do some 12G parameter refinement here
      print("Reindexed OK")
"""Migration process:
   3) rid of dependency on process_dictionary and opt_ choices (DONE)
   4) get rid of saga spot status.  Use return values exclusively. (DONE)
   5) then implement efficiency by pushing to C++ the raw_spor_positions_mm_to_recip_space_xyz
   save 12 out of 36 seconds.
"""


 *******************************************************************************


 *******************************************************************************
rstbx/indexing_api/sampling.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import math
from rstbx.array_family import flex
from rstbx.dps_core import Direction
from annlib_ext import AnnAdaptor
from rstbx.dps_core import sampling
from rstbx.dps_core import Direction, directional_show
#from libtbx.development.timers import Timer

# various sampling algorithms to cover the directional hemisphere

class SimpleSampler(sampling.SimpleSamplerTool):
  # replaces the setSampling_internal() algorithm from dptbx.cpp, with
  # no confinement directions

  def __init__(self,maxgrid,charactergrid):
    self.max_grid = maxgrid  # the maximum allowable grid, corresponding
                             # to the grid sampling used in the Rossman DPS
                             # paper; approx 7900 directions; 0.4 seconds

    self.characteristic_grid = charactergrid # the natural grid sampling
                             # reflective of the problem at hand =
                             # obs. resolution limit / most conservative cell

    sampling.SimpleSamplerTool.__init__(self,
      min( self.max_grid, self.characteristic_grid ))
      # initial directional sampling in radians

  def get_top_solutions(self,ai,input_directions,size,cutoff_divisor):
    # size was 30 in the Rossmann DPS paper
    kval_cutoff = ai.getXyzSize()/cutoff_divisor;

    hemisphere_solutions = flex.Direction();
    hemisphere_solutions.reserve(size);

    for i in range(len(input_directions)):
      sampled_direction = ai.fft_result(input_directions[i])
      if sampled_direction.kval > kval_cutoff:
        hemisphere_solutions.append(sampled_direction)

    if (hemisphere_solutions.size()<3):
      return hemisphere_solutions
    kvals = flex.double([
  hemisphere_solutions[x].kval for x in range(len(hemisphere_solutions))])

    perm = flex.sort_permutation(kvals,True)

    #  conventional algorithm; just take the top scoring hits.
    #  use this for quick_grid, when it is known ahead of time
    #  that many (hundreds) of directions will be retained

    hemisphere_solutions_sort = flex.Direction(
        [hemisphere_solutions[p] for p in perm[0:min(size,len(perm))]])

    return hemisphere_solutions_sort;

  def hemisphere(self,ai,size,cutoff_divisor):
    #this replaces the hemisphere() function in dptbx.cpp
    return self.get_top_solutions(ai,self.angles,size,cutoff_divisor,
                                  grid=self.incr)

class HemisphereSampler(SimpleSampler):
  def __init__(self,max_grid,characteristic_grid,quick_grid):
    self.quick_grid = quick_grid
    SimpleSampler.__init__(self,max_grid,characteristic_grid)

    if self.quick_grid >= self.incr:
      #some quick benchmarks:
      estimate_number_of_target_grids = 2.*math.pi / (self.incr*self.incr)

      estimate_number_of_quick_grids = 2.*math.pi / (self.quick_grid*self.quick_grid)

      self.second_round_sampling = (
        math.pi*self.quick_grid*self.quick_grid)/(self.incr*self.incr)

      self.target_size_for_quick_grid_answer_vector = int(
        estimate_number_of_quick_grids /2./self.second_round_sampling)

  def get_top_solutions(self,ai,input_directions,size,cutoff_divisor,grid):
    # size was 30 in the Rossmann DPS paper

    kval_cutoff = ai.getXyzSize()/cutoff_divisor;

    hemisphere_solutions = flex.Direction();
    hemisphere_solutions.reserve(size);
    for i in range(len(input_directions)):
      D = sampled_direction = ai.fft_result(input_directions[i])

      if D.real < self.max_cell_input and sampled_direction.kval > kval_cutoff:
        #directional_show(D, "ddd %5d"%i)
        hemisphere_solutions.append(sampled_direction)

    if (hemisphere_solutions.size()<3):
      return hemisphere_solutions
    kvals = flex.double([
  hemisphere_solutions[x].kval for x in range(len(hemisphere_solutions))])

    perm = flex.sort_permutation(kvals,True)

    #  need to be more clever than just taking the top 30.
    #  one huge cluster around a strong basis direction could dominate the
    #  whole hemisphere map, preventing the discovery of three basis vectors

    perm_idx = 0
    unique_clusters = 0
    hemisphere_solutions_sort = flex.Direction()
    while perm_idx < len(perm) and \
          unique_clusters < size:
      test_item = hemisphere_solutions[perm[perm_idx]]
      direction_ok = True
      for list_item in hemisphere_solutions_sort:
        distance = math.sqrt(math.pow(list_item.dvec[0]-test_item.dvec[0],2) +
                     math.pow(list_item.dvec[1]-test_item.dvec[1],2) +
                     math.pow(list_item.dvec[2]-test_item.dvec[2],2)
                     )
        if distance < 0.087: #i.e., 5 degrees
           direction_ok=False
           break
      if direction_ok:
        unique_clusters+=1
        hemisphere_solutions_sort.append(test_item)
      perm_idx+=1

    return hemisphere_solutions_sort;

  def hemisphere(self,ai,max_cell,size = 30,cutoff_divisor = 4.):
    self.max_cell_input = max_cell
    if self.quick_grid >= self.incr: #trigger adaptive sampling
      quick_directions = SimpleSampler.construct_hemisphere_grid(
        self,self.quick_grid)
      quick_sampling = SimpleSampler.get_top_solutions(
        self,ai,quick_directions,
        self.target_size_for_quick_grid_answer_vector,
        cutoff_divisor = 10.)
      self.angles = SimpleSampler.construct_hemisphere_grid(
        self,self.incr)
      self.restrict_target_angles_to_quick_winners(quick_sampling)
      # now the hemisphere search will be restricted to the adaptive
      # sample rather than the entire hemisphere
    else:
      self.angles = SimpleSampler.construct_hemisphere_grid(
        self,self.incr)

    unrefined_basis_vectors = self.get_top_solutions(ai,self.angles,size,
      cutoff_divisor,grid=self.incr)

    #now refine them by the first method
    grid_refined_vectors = self.refine_top_solutions_by_grid_search(
      old_solutions = unrefined_basis_vectors, ai = ai)

    ai.setSolutions(grid_refined_vectors)

  def restrict_target_angles_to_quick_winners(self,quick_sampling):

    # find which target_grid directions are near neighbors to
    # the quick_sampling winners from the first round of testing.
    # in the second round, restrict the calculation of FFTs to those directions

    self.restricted = flex.Direction()
    target = flex.double()
    for iz in self.angles:
      # fairly inefficient in Python; expect big improvement in C++
      v = iz.dvec
      target.append(v[0]);target.append(v[1]);target.append(v[2]);
    kn = int(2 *self.second_round_sampling)

    #construct k-d tree for the reference set
    A = AnnAdaptor(data = target, dim = 3, k = kn)

    query = flex.double()
    for j in quick_sampling:
      v = j.dvec
      query.append(v[0]);query.append(v[1]); query.append(v[2]);
      if abs((math.pi/2.) - j.psi) < 0.0001: #take care of equatorial boundary
        query.append(-v[0]);query.append(-v[1]); query.append(-v[2]);

    A.query(query) #find nearest neighbors of query points
    neighbors = flex.sqrt(A.distances)
    neighborid = A.nn

    accept_flag = flex.bool(len(self.angles))
    for idx in range(len(neighbors)):
      # use small angle approximation to test if target is within desired radius
      if neighbors[idx] < self.quick_grid:
        accept_flag[neighborid[idx]] = True

    #go through all of the original target angles
    for iz in range(len(self.angles)):
      if accept_flag[iz]:
        self.restricted.append(self.angles[iz])

    self.angles = self.restricted

  def refine_top_solutions_by_grid_search(self, old_solutions, ai):
    new_solutions = flex.Direction()
    final_target_grid = self.characteristic_grid / 100.
    for item in old_solutions:
      new_solutions.append(ai.refine_direction(
        candidate = item,
        current_grid = self.incr,
        target_grid = final_target_grid))
    return new_solutions

def hemisphere_shortcut(ai,characteristic_sampling,max_cell):
    H = HemisphereSampler(
      max_grid = 0.029,
      characteristic_grid = characteristic_sampling,
      quick_grid = 0.016) # all grid parameters in radians
    H.hemisphere(ai,max_cell,size=30,cutoff_divisor=4.)
def hemisphere_refine_shortcut(ai,characteristic_sampling,unrefined):
    H = HemisphereSampler(
      max_grid = 0.029,
      characteristic_grid = characteristic_sampling,
      quick_grid = 0.016) # all grid parameters in radians
    cutoff_divisor=4.
    H.kval_cutoff = ai.getXyzSize()/cutoff_divisor;
    return H.refine_top_solutions_by_grid_search(unrefined,ai)


"""New adaptive algorithms to provide directional sampling of FFT's; July 2006.

1) The legacy algorithm is moved from the C++ function
   AutoIndexEngine::hemisphere() to the Python class SimpleSampler.  Moving
   to Python makes it simpler to rapidly prototype new methods.  Alogrithm
   provides uniform sampling over the hemisphere with a default spacing of
   0.029 radians as described in the Rossmann DPS paper.  An override
   "characteristic grid" is allowed in case the cell lengths are very large.

2) The Python implementation is somewhat slower than C++ (~20% factor) probably
   owing to the many Boost.Python interface calls.  It is the eventual
   intention to port everything back to C++ once the redesign is stable.

3) The new algorithms are embodied in the derived class HemisphereSampler,
   with the intention of balancing competing design requirements of fine
   sampling for large unit cells, and sparse enough sampling for ideal
   performance.  Diffraction cases are now divided into three categories,
   and are treated differently depending on the likely characteristic
   scale determined in the tnear2.py module.

   a) Problems with a characteristic scale > 0.029 radians (parameter max_grid)
      are still treated exactly according to Rossmann DPS, giving about 7900
      sampling directions.

   b) For problems with a characteristic scale < max_grid, the Simple sampling
      method is still used, but with a sampling fineness set to the
      characteristic scale; provided that the characteristic scale is still
      > 0.016 radians (parameter quick_grid).  At this limit there are about
      25000 sampling directions, using ~2 seconds of CPU when encoded in C++
      and when run on the fastest processors. Large-cell problems tend to take
      somewhat longer as the transforms are larger and have more data. This is
      the upper acceptable time elapsed for the sampling process.

   c) For still larger problems, a two-step process is employed. i) The
      limiting resolution of the dataset is degraded until the characteristic
      scale is quick_grid.  The Rossmann DPS sampling is then run on this
      data subset.  Promising direction vectors are selected.  ii) Now
      using the full dataset, the hemisphere is resampled at the original
      characteristic scale (for the full-dataset); but ONLY in areas immediately
      surrounding the promising directions from step i).  This two-step process
      is very efficient, and executes in <10 seconds even for the
      largest-cell problems examined.

4) Workaround for coarse gridding.  In the two-step sampling for large-cell
   problems, there is a danger that the quick_grid sampling of step i) will
   be too coarse. Coarse sampling may completely overlook a sharp peak
   coresponding to a true unit cell basis vector.  Two workarounds are employed.
   First, the cutoff for defining a direction vector of interest is relaxed
   in step i).  Instead of using kval > xyzdata.size()/4 (as in the legacy
   algorithm) we take kval > xyzdata.size()/10.  Secondly, we allow a very large
   number of directions of interest.  Instead of the 30 peaks used by Rossmann
   DPS, we allow up to several thousand.  The determining parameter is that
   when the interesting directions are finely sampled in step ii), we still want
   to keep the total number of FFTs under about 12500 (half of the quick_grid
   burden).  Therefore the quick_grid parameter ultimately limits the accessible
   problem size, but none of the crystallographic structures to date are limited
   by this implementation.

5) Workaround for unequal basis vector scores.  Even when sampled at a known
   basis vector direction, the kval score of the FFT is never 100% of the
   number of input data.  Sometimes the kval score for one or two of the
   basis vectors is significantly lower.  Moreover the FWHM can differ
   dramatically among the three basis vectors.  In extreme cases, one huge
   cluster around a strong basis direction can dominate the entire hemisphere
   map, precluding the discovery of the other two basis vectors.  In the
   Rossmann DPS paper it is stated simply that the top 30 sampled directions
   are taken, but this clearly needs to be modified in view of these results.
   Instead, we modify the algorithm to accept a maximum of 30 (parameter "size")
   unique clusters, with an unlimited number of directions within each cluster.
   By definition a cluster encompasses any top-scoring direction vector that
   is within 5-degrees of the first-identified (top-score) vector within that
   cluster.  This procedure gives the needed flexibility to find the difficult basis
   vectors.  After finding the top-scoring sampled directions, the kval scores
   are refined by a grid-search.  A legacy C++ function (AutoIndexEngine::
   setSolutions) then resorts the vectors, and picks the top vector in each
   cluster.  This is completely general since Niggli cell basis vectors are
   never within 5-degrees of each other.

6) Improvement in vector refinement.  In the legacy C++ procedure,
   AutoIndexEngine::hemisphere_refine_by_grid_search(), the directional vector
   is refined with nested grid searches until the granularity reaches 0.0001
   radians.  In the new Python procedure refine_top_solutions_by_grid_search(),
   the final granularity is customized to be (1/50) of the characteristic
   scale.  From the experience of publications/largecell1/fwhm.py, this
   granularity should be enough to correctly sample the kval peak, and in routine
   cases the value will be >0.0001 and thus more efficient.
"""


 *******************************************************************************


 *******************************************************************************
rstbx/indexing_api/test_dps.py
from __future__ import absolute_import, division, print_function

def test_dps_single_panel_labelit_input_optimal_S0(process_dictionary,data,phil_set):
    from rstbx.indexing_api.lattice import DPS_primitive_lattice
    from scitbx.matrix import col

    sample_to_beamspot = col((0.,0.,float(process_dictionary['distance'])))
    detector_d1 = col((1., 0., 0.))
    detector_d2 = col((0., 1., 0.))
    detector_origin = sample_to_beamspot - \
      detector_d1 * float(process_dictionary['xbeam']) - \
      detector_d2 * float(process_dictionary['ybeam'])
    assert detector_d1.length() == 1.0
    assert detector_d2.length() == 1.0
    assert detector_d1.dot(detector_d2) == 0.0

    beam_vector = sample_to_beamspot.normalize() * (
                  1./float(process_dictionary['wavelength']))
    rot_axis = col(process_dictionary["endstation"].rot_axi) - col((0.0,0.0,0.0)) # coerce to float type

    DPS = DPS_primitive_lattice(max_cell = float(process_dictionary['ref_maxcel']),
          recommended_grid_sampling_rad = process_dictionary['recommended_grid_sampling'],
          horizon_phil = phil_set)
    DPS.set_beam_vector(beam = -beam_vector)
    DPS.set_rotation_axis(axis = rot_axis)
    DPS.set_detector_position(origin = detector_origin, d1 = detector_d1, d2 = detector_d2)

    DPS.index(raw_spot_input = data)
    L = DPS.get_basis_general() # can skip this first time around
    new_S0_vector = DPS.optimize_S0_local_scope()

    DPS2= DPS_primitive_lattice(max_cell = float(process_dictionary['ref_maxcel']),
        recommended_grid_sampling_rad = process_dictionary['recommended_grid_sampling'],
        horizon_phil = phil_set)
    DPS2.set_beam_vector(beam = -new_S0_vector)
    DPS2.set_rotation_axis(axis = rot_axis)
    DPS2.set_detector_position(origin = detector_origin, d1 = detector_d1, d2 = detector_d2)
    DPS2.index(raw_spot_input = data)
    L = DPS2.get_basis_general()
    #new_S0_vector = DPS2.optimize_S0_local_scope() #can skip this second time around

def test_dps_single_panel_labelit_input_optimal_origin(process_dictionary,data,phil_set):
    from rstbx.indexing_api.lattice import DPS_primitive_lattice
    from scitbx.matrix import col

    sample_to_beamspot = col((0.,0.,float(process_dictionary['distance'])))
    detector_d1 = col((1., 0., 0.))
    detector_d2 = col((0., 1., 0.))
    detector_origin = sample_to_beamspot - \
      detector_d1 * float(process_dictionary['xbeam']) - \
      detector_d2 * float(process_dictionary['ybeam'])
    assert detector_d1.length() == 1.0
    assert detector_d2.length() == 1.0
    assert detector_d1.dot(detector_d2) == 0.0

    from dxtbx.model import DetectorFactory
    detector = DetectorFactory.make_detector(
      stype = "indexing",
      fast_axis = detector_d1,
      slow_axis = detector_d2,
      origin = detector_origin,
      pixel_size = (float(process_dictionary['pixel_size']),
                    float(process_dictionary['pixel_size'])),
      image_size = (int(process_dictionary['size1']),
                    int(process_dictionary['size2']))
      )

    beam_vector = sample_to_beamspot.normalize() * (
                  1./float(process_dictionary['wavelength']))
    rot_axis = col(process_dictionary["endstation"].rot_axi) - col((0.0,0.0,0.0)) # coerce to float type

    DPS = DPS_primitive_lattice(max_cell = float(process_dictionary['ref_maxcel']),
          recommended_grid_sampling_rad = process_dictionary['recommended_grid_sampling'],
          horizon_phil = phil_set)
    DPS.set_beam_vector(beam = -beam_vector)
    DPS.set_rotation_axis(axis = rot_axis)
    DPS.set_detector(detector)

    DPS.index(raw_spot_input = data)
    #L = DPS.get_basis_general() # can skip this first time around
    new_detector = DPS.optimize_origin_offset_local_scope()

    DPS2= DPS_primitive_lattice(max_cell = float(process_dictionary['ref_maxcel']),
        recommended_grid_sampling_rad = process_dictionary['recommended_grid_sampling'],
        horizon_phil = phil_set)
    DPS2.set_beam_vector(beam = -beam_vector)
    DPS2.set_rotation_axis(axis = rot_axis)
    DPS2.set_detector(new_detector)
    DPS2.index(raw_spot_input = data)
    L = DPS2.get_basis_general()

    #new_conforming_data = DPS2.get_outlier_rejected_subset()

def test_out(process_dictionary,data,phil_set):
    from rstbx.indexing_api.lattice import DPS_primitive_lattice
    from scitbx.matrix import col

    sample_to_beamspot = col((0.,0.,float(process_dictionary['distance'])))
    detector_d1 = col((1., 0., 0.))
    detector_d2 = col((0., 1., 0.))
    detector_origin = sample_to_beamspot - \
      detector_d1 * float(process_dictionary['xbeam']) - \
      detector_d2 * float(process_dictionary['ybeam'])
    assert detector_d1.length() == 1.0
    assert detector_d2.length() == 1.0
    assert detector_d1.dot(detector_d2) == 0.0

    beam_vector = sample_to_beamspot.normalize() * (
                  1./float(process_dictionary['wavelength']))
    rot_axis = col(process_dictionary["endstation"].rot_axi) - col((0.0,0.0,0.0)) # coerce to float type

    DPS = DPS_primitive_lattice(max_cell = float(process_dictionary['ref_maxcel']),
          recommended_grid_sampling_rad = process_dictionary['recommended_grid_sampling'],
          horizon_phil = phil_set)
    DPS.set_beam_vector(beam = -beam_vector)
    DPS.set_rotation_axis(axis = rot_axis)
    DPS.set_detector_position(origin = detector_origin, d1 = detector_d1, d2 = detector_d2)

    DPS.index(raw_spot_input = data)
    L = DPS.get_basis_general()


    new_detector = DPS.optimize_origin_offset_local_scope()

    DPS2= DPS_primitive_lattice(max_cell = float(process_dictionary['ref_maxcel']),
        recommended_grid_sampling_rad = process_dictionary['recommended_grid_sampling'],
        horizon_phil = phil_set)
    DPS2.set_beam_vector(beam = -beam_vector)
    DPS2.set_rotation_axis(axis = rot_axis)
    DPS2.set_detector(new_detector)
    DPS2.index(raw_spot_input = data)
    L = DPS2.get_basis_general()

    from rstbx.indexing_api.outlier_procedure import main_go
    main_go(index_engine=DPS2, phil_set=phil_set)

    print("Finishing")
    exit()

"""
Still to do:
1) Implement origin refinement instead of S0 (DONE)
2) Implement target cell (DONE)
3) Implement outlier rejection (in process)
4) Implement full-parameter refinement
5) Figure out how to evaluate the scoring function analytically & use 2nd-derivative LBFGS
6) Implement quick-refinement of direction vectors as in labelit

Nov. 4:
1) Refine the parameters--triclinic
2) Outlier rejection-essentially done; some refactoring needed.
2b) Rationalize iotbx converter.  Where is the .constrain() applied? Role for dials.crystal models vs. cctbx.crystal symmetry?
3) Refine again & return list of all settings with LABELIT-style output
4) Input can be either cctbx.spotfinder or dials.spotfinder
4b) Document the phil parameters that impinge on my API
5) json output and web-service
6) profile the code as to what is rate limiting
7) multipanel, refined as single block
8) support for Aaron's detector Format
9) multipanel with other parameterizations like quadrant & sensor
"""


 *******************************************************************************


 *******************************************************************************
rstbx/indexing_api/tools.py
# -*- coding: utf-8 -*-

from __future__ import absolute_import, division, print_function

import itertools

import scitbx.matrix
from rstbx.dps_core.cell_assessment import unit_cell_too_small
from rstbx.indexing_api import cpp_absence_test

def _is_collinear(x,y): # X x Y cross product is zero
  return x[0]*y[1]-x[1]*y[0]==x[1]*y[2]-x[2]*y[1]==x[2]*y[0]-x[0]*y[2]==0

def _is_coplanar(x,y,z):
  #triple product is zero; (X x Y).Z
  x = scitbx.matrix.row(x)
  y = scitbx.matrix.row(y)
  z = scitbx.matrix.row(z)
  return x.cross(y).dot(z)==0

def _generate_reindex_transformations():
    '''This implementation is based on the algorithm described in §2.5
    steps 1-3 of Sauter et al. (2004). J. Appl. Cryst. 37, 399-409.
    https://doi.org/10.1107/S0021889804005874

    The reindex transformations are specific for a particular
    presence condition, such as H + 2K + 3L = 5n.  The transformation
    is applied in reciprocal space, and is intended to change the
    original incorrect basis set a*',b*',c*' into the correct basis
    set a*,b*,c*. The meaning of the correction matrix A is as follows:

           a* = A00(a*') + A01(b*') + A02(c*')
           b* = A10(a*') + A11(b*') + A12(c*')
           c* = A20(a*') + A21(b*') + A22(c*')

    The choice of A is not unique, we use an algorithm to select a
    particular solution.  Briefly, for the first row of A choose the row
    vector HKL which satisfies the presence condition, and is shortest in
    length.  For the second row choose the next shortest allowed row vector
    that is not collinear with the first.  The third allowed row vector is
    the next shortest not coplanar with the first two.  We check to see
    that the determinant is positive (or switch first two rows) and of
    magnitude equal to the mod factor; this assures that the unit cell will
    be reduced in volume by the appropriate factor.

    Our approach sometimes backfires:  an already too-small unit cell can
    produce a positive absence test; the cell will then be reduced in volume
    indefinitely.  Therefore the application always uses a cell volume filter
    after making the correction.
    '''
    # modularities 2,3,5 were sufficient for every two-image case
    # need up to 11 for Fig 4 in the single-image indexing
    modularities = [2,3,5]

    mod_range = range(max(modularities), -max(modularities)-1,-1)
    points = itertools.product(mod_range, mod_range, mod_range)
    # sort by increasing distance and descending size
    spiral_order = list(sorted(points, key=lambda v: (sum(c*c for c in v), -sum(v))))
    spiral_order.remove((0,0,0))  # G0 in the paper (step 1)

    representatives = []  # G1 in the paper (step 2)
    # The vector representations connote systematic absence conditions.
    # For example, the vector v = (1,2,3) means H + 2K + 3L = ?n,
    # where the ? represents the modularity (2,3,5,...) specified elsewhere
    for vector in spiral_order:
      if sum(c*c for c in vector) > 6: break
      if any(_is_collinear(vector, item) for item in representatives): continue
      representatives.append(vector)

    # Now generate the matrices for every reflection condition (step 3)
    reindex = []
    for vec in representatives:
      for mod in modularities:
        candidate_points = (pt for pt in spiral_order if sum(v*p for v,p in zip(vec,pt))%mod == 0)
        # find three points that are not coplanar
        first = next(candidate_points)
        while True:
          second = next(candidate_points)
          if not _is_collinear(first, second):
            break
        while True:
          third = next(candidate_points)
          if not _is_coplanar(first, second, third):
            break
        A = scitbx.matrix.sqr(first + second + third)
        if A.determinant() < 0:
          A = scitbx.matrix.sqr(second + first + third)
        assert A.determinant() == mod
        reindex.append({'mod':mod, 'vec':vec, 'trans':A})
    return reindex

R = _generate_reindex_transformations()

class AbsenceHandler:
  def __init__(self):
    self.recursion_limit=8

  def absence_detected(self,hkllist):
    self.hkl = hkllist
    self.N   = self.hkl.size()
    self.flag = None

    for test in R:
        cum = cpp_absence_test(self.hkl,test['mod'],test['vec'])
        for counter in range(test['mod']):
          #print test['vec'],test['mod'],float(cum[counter])/self.N
          if float(cum[counter])/self.N > 0.8 and counter==0:
            # (if counter != 0 there is no obvious way to correct this)
            #print "Detected exclusive presence of %dH %dK %dL = %dn, remainder %d"%(
            #         test['vec'][0],test['vec'][1],test['vec'][2],test['mod'],counter)
            self.flag = {'vec':test['vec'],'mod':test['mod'],
                         'remainder':counter, 'trans':test['trans'].elems}
            return 1
    return 0

  def correct(self,orientation):
    if self.flag is None:
      raise RuntimeError("no correction necessary")
    M1 = scitbx.matrix.sqr(self.flag['trans'])
    corrected = orientation.change_basis(M1.transpose().elems)
    unit_cell_too_small(corrected.unit_cell(),cutoff = 100.)
    return corrected

  def list(self,hkllist):
      self.hkl = hkllist
      count = 0
      for m in self.hkl:
        print(count,m)
        print("                ",(m[0]%2,m[1]%2,m[2]%2), end=' ')
        print((m[0]%3,m[1]%3,m[2]%3), end=' ')
        print(((m[1]-m[2])%2,(m[1]+m[2])%2), end=' ')
        print(((m[2]-m[0])%2,(m[2]+m[0])%2), end=' ')
        print(((m[0]-m[1])%2,(m[0]+m[1])%2))
        count+=1

if __name__=='__main__':
  def pelem(arg):
    return arg.elems.__repr__()
  scitbx.matrix.sqr.__repr__ = pelem
  import pprint
  pprint.pprint( R)
  print(len(R))


 *******************************************************************************


 *******************************************************************************
rstbx/new_horizons/__init__.py


 *******************************************************************************


 *******************************************************************************
rstbx/new_horizons/index.py
from __future__ import absolute_import, division, print_function
from labelit.command_line.stats_index import best_character_to_IndexPrinter
from labelit.command_line.default_param import establish_dict_for_refinement
from labelit.dptbx.autoindex import index_and_refine

class new_horizons_state:
  def __init__(self,horizons_phil,args):
    self.last_saved_best = None
    self.horizons_phil = horizons_phil
    if self.horizons_phil.spotfinder=="distl":
      print("Importing DISTL")
      from labelit.command_line.stats_index import AutoIndexOrganizer
    elif self.horizons_phil.spotfinder=="speck":
      print("Importing SPECK")
      from rstbx.new_horizons.stats_index import AutoIndexOrganizer

    self.organizer = AutoIndexOrganizer(
      verbose = self.horizons_phil.distl.bins.verbose,
      argument_module = args,
      horizons_phil = horizons_phil,
      delegate = self.index_and_integrate)
  def process(self):
    self.organizer.process()
    self.spotfinder_results = self.organizer.S
    self.frames = self.organizer.frames

  def index_and_integrate(self,frames,files,spotfinder_results):
    self.pd = establish_dict_for_refinement(frames,spotfinder_results)
    self.spotfinder_results = spotfinder_results
    self.frames = frames
    #------------------------------------------------------------
    ai,P = index_and_refine(pd = self.pd,
                            rawframes = files,
                            spotfinder_results = spotfinder_results,
                            verbose = False,
                            horizon_phil = self.horizons_phil)
    self.indexing_ai = ai
    #------------------------------------------------------------
    if self.horizons_phil.compatibility_allow==False:
      M = best_character_to_IndexPrinter(ai,P,self.pd,True,self.horizons_phil)
    else:
      from labelit.diffraction.compatibility import best_compatibility_to_IndexPrinter
      M = best_compatibility_to_IndexPrinter(ai,P,self.pd,files,
          spotfinder_results)
    #------------------------------------------------------------
    if "writer" in self.horizons_phil.__dict__:
      self.horizons_phil.writer.make_image_plots_detail(
        ai=ai,pd=self.pd,inframes=files,spotfinder_results=spotfinder_results)

    if not self.horizons_phil.index_only:
     if 0:
      from labelit.dps import IntegrateCharacters
      IC = IntegrateCharacters(M,self.pd)
      IC.write_mosflm_matrices()
      IC.find_best()
      IC.show()
     if 1:
      from rstbx.new_horizons.oscillation_shots import IntegrateCharacters
      IC = IntegrateCharacters(M,self.pd,self.horizons_phil,files,
        spotfinder_results)
      IC.find_best()
      self.last_saved_best = IC.save_best()
      IC.show()
    # Lattice character information added for special UCSF/ LLNL project (5/2014):
    self.pd["lattice_characters"]=M.best()
    for item in self.pd["lattice_characters"]: del item["minimizer"]
    return self.pd

def pre_indexing_validation(horizons_phil):
  if horizons_phil.indexing.data!=[]:
    horizons_phil.wedgelimit=len(horizons_phil.indexing.data)
    # for abutting or near-abutting images it is still necessary to set
    # codecamp.maxcell

def pack_names(horizons_phil):
  class Empty:pass
  E = Empty()
  E.argv=['Empty']
  if not isinstance(horizons_phil.indexing.data, list):
    return pack_dictionary(horizons_phil)
  for x in horizons_phil.indexing.data:
    E.argv.append(x)
  return E

def pack_dictionary(info):
  #print "special interface; CXI data passed in as dictionary"
  class Empty:pass
  E = Empty()
  E.argv=['Empty','data_in_object']
  E.data = info.indexing.data
  return E

def run_index(horizons_phil):
  pre_indexing_validation(horizons_phil)
  imagefile_arguments = pack_names(horizons_phil)
  S = new_horizons_state(horizons_phil,imagefile_arguments)
  S.process()
  return S


 *******************************************************************************


 *******************************************************************************
rstbx/new_horizons/oscillation_shots.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import math,os
from six.moves import cStringIO as StringIO
from six.moves import cPickle as pickle
from labelit.dptbx.status import cellstr
from libtbx.utils import Sorry
from libtbx.test_utils import approx_equal
from rstbx.dials_core.integration_core import show_observations

class IntegrateCharacters:
  def __init__(self,Characters,process_dictionary,horizons_phil,files,spotfinder_results):
    self.M = Characters
    self.process_dictionary = process_dictionary
    self.horizons_phil = horizons_phil
    self.files = files
    self.spotfinder_results = spotfinder_results
    self.triclinic = self.M.best()[-1]
    fres = ResLimitControl(self.process_dictionary,self.horizons_phil)

    self.triclinic['integration'] = self.integrate_one_character(
      setting=self.triclinic,
      integration_limit=fres.current_limit)
    #return # Enforces legacy behavior--no recycling to expand the integration limit
            # Comment this "return" in for testing without the macrocycle
    #With appropriate safeguards, macrocycle gives better resolution estimate:
    A = ResolutionAnalysisMetaClass(self.triclinic['integration'], self.horizons_phil)
    print(A)
    safety_counter = 2
    while A.retest_required() and safety_counter > 0 and \
          A.target_resol() < fres.current_limit:
      safety_counter -= 1
      tolerance = 2.E-2
      if (not approx_equal(A.target_resol(),fres.outer_limit,
             eps=tolerance,out=None)) and A.target_resol() < fres.outer_limit:
        break
      fres.current_limit = A.target_resol()

      trial = self.integrate_one_character(
        setting = self.triclinic,
        integration_limit = fres.current_limit)

      results = trial["results"]
      obs = [item.get_obs(trial["spacegroup"]) for item in results]
      A.stats_mtz(trial,obs)
      #print A
    if 'trial' in vars().keys(): self.triclinic['integration'] = trial

  def integrate_one_character(self,setting,integration_limit):
    #from libtbx.development.timers import Profiler
    #P = Profiler("Preliminary")
    import copy
    local = copy.deepcopy(self.process_dictionary)
    local['cell']=cellstr(setting)

    print("Cell in setting",setting["counter"],local["cell"])

    frames = list(sorted(self.spotfinder_results.pd['osc_start'].keys()))

    local['maxcel']='0'
    local['xbeam']="%f"%setting['minimizer'].new['xbeam']
    local['ybeam']="%f"%setting['minimizer'].new['ybeam']
    local['distance']="%f"%setting['minimizer'].new['distance']
    local["resolution"]= "%f"%integration_limit

    from labelit.steps import primaries
    local['spacegroup'] = primaries[setting['bravais']]

    local['procstart'] = local['procend'] = "%d"%frames[0]

    self.pixel_size = float(local['pixel_size'])

    from labelit.dptbx import AutoIndexEngine, Parameters
    ai = AutoIndexEngine(local['endstation'])

    P = Parameters(xbeam=setting["refined x beam"],ybeam=setting["refined y beam"],
             distance=setting["refined distance"],twotheta=float(local["twotheta"]))
    ai.setBase(P)
    ai.setWavelength(float(local['wavelength']))
    ai.setMaxcell(float(local['ref_maxcel']))
    print("Deltaphi is",float(local['deltaphi']))
    ai.setDeltaphi(float(local['deltaphi'])*math.pi/180.)
    ai.setMosaicity(setting["mosaicity"])
    ai.setOrientation(setting["orient"])
    refimage = self.files.images[0]
    ai.set_active_areas(self.horizons_phil,
                        beam=(int(refimage.beamx/refimage.pixel_size),
                              int(refimage.beamy/refimage.pixel_size)))

    image_centers = [(math.pi/180.)*float(x) for x in local["osc_start"].values()]

    print("Limiting resolution",integration_limit)
    local["results"] = []
    for i in range(len(frames)):
      print("---------BEGIN Integrate one frame %d %s" % \
          (frames[i], os.path.split(self.files.filenames()[i])[-1]))
      #P = Profiler("worker")
      if self.horizons_phil.integration.combine_sym_constraints_and_3D_target and setting["counter"]>1:
        from rstbx.apps.stills.dials_refinement_preceding_integration import integrate_one_frame
        integrate_worker = integrate_one_frame(self.triclinic["integration"]["results"][0])
      else:
        from rstbx.apps.stills.deltapsi_refinement_preceding_integration import integrate_one_frame
        integrate_worker = integrate_one_frame()
      integrate_worker.inputai = ai

      integrate_worker.inputpd = dict(masks=local["masks"],
                                      size1=local["size1"],
                                      size2=local["size2"],
                                      symmetry=setting["best_subsym"])
        # carefully select only the data items needed for integrate_worker
        # avoid giving the whole process dictionary; reference to "local"
        # is a circular reference creating memory leak, while copying the
        # whole thing is a big performance hit.
      integrate_worker.frame_numbers = frames
      integrate_worker.imagefiles = self.files
      integrate_worker.spotfinder = self.spotfinder_results
      integrate_worker.image_centers = image_centers

      integrate_worker.limiting_resolution = integration_limit
      integrate_worker.setting_id = setting["counter"]
      integrate_worker.pixel_size = self.pixel_size
      integrate_worker.set_pixel_size(self.pixel_size)
      integrate_worker.set_detector_size(int(local["size1"]),int(local["size2"]))

      integrate_worker.set_detector_saturation(refimage.saturation)
      integrate_worker.set_up_mask_focus()
      integrate_worker.initialize_increments(i)
      integrate_worker.horizons_phil = self.horizons_phil
      if self.horizons_phil.indexing.verbose_cv:
        print("EFFECTIVE TILING"," ".join(
          ["%d"%z for z in refimage.get_tile_manager(self.horizons_phil).effective_tiling_as_flex_int()]))
      integrate_worker.integration_concept(image_number = i,
        cb_op_to_primitive = setting["cb_op_inp_best"].inverse(),
        verbose_cv = self.horizons_phil.indexing.verbose_cv,
        background_factor = self.horizons_phil.integration.background_factor,
        )
      #P = Profiler("proper")
      integrate_worker.integration_proper()
      local["results"].append(integrate_worker)
      local["r_xbeam"]=ai.xbeam()
      local["r_ybeam"]=ai.ybeam()
      local["r_distance"]=ai.distance()
      local["r_wavelength"]=ai.wavelength
      local["r_residual"]=integrate_worker.r_residual
      local["r_mosaicity"]=setting["mosaicity"]
      try:
        local["ewald_proximal_volume"]=integrate_worker.ewald_proximal_volume
      except Exception as e:
        local["ewald_proximal_volume"]=None

      if (self.horizons_phil.indexing.open_wx_viewer):
       if True: #use updated slip viewer
        try:
          import wx
          from rstbx.slip_viewer.frame import XrayFrame as SlipXrayFrame
          from rstbx.command_line.slip_viewer import master_str as slip_params
          from iotbx import phil
          from spotfinder import phil_str
          from spotfinder.command_line.signal_strength import additional_spotfinder_phil_defs

          work_phil = phil.process_command_line("",master_string=slip_params + phil_str + additional_spotfinder_phil_defs)
          work_params = work_phil.work.extract()

          app = wx.App(0)
          wx.SystemOptions.SetOption("osx.openfiledialog.always-show-types", "1")
          frame = SlipXrayFrame(None, -1, "X-ray image display", size=(800,720))
          frame.Show()

          # Update initial settings with values from the command line.  Needs
          # to be done before image is loaded (but after the frame is
          # instantiated).
          frame.inherited_params = integrate_worker.horizons_phil
          frame.params = work_params

          if (frame.pyslip is None):
            frame.init_pyslip()
          if (frame.settings_frame is None):
            frame.OnShowSettings(None)
          frame.Layout()

          frame.pyslip.tiles.user_requests_antialiasing = work_params.anti_aliasing
          frame.settings_frame.panel.center_ctrl.SetValue(True)
          frame.settings_frame.panel.integ_ctrl.SetValue(True)
          frame.settings_frame.panel.spots_ctrl.SetValue(False)
          frame.settings.show_effective_tiling = work_params.show_effective_tiling
          frame.settings_frame.panel.collect_values()
          paths = work_phil.remaining_args

          frame.user_callback = integrate_worker.slip_callback
          frame.load_image(self.files.filenames()[i])

          app.MainLoop()
          del app
        except Exception:
          pass # must use phenix.wxpython for wx display

       elif False : #original wx viewer
        try:
          from rstbx.viewer.frame import XrayFrame
          import wx
          from rstbx.viewer import display
          display.user_callback = integrate_worker.user_callback

          app = wx.App(0)
          frame = XrayFrame(None, -1, "X-ray image display", size=(1200,1080))
          frame.settings.show_spotfinder_spots = False
          frame.settings.show_integration = False
          #frame.settings.enable_collect_values = False
          frame.SetSize((1024,780))
          frame.load_image(self.files.filenames()[i])
          frame.Show()
          app.MainLoop()
          del app
        except Exception:
          pass # must use phenix.wxpython for wx display

      # for the wx image viewer
      filename = self.horizons_phil.indexing.indexing_pickle
      if filename != None:
        filename = "%s_%d_%d.pkl"%(filename,setting["counter"],keys[i])

        SIO = StringIO()
        table_raw = show_observations(integrate_worker.get_obs(
          local["spacegroup"]),out=SIO)
        limitobject = ResolutionAnalysisMetaClass(local, self.horizons_phil)
        info = dict(table = SIO.getvalue(),
          table_raw = table_raw,
          xbeam = setting["refined x beam"],
          ybeam = setting["refined y beam"],
          distance = setting["refined distance"],
          residual = integrate_worker.r_residual,
          resolution = limitobject.value, # FIXME not reliable?
          mosaicity = setting["mosaicity"],
          pointgroup = local["spacegroup"],
          hkllist = integrate_worker.hkllist,
          predictions = (1./integrate_worker.pixel_size)*integrate_worker.predicted,
          mapped_predictions = integrate_worker.detector_xy,
          integration_masks_xy = integrate_worker.integration_masks_as_xy_tuples(),
          background_masks_xy = integrate_worker.background_masks_as_xy_tuples()
        )
        assert info["predictions"].size() >= info["mapped_predictions"].size()
        assert info["predictions"].size() == info["hkllist"].size()
        G = open(filename,"wb")
        pickle.dump(info,G,pickle.HIGHEST_PROTOCOL)
      print("---------END Integrate one frame",frames[i])

    return local

  def find_best(self):
    self.best_counter = 1

    for index in self.M.best()[0:len(self.M.best())-1]:
      if 'status' in index and index['status'] in [
        'unlikely','very_unlikely']:continue
      if float(self.triclinic['integration']['resolution'])==0.0:
        raise Sorry("No signal detected in triclinic integration trial")

      if self.horizons_phil.known_setting == index['counter'] and \
         self.horizons_phil.integration.montecarlo_integration_limit is not None:
        index['integration'] = self.integrate_one_character(
        setting=index,
        integration_limit=self.horizons_phil.integration.montecarlo_integration_limit)
      elif self.horizons_phil.integration.greedy_integration_limit:
        index['integration'] = self.integrate_one_character(
        setting=index,
        integration_limit=float(self.triclinic['integration']['results'][0].limiting_resolution))
      else:
        index['integration'] = self.integrate_one_character(
        setting=index,
        integration_limit=float(self.triclinic['integration']['resolution']))

      A = ResolutionAnalysisMetaClass(index['integration'],self.horizons_phil)
      print(A)
      if (self.horizons_phil.known_cell!=None or
         self.horizons_phil.known_symmetry!=None):
        self.best_counter = index['counter']
        break

      if ( float(index['integration']['r_residual']) <
           self.horizons_phil.mosflm_rmsd_tolerance *
           float(self.triclinic['integration']['r_residual']) ):
        self.best_counter = index['counter']
        break

  def save_best(self):
    file = self.horizons_phil.indexing.completeness_pickle
    for index in self.M.best():
      if 'integration' in index:
        if index['counter']==self.best_counter:
          local = index["integration"]
          info = dict(
            xbeam = local["r_xbeam"],
            ybeam = local["r_ybeam"],
            distance = local["r_distance"],
            wavelength = float(local["r_wavelength"]),
            residual = local["r_residual"],
            mosaicity = local["r_mosaicity"],
            pointgroup = local["spacegroup"],
            observations = [a.get_obs(local["spacegroup"]) for a in local["results"]],
            mapped_predictions = [a.detector_xy for a in local["results"]],
            model_partialities = [getattr(a,"partialities",None) for a in local["results"]],
            sa_parameters = [getattr(a,"best_params","None") for a in local["results"]],
            max_signal = [getattr(a,"max_signal",None) for a in local["results"]],
            current_orientation = [getattr(a,"current_orientation",None) for a in local["results"]],
            current_cb_op_to_primitive = [getattr(a,"current_cb_op_to_primitive",None) for a in local["results"]],
            correction_vectors = [getattr(a, 'correction_vectors', None)
                                  for a in local['results']],
            effective_tiling = self.files.images[0].get_tile_manager(
              self.horizons_phil).effective_tiling_as_flex_int()
          )
          for correction_type in self.horizons_phil.integration.absorption_correction:
            if correction_type.apply and correction_type.algorithm=="fuller_kapton":
              info['fuller_kapton_absorption_correction'] = [a.fuller_kapton_absorption_correction for a in local["results"]]
          if self.horizons_phil.integration.model=="user_supplied":
            info['ML_half_mosaicity_deg'] = [getattr(a,"ML_half_mosaicity_deg",0) for a in local["results"]]
            info['ML_domain_size_ang'] = [getattr(a,"ML_domain_size_ang",0) for a in local["results"]]
            info['ewald_proximal_volume'] = [getattr(a,"ewald_proximal_volume",0) for a in local["results"]]
          info["identified_isoform"] = local["results"][0].__dict__.get("identified_isoform",None)
          if file is not None:
            G = open(file,"wb")
            pickle.dump(info,G,pickle.HIGHEST_PROTOCOL)
          return info

  def show(self):
    print()
    print("New Horizons Integration results:")
    print("Solution  SpaceGroup Beam x   y  distance  Resolution Mosaicity RMS")
    for index in self.M.best():
      if 'integration' in index:
        limitobject = ResolutionAnalysisMetaClass( index['integration'], self.horizons_phil )
        if index['counter']==self.best_counter:
          print(":)", end=' ')
          self.process_dictionary['best_integration']=index
        else: print("  ", end=' ')
        # only write out the triclinic integration results if there is
        #  an application for the data--future expansion
        if index['counter']==1 and len(self.M.best())>1:
          self.process_dictionary['triclinic']=index
        print("%3d"%index['counter'], end=' ')
        print("%12s"%index['integration']["spacegroup"], end=' ')
        print("%6.2f %6.2f"%(float(index['integration']["r_xbeam"]),
                             float(index['integration']["r_ybeam"])), end=' ')
        print("%7.2f   "%(float(index['integration']["r_distance"])), end=' ')
        if limitobject.value == 0.00:
          #Analysis of mtz file gives no resolution estimate; revert to limit detected by spotpicking
          index['integration']['r_resolution'] = float(self.process_dictionary["resolution_inspection"])
        else: index['integration']['r_resolution'] = limitobject.value
        print("%7.2f   "%index['integration']['r_resolution'], end=' ')
        print(index['integration']["r_mosaicity"], end=' ')
        print("  ", end=' ')
        print("%5.3f"%index['integration']["r_residual"])

        continue # the following code merely demonstrates the unpacking of partiality info
        if hasattr(index["integration"]["results"][0],"partialities"):
          hackobs = index["integration"]["results"][0].get_obs(index["integration"]["spacegroup"])
          hackpart = index["integration"]["results"][0].partialities["data"]
          hackhkl = list(index["integration"]["results"][0].partialities["indices"])
          from scitbx.array_family import flex
          xx = flex.double()
          yy = flex.double()
          for idx in range(hackobs.indices().size()):
            hkl = hackobs.indices()[idx]
            thisobs = hackobs.data()[idx]
            lookupidx = hackhkl.index(hkl)
            print(hkl,thisobs,hackhkl[lookupidx],hackpart[lookupidx])
            if thisobs>0.:
              resolution = hackobs.unit_cell().d(hkl)
              if resolution > 2.5 and resolution < 4.0:
                # correlation between partiality & Iobs only within resolution shells
                xx.append(math.log(thisobs))
                yy.append(hackpart[lookupidx])
          from matplotlib import pyplot as plt
          plt.plot(xx,yy,"r.")
          plt.show()


    #Sublattice analysis
    for index in self.M.best():
      if index['counter']==1 and 'integration' in index:
        results = index['integration']["results"]
        obs = [item.get_obs(index['integration']["spacegroup"]) for item in results]
        try:
          get_limits(params = index['integration'],
                   file = obs,
                   verbose = False,
                   sublattice_flag = True,
                   override_maximum_bins = 12,
                   horizons_phil = self.horizons_phil)
        except: # intentional
          #Numpy multiarray.error raises an object not derived from Exception
          print("Catch any problem with sublattice analysis & numpy masked arrays")
          return

class limits_fix_engine:
  def __init__(self):
    pass
  def rawprocess(self,first_pass):
    self.xbeam = float(first_pass.get('labelit_x',first_pass["xbeam"]))
    self.ybeam = float(first_pass.get('labelit_y',first_pass["ybeam"]))
    self.px = float(first_pass["pixel_size"])
    self.sz1 = float(first_pass.get("size1"))
    self.sz2 = float(first_pass.get("size2"))

  def corners(self):
    #coordinates of the four detector corners relative to the beam (mm)
    return ((-self.xbeam,                  -self.ybeam),
            (-self.xbeam,                  self.px*self.sz2-self.ybeam),
            (self.px*self.sz1-self.xbeam,-self.ybeam),
            (self.px*self.sz1-self.xbeam,self.px*self.sz2-self.ybeam))
  def edges(self):
    #coordinates of the four detector edges relative to the beam (mm)
    return ((-self.xbeam+(self.px*self.sz1/2.)  ,-self.ybeam),
            (-self.xbeam,                         (self.px*self.sz2/2.) - self.ybeam),
            (self.px*self.sz1-self.xbeam,       (self.px*self.sz2/2.)-self.ybeam),
            ((self.px*self.sz1/2.) - self.xbeam,self.px*self.sz2-self.ybeam))

class SafetyLimits:
  def __init__(self,INFO,AI):
    self.INFO = INFO
    self.AI = AI

  def determined_limit_from_screen(self):
    try:
      screen = float(self.INFO['best_integration']['integration']['r_resolution'])
    except Exception:
      # no previous integration success.  Rely on DISTL results.
      screen = float(self.INFO['resolution'])
    return screen

  def safety_limit(self,algorithm="corner"):
    epsilon = 0.0001
    # Originally, the limit at edge of detector on a MarCCD, otherwise lambda/2.
    # But due to comments from Ana Gonzalez on problems with MOSFLM integration
    # out to the corner (Gordon Conference, 2006), a) a limit will be placed
    # on all detectors based on detector geometry, and b) it will be a
    # configurable choice whether the limit is based on the farthest corner
    # or the farthest edge.
    XE = limits_fix_engine()
    XE.rawprocess(self.INFO)
    from labelit.mathsupport import length
    if algorithm=="edge":
      mpoint = max([length(point) for point in XE.edges()])
    elif algorithm=="corner":
      mpoint = max([length(point) for point in XE.corners()])
    theta = math.atan2(mpoint,self.AI.distance())/2.0
    return epsilon + self.AI.wavelength/(2.0*math.sin(theta))

  def determined_limit_with_safety(self):
    return max(self.determined_limit_from_screen(), self.safety_limit() )

class ResLimitControl:
  def __init__(self,pd,horizons_phil):
    #initialVolumeFactor = 1.8 # Legacy value=1.5 prior to 7/17/07
    initialVolumeFactor = horizons_phil.integration.initial_volume_factor
    self.subsequentVolumeFactor = 1.5 # this value not used by stats_mtz; see stats_mtz code
    if horizons_phil.mosflm_integration_reslimit_override!=None:
      self.initial_limit = horizons_phil.mosflm_integration_reslimit_override
      self.outer_limit = self.initial_limit
    else:
      #resolution_inspection from DISTL is a conservative estimate
      #increase the reciprocal space volume by factor of 1.5 for trial integration
      from math import pow
      trial_reslimit = float(pd["resolution_inspection"])/pow(initialVolumeFactor,1.0/3.0)

      from labelit.dptbx import AutoIndexEngine,Parameters
      ai = AutoIndexEngine(pd['endstation'],
           horizons_phil.model_refinement_minimum_N) #Just a container for a few parameters
      base = Parameters(xbeam = float(pd['xbeam']), ybeam = float(pd['ybeam']),
                        distance = float(pd['distance']), twotheta = 0.0 )
      ai.setBase(base)
      ai.setWavelength(float(pd['wavelength']))

      safety = SafetyLimits(pd,ai).safety_limit(
        algorithm = horizons_phil.mosflm_safety_algorithm)

      self.outer_limit = safety
      self.initial_limit = max((trial_reslimit,safety))
      """synopsis:
      LABELIT integrates to determine the best-estimate of
      dataset resolution.  After integration out to the image corner, an
      intensity log-plot is used to linearly extrapolate out to an I/sigma
      of 0.75, based on all partials and fulls.  Ana Gonzalez requested that
      this be changed to the image edge, because of MOSFLM problems in cases
      where the diffraction is of low quality or low resolution.  Unfortunately,
      using a cutoff at the edge is counter-productive in cases where diffraction
      extends past the corners.  In those cases, the log-linear extrapolation
      invariably leads to a resolution estimate that is more optimistic (lower
      Angstrom cutoff) than when all of the data is used out to the image corner.
      Edge cutoff is therefore not recommended.  However, it is now available as
      a configurable choice using the command line argument:
      mosflm_safety_algorithm=[corner|edge]  (default = corner).
      #print pd['file'][pd['file'].keys()[0]]
      #print "corner: %.2f"%(max((trial_reslimit,interface.SafetyLimits(pd,ai).safety_limit(algorithm="corner"))))
      #print "  edge: %.2f"%(max((trial_reslimit,interface.SafetyLimits(pd,ai).safety_limit(algorithm="edge"))))
      """
    self.current_limit = self.initial_limit
  def show(self):
    print("initial resolution limit",self.initial_limit)
    print("outer resolution limit",self.outer_limit)
  def limits(self):
    yield self.current_limit
    while True:
      self.current_limit /=  pow(self.subsequentVolumeFactor,1.0/3.0)
      if self.current_limit <= self.outer_limit: break
      yield self.current_limit

from labelit.diffraction.stats_mtz import get_limits
class ResolutionAnalysisMetaClass(get_limits):
  def __init__(self,integration_dict,horizons_phil,verbose=False):
    self.integration_dict = integration_dict
    self.horizons_phil = horizons_phil
    results = self.integration_dict["results"]
    obs = [item.get_obs(self.integration_dict["spacegroup"]) for item in results]
    if verbose:
      for item in results:
        show_observations(item.get_obs(self.integration_dict["spacegroup"]))
    self.stats_mtz(self.integration_dict,obs)
    self.integration_dict["resolution"] = self.value

  def stats_mtz(self,integration_dict,file):
    try:
      get_limits.__init__(self,params=integration_dict,file=file,horizons_phil=self.horizons_phil)
    except: # intentional
      #Numpy multiarray.error raises an object not derived from Exception
      print("Catch any problem with stats mtz & numpy masked arrays")

  def retest_required(self):
    return self.status.require_expanded_limit != None

  def target_resol(self):
    tmp = self.status.require_expanded_limit.split(" ")
    return float(tmp[3])

  def target_Isig(self):
    tmp = self.status.require_expanded_limit.split(" ")
    return float(tmp[9])

  def __getattr__(self,key):
    if key=='value':
      return self.status.value
    else:
      return self.__dict__[key]


 *******************************************************************************


 *******************************************************************************
rstbx/new_horizons/pixel_spread.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import math
from scitbx.array_family import flex
from scitbx.matrix import col

switch = 0.12366
upper_cutoff = 0.5 - switch
xupper_cutoff = 0.5 + switch
lower_cutoff = -0.5 + switch # units of pixels
xlower_cutoff = -0.5 - switch # units of pixels
"""Above formulae reflect the line-spread results given in Koerner et al (2009)
Journal of Instrumentation 4: P03001.  Therefore this code is valid for the
CS-PAD, only."""

def line_spread_response_1d(x):
    if x > xupper_cutoff:
      return 0.
    elif x > upper_cutoff:
      return 1.+ math.cos(((x - upper_cutoff)/switch)*math.pi/2.)
    elif x < xlower_cutoff:
      return 0.
    elif x < lower_cutoff:
      return 1.+ math.cos(((x - lower_cutoff)/switch)*math.pi/2.)
    else:
      return 2.
def plot_line_spread_response_1d():
  xs = flex.double()
  ys = flex.double()
  for a in range(-15,16,1):
    x = 0.1 * a
    y = line_spread_response_1d(x)
    xs.append(x)
    ys.append(y)
  ys /= flex.sum(ys)
  from matplotlib import pyplot as plt
  plt.plot(xs,ys,"r.")
  plt.show()
def project_2d_response_onto_line(vector):
  xs = flex.double()
  ys = flex.double()
  axis2 = vector.rotate_2d(angle=90.,deg=True)
  #print vector.elems,axis2.elems
  for a in range(-15,16,1):
    ax = 0.1 * a
    xs.append(ax)
    sumx = 0.0
    for b in range(-15,16,1):
      bx = 0.1 * b
      #convert these rotating-frame coords to stationary frame
      stat = ax*vector + bx*axis2
      #print stat.elems
      sumx += line_spread_response_1d(stat[0]) * line_spread_response_1d(stat[1])
    ys.append(sumx)
  ys /= flex.sum(ys)
  return xs,ys

"""basic idea:  have a collection of bodypixels and intensities. Project each bodypixel
onto the projection direction of interest, either radial or azimuthal.  Convolute
each pixel value intensity with the projection of the line-spread-response function,
to get an observed spot projection.  Take the full-width-half-max of this,
then deconvolute with the point spread, essentially subtract one,
thus giving the FWHM of the diffracted rays, in units of pixels."""

class fwhm_2d_response:

  def __init__(self,rawdata,projection_vector,spotfinder_spot,verbose=False):
      # projection vector is either the radial or azimuthal unit vector
      #   at a specific Bragg spot position
      model_center = col((spotfinder_spot.ctr_mass_x(),spotfinder_spot.ctr_mass_y()))

      px_x,px_y = project_2d_response_onto_line(projection_vector)

      point_projections = flex.double()
      pixel_values = flex.double()
      for point in spotfinder_spot.bodypixels:
        point_projection = (col((point.x,point.y)) - model_center).dot( projection_vector )
        point_projections.append(point_projection)
        pxval = rawdata[(point.x,point.y)]
        if verbose:
          print("point_projection",point_projection, end=' ')
          print("signal",pxval)
        pixel_values.append(  pxval  )
      Lmin = flex.min(point_projections)
      Lmax = flex.max(point_projections)
      #print "Range %6.2f"%(Lmax-Lmin)
      Rmin = round(Lmin-2.0,1)
      Rmax = round(Lmax+2.0,1)
      #print "Range %6.2f"%(Rmax-Rmin)
      def histogram_bin(j) : return int(10.*(j-Rmin)) # bin units of 1/10 pixel

      histo_x = flex.double((int(10*(Rmax-Rmin))))
      histo_y = flex.double(len(histo_x))
      for ihis in range(len(histo_x)): histo_x[ihis] = Rmin + 0.1*ihis
      for ipp, point_projection in enumerate(point_projections):
        value = pixel_values[ipp]
        for isample in range(len(px_x)):
          histo_y[int(10*(point_projection + px_x[isample] - Rmin))] += value * px_y[isample]
      self.histo_x = histo_x
      self.histo_y = histo_y

  def show_plot(self):
      from matplotlib import pyplot as plt
      plt.plot(self.histo_x,self.histo_y,"r.")
      plt.show()
      del plt

  def fwhm_pix(self):
      half_max = flex.max(self.histo_y) / 2.

      min_idx = 0
      for x in range(len(self.histo_y)):
        if self.histo_y[x] > half_max: min_idx = x; break
      max_idx = len(self.histo_y)
      for x in range(len(self.histo_y)-1,0,-1):
        if self.histo_y[x] > half_max: max_idx = x; break
      #min_idx and max_idx give FWHM to 0.1 pixel as already constructed
      #but use simple linear interpolation to get FWHM to better than 0.1 pixel.
      lower_bound = min_idx - (self.histo_y[min_idx]-half_max)/(
                          self.histo_y[min_idx] - self.histo_y[min_idx-1])
      upper_bound = max_idx + (self.histo_y[max_idx]-half_max)/(
                          self.histo_y[max_idx] - self.histo_y[max_idx+1])
      potential_fwhm = 0.1 * (upper_bound - lower_bound) - 1.0
      return max(0.,potential_fwhm)

if __name__=="__main__":
  from scitbx.matrix import col
  starting_vec = col((1.0,0.,))
  for angle in range(0,360,5):
    f_angle = float(angle)
    projection_vec = starting_vec.rotate_2d(angle=f_angle,deg=True)
    print(f_angle,projection_vec.elems)
    xs, ys = project_2d_response_onto_line(projection_vec)
    from matplotlib import pyplot as plt
    plt.plot(xs,ys,"r.")
    plt.show()


 *******************************************************************************


 *******************************************************************************
rstbx/new_horizons/speckfinder.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import math
from scitbx.array_family import flex
from libtbx import adopt_init_args
from six.moves import zip

# alternate implementation of spotfinder; use the idea of identifying
# bright pixels on the inner 32 asics. (intensities > 2.0 * 90th percentile)
# Then cluster the brights into spots and use for autoindexing.
# Filter out any spots with area > 20 pixels...since this is a PAD with LCLS beam.

class speckfinder:

  def get_active_data_percentile(self):
    data = self.imgobj.linearintdata
    indexing = []
    for asic in self.corners:
      block = data.matrix_copy_block(
          i_row=asic[0],i_column=asic[1],
          n_rows=asic[2]-asic[0],
          n_columns=asic[3]-asic[1])
      active_data = block.as_1d().as_double()

      order = flex.sort_permutation(active_data)
      if self.verbose:
        print("The mean is ",flex.mean(active_data),"on %d pixels"%len(active_data))
        print("The 90-percentile pixel is ",active_data[order[int(0.9*len(active_data))]])
        print("The 99-percentile pixel is ",active_data[order[int(0.99*len(active_data))]])

      percentile90 = active_data[order[int(0.9*len(active_data))]]
      maximas = flex.vec2_double()
      for idx in range(len(active_data)-1, int(0.9*len(active_data)), -1):
        if active_data[order[idx]] > 2.0 * percentile90:
          if self.verbose: print("    ", idx, active_data[order[idx]])
          irow = order[idx] // (asic[3]-asic[1])
          icol = order[idx] % (asic[3]-asic[1])
          #self.green.append((asic[0]+irow, asic[1]+icol))
          maximas.append((irow, icol))
      CLUS = clustering(maximas)
      #coords = CLUS.as_spot_max_pixels(block,asic)
      coords = CLUS.as_spot_center_of_mass(block,asic,percentile90)
      intensities = CLUS.intensities
      for coord,height in zip(coords,intensities):
        self.green.append(coord)
        indexing.append( (
          coord[0] * float(self.inputpd["pixel_size"]),
          coord[1] * float(self.inputpd["pixel_size"]),
          0.0, # 0 -degree offset for still image
          height)
        )
    return indexing

  def get_active_data_sigma(self):
    data = self.imgobj.linearintdata
    indexing = []
    for asic in self.corners:
      block = data.matrix_copy_block(
          i_row=asic[0],i_column=asic[1],
          n_rows=asic[2]-asic[0],
          n_columns=asic[3]-asic[1])
      active_data = block.as_1d().as_double()

      order = flex.sort_permutation(active_data)
      if self.verbose:
        print("The mean is ",flex.mean(active_data),"on %d pixels"%len(active_data))
        print("The 90-percentile pixel is ",active_data[order[int(0.9*len(active_data))]])
        print("The 99-percentile pixel is ",active_data[order[int(0.99*len(active_data))]])

      stats = flex.mean_and_variance(active_data)
      print("stats are mean",stats.mean(),"sigma",stats.unweighted_sample_standard_deviation())
      maximas = flex.vec2_double()
      for idx in range(len(active_data)-1, int(0.9*len(active_data)), -1):
        if active_data[order[idx]] > stats.mean() + 6.0*stats.unweighted_sample_standard_deviation():
          if self.verbose: print("    ", idx, active_data[order[idx]])
          irow = order[idx] // (asic[3]-asic[1])
          icol = order[idx] % (asic[3]-asic[1])
          #self.green.append((asic[0]+irow, asic[1]+icol))
          maximas.append((irow, icol))
      CLUS = clustering(maximas)
      #coords = CLUS.as_spot_max_pixels(block,asic)
      coords = CLUS.as_spot_center_of_mass(block,asic,stats.mean())
      intensities = CLUS.intensities
      for coord,height in zip(coords,intensities):
        self.green.append(coord)
        indexing.append( (
          coord[0] * float(self.inputpd["pixel_size"]),
          coord[1] * float(self.inputpd["pixel_size"]),
          0.0, # 0 -degree offset for still image
          height)
        )
    return indexing

  def get_active_data_corrected_with_fft(self):
    #data = self.imgobj.linearintdata
    data = self.imgobj.correct_gain_in_place(
      filename = self.phil.speckfinder.dark_stddev,
      adu_scale = self.phil.speckfinder.dark_adu_scale,
      phil = self.phil
    )
    indexing = []
    for iraw,raw_asic in enumerate(self.corners):
      filtered_data = self.imgobj.correct_background_by_block(raw_asic)

      active_data = filtered_data.as_double().as_1d()

      order = flex.sort_permutation(active_data)
      stats = flex.mean_and_variance(active_data)
      if self.verbose:
        #print "Stats on %d pixels"%len(active_data)
        print("stats are mean",stats.mean(),"sigma",stats.unweighted_sample_standard_deviation())
        #print "The 90-percentile pixel is ",active_data[order[int(0.9*len(active_data))]]
        #print "The 99-percentile pixel is ",active_data[order[int(0.99*len(active_data))]]

      maximas = flex.vec2_double()
      for idx in range(len(active_data)-1, int(0.9*len(active_data)), -1):
        if active_data[order[idx]] > stats.mean() + 12.0*stats.unweighted_sample_standard_deviation():
          if self.verbose: print("    ", idx, active_data[order[idx]])
          irow = order[idx] // (raw_asic[3]-raw_asic[1])
          icol = order[idx] % (raw_asic[3]-raw_asic[1])
          maximas.append((irow, icol))
      CLUS = clustering(maximas)
      coords = CLUS.as_spot_center_of_mass(filtered_data,raw_asic,stats.mean())
      intensities = CLUS.intensities
      for coord,height in zip(coords,intensities):
        self.green.append(coord)
        indexing.append( (
          coord[0] * float(self.inputpd["pixel_size"]),
          coord[1] * float(self.inputpd["pixel_size"]),
          0.0, # 0 -degree offset for still image
          height)
        )
    return indexing

  def get_active_data(self):
    import time
    t0 = time.time()
    active = self.get_active_data_corrected_with_fft()
    print("time %.2f" % (time.time()-t0))
    return active

  def __init__(self,imgobj,phil,inputpd,verbose=False):
    adopt_init_args(self,locals())
    self.active_areas = imgobj.get_tile_manager(phil).effective_tiling_as_flex_int()
    B = self.active_areas

    #figure out which asics are on the central four sensors
    assert len(self.active_areas)%4 == 0
    # apply an additional margin of 1 pixel, since we don't seem to be
    # registering the global margin.
    asics = [(B[i]+1,B[i+1]+1,B[i+2]-1,B[i+3]-1) for i in range(0,len(B),4)]

    from scitbx.matrix import col
    centre_mm = col((float(inputpd["xbeam"]),float(inputpd["ybeam"])))
    centre = centre_mm / float(inputpd["pixel_size"])
    distances = flex.double()
    cenasics = flex.vec2_double()
    self.corners = []
    for iasic in range(len(asics)):
      cenasic = ((asics[iasic][2] + asics[iasic][0])/2. ,
                 (asics[iasic][3] + asics[iasic][1])/2. )
      cenasics.append(cenasic)
      distances.append(math.hypot(cenasic[0]-centre[0], cenasic[1]-centre[1]))
    orders = flex.sort_permutation(distances)

    self.flags = flex.int(len(asics),0)
    #Use the central 8 asics (central 4 sensors)
    self.green = []
    for i in range(32):
      #self.green.append( cenasics[orders[i]] )
      self.corners.append (asics[orders[i]])
      #self.green.append((self.corners[-1][0],self.corners[-1][1]))
      self.flags[orders[i]]=1
    self.asic_filter = "distl.tile_flags="+",".join(["%1d"%b for b in self.flags])

class clustering:
  def __init__(self,maxima):
    self.verbose=False
    #for the next spot, define the universe of possible pixels (working targets)
    # and the map of which pixels have been visited already (pixel_visited)
    working_targets = list(range(len(maxima)))
    pixel_visited = flex.bool(len(working_targets),False)
    self.spots = []
    while len(working_targets) > 0:
      # for this spot, indices of pixels known to be in the spot (pixel_members)
      # and a stack of indices of pixels still in process of testing for connections
      #  (connection_stack).  Pop/push operates on the end of stack
      pixel_members = [working_targets[0]]
      connection_stack = [0]
      assert len(pixel_visited)==len(working_targets)
      pixel_visited[0]=True
      while len(connection_stack) > 0:
        idx_current = connection_stack[-1]
        for idx_target in range(len(working_targets)):
          if not pixel_visited[idx_target]:
            distance = math.hypot( maxima[working_targets[idx_current]][0]-
                                   maxima[working_targets[idx_target]][0],
                                   maxima[working_targets[idx_current]][1]-
                                   maxima[working_targets[idx_target]][1])
            if distance >= 2.0: continue
            pixel_visited[idx_target]=True
            pixel_members.append(working_targets[idx_target])
            connection_stack.append(idx_target)
        if connection_stack[-1] == idx_current: connection_stack.pop()
      if self.verbose: print("new spot with %d pixels"%len(pixel_members),pixel_members)
      for idx in pixel_members:#[working_targets[i] for i in pixel_members]:
        working_targets.remove(idx)
      pixel_visited = flex.bool(len(working_targets),False)
      self.spots.append( [maxima[i] for i in pixel_members] )

  def as_spot_max_pixels(self,active_block,asic):
    maxima = flex.vec2_double()
    for spot in self.spots:
      if self.verbose:print([(int(row)+asic[0],int(col)+asic[1]) for row,col in spot])
      pixel_values = [active_block[(int(row),int(col))] for row,col in spot]
      if self.verbose:print("PIXEL_VALUES",pixel_values)
      addr = pixel_values.index(max(pixel_values))
      maxima.append( ( int(spot[addr][0]) + asic[0], int(spot[addr][1]) + asic[1] ))
      if self.verbose:print()
    return maxima

  def as_spot_center_of_mass(self,active_block,asic,percentile90):
    from scitbx.matrix import col
    maxima = flex.vec2_double()
    self.intensities = flex.double()
    for spot in self.spots:
      pixels = [col(((frow)+asic[0],(fcol)+asic[1])) for frow,fcol in spot]
      pixel_values = [active_block[(int(frow),int(fcol))] for frow,fcol in spot]
      numerator = col((0.0,0.0)); denominator = 0.0
      for ispot in range(len(spot)):
        numerator += (pixel_values[ispot]-percentile90)*pixels[ispot]
        denominator += pixel_values[ispot]-percentile90
      if len(spot) < 20:
        #any spot with more than 20 pixels is a clear outlier
        maxima.append( numerator/denominator )
        self.intensities.append(denominator)
    return maxima


 *******************************************************************************


 *******************************************************************************
rstbx/new_horizons/spot_shape.py
from __future__ import absolute_import, division, print_function
import math
from scitbx.array_family import flex

# This analysis is meant for CXI CSPAD only

def spot_shape_verbose(rawdata,beam_center_pix,indexed_pairs,spotfinder_observations,
      distance_mm, mm_per_pixel, hkllist, unit_cell, wavelength_ang):
    """requires:
    rawdata -- a 2d flex.int() with raw data
    beam_center_pix -- a scitbx.matrix.col() with beam xy in pixels
    indexed_pairs -- custom data structure
    spotfinder_observations -- spotfinder results
    """

    #--------------------------------------------- work on spot shape -------------
    from scitbx.matrix import col
    plotradial = flex.double()
    plotazim = flex.double()
    plotresol = flex.double()
    bodyx = flex.int()
    bodyy = flex.int()

    domain_sizes = flex.double()
    rot_mosaicities_deg = flex.double()
    implied_mosaicities_deg = flex.double()
    bandpasses = flex.double()
    unit_cell_deltas = flex.double()
    for ipidx,item in enumerate(indexed_pairs):
      #Not sure if xbeam & ybeam need to be swapped; values are very similar!!!
      radial, azimuthal = spotfinder_observations[item["spot"]].get_radial_and_azimuthal_size(
        beam_center_pix[0], beam_center_pix[1])

      model_center = col((spotfinder_observations[item["spot"]].ctr_mass_x(),
                          spotfinder_observations[item["spot"]].ctr_mass_y()))
      spot_vec_px = model_center - beam_center_pix
      spot_rvec = spot_vec_px.normalize()
      spot_avec = spot_rvec.rotate_2d(angle=90.,deg=True)

      from rstbx.new_horizons.pixel_spread import fwhm_2d_response
      RADIAL = fwhm_2d_response(rawdata,projection_vector = spot_rvec,spotfinder_spot = spotfinder_observations[item["spot"]])
      AZIMUT = fwhm_2d_response(rawdata,projection_vector = spot_avec,spotfinder_spot = spotfinder_observations[item["spot"]])


      overloaded = False
      for point in spotfinder_observations[item["spot"]].bodypixels:
        pixel_value = rawdata[(point.x,point.y)]
        if pixel_value > 5000.: #for XFEL only
          overloaded=True
      if not overloaded:
        # print out some properties:
        # first, simple model where all photon energy is assigned to the middle of the pixel
        print("%5d Johan radial %6.2f px, Johan azimuthal %6.2f px"%(ipidx, radial, azimuthal), end=' ')
        Miller = hkllist[item["pred"]]
        resolution = unit_cell.d(Miller)
        print("resolution %6.2f Miller %s"%(resolution,str(Miller)))

        fwhm_azi = AZIMUT.fwhm_pix()
        #small angle approximation
        rotational_mosaicity_deg = (fwhm_azi/spot_vec_px.length())*180./math.pi

        crystal_to_spot_ray_pix = math.hypot(distance_mm/mm_per_pixel,spot_vec_px.length())
        scherrer_fwhm_rad = fwhm_azi/crystal_to_spot_ray_pix
        scherrer_fwhm_deg = (scherrer_fwhm_rad)*180./math.pi
        if scherrer_fwhm_deg > 0.:
          scherrer_domain_size = wavelength_ang / (fwhm_azi/crystal_to_spot_ray_pix)
          domain_sizes.append(scherrer_domain_size)
        else:
          scherrer_domain_size = 0.

        print("      FWHM  radial %6.2f px, FWHM  azimuthal %6.2f px"%(RADIAL.fwhm_pix(), fwhm_azi))

        print("      Scherrer diffracted ray divergence FWHM %7.3f deg. Domain size %8.0f Angstrom"%(
          scherrer_fwhm_deg,scherrer_domain_size))

        print("          isotropic rotational FWHM mosaicity %7.3f deg"%rotational_mosaicity_deg)

        spot_vec_mm = spot_vec_px.length() * mm_per_pixel
        radial_hwhm_mm = (RADIAL.fwhm_pix() * mm_per_pixel)/2.
        two_theta_low = math.atan( (spot_vec_mm+radial_hwhm_mm)/distance_mm )
        two_theta_high = math.atan( (spot_vec_mm-radial_hwhm_mm)/distance_mm )
        mosaicity_fwhm_rad = 0.5 * ( two_theta_low - two_theta_high )

        Elow_Ehigh_ratio = math.sin( two_theta_high/2. ) / math.sin ( two_theta_low/2. )
        bandpass_fwhm = 1. - Elow_Ehigh_ratio

        print("     radial divergence implied FWHM mosaicity %7.3f deg bandpass FWHM = %8.5f"%(
          mosaicity_fwhm_rad * 180/math.pi, bandpass_fwhm))


        Delta_a_over_a = scherrer_fwhm_rad * resolution / wavelength_ang

        print("    diffracted ray divergence FWHM (delta a)/a %8.5f"%( Delta_a_over_a ))

        if resolution < 4.0:
          rot_mosaicities_deg.append(rotational_mosaicity_deg)
          implied_mosaicities_deg.append(mosaicity_fwhm_rad * 180/math.pi)
          bandpasses.append(bandpass_fwhm)
          unit_cell_deltas.append(Delta_a_over_a)
        print()

    if len(domain_sizes) > 10:
      print("  IMAGE      Scherrer domain size, lower bound = %8.0F Angstrom"%flex.min(domain_sizes))
    if len(rot_mosaicities_deg) > 10:
      print("  IMAGE rotational FWHM mosaicity, upper bound = %7.3f deg. mean = %7.3f deg."%(
        flex.max(rot_mosaicities_deg), flex.mean(rot_mosaicities_deg)))
      print("  IMAGE    implied FWHM mosaicity, upper bound = %7.3f deg. mean = %7.3f deg."%(
        flex.max(implied_mosaicities_deg), flex.mean(implied_mosaicities_deg)))
      print("  IMAGE             FWHM bandpass, upper bound =  %8.5f"%(
        flex.max(bandpasses)))
      print("  IMAGE   azimuthal FWHM deltaa/a, upper bound =  %8.5f from radial: %8.5f"%(
        flex.max(unit_cell_deltas), flex.max(implied_mosaicities_deg)*math.pi/180.))


    #-------------------- done with spot shape


 *******************************************************************************


 *******************************************************************************
rstbx/new_horizons/stats_index.py
from __future__ import absolute_import, division, print_function
import os
from labelit.command_line.imagefiles import ImageFiles

class spotfinder_proxy:
  def __init__(self,old_spotfinder,phil,frames):
    self.frames = frames
    self.phil = phil
    self.old_pd = old_spotfinder.pd
    self.old_S = old_spotfinder
  def get_aitbx_inputs(self):
    pd = dict(xbeam = self.old_pd["xbeam"],
              ybeam = self.old_pd["ybeam"],
              osc_start = self.old_pd["osc_start"],
              binning = "1",
              size1 = self.old_pd["size1"],
              size2 = self.old_pd["size2"],
              pixel_size = self.old_pd["pixel_size"],
              distance = self.old_pd["distance"],
              wavelength = self.old_pd["wavelength"],
              deltaphi = self.old_pd["deltaphi"],
              indexing = self.old_S.get_aitbx_inputs()["indexing"],
              endstation = self.old_pd["endstation"],
              recommended_grid_sampling = self.old_S.get_aitbx_inputs()["recommended_grid_sampling"],
              twotheta = self.old_pd["twotheta"],
              resolution_inspection = self.old_pd["resolution_inspection"],
              smallest_spot_sep = self.old_S.get_aitbx_inputs()["smallest_spot_sep"],
              masks = self.old_pd["masks"], #see practical heuristics
              spot_convention = self.old_pd["spot_convention"],
              vendortype = self.old_pd["vendortype"],
              #characteristic_grid_sampling =  0.01845574110881109,
              #characteristic_resolution_mm = 43.390947190873604
             )
    pd["ref_maxcel"] = self.old_pd["ref_maxcel"] #post-get_aitbx_inputs
    self.images = self.old_S.images # sublattice average profile
    self.pd = pd
    old_count = len(pd["indexing"])
    from rstbx.new_horizons.speckfinder import speckfinder
    self.pd["indexing"]=[] # zero out the old spotfinder spots; use speckfinder spots instead
    for key in self.images.keys():
      self.specks = speckfinder(imgobj = self.frames.imageindex(key),
                       phil = self.phil,
                       inputpd = self.pd)
      self.pd["indexing"] += self.specks.get_active_data()
    new_count = len(pd["indexing"])
    print("Comparing old count %d new count %d, difference %d"%(old_count,new_count,new_count-old_count))
    print(self.specks)
    return self.pd

class AutoIndexOrganizer:

  def __init__(self,verbose = 0,**kwargs):
    self.rundir = os.getcwd()
    self.verbose = verbose
    self.horizons_phil = kwargs["horizons_phil"]
    #self.horizons_phil.persist.show()
    assert 'argument_module' in kwargs
    self.setCommandInput(kwargs['argument_module'])
    if self.verbose: print("Process frames in directory:",self.Files.filenames.FN[0].cwd)

    if 'delegate' in kwargs:
      self.setIndexingDelegate(kwargs['delegate'])
    self.exception_passthru = 0
    if 'exception_passthru' in kwargs:
      self.exception_passthru = kwargs['exception_passthru']
    print('\n'.join(self.Files.filenames()))

  def setCommandInput(self,argument_module):
    self.Files = ImageFiles(argument_module,self.horizons_phil)
    self.frames = self.Files.frames()

  def printSpots(self):
    from labelit.procedure import spotfinder_and_pickle
    S = spotfinder_and_pickle(self.rundir,self.Files,
        spots_pickle = self.horizons_phil.spots_pickle,
        horizons_phil = self.horizons_phil)

    #print S.images

    NEW = spotfinder_proxy(S,self.horizons_phil,self.Files)
    NEW.images = {}
    NEW.overlapping = False
    NEW.phil_params = S.phil_params
    for frame in self.frames:
      NEW.images[frame]=dict(area=[1,]  # not actually used for new horizons
                            )

    self.S = NEW
    for frame in self.frames:
     if self.verbose:
      from labelit.command_line.stats_distl import pretty_image_stats,notes
      pretty_image_stats(S,frame)
      notes(S,self.frames[0])
    print()
    NEW.get_aitbx_inputs()

  def setIndexingDelegate(self,function):
    self.indexing_delegate = function

  def executeDelegate(self):
      self.info = self.indexing_delegate(self.frames,self.Files,self.S)

  def pickle_the_results(self):
      for key in ['best_integration','triclinic']:
        if key in self.info:
         if 'minimizer' in self.info[key]: #not attained when best==tri
          del self.info[key]['minimizer'] # Must remove

         # temporary section pending an analysis of which data need to be persistent
         if 'results' in self.info[key]["integration"]:
          #future options 1) make the whole object picklable--write test script
          #2) just pickle the data needed for the GUI
          del self.info[key]["integration"]['results']
      from labelit.dptbx.pickle_support import pickle_refinements
      pickle_refinements(self.info,self.horizons_phil.refinements_pickle)

  def process(self):
    self.printSpots()
    self.executeDelegate()
    if 'info' in self.__dict__: #if indexing worked
      self.pickle_the_results()
      return self.info


 *******************************************************************************


 *******************************************************************************
rstbx/outlier_spots/__init__.py


 *******************************************************************************


 *******************************************************************************
rstbx/outlier_spots/fit_distribution.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import math
import scitbx.math
from scitbx import lbfgs
from scitbx.array_family import flex

class fit_cdf(object):
  """
  =============================================================================
  Class fits a distribution according to its cumulative distribution function

  Arguments:
    x_data - measured property (list)
    y_data - fraction of points with measured property (cdf) (list)
    distribution - type of distribution to be fit

  Useful accessible attributes:
    self.x - the final parameters (list)
    self.distribution - the distribution being modeled (see notes)

  Notes:
    The available distributions are,
      gaussian
      rayleigh
      rice
    The distribution argument should not have quotes since it is used directly
    to access the class.
    The lbfgs method is used to minimize, f = (predicted - observed)^2
  -----------------------------------------------------------------------------
  """
  def __init__(self,x_data=None,y_data=None,distribution=None,
               max_iterations=50):
    # setup data
    assert(len(x_data) == len(y_data))
    self.n = len(x_data)
    self.x_data = flex.double(x_data)
    self.y_data = flex.double(y_data)
    if (flex.max(self.y_data) > 1.0):
      print("The cumulative distribution function (y_data) should only have values be between 0 and 1.")
      exit()

    # intialize distribution with guess
    self.distribution = distribution()
    self.distribution.estimate_parameters_from_cdf(x_data=self.x_data,
                                                   y_data=self.y_data)
    self.x = self.distribution.get_parameters()

    # optimize parameters
    self.minimizer = lbfgs.run(target_evaluator=self)

    # set optimized parameters
    self.distribution.set_parameters(p=self.x)

  def compute_functional_and_gradients(self):
    # caculate difference between predicted and observed values
    self.distribution.set_parameters(p=self.x)
    is_cpp_ = getattr(self.distribution,"interface","Python")=="C++"
    if is_cpp_:
      predicted = self.distribution.cdf(x=self.x_data)
    else:
      predicted = flex.double(self.n)
      for i in range(self.n):
        predicted[i] = self.distribution.cdf(x=self.x_data[i])
    difference = predicted - self.y_data

    # target function for minimization is sum of rmsd
    f = flex.sum(flex.sqrt(difference*difference))
    if is_cpp_:
      gradients = self.distribution.gradients(x=self.x_data, nparams=len(self.x), difference=difference)
      return f,gradients
    gradients = flex.double(len(self.x))
    for i in range(self.n):
      g_i = self.distribution.cdf_gradients(x=self.x_data[i])
      for j in range(len(self.x)):
        gradients[j] = gradients[j] + difference[i]*g_i[j]
    gradients = 2.0*gradients
    return f,gradients

# =============================================================================
class rayleigh(object):
  """
  =============================================================================
  Class models a 1-d Rayleigh distribution using one parameter, sigma.

              x                x^2
    pdf = --------- exp(- ------------)
           sigma^2         2 sigma^2

                        x^2
    cdf = 1 - exp(- -----------)
                     2 sigma^2

  The derivative of the cdf with respect to sigma is,

      d(cdf)          x^2               x^2             x
    ---------- = - --------- exp( - -----------) = - ------- pdf
     d(sigma)       sigma^3          2 sigma^2        sigma

  Methods:
    set_parameters
    get_parameters
    estimate_parameters_from_cdf
    pdf
    cdf
    d_cdf_d_sigma
    d_cdf_d_sigma_finite
    cdf_gradients
  -----------------------------------------------------------------------------
  """
  def __init__(self,mean=None,sigma=None):

    # parameter for Rayleigh distribution
    if (sigma is None):
      sigma = 1.0
    self.sigma = sigma

  def set_parameters(self,p=None):
    """
    Function sets the all the parameters
    """
    assert(len(p) == 1)
    self.sigma = float(p[0])

  def get_parameters(self):
    """
    Function returns all the parameters
    """
    return flex.double([self.sigma])

  def estimate_parameters_from_cdf(self,x_data=None,y_data=None):
    """
    Function estimates the parameter values based on the data (cdf)
    """
    # sigma is the mode of the distribution
    # approximate with the median (cdf = 0.5)
    midpoint = None
    for i in range(len(x_data)):
      if (y_data[i] > 0.5):
        midpoint = i
        break
    if (midpoint is None):
      midpoint = len(x_data) - 1
    self.sigma = x_data[midpoint]

  def pdf(self,x=None):
    """
    Function returns the probability density function at x
    """
    x_sigma = x/self.sigma
    f = (x_sigma/self.sigma)*math.exp(-0.5*x_sigma*x_sigma)
    return f

  def cdf(self,x=None):
    """
    Function returns the cumulative distribution function at x
    """
    x_sigma = x/self.sigma
    f = 1.0 - math.exp(-0.5*x_sigma*x_sigma)
    return f

  def inv_cdf(self,cdf=None):
    """
    Function returns the inverse cumulative distribution function at cdf
    """
    return math.sqrt(-2.*self.sigma*self.sigma*math.log(1.-cdf))

  def d_cdf_d_sigma(self,x=None):
    """
    Function returns the derivative of the cdf at x with respect to the
    standard deviation
    """
    df = -(x/self.sigma)*self.pdf(x=x)
    return df

  def d_cdf_d_sigma_finite(self,x=None,delta=0.00001):
    """
    Function returns the derivative of the cdf at x with respect to the
    standard deviation
    """
    sigma0 = self.sigma
    self.sigma = sigma0 + delta
    f1 = self.cdf(x=x)
    self.sigma = sigma0 - delta
    f2 = self.cdf(x=x)
    self.sigma = sigma0
    return (f1-f2)/(2.0*delta)

  def cdf_gradients(self,x=None):
    """
    Function returns a flex.double containing all derivatives
    """
    return flex.double([self.d_cdf_d_sigma(x)])


 *******************************************************************************


 *******************************************************************************
rstbx/phil/__init__.py


 *******************************************************************************


 *******************************************************************************
rstbx/phil/phil_preferences.py
from __future__ import absolute_import, division, print_function
from libtbx.phil.command_line import argument_interpreter as model_argument_interpreter
from libtbx.utils import Sorry
from spotfinder.command_line.signal_strength import additional_spotfinder_phil_defs # implicit import

try:
  from dials.algorithms.integration.kapton_correction import absorption_defs
except ImportError:
  absorption_defs = ""

libtbx_misc_defs = """
predictions_file = ""
    .type = str
    .help = File has xds parameters for spot predictions in XDS XPARAM format.

parallel = 0
  .type = int

integration {
  file_template = None
    .type = str
    .help = "Full path for the files to integrate, expressed as template like lyso_1_###.img"
  file_range = None
    .type = ints (value_min=1)
    .help = First and last file number to integrate, forming a contiguous INCLUSIVE range (not like Python range).
  rocking_curve = *none gh1982a
    .type = choice
    .help = gh1982a is the Greenhough & Helliwell 1982 section I model, with epsilon from eqn (V.6)
  mosaicity_deg = 0.0
    .type = float
    .help = full width effective mosaicity, degrees, for specified rocking curve model such as gh1982a
  guard_width_sq = 11
    .type = int
    .help = Guard is the reserved area around the Bragg spot mask that cannot be used for background
    .help = plane determination because the tail of the signal distribution may leak from the spot.
    .help = Value represents max disallowed squared hypotenuse between neighboring signal & background pixels, as integer px**2.
  detector_gain = 0.0
    .type = float
    .help = Detector gain in units of Analog Digital Units per photon.
    .help = Future plan: this value overrides any given through the dxtbx format mechanism.
    .help = Present: this is a mandatory value, code throws an exception with the default value.
  background_factor = 1
    .type = int (value_min=1)
    .help = require minimum number of pixels for background fit = background_factor x # spot pixels
  model = *rossmann1979jac12-225 use_case_3_simulated_annealing use_case_3_simulated_annealing_7 use_case_3_simulated_annealing_9 user_supplied
    .type = choice
    .help = algorithm for prediction of spots
    .help = Michael Rossman (1979) J. Appl. Cryst. 12, 225-238.
  use_subpixel_translations = None
    .type = floats
    .help = list slow,fast offsets for correcting tile positions to subpixel resolution (2 numbers for each tile)
  subpixel_joint_model {
    rotations = None
      .type=floats
      .help = joint-refined tile rotations & translations [along with per-image beam,dist,rotz,wavelength, not stored]
    translations = None
      .type=floats
      .help = joint-refined tile rotations & translations [along with per-image beam,dist,rotz,wavelength, not stored]
  }
  spot_shape_verbose = False
    .type = bool
    .help = analysis of radial and azimuthal spot shapes.
  signal_penetration = 0.5
    .type = float
    .help = For computing parallax effect due to finite sensor thickness, fraction of signal to attenuate before
    .help = ignoring the remaining trailing parallax.  Small value (0.0) means do not account for parallax.
  spotfinder_subset = *inlier_spots goodspots spots_non-ice
    .type = choice
    .help = which subset to use for parameter refinement and constructing integration profiles.
    .help = subsets are nested goodspots > spots_non-ice > inlier_spots
  mask_pixel_value = None
    .type = int
    .help = pixels set to this value will be ignored during integration
  mosaic {
    refinement_target = LSQ *ML
      .type = choice
      .help = Modeling of still shots, refinement of crystal physical parameters by one of two targets given in the paper
      .help = [Sauter et al. Acta D (2014) 70:3299-3309]: least squares (LSQ) or maximum likelihood (ML).
      .help = As the paper describes, ML gives superior results and is set to be the default as of 7/2017.
    kludge1 = 1
      .type = float
      .help = bugfix 1 of 2 for protocol 6, equation 2.  When matching obs and model reflections,
      .help = must increase the provisional model mosaicity so as to model enough spots.
      .help = default 1 might be too small for a bad model, aborting indexing with too few matches.
      .help = > 2 might be too large and could lead to misindexing.
    bugfix2_enable = True
      .type = bool
      .help = bugfix 2 of 2 for protocol 6, equation 2.  Assure that the result of the arctan()
      .help = function call is placed in the correct principle value region.
      .help = choice of "False" preserves the ability to roll back program behavior
      .help = as promised to the Nature Methods editor.
    domain_size_lower_limit = 10
      .type = float
      .help = Mosaic blocks must be at least this many unit cells on edge (refers to cube-root of cell volume)
      .help = used in simple_integration.py as an initial value
      .help = too-small value predicts too many spots, leading to misindexing.
      .help = too-large value doesn't predict enough spots
      .help = choose lowest value physically reasonable, 10 unit cells.
    enable_rotational_target_highsym = True
      .type = bool
      .help = Protocol 6, use equation 2 (true) or protocol 5, use equation 1 (false) for higher Bravais-setting refinement
    enable_rotational_target_triclinic = True
      .type = bool
      .help = Protocol 5, use equation 2 (true) or protocol 4, use equation 1 (false) for triclinic Bravais-setting refinement
    enable_simplex = False
      .type = bool
      .help = simplex_optimization
    enable_AD14F7B = False
      .type = bool
      .help = enable Sauter et al. Acta D (2014) 70:3299-3309, Fig. 7(b) plot, angular excursion vs two theta
    enable_polychromatic = False
      .type = bool
      .help = followup top hat wavelength dispersion
    ewald_proximal_volume_resolution_cutoff = 2.5
      .type = float
      .help = Resolution cutoff in Angstroms.  Set roughly to the apparent cutoff of bright spots for one or a group of images.
      .help = governs the cutoff resolution for teh ewald proximal volume: that volume of reciprocal space containing
      .help =  spots predicted by the current mosaicity model.
  }
  initial_volume_factor = 1.8
      .type = float
      .help = controls the initial integration limits before filtering by sig(I) durig merging
      .help = used in rstbx/new_horizons/oscillation_shots.py -- not fully understood
      .help = a larger volume factor integrates a larger volume of reciprocal space
  enable_residual_map = False
      .type = bool
      .help = x,y model vs. spotfinder residuals plotted vs. image position
  enable_residual_map_deltapsi = False
      .type = bool
      .help = if residual map is enabled, colorcode the Bragg spots by Delta-Psi
      .help = blue, negative delta psi, outside Ewald sphere. red, positive delta psi, inside Ewald sphere.
      .help = requires also setting enable_residual_map to True and spot_prediction to dials
  enable_residual_scatter = False
      .type = bool
      .help = x,y model vs. spotfinder residuals scatter plot
  graphics_backend = *screen pdf
      .type = choice
      .help = data plots are written to the terminal screen, or to pdf files
  pdf_output_dir = "."
      .type = str
      .help = plots are written to this directory
  montecarlo_integration_limit = None
    .type = float
    .help = use None to limit the integration resolution based on Wilson plot
    .help = alternatively set integration limit to a fixed value given in Angstroms.
  greedy_integration_limit = False
    .type = bool
    .help = governs the resolution limits used for higher Bravais settings
    .help = if true, use the expanded limiting_resolution used for the last triclinic round
    .help = if false, attempt to analyze triclinic integrated spots for limits (default until Feb 2015, not so reliable)
  combine_sym_constraints_and_3D_target = False
    .type = bool
    .help = enable redesigned refinement protocol from Acta D 2014 Sauter paper
    .help = if false (default as of Feb 2015) symmetry constraints are applied after eqn (1) positional refinement
    .help =   but before indexing of high-Bravais symmetry spots, possibly leading to misindexing of high-angle spots
    .help = if true, spots are indexed once only, in triclinic setting.  After application of high-symmetry constraints
    .help =   dials is used to refine positions (eqn 1) and deltapsi angle (eqn 2).
  spot_prediction = *ucbp3 dials
      .type = choice
      .help = in high-symmetry integration protocol, predict with ucbp3 (CSPAD subpixel corrections) or dials (detector tilt, but unit translations)
  enable_one_to_one_safeguard = False
    .type = bool
    .help = Flag enables a safeguard within the stills integration algorithm, within the mapping of
    .help = predicted spots to observations prior to model refinement.  The safeguard prevents
    .help = a many-to-one predicted-to-observation mapping, assuring that each prediction is mapped
    .help = to at most one observation, one that is closest.  The legacy code did not include
    .help = this safeguard.  It is expected that True should become the default after a testing period.
  dials_refinement{
    strategy = *distance wavelength fix
      .type = choice
      .help = either refine the distance or the wavelength for XFEL stills, or fix them both in place
  }%s
}
""" % absorption_defs
indexing_defs = """
include scope spotfinder.command_line.signal_strength.master_params
spotfinder = *distl speck
  .type=choice
  .help = "Choose among spotfinder implementations [distl|speck]"

speckfinder {

  dark_stddev = ""
    .type = str
    .help = Mandatory dark standard deviation image for gain correction.
  dark_adu_scale = 100
    .type = int
    .help = "Mandatory scale at which dark was calculated; must be >1 on account of integer rounding."
}

indexing {
  data = None
    .type=str
    .multiple=True
    .help="Relative or absolute path names for raw image files to be indexed"
  indexing_pickle = None
    .type=str
    .help = "pickle file name for integration results subsequent to indexing."
  completeness_pickle = None
    .type=str
    .help = "pickle file name for HKL, I, SIGI, XY."
  open_wx_viewer = False
    .type = bool
  verbose_cv = False
    .type = bool
    .help = "screen printout of the obs vs predicted spot correction vectors,"
    .help = "for empriical repositioning of the detector tiles."
  lattice_model_scoring_cutoff = 2.0
    .type = float
    .help = Cutoff value for the <Z-score> over integrated signal from the model lattice.
    .help = Used for choosing the most accurate combination of candidate basis vectors.
  devel_algorithm = None
    .type = str
    .help = for development only, turn on whatever testing behavior

  outlier_detection {
    allow = True
      .type = bool
      .multiple=False
      .help="Algorithm (Sauter&Poon[2010] J Appl Cryst 43:611) provides superior positional fit with noisy data."
    switch=False
      .type=bool
      .multiple=False
      .help="Switch to the outlying spots to detect a second lattice. False==first lattice; True==second lattice"
    verbose=False
      .type=bool
      .multiple=False
      .help="Verbose output."
    pdf=None
      .type=str
      .multiple=False
      .help="Output file name for making graphs of |dr| vs spot number and dy vs dx."
  }
  plot_search_scope = False
    .type = bool
    .help = improvement of the model, plot target function of origin offset or S0
  mm_search_scope = 4.0
    .type = float
    .help = global radius of origin_offset search, used for plotting the search scope
  improve_local_scope = *origin_offset S0_vector
    .type = choice
    .help = improve 'beam position' according to Sauter et al (2004).  Local minimum only
    .help = specifies which parameter to optimize.
}
"""

iotbx_defs_viewer_detail = """
  powder_arcs{
    show = False
      .type=bool
      .help = "show powder arcs calculated from PDB file."
    code = None
      .type=str
      .help = "PDB code (4 characters) for file; fetch it from the Internet."
  }
  calibrate_silver = False
      .type=bool
      .help = "Open special GUI for distance/metrology from silver behenate."
  calibrate_pdb{
    code = None
      .type=str
      .help = "Open pdb code (over Internet) to get unit cell & symmetry for powder rings."
      .help = "Most useful for calibrating low-Q rings on far detector."
      .help = "Option is mutually exclusive with calibrate silver, unit cell and powder arcs options."
    d_min = 20.
      .type=float
      .help = "Limiting resolution to calculate powder rings"
  }
  calibrate_unitcell{
    unitcell = None
      .type=unit_cell
      .help = "Specify unit cell for powder rings."
      .help = "Option is mutually exclusive with calibrate silver, pdb and powder arcs options."
    d_min = 20.
      .type=float
      .help = "Limiting resolution to calculate powder rings"
    spacegroup = None
      .type=str
      .help = "Specify spacegroup for the unit cell"
  }
"""

iotbx_defs_viewer = """
viewer {
  %s
}
"""%iotbx_defs_viewer_detail

iotbx_defs_target = """
target_cell=None
  .type=unit_cell
  .multiple=False
  .help="Imperative unit cell applied at the level of DPS algorithm basis selection."
target_cell_centring_type= *P C I R F
  .type=choice
  .multiple=False
  .help="Centring symbol for the target cell"
isoforms
  .help=Constrain the unit cell to specific values during refinement
  .help=As presently implemented, applies only to dials_refinement_preceding_integration
  .help=and applies only to the higher-symmetry integration trial, not the initial triclinic
  .multiple=True
  {
    name=None
      .type=str
    cell=None
      .type=unit_cell
    lookup_symbol=None
      .type=str
      .help=The sgtbx lookup symbol of the reflections pointgroup
    rmsd_target_mm=None
      .type=float
      .help=Maximum acceptable DIALS positional rmsd, in mm
    beam_restraint=None
      .type=floats(size=2)
      .help=Known beam position in mm X,Y, rmsd_target_mm is reused here as a circle of confusion
      .help=to assure that no images are accepted where the lattice is misindexed by a unit shift.
  }
"""
libtbx_defs = indexing_defs + libtbx_misc_defs
iotbx_defs = iotbx_defs_viewer + iotbx_defs_target
indexing_api_defs = indexing_defs + iotbx_defs_target

class EffectiveParamGenerator:
  def __init__(self,libtbx_defs,iotbx_defs):
    from libtbx import adopt_init_args
    adopt_init_args(self, locals())

  def master(self,package = 'iotbx'):
    libselector = {'libtbx':self.libtbx_defs,
                   'iotbx':self.iotbx_defs+self.libtbx_defs,
                  } [package]
    if (package == "libtbx"):
      from libtbx import phil
    else:
      from iotbx import phil
    return phil.parse(input_string=libselector, process_includes=True)

  def default(self,item = 'iotbx'):
    app_master = self.master(item)
    return app_master.fetch(sources=[app_master,])

  def show(self, modpython):
    modified_params = self.master().format(python_object = modpython)
    modified_params.show()

  def merge(self, args):
    #future:  this member function should be deprecated; replace with preferences.py

    effective_params = self.default()

    argument_interpreter = model_argument_interpreter(
      master_phil=self.master(),
      #home_scope =
    )
    consume = []
    for arg in args:

      try:
        command_line_params = argument_interpreter.process(
          arg=arg
        )
        effective_params = effective_params.fetch(sources=[command_line_params,])
        consume.append(arg)

      except Sorry as e:
        pass

    for item in consume:
      args.remove(item)


    # effective_params.show()

    params = effective_params.extract()

    self.validation(params)

    self.effective_params = effective_params
    return params

  def validation(self,trial_params):
    pass

effective_param_generator = EffectiveParamGenerator(libtbx_defs,iotbx_defs) #singleton


 *******************************************************************************


 *******************************************************************************
rstbx/phil/preferences.py
from __future__ import absolute_import, division, print_function
import os
from rstbx.phil import phil_preferences
from rstbx.phil.scope import scope
from libtbx.utils import Sorry

class Extract(object):
  def __init__(self,other):
    self.persist = other
  def __getattr__(self,item):
    if item=="persist":
      return super(Extract,self).__getattr__(item)
    return self.persist.commands.__getattribute__(item)
  def __setattr__(self,item,value):
    if item=="persist":
      super(Extract,self).__setattr__(item,value)
    else:
      self.persist.commands.__setattr__(item,value)

class RunTimePreferences(object):
  def __init__(self,scope = scope.value):

    "Parameters governed by PHIL module"
    self.phil_scope = phil_preferences.effective_param_generator.default(scope)
    self.new_scope_extract()
    self.command_extractor = Extract(self)

  def show(self):
    phil_preferences.effective_param_generator.show(self.commands)

  def rollback_dataset_preferences(self):
    self.phil_scope = phil_preferences.effective_param_generator.default(scope.value)
    self.new_scope_extract()

  def try_any_preferences_file(self,filename):
    if os.path.isfile(filename):
      import libtbx
      user_phil = libtbx.phil.parse(open(filename).read())
      self.phil_scope = self.phil_scope.fetch(source=user_phil)
      self.new_scope_extract()

  def try_dataset_preferences(self):
    filename = "dataset_preferences.py"
    self.try_any_preferences_file(filename)

  def merge_command_line(self,args):

    from libtbx.phil.command_line import argument_interpreter

    argument_interpreter = argument_interpreter(
      master_phil=phil_preferences.effective_param_generator.master(),
    )
    consume = []
    for arg in args:

      try:
        command_line_params = argument_interpreter.process(
          arg=arg
        )
        self.phil_scope = self.phil_scope.fetch(sources=[command_line_params,])
        consume.append(arg)

      except Sorry as e:
        pass

    for item in consume:
      args.remove(item)

    self.new_scope_extract()

  def new_scope_extract(self):
    self.commands = self.phil_scope.extract()
    if scope.value=="iotbx": # not libtbx
      phil_preferences.effective_param_generator.validation(self.commands)


 *******************************************************************************


 *******************************************************************************
rstbx/phil/scope.py
from __future__ import absolute_import, division, print_function
class scope:
  value = 'iotbx' #highest-level package needed to define phil preferences


 *******************************************************************************


 *******************************************************************************
rstbx/run_tests.py
from __future__ import absolute_import, division, print_function
from libtbx import test_utils
import libtbx.load_env

def run():
  from rstbx.indexing.tst_auto_monoscan import test_automatic_monoscan
  from rstbx.indexing.tst_dataset1 import test_case_obs_data
  from rstbx.indexing.tst_dataset1 import test_case_synthetic_data

  from rstbx.dps_core.sampling import test_simple_sampler
  from rstbx.dps_core.zuoreduction import test_reduction

  for functional_test in [test_automatic_monoscan,
                          test_case_obs_data,test_case_synthetic_data]:
    dps,groups = functional_test(verbose=False)
    assert groups[0].reference_lookup_symbol() == "F m -3 m"
  assert test_simple_sampler()
  assert test_reduction()
  print("OK")

tst_list = (
  #"$D/simage/tst.py",
  "$D/dps_core/tst_iotbx_converter.py",
  "$D/diffraction/tst_predict.py",
  "$D/diffraction/tst_ewald_sphere.py",
  "$D/diffraction/tst_partial_derivatives.py",
  "$D/diffraction/fastbragg/tst_bragg_minimal.py",
  "$D/sublattice_support/tst_sublattice.py",
  run,
  )

def run_standalones():
  build_dir = libtbx.env.under_build("rstbx")
  dist_dir = libtbx.env.dist_path("rstbx")

  test_utils.run_tests(build_dir, dist_dir, tst_list)

if (__name__ == "__main__"):
  run()
  run_standalones()


 *******************************************************************************


 *******************************************************************************
rstbx/simage/__init__.py
from __future__ import absolute_import, division, print_function
import boost_adaptbx.boost.python as bp
ext = bp.import_ext("rstbx_simage_ext")
from rstbx_simage_ext import *


 *******************************************************************************


 *******************************************************************************
rstbx/simage/create.py
from __future__ import absolute_import, division, print_function
phil_str = """\
base36_timestamp = None
  .type = str
pdb_id = None
  .type = str
pdb_file = None
  .type = path
reset_b_factors_value = None
  .type = float
change_of_basis_op_to_niggli_cell = None
  .type = str
unit_cell = None
  .type = unit_cell
intensity_symmetry = None
  .type = space_group
lattice_symmetry = None
  .type = space_group
lattice_symmetry_max_delta = 1.4
  .type = float
anomalous_flag = True
  .type = bool
euler_angles_xyz = 0 0 0
  .type = floats(size=3)
crystal_rotation_matrix = None
  .type = floats(size=9)
wavelength = 1
  .type = float
wavelength_2 = None
  .type = float
d_min = 2
  .type = float
ewald_proximity = 0.0025
  .type = float
signal_max = 60000
  .type = int
point_spread = 6
  .type = int
gaussian_falloff_scale = 4
  .type = float
noise {
  max = 10
    .type = int
  random_seed = 0
    .type = int
}
detector {
  distance = None
    .type = float
  size = 200 200
    .type = floats(size=2)
  pixels = 1000 1000
    .type = ints(size=2)
  use_corners = False
    .type = bool
}
force_unit_spot_intensities = False
  .type = bool
"""

def compute_detector_d_min(work_params):
  dsx, dsy = work_params.detector.size
  half_diag = (dsx**2 + dsy**2)**0.5 / 2
  import math
  theta_rad = math.atan2(half_diag, work_params.detector.distance) / 2
  assert theta_rad != 0
  denom = 2 * math.sin(theta_rad)
  assert denom != 0
  return round(work_params.wavelength / denom, 2)

def compute_detector_distance(work_params):
  sin_theta = work_params.wavelength / (2 * work_params.d_min)
  import math
  two_theta_rad = math.asin(sin_theta) * 2
  two_theta = two_theta_rad * 180 / math.pi
  if (two_theta > 89):
    raise RuntimeError(
      "two_theta = %.2f degrees (limit is 89 degrees)" % two_theta)
  tan_two_theta = math.tan(two_theta_rad)
  if (work_params.detector.use_corners):
    dsx, dsy = work_params.detector.size
    half_diag = (dsx**2 + dsy**2)**0.5 / 2
    limit = half_diag
  else:
    half_detector = min(work_params.detector.size) / 2
    limit = half_detector
  return int(limit / tan_two_theta)

def adjust_unit_cell_and_intensity_symmetry(work_params):
  assert work_params.change_of_basis_op_to_niggli_cell is None
  assert work_params.lattice_symmetry is None
  unit_cell = work_params.unit_cell
  intensity_symmetry = work_params.intensity_symmetry
  assert unit_cell is not None
  if (intensity_symmetry is not None):
    intensity_symmetry = intensity_symmetry.group()
    assert intensity_symmetry.is_compatible_unit_cell(unit_cell)
    from cctbx import crystal
    cb_op = crystal.symmetry(
      unit_cell=unit_cell,
      space_group=intensity_symmetry).change_of_basis_op_to_niggli_cell()
  else:
    cb_op = unit_cell.change_of_basis_op_to_niggli_cell()
  unit_cell = unit_cell.change_basis(cb_op)
  if (intensity_symmetry is None):
    g = h = unit_cell.lattice_symmetry_group(
      max_delta=work_params.lattice_symmetry_max_delta)
  else:
    h = intensity_symmetry \
      .change_basis(cb_op) \
      .build_derived_acentric_group() \
      .build_derived_reflection_intensity_group(anomalous_flag=True)
    assert h.is_compatible_unit_cell(unit_cell)
    g = unit_cell.lattice_symmetry_group(
      max_delta=work_params.lattice_symmetry_max_delta)
  assert not g.is_centric()
  assert not h.is_centric()
  work_params.change_of_basis_op_to_niggli_cell = str(cb_op)
  work_params.unit_cell = g.average_unit_cell(unit_cell)
  work_params.intensity_symmetry = h.info()
  work_params.lattice_symmetry = g.info()

def process_args(args, extra_phil_str="", out=None):
  if (out is None):
    import sys
    out = sys.stdout
  import iotbx.phil
  import os
  op = os.path
  master_phil = iotbx.phil.parse(input_string=phil_str+extra_phil_str)
  work_phil = master_phil.command_line_argument_interpreter() \
    .process_and_fetch(args=args)
  work_params = work_phil.extract()
  if (work_params.base36_timestamp is None):
    import libtbx.utils
    work_params.base36_timestamp = libtbx.utils.base36_timestamp()
  if (work_params.pdb_id is None and work_params.pdb_file is None):
    if (work_params.unit_cell is None):
      for cell_sym in [
            work_params.lattice_symmetry,
            work_params.intensity_symmetry]:
        if (cell_sym is None):
          continue
        work_params.unit_cell = cell_sym.primitive_setting() \
          .any_compatible_unit_cell(volume=50**3)
        work_params.lattice_symmetry = None
        break
      else:
        from cctbx import uctbx
        work_params.unit_cell = uctbx.unit_cell((48,58,50,85,95,105))
  else:
    import iotbx.pdb
    pdb_inp = iotbx.pdb.input(
      file_name=work_params.pdb_file,
      pdb_id=work_params.pdb_id)
    assert pdb_inp.source_info().startswith("file ")
    work_params.pdb_file = pdb_inp.source_info()[5:]
    crystal_symmetry = pdb_inp.crystal_symmetry()
    print("Crystal symmetry from PDB file:", file=out)
    crystal_symmetry.show_summary(f=out, prefix="  ")
    print(file=out)
    assert crystal_symmetry.unit_cell() is not None
    assert crystal_symmetry.space_group_info() is not None
    if (work_params.unit_cell is None):
      work_params.unit_cell = crystal_symmetry.unit_cell()
    if (work_params.intensity_symmetry is None):
      work_params.intensity_symmetry = crystal_symmetry.space_group_info()
  adjust_unit_cell_and_intensity_symmetry(work_params)
  if (work_params.d_min is None):
    work_params.d_min = compute_detector_d_min(work_params)
  else:
    work_params.detector.distance = compute_detector_distance(work_params)
  work_phil = master_phil.format(python_object=work_params)
  work_phil.show(out=out)
  print(file=out)
  work_params = work_phil.extract()
  work_params.__inject__("phil_master", work_phil)
  return work_params

def build_i_calc(work_params):
  from scitbx.array_family import flex
  d_min = work_params.d_min
  if (work_params.pdb_file is None):
    miller_set = work_params.unit_cell \
      .complete_miller_set_with_lattice_symmetry(
        d_min=d_min,
        anomalous_flag=True).expand_to_p1()
    if (work_params.intensity_symmetry is not None):
      miller_set = miller_set.customized_copy(
        space_group_info=work_params.intensity_symmetry,
        anomalous_flag=work_params.anomalous_flag).unique_under_symmetry()
    mt = flex.mersenne_twister(seed=work_params.noise.random_seed)
    i_calc_asu = miller_set.array(
      data=mt.random_double(size=miller_set.indices().size()))
  else:
    import iotbx.pdb
    pdb_inp = iotbx.pdb.input(file_name=work_params.pdb_file)
    xs = pdb_inp.xray_structure_simple().change_basis(
      cb_op=work_params.change_of_basis_op_to_niggli_cell)
    assert xs.unit_cell().is_similar_to(other=work_params.unit_cell)
    _ = work_params.reset_b_factors_value
    if (_ is not None):
      from cctbx import adptbx
      u_iso = adptbx.b_as_u(_)
      xs.convert_to_isotropic()
      for sc in xs.scatterers():
        sc.u_iso = u_iso
    miller_set = work_params.unit_cell \
      .complete_miller_set_with_lattice_symmetry(
        d_min=d_min,
        anomalous_flag=work_params.anomalous_flag) \
          .expand_to_p1().customized_copy(
            space_group_info=xs.space_group_info()) \
              .unique_under_symmetry() \
              .remove_systematic_absences() \
              .map_to_asu()
    i_calc_asu = miller_set.structure_factors_from_scatterers(
      xray_structure=xs).f_calc().intensities()
    if (i_calc_asu.data().size() != 0):
      i_calc_max = flex.max(i_calc_asu.data())
      if (i_calc_max > 0):
        i_calc_asu = i_calc_asu.array(data=i_calc_asu.data() * (1/i_calc_max))
    i_calc_asu = i_calc_asu.customized_copy(
      space_group_info=i_calc_asu.space_group()
        .build_derived_reflection_intensity_group(anomalous_flag=True).info()) \
      .map_to_asu() \
      .complete_array(new_data_value=0)
  i_calc_asu = i_calc_asu.sort(by_value="resolution")
  assert not i_calc_asu.space_group().is_centric()
  assert i_calc_asu.space_group().n_ltr() == 1
  assert i_calc_asu.space_group_info().type().is_symmorphic()
  if (work_params.force_unit_spot_intensities):
    i_calc_asu = i_calc_asu.array(
      data=flex.double(i_calc_asu.indices().size(), 1))
  i_asu_array = i_calc_asu.customized_copy(
    data=flex.size_t_range(i_calc_asu.indices().size()))
  if (not i_asu_array.anomalous_flag()):
    i_asu_array = i_asu_array.generate_bijvoet_mates()
  i_asu_array = i_asu_array.expand_to_p1()
  asu_iselection = i_asu_array.data()
  i_calc_p1_anom = i_asu_array.customized_copy(
    data=i_calc_asu.data().select(asu_iselection))
  from libtbx import group_args
  return group_args(
    asu=i_calc_asu,
    asu_iselection=asu_iselection,
    p1_anom=i_calc_p1_anom)

def add_noise(work_params, pixels):
  if (work_params.noise.max > 0):
    from scitbx.array_family import flex
    mt = flex.mersenne_twister(seed=work_params.noise.random_seed)
    noise = mt.random_size_t(
      size=pixels.size(),
      modulus=work_params.noise.max).as_int()
    noise.reshape(pixels.accessor())
    pixels += noise

def compute(
      work_params,
      use_wavelength_2=False,
      store_miller_index_i_seqs=False,
      store_spots=False,
      store_signals=False,
      set_pixels=False):
  i_calc = build_i_calc(work_params)
  from scitbx.math.euler_angles import xyz_matrix
  crystal_rotation_matrix = xyz_matrix(*work_params.euler_angles_xyz)
  work_params.crystal_rotation_matrix = crystal_rotation_matrix
  if (not use_wavelength_2):
    wavelength = work_params.wavelength
  else:
    wavelength = work_params.wavelength_2
  from rstbx.simage import image_simple
  return i_calc, image_simple(
    store_miller_index_i_seqs=store_miller_index_i_seqs,
    store_spots=store_spots,
    store_signals=store_signals,
    set_pixels=set_pixels).compute(
      unit_cell=i_calc.p1_anom.unit_cell(),
      miller_indices=i_calc.p1_anom.indices(),
      spot_intensity_factors=i_calc.p1_anom.data(),
      crystal_rotation_matrix=crystal_rotation_matrix,
      ewald_radius=1/wavelength,
      ewald_proximity=work_params.ewald_proximity,
      signal_max=work_params.signal_max,
      detector_distance=work_params.detector.distance,
      detector_size=work_params.detector.size,
      detector_pixels=work_params.detector.pixels,
      point_spread=work_params.point_spread,
      gaussian_falloff_scale=work_params.gaussian_falloff_scale)

def compute_image(work_params, use_wavelength_2=False):
  _, image_info = compute(
    work_params=work_params,
    use_wavelength_2=use_wavelength_2,
    set_pixels=True)
  pixels = image_info.pixels
  add_noise(work_params, pixels=pixels)
  return pixels


 *******************************************************************************


 *******************************************************************************
rstbx/simage/explore_completeness.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import libtbx
import sys
from six.moves import zip

class stats_manager(libtbx.slots_getstate_setstate):

  __slots__ = [
    "i_calc",
    "use_symmetry",
    "n_indices",
    "completeness_history",
    "min_count_history",
    "counts",
    "currently_zero",
    "new_0"]

  def __init__(O, n_reserve, i_calc, use_symmetry):
    from cctbx.array_family import flex
    O.i_calc = i_calc
    O.use_symmetry = use_symmetry
    if (use_symmetry):
      O.n_indices = O.i_calc.asu.indices().size()
    else:
      O.n_indices = O.i_calc.p1_anom.indices().size()
    O.completeness_history = flex.double()
    O.completeness_history.reserve(n_reserve)
    O.completeness_history.append(0)
    O.min_count_history = flex.size_t()
    O.min_count_history.reserve(n_reserve)
    O.min_count_history.append(0)
    O.counts = flex.size_t(O.n_indices, 0)
    O.currently_zero = O.n_indices
    O.new_0 = None

  def update(O, miller_index_i_seqs):
    from cctbx.array_family import flex
    if (O.use_symmetry):
      isel = O.i_calc.asu_iselection.select(miller_index_i_seqs)
    else:
      isel = miller_index_i_seqs
    previously_zero = O.counts.increment_and_track_up_from_zero(
      iselection=isel)
    O.new_0 = O.currently_zero - previously_zero
    O.completeness_history.append(1-O.new_0/O.n_indices)
    O.min_count_history.append(flex.min(O.counts))
    assert O.new_0 >= 0
    if (O.new_0 == 0 and O.currently_zero != 0):
      print("Complete with %d images." % (len(O.completeness_history)-1))
      print()
    O.currently_zero = O.new_0

  def report(O, plot=None, xy_prefix=None):
    from cctbx.array_family import flex
    print("Number of shots:", O.completeness_history.size()-1)
    print()
    print("Histogram of counts per reflection:")
    flex.histogram(O.counts.as_double(), n_slots=8).show(
      prefix="  ", format_cutoffs="%7.0f")
    print()
    print("Observations per reflection:")
    flex.show_count_stats(counts=O.counts, prefix="  ")
    print("  Median:", int(flex.median(O.counts.as_double())+0.5))
    print()
    sys.stdout.flush()
    if (xy_prefix is None):
      xy_prefix = ""
    elif (len(xy_prefix) != 0):
      xy_prefix = xy_prefix + "_"
    def dump_xy(name, array):
      f = open(xy_prefix + "%s.xy" % name, "w")
      for i,c in enumerate(array):
        print(i, c, file=f)
    dump_xy("completeness_history", O.completeness_history)
    dump_xy("min_count_history", O.min_count_history)
    if (O.use_symmetry): _ = O.i_calc.asu
    else:                _ = O.i_calc.p1_anom
    _ = _.customized_copy(data=O.counts).sort(by_value="resolution")
    sym_factors = _.space_group().order_p()
    if (not O.i_calc.asu.anomalous_flag()):
      sym_factors *= 2
    sym_factors /= _.multiplicities().data()
    counts_sorted_by_resolution = _.data().as_int() * sym_factors
    dump_xy("counts_sorted_by_resolution", counts_sorted_by_resolution)
    dump_xy("d_spacings_sorted_by_resolution", _.d_spacings().data())
    if (plot == "completeness"):
      from libtbx import pyplot
      fig = pyplot.figure()
      ax = fig.add_subplot(1, 1, 1)
      _ = O.completeness_history
      nx = _.size()
      ax.plot(range(nx), _, "r-")
      ax.axis([0, nx, 0, 1])
      pyplot.show()
    elif (plot == "redundancy"):
      from libtbx import pyplot
      fig = pyplot.figure()
      ax = fig.add_subplot(1, 1, 1)
      _ = counts_sorted_by_resolution
      ax.plot(range(len(_)), _, "r-")
      ax.axis([-_.size()*0.05, _.size()*1.05, 0, None])
      pyplot.show()
    elif (plot is not None):
      raise RuntimeError('Unknown plot type: "%s"' % plot)

def kirian_delta_vs_ewald_proximity(
      unit_cell,
      miller_indices,
      crystal_rotation_matrix,
      ewald_radius,
      d_min,
      detector_distance,
      detector_size,
      detector_pixels):
  from scitbx import matrix
  from libtbx.math_utils import nearest_integer
  cr = matrix.sqr(crystal_rotation_matrix)
  a_matrix = cr * matrix.sqr(unit_cell.fractionalization_matrix()).transpose()
  a_inv = a_matrix.inverse()
  dsx, dsy = detector_size
  dpx, dpy = detector_pixels
  deltas = [[] for _ in range(len(miller_indices))]
  h_lookup = {}
  for i,h in enumerate(miller_indices):
    h_lookup[h] = i
  for pi in range(dpx):
    for pj in range(dpy):
      cx = ((pi + 0.5) / dpx - 0.5) * dsx
      cy = ((pj + 0.5) / dpy - 0.5) * dsy
      lo = matrix.col((cx, cy, -detector_distance))
      ko = lo.normalize() * ewald_radius
      ki = matrix.col((0,0,-ewald_radius))
      dk = ki - ko
      h_frac = a_inv * dk
      h = matrix.col([nearest_integer(_) for _ in h_frac])
      if (h.elems == (0,0,0)):
        continue
      g_hkl = a_matrix * h
      delta = (dk - g_hkl).length()
      i = h_lookup.get(h.elems)
      if (i is None):
        assert unit_cell.d(h) < d_min
      else:
        deltas[i].append(delta)
  def ewald_proximity(h): # compare with code in image_simple.hpp
    rv = matrix.col(unit_cell.reciprocal_space_vector(h))
    rvr = cr * rv
    rvre = matrix.col((rvr[0], rvr[1], rvr[2]+ewald_radius))
    rvre_len = rvre.length()
    return abs(1 - rvre_len / ewald_radius)
  def write_xy():
    fn_xy = "kirian_delta_vs_ewald_proximity.xy"
    print("Writing file:", fn_xy)
    f = open(fn_xy, "w")
    print("""\
@with g0
@ s0 symbol 1
@ s0 symbol size 0.1
@ s0 line type 0""", file=f)
    for h, ds in zip(miller_indices, deltas):
      if (len(ds) != 0):
        print(min(ds), ewald_proximity(h), file=f)
    print("&", file=f)
    print()
  write_xy()
  STOP()

def simulate(work_params, i_calc):
  from rstbx.simage import image_simple
  from cctbx.array_family import flex
  n_shots = work_params.number_of_shots
  stats = stats_manager(
    n_reserve=max(n_shots, 1000000),
    i_calc=i_calc,
    use_symmetry=work_params.use_symmetry)
  mc_target = work_params.min_count_target
  def update_stats(miller_index_i_seqs):
    stats.update(miller_index_i_seqs)
    if (n_shots is not None and stats.min_count_history.size()-1 < n_shots):
      return False
    if (mc_target is not None and stats.min_count_history[-1] < mc_target):
      return False
    if (stats.new_0 != 0 and n_shots is None and mc_target is None):
      return False
    return True
  def get_miller_index_i_seqs(i_img, parallel=True):
    mt = flex.mersenne_twister(seed=work_params.noise.random_seed+i_img)
    crystal_rotation = mt.random_double_r3_rotation_matrix_arvo_1992()
    if (work_params.kirian_delta_vs_ewald_proximity):
      kirian_delta_vs_ewald_proximity(
        unit_cell=i_calc.p1_anom.unit_cell(),
        miller_indices=i_calc.p1_anom.indices(),
        crystal_rotation_matrix=crystal_rotation,
        ewald_radius=1/work_params.wavelength,
        d_min=work_params.d_min,
        detector_distance=work_params.detector.distance,
        detector_size=work_params.detector.size,
        detector_pixels=work_params.detector.pixels)
    img = image_simple(
        store_miller_index_i_seqs=True,
        store_signals=True).compute(
      unit_cell=i_calc.p1_anom.unit_cell(),
      miller_indices=i_calc.p1_anom.indices(),
      spot_intensity_factors=None,
      crystal_rotation_matrix=crystal_rotation,
      ewald_radius=1/work_params.wavelength,
      ewald_proximity=work_params.ewald_proximity,
      signal_max=1,
      detector_distance=work_params.detector.distance,
      detector_size=work_params.detector.size,
      detector_pixels=work_params.detector.pixels,
      point_spread=work_params.point_spread,
      gaussian_falloff_scale=work_params.gaussian_falloff_scale)
    result = img.miller_index_i_seqs
    if (work_params.usable_partiality_threshold is not None):
      result = result.select(
        img.signals > work_params.usable_partiality_threshold)
    if (parallel):
      return result.copy_to_byte_str()
    return result
  i_img = 0
  stop = False
  if (not work_params.multiprocessing):
    while (not stop):
      try:
        miller_index_i_seqs = get_miller_index_i_seqs(i_img, parallel=False)
      except KeyboardInterrupt:
        print()
        print("KeyboardInterrupt")
        print()
        stop = True
      else:
        i_img += 1
        stop = update_stats(miller_index_i_seqs)
  else:
    from libtbx import easy_mp
    pool = easy_mp.Pool(fixed_func=get_miller_index_i_seqs)
    try:
      print("multiprocessing pool size:", pool.processes)
      print()
      sys.stdout.flush()
      while (not stop):
        next_i_img = i_img + pool.processes
        args = range(i_img, next_i_img)
        mp_results = pool.map_fixed_func(iterable=args)
        i_img = next_i_img
        for miller_index_i_seqs in mp_results:
          assert miller_index_i_seqs is not None
          miller_index_i_seqs = flex.size_t_from_byte_str(
            byte_str=miller_index_i_seqs)
          stop = update_stats(miller_index_i_seqs)
          if (stop):
            break
    finally:
      pool.close()
      pool.join()
  return stats

def run(args):
  from libtbx.utils import show_times_at_exit
  show_times_at_exit()
  from rstbx.simage import create
  work_params = create.process_args(
    args=args,
    extra_phil_str="""\
use_symmetry = False
  .type = bool
number_of_shots = None
  .type = int
min_count_target = None
  .type = int
usable_partiality_threshold = 0.1
  .type = float
kirian_delta_vs_ewald_proximity = False
  .type = bool
multiprocessing = False
  .type = bool
xy_prefix = None
  .type = str
plot = completeness redundancy
  .type = choice
""")
  i_calc = create.build_i_calc(work_params)
  i_calc.p1_anom.show_comprehensive_summary()
  print()
  sys.stdout.flush()
  stats = simulate(work_params, i_calc)
  stats.report(plot=work_params.plot, xy_prefix=work_params.xy_prefix)


 *******************************************************************************


 *******************************************************************************
rstbx/simage/integrate_crude.py
from __future__ import absolute_import, division, print_function
from six.moves import range
def predict_spot_positions(
      work_params,
      miller_indices,
      unit_cell,
      crystal_rotation):
  from rstbx.simage import image_simple
  image = image_simple(
    store_spots=True,
    store_miller_index_i_seqs=True).compute(
      unit_cell=unit_cell,
      miller_indices=miller_indices,
      spot_intensity_factors=None,
      crystal_rotation_matrix=crystal_rotation,
      ewald_radius=1/work_params.wavelength,
      ewald_proximity=work_params.ewald_proximity,
      signal_max=work_params.signal_max,
      detector_distance=work_params.detector.distance,
      detector_size=work_params.detector.size,
      detector_pixels=work_params.detector.pixels,
      point_spread=work_params.point_spread,
      gaussian_falloff_scale=work_params.gaussian_falloff_scale)
  return image.spots, image.miller_index_i_seqs

def sum_pixels(pixels, point_spread_inner, point_spread_outer, center):
  circle_radius_sq_inner = point_spread_inner**2 / 4
  circle_radius_sq_outer = point_spread_outer**2 / 4
  dpx, dpy = pixels.focus()
  pxf, pyf, _ = center
  pxi = int(pxf)
  pyi = int(pyf)
  pxb = pxi - point_spread_outer // 2
  pyb = pyi - point_spread_outer // 2
  if (point_spread_outer % 2 == 0):
    if (pxf - pxi > 0.5): pxb += 1
    if (pyf - pyi > 0.5): pyb += 1
  n_outer = 0
  n_inner = 0
  sum_outer = 0
  sum_inner = 0
  for i in range(0, point_spread_outer+1):
    pi = pxb + i
    if (pi < 0 or pi >= dpx): return 0
    for j in range(0, point_spread_outer+1):
      pj = pyb + j
      if (pj < 0 or pj >= dpy): return 0
      pcx = (pi + 0.5) - pxf
      pcy = (pj + 0.5) - pyf
      pc_sq = pcx*pcx + pcy*pcy
      if (pc_sq > circle_radius_sq_outer): continue
      c = pixels[pi,pj]
      if (pc_sq > circle_radius_sq_inner):
        n_outer += 1
        sum_outer += c
      else:
        n_inner += 1
        sum_inner += c
  assert n_outer != 0
  return max(0, sum_inner - sum_outer * n_inner / n_outer)

def collect_spot_intensities(
      pixels,
      spot_positions,
      point_spread_inner,
      point_spread_outer):
  from scitbx.array_family import flex
  raw_sums = flex.double()
  raw_sums.reserve(spot_positions.size())
  for position in spot_positions:
    raw_sums.append(sum_pixels(
      pixels=pixels,
      point_spread_inner=point_spread_inner,
      point_spread_outer=point_spread_outer,
      center=position))
  return raw_sums


 *******************************************************************************


 *******************************************************************************
rstbx/simage/refine_uc_cr.py
from __future__ import absolute_import, division, print_function
from six.moves import range
from six.moves import zip
class InfeasibleError(RuntimeError): pass

class refinery(object):

  def __init__(O,
        work_params,
        spots_xy0,
        miller_indices,
        unit_cell,
        crystal_rotation_uq):
    assert spots_xy0.size() == miller_indices.size()
    O.work_params = work_params
    O.spots_xy0 = spots_xy0
    O.miller_indices = miller_indices
    O.uq_scale = 100 # to balance gradients
    O.average_unit_cell = O.work_params.lattice_symmetry.group() \
      .average_unit_cell
    unit_cell = O.average_unit_cell(unit_cell)
    from scitbx.array_family import flex
    O.x = flex.double(
        unit_cell.parameters()
      + (crystal_rotation_uq * O.uq_scale).elems)
    assert O.x.size() == 10
    O.initial_functional = None
    import scitbx.lbfgs
    scitbx.lbfgs.run(target_evaluator=O)
    O.final_functional = O.compute_functional_and_gradients(
      functional_only=True)
    del O.average_unit_cell

  def compute_functional_and_gradients(O, functional_only=False):
    from cctbx import uctbx
    from scitbx import matrix
    def get_f():
      vals = tuple(O.x[:6])
      try:
        O.unit_cell = uctbx.unit_cell(vals)
      except RuntimeError as e:
        raise InfeasibleError(str(e))
      vals = matrix.col(O.x[6:]) / O.uq_scale
      try:
        vals = vals.normalize()
      except ZeroDivisionError as e:
        raise InfeasibleError(str(e))
      if (O.average_unit_cell is not None):
        O.unit_cell = O.average_unit_cell(O.unit_cell)
      O.crystal_rotation = vals.unit_quaternion_as_r3_rotation_matrix()
      from rstbx.simage import image_simple
      O.predicted_spots = image_simple(
        apply_detector_clipping=False,
        apply_proximity_filter=False,
        store_spots=True).compute(
          unit_cell=O.unit_cell,
          miller_indices=O.miller_indices,
          spot_intensity_factors=None,
          crystal_rotation_matrix=O.crystal_rotation,
          ewald_radius=1/O.work_params.wavelength,
          ewald_proximity=O.work_params.ewald_proximity,
          signal_max=O.work_params.signal_max,
          detector_distance=O.work_params.detector.distance,
          detector_size=O.work_params.detector.size,
          detector_pixels=O.work_params.detector.pixels,
          point_spread=O.work_params.point_spread,
          gaussian_falloff_scale=O.work_params.gaussian_falloff_scale).spots
      assert O.predicted_spots.size() == O.spots_xy0.size()
      return O.spots_xy0.rms_difference(O.predicted_spots)
    f = get_f()
    if (O.initial_functional is None):
      O.initial_functional = f
    if (functional_only):
      return f
    from scitbx.array_family import flex
    g = flex.double()
    g.reserve(10)
    eps = 1e-5
    for i in range(10):
      xi = O.x[i]
      O.x[i] = xi+eps
      f_eps = get_f()
      O.x[i] = xi
      g.append((f_eps-f)/eps)
    return f, g

  def outlier_removal(O, outlier_factor=3):
    if (O.spots_xy0.size() < 3): return None
    distances = (O.spots_xy0 - O.predicted_spots).dot()**0.5
    from scitbx.array_family import flex
    perm = flex.sort_permutation(distances, reverse=True)
    if (distances[perm[0]] > distances[perm[1]] * outlier_factor):
      return perm[1:]
    return None

  def show_summary(O):
    print("refinement target:")
    print("  initial: %.6g" % O.initial_functional)
    print("    final: %.6g" % O.final_functional)
    print("refined:")
    print(O.unit_cell)
    print(O.crystal_rotation)
    return O

  def show_distances(O):
    if (O.spots_xy0.size() == 0):
      return
    distances = (O.spots_xy0 - O.predicted_spots).dot()**0.5
    from scitbx.array_family import flex
    perm = flex.sort_permutation(distances, reverse=True)
    from itertools import count
    d0 = distances[perm[0]]
    for i,h,d in zip(
          count(),
          O.miller_indices.select(perm),
          distances.select(perm)):
      if (i >= 3 and (i >= 12 or d < d0*0.1)):
        j = perm.size() - i
        if (j > 1):
          print("... remaining %d distances not shown" % j)
          break
      print("%3d %3d %3d" % h, " %7.5f" % d)
    return O

def refine(
      work_params,
      spots,
      good_i_seqs,
      miller_indices,
      unit_cell,
      crystal_rotation):
  from scitbx.array_family import flex
  spots_xy0 = flex.vec3_double()
  for spot in spots.select(good_i_seqs):
    x,y = spot.ctr_mass_x()+0.5, spot.ctr_mass_y()+0.5
    spots_xy0.append((x,y,0))
  refined = refinery(
    work_params=work_params,
    spots_xy0=spots_xy0,
    miller_indices=miller_indices,
    unit_cell=unit_cell,
    crystal_rotation_uq=crystal_rotation
      .r3_rotation_matrix_as_unit_quaternion())
  refined.show_summary().show_distances()
  print()
  while True:
    remaining_sel = refined.outlier_removal()
    if (remaining_sel is None):
      break
    print("Removing one outlier and re-refining.")
    print()
    refined = refinery(
      work_params=refined.work_params,
      spots_xy0=refined.spots_xy0.select(remaining_sel),
      miller_indices=refined.miller_indices.select(remaining_sel),
      unit_cell=refined.unit_cell,
      crystal_rotation_uq=refined.crystal_rotation
        .r3_rotation_matrix_as_unit_quaternion())
    refined.show_summary().show_distances()
    print()
  return refined


 *******************************************************************************


 *******************************************************************************
rstbx/simage/run_labelit_index.py
from __future__ import absolute_import, division, print_function
def get_spots_high_resolution(work_params, spots):
  from scitbx.array_family import flex
  import math
  dsx,dsy = work_params.detector.size
  dpx,dpy = work_params.detector.pixels
  dists = spots.ctr_mass_distances_from_direct_beam(
    detector_size=(dsx,dsy),
    detector_pixels=(dpx,dpy),
    xy_beam=(dsx/2-0.5,dsy/2-0.5))
  def resolution(distance_from_direct_beam):
    theta = 0.5 * math.atan2(
      distance_from_direct_beam,
      work_params.detector.distance)
    den = 2 * math.sin(theta)
    if (den == 0):
      return None
    return work_params.wavelength / den
  dists_max = flex.max(dists)
  result = resolution(dists_max)
  print("Highest-resolution of spots: %.3f" % result)
  print()
  return result

def process(work_params, spots, sampling_resolution_factor=0.5):
  import libtbx.load_env
  libtbx.env.require_module("labelit")
  spots_high_res = get_spots_high_resolution(
    work_params=work_params, spots=spots)
  if (spots_high_res is None):
    return
  uc = work_params.unit_cell
  uc_max_length = max(uc.parameters()[0:3])
  sampling = sampling_resolution_factor * spots_high_res / uc_max_length
  #
  from labelit.preferences import labelit_commands
  labelit_commands.model_refinement_minimum_N = 10
  labelit_commands.target_cell = uc
  print("labelit_commands.target_cell:", labelit_commands.target_cell)
  #
  from scitbx.array_family import flex
  raw_spot_input = flex.vec3_double()
  dsx,dsy = work_params.detector.size
  dpx,dpy = work_params.detector.pixels
  sopx,sopy = dsx/dpx, dsy/dpy
  for spot in spots: # XXX C++
    x, y = spot.ctr_mass_x(), spot.ctr_mass_y()
    raw_spot_input.append((x*sopx+0.5, y*sopy+0.5, 0.0))
  #
  from labelit.dptbx import AutoIndexEngine, Parameters
  from iotbx.detectors.context.endstation import EndStation
  ai = AutoIndexEngine(EndStation(), sampling)
  ai.setData(raw_spot_input)
  ai_params = Parameters(
    xbeam=dsx/2,
    ybeam=dsy/2,
    distance=work_params.detector.distance,
    twotheta=0.)
  ai.setBase(ai_params)
  ai.setWavelength(work_params.wavelength)
  ai.setMaxcell(1.25*max(uc.parameters()[0:3]))
  ai.setDeltaphi(0.0)
  f = ai.film_to_camera()
  c = ai.camera_to_xyz()
  #
  from labelit.dptbx.sampling import HemisphereSampler
  hem_samp = HemisphereSampler(
    max_grid=sampling,
    characteristic_grid=sampling,
    quick_grid=0.016) # all grid parameters in radians
  hem_samp.hemisphere(
    ai=ai, size=30, cutoff_divisor=4.) # never change these parameters
  #
  from labelit.dptbx.basis_choice import SelectBasisMetaprocedure
  pd = {}
  sbm = SelectBasisMetaprocedure(
    input_index_engine=ai,
    input_dictionary=pd,
    opt_rawframes=False,
    opt_target=True,
    reduce_target=False)
  ai.fixsign()
  return ai

def report_uc_cr(ai):
  miller_indices = ai.hklobserved()
  good_i_seqs = ai.get_observed_spot_i_seqs_good_only()
  assert miller_indices.size() == good_i_seqs.size()
  print("Number of spots indexed:", miller_indices.size())
  co = ai.getOrientation()
  print("Recovered unit cell:", co.unit_cell())
  print("Recovered crystal_rotation_matrix:")
  print(co.crystal_rotation_matrix())
  print()
  return good_i_seqs, miller_indices, co


 *******************************************************************************


 *******************************************************************************
rstbx/simage/run_spotfinder.py
from __future__ import absolute_import, division, print_function
def process(work_params, pixels, show_spots=True):
  from spotfinder import core_toolbox
  import time
  t0 = time.time()
  options = ""
  report_overloads = True
  dobj = core_toolbox.w_Distl(options, report_overloads)
  dsx,dsy = work_params.detector.size
  dpx,dpy = work_params.detector.pixels
  pixel_size = dsx / dpx
  assert pixel_size == dsy / dpy
  dobj.setspotimg(
    pixel_size=pixel_size,
    distance=work_params.detector.distance,
    wavelength=work_params.wavelength,
    xbeam=dsx/2,
    ybeam=dsy/2,
    rawdata=pixels,
    peripheral_margin=work_params.spotfinder.peripheral_margin,
    saturation=work_params.signal_max+2*work_params.noise.max)
  dobj.set_tiling("")
  dobj.set_scanbox_windows(work_params.spotfinder.scanbox_windows)
  dobj.parameter_guarantees()
  dobj.get_underload()
  dobj.pxlclassify()
  dobj.search_icerings()
  dobj.search_maximas()
  dobj.search_spots()
  dobj.search_overloadpatches()
  dobj.finish_analysis()
  print("Time spot finding: %.2f" % (time.time()-t0))
  print("Number of spots:", dobj.spots.size())
  if (show_spots):
    print("        Pixel")
    print("   Center of mass     Weight")
    for spot in dobj.spots:
      print("(%8.3f, %8.3f)  %8.2e" % (
        spot.ctr_mass_x(), spot.ctr_mass_y(), spot.total_mass))
  print()
  return dobj.spots

def process_args(args, extra_phil_str=""):
  import libtbx.load_env
  libtbx.env.require_module("spotfinder")
  import spotfinder
  from rstbx.simage import create
  return create.process_args(
    args=args,
    extra_phil_str=spotfinder.phil_str+extra_phil_str)


 *******************************************************************************


 *******************************************************************************
rstbx/simage/solver.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import os
from six.moves import zip
op = os.path
import sys
import time

_show_vm_info_time = time.time()

def show_vm_info(msg):
  print(msg)
  from libtbx import introspection
  introspection.virtual_memory_info().show(prefix="  ", show_max=True)
  global _show_vm_info_time
  t = time.time()
  print("  time since previous: %.2f seconds" % (t-_show_vm_info_time))
  _show_vm_info_time = t
  print()
  sys.stdout.flush()

import libtbx

class image_model(libtbx.slots_getstate_setstate):

  __slots__ = [
    "pixels",
    "spot_positions",
    "spot_intensities",
    "miller_index_i_seqs",
    "unit_cell",
    "crystal_rotation",
    "partialities",
    "scale",
    "i_perm",
    "backup"]

  def __init__(O,
        pixels=None,
        spot_positions=None,
        spot_intensities=None,
        miller_index_i_seqs=None,
        unit_cell=None,
        crystal_rotation=None,
        partialities=None,
        scale=None,
        i_perm=None):
    O.pixels = pixels
    O.spot_positions = spot_positions
    O.spot_intensities = spot_intensities
    O.miller_index_i_seqs = miller_index_i_seqs
    O.unit_cell = unit_cell
    O.crystal_rotation = crystal_rotation
    O.partialities = partialities
    O.scale = scale
    O.i_perm = i_perm
    O.backup = None

  def make_backup(O):
    O.backup = image_model(
      spot_positions=O.spot_positions,
      spot_intensities=O.spot_intensities,
      miller_index_i_seqs=O.miller_index_i_seqs,
      unit_cell=O.unit_cell,
      crystal_rotation=O.crystal_rotation,
      partialities=O.partialities,
      scale=O.scale,
      i_perm=O.i_perm)

  def erase_spot_model(O):
    O.spot_positions = None
    O.spot_intensities = None
    O.miller_index_i_seqs = None
    O.unit_cell = None
    O.crystal_rotation = None
    O.partialities = None
    O.scale = None

  def reset_spot_model(O, other):
    if (other is None):
      O.erase_spot_model()
    else:
      O.spot_positions = other.spot_positions
      O.spot_intensities = other.spot_intensities
      O.miller_index_i_seqs = other.miller_index_i_seqs
      O.unit_cell = other.unit_cell
      O.crystal_rotation = other.crystal_rotation
      O.partialities = other.partialities
      O.scale = other.scale

  def reindex_in_place(O,
        reindexing_assistant=None,
        cb_op=None,
        miller_indices=None):
    assert [reindexing_assistant, cb_op].count(None) == 1
    assert (cb_op is None) == (miller_indices is None)
    if (reindexing_assistant is not None):
      assert O.i_perm is not None
      cb_op = reindexing_assistant.cb_ops[O.i_perm]
    if (not O.unit_cell.is_similar_to(
              other=O.unit_cell.change_basis(cb_op),
              relative_length_tolerance=1e-5,
              absolute_angle_tolerance=1e-3)):
      raise RuntimeError(
        "Unit cell is not compatible with reindexing operation.")
    if (reindexing_assistant is not None):
      assert O.i_perm is not None
      perm = reindexing_assistant.perms[O.i_perm]
      O.miller_index_i_seqs = perm.select(O.miller_index_i_seqs)
    else:
      mi_cb = cb_op.apply(miller_indices.select(O.miller_index_i_seqs))
      from cctbx import miller
      matches = miller.match_indices(miller_indices, mi_cb)
      assert matches.singles(1).size() == 0
      O.miller_index_i_seqs = matches.pairs().column(0)
    from scitbx.array_family import flex
    sort_perm = flex.sort_permutation(data=O.miller_index_i_seqs)
    O.miller_index_i_seqs = O.miller_index_i_seqs.select(sort_perm)
    O.spot_positions = O.spot_positions.select(sort_perm)
    O.spot_intensities = O.spot_intensities.select(sort_perm)
    from scitbx import matrix
    c_cart = matrix.sqr(O.unit_cell.matrix_cart(rot_mx=cb_op.c_inv().r()))
    O.crystal_rotation = (matrix.sqr(O.crystal_rotation) * c_cart).elems
    O.partialities = None
    O.i_perm = 0
    if (O.backup is not None):
      O.backup.i_perm = 0

  def reset_partialities(O, work_params, miller_indices):
    from rstbx.simage import image_simple
    O.partialities = image_simple(
      apply_detector_clipping=False,
      apply_proximity_filter=False,
      store_signals=True).compute(
        unit_cell=O.unit_cell,
        miller_indices=miller_indices.select(O.miller_index_i_seqs),
        spot_intensity_factors=None,
        crystal_rotation_matrix=O.crystal_rotation,
        ewald_radius=1/work_params.wavelength,
        ewald_proximity=work_params.ewald_proximity,
        signal_max=1,
        detector_distance=work_params.detector.distance,
        detector_size=work_params.detector.size,
        detector_pixels=work_params.detector.pixels,
        point_spread=work_params.point_spread,
        gaussian_falloff_scale=work_params.gaussian_falloff_scale).signals
    assert O.partialities.size() == O.miller_index_i_seqs.size()

  def usable(O, partiality_threshold):
    sel = O.partialities > partiality_threshold
    return libtbx.group_args(
      miis = O.miller_index_i_seqs.select(sel),
      esti = O.spot_intensities.select(sel) / O.partialities.select(sel))

  def extract_i_obs_est(O, work_params, miller_indices):
    assert O.partialities is not None
    usable = O.usable(work_params.usable_partiality_threshold)
    from cctbx import crystal
    return crystal.symmetry(
      unit_cell=work_params.unit_cell,
      space_group_symbol="P1").miller_set(
        indices=miller_indices.select(usable.miis),
        anomalous_flag=True).array(
          data=usable.esti)

class miller_image_map(libtbx.slots_getstate_setstate):

  __slots__ = ["miller_indices", "map"]

  def __init__(O, miller_indices):
    O.miller_indices = miller_indices
    O.map = [[] for i in range(O.miller_indices.size())]

  def enter(O, i_img, miller_index_i_seqs):
    map = O.map
    for ii_seq,i_seq in enumerate(miller_index_i_seqs):
      map[i_seq].append((i_img, ii_seq))

  def show_images_per_miller_index(O, first_block_size=20):
    print("Images per Miller index:")
    from libtbx import dict_with_default_0
    counts = dict_with_default_0()
    for iiis in O.map:
      counts[len(iiis)] += 1
    n_seq = O.miller_indices.size()
    have_break = False
    for n_imgs in sorted(counts.keys()):
      if (n_imgs > first_block_size and n_imgs < len(counts)-5):
        if (not have_break):
          have_break = True
          print("        ...")
      else:
        c = counts[n_imgs]
        print("  %6d %6d %8.6f" % (n_imgs, c, c/n_seq))
    print()
    sys.stdout.flush()

def collect_estis(image_mdls_array, iiis, partiality_threshold):
  from scitbx.array_family import flex
  result = flex.double()
  for i_img,ii_seq in iiis:
    im = image_mdls_array[i_img]
    scale = im.scale
    if (scale != 0):
      signal = im.spot_intensities[ii_seq]
      if (signal == 0):
        result.append(0)
      else:
        part = im.partialities[ii_seq]
        if (part != 0 and part >= partiality_threshold):
          result.append(signal / part / scale)
  return result

class image_models(libtbx.slots_getstate_setstate):

  __slots__ = ["miller_indices", "array", "miller_image_map"]

  def __init__(O, miller_indices, array, miller_image_map=None):
    O.miller_indices = miller_indices
    O.array = array
    O.miller_image_map = miller_image_map

  def size(O):
    return len(O.array)

  def check_i_perm_vs_backup(O, reindexing_assistant):
    im0_i_perm = O.array[0].backup.i_perm
    for im in O.array:
      assert im.i_perm is not None
      assert im.i_perm == reindexing_assistant.i_j_inv_multiplication_table[
        im0_i_perm][
        im.backup.i_perm]

  def erase_spot_models(O):
    for im in O.array:
      im.erase_spot_model()

  def extract_scales(O):
    from scitbx.array_family import flex
    result = flex.double()
    for im in O.array:
      result.append(im.scale)
    return result

  def erase_scales(O):
    for im in O.array:
      im.scale = None

  def reset_scales(O, all_scales):
    for im,scale in zip(O.array, all_scales):
      im.scale = scale

  def iselection_entries_with_spot_model(O):
    from scitbx.array_family import flex
    result = flex.size_t()
    for i,im in enumerate(O.array):
      if (im.spot_positions is not None):
        result.append(i)
    return result

  def remove_all_entries_without_spot_model(O):
    remaining = []
    for im in O.array:
      if (im.spot_positions is not None):
        remaining.append(im)
    return image_models(miller_indices=O.miller_indices, array=remaining)

  def normalize_spot_intensities(O, target_mean):
    from scitbx.array_family import flex
    sum_si = 0
    num_si = 0
    for im in O.array:
      sum_si += flex.sum(im.spot_intensities)
      num_si += im.spot_intensities.size()
    if (sum_si != 0):
      global_scale = target_mean * num_si / sum_si
      for im in O.array:
        im.spot_intensities *= global_scale

  def reset_miller_image_map(O):
    O.miller_image_map = miller_image_map(miller_indices=O.miller_indices)
    for i_img,im in enumerate(O.array):
      O.miller_image_map.enter(
        i_img=i_img, miller_index_i_seqs=im.miller_index_i_seqs)

  def reset_partialities(O, work_params):
    for im in O.array:
      im.reset_partialities(work_params, O.miller_indices)

  def check_i_obs_vs_backup(O, work_params):
    print("Current i_obs vs. backup:")
    for im in O.array:
      im.backup.reset_partialities(work_params, O.miller_indices)
      b_obs = im.backup.extract_i_obs_est(work_params, O.miller_indices)
      im.reset_partialities(work_params, O.miller_indices)
      i_obs = im.extract_i_obs_est(work_params, O.miller_indices)
      max_common_size = -1
      max_cb_ci = None
      for s in work_params.lattice_symmetry.group():
        i_obs_cb = i_obs.change_basis(str(s))
        cb, ci = b_obs.common_sets(other=i_obs_cb)
        common_size = cb.indices().size()
        if (max_common_size < common_size):
          max_common_size = common_size
          max_cb_ci = cb, ci
      assert max_cb_ci is not None
      cb, ci = max_cb_ci
      from scitbx.array_family import flex
      num = flex.sum(cb.data()*ci.data())
      den = flex.sum_sq(cb.data())
      if (den == 0): scale = None
      else:          scale = num / den
      print(" ", b_obs.indices().size(), i_obs.indices().size(), \
        cb.indices().size(), scale)
    print()

  def refinement_target(O, partiality_threshold):
    assert O.miller_image_map.map is not None
    from scitbx.array_family import flex
    result_num = 0
    result_den = 0
    for iiis in O.miller_image_map.map:
      estis = collect_estis(O.array, iiis, partiality_threshold)
      if (estis.size() < 2): continue
      i_obs_est = flex.mean(estis)
      result_num += flex.sum_sq(estis - i_obs_est)
      result_den += estis.size()
    return result_num / max(1, result_den)

  def extract_estimated_i_obs(O, partiality_threshold):
    from cctbx.array_family import flex
    indices = flex.miller_index()
    data = flex.double()
    mimmi = O.miller_image_map.miller_indices
    indices.reserve(mimmi.size())
    data.reserve(mimmi.size())
    for h,iiis in zip(mimmi, O.miller_image_map.map):
      estis = collect_estis(O.array, iiis, partiality_threshold)
      if (estis.size() != 0):
        indices.append(h)
        data.append(flex.mean(estis))
    return (indices, data)

  def write_to_mtz_files(O, common_unit_cell):
    from cctbx import crystal
    crystal_symmetry = crystal.symmetry(
      unit_cell=common_unit_cell,
      space_group_symbol="P1")
    def write_mtz(file_name, counts=None, miis=None):
      if (miis is None):
        isel = (counts != 0).iselection()
        data = counts.select(isel)
      else:
        isel = miis
        data = flex.size_t(isel.size(), 1)
      ma = crystal_symmetry.miller_set(
        indices=O.miller_indices.select(isel),
        anomalous_flag=True).array(data=data)
      ma.as_mtz_dataset(column_root_label="NOBS").mtz_object().write(
        file_name=file_name)
    n_indices = O.miller_indices.size()
    from scitbx.array_family import flex
    counts_all = flex.size_t(n_indices, 0)
    miis_0 = None
    for i_img,im in enumerate(O.array):
      miis = im.miller_index_i_seqs
      write_mtz(file_name="nobs_%03d.mtz" % i_img, miis=miis)
      counts_all.increment_and_track_up_from_zero(
        iselection=im.miller_index_i_seqs)
      if (miis_0 is None):
        miis_0 = miis
      else:
        counts_pair = flex.size_t(n_indices, 0)
        for isel in [miis_0, miis]:
          counts_pair.increment_and_track_up_from_zero(iselection=isel)
        write_mtz(file_name="nobs_000_%03d.mtz" % i_img, counts=counts_pair)
    write_mtz(file_name="nobs_all.mtz", counts=counts_all)

class refinement_target_eps(object):

  __slots__ = ["image_mdls", "partiality_threshold", "eps"]

  def __init__(O, image_mdls, partiality_threshold, eps):
    O.image_mdls = image_mdls
    O.partiality_threshold = partiality_threshold
    O.eps = eps

  def __call__(O, i_img):
    im = O.image_mdls.array[i_img]
    scale_orig = im.scale
    im.scale = scale_orig + O.eps
    result = O.image_mdls.refinement_target(O.partiality_threshold)
    im.scale = scale_orig
    return (i_img, result)

class refinery(object):

  def __init__(O, work_params, image_mdls):
    O.work_params = work_params
    O.image_mdls = image_mdls
    from scitbx.array_family import flex
    O.x = flex.double()
    O.x.reserve(O.image_mdls.size())
    for im in O.image_mdls.array:
      O.x.append(im.scale)
    O.initial_functional = None
    O.number_of_iterations = 0
    O.number_of_function_evaluations = 0
    import scitbx.lbfgs
    scitbx.lbfgs.run(
      target_evaluator=O,
      termination_params=scitbx.lbfgs.termination_parameters(
        max_iterations=O.work_params.refine_scales.max_iterations),
      exception_handling_params=scitbx.lbfgs.exception_handling_parameters(
        ignore_line_search_failed_step_at_lower_bound=True))
    O.image_mdls.reset_scales(all_scales=O.x)
    O.final_functional = O.image_mdls.refinement_target(
      partiality_threshold=O.work_params.usable_partiality_threshold)

  def compute_functional_and_gradients(O):
    O.image_mdls.reset_scales(all_scales=O.x)
    f = O.image_mdls.refinement_target(
      O.work_params.usable_partiality_threshold)
    if (O.initial_functional is None):
      O.initial_functional = f
    O.number_of_function_evaluations += 1
    n_mdls = O.x.size()
    from scitbx.array_family import flex
    g = flex.double()
    g.reserve(n_mdls)
    eps = O.work_params.refine_scales.finite_difference_eps
    if (not O.work_params.multiprocessing or n_mdls < 2):
      for im,x in zip(O.image_mdls.array, O.x):
        im.scale = x+eps
        f_eps = O.image_mdls.refinement_target(
          O.work_params.usable_partiality_threshold)
        im.scale = x
        g.append((f_eps-f)/eps)
    else:
      from libtbx import easy_mp
      mp_results = easy_mp.pool_map(
        fixed_func=refinement_target_eps(
          O.image_mdls, O.work_params.usable_partiality_threshold, eps),
        args=list(range(n_mdls)),
        chunksize=1,
        log=sys.stdout)
      g.resize(n_mdls)
      for i,f_eps in mp_results:
        g[i] = (f_eps-f)/eps
    print("refine scale f, |g|: %.6g, %.6g" % (f, g.norm()))
    sys.stdout.flush()
    return f, g

  def callback_after_step(O, minimizer):
    O.number_of_iterations += 1

  def show_summary(O):
    print("refinement target:")
    print("  initial: %.6g" % O.initial_functional)
    print("    final: %.6g" % O.final_functional)
    print("            iterations:", O.number_of_iterations)
    print("  function evaluations:", O.number_of_function_evaluations)
    print()
    sys.stdout.flush()

def index_and_integrate_one(work_params, image_mdls_miller_indices, pixels):
  from rstbx.simage import run_spotfinder
  spots = run_spotfinder.process(
    work_params=work_params, pixels=pixels, show_spots=False)
  if (spots.size() < work_params.min_number_of_spots_for_indexing):
    print("Insufficient number of spots for indexing.")
    print()
    sys.stdout.flush()
    return (spots.size(), None)
  from rstbx.simage import run_labelit_index
  ai = run_labelit_index.process(work_params=work_params, spots=spots)
  good_i_seqs, miller_indices, co = run_labelit_index.report_uc_cr(ai)
  from rstbx.simage import refine_uc_cr
  refined = refine_uc_cr.refine(
    work_params=work_params,
    spots=spots,
    good_i_seqs=good_i_seqs,
    miller_indices=miller_indices,
    unit_cell=co.unit_cell(),
    crystal_rotation=co.crystal_rotation_matrix())
  from rstbx.simage import integrate_crude
  predicted_spot_positions, \
  predicted_spot_miller_index_i_seqs = integrate_crude.predict_spot_positions(
    work_params=work_params,
    miller_indices=image_mdls_miller_indices,
    unit_cell=refined.unit_cell,
    crystal_rotation=refined.crystal_rotation)
  print("Number of predicted spot positions:", predicted_spot_positions.size())
  print()
  spot_intensities = integrate_crude.collect_spot_intensities(
    pixels=pixels,
    spot_positions=predicted_spot_positions,
    point_spread_inner=work_params.point_spread,
    point_spread_outer=work_params.point_spread+4)
  sel = spot_intensities != 0
  return (
    spots.size(), image_model(
      spot_positions=predicted_spot_positions.select(sel),
      spot_intensities=spot_intensities.select(sel),
      miller_index_i_seqs=predicted_spot_miller_index_i_seqs.select(sel),
      unit_cell=refined.unit_cell,
      crystal_rotation=refined.crystal_rotation))

def index_and_integrate(work_params, image_mdls):
  n_mdls = image_mdls.size()
  if (not work_params.multiprocessing or n_mdls < 2):
    for im in image_mdls.array:
      n_spots, updated_im = index_and_integrate_one(
        work_params, image_mdls.miller_indices, im.pixels)
      im.reset_spot_model(other=updated_im)
  else:
    # import all before fork
    from rstbx.simage import \
      run_spotfinder, \
      run_labelit_index, \
      refine_uc_cr, \
      integrate_crude
    def mp_func(i_img):
      return index_and_integrate_one(
        work_params,
        image_mdls.miller_indices,
        image_mdls.array[i_img].pixels)
    from libtbx import easy_mp
    mp_results = easy_mp.pool_map(
      fixed_func=mp_func,
      args=list(range(n_mdls)),
      chunksize=1,
      log=sys.stdout,
      func_wrapper="buffer_stdout_stderr")
    print()
    sys.stdout.flush()
    for i_img,(log,mp_result) in enumerate(mp_results):
      if (mp_result is None):
        print("ERROR index_and_integrate_one:")
        print("-"*80)
        sys.stdout.write(log)
        print("-"*80)
        print()
      else:
        n_spots, updated_im = mp_result
        if (updated_im is None):
          uc = None
        else:
          uc = updated_im.unit_cell
        print("Refined unit cell %d (%d spots):" % (i_img, n_spots), uc)
        image_mdls.array[i_img].reset_spot_model(other=updated_im)
      sys.stdout.flush()
    print()
    if (work_params.show_refine_uc_cr):
      for _,(log,_) in enumerate(mp_results):
        print("v"*80)
        sys.stdout.write(log)
        print("^"*80)
        print()
      sys.stdout.flush()

def check_refine_uc_cr(work_params, image_mdls,
      unit_cell_perturbation_factor=2,
      crystal_rotation_perturbation_angle=10):
  from cctbx import uctbx
  from scitbx.array_family import flex
  from scitbx import matrix
  for i_img,im in enumerate(image_mdls.array):
    print("Image number:", i_img)
    mt = flex.mersenne_twister(seed=work_params.noise.random_seed+i_img)
    unit_cell = uctbx.unit_cell([
      v + unit_cell_perturbation_factor*(mt.random_double()-0.5)
        for v in im.unit_cell.parameters()])
    crystal_rotation = matrix.sqr(im.crystal_rotation) \
      * matrix.col(mt.random_double_point_on_sphere()) \
          .axis_and_angle_as_r3_rotation_matrix(
            angle=crystal_rotation_perturbation_angle, deg=True)
    refined = refine_uc_cr.refinery(
      work_params=work_params,
      spots_xy0=im.spot_positions,
      miller_indices=image_mdls.miller_indices.select(im.miller_index_i_seqs),
      unit_cell=unit_cell,
      crystal_rotation_uq=crystal_rotation
        .r3_rotation_matrix_as_unit_quaternion())
    refined.show_summary().show_distances()
    print()

def build_images(work_params, i_calc, reindexing_assistant):
  result = []
  from .create import add_noise
  from rstbx.simage import image_simple
  from cctbx.array_family import flex
  if (not work_params.apply_random_reindexing):
    i_calc_data_perms = [i_calc.data()]
  else:
    i_calc_data_perms = [i_calc.data().select(perm)
      for perm in reindexing_assistant.inv_perms]
  n_mdls = work_params.number_of_shots
  use_mp = (work_params.multiprocessing and n_mdls > 1)
  def build_one_image(i_img):
    mt = flex.mersenne_twister(seed=work_params.noise.random_seed+i_img)
    scale = int(work_params.signal_max*(0.1+0.9*mt.random_double()))
    crystal_rotation = mt.random_double_r3_rotation_matrix_arvo_1992()
    i_perm = mt.random_size_t() % len(i_calc_data_perms)
    image = image_simple(
      store_miller_index_i_seqs=True,
      store_spots=True,
      store_signals=True,
      set_pixels=True).compute(
        unit_cell=i_calc.unit_cell(),
        miller_indices=i_calc.indices(),
        spot_intensity_factors=i_calc_data_perms[i_perm],
        crystal_rotation_matrix=crystal_rotation,
        ewald_radius=1/work_params.wavelength,
        ewald_proximity=work_params.ewald_proximity,
        signal_max=scale,
        detector_distance=work_params.detector.distance,
        detector_size=work_params.detector.size,
        detector_pixels=work_params.detector.pixels,
        point_spread=work_params.point_spread,
        gaussian_falloff_scale=work_params.gaussian_falloff_scale)
    add_noise(work_params, pixels=image.pixels)
    if (not work_params.index_and_integrate):
      pixels = None
    else:
      pixels = image.pixels
    miller_index_i_seqs = image.miller_index_i_seqs
    if (use_mp):
      # to by-pass portable but slower pickling
      if (pixels is not None):
        assert pixels.is_0_based()
        assert not pixels.is_padded()
        assert pixels.all() == tuple(work_params.detector.pixels)
        pixels = pixels.copy_to_byte_str()
      miller_index_i_seqs = miller_index_i_seqs.copy_to_byte_str()
    return image_model(
      pixels=pixels,
      spot_positions=image.spots,
      spot_intensities=image.signals,
      unit_cell=i_calc.unit_cell(),
      crystal_rotation=crystal_rotation,
      miller_index_i_seqs=miller_index_i_seqs,
      scale=scale,
      i_perm=i_perm)
  if (not use_mp):
    for i_img in range(n_mdls):
      result.append(build_one_image(i_img))
  else:
    from libtbx import easy_mp
    result = easy_mp.pool_map(
      fixed_func=build_one_image,
      args=list(range(n_mdls)),
      chunksize=1,
      log=sys.stdout)
    for im in result:
      if (im is None): raise RuntimeError("Failure building image.")
      if (im.pixels is not None):
        im.pixels = flex.int_from_byte_str(im.pixels)
        im.pixels.reshape(flex.grid(work_params.detector.pixels))
      im.miller_index_i_seqs = flex.size_t_from_byte_str(
        byte_str=im.miller_index_i_seqs)
  for im in result:
    im.make_backup()
  return image_models(miller_indices=i_calc.indices(), array=result)

class perm_rms_info(libtbx.slots_getstate_setstate):

  __slots__ = ["n", "scale", "rms"]

  def __init__(O, n, scale, rms):
    O.n = n
    O.scale = scale
    O.rms = rms

class perm_rms_list(libtbx.slots_getstate_setstate):

  __slots__ = ["array", "i_small", "score"]

  def __init__(O, array, i_small=None, score=None):
    O.array = array
    O.i_small = i_small
    O.score = score

  def set_score(O):
    if (len(O.array) == 1):
      O.i_small = 0
      _ = O.array[0]
      O.score = _.n / (1 + _.rms)
    else:
      from scitbx.array_family import flex
      rms_list = flex.double([_.rms for _ in O.array])
      sort_perm = flex.sort_permutation(rms_list)
      O.i_small = sort_perm[0]
      i_2nd = sort_perm[1]
      rms_min, rms_2nd = [rms_list[_] for _ in sort_perm[:2]]
      O.score = (rms_2nd - rms_min) * (O.array[O.i_small].n + O.array[i_2nd].n)

def build_usables(work_params, reindexing_assistant, image_mdls):
  from scitbx.array_family import flex
  usable_fractions = flex.double()
  usables = []
  for i_img,im in enumerate(image_mdls.array):
    usable = im.usable(
      partiality_threshold=work_params.usable_partiality_threshold)
    usable_fractions.append(usable.miis.size() / im.miller_index_i_seqs.size())
    miis_perms = []
    for perm in reindexing_assistant.inv_perms:
      m = perm.select(usable.miis)
      p = flex.sort_permutation(data=m)
      miis_perms.append((m.select(p), usable.esti.select(p)))
    usables.append(miis_perms)
  print("Usable fraction of estimated image intensities:")
  usable_fractions.min_max_mean().show(prefix="  ")
  print()
  sys.stdout.flush()
  return usables

class i_perm_and_scale(object):

  __slots__ = ["i_perm", "scale"]

  def __init__(O, i_perm=None, scale=None):
    O.i_perm = i_perm
    O.scale = scale

class cluster_info(object):

  __slots__ = ["i_perm_and_scale_by_i_img", "miis_perms", "esti_perms"]

  def __init__(O,
        i_perm_and_scale_by_i_img=None,
        miis_perms=None,
        esti_perms=None):
    O.i_perm_and_scale_by_i_img = i_perm_and_scale_by_i_img
    O.miis_perms = miis_perms
    O.esti_perms = esti_perms

  def build_cluster_pair_info(O, other, work_params, reindexing_assistant):
    from scitbx.array_family import flex
    scale_max = work_params.scale_estimation_scale_max
    assert scale_max > 0
    scale_min = 1/scale_max
    miis_i, esti_i = O.miis_perms[0], O.esti_perms[0]
    result = []
    for j_perm in range(len(reindexing_assistant.cb_ops)):
      miis_j, esti_j = other.miis_perms[j_perm], other.esti_perms[j_perm]
      i_seqs, j_seqs = miis_i.intersection_i_seqs(other=miis_j)
      if (i_seqs.size() < 2):
        return None
      x = esti_i.select(i_seqs)
      y = esti_j.select(j_seqs)
      if (((x != 0) | (y != 0)).count(True) < 2):
        return None
      num = flex.sum(x*y)
      den = flex.sum_sq(x)
      if (num > den * scale_min and num < den * scale_max):
        scale = num / den
        rms = flex.mean_sq(x*scale-y)**0.5
        result.append(perm_rms_info(n=x.size(), scale=scale, rms=rms))
      else:
        return None
    result = perm_rms_list(array=result)
    result.set_score()
    return result

  def merge(O, other, pair_info, reindexing_assistant, image_mdls):
    # TODO: refine combined scales so that rms for entire cluster
    #       is minimal then compute esti
    miis_i, esti_i = O.miis_perms[0], O.esti_perms[0]
    j_perm = pair_info.i_small
    miis_j, esti_j = other.miis_perms[j_perm], other.esti_perms[j_perm]
    scale_j = pair_info.array[j_perm].scale
    mrg_miis = miis_i.concatenate(miis_j)
    mrg_esti = esti_i.concatenate(esti_j * (1/scale_j))
    from scitbx.array_family import flex
    sort_perm = flex.sort_permutation(mrg_miis)
    mrg_miis = mrg_miis.select(sort_perm)
    mrg_esti = mrg_esti.select(sort_perm)
    new_miis = flex.size_t()
    new_esti = flex.double()
    n = mrg_miis.size()
    i = 0
    while (i < n):
      new_miis.append(mrg_miis[i])
      if (i+1 == n or mrg_miis[i] != mrg_miis[i+1]):
        new_esti.append(mrg_esti[i])
        i += 1
      else:
        new_esti.append((mrg_esti[i] + mrg_esti[i+1]) / 2)
        i += 2
    for i_img,i_perm_and_scale_ in other.i_perm_and_scale_by_i_img.items():
      O.i_perm_and_scale_by_i_img[i_img] = i_perm_and_scale(
        i_perm=reindexing_assistant.i_inv_j_multiplication_table[
          j_perm][
          i_perm_and_scale_.i_perm],
        scale=scale_j*i_perm_and_scale_.scale)
    O.miis_perms = []
    O.esti_perms = []
    for perm in reindexing_assistant.inv_perms:
      m = perm.select(new_miis)
      p = flex.sort_permutation(data=m)
      O.miis_perms.append(m.select(p))
      O.esti_perms.append(new_esti.select(p))

def build_image_cluster(work_params, reindexing_assistant, image_mdls, usables):
  n_imgs = len(usables)
  clusters = []
  for i_img,miis_perms in enumerate(usables):
    clusters.append(cluster_info(
      i_perm_and_scale_by_i_img={i_img: i_perm_and_scale(0, 1)},
      miis_perms=[_ for _,__ in miis_perms],
      esti_perms=[_ for __,_ in miis_perms]))
  remaining = list(range(n_imgs))
  cluster_pairs = [{} for _ in range(n_imgs)]
  def process_cp(i_rem, j_rem):
    i_clu = remaining[i_rem]
    j_clu = remaining[j_rem]
    cp = clusters[i_clu].build_cluster_pair_info(
      other=clusters[j_clu],
      work_params=work_params,
      reindexing_assistant=reindexing_assistant)
    if (cp is not None):
      cluster_pairs[i_clu][j_clu] = cp
  while (len(remaining) != 1):
    if (len(remaining) == n_imgs):
      chunk_size = 3000 # ad-hoc
      if (not work_params.multiprocessing or n_imgs*(n_imgs-1) <= chunk_size):
        import time
        time_start = time.time()
        for i_rem in range(n_imgs):
          for j_rem in range(i_rem+1, n_imgs):
            process_cp(i_rem, j_rem)
        from libtbx.utils import show_wall_clock_time
        show_wall_clock_time(seconds=time.time()-time_start)
      else:
        def mp():
          ij_list = []
          for i_rem in range(n_imgs):
            for j_rem in range(i_rem+1, n_imgs):
              ij_list.append((i_rem,j_rem))
          n_chunks = len(ij_list) // chunk_size
          print("Number of chunks for computing cluster pairs:", n_chunks)
          print()
          def process_chunk(i_chunk):
            for j_chunk in range(chunk_size):
              i = i_chunk * chunk_size + j_chunk
              if (i == len(ij_list)):
                break
              i_rem, j_rem = ij_list[i]
              process_cp(i_rem, j_rem)
            return cluster_pairs
          from libtbx import easy_mp
          mp_results = easy_mp.pool_map(
            fixed_func=process_chunk,
            args=list(range(n_chunks)),
            chunksize=1,
            log=sys.stdout)
          for cps in mp_results:
            for main,sub in zip(cluster_pairs,cps):
              main.update(sub)
        mp()
    else:
      for i_rem in range(max_j_rem):
        i_clu = remaining[i_rem]
        cps_i = cluster_pairs[i_clu]
        if (max_j_clu in cps_i):
          del cps_i[max_j_clu]
        if (i_rem < max_i_rem):
          if (max_i_clu in cps_i):
            del cps_i[max_i_clu]
          process_cp(i_rem, max_i_rem)
      for j_rem in range(max_i_rem+1, len(remaining)):
        process_cp(max_i_rem, j_rem)
    max_score = 0
    max_i_rem = None
    max_j_clu = None
    for i_rem,i_clu in enumerate(remaining):
      cps_i = cluster_pairs[i_clu]
      for j_clu,cp in cps_i.items():
        if (max_score < cp.score):
          max_score = cp.score
          max_i_rem = i_rem
          max_j_clu = j_clu
    if (max_i_rem is None):
      raise RuntimeError("Insufficient connectivity between images.")
    max_i_clu = remaining[max_i_rem]
    max_j_rem = remaining.index(max_j_clu)
    print("max_score:", max_score, (max_i_rem, max_j_rem))
    cp = cluster_pairs[max_i_clu][max_j_clu]
    clusters[max_i_clu].merge(
      other=clusters[max_j_clu],
      pair_info=cp,
      reindexing_assistant=reindexing_assistant,
      image_mdls=image_mdls)
    cluster_pairs[max_j_clu] = None
    clusters[max_j_clu] = None
    del remaining[max_j_rem]
  return clusters[remaining[0]]

def check_image_cluster(
      work_params,
      i_calc,
      reindexing_assistant,
      image_mdls,
      scales_input,
      cluster):
  from scitbx.array_family import flex
  for i_perm in range(len(cluster.miis_perms)):
    expected = i_calc.select(cluster.miis_perms[i_perm])
    reconstr = expected.customized_copy(data=cluster.esti_perms[i_perm])
    print("i_perm:", i_perm)
    flex.linear_correlation(x=expected.data(), y=reconstr.data()).show_summary()
    r1 = expected.f_sq_as_f().r1_factor(
      other=reconstr.f_sq_as_f(), scale_factor=libtbx.Auto)
    print("r1: %.5f" % r1)
    print()
  for i_img,i_perm_and_scale in cluster.i_perm_and_scale_by_i_img.items():
    im = image_mdls.array[i_img]
    im.i_perm = i_perm_and_scale.i_perm
    im.scale = i_perm_and_scale.scale
  if (    not work_params.index_and_integrate
      and not work_params.force_unit_spot_intensities):
    image_mdls.check_i_perm_vs_backup(reindexing_assistant)
  cluster_scales = image_mdls.extract_scales()
  print("input vs. cluster scales:")
  flex.linear_correlation(x=scales_input, y=cluster_scales).show_summary()
  print()

def show_i_calc_reindexing_correlations(i_calc, reindexing_assistant):
  assert i_calc.indices().all_eq(reindexing_assistant.miller_indices)
  assert i_calc.space_group_info().type().number() == 1
  assert i_calc.anomalous_flag()
  from scitbx.array_family import flex
  print("I-calc reindexing correlations:")
  for cb_op,inv_perm in zip(
        reindexing_assistant.cb_ops,
        reindexing_assistant.inv_perms):
    i_calc_cb = i_calc.change_basis(cb_op)
    i_calc_perm = i_calc_cb.select(inv_perm)
    assert i_calc_perm.indices().all_eq(i_calc.indices())
    cc = flex.linear_correlation(
      i_calc.data(),
      i_calc_perm.data()).coefficient()
    r1 = i_calc.f_sq_as_f().r1_factor(
      other=i_calc_perm.f_sq_as_f(), scale_factor=libtbx.Auto)
    print("  %-12s  %8.5f (r1: %.5f)" % (cb_op.c().r().as_hkl(), cc, r1))
  print()

def process_core(work_params, i_calc, reindexing_assistant, image_mdls):
  show_i_calc_reindexing_correlations(i_calc, reindexing_assistant)
  if (work_params.index_and_integrate):
    input_im0_i_perm = None
  else:
    input_im0_i_perm = image_mdls.array[0].backup.i_perm
  if (work_params.check_refine_uc_cr):
    check_refine_uc_cr(work_params, image_mdls)
  scales_input = image_mdls.extract_scales()
  image_mdls.erase_scales()
  if (work_params.index_and_integrate):
    image_mdls.erase_spot_models()
    index_and_integrate(work_params, image_mdls)
    show_vm_info("After index_and_integrate():")
    isel = image_mdls.iselection_entries_with_spot_model()
    print("Removing %d image models for which" \
      " indexing or integration failed." % (image_mdls.size() - isel.size()))
    scales_input = scales_input.select(isel)
    image_mdls = image_mdls.remove_all_entries_without_spot_model()
    print()
  image_mdls.normalize_spot_intensities(target_mean=100)
  image_mdls.check_i_obs_vs_backup(work_params)
  image_mdls.reset_miller_image_map()
  image_mdls.miller_image_map.show_images_per_miller_index()
  image_mdls.reset_partialities(work_params)
  if (work_params.pickle_image_models and work_params.index_and_integrate):
    from libtbx import easy_pickle
    file_name = "%s_image_mdls_index_and_integrate.pickle" % str(
      work_params.base36_timestamp)
    easy_pickle.dump(
      file_name=file_name,
      obj=(work_params, image_mdls, reindexing_assistant))
    show_vm_info("After %s:" % file_name)
  if (work_params.write_image_models_to_mtz_files):
    image_mdls.write_to_mtz_files(common_unit_cell=work_params.unit_cell)
    show_vm_info("After write_image_models_to_mtz_files:")
  usables = build_usables(work_params, reindexing_assistant, image_mdls)
  image_cluster = build_image_cluster(
    work_params, reindexing_assistant, image_mdls, usables)
  show_vm_info("After build_image_cluster():")
  check_image_cluster(
    work_params, i_calc, reindexing_assistant, image_mdls,
    scales_input, image_cluster)
  cluster_scales = image_mdls.extract_scales()
  for im in image_mdls.array:
    im.reindex_in_place(reindexing_assistant)
  image_mdls.reset_miller_image_map()
  image_mdls.miller_image_map.show_images_per_miller_index()
  image_mdls.reset_partialities(work_params)
  from scitbx.array_family import flex
  def show_correlation_of_scales(assert_perfect=False):
    expected = scales_input / scales_input[0]
    estimated = image_mdls.extract_scales()
    print("Correlation of expected and estimated scales:")
    flex.linear_correlation(expected, estimated).show_summary(prefix="  ")
    print()
    sys.stdout.flush()
    if (assert_perfect):
      from libtbx.test_utils import approx_equal
      assert approx_equal(estimated, expected)
  show_correlation_of_scales(
    assert_perfect=not work_params.index_and_integrate)
  indices, data = image_mdls.extract_estimated_i_obs(
    work_params.usable_partiality_threshold)
  i_obs_cluster = i_calc.customized_copy(indices=indices, data=data)
  refined_scales = None
  if (work_params.refine_scales.max_iterations in [None, 0]):
    print("refinement target: %.6g" % image_mdls.refinement_target(
      work_params.usable_partiality_threshold))
    print()
  else:
    refined = refinery(work_params, image_mdls)
    refined.show_summary()
    show_correlation_of_scales()
    refined_scales = image_mdls.extract_scales()
  indices, data = image_mdls.extract_estimated_i_obs(
    work_params.usable_partiality_threshold)
  i_obs_est = i_calc.customized_copy(indices=indices, data=data)
  from libtbx import easy_pickle
  from libtbx import group_args
  easy_pickle.dump(
    file_name="%s_solver_results.pickle" % work_params.base36_timestamp,
    obj=group_args(
      work_params=work_params,
      i_calc=i_calc,
      reindexing_assistant=reindexing_assistant,
      scales_input=scales_input,
      cluster_scales=cluster_scales,
      refined_scales=refined_scales,
      i_obs_cluster=i_obs_cluster,
      i_obs_est=i_obs_est))
  print("Input I-calc:")
  i_calc.show_comprehensive_summary(prefix="  ")
  print()
  print("Estimated I-obs:")
  i_obs_est.show_comprehensive_summary(prefix="  ")
  print()
  if (i_obs_est.indices().size() > 2):
    if (input_im0_i_perm is not None):
      print("input_im0_i_perm:", input_im0_i_perm)
      print()
    print("Correlation of input and estimated I-obs:")
    cc_im0_i_perm = None
    best_cc = -2
    for i_perm,cb_op in enumerate(reindexing_assistant.cb_ops):
      c, e = i_calc.change_basis(cb_op).common_sets(i_obs_est)
      assert c.indices().size() == i_obs_est.indices().size()
      corr = flex.linear_correlation(c.data(), e.data())
      assert corr.is_well_defined()
      cc = corr.coefficient()
      if (best_cc < cc): best_cc = cc
      if (input_im0_i_perm is not None and i_perm == input_im0_i_perm):
        cc_im0_i_perm = cc
      r1 = c.f_sq_as_f().r1_factor(
        other=e.f_sq_as_f(), scale_factor=libtbx.Auto)
      print("  i_perm=%d: %8.5f (r1: %.5f)" % (i_perm, cc, r1))
    if (input_im0_i_perm is not None):
      assert cc_im0_i_perm is not None
      from libtbx.test_utils import approx_equal
      assert approx_equal(cc_im0_i_perm, 1)
    print("  Best correlation: %8.5f" % best_cc)
    print()
  return True

def process(work_params, i_calc):
  from cctbx.miller import reindexing
  reindexing_assistant = reindexing.assistant(
    lattice_group=work_params.lattice_symmetry.group(),
    intensity_group=work_params.intensity_symmetry.group(),
    miller_indices=i_calc.p1_anom.indices())
  reindexing_assistant.show_summary()
  print()
  image_mdls = build_images(work_params, i_calc.p1_anom, reindexing_assistant)
  show_vm_info("After build_images():")
  if (work_params.pickle_image_models):
    file_name = "%s_image_mdls.pickle" % work_params.base36_timestamp
    from libtbx import easy_pickle
    easy_pickle.dump(
      file_name=file_name,
      obj=(work_params, i_calc, reindexing_assistant, image_mdls))
    show_vm_info("After %s:" % file_name)
  process_core(work_params, i_calc.p1_anom, reindexing_assistant, image_mdls)

def run_with_pickle(file_name):
  from libtbx import easy_pickle
  work_params, i_calc, reindexing_assistant, image_mdls = easy_pickle.load(
    file_name)
  work_params.phil_master.format(work_params).show()
  print()
  i_calc.p1_anom.show_comprehensive_summary()
  print()
  reindexing_assistant.show_summary()
  print()
  show_vm_info("After unpickling:")
  process_core(work_params, i_calc.p1_anom, reindexing_assistant, image_mdls)

def run_fresh(args):
  from . import run_spotfinder
  work_params = run_spotfinder.process_args(
    args=args,
    extra_phil_str="""\
number_of_shots = 10
  .type = int
usable_partiality_threshold = 0.1
  .type = float
scale_estimation_scale_max = 1e3
  .type = float
min_number_of_spots_for_indexing = 16
  .type = int
sample_random_seeds = None
  .type = int
check_refine_uc_cr = False
  .type = bool
index_and_integrate = False
  .type = bool
show_refine_uc_cr = False
  .type = bool
apply_random_reindexing = True
  .type = bool
multiprocessing = False
  .type = bool
refine_scales {
  max_iterations = None
    .type = int
  finite_difference_eps = 1e-4
    .type = float
}
pickle_image_models = False
  .type = bool
write_image_models_to_mtz_files = False
  .type = bool
""")
  from .create import build_i_calc
  i_calc = build_i_calc(work_params)
  i_calc.p1_anom.show_comprehensive_summary()
  print()
  show_vm_info("After build_i_calc:")
  if (work_params.sample_random_seeds is None):
    process(work_params, i_calc)
  else:
    _ = work_params
    base36_timestamp = _.base36_timestamp
    for _.noise.random_seed in range(_.sample_random_seeds):
      _.base36_timestamp = base36_timestamp + "_%04d" % _.noise.random_seed
      process(_, i_calc)
  show_vm_info("Final:")

def run(args):
  import libtbx.utils
  libtbx.utils.show_times_at_exit()
  if (len(args) == 1):
    file_name = args[0]
    if (file_name.endswith(".pickle") and op.isfile(file_name)):
      return run_with_pickle(file_name)
  return run_fresh(args)


 *******************************************************************************
