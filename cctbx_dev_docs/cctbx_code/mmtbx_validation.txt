

 *******************************************************************************
mmtbx/validation/__init__.py

from __future__ import absolute_import, division, print_function
from libtbx import slots_getstate_setstate
from libtbx.str_utils import format_value
import sys
import six
import json

class entity(slots_getstate_setstate):
  """
  Base class for all validation results.  This includes a boolean outlier flag,
  the information used to zoom in the Phenix GUI (optional, but strongly
  recommended), and some kind of numerical score (also optional, but strongly
  recommended - although some analyses may require multiple distinct scores).
  """
  __slots__ = [
    "atom_selection",
    "xyz",
    "outlier",
    "score",
  ]
  score_format = "%s"
  molprobity_table_labels = []

  def __init__(self, **kwds):
    for name in self.__slots__:
      setattr(self, name, None)
    for name, value in six.iteritems(kwds):
      assert (name in self.__slots__), name
      setattr(self, name, value)

  @staticmethod
  def header():
    """
    Format for header in result listings.
    """
    raise NotImplementedError()

  def format_score(self, replace_none_with="None"):
    return format_value(self.score_format, self.score,
      replace_none_with=replace_none_with)

  def is_outlier(self):
    return self.outlier

  def as_string(self, prefix=""):
    raise NotImplementedError()

  def __str__(self):
    return self.as_string()

  def id_str(self, ignore_altloc=None):
    """
    Returns a formatted (probably fixed-width) string describing the molecular
    entity being validation, independent of the analysis type.
    """
    raise NotImplementedError()

  def __hash__(self):
    return self.id_str().__hash__()

  def as_list(self):
    """
    Optional; returns old format used by some tools in mmtbx.validation.
    """
    raise NotImplementedError()

  def as_table_row_molprobity(self):
    """
    Returns a list of formatted table cells for display by MolProbity.
    """
    raise NotImplementedError()

  def as_table_row_phenix(self):
    """
    Returns a list of formatted table cells for display by Phenix.
    """
    raise NotImplementedError()

  def as_kinemage(self):
    """
    Returns a kinemage string for displaying an outlier.
    """
    raise NotImplementedError()

  def format_old(self):
    raise NotImplementedError()

  def as_JSON(self):
    """
    Returns a (empty) JSON object representing a single validation object.
    Should be overwritten by each validation script to actually output the data.
    """
    return json.dumps({})

  def __eq__(self, other):
    """
    Compare two validation results to determine whether they correspond to the
    same molecular entity and analysis type.  This is intended to be used for
    analysis of a structure before-and-after refinement (etc.).
    """
    return self.score == other.score

  def __cmp__(self, other):
    return cmp(self.score, other.score)

  def __ne__(self, other):
    return self.score != other.score

  def __lt__(self, other):
    return self.score < other.score

  def __le__(self, other):
    return self.score <= other.score

  def __gt__ (self, other):
    return self.score > other.score

  def __ge__(self, other):
    return self.score >= other.score

  def is_single_residue_object(self):
    raise NotImplementedError()

  def as_selection_string(self):
    """
    Returns PDB atom selection string for the atom(s) involved.
    """
    return None

  def zoom_info(self):
    """
    Returns data needed to zoom/recenter the graphics programs from the Phenix
    GUI.
    """
    return [ self.as_selection_string(), self.xyz ]

__residue_attr__ = [
  "chain_id",
  "resseq",
  "icode",
  "resname",
  "altloc",
  "segid",
]

class residue(entity):
  """
  Base class for validation information about a single residue, which depending
  on context could mean either any one of the residue_group, atom_group, or
  residue objects from the PDB hierarchy.
  """
  __slots__ = entity.__slots__ + __residue_attr__ + ["occupancy"]
  def _copy_constructor(self, other):
    for attr in __residue_attr__ :
      setattr(self, attr, getattr(other, attr))

  def assert_all_attributes_defined(self):
    for name in self.__slots__ :
      assert (getattr(self, name) is not None) or (name == "segid")

  def id_str(self, ignore_altloc=False):
    base = "%2s%4s%1s" % (self.chain_id, self.resseq, self.icode)
    if (not ignore_altloc):
      base += "%1s" % self.altloc
    else :
      base += " "
    base += "%3s" % self.resname
    if (self.segid is not None):
      base += " segid='%4s'" % self.segid
    return base

  def resseq_as_int(self):
    from iotbx.pdb import hybrid_36
    return hybrid_36.hy36decode(len(self.resseq), self.resseq)

  @property
  def resid(self):
    return "%4s%1s" % (self.resseq, self.icode)

  def residue_id(self, ignore_altloc=False):
    return self.id_str(ignore_altloc=ignore_altloc)

  def simple_id(self):
    return ("%s%s%s" % (self.chain_id, self.resseq, self.icode)).strip()

  # XXX probably needs to be flexible about altloc...
  def is_same_residue(self, other, ignore_altloc=False):
    if hasattr(other, "residue_id"):
      return (self.residue_id(ignore_altloc=ignore_altloc) ==
              other.residue_id(ignore_altloc=ignore_altloc))
    return (self.id_str(ignore_altloc=ignore_altloc) ==
            other.id_str(ignore_altloc=ignore_altloc))

  def is_same_residue_group(self, other):
    return ((self.chain_id == other.chain_id) and
            (self.resseq == other.resseq) and
            (self.icode == other.icode) and
            (self.segid == other.segid))

  def atom_group_id_str(self):
    return "%1s%3s%2s%4s%1s" % (self.altloc, self.resname, self.chain_id,
      self.resseq, self.icode)

  def residue_group_id_str(self):
    return "%2s%4s%1s" % (self.chain_id, self.resseq, self.icode)

  def __eq__(self, other):
    assert type(self) == type(other), type(other)
    return self.id_str() == other.id_str()

  def is_single_residue_object(self):
    return True

  def atom_selection_string(self):
    return "(chain '%s' and resid '%s' and resname %s and altloc '%s')" % \
      (self.chain_id, self.resid, self.resname, self.altloc)

  def set_coordinates_from_hierarchy(self, pdb_hierarchy,
      atom_selection_cache=None):
    if (atom_selection_cache is None):
      atom_selection_cache = pdb_hierarchy.atom_selection_cache()
    sel = atom_selection_cache.selection(self.atom_selection_string())
    assert (len(sel) > 0)
    self.xyz = pdb_hierarchy.atoms().select(sel).extract_xyz().mean()

  # for creating the hierarchical json output
  def nest_dict(self, level_list, upper_dict):
    inner_dict = {}
    if len(level_list) > 0:
      next_level = level_list[0]
      upper_dict[getattr(self, next_level)] = self.nest_dict(level_list[1:], inner_dict)
    else:
      return json.loads(self.as_JSON())
    return upper_dict

class atoms(entity):
  """
  Base class for validation results involving a specific set of atoms, such
  as covalent geometry restraints, clashes, etc.
  """
  __atoms_attr__ = [
    "atoms_info",
  ]
  __slots__ = entity.__slots__ + __atoms_attr__

  def n_atoms(self):
    return len(self.atoms_info)

  def __eq__(self, other):
    assert type(self) == type(other), type(other)
    return sorted(self.atoms_info) == sorted(other.atoms_info)

  def is_single_residue_object(self):
    return False

  def get_altloc(self):
    consensus_altloc = ''
    for atom in self.atoms_info :
      if (atom.altloc.strip() != ''):
        if (consensus_altloc == ''):
          consensus_altloc = atom.altloc
        else :
          assert (atom.altloc == consensus_altloc)
    return consensus_altloc

  def sites_cart(self):
    return [ a.xyz for a in self.atoms_info ]

  def is_in_chain(self, chain_id):
    if (chain_id == None):
      return True
    for a in self.atoms_info :
      if (a.chain_id == chain_id):
        return True
    return False

  def nest_dict(self, level_list, upper_dict):
    inner_dict = {}
    if len(level_list) > 0:
      next_level = level_list[0]
      try:
        upper_dict[getattr(self, next_level)] = self.nest_dict(level_list[1:], inner_dict)
      except AttributeError:
        upper_dict[getattr(self.atoms_info[0], next_level)] = self.nest_dict(level_list[1:], inner_dict)
    else:
      return [json.loads(self.as_JSON())]
    return upper_dict

  def merge_two_dicts(self, x, y):
    """Given two dictionaries, merge them into a new dict as a shallow copy, for json output."""
    z = x.copy()
    z.update(y)
    return z

class atom_base(slots_getstate_setstate):
  """
  Container for metadata for a single atom, in the context of validation
  results involving multiple atoms.  Intended to be used as-is inside various
  atoms classes.
  """
  __atom_attr__ = [
    "name",
    "element", # XXX is this necessary?
    "xyz",
    "symop",
    "occupancy",
    "b_iso",
    "model_id"
  ]
  __atom_slots__ = __residue_attr__ + __atom_attr__
  # XXX __slots__ should be left empty here

  def __init__(self, **kwds):
    pdb_atom = kwds.get("pdb_atom", None)
    if (pdb_atom is not None):
      del kwds['pdb_atom']
    for name in self.__slots__:
      setattr(self, name, None)
    for name, value in six.iteritems(kwds):
      assert (name in self.__slots__), name
      setattr(self, name, value)

    if (pdb_atom is not None):
      labels = pdb_atom.fetch_labels()
      self.model_id = labels.model_id
      self.chain_id = labels.chain_id
      self.resseq = labels.resseq
      self.icode = labels.icode
      self.resname = labels.resname
      self.altloc = labels.altloc
      self.segid = labels.segid
      self.name = pdb_atom.name
      self.xyz = pdb_atom.xyz
      self.occupancy = pdb_atom.occ
      self.b_iso = pdb_atom.b
      self.element = pdb_atom.element

  @property
  def resid(self):
    return "%4s%1s" % (self.resseq, self.icode)

  def __cmp__(self, other):
    return cmp(self.id_str(), other.id_str())

  def __eq__(self, other):
    assert isinstance(other, atom_base), type(other)
    return self.id_str() == other.id_str()

  def __ne__(self, other):
    return self.id_str() != other.id_str()

  def __lt__(self, other):
    return self.id_str() < other.id_str()

  def __le__(self, other):
    return self.id_str() <= other.id_str()

  def __gt__ (self, other):
    return self.id_str() > other.id_str()

  def __ge__(self, other):
    return self.id_str() >= other.id_str()

  def id_str(self, ignore_altloc=False, ignore_segid=False):
    base = "%2s%4s%1s" % (self.chain_id, self.resseq, self.icode)
    if (not ignore_altloc):
      base += "%1s" % self.altloc
    else :
      base += " "
    base += "%3s %4s" % (self.resname, self.name)
    if ( (self.segid is not None) and (not ignore_segid) ):
      base += " segid='%4s'" % self.segid
    return base

  def residue_group_id_str(self):
    return "%2s%4s%1s" % (self.chain_id, self.resseq, self.icode)

  def atom_group_id_str(self):
    return "%1s%3s%2s%4s%1s" % (self.altloc, self.resname, self.chain_id,
      self.resseq, self.icode)

class atom_info(atom_base):
  """
  Container for metadata for a single atom, in the context of validation
  results involving multiple atoms.  Intended to be used as-is inside various
  atoms classes.
  """
  __slots__ = atom_base.__atom_slots__ + ["symop"]

def get_atoms_info(pdb_atoms, iselection,
      use_segids_in_place_of_chainids=False):
  proxy_atoms = []
  for n, i_seq in enumerate(iselection):
    atom = pdb_atoms[i_seq]
    labels = atom.fetch_labels()
    if use_segids_in_place_of_chainids:
      chain_id = atom.segid
    else:
      chain_id = labels.chain_id
    info = atom_info(
      name=atom.name,
      element=atom.element,
      model_id=labels.model_id,
      chain_id=chain_id,
      resseq=labels.resseq,
      icode=labels.icode,
      resname=labels.resname,
      altloc=labels.altloc,
      occupancy=atom.occ,
      #segid=atom.segid,
      xyz=atom.xyz)
    proxy_atoms.append(info)
  return proxy_atoms

class atom(atom_base, entity):
  """
  Base class for validation results for a single atom.  This is distinct from
  the atom_info class above, which is used to track individual atoms within
  a multi-atom validation result.
  """
  __slots__ = entity.__slots__ + atom_base.__atom_slots__

  def is_single_residue_object(self):
    return True

#-----------------------------------------------------------------------
class validation(slots_getstate_setstate):
  """
  Container for a set of results from a single analysis (rotamers, clashes,
  etc.).  This is responsible for the console display of these results and
  associated statistics.  Individual modules will subclass this and override
  the unimplemented methods.
  """
  __slots__ = ["n_outliers", "n_total", "results", "_cache", "n_outliers_by_model", "n_total_by_model"]
  program_description = None
  output_header = None
  gui_list_headers = [] # for Phenix GUI ListCtrl widgets
  gui_formats = []      # for Phenix GUI ListCtrl widgets
  wx_column_widths = []

  def __init__(self):
    self.n_outliers = 0
    self.n_total = 0
    self.n_outliers_by_model = {}
    self.n_total_by_model = {}
    self.results = []
    self._cache = None
    assert (len(self.gui_list_headers) == len(self.gui_formats) ==
            len(self.wx_column_widths))

  def get_outliers_count_and_fraction(self):
    if (self.n_total != 0):
      fraction = float(self.n_outliers) / self.n_total
      assert fraction <= 1.0
      return self.n_outliers, fraction
    return 0, 0.

  def get_outliers_fraction_for_model(self, model_id):
    if (self.n_total_by_model[model_id] != 0):
      fraction = float(self.n_outliers_by_model[model_id]) / self.n_total_by_model[model_id]
      assert fraction <= 1.0
      return fraction
    return 0.

  @property
  def percent_outliers(self):
    n_outliers, frac_outliers = self.get_outliers_count_and_fraction()
    return frac_outliers * 100.

  def outlier_selection(self):
    """
    Return a flex.size_t object containing the i_seqs of atoms flagged as
    outliers (either individually or as part of an atom_group).  This needs
    to be implemented in the underlying classes unless they include a
    pre-built _outlier_i_seqs attribute.
    """
    if hasattr(self, "_outlier_i_seqs"):
      return self._outlier_i_seqs
    raise NotImplementedError()

  def get_outliers_goal(self):
    raise NotImplementedError()

  def get_result_class(self):
    raise NotImplementedError()

  def coot_todo(self):
    return ""

  def show_old_output(self, out=sys.stdout, verbose=False):
    """
    For backwards compatibility with output formats of older utilities
    (phenix.ramalyze et al.).
    """
    if (verbose):
      assert (self.output_header is not None)
      print(self.output_header, file=out)
    for result in self.results :
      print(result.format_old(), file=out)
    if (verbose):
      self.show_summary(out)

  def show_summary(self, out=sys.stdout, prefix=""):
    raise NotImplementedError()

  def show(self, out=sys.stdout, prefix="  ", outliers_only=True,
      verbose=True):
    if (len(self.results) > 0):
      print(prefix + self.get_result_class().header(), file=out)
      for result in self.iter_results(outliers_only):
        print(prefix + str(result), file=out)
    self.show_summary(out=out, prefix=prefix)

  def iter_results(self, outliers_only=True):
    for result in self.results :
      if (not outliers_only) or (result.is_outlier()):
        yield result

  def as_kinemage(self):
    return None

  def as_coot_data(self):
    """
    Return results in a format suitable for unpickling in Coot.
    """
    raise NotImplementedError()

  def as_gui_table_data(self, outliers_only=True, include_zoom=False):
    """
    Format results for display in the Phenix GUI.
    """
    table = []
    for result in self.iter_results(outliers_only):
      extra = []
      if (include_zoom):
        extra = result.zoom_info()
      row = result.as_table_row_phenix()
      assert (len(row) == len(self.gui_list_headers))
      table.append(row + extra)
    return table

  def save_table_data(self, file_name=None):
    """
    Save all results as a comma separated, text file
    """
    if (file_name is not None):
      outliers_only = False
      table = self.as_gui_table_data(outliers_only=outliers_only)
      if (len(table) > 0):
        f = open(file_name, 'w')
        f.write('%s\n' % ', '.join(self.gui_list_headers))
        for row in table:
          f.write('%s\n' % ', '.join([str(x) for x in row]))
        f.close()
        return True
    return False

  def merge_dict(self, a, b, path=None):
    """
    Recursive function for merging two dicts, merges b into a
    Mainly used to build hierarchical JSON outputs
    """
    if path is None: path = []
    for key in b:
        if key in a:
            if isinstance(a[key], dict) and isinstance(b[key], dict):
                self.merge_dict(a[key], b[key], path + [str(key)])
            elif a[key] == b[key]:
                pass # same leaf value
            else:
                #raise Exception('Conflict at %s' % '.'.join(path + [str(key)]))
                # should only get called for JSONs that have a list
                # of different validations for a residue (e.g. clashes)
                a[key] = a[key]+b[key]
        else:
            a[key] = b[key]
    return a

  def find_residue(self, other=None, residue_id_str=None):
    assert ([other, residue_id_str].count(None) == 1)
    if (other is not None):
      if hasattr(other, "residue_group_id_str"):
        residue_id_str = other.residue_group_id_str()
      elif hasattr(other, "id_str"):
        residue_id_str = other.id_str()
      else :
        residue_id_str = str(other)
    if (self._cache is None):
      self._cache = {}
      for i_res, result in enumerate(self.results):
        result_id_str = result.residue_group_id_str()
        self._cache[result_id_str] = i_res
    i_res = self._cache.get(residue_id_str, None)
    if (i_res is not None):
      return self.results[i_res]
    return None

  def find_atom_group(self, other=None, atom_group_id_str=None):
    """
    Attempt to locate a result corresponding to a given atom_group object.
    """
    assert ([other, atom_group_id_str].count(None) == 1)
    if (other is not None):
      if hasattr(other, "atom_group_group_id_str"):
        atom_group_id_str = other.atom_group_group_id_str()
      elif hasattr(other, "id_str"):
        atom_group_id_str = other.id_str()
      else :
        atom_group_id_str = str(other)
    if (self._cache is None):
      self._cache = {}
      for i_res, result in enumerate(self.results):
        result_id_str = result.atom_group_group_id_str()
        self._cache[result_id_str] = i_res
    i_res = self._cache.get(atom_group_id_str, None)
    if (i_res is not None):
      return self.results[i_res]
    return None

class rna_geometry(validation):
  #__slots__ = validation.__slots__ + ["n_outliers_small_by_model", "n_outliers_large_by_model"]

  def show(self, out=sys.stdout, prefix="  ", verbose=True):
    if (len(self.results) > 0):
      print(prefix + self.get_result_class().header(), file=out)
      for result in self.results :
        print(result.as_string(prefix=prefix), file=out)
    self.show_summary(out=out, prefix=prefix)

class test_utils(object):

  def count_dict_values(prod, count_key, c=0):
    """
    for counting hierarchical values for testing hierarchical jsons
    """
    for mykey in prod:
      if prod[mykey] == count_key:
        c += 1
      if isinstance(prod[mykey], dict):
        # calls repeatedly
        c = test_utils.count_dict_values(prod[mykey], count_key, c)
      elif isinstance(prod[mykey], list):
        for d in prod[mykey]:
          if isinstance(d, dict):
            c = test_utils.count_dict_values(d, count_key, c)
    return c

class dummy_validation(object):
  """
  Placeholder for cases where values may be undefined because of molecule type
  (e.g. all-RNA structures) but we want to substitute None automatically.
  """
  def __getattr__(self, name):
    return None

  def __bool__(self):
    return False

  __nonzero__ = __bool__

molprobity_cmdline_phil_str = """
  model = None
      .type = path
      .help = "Model file (PDB or mmCIF)"

  outliers_only = False
    .type = bool
    .help = "Only display outliers"

  verbose = True
    .type = bool
    .help = '''Verbose'''
"""


 *******************************************************************************


 *******************************************************************************
mmtbx/validation/analyze_peptides.py
from __future__ import absolute_import, division, print_function
import sys
import mmtbx.rotamer
from libtbx.utils import Sorry

def get_master_phil():
  import libtbx.phil
  return libtbx.phil.parse(
    input_string="""
    analyze_peptides {
      pdb = None
        .type = path
        .multiple = True
        .help = '''Enter a PDB file name'''
      cis_only = True
        .type = bool
        .help = '''Only print cis peptides'''
      verbose = True
        .type = bool
        .help = '''Verbose'''
}
""")

def usage():
  return """
mmtbx.analyze_peptides file.pdb [params.eff] [options ...]

Options:

  pdb=input_file        input PDB file
  cis_only=True         only list cis peptides
  verbose=True          verbose text output

Example:

  mmtbx.analyze_peptides pdb=1xwl.pdb

"""

def analyze(pdb_hierarchy):
  cis_peptides = []
  trans_peptides = []
  outliers = []
  for model in pdb_hierarchy.models():
    for chain in model.chains():
      prev_rg = None
      chain_id = chain.id
      for rg in chain.residue_groups():
        if prev_rg is None:
          prev_rg = rg
          continue
        for conformer in rg.conformers():
          if conformer.is_protein():
            altloc = conformer.altloc
            for residue in conformer.residues():
              atoms = residue.atoms()
              resname = residue.resname
              resid = residue.resid()
              #get appropriate prev_atoms
              for prev_con in prev_rg.conformers():
                if prev_con.is_protein():
                  if( (prev_con.altloc == altloc) or
                      (prev_con.altloc == ' ' or
                       altloc == ' ') ):
                    prev_altloc = prev_con.altloc
                    for prev_res in prev_con.residues():
                      prev_atoms = prev_res.atoms()
                      prev_resname = prev_res.resname
                      prev_resid = prev_res.resid()
                      is_cis = is_cis_peptide(prev_atoms=prev_atoms,
                                              atoms=atoms)
                      is_trans = is_trans_peptide(prev_atoms=prev_atoms,
                                                  atoms=atoms)

                      res1 = "%s%5s %s%s" % (chain.id,
                                             prev_resid,
                                             prev_altloc,
                                             prev_resname)
                      res2 = "%s%5s %s%s" % (chain.id,
                                             resid,
                                             altloc,
                                             resname)
                      if is_cis:
                        cis_peptides.append( (res1, res2) )
                      elif is_trans:
                        trans_peptides.append( (res1, res2) )
                      elif ( (not is_cis) and
                             (not is_trans) ):
                        outliers.append( (res1, res2) )
                      else:
                        raise Sorry("%s:%s " % (res1, res2) +
                                    "have uncharacteristic peptide geometry")
        prev_rg = rg
  return cis_peptides, trans_peptides, outliers

def get_omega(prev_atoms, atoms):
  prevCA, prevC, curN, curCA = None, None, None, None
  if (prev_atoms is not None):
    for atom in prev_atoms:
      if (atom.name == " CA "): prevCA = atom
      if (atom.name == " C  "): prevC = atom
  if (atoms is not None):
    for atom in atoms:
      if (atom.name == " N  "): curN = atom
      if (atom.name == " CA "): curCA = atom
  if (prevCA is not None and
      prevC is not None and
      curN is not None and
      curCA is not None):
    return mmtbx.rotamer.omega_from_atoms(prevCA, prevC, curN, curCA)

def is_cis_peptide(prev_atoms, atoms):
  omega = get_omega(prev_atoms, atoms)
  if(omega > -30 and omega < 30):
    return True
  else:
    return False

def is_trans_peptide(prev_atoms, atoms):
  omega = get_omega(prev_atoms, atoms)
  if( (omega > 150 and omega <= 180) or
      (omega >= -180 and omega < -150) ):
    return True
  else:
    return False

def show(
      cis_peptides,
      trans_peptides,
      outliers,
      log=None,
      cis_only=True):
  print_list = []
  for pair in cis_peptides:
    txt = "%s:%s:cis" % (pair[0], pair[1])
    print_list.append(txt)
  if not cis_only:
    for pair in trans_peptides:
      txt = "%s:%s:trans" % (pair[0], pair[1])
      print_list.append(txt)
    for pair in outliers:
      txt = "%s:%s:OUTLIER" % (pair[0], pair[1])
      print_list.append(txt)
  def get_sort_key(pair):
    return pair[0:6]
  print_list.sort(key=get_sort_key)
  if log is None:
    log = sys.stdout
  print("#\n#peptide geometry analysis\n#", file=log)
  print("residue1:residue2:classification", file=log)
  for line in print_list:
    print(line, file=log)


 *******************************************************************************


 *******************************************************************************
mmtbx/validation/barbed_wire_analysis.py
from __future__ import absolute_import, division, print_function
import os, sys
import iotbx
from libtbx.utils import null_out

packing_quality_key = ["no packing", "underpacked", "marginal packing", "well packed"]  # indices 0,1,2,3

def loadModel(filename):  # from suitename/suites.py
  from iotbx.data_manager import DataManager
  dm = DataManager()  # Initialize the DataManager and call it dm
  dm.set_overwrite(True)  # tell the DataManager to overwrite files with the same name
  model = dm.get_model(filename)
  return model

#TODO: update to Probe2. Unformatted output parsing (used below) is not mmCIF compatible,
#  so Probe2 should be parsed directly from python objects.
class single_contact():
  def __init__(self, probe_line):
    x = probe_line.split(':')
    # Probe Unformatted Output:
    # name:pat:type:srcAtom:targAtom:min-gap:gap:spX:spY:spZ:spikeLen:score:stype:ttype:x:y:z:sBval:tBval
    # for condensed output we have:
    # name:pat:type:srcAtom:targAtom:*dotcount*:min-gap:gap:spX:spY:spZ:spikeLen:score:stype:ttype:x:y:z:sBval:tBval
    ###'name' is set by the user on the command line
    ###'pat' is one of 1->1, 1->2, or 2->1; where 1 is src and 2 is targ.
    ###'type' is one of wc, cc, so, bo, hb (wide/close contact, small/bad overlap, h-bond).
    ###'srcAtom' and 'targAtom' follow the pattern CNNNNITTT AAAAL, where C is chain, N is number, I is insertion code, T is residue type, A is atom name, and L is alternate conformation flag.
    ###'*dotcount*' is condensed-output-only, and gives the number of dots in the contact
    ###'min-gap' is the distance between atoms, minus their van der Waals radii; i.e., the distance of closest approach for their vdW surfaces. gap is the distance between vdW surfaces at the current dot. Negative values indicate overlap (clashes or H-bonds).
    ###'x','y','z' is a point on the vdW surface; 'spX','spY','spZ' is tip of spike, if any (same as x,y,z for contacts)
    ###'score' is "this dot's contribution to the [Probe] score" (scaled already? YES)
    ###'stype' and 'ttype' are heavy-atom element name (C, N, O, etc)

    self.name = x[0]
    self.pattern = x[1]
    self.interactiontype = x[2]

    self.srcAtom = x[3]
    self.srcChain = self.srcAtom[0:2].strip()
    self.srcNum = int(self.srcAtom[2:6].strip())
    self.srcNumStr = self.srcAtom[2:6]
    self.srcIns = self.srcAtom[6:7]  # .strip()
    self.srcResname = self.srcAtom[7:10].strip()
    ###if srcResname == 'HOH': continue  # skip waters
    self.srcAtomname = self.srcAtom[11:15]  # .strip()
    self.srcAlt = self.srcAtom[15:16].strip()

    self.trgAtom = x[4]
    # going to count dots per bond as a measure of strength instead
    self.trgChain = self.trgAtom[0:2].strip()
    self.trgNum = int(self.trgAtom[2:6].strip())
    self.trgNumStr = self.trgAtom[2:6]
    self.trgIns = self.trgAtom[6:7]  # .strip()
    self.trgResname = self.trgAtom[7:10].strip()
    ### if trgResname == 'HOH': continue #skip waters
    self.trgAtomname = self.trgAtom[11:15]  # .strip()
    self.trgAlt = self.trgAtom[15:16].strip()

    self.dotcount = x[5]
    self.mingap = x[6]

    ##mc_contact = self.contact_type(srcAtomname,trgAtomname)
    self.srcAtomid = ",".join(
      [self.srcChain.strip(), self.srcAlt.strip(), self.srcNumStr, self.srcIns, self.srcAtomname])
    self.srcResid = ','.join([self.srcChain.strip(), self.srcNumStr])
    self.trgAtomid = ",".join(
      [self.trgChain.strip(), self.trgAlt.strip(), self.trgNumStr, self.trgIns, self.trgAtomname])
    self.trgResid = ','.join([self.trgChain.strip(), self.trgNumStr])
    self.contactId = ",".join([self.srcAtomid, self.trgAtomid])

#TODO update to Reduce2 and Probe2 - clashscore2 will be likely template
def do_probe(model):
  from libtbx import easy_run

  pdblines = model.get_hierarchy().as_pdb_string()
  probe_temp = open("probe_contacts_temp.pdb", "w")
  print(pdblines, file=probe_temp)
  probe_temp.close()
  cmd = "phenix.reduce -noflip probe_contacts_temp.pdb"
  reduce_run = easy_run.fully_buffered(cmd).stdout_lines
  probe_temp = open("probe_contacts_temp.pdb", "w")
  for line in reduce_run:
    print(line, file=probe_temp)
  probe_temp.close()
  cmd = "phenix.probe -u -con -self -mc ALL probe_contacts_temp.pdb"
  probe_run = easy_run.fully_buffered(cmd).stdout_lines

  contacts = []
  for line in probe_run:
    if not line.strip(): continue  # averts an IndexError problem with empty lines
    contacts.append(single_contact(line))

  sort_order = {'bo': 1, 'hb': 2, 'so': 3, 'cc': 4, 'wc': 5}
  contacts.sort(key=lambda x: sort_order[x.interactiontype], reverse=True)
  contact_dict = {}
  for contact in contacts:
    contact_dict[contact.contactId] = contact
  # a single atom pair can appear in multiple categories
  # this sort->dict means that each atom pair gets a single contact entry of the highest priority type
  # lower priority types get overwritten by higher (eg 'so' is stored first, then overwritten by 'bo' if present)
  os.remove("probe_contacts_temp.pdb")  # cleanup
  return contact_dict

class alphafold_chunk():
  def __init__(self, res_list, prediction_type):
    self.members = [r for r in res_list]
    self.prediction_type = prediction_type

  @property
  def start(self):
    return self.members[0]

  @property
  def end(self):
    return self.members[-1]

  @property
  def chain(self):
    return self.start.split(',')[0]

  def as_selection_string(self):
    # resid is "chain,resseq"
    chain = self.start.split(',')[0]
    start_res = self.start.split(',')[1].strip()
    end_res = self.end.split(',')[1].strip()
    return "(chain %s and resseq %s:%s)" % (chain, start_res, end_res)

  def add_to_start(self, resid):
    self.members.insert(0, resid)

  def add_to_end(self, resid):
    self.members.append(resid)

  def remove_from_start(self):
    return self.members.pop(0)

  def remove_from_end(self):
    return self.members.pop(-1)

  def change_prediction_type(self, new_prediction_type):
    self.prediction_type = new_prediction_type
    for r in self.members:
      r.feedback = new_prediction_type

class predicted_residue():
  def __init__(self, rg):
    self.chain = rg.parent().id
    self.resseq = rg.resseq
    self.resnum = int(self.resseq.strip())
    self.resid = ','.join([self.chain, self.resseq])  # assume no alternates, no inserts in predictions
    self.caxyz, self.plddt = self.find_ca_plddt(rg)
    self.heavy_atom_count = self.find_heavy_atom_count(rg)
    self.out_rama = None
    self.rama_high_psi = None
    self.rama_high_phi = None
    self.out_omega = None #This is a signature outlier
    self.cis_pro = None #This is suspicious, but could occur in near-predictive
    self.out_geom = None
    self.out_geom_cnca = None #This is a signature outlier
    self.out_cablam = None
    self.out_ca_geom = None
    self.ss_type = None  # "helix","sheet", or None for coil
    self.ss_index = None  # index of helix or sheet, used to tell if residues are part of same element
    self.packing_hb = []
    self.packing_vdw = []
    self.packing_bo = []
    self.severe_clashes = 0  # cutoff set in add_contacts, this may indicate knots/interpenetration
    self.packing_quality = None
    self.barbed_wire_signature = None
    self.high_outlier_density = None
    self.feedback = ''
    #self.text_code = '      '
    self.text_code = ['','','','','',''] #is this okay

  def find_ca_plddt(self, rg):
    for atom in rg.atoms():
      if atom.name == " CA ":
        return atom.xyz, atom.b
    return None, None

  def find_heavy_atom_count(self, rg):
    # print(rg.atom_groups()[0].resname, len(rg.atoms()))
    return (len(rg.atoms()))

  def packing_contact_count(self):
    # sum of contacts with this residue
    # hb and bo types my be upweighted
    return len(self.packing_hb) + len(self.packing_vdw) + len(self.packing_bo)

  def has_any_validation_errors(self):
    if self.text_code[2:] == '----':
      # text_code is 'Lprocg' for all problems.  Any letter may be replaced with '-'
      # '----' in last 4 slots indicates no validation problems
      return False
    else:
      return True

  def as_text(self):
    return (' '.join([self.chain, self.resseq, self.text_code, self.feedback]))

  #I might make a detailed_dict version that also includes self.rama_high_psi etc
  def as_dict(self):
    return {"chain":self.chain,
            "resseq":self.resseq,
            "category":self.feedback,
            "text_code":self.text_code,
            "plddt":self.plddt,
            "ca_xyz":self.caxyz}

class barbed_wire_analysis():
  def __init__(self, model):
    self.res_dict = {}  # keyed for easy lookup
    self.res_list = {}  # separated by chain, ordered for finding sequence-related residues
    self.chunk_list = []

    hierarchy = model.get_hierarchy()
    self.load_residues(hierarchy)
    ##print('\n'.join(bwa.res_dict.keys()))

    self.add_secondary_structure(model, hierarchy)
    self.add_contacts(model)
    self.analyze_contacts()
    self.add_ramalyze(hierarchy)
    self.add_cablam(hierarchy)
    self.add_omegalyze(hierarchy)
    self.add_covalent_geometry(model)

    self.predictalyze()
    self.merge_analyses()
    self.merge_small_chunks()

  def load_residues(self, hierarchy):
    for chain in hierarchy.chains():
      self.res_list[chain.id] = [] #chain.id is already whitespace stripped
      for rg in chain.residue_groups():
        r = predicted_residue(rg)
        self.res_dict[r.resid] = r
        self.res_list[chain.id].append(r)

  def add_secondary_structure(self, model, hierarchy):
    from mmtbx.secondary_structure import sec_str_master_phil_str
    from mmtbx.secondary_structure import manager as ss_manager
    asc = model.get_atom_selection_cache()
    sec_str_master_phil = iotbx.phil.parse(sec_str_master_phil_str)
    ss_params = sec_str_master_phil.fetch().extract()
    ss_params.secondary_structure.protein.search_method = "from_ca"
    ssm = ss_manager(hierarchy,
                     atom_selection_cache=asc,
                     geometry_restraints_manager=None,
                     sec_str_from_pdb_file=None,
                     # params=None,
                     params=ss_params.secondary_structure,
                     was_initialized=False,
                     mon_lib_srv=None,
                     verbose=-1,
                     log=null_out(),
                     # log=sys.stdout,
                     )
    secstr = ssm.find_sec_str(hierarchy)
    for helix in secstr.helices:
      helix_id = helix.helix_id
      start_chain_id = helix.start_chain_id.strip()
      start_resseq = helix.start_resseq
      start_resid = ','.join([start_chain_id, start_resseq])
      r = self.res_dict[start_resid]
      i = self.res_list[start_chain_id].index(r)
      res_range = self.res_list[start_chain_id][i: i + helix.length]
      for r in res_range:
        r.ss_type = "helix"
        r.ss_index = helix_id

    for sheet in secstr.sheets:
      for strand in sheet.strands:
        sheet_id = strand.sheet_id
        # strand_id = strand.strand_id
        start_chain_id = strand.start_chain_id.strip()
        start_resseq = strand.start_resseq
        end_resseq = strand.end_resseq
        start_resid = ','.join([start_chain_id, start_resseq])
        r = self.res_dict[start_resid]
        i = self.res_list[start_chain_id].index(r)
        strand_length = int(end_resseq.strip()) - int(start_resseq.strip()) + 1
        res_range = self.res_list[start_chain_id][i: i + strand_length]
        for r in res_range:
          r.ss_type = "sheet"
          r.ss_index = sheet_id

  def are_same_ss_element(self, resid1, resid2):
    res1 = self.res_dict[resid1]
    res2 = self.res_dict[resid2]
    if not (res1.ss_type and res2.ss_type):
      # at least one coil - only sequence proximity matters for coil
      return False
    if not res1.ss_type == res2.ss_type:
      # different secondary structure types cannot be same element
      return False
    if not res1.ss_index == res2.ss_index:
      # same index means same element if same type (HELIX and SHEET can share indices)
      return False
    # same type and same index
    return True

  def add_ramalyze(self, hierarchy):
    from mmtbx.validation import ramalyze
    rama_results = ramalyze.ramalyze(pdb_hierarchy=hierarchy,
                                     outliers_only=False)
    for result in rama_results.results:
      resid = ','.join([result.chain_id, result.resseq])
      if result.is_outlier():
        self.res_dict[resid].out_rama = True
      if 60.0 < result.psi < 170.0:
        self.res_dict[resid].rama_high_psi = True
      if -15.0 < result.phi < 170.0:  # this region is indicative of general-case residue types
        self.res_dict[resid].rama_high_phi = True

  def add_omegalyze(self, hierarchy):
    from mmtbx.validation import omegalyze
    omega_results = omegalyze.omegalyze(pdb_hierarchy=hierarchy,
                                        nontrans_only=True)
    for result in omega_results.results:
      resid = ','.join([result.chain_id, result.resseq])
      if result.resname == "PRO":
        if result.omegalyze_type() == "Twisted":  # (cis Pro is okay by default)
          self.res_dict[resid].out_omega = True
        else:
          self.res_dict[resid].cis_pro = True
      else:
        if result.is_outlier():  # should always be true with nontrans_only=True run
          self.res_dict[resid].out_omega = True

  def add_contacts(self, model):
    contacts = do_probe(model)
    for c in contacts.values():
      if self.are_same_ss_element(c.srcResid, c.trgResid):
        # only interested in packing between different elements
        # will have to develop better method for large beta, eventually
        continue
      r = self.res_dict[c.srcResid]
      if abs(r.resnum - self.res_dict[c.trgResid].resnum) <= 4:
        # sequence distance between src and trg
        # local contacts do not count for packing
        # sequence dist cutoff subject to change
        continue
      if c.interactiontype == "bo":
        r.packing_bo.append([c.contactId])
        if float(c.mingap) >= 0.9:
          r.severe_clashes += 1
      elif c.interactiontype == "hb":
        r.packing_hb.append([c.contactId])
      else:  # so, cc, wc
        r.packing_vdw.append([c.contactId])

  def analyze_contacts(self):
    for chain in self.res_list:
      i, j = 0, 5  # window of 5 res
      while j < len(self.res_list[chain]):
        res_slice = self.res_list[chain][i:j]
        total_packing = sum([r.packing_contact_count() for r in res_slice])
        total_heavy_atoms = sum([r.heavy_atom_count for r in res_slice])
        packing_ratio = total_packing / total_heavy_atoms
        r = res_slice[2]
        if r.ss_type == "sheet":
          if packing_ratio <= 0.1:
            r.packing_quality = 0
          elif packing_ratio <= 0.35:  # different for sheet
            r.packing_quality = 1
          elif packing_ratio <= 1:
            r.packing_quality = 2
          else:
            r.packing_quality = 3
        else:  # helix and coil
          if packing_ratio <= 0.1:
            r.packing_quality = 0
          elif packing_ratio <= 0.6:  # general case (helix/coil)
            r.packing_quality = 1
          elif packing_ratio <= 1:
            r.packing_quality = 2
          else:
            r.packing_quality = 3
          pass
        # print(res_slice[2].resseq, res_slice[2].ss_type, res_slice[2].ss_index, "%.3f" % (total_packing/total_heavy_atoms))#, res_slice[2].packing_vdw)
        i += 1
        j += 1

  def add_covalent_geometry(self, model):
    from mmtbx.validation.mp_validate_bonds import mp_angles
    from mmtbx.model import manager
    from libtbx.utils import null_out
    mc_atoms = [" CA ", " N  ", " C  ", " O  "]

    model.set_stop_for_unknowns(False)
    hierarchy = model.get_hierarchy()
    p = manager.get_default_pdb_interpretation_params()
    p.pdb_interpretation.allow_polymer_cross_special_position = True
    p.pdb_interpretation.flip_symmetric_amino_acids = False
    p.pdb_interpretation.clash_guard.nonbonded_distance_threshold = None
    model.set_log(log = null_out())
    model.process(make_restraints=True, pdb_interpretation_params=p)
    geometry = model.get_restraints_manager().geometry
    atoms = hierarchy.atoms()
    angles = mp_angles(
      pdb_hierarchy=hierarchy,
      pdb_atoms=atoms,
      geometry_restraints_manager=geometry,
      outliers_only=True)
    for angle in angles.results:
      resid = ','.join([angle.atoms_info[1].chain_id, angle.atoms_info[1].resseq])
      atomnames = [angle.atoms_info[0].name, angle.atoms_info[1].name, angle.atoms_info[2].name]
      if atomnames[0] not in mc_atoms: continue
      if atomnames[1] not in mc_atoms: continue
      if atomnames[2] not in mc_atoms: continue
      self.res_dict[resid].out_geom = True
      #Outliers in the C-N-CA angle across the incoming peptide bond are a signature of barbed wire
      #This outlier alone is sufficient to identify barbed wire
      #Maybe useful to start flagging these at 3 sigma, but would require a non-outliers_only run
      if atomnames == [" C  ", " N  ", " CA "]:
        self.res_dict[resid].out_geom_cnca = True

  def add_cablam(self, hierarchy):
    from mmtbx.validation import cablam
    cablam_results = cablam.cablamalyze(pdb_hierarchy=hierarchy,
                                        outliers_only=True,
                                        out=sys.stdout,
                                        quiet=True)
    for result in cablam_results.results:
      resid = ','.join([result.chain_id.strip(), result.resseq])
      if result.feedback.cablam_outlier:  # ignore cablam_disfavored
        self.res_dict[resid].out_cablam = True
      if result.feedback.c_alpha_geom_outlier:
        self.res_dict[resid].out_ca_geom = True

  def categorize_outliers(self, res_slice):
    #Certain outliers are signatures of barbed wire
    #  c-n-ca, cis-nonPro and all twisted, top right rama
    r0 = res_slice[0]
    r1 = res_slice[1]
    r1.text_code = ['-'] * 6  # Lproag
    if r1.out_omega:
      r1.barbed_wire_signature = True
      r0.barbed_wire_signature = True
      r0.text_code[3] = 'o'
      r1.text_code[3] = 'o'
    if r1.out_geom_cnca:
      r1.barbed_wire_signature = True
      r0.barbed_wire_signature = True
      r0.text_code[5] = 'g'
      r1.text_code[5] = 'g'
    if r1.out_rama and r1.rama_high_phi and r1.rama_high_psi:
      #top right Ramachandran and outlier
      r1.barbed_wire_signature = True
      r1.text_code[2] = 'r'

    #add check for None packing quality
    #certain outliers are highly suspicious, but must be allowed in predictive regions
    if r1.packing_quality < 1 and r1.plddt < 70:
      if r1.cis_pro:
        #cisPro is assumed to be barbed wire if it occurs in unpacked low pLDDT
        r1.barbed_wire_signature = True
        r0.barbed_wire_signature = True
        r0.text_code[3] = 'o'
        r1.text_code[3] = 'o'
      if r1.out_ca_geom:
        #CaBLAM's ca_geom_outlier is assumed to be barbed wire if it occurs in unpacked low pLDDT
        r1.barbed_wire_signature = True
        r1.text_code[4] = 'c'

    #Look at a window of 3 residues. If the local density of backbone outliers is high,
    #  center residue is assumed barbed-wire-like
    rama = 0
    omega = 0
    cablam = 0
    geom = 0
    psi = 0
    for r in res_slice:
      if r.out_rama: rama += 1
      if r.out_cablam: cablam += 1
      if r.out_omega: omega += 1
      if r.out_geom: geom += 1
      if r.rama_high_psi: psi += 1
    score = 0
    if rama >= 1 and psi == 3:  # all in high-psi region, plus at least 1 outlier
      score += 1
      r1.text_code[2] = 'r'
    if omega >= 2:
      score += 1
      r1.text_code[3] = 'o'
    if cablam >= 2:
      score += 1
      r1.text_code[4] = 'c'
    if geom >= 2:
      score += 1
      r1.text_code[5] = 'g'
    if score >= 2:
      r1.high_outlier_density = True
      #is_barbed_like = True
    else:
      r1.high_outlier_density = False

  def has_barbed_wire_errors(self, res_slice):
    #Look at a window of 3 residues. If the local density of backbone outliers is high,
    #  center residue is assumed barbed-wire-like
    rama = 0
    omega = 0
    cablam = 0
    geom = 0
    psi = 0
    for r in res_slice:
      if r.out_rama: rama += 1
      if r.out_cablam: cablam += 1
      if r.out_omega: omega += 1
      if r.out_geom: geom += 1
      if r.rama_high_psi: psi += 1
    score = 0
    if rama >= 1 and psi == 3:  # all in high-psi region, plus at least 1 outlier
      score += 1
      text_code[2] = 'r'
    if omega >= 2:
      score += 1
      text_code[3] = 'o'
    if cablam >= 2:
      score += 1
      text_code[4] = 'c'
    if geom >= 2:
      score += 1
      text_code[5] = 'g'
    if score >= 2:
      is_barbed_like = True
    else:
      is_barbed_like = False
    return is_barbed_like, text_code

  def predictalyze(self):
    for chain in self.res_list:
      i, j = 0, 3  # window of 3 res
      while j < len(self.res_list[chain]):
        res_slice = self.res_list[chain][i:j]
        r = res_slice[1]
        if r.packing_quality is None:
          i += 1; j += 1
          continue
        self.categorize_outliers(res_slice)
        i += 1; j += 1

      for r in self.res_list[chain][1:-1]: #First and last residues not fully assessable
        if r.packing_quality is None:
          continue
        if r.plddt >= 70:
          if r.packing_quality >= 1:
            r.feedback = "Predictive"
          else:
            r.text_code[1] = 'p'
            r.feedback = "Unpacked high pLDDT"
        else:  # plddt should get subdivided based on further analysis
          r.text_code[0] = 'L'
          if r.packing_quality >= 1:
            if r.barbed_wire_signature or r.high_outlier_density:
              r.feedback = "Unphysical" #this rare category catches chain intersections
            else:
              r.feedback = "Near-predictive"
          else:  # poor packing
            r.text_code[1] = 'p'
            if r.barbed_wire_signature or r.high_outlier_density:
              r.feedback = "Barbed wire"
            else:
              #r.feedback = "Unpacked possible"
              r.feedback = "Pseudostructure" #name changed
        r.text_code = ''.join(r.text_code)

  def merge_analyses(self):
    chunk_list = []
    for chain in self.res_list:
      prev_r = None
      current_chunk = []
      for r in self.res_list[chain]:
        if prev_r is None:
          current_chunk.append(r.resid)
          prev_r = r
          continue

        if r.feedback == prev_r.feedback:
          # if same type, extend current chunk
          current_chunk.append(r.resid)
        else:
          # if different, end this chunk and start a new one with this residue
          chunk_list.append(alphafold_chunk(current_chunk, prev_r.feedback))
          current_chunk = [r.resid]
        prev_r = r
      chunk_list.append(alphafold_chunk(current_chunk, r.feedback))
      self.chunk_list = chunk_list

  def merge_small_chunks(self):
    #for smoother presentation short segments (1 or 2 residues) that are surrounded by another type
    #  are merged into that type
    #This is primarily for clean presentation, reducing visual fragmentation
    #  Though 2 residues of pseudo in the middle of lots of barbed isn't very believable
    #As the least defined behavior, merging pseudostructure into barbed wire ot near-predictive takes
    #  priority. Afterwards, stray barbed wire is merged into pseudostructure.
    #Barbed-wire is never promoted to near-predictive, and near-predictive is never demoted.
    i, j = 0, 3  # window of 3 chunks
    while j < len(self.chunk_list):
      c = self.chunk_list[i:j]
      if len(c[1].members) > 2:
        i += 1; j += 1
        continue
      if c[1].prediction_type == "Pseudostructure":
        if c[0].prediction_type == "Barbed wire" and c[2].prediction_type == "Barbed wire":
          c[1].prediction_type = "Barbed wire"
          for resid in c[1].members:
            self.res_dict[resid].feedback = "Barbed wire"
        elif c[0].prediction_type == "Near-predictive" and c[2].prediction_type == "Near-predictive":
          c[1].prediction_type = "Near-predictive"
          for resid in c[1].members:
            self.res_dict[resid].feedback = "Near-predictive"
      elif c[0].prediction_type == "Unphysical" and c[2].prediction_type == "Unphysical":
        #Unphysical (intersection) regions should not be interrupted by near-predictive
          c[1].prediction_type = "Unphysical"
          for resid in c[1].members:
            self.res_dict[resid].feedback = "Unphysical"
      i += 1; j += 1
    #Barbed wire if merged in a separate pass so that barbed wire "wins" in regions that alternate
    #  between short segments of barbed wire and pseudostructure
    i, j = 0, 3  # window of 3 chunks
    while j < len(self.chunk_list):
      c = self.chunk_list[i:j]
      if len(c[1].members) > 2:
        i += 1; j += 1
        continue
      if c[1].prediction_type == "Barbed wire":
        if c[0].prediction_type == "Pseudostructure" and c[2].prediction_type == "Pseudostructure":
          c[1].prediction_type = "Pseudostructure"
          for resid in c[1].members:
            self.res_dict[resid].feedback = "Pseudostructure"
      i += 1; j += 1
    self.merge_similar_chunks()
    self.remove_empty_chunks()

  def old_merge_small_chunks(self):
    i = 0
    while i < len(self.chunk_list):
      c = self.chunk_list[i]
      if (i - 1 < 0) or (i + 1 >= len(self.chunk_list)):
        i += 1
        continue
      prev_c = self.chunk_list[i - 1]
      next_c = self.chunk_list[i + 1]
      if len(c.members) > 2 or c.prediction_type != "Pseudostructure":#"Unpacked possible":
        i += 1
        continue
      if prev_c.prediction_type == "Barbed wire":
        for r in c.members:
          if self.res_dict[r].has_any_validation_errors():
            prev_c.add_to_end(c.remove_from_start())
          else:
            break
      if len(c.members) == 0:
        i += 1
        continue
      else:
        if next_c.prediction_type == "Barbed wire":
          for r in reversed(c.members):
            if self.res_dict[r].has_any_validation_errors(): #This step has not been added to the new version yet
              next_c.add_to_start(c.remove_from_end())
            else:
              break
      i += 1
    self.remove_empty_chunks()
    self.old_merge_similar_chunks()
    self.remove_empty_chunks()

  def remove_empty_chunks(self):
    i = len(self.chunk_list) - 1
    while i >= 0:
      c = self.chunk_list[i]
      if len(c.members) == 0:
        self.chunk_list.pop(i)
      i -= 1

  def old_merge_similar_chunks(self):
    i = 0
    while True:
      if i+1 >= len(self.chunk_list):
        #len changes if there's a pop, so fully check it each time
        break
      c = self.chunk_list[i]
      next_c = self.chunk_list[i+1]
      if c.prediction_type == next_c.prediction_type:
        c.members = prev_c.members + c.members + next_c.members
        self.chunk_list.pop(i+1)
      else: #only iterate if the next chunk doesn't match
        i+1

  def merge_similar_chunks(self):
    #Adjacent chunks with the same prediction type are collapsed into a single
    #  chunk.
    #This case arises after merge_small_chunks reassigns some chunks'
    #  prediction types for smoothing
    i = 0
    while i < len(self.chunk_list):
      c = self.chunk_list[i]
      if (i - 1 < 0):
        i += 1
        continue
      prev_c = self.chunk_list[i - 1]
      if c.chain != prev_c.chain:
        i += 1
        continue
      if c.prediction_type == prev_c.prediction_type:
        c.members = prev_c.members + c.members
        prev_c.members = []
      i += 1

  def count_assessed(self):
    #residues near chain termini are not assessed
    count = 0
    for chain in self.res_list:
      for r in self.res_list[chain]:
        if r.feedback:
          count += 1
    return count

  def count_assessed_in_chain(self, chain):
    count = 0
    for c in self.res_list:
      if not c == chain:
        continue
      for r in self.res_list[chain]:
        if r.feedback:
          count += 1
    return count

  def count_by_type(self, prediction_type):
    count = 0
    for chain in self.res_list:
      for r in self.res_list[chain]:
        if r.feedback == prediction_type:
          count += 1
    return count

  def count_by_type_in_chain(self, prediction_type, chain):
    count = 0
    for c in self.res_list:
      if not c == chain:
        continue
      for r in self.res_list[chain]:
        if r.feedback == prediction_type:
          count += 1
    return count

  # ----------------------OUTPUT---------------------
  def as_text_residues(self, out=sys.stdout):
    for chain in self.res_list:
      for r in self.res_list[chain]:
        print(r.as_text(), file=out)

  def as_text_chunks(self, out=sys.stdout):
    for c in self.chunk_list:
      print(c.start, "to", c.end, c.prediction_type, len(c.members), file=out)

  def as_json(self, out=sys.stdout):
    import json
    j = {"flat_results":[],
         "chunks":[],
         "residues_by_category":{},
         "summary":{}}
    total_residues = 0
    #---flat results---
    for chain in self.res_list:
      for r in self.res_list[chain]:
        j["flat_results"].append(r.as_dict())
        total_residues += 1
    #---chunks---
    for c in self.chunk_list:
      j["chunks"].append({"chain":c.start.split(",")[0],
                          "start":c.start.split(",")[1],
                          "end":c.end.split(",")[1],
                          "category":c.prediction_type,
                          "length":len(c.members)})
    #---residues_by_category---
    for prediction_type in ["Predictive", "Unpacked high pLDDT", "Near-predictive",
                            "Pseudostructure", "Barbed wire", "Unphysical"]:
      j["residues_by_category"][prediction_type] = []
    for chain in self.res_list:
      for r in self.res_list[chain]:
        if r.feedback:
          j["residues_by_category"][r.feedback].append(r.resid)
    #---summary---
    j["summary"]["count_by_type"] = {}
    j["summary"]["count_by_type"]["total"] = total_residues
    assessed = self.count_assessed()
    j["summary"]["count_by_type"]["assessed"] = assessed
    j["summary"]["percent_by_type"] = {}
    for prediction_type in ["Predictive", "Unpacked high pLDDT", "Near-predictive",
                            "Pseudostructure", "Barbed wire", "Unphysical"]:
      count = self.count_by_type(prediction_type)
      if assessed == 0:
        pct = 0
      else:
        pct = count/assessed*100.0
      j["summary"]["count_by_type"][prediction_type] = count
      j["summary"]["percent_by_type"][prediction_type] = pct

    print(json.dumps(j, indent='  '), file=out)

  def as_kinemage(self, out=sys.stdout):
    # colored ball at each CA, color based on current synthesis
    # label with bc--go- style text showing components of decision
    prediction_types = ["Predictive", "Unpacked high pLDDT", "Near-predictive",
                            "Pseudostructure", "Barbed wire", "Unphysical"]
    colors = {"Predictive":"sky",
              "Unpacked high pLDDT":"gray",
              "Near-predictive":"green",
              "Pseudostructure":"gold",
              "Barbed wire":"hotpink",
              "Unphysical":"purple"}
    balls = {}
    labels = {}
    for prediction_type in prediction_types:
      balls[prediction_type] = []
      labels[prediction_type] = []
    for chain in self.res_list:
      for r in self.res_list[chain]:
        if not r.feedback:
          continue
        ballline = "{%s %s %s} %.3f %.3f %.3f" % (r.resid, r.feedback, r.text_code, r.caxyz[0], r.caxyz[1], r.caxyz[2])
        #print(ballline, file=sys.stderr)
        labelline = "{  %s} %.3f %.3f %.3f" % (r.text_code, r.caxyz[0], r.caxyz[1], r.caxyz[2])
        balls[r.feedback].append(ballline)
        labels[r.feedback].append(labelline)
      print("@group {bwa markup} collapsible", file=out)
    for prediction_type in prediction_types:
      if not balls[prediction_type]:
        continue
      print("@subgroup{%s}" % prediction_type, file=out)
      print("@balllist{balls} radius= 0.6 color= %s master= {bwa_balls}" % colors[prediction_type], file=out)
      for kinline in balls[prediction_type]:
        print(kinline, file=out)
      print("@labellist{labels} color= %s master= {bwa_labels}" % colors[prediction_type], file=out)
      for kinline in labels[prediction_type]:
        print(kinline, file=out)

  def old_as_kinemage(self, out=sys.stdout):
    # colored ball at each CA, color based on current synthesis
    # label with bc--go- style text showing components of decision
    balls = []
    labels = []
    for r in self.res_list:
      if not r.feedback:
        continue
      ball_color = None
      if r.feedback == "Predictive":
        ball_color = "sky"
      elif r.feedback == "Unphysical":
        ball_color = "purple"
      elif r.feedback == "Barbed wire":
        ball_color = "hotpink"
      elif r.feedback == "Pseudostructure": #"Unpacked possible":
        ball_color = "gold"
      elif r.feedback == "Near-predictive":
        ball_color = 'green'
      elif r.feedback == "Unpacked high pLDDT":
        ball_color = 'gray'
      else:
        ball_color = 'brown'
      ballline = "{%s %s %s}%s %.3f %.3f %.3f" % (r.resid, r.feedback, r.text_code, ball_color, r.caxyz[0], r.caxyz[1], r.caxyz[2])
      labelline = "{  %s}%s %.3f %.3f %.3f" % (r.text_code, ball_color, r.caxyz[0], r.caxyz[1], r.caxyz[2])
      balls.append(ballline)
      labels.append(labelline)
    print("@group {bwa markup}", file=out)
    print("@balllist {bwa balls} radius=0.6", file=out)
    for line in balls:
      print(line, file=out)
    print("@labellist {bwa labels}", file=out)
    for line in labels:
      print(line, file=out)

  def as_selection_string(self, modes=['1', '3']):
    #Return the selection syntax string for residues matching selected prediction modes
    mode_dict = {"1":"Predictive",
                 "2":"Unpacked high pLDDT",
                 "3":"Near-predictive",
                 "4":"Pseudostructure",
                 "5":"Barbed wire",
                 "6":"Unphysical"}
    modes_to_print = []
    for mode_key in mode_dict:
      if mode_key in modes:
        modes_to_print.append(mode_dict[mode_key])
    selection_list = []
    for c in self.chunk_list:
      if c.prediction_type in modes_to_print:
        selection_list.append(c.as_selection_string())
    return " or ".join(selection_list)

  def as_selection_file(self, model, out=sys.stdout, modes=['1', '3'], extension=".cif"):
    #Print a model file, mmcif or pdb based on input format, containing residues matching
    #  selected prediction modes
    hierarchy = model.get_hierarchy()
    atom_selection = self.as_selection_string(modes)
    sele = hierarchy.apply_atom_selection(atom_selection)
    if extension == ".cif":
      print(sele.as_mmcif_string(), file=out)
    else:
      print(sele.as_pdb_string(), file=out) #PDB OK

  # ----------------------END OUTPUT---------------------


 *******************************************************************************


 *******************************************************************************
mmtbx/validation/cablam.py
from __future__ import absolute_import, division, print_function
# (jEdit options) :folding=explicit:collapseFolds=1:
from mmtbx.validation import residue, validation, atom
from cctbx import geometry_restraints
from mmtbx.rotamer.n_dim_table import NDimTable #handles contours
from libtbx import easy_pickle #NDimTables are stored as pickle files
import libtbx.load_env
import os, sys
from iotbx.pdb.hybrid_36 import hy36decode
import json

#{{{ global constants
#-------------------------------------------------------------------------------
MAINCHAIN_ATOMS = [" N  "," C"  ," O  "," CA "]
CA_PSEUDOBOND_DISTANCE = 4.5
PEPTIDE_BOND_DISTANCE = 2.0
#2.0A is the peptide bond cutoff used by O for model building and potentially-
#  fragmented chains. O's generous cutoff seemed appropriate since I expect to
#  process in-progress models with this program
#RLab (probably) uses 1.4 +- 0.3, official textbook is about 1.33
#O's Ca-Ca distance is 4.5A
#Tom T suggests 2.5A as low-end cutoff for Ca-Ca distance
#--He also suggests using the pdb interpreter's internal chain break def

CABLAM_OUTLIER_CUTOFF = 0.01
CABLAM_DISFAVORED_CUTOFF = 0.05
CA_GEOM_CUTOFF = 0.005
#These cutoffs are used to identify outliers in CaBLAM parameter spaces
#These values were set heuristically through inspection of known outliers

ALPHA_CUTOFF = 0.001
BETA_CUTOFF = 0.0001
THREETEN_CUTOFF = 0.001
#These cutoffs are used to identify probable secondary structure residues
#Individual secondary structure residues are assembled into complete secondary
#  structure elements
#These values were set heuristically through inspection of known secondary
#  structure in low-resolution models
#-------------------------------------------------------------------------------
#}}}

#{{{ interpretation
#-------------------------------------------------------------------------------
def interpretation():
  #prints a brief guide to interpreting CaBLAM results
  sys.stderr.write("""
---------------- *** CaBLAM validation data interpretation *** -----------------

CaBLAM uses CA-trace geometry to validate protein backbone. Since the CA trace
  is more reliably modeled from low-resolution density than the full backbone
  trace, CaBLAM can identify modeling errors and intended secondary structure
  elements in low-resolution models.

Text:
  Text output is provided in colon-separated format. The columns are as follows:
    residue : A residue identifier
    outlier_type : If the residue is an outlier by CaBLAM metrics, the type will
      appear here. There are 3 types of outliers:
        CaBLAM Outlier is similar to Ramachandran 'Outlier' in severity
        CaBLAM Disfavored is similar to Ramachandran 'Allowed'
        CA Geom Outlier indicates a severe error in the CA trace
    contour_level : The CaBLAM contour percentile score for the residue.
      0.05 (5%) or lower is Disfavored.  0.01 (1%) or lower is Outlier.
    ca_contour_level : The CA geometry contour percentile score for the residue.
      Serves as a sanity check for CaBLAM's other metrics.
      0.005 (0.5%) or lower is Outlier.
    sec struc recommendation : Secondary structure identification for this
      residue. There are 3 possible identifications - try alpha helix,
      try beta sheet, and try three-ten.
    alpha score : The alpha helix contour percentile score for this residue.
      0.001 (0.1%) or better is probable helix.
    beta score : The beta strand contour percentile score for this residue.
      0.0001 (0.01%) or better is probable strand.
    three-ten score : The 3-10 helix contour percentile score for this residue.
      0.001 (0.1%) or better is probable helix.

Kinemage:
  Kinemage output is available for visual markup of structures in KiNG.
""")
#-------------------------------------------------------------------------------
#}}}

#{{{ math functions
#-------------------------------------------------------------------------------
def perptersect(a1, a2, b1):
  #Finds the line from a1 to a2, drops a perpendicular to it from b1, and returns
  #  the point of intersection.
  A = [a2[0]-a1[0], a2[1]-a1[1], a2[2]-a1[2]]
  #Find the slope of line A in each direction, A is in vector notation
  t = (A[0]*(b1[0]-a1[0]) + A[1]*(b1[1]-a1[1]) + A[2]*(b1[2]-a1[2])) / ((A[0]**2)+(A[1]**2)+(A[2]**2))
  #Solve the parametric equations (dot of perpendiculars=0). . .
  b2 = [a1[0]+A[0]*t, a1[1]+A[1]*t, a1[2]+A[2]*t]
  # . . . and use the result to find the new point b2 on the line
  return b2

def calculate_mu(CA1,CA2,CA3,CA4):
  #dihedral calculation for CA trace
  if None in [CA1,CA2,CA3,CA4]:
    return None
  return geometry_restraints.dihedral(sites=[CA1.xyz,CA2.xyz,CA3.xyz,CA4.xyz],
    angle_ideal=180, weight=1).angle_model

def calculate_ca_virtual_angle(CA1,CA2,CA3):
  #angle calculation for CA trace
  if None in [CA1,CA2,CA3]:
    return None
  return geometry_restraints.angle(sites=[CA1.xyz,CA2.xyz,CA3.xyz],
    angle_ideal=120, weight=1).angle_model

def calculate_nu(CA1,CA2,CA3,O1,O2):
  #dihedral calculation for peptide plane orientations
  if None in [CA1,CA2,CA3,O1,O2]:
    return None
  X1 = perptersect(CA1.xyz,CA2.xyz,O1.xyz)
  X2 = perptersect(CA2.xyz,CA3.xyz,O2.xyz)
  return geometry_restraints.dihedral(sites=[O1.xyz,X1,X2,O2.xyz],
    angle_ideal=180, weight=1).angle_model

def calculate_omega(CA1,C1,N2,CA2):
  #dihedral calculation for peptide bond
  if None in [CA1,C1,N2,CA2]:
    return None
  return geometry_restraints.dihedral(sites=[CA1.xyz,C1.xyz,N2.xyz,CA2.xyz],
    angle_ideal=180, weight=1).angle_model
#-------------------------------------------------------------------------------
#}}}

#{{{ contour fetching
#-------------------------------------------------------------------------------
def fetch_peptide_expectations():
  #This function finds, unpickles, and returns N-Dim Tables of expected residue
  #  behavior for use in determining cablam outliers
  #The return object is a dict keyed by residue type:
  #  'general','gly','transpro','cispro'
  #This set of contours defines peptide behavior (mu_in,mu_out,nu)
  categories = ['general','gly','transpro','cispro']
  unpickled = {}
  for category in categories:
    picklefile = libtbx.env.find_in_repositories(
      relative_path=(
        "chem_data/cablam_data/cablam.8000.expected."+category+".pickle"),
      test=os.path.isfile)
    if (picklefile is None):
      sys.stderr.write("\nCould not find a needed pickle file for category "+
        category+" in chem_data.\nPlease run mmtbx.rebuild_cablam_cache\nExiting.\n")
      sys.exit()
    ndt = easy_pickle.load(file_name=picklefile)
    unpickled[category] = ndt
  return unpickled

def fetch_ca_expectations():
  #This function finds, unpickles, and returns N-Dim Tables of expected residue
  #  behavior for use in determining ca geometry outliers
  #The return object is a dict keyed by residue type:
  #  'general','gly','transpro','cispro'
  #This set of contours defines CA trace quality (mu_in,mu_d_out,ca_virtual)
  categories = ['general','gly','transpro','cispro']
  unpickled = {}
  for category in categories:
    picklefile = libtbx.env.find_in_repositories(
      relative_path=(
        "chem_data/cablam_data/cablam.8000.expected."+category+"_CA.pickle"),
      test=os.path.isfile)
    if (picklefile is None):
      sys.stderr.write("\nCould not find a needed pickle file for category "+
        category+" in chem_data.\nPlease run mmtbx.rebuild_cablam_cache\nExiting.\n")
      sys.exit()
    ndt = easy_pickle.load(file_name=picklefile)
    unpickled[category] = ndt
  return unpickled

def fetch_motif_contours():
  #This function finds, unpickles, and returns N-Dim Tables of secondary structure
  #  structure behavior for use in determining what an outlier residue might
  #  really be
  #The return object is a dict keyed by secondary structure type:
  #  'regular_beta','loose_alpha','loose_threeten'
  motifs = ['regular_beta','loose_alpha','loose_threeten']
  unpickled = {}
  for motif in motifs:
    picklefile = libtbx.env.find_in_repositories(
      relative_path=(
        "chem_data/cablam_data/cablam.8000.motif."+motif+".pickle"),
      test=os.path.isfile)
    if (picklefile is None):
      sys.stderr.write("\nCould not find a needed pickle file for motif "+
        motif+" in chem_data.\nPlease run mmtbx.rebuild_cablam_cache\nExiting.\n")
      sys.exit()
    ndt = easy_pickle.load(file_name=picklefile)
    unpickled[motif] = ndt
  return unpickled
#-------------------------------------------------------------------------------
#}}}

#{{{ cablam data storage classes
#-------------------------------------------------------------------------------
class cablam_geometry():
  #holds cablam's geometry parameters for one residue
  def __init__(self, mu_in=None, mu_out=None, nu=None, ca_virtual=None, omega=None):
    self.mu_in = mu_in
    self.mu_out = mu_out
    self.nu = nu
    self.ca_virtual = ca_virtual
    self.omega=omega

class cablam_score():
  #holds cablam contour scores for one residue
  def __init__(self,cablam=None,c_alpha_geom=None,alpha=None,beta=None,threeten=None):
    self.cablam = cablam
    self.c_alpha_geom = c_alpha_geom
    self.alpha = alpha
    self.beta = beta
    self.threeten = threeten

class cablam_feedback():
  #holds outliers status and secondary structure identifications for one residue
  def __init__(self):
    self.cablam_outlier = None
    self.cablam_disfavored = None
    self.c_alpha_geom_outlier = None
    self.alpha=None
    self.beta=None
    self.threeten=None

#cablam results are stored by chains and by conformers within chains, in
#  parallel with the conformer organization of the pdb hierarchy.  These classes
#  handle that organization of the results
class cablam_chain():
  #chain-level organization for cablam results
  def __init__(self):
    self.conf_names = []
    self.confs = {}

  def __repr__(self):
    return str(self.conf_names)+str(self.confs)

class cablam_conf():
  #conformer-level organization for cablam results
  def __init__(self):
    self.conf_name = None
    self.results = {}
    self.sec_struc_records = []

  def __repr__(self):
    return str(self.conf_name)+str(self.results)+str(self.sec_struc_records)

class secondary_structure_segment():
  #holds a secondary structure element identified by cablam
  def __init__(self, start, end, segment_type, segment_length):
    self.start = start
    self.end = end
    self.segment_type = segment_type
    self.segment_length = segment_length

class wheel_wedge():
  #a wedge of space generated from rotating a peptide plane, and the corresponding cablam score
  #for drawing kinemage markup
  def __init__(self, start, end, offset, cablam_score):
    self.start = [start[0]-offset[0], start[1]-offset[1], start[2]-offset[2]]
    self.end = [end[0]-offset[0], end[1]-offset[1], end[2]-offset[2]]
    self.cablam_score = cablam_score
#-------------------------------------------------------------------------------
#}}}

#{{{ get_cablam_scores_from_atoms
#-------------------------------------------------------------------------------
def get_cablam_scores_from_atoms(CA0, CA1, CA2, CA3, CA4, O1, O2, residue_type='general', cablam_contours=None, ca_contours=None):
  #this funtion is meant for use by outside programs that already know the
  #  positions of the atoms CaBLAM needs
  #It takes 5 CA positions and 2 O positions and a residue type
  #  Positions should be x,y,z coords in list-like form
  #  Supported residue types are strings "general", "pro", and "gly"
  #  Assumes general case residue by default
  #  For this 0-indexed list of residues, residue #2 is the residue of interest
  #    and the one whose type matters
  #It returns a list of CaBLAM validation scores [CaBLAM,  CA geom]
  ca_in = geometry_restraints.dihedral(sites=[CA0,CA1,CA2,CA3],
    angle_ideal=180, weight=1).angle_model
  ca_out = geometry_restraints.dihedral(sites=[CA1,CA2,CA3,CA4],
    angle_ideal=180, weight=1).angle_model
  ca_virtual_angle = geometry_restraints.angle(sites=[CA1,CA2,CA3],
    angle_ideal=120, weight=1).angle_model
  X1 = perptersect(CA1,CA2,O1)
  X2 = perptersect(CA2,CA3,O2)
  peptide_dihedral = geometry_restraints.dihedral(sites=[O1,X1,X2,O2],
    angle_ideal=180, weight=1).angle_model
  if not cablam_contours:
    cablam_contours = fetch_peptide_expectations()
  if not ca_contours:
    ca_contours = fetch_ca_expectations()
  cablam_point = [ca_in, ca_out, peptide_dihedral]
  ca_point = [ca_in, ca_out, ca_virtual_angle]
  cablam_score = cablam_contours[residue_type].valueAt(cablam_point)
  c_alpha_geom_score = ca_contours[residue_type].valueAt(ca_point)
  return [cablam_score,c_alpha_geom_score]
#-------------------------------------------------------------------------------
#}}}

#{{{ cablam_result class
#-------------------------------------------------------------------------------
class cablam_result(residue):
  """
  Result class for cablam analysis
  """
  __slots__ = residue.__slots__ + [
    "residue",
    "prevres",
    "nextres",
    "_outlier_i_seqs",
    "has_ca",
    "has_mc",
    "has_any_alts",
    "has_mc_alts",
    "mc_alts",
    "alts",
    "measures",
    "scores",
    "feedback",
    "model_id"
    ]

  @staticmethod
  def header():
    return "%-12s %-19s %-7s %-7s %-17s %-7s %-7s %-7s" % (
      'Residue','Outlier type','Cablam','CA geom',' Sec structure','Alpha','Strand','3-10')

  #{{{ as_string
  #-----------------------------------------------------------------------------
  def as_string(self):
    #print text output
    outlist = self.as_list_for_text(outliers_only=False)
    if outlist is None:
      return ""
    else:
      return "%s %s %s %s %s %s %s %s" % (
        outlist[0], outlist[1], outlist[2], outlist[3], outlist[4], outlist[5], outlist[6], outlist[7])
  #-----------------------------------------------------------------------------
  #}}}

  def __repr__(self):
    return self.as_string()

  #{{{ mp_id
  #-----------------------------------------------------------------------------
  def mp_id(self):
    #Returns an id consistent with MolProbity 'cnit' ids
    #Formatted as: ccnnnnilttt
    #  c: 2-char Chain ID, space for none
    #  n: sequence number, right justified, space padded
    #  i: insertion code, space for none
    #  l: alternate ID, space for none
    #  t: residue type (ALA, LYS, etc.), all caps left justified, space padded
    chain = self.chain_id
    resnum = self.residue.resseq
    ins = self.residue.icode
    resname = self.resname
    alt = self.altloc
    if alt == '':
      alt = ' '
    return chain + resnum + ins + alt + resname
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ sorting_id
  #-----------------------------------------------------------------------------
  def sorting_id(self):
    #Returns an id used for sorting residues
    #Formatted as: ccnnnni
    #  c: 2-char Chain ID, space for none
    #  n: sequence number, right justified, space padded
    #  i: insertion code, space for none
    model = self.model_id
    chain = self.chain_id
    resnum = self.residue.resseq
    ins = self.residue.icode
    return model + chain + resnum + ins
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ get_atom
  #-----------------------------------------------------------------------------
  #returns the desired atom from a hierarchy residue object
  def get_atom(self, atom_name):
    #for atom in self.residue.atoms():
    #  if atom.name == atom_name: return atom
    atom = self.residue.find_atom_by(atom_name)
    return atom
    #else: return None
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ get_cablam_atoms
  #-----------------------------------------------------------------------------
  def get_cablam_atoms(self):
    #finds and returns all the atoms necessary for the CaBLAM calculations for
    #  this residue
    #returned atoms are:
    #res0_CA
    #res1_CA, res1_O, res1_C
    #res2_CA, res2_O, res2_N (res2 is the current residue)
    #res3_CA
    #res4_CA
    #returns None for missing/unfound atoms
    atom_set = {}
    atom_set["res2_CA"]=self.get_atom(" CA ")
    atom_set["res2_O"]= self.get_atom(" O  ")#for nu
    atom_set["res2_N"]= self.get_atom(" N  ")#for omega
    if self.prevres:
      atom_set["res1_CA"]=self.prevres.get_atom(" CA ")
      atom_set["res1_O"]= self.prevres.get_atom(" O  ")#for nu
      atom_set["res1_C"]= self.prevres.get_atom(" C  ")#for omega
      if self.prevres.prevres:
        atom_set["res0_CA"]=self.prevres.prevres.get_atom(" CA ")
      else:
        atom_set["res0_CA"]=None
    else:
      atom_set["res1_CA"]=None
      atom_set["res1_O"]= None
      atom_set["res1_C"]= None
      atom_set["res0_CA"]=None
    if self.nextres:
      atom_set["res3_CA"]=self.nextres.get_atom(" CA ")
      if self.nextres.nextres:
        atom_set["res4_CA"]=self.nextres.nextres.get_atom(" CA ")
      else:
        atom_set["res4_CA"]=None
    else:
      atom_set["res3_CA"]=None
      atom_set["res4_CA"]=None
    return atom_set
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ check_atoms
  #-----------------------------------------------------------------------------
  #checks whether atoms necessary for cablam calculations are present in this
  #  residue.
  #Sets has_ca = True if the residue has a CA atom (minimum cablam requirement)
  #Sets has_mc = True if the residue has all 4 mainchain heavy atoms
  def check_atoms(self):
    if self.get_atom(' CA ') is None: pass
    else:
      self.has_ca = True
      for atom_name in [' N  ',' C  ',' O  ']:
        if self.get_atom(atom_name) is None:
          break
      else:
        self.has_mc = True
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ link_residues
  #-----------------------------------------------------------------------------
  def link_residues(self, previous_result):
    #CaBLAM calculations depend on traversing sequential residues both backwards
    #  and forwards in sequence.  This function creates "links" between
    #  sequential residues.
    #Links are established if the residues are within probable bonding distance.
    #prevres and nextres values default to None during cablam_data.__init__()
    if previous_result is None:
      #no previous residue to link to
      return #Default link is None
    elif not previous_result.has_ca or not self.has_ca:
      #previous residue's proximity cannot be checked
      return
    elif not previous_result.has_mc or not self.has_mc:
      #CA-trace-only: use CA to check proximity
      ca1 = previous_result.get_atom(' CA ').xyz
      ca2 = self.get_atom(' CA ').xyz
      cadist = ((ca1[0]-ca2[0])**2 + (ca1[1]-ca2[1])**2 + (ca1[2]-ca2[2])**2)**0.5
      if cadist > CA_PSEUDOBOND_DISTANCE:
        #CA atoms are too far apart
        return
      else:
        #
        previous_result.nextres = self
        self.prevres = previous_result
    else: #has full set of mc heavy atoms available
      # use previous ' C  ' and current ' N  ' to check proximity
      c = previous_result.get_atom(' C  ').xyz
      n = self.get_atom(' N  ').xyz
      peptidedist = ((c[0]-n[0])**2 + (c[1]-n[1])**2 + (c[2]-n[2])**2)**0.5
      if peptidedist > PEPTIDE_BOND_DISTANCE:
        #atoms that would peptide bond are too far apart
        return
      else:
        previous_result.nextres = self
        self.prevres = previous_result
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ calculate_cablam_geomtery
  #-----------------------------------------------------------------------------
  def calculate_cablam_geometry(self):
    #populates self.measures with geometric measures relevant to CaBLAM
    #these measures are: mu_in, mu_out, nu, ca_virtual, omega
    atom_set = self.get_cablam_atoms()
    self.measures = cablam_geometry(
      mu_in  = calculate_mu(atom_set['res0_CA'],atom_set['res1_CA'],atom_set['res2_CA'],atom_set['res3_CA']),
      mu_out = calculate_mu(atom_set['res1_CA'],atom_set['res2_CA'],atom_set['res3_CA'],atom_set['res4_CA']),
      nu = calculate_nu(atom_set['res1_CA'],atom_set['res2_CA'],atom_set['res3_CA'],atom_set['res1_O'],atom_set['res2_O']),
      ca_virtual = calculate_ca_virtual_angle(atom_set['res1_CA'],atom_set['res2_CA'],atom_set['res3_CA']),
      omega = calculate_omega(atom_set['res1_CA'],atom_set['res1_C'],atom_set['res2_N'],atom_set['res2_CA'])
      )
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ contour_category
  #-----------------------------------------------------------------------------
  def contour_category(self):
    #determines the category of the current residue so that it can be paired
    #  with the correct contours
    #these categories are: 'general', 'gly', 'transpro', 'cispro'
    resname = self.resname.upper()
    if resname == "GLY": return "gly"
    elif resname == "PRO":
      if self.measures.omega is not None and self.measures.omega < 90 and self.measures.omega > -90:
        return "cispro"
      else: return "transpro"
    else: return "general"
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ calculate_contour_values
  #-----------------------------------------------------------------------------
  def calculate_contour_values(self, cablam_contours, ca_contours, motif_contours):
    #populates self.scores[alt] with contour values for the current residue
    #these contour values are: cablam, c_alpha_geom, alpha, beta, threeten
    category = self.contour_category()
    cablam_point = [self.measures. mu_in,self.measures.mu_out, self.measures.nu]
    ca_point = [self.measures.mu_in, self.measures.mu_out, self.measures.ca_virtual]
    motif_point = [self.measures.mu_in, self.measures.mu_out]
    if None in cablam_point: cablam=None
    else: cablam = cablam_contours[category].valueAt(cablam_point)
    if None in ca_point: c_alpha_geom=None
    else: c_alpha_geom = ca_contours[category].valueAt(ca_point)
    if None in motif_point: alpha, beta, threeten = 0, 0, 0
    else:
      alpha =    motif_contours['loose_alpha'].valueAt(motif_point)
      beta =     motif_contours['regular_beta'].valueAt(motif_point)
      threeten = motif_contours['loose_threeten'].valueAt(motif_point)
    self.scores = cablam_score(
      cablam=cablam,
      c_alpha_geom=c_alpha_geom,
      alpha=alpha,
      beta=beta,
      threeten=threeten)
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ set_cablam_feedback
  #-----------------------------------------------------------------------------
  def set_cablam_feedback(self):
    #populates self.feedback with True/False outlier status for easy access
    #these statuses are: cablam_outlier, cablam_disfavored, c_alpha_geom_outlier
    self.feedback = cablam_feedback()
    #outlier flags
    if self.scores.cablam is not None and self.scores.cablam < CABLAM_OUTLIER_CUTOFF:
      self.feedback.cablam_outlier = True
    else:
      self.feedback.cablam_outlier = False
    if self.scores.cablam is not None and self.scores.cablam < CABLAM_DISFAVORED_CUTOFF:
      self.feedback.cablam_disfavored = True
    else:
      self.feedback.cablam_disfavored = False
    if self.scores.c_alpha_geom is not None and self.scores.c_alpha_geom < CA_GEOM_CUTOFF:
      self.feedback.c_alpha_geom_outlier = True
    else:
      self.feedback.c_alpha_geom_outlier = False
    if self.feedback.cablam_outlier or self.feedback.cablam_disfavored or self.feedback.c_alpha_geom_outlier:
      self.outlier = True
    #secondary structure
    #This is semi-duplicated from assemble_secondary_structure
    if not self.prevres or not self.nextres or not self.prevres.scores or not self.nextres.scores:
      #alpha, beta, and threeten defaults are already None
      return
    if ((self.scores.alpha >= ALPHA_CUTOFF or self.scores.threeten >=THREETEN_CUTOFF)
      and (self.prevres.scores.alpha >= ALPHA_CUTOFF or self.prevres.scores.threeten >= THREETEN_CUTOFF)
      and (self.nextres.scores.alpha >= ALPHA_CUTOFF or self.nextres.scores.threeten >= THREETEN_CUTOFF)):
      if (self.scores.threeten > self.scores.alpha and
        (self.nextres.scores.threeten > self.nextres.scores.alpha or
          self.prevres.scores.threeten > self.prevres.scores.alpha)):
        self.feedback.threeten=True
      else:
        self.feedback.alpha=True
    if self.scores.beta >= BETA_CUTOFF and self.prevres.scores.beta >= BETA_CUTOFF and self.nextres.scores.beta >= BETA_CUTOFF:
      self.feedback.beta=True
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ find_single_outlier_type, find_single_structure_suggestion
  #-----------------------------------------------------------------------------
  def find_single_outlier_type(self):
    if self.feedback.c_alpha_geom_outlier:
      outlier_type = ' CA Geom Outlier    '
    elif self.feedback.cablam_outlier:
      outlier_type = ' CaBLAM Outlier     '
    elif self.feedback.cablam_disfavored:
      outlier_type = ' CaBLAM Disfavored  '
    else:
      outlier_type = '                    '
    return outlier_type

  def find_single_structure_suggestion(self):
    if self.feedback.c_alpha_geom_outlier:
      suggestion = '                 '
    elif self.feedback.threeten:
      suggestion = ' try three-ten   '
    elif self.feedback.alpha:
      suggestion = ' try alpha helix '
    elif self.feedback.beta:
      suggestion = ' try beta sheet  '
    else:
      suggestion = '                 '
    return suggestion
  #-----------------------------------------------------------------------------
  #}}}

  def as_JSON(self):
    non_serializable_list = ['residue','prevres','nextres','measures','scores','feedback']
    serializable_slots = [s for s in self.__slots__ if s not in non_serializable_list and hasattr(self, s)]
    slots_as_dict = ({s: getattr(self, s) for s in serializable_slots})
    slots_as_dict['outlier_type'] = self.find_single_outlier_type().strip()
    slots_as_dict['feedback'] = self.find_single_structure_suggestion().strip()
    slots_as_dict['scores'] = {
      "cablam": self.scores.cablam,
      "c_alpha_geom": self.scores.c_alpha_geom,
      "alpha": self.scores.alpha,
      "beta": self.scores.beta,
      "threeten": self.scores.threeten
    }
    slots_as_dict['measures'] = {
      "mu_in" : self.measures.mu_in,
      "mu_out" : self.measures.mu_out,
      "nu" : self.measures.nu,
      "ca_virtual" : self.measures.ca_virtual,
      "omega" : self.measures.omega
    }
    return json.dumps(slots_as_dict, indent=2)

  def as_hierarchical_JSON(self):
    hierarchical_dict = {}
    hierarchy_nest_list = ['model_id', 'chain_id', 'resid', 'altloc']
    return json.dumps(self.nest_dict(hierarchy_nest_list, hierarchical_dict), indent=2)

  #{{{ as_kinemage
  #-----------------------------------------------------------------------------
  def as_kinemage(self, mode=None, out=sys.stdout):
    #prints kinemage markup for this residue
    #has separate output modes for cablam outliers and for ca geom outliers
    if mode == 'ca_geom':
      if self.feedback.c_alpha_geom_outlier is not None:
        stats = self.mp_id() + " ca_geom=%.2f alpha=%.2f beta=%.2f three-ten=%.2f" %(self.scores.c_alpha_geom*100, self.scores.alpha*100, self.scores.beta*100, self.scores.threeten*100)
        CA_1 = self.prevres.get_atom(' CA ').xyz
        CA_2 = self.get_atom(' CA ').xyz
        CA_3 = self.nextres.get_atom(' CA ').xyz
        out.write('\n{'+stats+'} P '+str(CA_2[0]-(CA_2[0]-CA_1[0])*0.9)+' '+str(CA_2[1]-(CA_2[1]-CA_1[1])*0.9)+' '+str(CA_2[2]-(CA_2[2]-CA_1[2])*0.9))
        out.write('\n{'+stats+'} '+str(CA_2[0])+' '+str(CA_2[1])+' '+str(CA_2[2]))
        out.write('\n{'+stats+'} '+str(CA_2[0]-(CA_2[0]-CA_3[0])*0.9)+' '+str(CA_2[1]-(CA_2[1]-CA_3[1])*0.9)+' '+str(CA_2[2]-(CA_2[2]-CA_3[2])*0.9))
    elif mode == 'cablam':
      if self.feedback.cablam_outlier is not None:
        stats = self.mp_id() + " cablam=%.2f alpha=%.2f beta=%.2f three-ten=%.2f" %(self.scores.cablam*100, self.scores.alpha*100, self.scores.beta*100, self.scores.threeten*100)
        CA_1, O_1 = self.prevres.get_atom(' CA ').xyz,self.prevres.get_atom(' O  ').xyz
        CA_2, O_2 = self.get_atom(' CA ').xyz,self.get_atom(' O  ').xyz
        CA_3      = self.nextres.get_atom(' CA ').xyz
        X_1 = perptersect(CA_1,CA_2,O_1)
        X_2 = perptersect(CA_2,CA_3,O_2)
        midpoint = [ (X_1[0]+X_2[0])/2.0 , (X_1[1]+X_2[1])/2.0 , (X_1[2]+X_2[2])/2.0 ]
        out.write('\n{'+stats+'} P '+ str(O_1[0]) +' '+ str(O_1[1]) +' '+ str(O_1[2]))
        out.write('\n{'+stats+'} '+ str(X_1[0]) +' '+ str(X_1[1]) +' '+ str(X_1[2]))
        out.write('\n{'+stats+'} '+ str(midpoint[0]) +' '+ str(midpoint[1]) +' '+ str(midpoint[2]))
        out.write('\n{'+stats+'} '+ str(X_2[0]) +' '+ str(X_2[1]) +' '+ str(X_2[2]))
        out.write('\n{'+stats+'} '+ str(O_2[0]) +' '+ str(O_2[1]) +' '+ str(O_2[2]))
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ calculate_kinemage_wheels
  #-----------------------------------------------------------------------------
  def calculate_kinemage_wheels(self, cablam_contours):
    from scitbx.matrix import rotate_point_around_axis
    category = self.contour_category()
    CA_1 = self.prevres.get_atom(' CA ').xyz
    O_1  = self.prevres.get_atom(' O  ').xyz
    CA_2 = self.get_atom(' CA ').xyz
    O_2  = self.get_atom(' O  ').xyz
    CA_3 = self.nextres.get_atom(' CA ').xyz
    if None in [CA_1, CA_2, CA_3, O_1, O_2]: return
    #markup wheels should extend to less than the full CO length
    #moving the used CO position is an easy way to propagate this across calculations
    #along the X-O line used in the dihedral so as not to change the cablam results
    scaling = 0.75
    X1 = perptersect(CA_1,CA_2,O_1)
    X2 = perptersect(CA_2,CA_3,O_2)
    O_1 = ( (O_1[0]-X1[0])*scaling+X1[0], (O_1[1]-X1[1])*scaling+X1[1], (O_1[2]-X1[2])*scaling+X1[2])
    O_2 = ( (O_2[0]-X2[0])*scaling+X2[0], (O_2[1]-X2[1])*scaling+X2[1], (O_2[2]-X2[2])*scaling+X2[2])
    #-----------------------------
    #markup wheels are offset from the carbonyl oxygen position for visual clarity
    #offset is a move along the CA-CA line
    #calculate unit vector alone CA-CA line, offset is some fraction of that
    offset = 0.15
    CA_2_1 = (CA_1[0]-CA_2[0], CA_1[1]-CA_2[1], CA_1[2]-CA_2[2])
    CA_2_1_len = (CA_2_1[0]**2 + CA_2_1[1]**2 + CA_2_1[2]**2)**0.5
    CA_2_1_offset = (CA_2_1[0]/CA_2_1_len*offset, CA_2_1[1]/CA_2_1_len*offset, CA_2_1[2]/CA_2_1_len*offset)
    CA_2_3 = (CA_3[0]-CA_2[0], CA_3[1]-CA_2[1], CA_3[2]-CA_2[2])
    CA_2_3_len = (CA_2_3[0]**2 + CA_2_3[1]**2 + CA_2_3[2]**2)**0.5
    CA_2_3_offset = (CA_2_3[0]/CA_2_3_len*offset, CA_2_3[1]/CA_2_3_len*offset, CA_2_3[2]/CA_2_3_len*offset)

    #Each CaBLAM outlier is based on the relative positions of *two* peptide planes, represented by CO positions
    #Test rotation of each peptide plane independently, and draw a wheel for each
    #Starting with O_2 is arbitrary, but O_2 is the CO in the residue named as an outlier by CaBLAM convention
    angle = -10
    #prev_O_2_xyz = O_2
    wheel1_center = (X2[0]-CA_2_3_offset[0], X2[1]-CA_2_3_offset[1], X2[2]-CA_2_3_offset[2])
    wheel1 = []
    while angle < 360:
      angle += 10
      new_xyz = rotate_point_around_axis(
        axis_point_1 = CA_2,
        axis_point_2 = CA_3,
        point        = O_2,
        angle        = angle,
        deg          = True)
      new_nu = geometry_restraints.dihedral(sites=[O_1, X1, X2, new_xyz],
                                            angle_ideal=180, weight=1).angle_model
      cablam_point = [self.measures.mu_in, self.measures.mu_out, new_nu]
      cablam_score = cablam_contours[category].valueAt(cablam_point)
      if cablam_score >= 0.05:
        wheel1.append(None)
        continue
      wedge_start = rotate_point_around_axis(
        axis_point_1 = CA_2,
        axis_point_2 = CA_3,
        point        = O_2,
        angle        = angle-5,
        deg          = True)
      wedge_end = rotate_point_around_axis(
        axis_point_1=CA_2,
        axis_point_2=CA_3,
        point=O_2,
        angle=angle+5,
        deg=True)
      wedge = wheel_wedge(wedge_start, wedge_end, CA_2_3_offset, cablam_score)
      wheel1.append(wedge)

    angle = -10
    #prev_O_1_xyz = O_1
    wheel2_center = (X1[0]-CA_2_1_offset[0], X1[1]-CA_2_1_offset[1], X1[2]-CA_2_1_offset[2])
    wheel2 = []
    while angle <= 360:
      angle += 10
      new_xyz = rotate_point_around_axis(
        axis_point_1 = CA_1,
        axis_point_2 = CA_2,
        point        = O_1,
        angle        = angle,
        deg          = True)
      new_nu = geometry_restraints.dihedral(sites=[new_xyz,X1,X2,O_2],
        angle_ideal=180, weight=1).angle_model
      cablam_point = [self.measures.mu_in,self.measures.mu_out, new_nu]
      cablam_score = cablam_contours[category].valueAt(cablam_point)
      if cablam_score >= 0.05:
        wheel2.append(None)
        continue
      wedge_start = rotate_point_around_axis(
        axis_point_1=CA_1,
        axis_point_2=CA_2,
        point=O_1,
        angle=angle-5,
        deg=True)
      wedge_end = rotate_point_around_axis(
        axis_point_1=CA_1,
        axis_point_2=CA_2,
        point=O_1,
        angle=angle+5,
        deg=True)
      wedge = wheel_wedge(wedge_start, wedge_end, CA_2_1_offset, cablam_score)
      wheel2.append(wedge)
    return [(wheel1, wheel1_center), (wheel2, wheel2_center)]
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ as_kinemage_point
  #-----------------------------------------------------------------------------
  def as_kinemage_point(self, out=sys.stdout):
    #printing for pointcloud kinemage output
    if not (self.measures.mu_in and self.measures.mu_out and self.measures.nu):
      return
    point_name = "{"+self.mp_id()+"}"
    print(point_name, "%.2f %.2f %.2f" % (self.measures.mu_in, self.measures.mu_out, self.measures.nu), file=out)
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ is_same_as_other_result
  #-----------------------------------------------------------------------------
  def is_same_as_other_result(self,other_result):
    #Compare this result object to another to see if they are effectively the
    #  same.  Identical cablam geometry (mu_in, mu_out, and nu) is assumed to
    #  mean identical residues:
    #This method will probably change
    if (self.measures.mu_in != other_result.measures.mu_in
      or self.measures.mu_out != other_result.measures.mu_out
      or self.measures.nu != other_result.measures.nu):
      return False
    return True
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ as_list_for_text
  #-----------------------------------------------------------------------------
  def as_list_for_text(self, outliers_only=False):
    if not self.has_ca:
      return None
    if outliers_only:
      if not (self.feedback.cablam_disfavored or self.feedback.c_alpha_geom_outlier):
        return None

    if self.feedback.c_alpha_geom_outlier:
      outlier_type = ' CA Geom Outlier    '
    elif self.feedback.cablam_outlier:
      outlier_type = ' CaBLAM Outlier     '
    elif self.feedback.cablam_disfavored:
      outlier_type = ' CaBLAM Disfavored  '
    else:
      outlier_type = '                    '

    if self.scores.cablam is not None:
      cablam_level = '%.5f' %self.scores.cablam
    else:
      cablam_level = '       ' #default printing for CA-only models
    if self.scores.c_alpha_geom is not None:
      ca_geom_level = '%.5f' %self.scores.c_alpha_geom
    else:
      return None #if this is missing, there's nothing

    if self.feedback.c_alpha_geom_outlier:
      suggestion = '                 '
    elif self.feedback.threeten:
      suggestion = ' try three-ten   '
    elif self.feedback.alpha:
      suggestion = ' try alpha helix '
    elif self.feedback.beta:
      suggestion = ' try beta sheet  '
    else:
      suggestion = '                 '

    outlist = [self.mp_id() ,outlier_type, cablam_level, ca_geom_level, suggestion, '%.5f' %self.scores.alpha, '%.5f' %self.scores.beta, '%.5f' %self.scores.threeten]
    return outlist
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ as_table_row_phenix
  #-----------------------------------------------------------------------------
  def as_table_row_phenix(self):
    return [ self.chain_id,
      "%s %s" % (self.resname, self.resid),
      self.find_single_outlier_type().strip(),
      self.scores.cablam,
      self.scores.c_alpha_geom,
      self.find_single_structure_suggestion().strip(),
      self.scores.alpha,
      self.scores.beta,
      self.scores.threeten ]
  #-----------------------------------------------------------------------------
  #}}}
#-------------------------------------------------------------------------------
#}}}

#{{{ cablamalyze class
#-------------------------------------------------------------------------------
class cablamalyze(validation):
  """
  Frontend for calculating cablam statistics for a model
  """
  __slots__ = validation.__slots__ + [
    "residue_count",
    "outlier_count",
    "out",
    "pdb_hierarchy",
    "all_results",
    "summary_stats"
    ]

  program_description = "Analyze protein CA geometry for secondary structure identification - recommended for low-resolution structures"

  gui_list_headers = ["Chain","Residue","Evaluation","CaBLAM Score","CA Geometry Score","Secondary Structure","Helix Score","Beta Score","3-10 Score"]
  gui_formats = ["%s", "%s", "%s", "%.5f", "%.5f", "%s", "%.5f", "%.5f", "%.5f"]
  wx_column_widths = [125]*9

  def get_result_class(self): return cablam_result

  #{{{ __init__
  #-----------------------------------------------------------------------------
  def __init__(self,
    pdb_hierarchy,
      outliers_only,
      out,
      quiet,
      cablam_contours=None,
      ca_contours=None,
      motif_contours=None):
    validation.__init__(self)
    from mmtbx.validation import utils
    from scitbx.array_family import flex
    #self._outlier_i_seqs = flex.size_t()
    self.out = out
    if cablam_contours is None:
      cablam_contours = fetch_peptide_expectations()
    if ca_contours is None:
      ca_contours = fetch_ca_expectations()
    if motif_contours is None:
      motif_contours = fetch_motif_contours()
    self.pdb_hierarchy = pdb_hierarchy.deep_copy()
    pdb_atoms = pdb_hierarchy.atoms()
    all_i_seqs = pdb_atoms.extract_i_seq()
    if all_i_seqs.all_eq(0):
      pdb_atoms.reset_i_seq()
    use_segids = utils.use_segids_in_place_of_chainids(
      hierarchy=pdb_hierarchy)

    self.all_results = {}
    ordered_keys = {}
    all_keys = []
    confs = []
    for model in pdb_hierarchy.models():
      self.all_results[model.id] = {}
      for chain in model.chains():
        if not chain.is_protein():
          continue
        for conf in chain.conformers():
          if conf.is_protein(): break #at least one conformer must be protein
        else: continue
        if use_segids:
          chain_id = utils.get_segid_as_chainid(chain=chain)
        else:
          chain_id = chain.id
        if len(chain_id) < 2:
          chain_id = chain_id.rjust(2)
        #The above .rjust(2)'s are to force 2-char chain ids
        current_chain = cablam_chain()
        self.all_results[model.id][chain_id] = current_chain
        previous_result = None
        for conf in chain.conformers():
          if not conf.is_protein():
            continue
          current_conf = cablam_conf()
          current_conf.conf_name = conf.altloc
          current_chain.confs[conf.altloc] = current_conf
          current_chain.conf_names.append(conf.altloc)
          for residue in conf.residues():
            result = cablam_result(
              model_id=model.id,
              residue=residue,
              resseq=residue.resseq,
              icode=residue.icode,
              altloc=conf.altloc,
              chain_id=chain_id,
              resname = residue.resname,
              prevres=None,
              nextres=None,
              has_ca=False,
              has_mc=False,
              outlier = False,
              measures=None,
              scores=None
              )
            result.check_atoms()
            result.link_residues(previous_result)
            #Occasionally a conformer may have more than one "residue" that has
            # the same sorting_id (sorting_id is just chain+resseq+icode)
            # see phenix_regression/pdb/lysozyme_nohoh_plus6H.pdb, where WAT 14
            # and ARG 14 have the same sorting_id
            #This check my not be the correct behavior to catch such a
            # formatting error.
            if result.sorting_id() not in current_conf.results:
              current_conf.results[result.sorting_id()] = result
            previous_result = result
    for model_id in self.all_results:
      for chain in self.all_results[model_id]:
        for conf in self.all_results[model_id][chain].confs:
          for result in self.all_results[model_id][chain].confs[conf].results:
            if self.all_results[model_id][chain].confs[conf].results[result].has_ca:
              self.all_results[model_id][chain].confs[conf].results[result].calculate_cablam_geometry()
              self.all_results[model_id][chain].confs[conf].results[result].calculate_contour_values(cablam_contours, ca_contours, motif_contours)
              self.all_results[model_id][chain].confs[conf].results[result].xyz = self.all_results[model_id][chain].confs[conf].results[result].get_atom(' CA ').xyz
    self.outlier_count = 0
    for model_id in self.all_results:
      for chain in self.all_results[model_id]:
        for conf in self.all_results[model_id][chain].confs:
          for result in self.all_results[model_id][chain].confs[conf].results:
            if self.all_results[model_id][chain].confs[conf].results[result].has_ca:
              self.all_results[model_id][chain].confs[conf].results[result].set_cablam_feedback()
              if self.all_results[model_id][chain].confs[conf].results[result].outlier:
                self.outlier_count += 1
    self.assemble_secondary_structure()
    self.make_single_results_object(confs, all_keys)
    self.residue_count = len(self.results)
    self.summary_stats = self.make_summary_stats()
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ assemble_secondary_structure
  #-----------------------------------------------------------------------------
  def assemble_secondary_structure(self):
    #assembles complete secondary structure elements (alpha helices, three-ten
    #  helices, and beta strands) from individual residue
    #have to assemble from scores to handle helix transitions
    for model_id in self.all_results:
      for chain in self.all_results[model_id]:
        for conf_id in self.all_results[model_id][chain].confs:
          conf = self.all_results[model_id][chain].confs[conf_id]
          records = []
          record_start = None
          helix_in_progress = False
          result_ids = list(conf.results.keys())
          #result_ids.sort(key=lambda k: (k[0:2], int(hy36decode(4,k[2:6])), k[6:7])) #this broke for non 2-char segids
          result_ids.sort(key=lambda k: (conf.results[k].chain_id, int(hy36decode(len(conf.results[k].resseq),conf.results[k].resseq)), conf.results[k].icode))
          for result_id in result_ids:
            result = conf.results[result_id]
            #is it evaluable?
            if not result.prevres or not result.prevres.scores:
              continue
            if not result.has_ca or not result.nextres or not result.nextres.scores:
              if helix_in_progress:
                records.append(secondary_structure_segment(start=record_start, end=result.prevres, segment_type=helix_in_progress, segment_length=record_length))
                helix_in_progress = False
              continue

            #helix building
            #This requires that the residues be the center of three in any combination of helix types
            #threeten segments of only 2 are lost in this method relative to the previous
            if ((result.scores.alpha >= ALPHA_CUTOFF or result.scores.threeten >=THREETEN_CUTOFF)
              and (result.prevres.scores.alpha >= ALPHA_CUTOFF or result.prevres.scores.threeten >= THREETEN_CUTOFF)
              and (result.nextres.scores.alpha >= ALPHA_CUTOFF or result.nextres.scores.threeten >= THREETEN_CUTOFF)):
              #now determine which helix type the current residue should be identified as
              #if at least two residues in a row have higher threeten scores than alpha scores, they can be considered threeten
              if (result.scores.threeten > result.scores.alpha and
                (result.nextres.scores.threeten > result.nextres.scores.alpha or
                  result.prevres.scores.threeten > result.prevres.scores.alpha)):
                thisres = 'threeten'
              else:
                thisres = 'alpha'
              if helix_in_progress:
                if thisres == helix_in_progress: #is it same as previous residue
                  record_length += 1
                  continue
                else: #or has it changed helix types
                  records.append(secondary_structure_segment(start=record_start, end=result.prevres, segment_type=helix_in_progress, segment_length=record_length))
                  helix_in_progress = thisres
                  record_start = result
                  record_length = 1
              else:
                helix_in_progress = thisres
                record_start = result
                record_length = 1
            else: #(current residue is not helix)
              if helix_in_progress:
                #might fail on chain breaks?
                records.append(secondary_structure_segment(start=record_start, end=result.prevres, segment_type=helix_in_progress, segment_length=record_length))
                helix_in_progress = False
                record_start = None
                record_length = 0
              else:
                continue
          #helix building end

          #beta strands require another, separate pass
          strand_in_progress = False
          record_start = None
          for result_id in result_ids:
            result = conf.results[result_id]
            if not result.prevres or not result.prevres.scores:
              continue
            if not result.has_ca or not result.nextres or result.nextres.scores:
              if strand_in_progress:
                records.append(secondary_structure_segment(start=record_start, end=result.prevres, segment_type='beta', segment_length=record_length))
                strand_in_progress = False
              continue
            if result.scores and result.prevres.scores and result.nextres.scores and result.scores.beta >= BETA_CUTOFF and result.prevres.scores.beta >= BETA_CUTOFF and result.nextres.scores.beta >= BETA_CUTOFF:
              if strand_in_progress:
                record_length += 1
                continue
              else:
                strand_in_progress = True
                record_start = result
                record_length = 1
            else:
              if strand_in_progress:
                records.append(secondary_structure_segment(start=record_start, end=result.prevres, segment_type='beta', segment_length=record_length))
                strand_in_progress = False
                record_start = None
                record_length = 0
              else:
                continue
          #beta strand building end
          #NOTE: Each strand is currently treated as its own sheet
          #Developing or implementing strand-to-sheet assembly that does not rely on
          #  H-bonds is a major future goal
          conf.sec_struc_records = records
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ make_single_results_object
  #-----------------------------------------------------------------------------
  def make_single_results_object(self, confs, all_keys):
    #should work without any arguments
    #populates self.results
    self.results = []
    for model_id in self.all_results:
      chains = list(self.all_results[model_id].keys())
      chains.sort()
      for chain_id in chains:
        chain = self.all_results[model_id][chain_id]
        #take the first conformer as the basis for comparison
        conf  = chain.confs[chain.conf_names[0]]
        if len(chain.conf_names) == 0:
          for result_id in conf.results:
            result = conf.results[result_id]
            result.altloc = ''
            #set self.results id
          continue #go to next chain
        #else, combine results into single list
        result_ids = list(conf.results.keys())
        ###result_ids.sort()
        #for result_id in conf.results:
        for result_id in result_ids:
          result = conf.results[result_id]
          if not result.has_ca: continue
          #results without CAs have measures=None and break the
          #  is_same_as_other_result check. Also, they aren't evaluable residues.
          self.results.append(result)
          found_meaningful_alt = False
          for other_conf in chain.conf_names[1:]:
            if result.sorting_id() in chain.confs[other_conf].results:
              other_result = chain.confs[other_conf].results[result.sorting_id()]
              if not other_result.has_ca: continue
              if not result.is_same_as_other_result(other_result):
                self.results.append(other_result)
                found_meaningful_alt = True
          if not found_meaningful_alt:
            result.altloc = ''
            #set self.results id
            pass
    self.results.sort(key=lambda r: (r.chain_id, int(hy36decode(len(r.resseq),r.resseq)), r.icode, r.altloc))
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ as_text
  #-----------------------------------------------------------------------------
  def as_text(self, outliers_only=False):
    #prints colon-separated text for CaBLAM validation
    #one line per residue (or alternate)
    #Output is formatted to be human-readable, and is also used by MolProbity
    #This is the default output for running this script from commandline
    self.out.write('residue : outlier_type : contour_level : ca_contour_level : sec struc recommendation : alpha score : beta score : three-ten score')
    for result in self.results:
      if not result.has_ca:
        continue
      #if not result.feedback:
      #  continue
      if outliers_only:
        if not (result.feedback.cablam_disfavored or result.feedback.c_alpha_geom_outlier):
          continue

      if result.feedback.c_alpha_geom_outlier:
        outlier_type = ' CA Geom Outlier    '
      elif result.feedback.cablam_outlier:
        outlier_type = ' CaBLAM Outlier     '
      elif result.feedback.cablam_disfavored:
        outlier_type = ' CaBLAM Disfavored  '
      else:
        outlier_type = '                    '

      if result.scores.cablam is not None:
        cablam_level = '%.5f' %result.scores.cablam
      else:
        cablam_level = '       ' #default printing for CA-only models
      if result.scores.c_alpha_geom is not None:
        ca_geom_level = '%.5f' %result.scores.c_alpha_geom
      else:
        continue #if this is missing, there's nothing

      if result.feedback.c_alpha_geom_outlier:
        suggestion = '                 '
      elif result.feedback.threeten:
        suggestion = ' try three-ten   '
      elif result.feedback.alpha:
        suggestion = ' try alpha helix '
      elif result.feedback.beta:
        suggestion = ' try beta sheet  '
      else:
        suggestion = '                 '

      outlist = [result.mp_id() ,outlier_type, cablam_level, ca_geom_level, suggestion, '%.5f' %result.scores.alpha, '%.5f' %result.scores.beta, '%.5f' %result.scores.threeten]
      self.out.write('\n'+':'.join(outlist))
    self.out.write('\n')
    self.show_summary(out=self.out,prefix="")
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ make_summary_stats
  #-----------------------------------------------------------------------------
  def make_summary_stats(self):
    #calculates whole-model stats used by show_summary, gui_summary, and
    #  as_oneline
    residue_count = 0
    ca_residue_count = 0
    cablam_outliers = 0
    cablam_disfavored = 0
    ca_geom_outliers = 0
    prev_result_id = None
    is_residue = 0
    is_cablam_outlier = 0
    is_cablam_disfavored = 0
    is_ca_geom_outlier = 0
    alpha_count, beta_count, threeten_count = 0,0,0
    is_alpha, is_beta, is_threeten = 0,0,0
    correctable_helix_count, correctable_beta_count = 0,0
    is_correctable_helix, is_correctable_beta = 0,0
    residue_count_by_model = {}
    outlier_count_by_model = {}
    disfavored_count_by_model = {}
    ca_geom_outlier_count_by_model = {}
    for result in self.results:
      if not result.has_ca:
        continue
      if result.model_id not in residue_count_by_model:
        residue_count_by_model[result.model_id] = 0
        outlier_count_by_model[result.model_id] = 0
        disfavored_count_by_model[result.model_id] = 0
        ca_geom_outlier_count_by_model[result.model_id] = 0
      is_residue = 1
      if result.scores.cablam is None:
        is_residue = 0
      if result.sorting_id() != prev_result_id:
        #new residue; update counts
        residue_count    += is_residue
        residue_count_by_model[result.model_id] += is_residue
        if result.scores.c_alpha_geom is not None:
          ca_residue_count += 1
        cablam_outliers  += is_cablam_outlier
        cablam_disfavored+= is_cablam_disfavored
        ca_geom_outliers += is_ca_geom_outlier
        outlier_count_by_model[result.model_id] += is_cablam_outlier
        disfavored_count_by_model[result.model_id] += is_cablam_disfavored
        ca_geom_outlier_count_by_model[result.model_id] += is_ca_geom_outlier
        is_cablam_outlier    = 0
        is_cablam_disfavored = 0
        is_ca_geom_outlier   = 0
        alpha_count    += is_alpha
        beta_count     += is_beta
        threeten_count += is_threeten
        is_alpha, is_beta, is_threeten = 0,0,0
        correctable_helix_count += is_correctable_helix
        correctable_beta_count += is_correctable_beta
        is_correctable_helix, is_correctable_beta = 0,0
      if result.scores.cablam is not None and result.scores.cablam < CABLAM_OUTLIER_CUTOFF and is_residue:
        is_cablam_outlier    = 1
        if result.feedback.alpha or result.feedback.threeten:
          is_correctable_helix = 1
        elif result.feedback.beta:
          is_correctable_beta = 1
      if result.scores.cablam is not None and result.scores.cablam < CABLAM_DISFAVORED_CUTOFF and is_residue:
        is_cablam_disfavored = 1
      if result.scores.c_alpha_geom is not None and result.scores.c_alpha_geom < CA_GEOM_CUTOFF:
        is_ca_geom_outlier = 1
      #---parse secondary structure---
      if result.feedback.alpha and not is_beta and not is_threeten:
        is_alpha = 1
      elif result.feedback.beta and not is_alpha and not is_threeten:
        is_beta = 1
      elif result.feedback.threeten and not is_alpha and not is_beta:
        is_threeten = 1
      #---endparse secondary structure---
      prev_result_id = result.sorting_id()
    residue_count    += is_residue
    cablam_outliers  += is_cablam_outlier
    cablam_disfavored+= is_cablam_disfavored
    ca_geom_outliers += is_ca_geom_outlier
    #outlier_count_by_model[result.model_id] += is_cablam_outlier
    #disfavored_count_by_model[result.model_id] += is_cablam_disfavored
    #ca_geom_outlier_count_by_model[result.model_id] += is_ca_geom_outlier
    alpha_count    += is_alpha
    beta_count     += is_beta
    threeten_count += is_threeten
    return {'residue_count':residue_count,'ca_residue_count':ca_residue_count,
      'cablam_outliers':cablam_outliers,'cablam_disfavored':cablam_disfavored,'ca_geom_outliers':ca_geom_outliers,
      'alpha_count':alpha_count, 'beta_count':beta_count,'threeten_count':threeten_count,
      'correctable_helix_count':correctable_helix_count,'correctable_beta_count':correctable_beta_count,
      'num_cablam_outliers_by_model': outlier_count_by_model,
      'num_cablam_disfavored_by_model': disfavored_count_by_model,
      'num_ca_geom_outliers_by_model': ca_geom_outlier_count_by_model,
      'num_residues_by_model': residue_count_by_model}
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ as_oneline
  #-----------------------------------------------------------------------------
  def as_oneline(self,pdbid='pdbid'):
    #prints a one-line summary of cablam statistics for a structure
    #for oneline purposes, alternates are collapsed: each residue contributes up
    #  to 1 to each outlier count, regarless of how many outlier alternates it
    #  may contain
    if self.count_residues() == 0:
      self.out.write(pdbid+':0:0:0:0\n')
    else:
      self.out.write('%s:%i:%.1f:%.1f:%.2f\n' %(pdbid,self.count_residues(), self.percent_outliers(), self.percent_disfavored(), self.percent_ca_outliers()) )
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ Summary retrieval functions
  #-----------------------------------------------------------------------------
  def count_residues(self):
    return self.summary_stats['residue_count']
  def count_ca_residues(self):
    return self.summary_stats['ca_residue_count']
  def count_outliers(self):
    return self.summary_stats['cablam_outliers']
  def percent_outliers(self):
    if self.count_residues() == 0:
      return 0
    return self.count_outliers()/self.count_residues()*100
  def count_disfavored(self):
    return self.summary_stats['cablam_disfavored']
  def percent_disfavored(self):
    if self.count_residues() == 0:
      return 0
    return self.count_disfavored()/self.count_residues()*100
  def count_ca_outliers(self):
    return self.summary_stats['ca_geom_outliers']
  def percent_ca_outliers(self):
    if self.count_ca_residues() == 0:
      return 0
    return self.count_ca_outliers()/self.count_ca_residues()*100
  def count_helix(self):
    return self.summary_stats['alpha_count']+self.summary_stats['threeten_count']
  def count_beta(self):
    return self.summary_stats['beta_count']
  def percent_helix(self):
    if self.count_ca_residues() == 0:
      return 0
    return self.count_helix()/self.count_ca_residues()*100
  def percent_beta(self):
    if self.count_ca_residues() == 0:
      return 0
    return self.count_beta()/self.count_ca_residues()*100
  def count_correctable_helix(self):
    return self.summary_stats['correctable_helix_count']
  def count_correctable_beta(self):
    return self.summary_stats['correctable_beta_count']
  def percent_correctable_helix(self):
    if self.count_residues() == 0:
      return 0
    return self.count_correctable_helix()/self.count_residues()*100
  def percent_correctable_beta(self):
    if self.count_residues() == 0:
      return 0
    return self.count_correctable_beta()/self.count_residues()*100
  #-----------------------------------------------------------------------------
  #}}}

  def cablam_wheel_triangle(self, wheel_center, wedge, color):
    self.out.write('\n{} P X %s %.3f %.3f %.3f' % (color, wheel_center[0], wheel_center[1], wheel_center[2]))
    self.out.write('\n{} %s %.3f %.3f %.3f' % (color, wedge.start[0], wedge.start[1], wedge.start[2]))
    self.out.write('\n{} %s %.3f %.3f %.3f' % (color, wedge.end[0], wedge.end[1], wedge.end[2]))

  def cablam_wheel_edge(self, wheel_center, wedge, prevwedge):
    pass

  #{{{ as_kinemage
  #-----------------------------------------------------------------------------
  def as_kinemage(self):
    #output cablam validation as standalone kinemage markup for viewing in KiNG
    self.out.write('\n@subgroup {cablam disfavored} dominant\n')
    self.out.write('@vectorlist {cablam disfavored} color= purple width= 4 master={cablam disfavored} off') #default off
    for result in self.results:
      if not result.has_ca:
        continue
      if result.feedback.cablam_disfavored:
        result.as_kinemage(mode="cablam", out=self.out)
    self.out.write('\n@subgroup {cablam outlier} dominant\n')
    self.out.write('@vectorlist {cablam outlier} color= magenta width= 4 master={cablam outlier}') #default on
    for result in self.results:
      if not result.has_ca:
        continue
      if result.feedback.cablam_outlier:
        result.as_kinemage(mode="cablam", out=self.out)
    self.out.write('\n@subgroup {ca geom outlier} dominant\n')
    self.out.write('@vectorlist {ca geom outlier} color= red width= 4 master={ca geom outlier}') #default on
    for result in self.results:
      if not result.has_ca:
        continue
      if result.feedback.c_alpha_geom_outlier:
        result.as_kinemage(mode="ca_geom", out=self.out)
    #----------------------------
    #"wheels" show favorable and unfavorabe regions for each peptide plane involved in a cablam outlier
    #Some additional calculations are required to generate these wheels
    cablam_contours = fetch_peptide_expectations()
    wheels_list = []
    for result in self.results:
      if not result.has_ca:
        continue
      if result.feedback.cablam_disfavored:
        wheels_list.extend(result.calculate_kinemage_wheels(cablam_contours=cablam_contours))
    #wheels are made of 10-degree wedges, each wedge drawn as a triangle
    self.out.write('\n@subgroup {cablam_wheels} dominant master={cablam wheels}\n')
    self.out.write('@trianglelist {cablam_wheels} alpha=0.75')
    for cablam_wheel in wheels_list:
      wheel = cablam_wheel[0]
      wheel_center = cablam_wheel[1]
      for wedge in wheel:
        if wedge is None:
          continue
        elif wedge.cablam_score < 0.01:
          color = 'magenta'
        else:
          color = 'purple'
        self.cablam_wheel_triangle(wheel_center, wedge, color)
        #self.out.write('\n{} P X %s %.3f %.3f %.3f' % (color, wheel_center[0], wheel_center[1], wheel_center[2]))
        #self.out.write('\n{} %s %.3f %.3f %.3f' % (color, wedge.start[0], wedge.start[1], wedge.start[2]))
        #self.out.write('\n{} %s %.3f %.3f %.3f' % (color, wedge.end[0], wedge.end[1], wedge.end[2]))
    #a thin black line outlining the wheel greatly aids visual interpretation
    self.out.write('\n@vectorlist {cablam_wheels_lines} color=deadblack width= 1 alpha=0.75')
    for cablam_wheel in wheels_list:
      wheel = cablam_wheel[0]
      wheel_center = cablam_wheel[1]
      prevwedge = wheel[-1]
      new_poly = ' P' #starts a new polyline in kinemage format, print this for each new wheel
      for wedge in wheel:
        if wedge and prevwedge:
          self.out.write('\n{}%s %.3f %.3f %.3f' % (new_poly, wedge.start[0], wedge.start[1], wedge.start[2]))
          self.out.write('\n{} %.3f %.3f %.3f' % (wedge.end[0], wedge.end[1], wedge.end[2]))
        elif wedge and not prevwedge:
          self.out.write('\n{}%s %.3f %.3f %.3f' % (new_poly, wheel_center[0], wheel_center[1], wheel_center[2]))
          self.out.write('\n{} %.3f %.3f %.3f' % (wedge.start[0], wedge.start[1], wedge.start[2]))
          self.out.write('\n{} %.3f %.3f %.3f' % (wedge.end[0], wedge.end[1], wedge.end[2]))
        elif prevwedge and not wedge:
          self.out.write('\n{}%s %.3f %.3f %.3f' % (new_poly, prevwedge.end[0], prevwedge.end[1], prevwedge.end[2]))
          self.out.write('\n{} %.3f %.3f %.3f' % (wheel_center[0], wheel_center[1], wheel_center[2]))
        else:
          prevwedge = wedge
          continue
        prevwedge = wedge
        new_poly=''
    #----------------------------
    self.out.write('\n')
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ as_full_kinemage
  #-----------------------------------------------------------------------------
  def as_full_kinemage(self,pdbid=''):
    #output cablam validation as kinemage markup on pdb model. Future version
    #  of this will also print ribbons, but standalone ribbon code must be
    #  developed first.
    #Pdb-to-kinemage printing has been hijacked from mmtbx.kinemage.validation
    #  That code was not meant to be used outside its original context, so this
    #  may be fragile.
    from mmtbx.kinemage.validation import get_kin_lots, build_name_hash
    from mmtbx import monomer_library
    from mmtbx.monomer_library import pdb_interpretation
    i_seq_name_hash = build_name_hash(pdb_hierarchy=self.pdb_hierarchy)
    sites_cart=self.pdb_hierarchy.atoms().extract_xyz()
    mon_lib_srv = monomer_library.server.server()
    ener_lib = monomer_library.server.ener_lib()
    processed_pdb_file = pdb_interpretation.process(
        mon_lib_srv=mon_lib_srv,
        ener_lib=ener_lib,
        pdb_hierarchy=self.pdb_hierarchy,
        #params=work_params.kinemage.pdb_interpretation,
        substitute_non_crystallographic_unit_cell_if_necessary=True)
    geometry = processed_pdb_file.geometry_restraints_manager()
    flags = geometry_restraints.flags.flags(default=True)
    #angle_proxies = geometry.angle_proxies
    pair_proxies = geometry.pair_proxies(flags=flags, sites_cart=sites_cart)
    bond_proxies = pair_proxies.bond_proxies
    quick_bond_hash = {}
    for bp in bond_proxies.simple:
      if (i_seq_name_hash[bp.i_seqs[0]][9:14] == i_seq_name_hash[bp.i_seqs[1]][9:14]):
        if quick_bond_hash.get(bp.i_seqs[0]) is None:
          quick_bond_hash[bp.i_seqs[0]] = []
        quick_bond_hash[bp.i_seqs[0]].append(bp.i_seqs[1])

    for model in self.pdb_hierarchy.models():
      for chain in model.chains():
        self.out.write(get_kin_lots(chain, bond_hash=quick_bond_hash, i_seq_name_hash=i_seq_name_hash, pdbID=pdbid, index=0, show_hydrogen=True))
    self.as_kinemage()
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ as_pointcloud_kinemage
  #-----------------------------------------------------------------------------
  def as_pointcloud_kinemage(self):#, out=self.out):
    print("@group {cablam-space points} dominant", file=self.out)
    print("@dotlist (cablam-space points)", file=self.out)
    for result in self.results:
      result.as_kinemage_point()
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ as_secondary_structure
  #-----------------------------------------------------------------------------
  def as_secondary_structure(self, conf_request=None):
    #returns CaBLAM secondary structure identification as phenix's preferred
    #  iotbx.secondary_structure objects
    #each individual beta strand is currently represented as a whole sheet
    #Proper sheet reporting will depend on developing or implementing a
    #  strand-to-sheet assembly that does not require H-bonds
    from iotbx.pdb import secondary_structure
    helix_i = 0
    sheet_i = 0
    helix_records = []
    strand_records = []

    for model_id in self.all_results:
      chain_list = list(self.all_results[model_id].keys())
      chain_list.sort()
      for chain_id in chain_list:
        chain = self.all_results[model_id][chain_id]
        if conf_request in chain.conf_names:
          conf = conf_request
        else:
          conf = chain.conf_names[0]
        for record in chain.confs[conf].sec_struc_records:
          if record.segment_type == 'alpha' or record.segment_type == 'threeten':
            helix_i += 1
            if record.segment_type == 'alpha':
              helix_class = 1
            elif record.segment_type == 'threeten':
              helix_class = 5
            return_record = secondary_structure.pdb_helix(
              serial = helix_i,
              helix_id = helix_i,
              start_resname  = record.start.resname,
              start_chain_id = record.start.chain_id,
              #start_chain_id = " A",
              start_resseq   = record.start.resseq,
              start_icode    = record.start.icode,
              end_resname    = record.end.resname,
              end_chain_id   = record.end.chain_id,
              end_resseq     = record.end.resseq,
              end_icode      = record.end.icode,
              helix_class    = helix_class,
              comment = "",
              length = record.segment_length)
            helix_records.append(return_record)
          if record.segment_type == 'beta':
            sheet_i += 1
            strand_record = secondary_structure.pdb_strand(
              sheet_id       = sheet_i,
              strand_id      = 1,
              start_resname  = record.start.resname,
              start_chain_id = record.start.chain_id,
              start_resseq   = record.start.resseq,
              start_icode    = record.start.icode,
              end_resname    = record.end.resname,
              end_chain_id   = record.end.chain_id,
              end_resseq     = record.end.resseq,
              end_icode      = record.end.icode,
              sense          = 1
              )
            return_record=secondary_structure.pdb_sheet(
              sheet_id = sheet_i,
              n_strands = 1,
              strands = [strand_record],
              registrations = [None],
              hbond_list = []
              )
            strand_records.append(return_record)
    return secondary_structure.annotation(helices=helix_records,sheets=strand_records)
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ as_records
  #-----------------------------------------------------------------------------
  def as_records(self, conf_request=None):
    #outputs pdb-style HELIX and SHEET secondary structure records
    #uses the iotbx.secondary_structure object
    #By default, this returns the first conformation (alt) in each chain
    #Other conformations can be accessed with conf_request
    records = self.as_secondary_structure(conf_request=conf_request)
    self.out.write(records.as_pdb_str()+'\n')
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ as_records_and_pdb
  #-----------------------------------------------------------------------------
  def as_records_and_pdb(self, conf_request=None):
    #outputs pdb-style HELIX and SHEET secondary structure records, followed by
    #  the pdb file
    self.as_records(conf_request=conf_request)
    self.out.write(self.pdb_hierarchy.as_pdb_string())
  #-----------------------------------------------------------------------------
  #}}}

  def as_JSON(self, addon_json={}):
    if not addon_json:
      addon_json = {}
    addon_json["validation_type"] = "cablam"
    data = addon_json
    flat_results = []
    hierarchical_results = {}
    for result in self.results:
      if result.scores.cablam:
        flat_results.append(json.loads(result.as_JSON()))
        hier_result = json.loads(result.as_hierarchical_JSON())
        hierarchical_results = self.merge_dict(hierarchical_results, hier_result)

    data['flat_results'] = flat_results
    data['hierarchical_results'] = hierarchical_results
    data['summary_results'] = self.summary_JSON()
    return json.dumps(data, indent=2)

  def summary_JSON(self):
    summary_results = {}
    for model_id in self.summary_stats['num_residues_by_model'].keys():
      if self.summary_stats['num_residues_by_model'][model_id]:
        num_cablam_outliers = self.summary_stats['num_cablam_outliers_by_model'][model_id]
        num_cablam_disfavored = self.summary_stats['num_cablam_disfavored_by_model'][model_id]
        num_ca_geom_outliers = self.summary_stats['num_ca_geom_outliers_by_model'][model_id]
        num_residues = self.summary_stats['num_residues_by_model'][model_id]
        cablam_outliers_percentage = 0
        cablam_disfavored_percentage = 0
        ca_geom_outliers_percentage = 0
        if num_residues != 0:
          cablam_outliers_percentage = num_cablam_outliers/num_residues*100
          cablam_disfavored_percentage = num_cablam_disfavored/num_residues*100
          ca_geom_outliers_percentage = num_ca_geom_outliers/num_residues*100
        summary_results[model_id] = {
          "num_cablam_outliers" : num_cablam_outliers,
          "cablam_outliers_percentage" : cablam_outliers_percentage,
          "num_cablam_disfavored" : num_cablam_disfavored,
          "cablam_disfavored_percentage" : cablam_disfavored_percentage,
          "num_ca_geom_outliers" : num_ca_geom_outliers,
          "ca_geom_outliers_percentage" : ca_geom_outliers_percentage,
          "num_residues": num_residues,
        }
    return summary_results
  #{{{ as_coot_data
  #-----------------------------------------------------------------------------
  def as_coot_data(self):
    data = []
    for result in self.results:
      if result.is_outlier:
        data.append((result.chain_id, result.resid, result.resname, result.scores.cablam, result.xyz))
    return data
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ show_summary
  #-----------------------------------------------------------------------------
  def show_summary(self, out=sys.stdout, prefix="  "):
    #print whole-model cablam summary
    # double percent sign %% is how to get an escape-char'd % in string formatting
    if self.count_residues() == 0 and self.count_ca_residues == 0:
      out.write("SUMMARY: CaBLAM found no evaluable protein residues.\n")
    else:
      out.write(prefix+"SUMMARY: Note: Regardless of number of alternates, each residue is counted as having at most one outlier.\n")
      out.write(prefix+"SUMMARY: CaBLAM found %s full protein residues and %s CA-only residues\n" % (self.count_residues(),self.count_ca_residues()-self.count_residues()))
      out.write(prefix+"SUMMARY: %s residues (%.1f%%) have disfavored conformations. (<=5%% expected).\n" % (self.count_disfavored(),self.percent_disfavored()))
      out.write(prefix+"SUMMARY: %s residues (%.1f%%) have outlier conformations. (<=1%% expected)\n" % (self.count_outliers(),self.percent_outliers()))
      out.write(prefix+"SUMMARY: %s residues (%.2f%%) have severe CA geometry outliers. (<=0.5%% expected)\n" % (self.count_ca_outliers(),self.percent_ca_outliers()))
      out.write(prefix+"SUMMARY: %s residues (%.2f%%) are helix-like, %s residues (%.2f%%) are beta-like\n" % (self.count_helix(),self.percent_helix(),self.count_beta(),self.percent_beta()))
      out.write(prefix+"SUMMARY: %s residues (%.2f%%) are correctable to helix, %s residues (%.2f%%) are correctable to beta\n" % (self.count_correctable_helix(),self.percent_correctable_helix(),self.count_correctable_beta(),self.percent_correctable_beta()))

    #if self.count_residues() == 0:
    #  print >> out, "SUMMARY: CaBLAM found no fully evaluable protein residues."
    #  if self.count_ca_residues() > 0:
    #    print >> out,prefix+"SUMMARY: CaBLAM found %s CA geometry evaluable residues." % (self.count_ca_residues())
    #    print >> out,prefix+"SUMMARY: %.2f" % (self.percent_ca_outliers())+"% of these residues have severe CA geometry outliers. (<=0.5% expected)"
    #    print >> out,prefix+"SUMMARY: %.2f%% helix, %.2f%% beta" % (self.percent_helix(),self.percent_beta())
    #else:
    #  print >> out,prefix+"SUMMARY: Note: Regardless of number of alternates, each residue is counted as having at most one outlier."
    #  print >> out,prefix+"SUMMARY: CaBLAM found %s fully evaluable residues and %s CA geometry evaluable residues." % (self.count_residues(),self.count_ca_residues()-self.count_residues())
    #  print >> out,prefix+"SUMMARY: %.1f" % (self.percent_disfavored())+"% of these residues have disfavored conformations. (<=5% expected)"
    #  print >> out,prefix+"SUMMARY: %.1f" % (self.percent_outliers())+"% of these residues have outlier conformations. (<=1% expected)"
    #  print >> out,prefix+"SUMMARY: %.2f" % (self.percent_ca_outliers())+"% of these residues have severe CA geometry outliers. (<=0.5% expected)"
    #  print >> out,prefix+"SUMMARY: %.2fpct helix, %.2fpct beta" % (self.percent_helix(),self.percent_beta())
  #-----------------------------------------------------------------------------
  #}}}

  #{{{ gui_summary
  #-----------------------------------------------------------------------------
  def gui_summary(self):
    output = []
    return "GUI summary goes here!"
  #-----------------------------------------------------------------------------
  #}}}
#-------------------------------------------------------------------------------
#}}}


 *******************************************************************************


 *******************************************************************************
mmtbx/validation/cbd_utils.py
from __future__ import division
from math import pi, sin, cos, atan2, sqrt

from mmtbx.conformation_dependent_library.cdl_utils import round_to_ten
from mmtbx.validation.mean_devs_PRO_phi_psi import mean_devs as PRO_phi_psi
from mmtbx.validation.mean_devs_others_phi_psi import mean_devs as others_phi_psi
from mmtbx.validation.mean_devs_VAL_THR_ILE_phi_psi import mean_devs as VAL_THR_ILE_phi_psi

class radial_deviation:
  def __init__(self, r, t, input_in_radians=False):
    self.r = r
    self.t = t
    if not input_in_radians: self.t *= pi/180

  def __repr__(self):
    #return u'%0.2f \u2220 %6.1f' % (self.r, self.t/pi*180)
    return "'%4.1f/_%6.1f'" % (self.r*100, self.t/pi*180)

  def __add__(self, other):
    r = sqrt(self.r**2 + other.r**2 + 2*self.r*other.r*cos(other.t-self.t))
    numer = other.r * sin(other.t-self.t)
    denom = self.r + other.r * cos(other.t-self.t)
    t = self.t + atan2(numer, denom)
    new = radial_deviation(0,0)
    new.r = r
    new.t = t
    return new

  def __sub__(self, other):
    new = radial_deviation(0,0)
    new.r = other.r
    new.t = other.t + pi
    return self.__add__(new)

  def __truediv__(self, other):
    if type(other)==type(1):
      self.r/=other
      return self
    else:
      assert 0

  def __cmp__(self, other):
    if self.r<other.r: return -1
    return 1

  def __eq__(self, other):
    return self.r == other.r

  def __ne__(self, other):
    return self.r != other.r

  def __lt__(self, other):
    return self.r < other.r

  def __le__(self, other):
    return self.r <= other.r

  def __gt__ (self, other):
    return self.r > other.r

  def __ge__(self, other):
    return self.r >= other.r

def get_phi_psi_correction(result,
                           residue,
                           phi_psi,
                           display_phi_psi_correction=False,
                           verbose=False):
  rc = None
  key = (round_to_ten(phi_psi[0]), round_to_ten(phi_psi[1]))
  if residue.resname=='PRO':
    correction = PRO_phi_psi.get(key, None)
  elif residue.resname in ['VAL', 'THR', 'ILE']:
    correction = VAL_THR_ILE_phi_psi.get(key, None)
  else:
    correction = others_phi_psi.get(key, None)
  if correction:
    correction = radial_deviation(*tuple(correction), input_in_radians=True)
    current = radial_deviation(result.deviation, result.dihedral)
    rc = current-correction
    start = (current.r>=0.25)
    finish = (rc.r>=0.25)
    if verbose:
      print('current %s is corrected with %s' % (current, correction)),
      print(' to give %s\n' % rc)
    if display_phi_psi_correction and (start or finish):
      show_phi_psi_correction(residue, phi_psi, current, correction, rc)
    return rc.r, rc.t/pi*180, start, finish
  else:
    return None

def show_phi_psi_correction(residue, phi_psi, current, correction, rc, units=100):
  import matplotlib.pyplot as plt

  xs = [current.t, correction.t, rc.t]
  ys = [current.r, correction.r, rc.r]
  labels = ['default', 'correction', 'result']

  for i, (x, y) in enumerate(zip(xs, ys)):
      y*=units
      plt.polar(x, y, 'ro')
      plt.text(x, y,
               '%s' % (labels[i]),
               )
  key = (round_to_ten(phi_psi[0]), round_to_ten(phi_psi[1]))
  plt.title('%s (%5.1f, %5.1f) (%4.0f, %4.0f)\n%s minus %s equals %s' %(
    residue.id_str(),
    phi_psi[0],
    phi_psi[1],
    key[0],
    key[1],
    current,
    correction,
    rc,
   )
  )
  plt.show()

if __name__ == '__main__':
  # tests
  v1 = radial_deviation(1,0)
  v2 = radial_deviation(-1,0)
  print(v1,v2)
  print(v1-v2)


 *******************************************************************************


 *******************************************************************************
mmtbx/validation/cbetadev.py

"""
Validation of protein geometry by analysis of the positions of C-beta sidechain
atoms.  Significant deviations from ideality often indicate misfit rotamers
and/or necessity of mainchain movement, especially alternate conformations.

Reference:

Lovell SC, Davis IW, Arendall WB 3rd, de Bakker PI, Word JM, Prisant MG,
Richardson JS, Richardson DC.  Structure validation by Calpha geometry: phi,psi
and Cbeta deviation.  Proteins. 2003 Feb 15;50(3):437-50.
http://www.ncbi.nlm.nih.gov/pubmed/12557186
"""

from __future__ import absolute_import, division, print_function
from mmtbx.validation import residue, validation
from scitbx.matrix import col, dihedral_angle, rotate_point_around_axis
import sys
from scitbx.array_family import flex
from libtbx import group_args

from mmtbx.conformation_dependent_library import generate_protein_threes
from mmtbx.validation import cbd_utils
import json

relevant_atom_names = {
  " CA ": None, " N  ": None, " C  ": None, " CB ": None} # FUTURE: set

def get_phi_psi_dict(pdb_hierarchy):
  rc = {}
  for i, three in enumerate(generate_protein_threes(hierarchy=pdb_hierarchy,
                                                    geometry=None)):
    phi_psi_angles = three.get_phi_psi_angles()
    is_alt_conf = ' '
    relevant_atoms = {}
    for atom in three[1].atoms():
      if (atom.name in relevant_atom_names):
        if (len(atom.parent().altloc) != 0):
          is_alt_conf = atom.parent().altloc
          break
    id_str = '|%s:%s|' % (three[1].id_str(), is_alt_conf)
    rc[id_str] = phi_psi_angles
  return rc

class cbeta(residue):
  """
  Result class for protein C-beta deviation analysis (phenix.cbetadev).
  """
  __cbeta_attr__ = [
    "deviation",
    "dihedral_NABB",
    "ideal_xyz",
    "model_id"
  ]
  __slots__ = residue.__slots__ + __cbeta_attr__

  @staticmethod
  def header():
    return "%-20s  %5s" % ("Residue", "Dev.")

  def as_string(self):
    return "%-20s  %5.2f" % (self.id_str(), self.deviation)

  # Backwards compatibility
  def format_old(self):
    return "%s:%s:%2s:%4s%1s:%7.3f:%7.2f:%7.2f:%s:" % (self.altloc,
      self.resname.lower(), self.chain_id, self.resseq, self.icode,
      self.deviation, self.dihedral_NABB, self.occupancy, self.altloc)

  def as_kinemage(self):
    key = "cb %3s%2s%4s%1s  %.3f %.2f" % (self.resname.lower(),
      self.chain_id, self.resseq, self.icode, self.deviation,
      self.dihedral_NABB)
    return "{%s} r=%.3f %s %.3f, %.3f, %.3f" % (key,
      self.deviation, '', #The blank char is a placeholder for optional color
      self.ideal_xyz[0], self.ideal_xyz[1], self.ideal_xyz[2])

  def as_bullseye_point(self):
    #print a point in kinemage format for the "bulleye" plot of cbdev distribution
    import numpy as np
    #original point id format: {4hum  trp A 427; dev=0.090}
    key = "%s%3s%2s%4s%1s;  dev=%.3f" % (self.altloc.strip(), self.resname.lower(),
      self.chain_id, self.resseq, self.icode, self.deviation)
    #convert polar position to cartesian
    angle = np.radians(self.dihedral_NABB)
    x = self.deviation * np.cos(angle)
    y = self.deviation * np.sin(angle)
    return "{%s} %.3f %.3f 0" % (key,x,y)

  def as_bullseye_label(self):
    #label a point in the cbdev bulleye, intended for outliers
    import numpy as np
    #expected label format: { 4hum  trp A 427}
    #shorter than full point id, leading space to offset from point
    key = "  %s%3s%2s%4s%1s" % (self.altloc.strip(), self.resname.lower(),
      self.chain_id, self.resseq, self.icode)
    #convert polar position to cartesian
    angle = np.radians(self.dihedral_NABB)
    x = self.deviation * np.cos(angle)
    y = self.deviation * np.sin(angle)
    return "{%s} %.3f %.3f 0" % (key,x,y)

  def as_JSON(self):
    serializable_slots = [s for s in self.__slots__ if hasattr(self, s)]
    slots_as_dict = ({s: getattr(self, s) for s in serializable_slots})
    return json.dumps(slots_as_dict, indent=2)

  def as_hierarchical_JSON(self):
    hierarchical_dict = {}
    hierarchy_nest_list = ['model_id', 'chain_id', 'resid', 'altloc']
    return json.dumps(self.nest_dict(hierarchy_nest_list, hierarchical_dict), indent=2)

  def as_table_row_phenix(self):
    return [ self.chain_id, "%1s%s %s" % (self.altloc,self.resname, self.resid),
             self.deviation, self.dihedral_NABB ]

class cbetadev(validation):
  __slots__ = validation.__slots__ + ["beta_ideal","_outlier_i_seqs","stats",
                                      'percent_outliers', 'new_outliers', 'outliers_removed']
  program_description = "Analyze protein sidechain C-beta deviation"
  output_header = "pdb:alt:res:chainID:resnum:dev:dihedralNABB:Occ:ALT:"
  gui_list_headers = ["Chain", "Residue","Deviation","Angle"]
  gui_formats = ["%s", "%s", "%.3f", "%.2f"]
  wx_column_widths = [75, 125, 100, 100]

  def get_result_class(self) : return cbeta

  def __init__(self, pdb_hierarchy,
      outliers_only=False,
      out=sys.stdout,
      collect_ideal=False,
      apply_phi_psi_correction=False,
      display_phi_psi_correction=False,
      exclude_d_peptides=False,
      quiet=False):
    validation.__init__(self)
    self._outlier_i_seqs = flex.size_t()
    self.beta_ideal = {}
    output_list = []
    self.stats = group_args(n_results=0,
                            n_weighted_results = 0,
                            n_weighted_outliers = 0)
    total_residues = 0
    new_outliers = None
    outliers_removed = None
    if apply_phi_psi_correction:
      phi_psi_angles = get_phi_psi_dict(pdb_hierarchy)
      new_outliers = 0
      outliers_removed = 0
    from mmtbx.validation import utils
    use_segids = utils.use_segids_in_place_of_chainids(
      hierarchy=pdb_hierarchy)
    for model in pdb_hierarchy.models():
      if model.id not in self.n_total_by_model:
        self.n_total_by_model[model.id] = 0
        self.n_outliers_by_model[model.id] = 0
      for chain in model.chains():
        if use_segids:
          chain_id = utils.get_segid_as_chainid(chain=chain)
        else:
          chain_id = chain.id
        for rg in chain.residue_groups():
          for i_cf,cf in enumerate(rg.conformers()):
            for i_residue,residue in enumerate(cf.residues()):
              if (residue.resname == "GLY"):
                continue
              is_first = (i_cf == 0)
              is_alt_conf = False
              relevant_atoms = {}
              for atom in residue.atoms():
                if (atom.name in relevant_atom_names):
                  relevant_atoms[atom.name] = atom
                  if (len(atom.parent().altloc) != 0):
                    is_alt_conf = True
              if ((is_first or is_alt_conf) and len(relevant_atoms) == 4):
                result = calculate_ideal_and_deviation(
                  relevant_atoms=relevant_atoms,
                  resname=residue.resname)
                dev = result.deviation
                dihedralNABB = result.dihedral
                betaxyz = result.ideal
                if (dev is None) : continue
                resCB = relevant_atoms[" CB "]
                self.stats.n_results += 1
                self.n_total_by_model[model.id] += 1
                self.stats.n_weighted_results += resCB.occ
                if (is_alt_conf):
                  altchar = cf.altloc
                else:
                  altchar = " "
                total_residues+=1
                if apply_phi_psi_correction:
                  id_str = '|%s:%s|' % (residue.id_str(), altchar)
                  phi_psi = phi_psi_angles.get(id_str, None)
                  if phi_psi:
                    rc = cbd_utils.get_phi_psi_correction(
                      result,
                      residue,
                      phi_psi,
                      display_phi_psi_correction=display_phi_psi_correction,
                      )
                    if rc:
                      dev, dihedralNABB, start, finish = rc
                      # if start or finish: print(id_str,dev,dihedralNABB,start,finish)
                      if start and not finish:
                        outliers_removed += 1
                      elif not start and finish:
                        new_outliers += 1
                if(exclude_d_peptides and dev>=2.):
                  pass
                elif(dev >=0.25 or outliers_only==False):
                  if(dev >=0.25):
                    self.n_outliers+=1
                    self.n_outliers_by_model[model.id]+=1
                    self.stats.n_weighted_outliers += resCB.occ
                    self._outlier_i_seqs.append(atom.i_seq)
                  res=residue.resname.lower()
                  sub=chain.id
                  if(len(sub)==1):
                    sub=" "+sub
                  result = cbeta(
                    model_id=model.id,
                    chain_id=chain_id,
                    resname=residue.resname,
                    resseq=residue.resseq,
                    icode=residue.icode,
                    altloc=altchar,
                    xyz=resCB.xyz,
                    occupancy=resCB.occ,
                    deviation=dev,
                    dihedral_NABB=dihedralNABB,
                    ideal_xyz=betaxyz,
                    outlier=(dev >= 0.25))
                  self.results.append(result)
                  key = result.id_str()
                  if (collect_ideal):
                    self.beta_ideal[key] = betaxyz
      if apply_phi_psi_correction:
        print('''
  Outliers removed : %5d
  New outliers     : %5d
  Num. of outliers : %5d
  Num. of residues : %5d
  ''' % (outliers_removed,
         new_outliers,
         self.n_outliers,
         total_residues,
        ))
    self.new_outliers=new_outliers
    self.outliers_removed=outliers_removed
    if total_residues:
      self.percent_outliers=self.n_outliers/total_residues*100
    else:
      self.percent_outliers = None

  def show_old_output(self, out, verbose=False, prefix="pdb"):
    if (verbose):
      print(self.output_header, file=out)
    for result in self.results :
      print(prefix + " :" + result.format_old(), file=out)
    if (verbose):
      self.show_summary(out)

  def show_summary(self, out, prefix=""):
    print(prefix + \
      'SUMMARY: %d C-beta deviations >= 0.25 Angstrom (Goal: 0)' % \
      self.n_outliers, file=out)

  #functions for internal access of summary statistics
  def get_outlier_count(self):
    return self.n_outliers

  def get_weighted_outlier_count(self):
    return self.stats.n_weighted_outliers

  def get_result_count(self):
    return self.stats.n_results

  def get_weighted_result_count(self):
    return self.stats.n_weighted_results

  def get_outlier_percent(self):
    if self.stats.n_results == 0:
      return 0
    return self.n_outliers/self.stats.n_results*100

  def get_weighted_outlier_percent(self):
    weighted_result_count = self.get_weighted_result_count()
    if weighted_result_count == 0:
      return 0
    return self.get_weighted_outlier_count()/weighted_result_count*100

  def get_expected_count(self):
    return 0

  def get_beta_ideal(self):
    return self.beta_ideal

  def as_kinemage(self, chain_id=None):
    cbeta_out = "@subgroup {CB dev} dominant\n"
    cbeta_out += "@balllist {CB dev Ball} color= magenta master= {Cbeta dev}\n"
    for result in self.results :
      if result.is_outlier():
        if (chain_id is None) or (chain_id == result.chain_id):
          cbeta_out += result.as_kinemage() + "\n"
    return cbeta_out

  def as_bullseye_kinemage(self, pdbid=""):
    from mmtbx.validation.molprobity import kinemage_templates
    header = []
    header.append(kinemage_templates.cbetadev_bullseye(pdbid=pdbid))
    header.append("@group {Cbeta dev}")
    cbeta_main = ["@dotlist {Cb scatter} color= white"]
    cbeta_main_labels = ["@labellist {outlier labels} color= white"]
    cbeta_alt = ["@dotlist {alt conf Cb scatter} color= pink"]
    cbeta_alt_labels = ["@labellist {alt conf outlier labels} color= pinktint"]

    #cbeta_main.append("@dotlist {Cb scatter} color= white")
    for result in self.results:
      if result.altloc in ['',' ','A']:
        cbeta_main.append(result.as_bullseye_point())
        if result.is_outlier():
          cbeta_main_labels.append(result.as_bullseye_label())
      else:
        cbeta_alt.append(result.as_bullseye_point())
        if result.is_outlier():
          cbeta_alt_labels.append(result.as_bullseye_label())
    #if no residues were added to any of the extra lists, also empty the header for that list
    if len(cbeta_main_labels) == 1: cbeta_main_labels = []
    if len(cbeta_alt) == 1: cbeta_alt = []
    if len(cbeta_alt_labels) == 1: cbeta_alt_labels = []
    return "\n".join(header + cbeta_main + cbeta_main_labels + cbeta_alt + cbeta_alt_labels)

  def as_JSON(self, addon_json={}):
    if not addon_json:
      addon_json = {}
    addon_json["validation_type"] = "cbetadev"
    data = addon_json
    flat_results = []
    hierarchical_results = {}
    summary_results = {}
    for result in self.results:
      flat_results.append(json.loads(result.as_JSON()))
      hier_result = json.loads(result.as_hierarchical_JSON())
      hierarchical_results = self.merge_dict(hierarchical_results, hier_result)

    data['flat_results'] = flat_results
    data['hierarchical_results'] = hierarchical_results
    data['summary_results'] = summary_results
    for model_id in self.n_total_by_model.keys():
      if self.n_total_by_model[model_id]:
        summary_results[model_id] = {
          "num_outliers" : self.n_outliers_by_model[model_id],
          "num_cbeta_residues" : self.n_total_by_model[model_id],
          "outlier_percentage" : self.n_outliers_by_model[model_id]/self.n_total_by_model[model_id]*100,
          "outlier_goal" : 0,
        }
    data['summary_results'] = summary_results
    return json.dumps(data, indent=2)

  def as_coot_data(self):
    data = []
    for result in self.results :
      if result.is_outlier():
        data.append((result.chain_id, result.resid, result.resname,
          result.altloc, result.deviation, result.xyz))
    return data

class calculate_ideal_and_deviation(object):
  __slots__ = ["deviation", "ideal", "dihedral"]
  def __init__(self, relevant_atoms, resname):
    assert (resname != "GLY")
    from cctbx.geometry_restraints import chirality
    self.deviation = None
    self.ideal = None
    self.dihedral = None
    resCA = relevant_atoms[" CA "]
    resN  = relevant_atoms[" N  "]
    resC  = relevant_atoms[" C  "]
    resCB = relevant_atoms[" CB "]
    if None not in [resCA, resN, resCB, resC]:
      chiral_volume = chirality([resCA.xyz, resN.xyz, resCB.xyz, resC.xyz],
                               volume_ideal=0.,
                               both_signs=True,
                               weight=1.,
                               ).volume_model
    else:
      chiral_volume = None
    dist, angleCAB, dihedralNCAB, angleNAB, dihedralCNAB, angleideal= \
      idealized_calpha_angles(resname, chiral_volume)
    betaNCAB = construct_fourth(resN,
                                resCA,
                                resC,
                                dist,
                                angleCAB,
                                dihedralNCAB,
                                method="NCAB")
    betaCNAB = construct_fourth(resN,
                                resCA,
                                resC,
                                dist,
                                angleNAB,
                                dihedralCNAB,
                                method="CNAB")
    if (not None in [betaNCAB, betaCNAB]):
      betaxyz = (col(betaNCAB) + col(betaCNAB)) / 2
      betadist = abs(col(resCA.xyz) - betaxyz)
      if betadist != 0:
        if(betadist != dist):
          distTemp = betaxyz - col(resCA.xyz)
          betaxyz = col(resCA.xyz) + distTemp * dist/betadist
        self.deviation = abs(col(resCB.xyz) - betaxyz)
        self.dihedral = dihedral_angle(
          sites=[resN.xyz,resCA.xyz,betaxyz.elems,resCB.xyz], deg=True)
        self.ideal = betaxyz.elems

def idealized_calpha_angles(resname, chiral_volume=None):
  from iotbx.pdb import common_residue_names_get_class
  if (resname in ["ALA", "DAL"]):
    #target values are for L-aminao acids.
    #D-amino acids will require the signs of the dihedrals to be flipped
    #This will be performed just before the return
    dist = 1.536
    angleCAB = 110.1
    dihedralNCAB = 122.9
    angleNAB = 110.6
    dihedralCNAB = -122.6
    angleideal = 111.2
  elif (resname in ["PRO", "DPR"]):
    dist = 1.530
    angleCAB = 112.2
    dihedralNCAB = 115.1
    angleNAB = 103.0
    dihedralCNAB = -120.7
    angleideal = 111.8
  elif (resname in ["VAL", "THR", "ILE",
                    "DVA", "DTH", "DIL"]):
    dist = 1.540
    angleCAB = 109.1
    dihedralNCAB = 123.4
    angleNAB = 111.5
    dihedralCNAB = -122.0
    angleideal = 111.2
  elif (resname == "GLY"):
    dist = 1.10
    angleCAB = 109.3
    dihedralNCAB = 121.6
    angleNAB = 109.3
    dihedralCNAB = -121.6
    angleideal = 112.5
  else:
    dist = 1.530
    angleCAB = 110.1
    dihedralNCAB = 122.8
    angleNAB = 110.5
    dihedralCNAB = -122.6
    angleideal = 111.2
  #check if dihedral signs should be flipped becasue residue has D chirality
  #rely on residue names first, so that mis-named residues gat marked as outliers
  #if the residue name is nonstandard, use the chiral volume for best guess about target
  res_class = common_residue_names_get_class(resname)
  if res_class == "common_amino_acid" or chiral_volume is None:
    pass
  elif res_class == "d_amino_acid" or chiral_volume > 0:
    dihedralNCAB = dihedralNCAB * -1
    dihedralCNAB = dihedralCNAB * -1
  return dist, angleCAB, dihedralNCAB, angleNAB, dihedralCNAB, angleideal

def construct_fourth(resN,resCA,resC,dist,angle,dihedral,method="NCAB"):
  if (not None in [resN, resCA, resC]):
    if (method == "NCAB"):
      res0 = resN
      res1 = resC
      res2 = resCA
    elif (method == "CNAB"):
      res0 = resC
      res1 = resN
      res2 = resCA
    a = col(res2.xyz) - col(res1.xyz)
    b = col(res0.xyz) - col(res1.xyz)
    c = a.cross(b)
    cmag = abs(c)
    if(cmag > 0.000001):
      c *= dist/cmag
    c += col(res2.xyz)
    d = c
    angledhdrl = dihedral - 90
    a = col(res1.xyz)
    b = col(res2.xyz)
    # XXX is there an equivalent method for 'col'?
    newD = col(rotate_point_around_axis(
      axis_point_1=res1.xyz,
      axis_point_2=res2.xyz,
      point=d.elems,
      angle=angledhdrl,
      deg=True))
    a = newD - col(res2.xyz)
    b = col(res1.xyz) - col(res2.xyz)
    c = a.cross(b)
    cmag = abs(c)
    if(cmag > 0.000001):
      c *= dist/cmag
    angledhdrl = 90 - angle;
    a = col(res2.xyz)
    c += a
    b = c
    if a == b: return newD.elems
    return rotate_point_around_axis(
      axis_point_1=a.elems,
      axis_point_2=b.elems,
      point=newD.elems,
      angle=angledhdrl,
      deg=True)
  return None

def extract_atoms_from_residue_group(residue_group):
  """
  Given a residue_group object, which may or may not have multiple
  conformations, extract the relevant atoms for each conformer, taking into
  account any atoms shared between conformers.  This is implemented
  separately from the main validation routine, which accesses the hierarchy
  object via the chain->conformer->residue API.  Returns a list of hashes,
  each suitable for calling calculate_ideal_and_deviation.
  """
  atom_groups = residue_group.atom_groups()
  if (len(atom_groups) == 1):
    relevant_atoms = {}
    for atom in atom_groups[0].atoms():
      relevant_atoms[atom.name] = atom
    return [ relevant_atoms ]
  else :
    all_relevant_atoms = []
    expected_names = [" CA ", " N  ", " CB ", " C  "]
    main_conf = {}
    for atom_group in atom_groups :
      if (atom_group.altloc.strip() == ''):
        for atom in atom_group.atoms():
          if (atom.name in expected_names):
            main_conf[atom.name] = atom
      else :
        relevant_atoms = {}
        for atom in atom_group.atoms():
          if (atom.name in expected_names):
            relevant_atoms[atom.name] = atom
        if (len(relevant_atoms) == 0) : continue
        for atom_name in main_conf.keys():
          if (not atom_name in relevant_atoms):
            relevant_atoms[atom_name] = main_conf[atom_name]
        if (len(relevant_atoms) != 0):
          all_relevant_atoms.append(relevant_atoms)
    if (len(main_conf) == 4):
      all_relevant_atoms.insert(0, main_conf)
    return all_relevant_atoms


 *******************************************************************************


 *******************************************************************************
mmtbx/validation/chain_comparison.py
from __future__ import absolute_import, division, print_function

# chain_comparison.py
# a tool to compare main-chain from two structures with or without crystal
# symmetry
#


import iotbx.phil
import sys,os
from operator import itemgetter
from libtbx.utils import Sorry,null_out
from libtbx import group_args
from scitbx.array_family import flex
from copy import deepcopy
from six.moves import zip
from six.moves import range


master_params = """

  input_files {
    pdb_in = None
      .type = path
      .multiple = True
      .help = Input PDB file (enter target first and then query)\
              query_dir is set)
      .short_caption = Target/query model
      .style = file_type:pdb input_file

    unique_query_only = False
      .type = bool
      .help = Use only unique chains in query. Normally use \
         unique_query_only=False and unique_part_of_target_only=True.
      .short_caption = Unique query only

    unique_target_pdb_in = None
      .type = path
      .help = Target model identifying which element is selected with \
           unique_query_only. NOTE: must be specified by keyword.
      .short_caption = Target model
      .style = file_type:pdb input_file

    unique_part_of_target_only = None
      .type = bool
      .help = Use only unique chains in target (see also unique_query_only). \
      .short_caption = Unique target only

    test_unique_part_of_target_only = True
      .type = bool
      .help = Try both unique_part_of_target_only as True and False and \
             report result for whichever gives higher value of \
              fraction matching.  Cannot be used with match_pdb_file
      .short_caption = Test unique target only

    allow_extensions = False
      .type = bool
      .help = If True, ignore parts of chains that do not overlap.  Normally \
              use False: identity is identity of overlapping part times \
              fraction of chain that overlaps.
      .short_caption = Allow extensions

    ncs_file = None
      .type = path
      .short_caption = NCS file (optional)
      .help = NCS file. \
               If unique_query_only is False (typically) \
               apply NCS to it to generate full query.  Normally used with \
               test_unique_part_of_target_only=True. \
               NOTE: if your structure has very high symmetry, including \
               an NCS file can result in extremely long run times. It may \
               be better in such cases to supply target and query files \
               that have NCS applied (or matching structures without NCS) \
               and not to supply an NCS file.

    query_dir = None
      .type = path
      .help = directory containing query PDB files (any number)
      .short_caption = Query directory (optional)
      .style = directory
  }
  output_files {

   match_pdb_file = None
     .type = path
     .help = Output file containing segments with specified match percentage
     .short_caption = Match PDB
  }
  crystal_info {
    chain_type = *PROTEIN RNA DNA
      .type = choice
      .short_caption = Chain type
      .help = Chain type.  All residues of other chain types ignored.

    use_crystal_symmetry = None
      .type = bool
      .short_caption = Use crystal symmetry in comparison
      .help = Default is True if space group is not P1.  \
              If set, use crystal symmetry to map atoms to closest positions
  }
  comparison {
    max_dist = 3.
      .type = float
      .short_caption = Maximum close distance
      .help= Maximum distance between atoms to be considered close

    distance_per_site = None
      .type = float
      .short_caption = Maximum distance spanned by a pair of residues
      .help =Maximum distance spanned by a pair of residues.  Set by \
            default as 3.8 A for protein and 8 A for RNA

    min_similarity = 0.99
      .type = float
      .short_caption = Minimum similarity in chains for uniqueness
      .help = When choosing unique chains, use min_similarity cutoff. \
               This applies to both chain length and the sequence itself.

    target_length_from_matching_chains = False
      .type = bool
      .short_caption = Use matching chains to get length
      .help= Use length of chains in target that are matched to \
                define full target length (as opposed to all unique \
                chains in target).

    minimum_percent_match_to_select = 1
      .type = float
      .help = You can specify minimum_percent_match_to_select and \
              maximum_percent_match_to_select and match_pdb_file \
              in which case all segments in the query model that have \
              a percentage match (within max_dist of atom in target) \
              in this range will be written out to match_pdb_file.
    maximum_percent_match_to_select = 100
      .type = float
      .help = You can specify minimum_percent_match_to_select and \
              maximum_percent_match_to_select and match_pdb_file \
              in which case all segments in the query model that have \
              a percentage match (within max_dist of atom in target) \
              in this range will be written out to match_pdb_file.
    remove_alt_conf = True
      .type = bool
      .help = Remove alternate conformers before analysis. This is normally \
               required to align correctly.

    residue_groups = "VGASCTI P LDNEQM KR FHY W"
      .type = str
      .help = Optional groups of residues to score together
      .short_caption = Residue groups
      .expert_level = 3

    score_by_residue_groups = False
      .type = bool
      .help = Use residue groups in sequence alignment
      .short_caption = Score by residue groups
      .expert_level = 3

    only_keep_best_chain = False
      .type = bool
      .help = Use keep the best-matching chain in match_pdb
      .short_caption = Best chain only in match_pdb

  }
  control {
      verbose = False
        .type = bool
        .help = Verbose output
        .short_caption = Verbose output

      quiet = False
        .type = bool
        .help = No printed output
        .short_caption = No printed output
  }

  include scope libtbx.phil.interface.tracking_params
  gui
    .help = "GUI-specific parameter required for output directory"
  {
    output_dir = None
    .type = path
    .style = output_dir
  }
"""
master_phil = iotbx.phil.parse(master_params, process_includes=True)

class rmsd_values:
  def __init__(self,params=None):
    self.id_list=[]
    self.rmsd_list=[]
    self.n_list=[]
    self.match_percent_list=[]
    self.target_length_list=[]
    self.total_chain=None
    self.used_chain=None
    self.total_target=None
    self.total_query=None
    self.used_target=None
    self.used_query=None
    self.n_fragments_list=[]
    self.incorrect_connections = None
    self.input_fragments = None
    self.file_info=""
    self.params=params

  def add_incorrect_connections(self,incorrect_connections):
    self.incorrect_connections = incorrect_connections

  def add_input_fragments(self,input_fragments):
    self.input_fragments = input_fragments

  def add_match_percent(self,id=None,match_percent=None):
    ipoint=self.id_list.index(id)
    self.match_percent_list[ipoint]=match_percent

  def add_target_length(self,id=None,target_length=None):
    ipoint=self.id_list.index(id)
    self.target_length_list[ipoint]=target_length

  def add_fragment_count(self,id=None,n=None):
    ipoint=self.id_list.index(id)
    self.fragment_count[ipoint]=n

  def add_rmsd(self,id=None,rmsd=None,n=None,n_fragments=None):
    self.id_list.append(id)
    self.rmsd_list.append(rmsd)
    self.n_list.append(n)
    self.match_percent_list.append(0)
    self.target_length_list.append(0)
    self.n_fragments_list.append(n_fragments)

  def get_n_fragments(self,id=None):
    for x in ['id_list','n_fragments_list']:
      if not getattr(self,x,None):
        return 0
    for local_id,local_n_fragments in zip(
       self.id_list,self.n_fragments_list):
      if id==local_id:
        return local_n_fragments
    return 0

  def get_match_percent(self,id=None):
    for x in ['id_list','match_percent_list']:
      if not getattr(self,x,None):
        return 0
    for local_id,local_match_percent in zip(
       self.id_list,self.match_percent_list):
      if id==local_id:
        return local_match_percent
    return 0

  def get_target_length(self,id=None):
    for local_id,local_target_length in zip(
       self.id_list,self.target_length_list):
      if id==local_id:
        return local_target_length
    return 0

  def get_close_to_target_percent(self,id=None):
    target_length=self.get_target_length(id=id)
    rmsd,n=self.get_values(id=id)
    if target_length is not None and n is not None:
      value=100.*n/max(1.,target_length if target_length is not None else 0)
      # ZZZ may need to scale by self.used_target/self.total_target
      return value


    else:
      return 0.

  def get_values(self,id=None):
    for x in ['id_list','rmsd_list','n_list']:
      if not getattr(self,x,None):
        return 0,0
    for local_id,local_rmsd,local_n in zip(
       self.id_list,self.rmsd_list,self.n_list):
      if id==local_id:
        return local_rmsd,local_n
    return 0,0

  def show_summary(self,full_rows=None,out=sys.stdout):
    from mmtbx.validation.chain_comparison import write_summary
    write_summary(params=self.params,file_list=[self.file_info],
      rv_list=[self], full_rows=full_rows, out=out)

def get_params(args,out=sys.stdout):
    command_line = iotbx.phil.process_command_line_with_files(
      args=args,
      master_phil=master_phil,
      pdb_file_def="input_files.pdb_in")

    params = command_line.work.extract()
    print("\nFind similarity between two main-chains", file=out)
    master_phil.format(python_object=params).show(out=out)
    return params

def best_match(sites1,sites2,crystal_symmetry=None,
     reject_if_too_far=None,distance_per_site=None):
  assert distance_per_site is not None
  # if reject_if_too_far and the centers of the two are further than can
  #  be reached by the remainders, skip

  unit_cell=crystal_symmetry.unit_cell()
  sps=crystal_symmetry.special_position_settings(min_distance_sym_equiv=0.5)

  # Match coordinates
  from cctbx import sgtbx

  # check central atoms if n>5 for each
  if sites1.size()>5 and sites2.size()>5:
    # what is distance?
    index1=sites1.size()//2
    index2=sites2.size()//2
    x1_ses=sps.sym_equiv_sites(site=sites1[index1])
    info=sgtbx.min_sym_equiv_distance_info(reference_sites=x1_ses,
           other=sites2[index2])
    dd=info.dist()

    # what is distance spannable by ends of each?
    max_dist=(index1+index2)*distance_per_site
    if dd > max_dist:
      info.i=index1
      info.j=index2
      return info  # hopeless

  best_info=None
  best_dist=None
  i=0
  for site in sites1:
    x1_ses=sps.sym_equiv_sites(site=site)
    j=0
    for site2 in sites2:
      info=sgtbx.min_sym_equiv_distance_info(reference_sites=x1_ses,
           other=site2)
      dd=info.dist()
      if best_dist is None or dd<best_dist:
         best_dist=dd
         best_info=info
         best_info.i=i  # just tack them on
         best_info.j=j
      j+=1
    i+=1
  return best_info

def select_atom_lines(hierarchy):
  lines=[]
  for line in hierarchy.as_pdb_string().splitlines(): # PDB OK converted above
    if line.startswith("ATOM "):
      line=line.strip()
      lines.append(line)
  return lines

def get_best_match(xyz1,xyz2,crystal_symmetry=None,
    distance_per_site=None,used_j_list=None,removed_j=False):
  if crystal_symmetry:
    assert distance_per_site is not None
    info=best_match(
      xyz1,xyz2,
      crystal_symmetry=crystal_symmetry,
      distance_per_site=distance_per_site)
  else: # do it without symmetry
    (distance,i,j)=xyz1.min_distance_between_any_pair_with_id(xyz2)
    info=group_args(i=i,j=j,distance=distance)

  if used_j_list and info.j in used_j_list: # used an atom twice

    if removed_j: # we already tried it...give up
      return None
    else:
      # move atom j away and try again
      if not distance_per_site:
        distance_per_site=4. # just need to move it away
      xyz2_new=xyz2.deep_copy()
      new_value=[]
      for x in xyz2_new[info.j]:
        new_value.append(x+distance_per_site*2.)
      xyz2_new[info.j]=tuple(new_value)

      info=get_best_match(xyz1,xyz2_new,crystal_symmetry=crystal_symmetry,
           distance_per_site=distance_per_site,used_j_list=used_j_list+[info.j],
           removed_j=True)

  return info

def get_pdb_inp(text=None,file_name=None,source_info="string"):
  import iotbx.pdb
  if file_name:
    text=open(file_name).read()
    source_info="file %s" %(file_name)
  elif not text:
    text=""
  from cctbx.array_family import flex
  return iotbx.pdb.input(source_info=source_info,
       lines=flex.split_lines(text))

def get_chains_from_lines(lines):
  chains=[]
  for line in lines:
    h=get_pdb_inp(line).construct_hierarchy()
    id=h.models()[0].chains()[0].id
    if not id in chains:
      chains.append(id)
  return chains

def get_seq_from_lines(lines):
  seq=[]
  for line in lines:
    h=get_pdb_inp(line).construct_hierarchy()
    for res in h.models()[0].chains()[0].residues():
      seq.append(res.resname)
  return seq

def get_match_percent(seq1,seq2,params=None):
  assert len(seq1)==len(seq2)
  assert len(seq1)>0
  if params and params.comparison.score_by_residue_groups and \
       params.crystal_info.chain_type=="PROTEIN":
     seq1=convert_to_reduced_set(seq1,params=params)
     seq2=convert_to_reduced_set(seq2,params=params)
  match_n=0
  for a,b in zip(seq1,seq2):
    if a.replace(" ","")==b.replace(" ",""): match_n+=1
  match_percent=100.*match_n/len(seq1)
  return match_n,match_percent

def get_one_letter_seq(sequence):
   # only applies to protein
   from iotbx.pdb import amino_acid_codes
   seq_one_letter=""
   for resn in sequence:
     seq_one_letter+= amino_acid_codes.one_letter_given_three_letter[resn]
   return seq_one_letter

def convert_to_reduced_set(sequence,params=None):
    sequence=get_one_letter_seq(sequence)

    residue_groups = params.comparison.residue_groups.upper().split()
    residue_dict={}
    for g in residue_groups:
        for x in g:
          residue_dict[x]=g[0]
    text=""
    for x in sequence.upper():
      text+=residue_dict.get(x,'V')
    return text


def extract_representative_chains_from_hierarchy(ph,
    min_similarity=0.90,
    allow_extensions=False,
    allow_mismatch_in_number_of_copies=False,out=sys.stdout):

  unique_ph=extract_unique_part_of_hierarchy(ph,
    min_similarity=min_similarity,
    allow_mismatch_in_number_of_copies=allow_mismatch_in_number_of_copies,
    out=out)
  if ph==unique_ph:  # nothing is unique...
    print("No representative unique chains available", file=out)
    return None

  biggest_chain=None
  longest_chain=None
  for model in unique_ph.models()[:1]:
    for chain in model.chains():
      try:
        target_seq=chain.as_padded_sequence()  # has XXX for missing residues
        target_seq.replace("X","")
        chain_length=len(target_seq)
      except Exception as e:
        chain_length=0
      if chain_length and (longest_chain is None or chain_length>longest_chain):
        longest_chain=chain_length
        biggest_chain=chain
  if biggest_chain is None:
    print("Unable to extract unique part of hierarchy", file=out)
    return None

  biggest_chain_hierarchy=iotbx.pdb.input(
    source_info="Model",lines=flex.split_lines("")).construct_hierarchy()
  mm=iotbx.pdb.hierarchy.model()
  biggest_chain_hierarchy.append_model(mm)
  mm.append_chain(biggest_chain.detached_copy())

  # ready with biggest chain

  copies_of_biggest_chain_ph=extract_copies_identical_to_target_from_hierarchy(
     ph,target_ph=biggest_chain_hierarchy,
     allow_extensions=False,out=sys.stdout)
  return copies_of_biggest_chain_ph

def extract_copies_identical_to_target_from_hierarchy(ph,
     allow_extensions=False,
     min_similarity=None,target_ph=None,out=sys.stdout):
  new_hierarchy=iotbx.pdb.input(
    source_info="Model",lines=flex.split_lines("")).construct_hierarchy()
  mm=iotbx.pdb.hierarchy.model()
  new_hierarchy.append_model(mm)

  assert target_ph is not None
  target_seq=None
  for model in target_ph.models()[:1]:
    for chain in model.chains():
      try:
        target_seq=chain.as_padded_sequence()  # has XXX for missing residues
        target_seq.replace("X","")
        break
      except Exception as e:
        pass
  if not target_seq:
     raise Sorry("No sequence found in target sequence for "+
       "extract_copies_identical_to_target_from_hierarchy")

  matching_chain_list=[]
  for model in ph.models()[:1]:
    for chain in model.chains():
      try:
        seq=chain.as_padded_sequence()  # has XXX for missing residues
        seq=seq.replace("X","")
      except Exception as e:
        seq=""
      similar_seq=seq_it_is_similar_to(
         seq=seq,unique_sequences=[target_seq],
         allow_extensions=allow_extensions,
         min_similarity=min_similarity)  # check for similar...
      if similar_seq:
        matching_chain_list.append(chain)

  total_chains=0
  for chain in matching_chain_list:
    mm.append_chain(chain.detached_copy())
    total_chains+=1
  print("Total chains extracted: %s" %(total_chains), file=out)
  return new_hierarchy

def seq_it_is_similar_to(seq=None,unique_sequences=None,min_similarity=1.0,
   allow_extensions=False):
  from phenix.loop_lib.sequence_similarity import sequence_similarity
  for s in unique_sequences:
    sim=sequence_similarity().run(seq,s,use_fasta=True,verbose=False)
    if not allow_extensions and len(seq)!=len(s):
      fract_same=min(len(seq),len(s))/max(len(seq) if seq is not None else 1,
        len(s) if s is not None else 1)
      sim=sim*fract_same
    if sim >= min_similarity:
      return s # return the one it is similar to
  return None

def extract_unique_part_of_sequences(sequence_list=None,
    allow_mismatch_in_number_of_copies=True,
    allow_extensions=False,
    min_similarity=1.0,out=sys.stdout):

  unique_sequences=[]
  best_chain_copies_dict={}

  unique_sequence_dict={}  # unique_sequence_dict[seq] is unique sequence sim
                           # to seq

  for seq in sequence_list:
      if not seq: continue
      similar_seq=seq_it_is_similar_to(
         seq=seq,unique_sequences=unique_sequences,
         allow_extensions=allow_extensions,
         min_similarity=min_similarity)  # check for similar...
      if similar_seq:
        unique_sequence_dict[seq]=similar_seq
        seq=similar_seq
      else:
        unique_sequences.append(seq)
        unique_sequence_dict[seq]=seq
      if not seq in best_chain_copies_dict:
        best_chain_copies_dict[seq]=0
      best_chain_copies_dict[seq]+=1

  copies_list=[]
  for seq in unique_sequences:
    copies_found=best_chain_copies_dict[seq]
    if not copies_found in copies_list: copies_list.append(copies_found)
  copies_list.sort()
  print("Numbers of copies of sequences: %s" %(str(copies_list)), file=out)
  copies_in_unique={}
  if not copies_list:
    print("\nNothing to compare...", file=out)
    copies_base=0
    return copies_in_unique,copies_base,unique_sequence_dict
  copies_base=copies_list[0]
  all_ok=True
  if len(copies_list)==1:
    print("Number of copies of all sequences is: %s" %(copies_base), file=out)
    for seq in unique_sequences:
      copies_in_unique[seq]=1 # unique set has 1 of this one sequence
    return copies_in_unique,copies_base,unique_sequence_dict
  else:
    for cf in copies_list[1:]:
      if cf//copies_base != cf/copies_base:  # not integral
        all_ok=False
        break
    if all_ok:
      print("Copies are all multiples of %s...taking " %(
          copies_base), file=out)
      for seq in unique_sequences:
        copies_found=best_chain_copies_dict[seq]
        copies_in_unique[seq]=copies_found//copies_base
      return copies_in_unique,copies_base,unique_sequence_dict
    else:
      print("Copies are not all multiples of %s...taking all" %(
          copies_base), file=out)
      for seq in unique_sequences:
        copies_in_unique[seq]=best_chain_copies_dict[seq]
      return copies_in_unique,copies_base,unique_sequence_dict

def get_matching_ids(unique_seq=None,sequences=None,
      chains=None,unique_sequence_dict=None):

    matching_ids=[]
    matching_chains=[]
    for s1,c1 in zip(sequences,chains):
      if unique_sequence_dict[s1]==unique_seq:
          matching_ids.append(c1.id)
          matching_chains.append(c1)
    return matching_ids,matching_chains

def get_sorted_matching_chains(
   chains=None,
   target_centroid_list=None):
  if not target_centroid_list:
    return chains, len(chains)*[0]
  sort_list=[]
  for chain in chains:
    if target_centroid_list:
        xx=flex.vec3_double()
        xx.append(chain.atoms().extract_xyz().mean())
        dist=xx.min_distance_between_any_pair(target_centroid_list)
    else:
        dist=0.
    sort_list.append([dist,chain])
  sort_list.sort(key=itemgetter(0))
  sorted_chains=[]
  sorted_distances=[]
  for dist,chain in sort_list:
    sorted_chains.append(chain)
    sorted_distances.append(dist)
  return sorted_chains,sorted_distances

def split_chains_with_unique_four_char_id(ph):
  from mmtbx.secondary_structure.find_ss_from_ca import split_model,model_info,\
    make_four_char_unique_chain_id
  chain_model=model_info(hierarchy=ph)
  distance_cutoff=15. # basically use sequence jumps to ID breaks
  chain_models=split_model(model=chain_model,distance_cutoff=distance_cutoff)
  new_hierarchy=iotbx.pdb.input(
         source_info="Model", lines=flex.split_lines("")).construct_hierarchy()
  new_model=iotbx.pdb.hierarchy.model()
  new_hierarchy.append_model(new_model)
  used_chain_ids=[]
  for mi in chain_models:
    for mm in mi.hierarchy.models()[:1]:
      for cc in mm.chains():
        cc1=cc.detached_copy()
        cc1.id,used_chain_ids=make_four_char_unique_chain_id(cc1.id,
              used_chain_ids=used_chain_ids)
        new_model.append_chain(cc1)
  return model_info(new_hierarchy)

def extract_unique_part_of_hierarchy(ph,target_ph=None,
    allow_mismatch_in_number_of_copies=True,
    allow_extensions=False,
    keep_chain_as_unit=True,
    min_similarity=1.0,out=sys.stdout):

  starting_chain_id_list=ph.chain_ids()
  if (not keep_chain_as_unit):
    ph=split_chains_with_unique_four_char_id(ph).hierarchy

  # Container for unique chains:

  new_hierarchy=iotbx.pdb.input(
    source_info="Model",lines=flex.split_lines("")).construct_hierarchy()
  mm=iotbx.pdb.hierarchy.model()
  new_hierarchy.append_model(mm)

  # Target location:

  if target_ph:
    target_centroid_list=flex.vec3_double()
    for model in target_ph.models()[:1]:
      target_centroid_list.append(model.atoms().extract_xyz().mean())
  else:
    target_centroid_list=None

  # Get unique set of sequences
  # Also save all the chains associated with each one

  sequences=[]
  chains=[]
  for model in ph.models()[:1]:
    for chain in model.chains():
      try:
        seq=chain.as_padded_sequence()  # has XXX for missing residues
        seq=seq.replace("X","")
        sequences.append(seq)
        chains.append(chain)
      except Exception as e:
        pass
  copies_in_unique,base_copies,unique_sequence_dict=\
        extract_unique_part_of_sequences(
    sequence_list=sequences,
    allow_mismatch_in_number_of_copies=allow_mismatch_in_number_of_copies,
    allow_extensions=allow_extensions,
    min_similarity=min_similarity,out=out)

  if not base_copies:
    print("Nothing to compare...quitting", file=out)
    return new_hierarchy
  sequences_matching_unique_dict={}
  for seq in sequences:
    unique_seq=unique_sequence_dict[seq]
    if not unique_seq in sequences_matching_unique_dict:
      sequences_matching_unique_dict[unique_seq]=[]
    sequences_matching_unique_dict[unique_seq].append(seq)

  # Now we are going to return the unique set...if choice of which copies,
  #  take those closest to the target (in that order)

  unique_chains=[]
  print("Unique set of sequences. Copies of the unique set: %s" %(
      base_copies), file=out)
  print("Copies in unique set ID  sequence", file=out)
  chains_unique=[]
  keys = sorted(list(copies_in_unique.keys()))
  for unique_seq in keys:
    matching_ids,matching_chains=get_matching_ids(
      unique_seq=unique_seq,sequences=sequences,chains=chains,
      unique_sequence_dict=unique_sequence_dict)
    print(" %s    %s    %s " %(
      copies_in_unique[unique_seq]," ".join(matching_ids),unique_seq), file=out)
    sorted_matching_chains,sorted_matching_distances=get_sorted_matching_chains(
      chains=matching_chains,
      target_centroid_list=target_centroid_list)
    for chain,dist in zip(
        sorted_matching_chains[:copies_in_unique[unique_seq]],
        sorted_matching_distances[:copies_in_unique[unique_seq]]):
      print("Adding chain %s: %s (%s): %7.2f" %(
         chain.id,unique_seq,str(chain.atoms().extract_xyz()[0]),
         dist), file=out)
      cc=chain.detached_copy()
      if (not keep_chain_as_unit):
        # Remove X and 3rd/4th characters from chain ID
        cc.id=get_two_char_id_from_four(cc.id,
           starting_chain_id_list=starting_chain_id_list)
      chains_unique.append(cc)

  # Sort the chains on chain id and then on starting residue number
  chains_unique=sort_chains(chains_unique)

  # Put them in the new_hierarchy now
  for chain in chains_unique:
    mm.append_chain(chain.detached_copy())
  return new_hierarchy

def get_chains_with_id(chains,id=None):
  new_chains=[]
  for chain in chains:
    if chain.id==id:
      new_chains.append(chain)
  return new_chains


def get_first_resno_of_chain(chain):
  for rg in chain.residue_groups():
     return rg.resseq_as_int()

def get_ids_from_chain_list(chains_unique):
  id_list=[]
  for chain in chains_unique:
    if not chain.id in id_list:
      id_list.append(chain.id)
  return id_list

def sort_chains(chains_unique):
  unique_id_list=get_ids_from_chain_list(chains_unique)
  new_chains=[]
  for chain_id in unique_id_list:
    chains=get_chains_with_id(chains_unique,id=chain_id)
    chains = sorted(chains, key = lambda c: get_first_resno_of_chain(c))
    new_chains+=chains
  return new_chains

def get_two_char_id_from_four(id,starting_chain_id_list=None):
  two_char_id=id[:2]
  if (not two_char_id in starting_chain_id_list): # not ok
    one_char_id=two_char_id.replace("X","")
    if one_char_id in starting_chain_id_list:
      new_id=one_char_id
    else:
      raise Sorry(
       "Unable to find the chain ID %s in starting list of %s"%(
         one_char_id,str(starting_chain_id_list)))
  else: # ok
    new_id=two_char_id
  return new_id

def run_test_unique_part_of_target_only(params=None,
       out=sys.stdout,
       ncs_obj=None,
       target_hierarchy=None,
       chain_hierarchy=None,
       target_file=None, # model
       chain_file=None, # query
       crystal_symmetry=None,
       max_dist=None,
       quiet=None,
       verbose=None,
       use_crystal_symmetry=None,
       chain_type=None,
       target_length_from_matching_chains=None,
       distance_per_site=None,
       min_similarity=None):
  if params.control.verbose:
    local_out=out
  else:
    local_out=null_out()
  rv_list=[]
  file_list=[]
  best_rv=None
  best_t=None
  best_percent_close=None
  if params.input_files.unique_part_of_target_only==True:
    to_test=[True]
    print("\nTesting unique_part_of_target_only as True", file=out)
  elif params.input_files.unique_part_of_target_only==False:
    to_test=[False]
    print("\nTesting unique_part_of_target_only as False ", file=out)
  else:
    to_test=[True,False]
    print("\nTesting unique_part_of_target_only as True and False and ", file=out)
    print("reporting results for whichever gives higher fraction matched.", file=out)
  for t in to_test:
      local_params=deepcopy(params)
      local_params.input_files.test_unique_part_of_target_only=False
      local_params.input_files.unique_part_of_target_only=t

      rv=run(params=local_params,out=local_out,
          ncs_obj=ncs_obj,
          target_hierarchy=target_hierarchy,
          chain_hierarchy=chain_hierarchy,
          crystal_symmetry=crystal_symmetry,
          max_dist=max_dist,
          quiet=quiet,
          verbose=verbose,
          use_crystal_symmetry=use_crystal_symmetry,
          chain_type=chain_type,
          target_length_from_matching_chains=target_length_from_matching_chains,
          distance_per_site=distance_per_site,
          min_similarity=min_similarity,
          )
      percent_close=rv.get_close_to_target_percent('close')
      print("Percent close with unique_part_of_target_only=%s: %7.1f" %(
          t,percent_close), file=out)
      if best_percent_close is None or percent_close>best_percent_close:
          best_percent_close=percent_close
          best_rv=rv
          best_t=t
  print("\nOriginal residues in target: %s  In query: %s" %(
    best_rv.total_target,best_rv.total_chain), file=out)
  print("Used residues in target:  %s  In query: %s" %(
    best_rv.used_target,best_rv.used_chain), file=out)
  rv_list=[best_rv]
  if best_t:
    file_list=['Unique_target']
  else:
    file_list=['Entire_target']
  write_summary(params=params,file_list=file_list,rv_list=rv_list, out=out)
  best_rv.file_info=file_list[0]
  return best_rv

def run_all(params=None,
       out=sys.stdout,
       ncs_obj=None,
       target_hierarchy=None,
       chain_hierarchy=None,
       target_file=None, # model
       chain_file=None, # query
       crystal_symmetry=None,
       max_dist=None,
       quiet=None,
       verbose=None,
       use_crystal_symmetry=None,
       chain_type=None,
       target_length_from_matching_chains=None,
       distance_per_site=None,
       min_similarity=None):

  if params.control.verbose:
    local_out=out
  else:
    local_out=null_out()
  rv_list=[]
  file_list=[]
  for query_file in os.listdir(params.input_files.query_dir):
    file_name=os.path.join(params.input_files.query_dir,query_file)
    if not os.path.isfile(file_name): continue
    local_params=deepcopy(params)
    local_params.input_files.query_dir=None
    local_params.input_files.pdb_in.append(file_name)
    try:
      rv=run(params=local_params,out=local_out,
        ncs_obj=ncs_obj,
        target_hierarchy=target_hierarchy,
        chain_hierarchy=chain_hierarchy,
        crystal_symmetry=crystal_symmetry,
        max_dist=max_dist,
        quiet=quiet,
        verbose=verbose,
        use_crystal_symmetry=use_crystal_symmetry,
        chain_type=chain_type,
        target_length_from_matching_chains=target_length_from_matching_chains,
        distance_per_site=distance_per_site,
        min_similarity=min_similarity,
      )

    except Exception as e:
      if str(e).find("CifParserError"):
        print("NOTE: skipping %s as it is not a valid model file" %(
           file_name), file=out)
        continue # it was not a valid PDB file...just skip it
      else:
        raise Sorry(str(e))
    rv_list.append(rv)
    rv.file_info=file_name
    file_list.append(file_name)

  write_summary(params=params,file_list=file_list,rv_list=rv_list, out=out)
  return rv_list

def write_summary(params=None,file_list=None,rv_list=None,
    max_dist=None,write_header=True,full_rows=True,out=sys.stdout):

  if params and max_dist is None and hasattr(params.comparison,'max_dist'):
     max_dist=params.comparison.max_dist
  if max_dist is None: max_dist=3.

  if write_header:
    print("CLOSE is within %4.1f A." %( max_dist), file=out)
    print("CA SCORE is (fraction close)/(rmsd of close residues)", file=out)
    print("SEQ SCORE is fraction (close and matching target sequence).", file=out)
    print("MEAN LENGTH is the mean length of contiguous "+\
        "segments in the match with "+\
       "target sequence. (Each gap/reverse of direction starts new segment).\n", file=out)
    if full_rows:
      print("\n", file=out)
      print("               ----ALL RESIDUES---  CLOSE RESIDUES ONLY    %", file=out)
      print("     MODEL     --CLOSE-    --FAR-- FORWARD REVERSE MIXED"+\
              " FOUND  CA                  SEQ", file=out)
      print("               RMSD   N      N       N       N      N "+\
     "        SCORE  SEQ MATCH(%)  SCORE  MEAN LENGTH  FRAGMENTS BAD CONNECTIONS"+"\n",
        file=out)

  results_dict={}
  score_list=[]
  for rv,full_f in zip(rv_list,file_list):
    results_dict[full_f]=rv
    (rmsd,n)=rv.get_values('close')
    target_length=rv.get_target_length('close')
    score=n/(max(1,target_length if target_length is not None else 0)*max(0.1,rmsd
       if rmsd is not None else 0))
    score_list.append([score,full_f])
  score_list.sort(key=itemgetter(0))
  score_list.reverse()
  for score,full_f in score_list:
    rv=results_dict[full_f]
    percent_close=rv.get_close_to_target_percent('close')

    seq_score=rv.get_match_percent('close')*percent_close/10000
    file_name=os.path.split(full_f)[-1]
    close_rmsd,close_n=rv.get_values('close')
    if not close_rmsd: close_rmsd=0
    far_away_rmsd,far_away_n=rv.get_values('far_away')
    forward_rmsd,forward_n=rv.get_values('forward')
    reverse_rmsd,reverse_n=rv.get_values('reverse')
    unaligned_rmsd,unaligned_n=rv.get_values('unaligned')
    match_percent=rv.get_match_percent('close')
    fragments=rv.get_n_fragments('forward')+rv.get_n_fragments('reverse')
    incorrect_connections = getattr(rv,'incorrect_connections',None)
    input_fragments = getattr(rv,'input_fragments',None)
    mean_length=close_n/max(1,fragments if fragments is not None else 0)
    if full_rows:
      print("%14s %4.2f %4d   %4d   %4d    %4d    %4d  %5.1f %6.2f   %5.1f      %6.2f  %5.1f %10s %15s" %(file_name,close_rmsd,close_n,far_away_n,forward_n,
         reverse_n,unaligned_n,percent_close,score,match_percent,seq_score,
         mean_length, input_fragments, incorrect_connections), file=out)
    else:
      print("ID: %14s \nClose rmsd: %4.2f A  (N=%4d)  (Far N=%4d) \n" %(
            file_name,close_rmsd,close_n,far_away_n)+\
         "Close residues in forward direction:"+\
            " %d  Reverse: %d  Unaligned: %d" %(
             forward_n, reverse_n,unaligned_n,) +\
         "\nPercent close: %.1f %%   Score: %.2f \n" %(
             percent_close,score)+\
         "Percent matching sequence: %.1f \n" %(
             match_percent)+\
         "Sequence score:  %.2f  Mean match length: %.1f" %(
               seq_score, mean_length) +\
         "Fragments: %s  Incorrect connections:  %s" %(
           input_fragments, incorrect_connections),
           file=out)

def get_target_length(target_chain_ids=None,hierarchy=None,
     target_length_from_matching_chains=None):
  total_length=0  # just counts residues
  for model in hierarchy.models()[:1]:
    for chain in model.chains():
      if (not target_length_from_matching_chains) or chain.id in target_chain_ids:
        for conformer in chain.conformers()[:1]:
          total_length+=len(conformer.residues())
  return total_length

def select_segments_that_match(params=None,
   chain_hierarchy=None,
   target_hierarchy=None,
   out=sys.stdout,
   ncs_obj=None,
   target_file=None, # model
   chain_file=None, # query
   crystal_symmetry=None,
   max_dist=None,
   quiet=None,
   verbose=None,
   use_crystal_symmetry=None,
   chain_type=None,
   target_length_from_matching_chains=None,
   distance_per_site=None,
   min_similarity=None):


  # Identify all the segments in chain_hierarchy that match target_hierarchy
  #  and write them out
  from mmtbx.secondary_structure.find_ss_from_ca import split_model,model_info,\
    merge_hierarchies_from_models
  chain_model=model_info(hierarchy=chain_hierarchy)
  if params.crystal_info.chain_type=="PROTEIN":
    distance_cutoff=5.
  else:
    distance_cutoff=15.
  chain_models=split_model(model=chain_model,distance_cutoff=distance_cutoff)
  print("Analyzing %s segments and identifying " %(len(chain_models)) +\
      " those with "+\
     "chain_type=%s and match percentage between %.1f %% and %.1f %% " %(
    params.crystal_info.chain_type,
    params.comparison.minimum_percent_match_to_select,
    params.comparison.maximum_percent_match_to_select), file=out)
  local_params=deepcopy(params)
  local_params.output_files.match_pdb_file=None # required
  models_to_keep=[]
  write_header=True
  for cm in chain_models:  # one segment
    rv_list=[]
    file_list=[]
    rv=run(
      params=local_params,
      ncs_obj=ncs_obj,
      target_hierarchy=target_hierarchy,
      quiet=True,
      chain_hierarchy=cm.hierarchy,out=null_out(),
        crystal_symmetry=crystal_symmetry,
        max_dist=max_dist,
        verbose=verbose,
        use_crystal_symmetry=use_crystal_symmetry,
        chain_type=chain_type,
        target_length_from_matching_chains=target_length_from_matching_chains,
        distance_per_site=distance_per_site,
        min_similarity=min_similarity,
      )
    rv_list.append(rv)
    file_list.append(params.crystal_info.chain_type)
    close_rmsd,close_n=rv.get_values('close')
    far_away_rmsd,far_away_n=rv.get_values('far_away')
    if close_n+far_away_n<1: continue # wrong chain type or other failure

    percent_matched=100.*close_n/max(1,close_n+far_away_n if
        close_n is not None and far_away_n is not None else 0)
    if percent_matched < params.comparison.minimum_percent_match_to_select:
      continue
    if percent_matched > params.comparison.maximum_percent_match_to_select:
      continue

    write_summary(params=params,file_list=file_list,rv_list=rv_list,
      write_header=write_header,out=out)
    write_header=False
    models_to_keep.append(group_args(
      group_args_type = 'model to keep',
      model = cm,
      percent_matched = percent_matched,
      close_n = close_n))

  if not models_to_keep:
    print("No matching chains...", file = out)
    return None

  if params.comparison.only_keep_best_chain:
    print("Keeping only best chain", file = out)
    models_to_keep = sorted(models_to_keep, key = lambda m: m.close_n,
      reverse=True)
    models_to_keep = models_to_keep[:1]
  model_list = []
  for ga in models_to_keep:
    model_list.append(ga.model)
  new_model=merge_hierarchies_from_models(models=model_list,resid_offset=5)
  ff=open(params.output_files.match_pdb_file,'w')
  print(new_model.hierarchy.as_pdb_string(), file=ff) # PDB OK (converted above)
  ff.close()
  print("Wrote %s %s chains with %s residues to %s" %(
    len(models_to_keep),params.crystal_info.chain_type,
    new_model.hierarchy.overall_counts().n_residues,
    params.output_files.match_pdb_file), file=out)
  return new_model

def get_ncs_obj(file_name,out=sys.stdout):
  from mmtbx.ncs.ncs import ncs
  ncs_object=ncs()
  ncs_object.read_ncs(file_name=file_name,log=out)
  return ncs_object

def apply_ncs_to_hierarchy(ncs_obj=None,
        hierarchy=None,out=sys.stdout):
  if not ncs_obj or ncs_obj.max_operators()<2 or hierarchy.overall_counts().n_residues<1:
    return hierarchy
  try:
    from phenix.command_line.apply_ncs import apply_ncs as apply_ncs_to_atoms
  except Exception as e:
    print("Need phenix for applying NCS")
    return hierarchy

  print("Applying NCS now...", file=out)
  from phenix.autosol.get_pdb_inp import get_pdb_hierarchy
  identity_copy=ncs_obj.identity_op_id_in_first_group()+1

  args=['pdb_out=None','match_copy=%s' %(identity_copy),
       'params_out=None' ]
  args.append("use_space_group_symmetry=False")
  an=apply_ncs_to_atoms(
      args,hierarchy=hierarchy,
      ncs_object=ncs_obj,
      out=out)
  new_hierarchy=get_pdb_hierarchy(text=an.output_text)
  return new_hierarchy

def get_fragment_count(forward_match_list):
  n=0
  i_last=None
  for i,j in forward_match_list:
    if i_last is None or i != i_last+1:
      n+=1
    i_last=i
  return n


def get_working_fragment():
  return group_args(
    group_args_type = 'working fragment',
    start_i = None,
    start_j = None,
    end_i = None,
    end_j = None,
    forward = None,
    )

def get_incorrect_connections(close_match_list):
  fragments = []
  wf = get_working_fragment()
  fragments.append(wf)
  for i,j in close_match_list:
    if wf.start_j is None:
      wf.start_i = i
      wf.start_j = j
      wf.end_i = i
      wf.end_j = j
    elif wf.forward in [True, None] and wf.end_j + 1 == j: # forward
      wf.end_i = i
      wf.end_j = j
      wf.forward = True
    elif wf.forward in [False, None] and wf.end_j - 1 == j: # reverse
      wf.end_i = i
      wf.end_j = j
      wf.forward = False
    else: # new one
      wf = get_working_fragment()
      fragments.append(wf)
      wf.start_i = i
      wf.start_j = j
      wf.end_i = i
      wf.end_j = j
  # Remove all fragments of length 1
  new_fragments = []
  for wf in fragments:
    if wf.forward is not None:
      new_fragments.append(wf)
  fragments = new_fragments
  # Incorrect connections are:
  #    True->False or False-> True
  #    True->True and end_j does not increase
  #    False->False and end_j does not decrease
  incorrect_connections = 0
  for wf,wf1 in zip(fragments[:-1],fragments[1:]):
    ok = True
    if wf.forward != wf1.forward:
      ok = False
    if wf.forward and wf1.end_j <= wf.end_j:
      ok = False
    if (not wf.forward) and wf1.end_j > wf.end_j:
      ok = False
    if not ok:
      incorrect_connections += 1
  return incorrect_connections

def get_input_fragments(chain_xyz_cart, distance_per_site = 3.8):
  gaps = ( (chain_xyz_cart[:-1] - chain_xyz_cart[1:]).norms() > 2 * distance_per_site).count(True)
  return gaps + 1

def run(args=None,
   ncs_obj=None,
   target_unique_hierarchy=None,
   target_hierarchy=None,
   chain_hierarchy=None,
   target_file=None, # model
   chain_file=None, # query
   crystal_symmetry=None,
   max_dist=None,
   quiet=None,
   verbose=None,
   use_crystal_symmetry=None,
   chain_type=None,
   params=None,
   target_length_from_matching_chains=None,
   distance_per_site=None,
   min_similarity=None,
   out=sys.stdout):
  if not args: args=[]
  if not params:
    params=get_params(args,out=out)
  if params.input_files.pdb_in:
    print("Using %s as target" %(params.input_files.pdb_in[0]), file=out)
  elif chain_file or chain_hierarchy:
    pass # it is fine
  else:
    raise Sorry("Need target model (pdb_in)")
  if params.input_files.test_unique_part_of_target_only and  \
    params.output_files.match_pdb_file:
    print("Note: Cannot use test_unique_part_of_target_only "+\
      "with match_pdb_file...\nturning "+
      "off test_unique_part_of_target_only", file=out)
    params.input_files.test_unique_part_of_target_only=False

  if params.input_files.unique_query_only and \
     params.input_files.unique_part_of_target_only:
    print("Warning: You have specified unique_query_only and" +\
       " unique_part_of_target_only. "+
      "\nThis is not normally appropriate ", file=out)
  if params.input_files.unique_target_pdb_in and \
         params.input_files.unique_query_only:
    print("Using %s as target for unique chains" %(
       params.input_files.unique_target_pdb_in), file=out)
  if params.input_files.query_dir and \
      os.path.isdir(params.input_files.query_dir) and \
      not params.output_files.match_pdb_file:
    print("\nUsing all files in %s as queries\n" %(
       params.input_files.query_dir), file=out)
    return run_all(params=params,out=out)


  if not ncs_obj and params.input_files.ncs_file:
    ncs_obj=get_ncs_obj(params.input_files.ncs_file,out=out)
    print("NCS with %s operators read from %s" %(ncs_obj.max_operators(),
       params.input_files.ncs_file), file=out)
    if ncs_obj.max_operators()<2:
      print("Skipping NCS (no operators)", file=out)
      ncs_obj=ncs_obj.set_unit_ncs()

  if verbose is None:
    verbose=params.control.verbose
  if quiet is None:
    quiet=params.control.quiet
  if chain_type is None:
    chain_type=params.crystal_info.chain_type
  if use_crystal_symmetry is None:
    use_crystal_symmetry=params.crystal_info.use_crystal_symmetry
  params.crystal_info.use_crystal_symmetry=use_crystal_symmetry
  if max_dist is None:
    max_dist=params.comparison.max_dist
  if distance_per_site is None:
    distance_per_site=params.comparison.distance_per_site
  if min_similarity is None:
    min_similarity=params.comparison.min_similarity
  if target_length_from_matching_chains is None:
    target_length_from_matching_chains=\
       params.comparison.target_length_from_matching_chains

  if verbose:
    local_out=out
  else:
    local_out=null_out()

  if not target_file and len(params.input_files.pdb_in)>0:
     target_file=params.input_files.pdb_in[0]  # model
  if not chain_file and len(params.input_files.pdb_in)>1:
     chain_file=params.input_files.pdb_in[1] # query

  # get the hierarchies
  if not chain_hierarchy or not target_hierarchy:
    if not chain_file or not target_file:
      raise Sorry("Need at least 2 models (target and query)" )
    assert chain_file and target_file
    pdb_inp=get_pdb_inp(file_name=chain_file)
    if params.input_files.unique_target_pdb_in:
      target_unique_hierarchy=get_pdb_inp(
        file_name=params.input_files.unique_target_pdb_in).construct_hierarchy()
    if not crystal_symmetry:
      crystal_symmetry=pdb_inp.crystal_symmetry_from_cryst1()
    chain_hierarchy=pdb_inp.construct_hierarchy()
    target_pdb_inp=get_pdb_inp(file_name=target_file)
    if not crystal_symmetry or not crystal_symmetry.unit_cell():
      crystal_symmetry=target_pdb_inp.crystal_symmetry_from_cryst1()
    target_hierarchy=target_pdb_inp.construct_hierarchy()
    # remove hetero atoms as they are not relevant
    chain_hierarchy=chain_hierarchy.apply_atom_selection('not hetero')
    target_hierarchy=target_hierarchy.apply_atom_selection('not hetero')

  # get the CA residues
  if chain_type in ["RNA","DNA"]:
    atom_selection="name P"
  else:
    atom_selection="name ca and (not element Ca)"

  chain_hierarchy=chain_hierarchy.apply_atom_selection(atom_selection)
  target_hierarchy=target_hierarchy.apply_atom_selection(atom_selection)

  # remove alt conformations if necessary
  if params.comparison.remove_alt_conf:
    chain_hierarchy.remove_alt_confs(always_keep_one_conformer=True)
    target_hierarchy.remove_alt_confs(always_keep_one_conformer=True)

  # Convert to forward_compatible_pdb if necessary
  conversion_info_dict = {}
  for ph in (chain_hierarchy, target_hierarchy):
    if not ph.fits_in_pdb_format():
      from iotbx.pdb.forward_compatible_pdb_cif_conversion \
         import forward_compatible_pdb_cif_conversion
      conversion_info = forward_compatible_pdb_cif_conversion(hierarchy = ph)
      conversion_info.\
        convert_hierarchy_to_forward_compatible_pdb_representation(ph)
      conversion_info_dict[ph] = conversion_info

  total_target=target_hierarchy.overall_counts().n_residues
  total_chain=chain_hierarchy.overall_counts().n_residues

  if params.input_files.test_unique_part_of_target_only or \
      (params.input_files.test_unique_part_of_target_only is None and
        params.input_files.ncs_file):
    return run_test_unique_part_of_target_only(params=params,out=out,
          ncs_obj=ncs_obj,
          target_hierarchy=target_hierarchy,
          chain_hierarchy=chain_hierarchy,
          crystal_symmetry=crystal_symmetry,
          max_dist=max_dist,
          quiet=quiet,
          verbose=verbose,
          use_crystal_symmetry=use_crystal_symmetry,
          chain_type=chain_type,
          target_length_from_matching_chains=target_length_from_matching_chains,
          distance_per_site=distance_per_site,
          min_similarity=min_similarity)


  # Take unique part of query if requested
  if target_unique_hierarchy:
    target_ph=target_unique_hierarchy
  else:
    target_ph=chain_hierarchy

  if params.input_files.unique_query_only or ncs_obj:
    print("\nExtracting unique part of query\n", file=out)
    chain_hierarchy=extract_unique_part_of_hierarchy(
      chain_hierarchy,target_ph=target_ph,
      allow_extensions=params.input_files.allow_extensions,
      min_similarity=min_similarity,out=local_out)
    print("Residues in unique part of query hierarchy: %s" %(
     chain_hierarchy.overall_counts().n_residues), file=out)
    if ncs_obj and not params.input_files.unique_query_only:
      # apply NCS to unique part of query
      print("Applying NCS to unique part of query", file=out)
      chain_hierarchy=apply_ncs_to_hierarchy(ncs_obj=ncs_obj,
        hierarchy=chain_hierarchy,out=out)
      print("Residues in full query hierarchy: %s" %(
        chain_hierarchy.overall_counts().n_residues), file=out)

  if params.input_files.unique_part_of_target_only:
    print("\nUsing only unique part of target \n", file=out)
    print("Residues in input target hierarchy: %s" %(
     target_hierarchy.overall_counts().n_residues), file=out)
    target_hierarchy=extract_unique_part_of_hierarchy(
      target_hierarchy,target_ph=target_ph,
      allow_extensions=params.input_files.allow_extensions,
      min_similarity=min_similarity,out=local_out)
    print("Residues in unique part of target hierarchy: %s" %(
     target_hierarchy.overall_counts().n_residues), file=out)


  if params.output_files.match_pdb_file and \
    params.comparison.minimum_percent_match_to_select is not None and \
    params.comparison.maximum_percent_match_to_select is not None:
      return select_segments_that_match(params=params,
       chain_hierarchy=chain_hierarchy,
       target_hierarchy=target_hierarchy,out=out)

  used_target=target_hierarchy.overall_counts().n_residues
  used_chain=chain_hierarchy.overall_counts().n_residues

  if params.crystal_info.use_crystal_symmetry is None: # set default
    if crystal_symmetry and crystal_symmetry.space_group() and \
       (not crystal_symmetry.space_group().type().number() in [0,1]):
      params.crystal_info.use_crystal_symmetry=True
    else:
      params.crystal_info.use_crystal_symmetry=False
      crystal_symmetry=None
  elif params.crystal_info.use_crystal_symmetry==False:
      crystal_symmetry=None

  if not crystal_symmetry or not crystal_symmetry.unit_cell():
    crystal_symmetry=get_pdb_inp(
        text="CRYST1 1000.000 1000.000 1000.000  90.00  90.00  90.00 P 1"
        ).crystal_symmetry_from_cryst1()
    print("\nCrystal symmetry will not be used in comparison.\n", file=out)
    if use_crystal_symmetry:
        raise Sorry("Please set use_crystal_symmetry"+
           "=False (no crystal symmetry supplied)")
  else:
    print("\nCrystal symmetry will be used in comparison.\n", file=out)
    print("Space group: %s" %(crystal_symmetry.space_group().info()), \
         "Unit cell: %7.2f %7.2f %7.2f  %7.2f %7.2f %7.2f \n" %(
        crystal_symmetry.unit_cell().parameters()), file=out)
    use_crystal_symmetry=True
  if not quiet:
    print("Looking for chain similarity for "+\
      "%s (%d residues) in the model %s (%d residues)" %(
     chain_file,chain_hierarchy.overall_counts().n_residues,
     target_file,target_hierarchy.overall_counts().n_residues), file=out)
    if verbose:
      print("Chain type is: %s" %(chain_type), file=out)
  if crystal_symmetry is None or crystal_symmetry.unit_cell() is None:
    raise Sorry("Need crystal symmetry in at least one input file")
  # get the CA residues
  if chain_type in ["RNA","DNA"]:
    if not distance_per_site:
      distance_per_site=8.
  else:
    if not distance_per_site:
      distance_per_site=3.8
  chain_ca=chain_hierarchy
  chain_ca_lines=select_atom_lines(chain_ca)
  target_ca=target_hierarchy
  target_xyz_lines=select_atom_lines(target_ca)
  chain_xyz_cart=chain_ca.atoms().extract_xyz()
  target_xyz_cart=target_ca.atoms().extract_xyz()

  if target_xyz_cart.size()<1 or chain_xyz_cart.size()<1:
    print("No suitable atoms in target", file = out)
    value = rmsd_values()
    value.n_forward = 0
    value.n_reverse= 0
    return value

  # for each xyz in chain, figure out closest atom in target and dist
  best_i=None
  best_i_dd=None
  best_pair=None
  pair_list=[]
  from scitbx.array_family import flex
  input_fragments = get_input_fragments(chain_xyz_cart, distance_per_site = distance_per_site)
  chain_xyz_fract=crystal_symmetry.unit_cell().fractionalize(chain_xyz_cart)
  target_xyz_fract=crystal_symmetry.unit_cell().fractionalize(target_xyz_cart)
  far_away_match_list=[]
  if use_crystal_symmetry:
    working_crystal_symmetry=crystal_symmetry
  else:
    working_crystal_symmetry=None
  used_j_list=[]
  for i in range(chain_xyz_fract.size()):
    best_j=None
    best_dd=None
    distance=None
    if working_crystal_symmetry:
      info=get_best_match(
        flex.vec3_double([chain_xyz_fract[i]]),target_xyz_fract,
        crystal_symmetry=working_crystal_symmetry,
        distance_per_site=distance_per_site,used_j_list=used_j_list)
      if info:
        distance=info.dist()
    else:
      info=get_best_match(
        flex.vec3_double([chain_xyz_cart[i]]),target_xyz_cart,
        used_j_list=used_j_list)
      if info:
        distance=info.distance
    if info and (best_dd is None or distance<best_dd):
        best_dd=distance
        best_j=info.j
    if info is None or best_dd > max_dist:
      far_away_match_list.append(i)
      if (not quiet) and verbose:
        print("%s" %(chain_ca_lines[i]), file=out)
      continue
    if best_i is None or best_dd<best_i_dd:
      best_i=i
      best_i_dd=best_dd
      best_pair=[i,best_j]
    used_j_list.append(best_j)
    pair_list.append([i,best_j,best_dd])
  n_forward=0
  n_reverse=0
  forward_match_list=[]
  reverse_match_list=[]
  unaligned_match_list=[]
  close_match_list=[]
  forward_match_rmsd_list=flex.double()
  reverse_match_rmsd_list=flex.double()
  unaligned_match_rmsd_list=flex.double()
  close_match_rmsd_list=flex.double()
  last_i=None
  last_j=None
  for [i,j,dd],[next_i,next_j,next_dd] in zip(
      pair_list,pair_list[1:]+[[None,None,None]]):
    if i is None or j is None: continue
    found=False
    if last_i is None: # first time
      if next_i==i+1: # starting a segment
        if next_j==j+1:
          n_forward+=1
          forward_match_list.append([i,j])
          close_match_list.append([i,j])
          forward_match_rmsd_list.append(dd**2)
          close_match_rmsd_list.append(dd**2)
          found=True
        elif next_j==j-1:
          n_reverse+=1
          reverse_match_list.append([i,j])
          close_match_list.append([i,j])
          reverse_match_rmsd_list.append(dd**2)
          close_match_rmsd_list.append(dd**2)
          found=True
    else: # not the first time
      if i==last_i+1: # continuing a segment
        if j==last_j+1:
          n_forward+=1
          forward_match_list.append([i,j])
          close_match_list.append([i,j])
          forward_match_rmsd_list.append(dd**2)
          close_match_rmsd_list.append(dd**2)
          found=True
        elif j==last_j-1:
          n_reverse+=1
          reverse_match_list.append([i,j])
          close_match_list.append([i,j])
          reverse_match_rmsd_list.append(dd**2)
          close_match_rmsd_list.append(dd**2)
          found=True
    if not found:
      last_i=None
      last_j=None
      unaligned_match_list.append([i,j])
      close_match_list.append([i,j])
      unaligned_match_rmsd_list.append(dd**2)
      close_match_rmsd_list.append(dd**2)
    else:
      last_i=i
      last_j=j

  incorrect_connections = get_incorrect_connections(close_match_list)

  if n_forward==n_reverse==0:
    direction='none'
  elif n_forward>= n_reverse:
    direction='forward'
  else:
    direction='reverse'
  if (not quiet) and verbose:
    print("%s %d  %d  N: %d" %(
     direction,n_forward,n_reverse,chain_xyz_fract.size()), file=out)

  rv=rmsd_values(params=params)
  rv.add_incorrect_connections(incorrect_connections)
  rv.add_input_fragments(input_fragments)


  id='forward'
  if forward_match_rmsd_list.size():
    rmsd=forward_match_rmsd_list.min_max_mean().mean**0.5
  else:
    rmsd=None
  n=forward_match_rmsd_list.size()
  fragments_forward=get_fragment_count(forward_match_list)
  rv.add_rmsd(id=id,rmsd=rmsd,n=n,n_fragments=fragments_forward)

  id='reverse'
  if reverse_match_rmsd_list.size():
    rmsd=reverse_match_rmsd_list.min_max_mean().mean**0.5
  else:
    rmsd=None
  n=reverse_match_rmsd_list.size()
  fragments_reverse=get_fragment_count(reverse_match_list)
  rv.add_rmsd(id=id,rmsd=rmsd,n=n,n_fragments=fragments_reverse)

  id='unaligned'
  if unaligned_match_rmsd_list.size():
    rmsd=unaligned_match_rmsd_list.min_max_mean().mean**0.5
  else:
    rmsd=None
  n=unaligned_match_rmsd_list.size()
  rv.add_rmsd(id=id,rmsd=rmsd,n=n)
  id='close'
  if close_match_rmsd_list.size():
      rmsd=close_match_rmsd_list.min_max_mean().mean**0.5
  else:
      rmsd=None
  n=close_match_rmsd_list.size()
  rv.add_rmsd(id=id,rmsd=rmsd,n=n)

  id='far_away'
  rmsd=None
  n=len(far_away_match_list)
  rv.add_rmsd(id=id,rmsd=rmsd,n=n)

  if verbose and not quiet:
    print("Total CA: %d  Too far to match: %d " %(
      chain_xyz_fract.size(),len(far_away_match_list)), file=out)

  rmsd,n=rv.get_values(id='forward')
  if n and not quiet:
    print("\nResidues matching in forward direction:   %4d  RMSD: %6.2f" %(
       n,rmsd), file=out)
    if verbose:
      for i,j in forward_match_list:
        print("ID:%d:%d  RESIDUES:  \n%s\n%s" %( i,j, chain_ca_lines[i],
         target_xyz_lines[j]), file=out)

  rmsd,n=rv.get_values(id='reverse')
  if n and not quiet:
    print("Residues matching in reverse direction:   %4d  RMSD: %6.2f" %(
       n,rmsd), file=out)
    if verbose:
      for i,j in reverse_match_list:
        print("ID:%d:%d  RESIDUES:  \n%s\n%s" %(
         i,j, chain_ca_lines[i],
         target_xyz_lines[j]), file=out)

  rmsd,n=rv.get_values(id='unaligned')
  if n and not quiet:
    print("Residues near but not matching one-to-one:%4d  RMSD: %6.2f" %(
       n,rmsd), file=out)
    if verbose:
      for i,j in unaligned_match_list:
        print("ID:%d:%d  RESIDUES:  \n%s\n%s" %(i,j, chain_ca_lines[i],
          target_xyz_lines[j]), file=out)

  rmsd,n=rv.get_values(id='close')
  if n:
    lines_chain_ca=[]
    lines_target_xyz=[]
    for i,j in close_match_list:
      lines_chain_ca.append(chain_ca_lines[i])
      lines_target_xyz.append(target_xyz_lines[j])
    seq_chain_ca=get_seq_from_lines(lines_chain_ca)
    seq_target_xyz=get_seq_from_lines(lines_target_xyz)
    target_chain_ids=get_chains_from_lines(lines_target_xyz)
    target_length=get_target_length(target_chain_ids=target_chain_ids,
      hierarchy=target_ca,
      target_length_from_matching_chains=target_length_from_matching_chains)
    rv.add_target_length(id='close',target_length=target_length)

    if verbose and not quiet:
       print("SEQ1:",seq_chain_ca,len(lines_chain_ca))
       print("SEQ2:",seq_target_xyz,len(lines_target_xyz))

    match_n,match_percent=get_match_percent(seq_chain_ca,seq_target_xyz,
      params=params)
    rv.add_match_percent(id='close',match_percent=match_percent)

    percent_close=rv.get_close_to_target_percent('close')

    if not quiet:
      print("\nAll residues near target: "+\
       "%4d  RMSD: %6.2f Seq match (%%):%5.1f  %% Found: %5.1f" %(
       n,rmsd,match_percent,percent_close), file=out)
      if verbose:
        for i,j in close_match_list:
          print("ID:%d:%d  RESIDUES:  \n%s\n%s" %(i,j, chain_ca_lines[i],
            target_xyz_lines[j]), file=out)

  rmsd,n=rv.get_values(id='far_away')
  if n and not quiet:
    print("Residues far from target: %4d " %(
       n), file=out)
    if verbose:
      for i in far_away_match_list:
        print("ID:%d  RESIDUES:  \n%s" %(i,chain_ca_lines[i]), file=out)

  rv.n_forward=n_forward
  rv.n_reverse=n_reverse
  rv.n=len(pair_list)
  rv.max_dist=params.comparison.max_dist
  rv.total_target=total_target
  rv.total_chain=total_chain
  rv.used_target=used_target
  rv.used_chain=used_chain
  return rv

if __name__=="__main__":
  args=sys.argv[1:]
  rv=run(args=args,out=sys.stdout)
  """
  rv.show_summary()
  """


 *******************************************************************************


 *******************************************************************************
mmtbx/validation/clashscore.py

"""
All-atom contact analysis.  Requires Reduce and Probe (installed separately).
"""

from __future__ import absolute_import, division, print_function
from mmtbx.validation import validation, atoms, atom_info, residue
from mmtbx.utils import run_reduce_with_timeout
from libtbx.math_utils import cmp
from libtbx.utils import Sorry
from libtbx import easy_run
import libtbx.load_env
import iotbx.pdb
import os
import re
import sys
import six
import json

class clash(atoms):
  __clash_attr__ = [
    "overlap",
    "probe_type",
    "max_b_factor",
  ]
  __slots__ = atoms.__slots__ + __clash_attr__

  @staticmethod
  def header():
    return "%-20s %-20s  %7s" % ("Atom 1", "Atom 2", "Overlap")

  def id_str(self, spacer=" "):
    return "%s%s%s" % (self.atoms_info[0].id_str(), spacer,
      self.atoms_info[1].id_str())

  def id_str_no_atom_name(self):
    return "%s %s" % (self.atoms_info[0].id_str()[0:11],
      self.atoms_info[1].id_str()[0:11])

  def id_str_src_atom_no_atom_name(self):
    return self.atoms_info[0].id_str()[0:11]

  def format_old(self):
    return "%s :%.3f" % (self.id_str(), abs(self.overlap))

  def as_JSON(self):
    atom0_slots_list = [s for s in self.atoms_info[0].__slots__]
    atom0_slots_as_dict = ({s: getattr(self.atoms_info[0], s) for s in atom0_slots_list if s != 'xyz' })
    atom1_slots_list = [s for s in self.atoms_info[1].__slots__]
    atom1_slots_as_dict = ({"target_"+s: getattr(self.atoms_info[1], s) for s in atom1_slots_list if s != 'xyz' })
    atoms_dict = self.merge_two_dicts(atom0_slots_as_dict, atom1_slots_as_dict)
    #atom0_slots_as_dict["resid"] = atom0_slots_as_dict['resseq']+atom0_slots_as_dict['icode']
    #print("atom0_dict: " + str(atom0_slots_as_dict))
    serializable_slots = [s for s in self.__slots__ if s != 'atoms_info' and hasattr(self, s) ]
    slots_as_dict = ({s: getattr(self, s) for s in serializable_slots})
    #print("slots_dict: " + str(slots_as_dict))
    #print("combo:")
    #print({**slots_as_dict, **atom0_slots_as_dict})
    return json.dumps(self.merge_two_dicts(slots_as_dict, atoms_dict), indent=2)
    #return json.dumps({**slots_as_dict, **atom0_slots_as_dict, **atom1_slots_as_dict}, indent=2)

  def as_hierarchical_JSON(self):
    hierarchical_dict = {}
    hierarchy_nest_list = ['model_id', 'chain_id', 'resid', 'altloc']
    return json.dumps(self.nest_dict(hierarchy_nest_list, hierarchical_dict), indent=2)

  def as_string(self):
    return "%-20s %-20s  %7.3f" % (self.atoms_info[0].id_str(),
      self.atoms_info[1].id_str(), abs(self.overlap))

  def as_table_row_phenix(self):
    return [ self.atoms_info[0].id_str(), self.atoms_info[1].id_str(),
             abs(self.overlap) ]

  def __cmp__(self, other) : # sort in descending order
    return cmp(self.overlap, other.overlap)

  def __eq__(self, other):
    return self.overlap == other.overlap

  def __ne__(self, other):
    return self.overlap != other.overlap

  def __lt__(self, other):
    return self.overlap < other.overlap

  def __le__(self, other):
    return self.overlap <= other.overlap

  def __gt__ (self, other):
    return self.overlap > other.overlap

  def __ge__(self, other):
    return self.overlap >= other.overlap

class clashscore(validation):
  __slots__ = validation.__slots__ + [
    "clashscore",
    "clashscore_b_cutoff",
    "clash_dict",
    "clash_dict_b_cutoff",
    "list_dict",
    "b_factor_cutoff",
    "fast",
    "condensed_probe",
    "probe_file",
    "probe_clashscore_manager"
  ]
  program_description = "Analyze clashscore for protein model"
  gui_list_headers = ["Atom 1", "Atom 2", "Overlap"]
  gui_formats = ["%s", "%s", ".3f"]
  wx_column_widths = [150, 150, 150] #actually set in GUI's Molprobity/Core.py

  def get_result_class(self) : return clash

  def __init__(self,
      pdb_hierarchy,
      fast = False, # do really fast clashscore, produce only the number
      condensed_probe = False, # Use -CON for probe. Reduces output 10x.
      keep_hydrogens=True,
      nuclear=False,
      force_unique_chain_ids=False,
      time_limit=120,
      b_factor_cutoff=None,
      save_modified_hierarchy=False,
      verbose=False,
      do_flips=False,
      out=sys.stdout):
    if (not pdb_hierarchy.fits_in_pdb_format()):
      from iotbx.pdb.forward_compatible_pdb_cif_conversion \
        import forward_compatible_pdb_cif_conversion
      conversion_info = forward_compatible_pdb_cif_conversion(
        hierarchy = pdb_hierarchy)
      conversion_info.\
       convert_hierarchy_to_forward_compatible_pdb_representation(pdb_hierarchy)
      if verbose:
        print(
        "Converted model to forward_compatible PDB for clashscore",file = out)
    else:
      conversion_info = None
    validation.__init__(self)
    self.b_factor_cutoff = b_factor_cutoff
    self.fast = fast
    self.condensed_probe = condensed_probe
    self.clashscore = None
    self.clashscore_b_cutoff = None
    self.clash_dict = {}
    self.clash_dict_b_cutoff = {}
    self.list_dict = {}
    self.probe_file = None
    if (not libtbx.env.has_module(name="probe")):
      raise RuntimeError(
        "Probe could not be detected on your system.  Please make sure "+
        "Probe is in your path.\nProbe is available at "+
        "http://kinemage.biochem.duke.edu/")
    if verbose:
      if not nuclear:
        print("\nUsing electron cloud x-H distances and vdW radii")
      else:
        print("\nUsing nuclear cloud x-H distances and vdW radii")
    import iotbx.pdb
    from scitbx.array_family import flex
    from mmtbx.validation import utils
    n_models = len(pdb_hierarchy.models())
    use_segids = utils.use_segids_in_place_of_chainids(
                   hierarchy=pdb_hierarchy)
    for i_mod, model in enumerate(pdb_hierarchy.models()):
      input_str,_ = check_and_add_hydrogen(
        pdb_hierarchy=pdb_hierarchy,
        model_number=i_mod,
        nuclear=nuclear,
        verbose=verbose,
        time_limit=time_limit,
        keep_hydrogens=keep_hydrogens,
        do_flips = do_flips,
        log=out)
      r = iotbx.pdb.hierarchy.root()
      mdc = model.detached_copy()
      r.append_model(mdc)
      occ_max = flex.max(r.atoms().extract_occ())
      self.probe_clashscore_manager = probe_clashscore_manager(
        h_pdb_string=input_str,
        nuclear=nuclear,
        fast=self.fast,
        condensed_probe=self.condensed_probe,
        largest_occupancy=occ_max,
        b_factor_cutoff=b_factor_cutoff,
        use_segids=use_segids,
        verbose=verbose,
        model_id=model.id)
      self.probe_clashscore_manager.run_probe_clashscore(input_str)
      if (save_modified_hierarchy):
        self.pdb_hierarchy = iotbx.pdb.input(
          pdb_string=self.probe_clashscore_manager.h_pdb_string).construct_hierarchy()
        if conversion_info:
          conversion_info.convert_hierarchy_to_full_representation(
             self.pdb_hierarchy)

      self.clash_dict[model.id] = self.probe_clashscore_manager.clashscore
      self.clash_dict_b_cutoff[model.id] = self.probe_clashscore_manager.\
                                           clashscore_b_cutoff
      self.list_dict[model.id] = self.probe_clashscore_manager.bad_clashes
      if (n_models == 1) or (self.clashscore is None):
        self.results = self.probe_clashscore_manager.bad_clashes
        self.n_outliers = len(self.results)
        self.clashscore = self.probe_clashscore_manager.clashscore
        self.clashscore_b_cutoff = self.probe_clashscore_manager.\
                                   clashscore_b_cutoff

    if conversion_info:
      if verbose:
        print("Converted model back to full representation", file = out)
      conversion_info.convert_hierarchy_to_full_representation(pdb_hierarchy)

  def get_clashscore(self):
    return self.clashscore

  def get_clashscore_b_cutoff(self):
    return self.clashscore_b_cutoff

  def show_old_output(self, out=sys.stdout, verbose=False):
    self.print_clashlist_old(out=out)
    self.show_summary(out=out)

  def show_summary(self, out=sys.stdout, prefix=""):
    if self.clashscore is None:
      raise Sorry("PROBE output is empty. Model is not compatible with PROBE.")
    elif (len(self.clash_dict) == 1):
      #FIXME indexing keys can break py2/3 compat if more than 1 key
      k = list(self.clash_dict.keys())[0]
      #catches case where file has 1 model, but also has model/endmdl cards
      print(prefix + "clashscore = %.2f" % self.clash_dict[k], file=out)
      if self.clash_dict_b_cutoff[k] is not None and self.b_factor_cutoff is not None:
        print("clashscore (B factor cutoff = %d) = %f" % \
          (self.b_factor_cutoff,
           self.clash_dict_b_cutoff[k]), file=out)
    else:
      for k in sorted(self.clash_dict.keys()):
        print(prefix + "MODEL %s clashscore = %.2f" % (k,
          self.clash_dict[k]), file=out)
        if self.clash_dict_b_cutoff[k] is not None and self.b_factor_cutoff is not None:
          print("MODEL%s clashscore (B factor cutoff = %d) = %f" % \
            (k, self.b_factor_cutoff, self.clash_dict_b_cutoff[k]), file=out)

  def print_clashlist_old(self, out=sys.stdout):
    if self.fast:
      print("Bad Clashes >= 0.4 Angstrom - not available in fast=True mode", file=out)
      return
    for k in self.list_dict.keys():
      if k == '':
        print("Bad Clashes >= 0.4 Angstrom:", file=out)
        for result in self.list_dict[k] :
          print(result.format_old(), file=out)
      else:
        print("Bad Clashes >= 0.4 Angstrom MODEL%s" % k, file=out)
        for result in self.list_dict[k] :
          print(result.format_old(), file=out)

  def show(self, out=sys.stdout, prefix="", outliers_only=None, verbose=None):
    if (len(self.clash_dict) == 1):
      for result in self.list_dict[''] :
        print(prefix + str(result), file=out)
    else :
      for k in self.list_dict.keys():
        for result in self.list_dict[k] :
          print(prefix + str(result), file=out)
    self.show_summary(out=out, prefix=prefix)

  def as_JSON(self, addon_json={}):
    if not addon_json:
      addon_json = {}
    addon_json["validation_type"] = "clashscore"
    data = addon_json
    flat_results = []
    hierarchical_results = {}
    residue_clash_list = []
    summary_results = {}
    for k in sorted(self.list_dict.keys()):
      for result in self.list_dict[k]:
        flat_results.append(json.loads(result.as_JSON()))
        hier_result = json.loads(result.as_hierarchical_JSON())
        hierarchical_results = self.merge_dict(hierarchical_results, hier_result)

    data['flat_results'] = flat_results
    data['hierarchical_results'] = hierarchical_results

    for k in sorted(self.clash_dict.keys()):
      summary_results[k] = {"clashscore": self.clash_dict[k],
                            "num_clashes": len(self.list_dict[k])}
    data['summary_results'] = summary_results
    return json.dumps(data, indent=2)

  def as_coot_data(self):
    data = []
    for result in self.results :
      if result.is_outlier():
        data.append((result.atoms_info[0].id_str(),
          result.atoms_info[1].id_str(), result.overlap, result.xyz))
    return data

class probe_line_info(object): # this is parent
  def __init__(self, line, model_id=""):
    self.overlap_value = None
    self.model_id = model_id

  def is_similar(self, other):
    assert type(self) is type(other)
    return (self.srcAtom == other.srcAtom and self.targAtom == other.targAtom)

  def as_clash_obj(self, use_segids):
    assert self.overlap_value is not None
    atom1 = decode_atom_string(self.srcAtom,  use_segids, self.model_id)
    atom2 = decode_atom_string(self.targAtom, use_segids, self.model_id)
    if (self.srcAtom < self.targAtom):
      atoms = [ atom1, atom2 ]
    else:
      atoms = [ atom2, atom1 ]
    clash_obj = clash(
      atoms_info=atoms,
      overlap=self.overlap_value,
      probe_type=self.type,
      outlier=self.overlap_value <= -0.4,
      max_b_factor=max(self.sBval, self.tBval),
      xyz=(self.x,self.y,self.z))
    return clash_obj

class condensed_probe_line_info(probe_line_info):
  def __init__(self, line, model_id=""):
    super(condensed_probe_line_info, self).__init__(line, model_id)
    # What is in line:
    # name:pat:type:srcAtom:targAtom:dot-count:mingap:gap:spX:spY:spZ:spikeLen:score:stype:ttype:x:y:z:sBval:tBval:
    sp = line.split(":")
    self.type = sp[2]
    self.srcAtom = sp[3]
    self.targAtom = sp[4]
    self.min_gap = float(sp[6])
    self.gap = float(sp[7])
    self.x = float(sp[-5])
    self.y = float(sp[-4])
    self.z = float(sp[-3])
    self.sBval = float(sp[-2])
    self.tBval = float(sp[-1])
    self.overlap_value = self.gap

class raw_probe_line_info(probe_line_info):
  def __init__(self, line, model_id=""):
    super(raw_probe_line_info, self).__init__(line, model_id)
    self.name, self.pat, self.type, self.srcAtom, self.targAtom, self.min_gap, \
    self.gap, self.kissEdge2BullsEye, self.dot2BE, self.dot2SC, self.spike, \
    self.score, self.stype, self.ttype, self.x, self.y, self.z, self.sBval, \
    self.tBval = line.split(":")
    self.gap = float(self.gap)
    self.x = float(self.x)
    self.y = float(self.y)
    self.z = float(self.z)
    self.sBval = float(self.sBval)
    self.tBval = float(self.tBval)
    self.overlap_value = self.gap


class probe_clashscore_manager(object):
  def __init__(self,
               h_pdb_string,
               fast = False,
               condensed_probe=False,
               nuclear=False,
               largest_occupancy=10,
               b_factor_cutoff=None,
               use_segids=False,
               verbose=False,
               model_id=""):
    """
    Calculate probe (MolProbity) clashscore

    Args:
      h_pdb_string (str): PDB string that contains hydrogen atoms
      nuclear (bool): When True use nuclear cloud x-H distances and vdW radii,
        otherwise use electron cloud x-H distances and vdW radii
      largest_occupancy (int)
      b_factor_cutoff (float)
      use_segids (bool)
      verbose (bool): verbosity of printout
      model_id (str): model ID number, used in json output
    """
    assert libtbx.env.has_module(name="probe")
    if fast and not condensed_probe:
      raise Sorry("Incompatible parameters: fast=True and condensed=False:\n"+\
        "There's no way to work fast without using condensed output.")

    self.b_factor_cutoff = b_factor_cutoff
    self.fast = fast
    self.condensed_probe = condensed_probe
    self.use_segids=use_segids
    self.model_id = model_id
    ogt = 10
    blt = self.b_factor_cutoff
    if largest_occupancy < ogt:
      ogt = largest_occupancy

    self.probe_atom_b_factor = None
    probe_command = libtbx.env.under_build(os.path.join('probe', 'exe', 'probe'))
    if os.getenv('CCP4'):
      ccp4_probe = os.path.join(os.environ['CCP4'],'bin','probe')
      if (os.path.isfile(ccp4_probe) and not os.path.isfile(probe_command)):
        probe_command = ccp4_probe
    probe_command = '"%s"' % probe_command   # in case of spaces in path
    self.probe_command = probe_command
    nuclear_flag = ""
    condensed_flag = ""
    if nuclear:
      nuclear_flag = "-nuclear"
    if self.condensed_probe:
      condensed_flag = "-CON"
    self.probe_txt = \
      '%s -u -q -mc -het -once -NOVDWOUT %s %s' % (probe_command, condensed_flag, nuclear_flag) +\
        ' "ogt%d not water" "ogt%d" -' % (ogt, ogt)
    #The -NOVDWOUT probe run above is faster for clashscore to parse,
    # the full_probe_txt version below is for printing to file for coot usage
    self.full_probe_txt = \
      '%s -u -q -mc -het -once %s' % (probe_command, nuclear_flag) +\
        ' "ogt%d not water" "ogt%d" -' % (ogt, ogt)
    self.probe_atom_txt = \
      '%s -q -mc -het -dumpatominfo %s' % (probe_command, nuclear_flag) +\
        ' "ogt%d not water" -' % ogt
    if blt is not None:
      self.probe_atom_b_factor = \
        '%s -q -mc -het -dumpatominfo %s' % (probe_command, nuclear_flag) +\
          ' "blt%d ogt%d not water" -' % (blt, ogt)

    self.h_pdb_string = h_pdb_string
    #self.run_probe_clashscore(self.h_pdb_string)

  def put_group_into_dict(self, line_info, clash_hash, hbond_hash):
    key = line_info.targAtom+line_info.srcAtom
    if (line_info.srcAtom < line_info.targAtom):
      key = line_info.srcAtom+line_info.targAtom
    if line_info.type == "bo":
      if (line_info.overlap_value <= -0.4):
        if (key in clash_hash):
          if (line_info.overlap_value < clash_hash[key].overlap_value):
            clash_hash[key] = line_info
        else :
          clash_hash[key] = line_info
    elif (line_info.type == "hb"):
      if self.condensed_probe:
        hbond_hash[key] = line_info
      else: # not condensed
        if (key in hbond_hash):
          if (line_info.gap < hbond_hash[key].gap):
            hbond_hash[key] = line_info
        else :
          hbond_hash[key] = line_info

  def filter_dicts(self, new_clash_hash, new_hbond_hash):
    temp = []
    for k,v in six.iteritems(new_clash_hash):
      if k not in new_hbond_hash:
        temp.append(v.as_clash_obj(self.use_segids))
    return temp

  def process_raw_probe_output(self, probe_unformatted):
    new_clash_hash = {}
    new_hbond_hash = {}
    if self.condensed_probe:
      for line in probe_unformatted:
        try:
          line_storage = condensed_probe_line_info(line, self.model_id)
        except KeyboardInterrupt: raise
        except ValueError:
          continue # something else (different from expected) got into output
        self.put_group_into_dict(line_storage, new_clash_hash, new_hbond_hash)
    else: # not condensed
      previous_line = None
      for line in probe_unformatted:
        processed=False
        try:
          line_storage = raw_probe_line_info(line, self.model_id)
        except KeyboardInterrupt: raise
        except ValueError:
          continue # something else (different from expected) got into output

        if previous_line is not None:
          if line_storage.is_similar(previous_line):
            # modify previous line to store this one if needed
            previous_line.overlap_value = min(previous_line.overlap_value, line_storage.overlap_value)
          else:
            # seems like new group of lines, then dump previous and start new
            # one
            self.put_group_into_dict(previous_line, new_clash_hash, new_hbond_hash)
            previous_line = line_storage
        else:
          previous_line = line_storage
      if previous_line is not None:
        self.put_group_into_dict(previous_line, new_clash_hash, new_hbond_hash)
    return self.filter_dicts(new_clash_hash, new_hbond_hash)

  def get_condensed_clashes(self, lines):
    # Standalone faster parsing of output when only clashscore is needed.
    def parse_line(line):
      sp = line.split(':')
      return sp[3], sp[4], float(sp[7])
    def parse_h_line(line):
      sp = line.split(':')
      return sp[3], sp[4]

    clashes = set() # [(src, targ), (src, targ)]
    hbonds = [] # (src, targ), (targ, src)
    for l in lines:
      rtype = l[6:8]
      if rtype == 'bo':
        srcAtom, targAtom, gap = parse_line(l)
        if gap <= -0.4:
          # print l[:43], "good gap, saving", gap
          if (srcAtom, targAtom) not in clashes and (targAtom, srcAtom) not in clashes:
            clashes.add((srcAtom, targAtom))
            # print (srcAtom, targAtom)
      elif rtype == 'hb':
        srcAtom, targAtom = parse_h_line(l)
        hbonds.append((srcAtom, targAtom))
        hbonds.append((targAtom, srcAtom))
        prev_line = l
    hbonds_set = set(hbonds)
    n_clashes = 0
    # print "clashes", len(clashes)
    # print "hbonds", len(hbonds)
    for clash in clashes:
      if clash not in hbonds_set:
        n_clashes += 1
      # else:
        # print "skipping", clash
    return n_clashes

  def run_probe_clashscore(self, pdb_string):
    self.n_clashes = 0
    self.n_clashes_b_cutoff = 0
    self.clashscore_b_cutoff = None
    self.bad_clashes = []
    self.clashscore = None
    self.n_atoms = 0
    self.natoms_b_cutoff = 0

    probe_out = easy_run.fully_buffered(self.probe_txt,
      stdin_lines=pdb_string)
    if (probe_out.return_code != 0):
      raise RuntimeError("Probe crashed - dumping stderr:\n%s" %
        "\n".join(probe_out.stderr_lines))
    probe_unformatted = probe_out.stdout_lines

    # Debugging facility, do not remove!
    # import random
    # tempdir = "tmp_for_probe_debug_%d" % random.randint(1000,9999)
    # while os.path.isdir(tempdir):
    #   tempdir = "tmp_for_probe_debug_%d" % random.randint(1000,9999)
    # os.mkdir(tempdir)
    # print ("Dumping info to %s" % tempdir)
    # with open(tempdir + os.sep + 'model.pdb', 'w') as f:
    #   f.write(pdb_string)
    # with open(tempdir + os.sep + 'probe_out.txt', 'w') as f:
    #   f.write('\n'.join(probe_unformatted))

    if not self.fast:
      temp = self.process_raw_probe_output(probe_unformatted)
      self.n_clashes = len(temp)
      # XXX Warning: one more probe call here
      printable_probe_out = easy_run.fully_buffered(self.full_probe_txt,
                                                    stdin_lines=pdb_string)
      self.probe_unformatted = "\n".join(printable_probe_out.stdout_lines)
    else:
      self.n_clashes = self.get_condensed_clashes(probe_unformatted)

    # getting number of atoms from probe
    probe_info = easy_run.fully_buffered(self.probe_atom_txt,
      stdin_lines=pdb_string) #.raise_if_errors().stdout_lines
    err = probe_info.format_errors_if_any()
    if err is not None and err.find("No atom data in input.")>-1:
      return
    #if (len(probe_info) == 0):
    #  raise RuntimeError("Empty PROBE output.")
    n_atoms = 0
    for line in probe_info.stdout_lines:
      try:
        dump, n_atoms = line.split(":")
      except KeyboardInterrupt: raise
      except ValueError:
        pass # something else (different from expected) got into output
    self.n_atoms = int(n_atoms)
    if self.n_atoms == 0:
      self.clashscore = 0.0
    else:
      self.clashscore = (self.n_clashes * 1000) / self.n_atoms

    if not self.fast:
      # The rest is not necessary, we already got clashscore
      if self.b_factor_cutoff is not None:
        clashes_b_cutoff = 0
        for clash_obj in temp:
          if clash_obj.max_b_factor < self.b_factor_cutoff:
            clashes_b_cutoff += 1
        self.n_clashes_b_cutoff = clashes_b_cutoff
      used = []

      for clash_obj in sorted(temp):
        test_key = clash_obj.id_str_no_atom_name()
        test_key = clash_obj.id_str()
        if test_key not in used:
          used.append(test_key)
          self.bad_clashes.append(clash_obj)

      if self.probe_atom_b_factor is not None:
        probe_info_b_factor = easy_run.fully_buffered(self.probe_atom_b_factor,
          stdin_lines=pdb_string).raise_if_errors().stdout_lines
        for line in probe_info_b_factor :
          dump_b, natoms_b_cutoff = line.split(":")
        self.natoms_b_cutoff = int(natoms_b_cutoff)
      self.clashscore_b_cutoff = None
      if self.natoms_b_cutoff == 0:
        self.clashscore_b_cutoff = 0.0
      else :
        self.clashscore_b_cutoff = \
          (self.n_clashes_b_cutoff*1000) / self.natoms_b_cutoff

def decode_atom_string(atom_str, use_segids=False, model_id=""):
  # Example:
  # ' A  49 LEU HD11B'
  if (not use_segids) or (len(atom_str) == 16):
    return atom_info(
      model_id=model_id,
      chain_id=atom_str[0:2],
      resseq=atom_str[2:6],
      icode=atom_str[6],
      resname=atom_str[7:10],
      altloc=atom_str[15],
      name=atom_str[11:15])
  else:
    return atom_info(
      model_id=model_id,
      chain_id=atom_str[0:4],
      resseq=atom_str[4:8],
      icode=atom_str[8],
      resname=atom_str[9:12],
      altloc=atom_str[17],
      name=atom_str[13:17])

def check_and_report_reduce_failure(fb_object, input_lines, output_fname):
  if (fb_object.return_code != 0):
    with open(output_fname, 'w') as f:
      f.write(input_lines)
    msg_str = "Reduce crashed with command '%s'.\nDumping stdin to file '%s'.\n" +\
        "Return code: %d\nDumping stderr:\n%s"
    raise Sorry(msg_str % (fb_object.command, output_fname, fb_object.return_code,
                           "\n".join(fb_object.stderr_lines)))

def check_and_add_hydrogen(
        pdb_hierarchy=None,
        file_name=None,
        nuclear=False,
        keep_hydrogens=True,
        verbose=False,
        model_number=0,
        n_hydrogen_cut_off=0,
        time_limit=120,
        allow_multiple_models=True,
        crystal_symmetry=None,
        do_flips=False,
        log=None):
  """
  If no hydrogens present, force addition for clashscore calculation.
  Use REDUCE to add the hydrogen atoms.

  Args:
    pdb_hierarchy : pdb hierarchy
    file_name (str): pdb file name
    nuclear (bool): When True use nuclear cloud x-H distances and vdW radii,
      otherwise use electron cloud x-H distances and vdW radii
    keep_hydrogens (bool): when True, if there are hydrogen atoms, keep them
    verbose (bool): verbosity of printout
    model_number (int): the number of model to use
    time_limit (int): limit the time it takes to add hydrogen atoms
    n_hydrogen_cut_off (int): when number of hydrogen atoms < n_hydrogen_cut_off
      force keep_hydrogens tp True
    allow_multiple_models (bool): Allow models that contain more than one model
    crystal_symmetry : must provide crystal symmetry when using pdb_hierarchy

  Returns:
    (str): PDB string
    (bool): True when PDB string was updated
  """
  if not log: log = sys.stdout
  if file_name:
    pdb_inp = iotbx.pdb.input(file_name=file_name)
    pdb_hierarchy = pdb_inp.construct_hierarchy()
    cryst_sym = pdb_inp.crystal_symmetry()
  elif not allow_multiple_models:
    assert crystal_symmetry
    cryst_sym = crystal_symmetry
  else:
    cryst_sym = None
  assert pdb_hierarchy
  assert model_number < len(pdb_hierarchy.models())
  models = pdb_hierarchy.models()
  if (len(models) > 1) and (not allow_multiple_models):
    raise Sorry("When using CCTBX clashscore, provide only a single model.")
  model = models[model_number]
  r = iotbx.pdb.hierarchy.root()
  mdc = model.detached_copy()
  r.append_model(mdc)
  if keep_hydrogens:
    elements = r.atoms().extract_element()
    # strangely the elements can have a space when coming from phenix.clashscore
    # but no space when coming from phenix.molprobity
    h_count = elements.count('H')
    if h_count <= n_hydrogen_cut_off: h_count += elements.count(' H')
    if h_count <= n_hydrogen_cut_off: h_count += elements.count('D')
    if h_count <= n_hydrogen_cut_off: h_count += elements.count(' D')
    if h_count > n_hydrogen_cut_off:
      has_hd = True
    else:
      has_hd = False
    if not has_hd:
      if verbose:
        print("\nNo H/D atoms detected - forcing hydrogen addition!\n", file=log)
      keep_hydrogens = False
  import libtbx.load_env
  has_reduce = libtbx.env.has_module(name="reduce")
  # add hydrogen if needed
  if has_reduce and (not keep_hydrogens):
    # set reduce running parameters
    if verbose:
      print("\nAdding H/D atoms with reduce...\n")
    build = "-oh -his -flip -keep -allalt -limit{}"
    if not do_flips : build += " -pen9999"
    if nuclear:
      build += " -nuc -"
    else:
      build += " -"
    build = build.format(time_limit)
    trim = " -quiet -trim -"
    stdin_lines = r.as_pdb_string(cryst_sym)
    clean_out = run_reduce_with_timeout(
        parameters=trim,
        stdin_lines=stdin_lines)
    stdin_fname = "reduce_fail.pdb"
    check_and_report_reduce_failure(
        fb_object=clean_out,
        input_lines=stdin_lines,
        output_fname="reduce_fail.pdb")
    build_out = run_reduce_with_timeout(
        parameters=build,
        stdin_lines=clean_out.stdout_lines)
    check_and_report_reduce_failure(
        fb_object=build_out,
        input_lines=stdin_lines,
        output_fname="reduce_fail.pdb")
    reduce_str = '\n'.join(build_out.stdout_lines)
    return reduce_str,True
  else:
    if not has_reduce:
      msg = 'molprobity.reduce could not be detected on your system.\n'
      msg += 'Cannot add hydrogen to PDB file'
      print(msg, file=log)
    if verbose:
      print("\nUsing input model H/D atoms...\n")
    return r.as_pdb_string(cryst_sym),False

#-----------------------------------------------------------------------
# this isn't really enough code to justify a separate module...
#
class nqh_flip(residue):
  """
  Backwards Asn/Gln/His sidechain, identified by Reduce's hydrogen-bond
  network optimization.
  """
  def as_string(self):
    return self.id_str()

  def as_table_row_phenix(self):
    if self.chain_id:
      return [ self.chain_id, "%1s%s %s" % (self.altloc,self.resname,self.resid) ]
    elif self.segid:
      return [ self.segid, "%1s%s %s" % (self.altloc,self.resname,self.resid) ]
    else:
      raise Sorry("no chain_id or segid found for nqh flip table row")

  #alternate residue class methods for segid compatibility
  #more method overrides may be necessary
  #a more robust propagation of segid would preferable, eventually
  def atom_selection_string(self):
    if self.chain_id:
      return "(chain '%s' and resid '%s' and resname %s and altloc '%s')" % \
        (self.chain_id, self.resid, self.resname, self.altloc)
    elif self.segid:
      return "(segid %s and resid '%s' and resname %s and altloc '%s')" % \
          (self.segid, self.resid, self.resname, self.altloc)
    else:
      raise Sorry("no chain_id or segid found for nqh flip atom selection")

  def id_str(self, ignore_altloc=False):
    if self.chain_id:
      base = "%2s%4s%1s" % (self.chain_id, self.resseq, self.icode)
    elif self.segid:
      base = "%4s%4s%1s" % (self.segid, self.resseq, self.icode)
    else:
      raise Sorry("no chain_id or segid found for nqh flip id_str")
    if (not ignore_altloc):
      base += "%1s" % self.altloc
    else :
      base += " "
    base += "%3s" % self.resname
    if (self.segid is not None):
      base += " segid='%4s'" % self.segid
    return base
  #end segid compatibility

class nqh_flips(validation):
  """
  N/Q/H sidechain flips identified by Reduce.
  """
  gui_list_headers = ["Chain", "Residue"]
  gui_formats = ["%s", "%s"]
  wx_column_widths = [75,220]
  def __init__(self, pdb_hierarchy):
    re_flip = re.compile(":FLIP")
    validation.__init__(self)
    in_lines = pdb_hierarchy.as_pdb_string()
    reduce_out = run_reduce_with_timeout(
        parameters="-BUILD -",
        stdin_lines=in_lines)
    check_and_report_reduce_failure(
        fb_object=reduce_out,
        input_lines=in_lines,
        output_fname="reduce_fail.pdb")
    from mmtbx.validation import utils
    use_segids = utils.use_segids_in_place_of_chainids(
      hierarchy=pdb_hierarchy)
    for line in reduce_out.stdout_lines:
    #chain format (2-char chain)
    #USER  MOD Set 1.1: B  49 GLN     :FLIP  amide:sc=    -2.7! C(o=-5.8!,f=-1.3!)
    #segid format (4-char segid)
    #USER  MOD Set 1.1:B     49 GLN     :FLIP  amide:sc=    -2.7! C(o=-5.8!,f=-1.3!)
      if re_flip.search(line):
        resid = line.split(":")[1]
        #reduce has slightly different outputs using chains versus segid
        if len(resid) == 15: #chain
          chain_id = resid[0:2].strip()
          segid = None
          if (len(chain_id) == 0):
            chain_id = ' '
          resid_less_chain = resid[2:]
        elif len(resid) == 17: #segid
          #self.results = []
          #return
          chain_id = None
          segid = resid[0:4].strip()
          #chain_id = resid[0:4].strip()
          resid_less_chain = resid[4:]
        else:
          raise Sorry("unexpected length of residue identifier in reduce USER MODs.")
        resname = resid_less_chain[5:8]

        assert (resname in ["ASN", "GLN", "HIS"])
        flip = nqh_flip(
          chain_id=chain_id,
          segid=segid,
          resseq=resid_less_chain[0:4].strip(),
          icode= resid_less_chain[4:5],
          altloc=resid_less_chain[12:13],
          resname=resname,
          outlier=True)
        flip.set_coordinates_from_hierarchy(pdb_hierarchy)
        self.results.append(flip)
        self.n_outliers += 1

  def show(self, out=sys.stdout, prefix=""):
    if (self.n_outliers == 0):
      print(prefix+"No backwards Asn/Gln/His sidechains found.", file=out)
    else :
      for flip in self.results :
        print(prefix+flip.as_string(), file=out)


 *******************************************************************************


 *******************************************************************************
mmtbx/validation/clashscore2.py

"""
All-atom contact analysis.  Calls mmtbx.reduce and mmtbx.probe.
This is a rewrite of the original clashscore that used stand-alone
external reduce and probe programs.
"""

from __future__ import absolute_import, division, print_function
from mmtbx.validation.clashscore import clash
from mmtbx.hydrogens import reduce_hydrogen
from mmtbx.reduce import Optimizers
from mmtbx.programs import probe2
from mmtbx.validation import validation, atoms, atom_info
from libtbx.utils import Sorry, null_out
import iotbx.pdb
import iotbx.cli_parser
import mmtbx
import os
import re
import sys
import six
import json
import copy
import tempfile

def remove_models_except_index(model_manager, model_index):
    hierarchy = model_manager.get_hierarchy()
    models = hierarchy.models()
    if model_index < len(models):
        selected_model = models[model_index]
        for model in models:
            if model != selected_model:
                hierarchy.remove_model(model=model)
    return model_manager

class clashscore2(validation):
  __slots__ = validation.__slots__ + [
    "clashscore",
    "clashscore_b_cutoff",
    "clash_dict",
    "clash_dict_b_cutoff",
    "list_dict",
    "b_factor_cutoff",
    "fast",
    "condensed_probe",
    "probe_file",
    "probe_clashscore_manager"
  ]
  program_description = "Analyze clashscore for protein model"
  gui_list_headers = ["Atom 1", "Atom 2", "Overlap"]
  gui_formats = ["%s", "%s", ".3f"]
  wx_column_widths = [150, 150, 150] #actually set in GUI's Molprobity/Core.py

  def get_result_class(self) : return clash

  def __init__(self,
      probe_parameters,
      data_manager,
      fast = False, # do really fast clashscore, produce only the number
      condensed_probe = False, # Use -CON for probe. Reduces output 10x.
      keep_hydrogens=True,
      nuclear=False,
      force_unique_chain_ids=False,
      time_limit=120,
      b_factor_cutoff=None,
      verbose=False,
      do_flips=False,
      out=sys.stdout):
    validation.__init__(self)
    self.b_factor_cutoff = b_factor_cutoff
    self.fast = fast
    self.condensed_probe = condensed_probe
    self.clashscore = None
    self.clashscore_b_cutoff = None
    self.clash_dict = {}
    self.clash_dict_b_cutoff = {}
    self.list_dict = {}
    self.probe_file = None
    if verbose:
      if not nuclear:
        print("\nUsing electron cloud x-H distances and vdW radii")
      else:
        print("\nUsing nuclear cloud x-H distances and vdW radii")
    import iotbx.pdb
    from scitbx.array_family import flex
    from mmtbx.validation import utils

    data_manager_model = data_manager.get_model()
    # Fix up bogus unit cell when it occurs by checking crystal symmetry.
    data_manager_model.add_crystal_symmetry_if_necessary()

    # If we've been asked to, add hydrogens to all of the models in the PDB hierarchy
    # associated with our data_manager_model.
    data_manager_model,_ = check_and_add_hydrogen(
      probe_parameters=probe_parameters,
      data_manager_model=data_manager_model,
      nuclear=nuclear,
      verbose=verbose,
      keep_hydrogens=keep_hydrogens,
      do_flips = do_flips,
      log=out)

    # First we must rebuild the model from the new hierarchy so that the copy can succeed.
    # Make a copy of the original model to use for submodel processing, we'll trim atoms out
    # of it for each submodel.
    data_manager_model = mmtbx.model.manager(
      model_input       = None,
      pdb_hierarchy     = data_manager_model.get_hierarchy(),
      stop_for_unknowns = False,
      crystal_symmetry  = data_manager_model.crystal_symmetry(),
      restraint_objects = None,
      log               = None)
    original_model = data_manager_model.deep_copy()

    pdb_hierarchy = data_manager_model.get_hierarchy()
    n_models = len(pdb_hierarchy.models())
    use_segids = utils.use_segids_in_place_of_chainids(
                   hierarchy=pdb_hierarchy)
    for i_mod, model in enumerate(pdb_hierarchy.models()):

      # Select only the current submodel from the hierarchy
      submodel = original_model.deep_copy()
      remove_models_except_index(submodel, i_mod)

      # Construct a hierarchy for the current submodel
      r = iotbx.pdb.hierarchy.root()
      mdc = submodel.get_hierarchy().models()[0].detached_copy()
      r.append_model(mdc)

      occ_max = flex.max(r.atoms().extract_occ())

      # Make yet another model for the new hierarchy
      subset_model_manager = mmtbx.model.manager(
        model_input       = None,
        pdb_hierarchy     = r,
        stop_for_unknowns = False,
        crystal_symmetry  = submodel.crystal_symmetry(),
        restraint_objects = None,
        log               = None)

      self.probe_clashscore_manager = probe_clashscore_manager(
        nuclear=nuclear,
        fast=self.fast,
        condensed_probe=self.condensed_probe,
        largest_occupancy=occ_max,
        b_factor_cutoff=b_factor_cutoff,
        use_segids=use_segids,
        verbose=verbose,
        model_id=model.id)
      self.probe_clashscore_manager.run_probe_clashscore(data_manager, subset_model_manager)

      self.clash_dict[model.id] = self.probe_clashscore_manager.clashscore
      self.clash_dict_b_cutoff[model.id] = self.probe_clashscore_manager.\
                                           clashscore_b_cutoff
      self.list_dict[model.id] = self.probe_clashscore_manager.bad_clashes
      if (n_models == 1) or (self.clashscore is None):
        self.results = self.probe_clashscore_manager.bad_clashes
        self.n_outliers = len(self.results)
        self.clashscore = self.probe_clashscore_manager.clashscore
        self.clashscore_b_cutoff = self.probe_clashscore_manager.\
                                   clashscore_b_cutoff

  def get_clashscore(self):
    return self.clashscore

  def get_clashscore_b_cutoff(self):
    return self.clashscore_b_cutoff

  def show_old_output(self, out=sys.stdout, verbose=False):
    self.print_clashlist_old(out=out)
    self.show_summary(out=out)

  def show_summary(self, out=sys.stdout, prefix=""):
    if self.clashscore is None:
      raise Sorry("PROBE output is empty. Model is not compatible with PROBE.")
    elif (len(self.clash_dict) == 1):
      #FIXME indexing keys can break py2/3 compat if more than 1 key
      k = list(self.clash_dict.keys())[0]
      #catches case where file has 1 model, but also has model/endmdl cards
      print(prefix + "clashscore = %.2f" % self.clash_dict[k], file=out)
      if self.clash_dict_b_cutoff[k] is not None and self.b_factor_cutoff is not None:
        print("clashscore (B factor cutoff = %d) = %f" % \
          (self.b_factor_cutoff,
           self.clash_dict_b_cutoff[k]), file=out)
    else:
      for k in sorted(self.clash_dict.keys()):
        print(prefix + "MODEL %s clashscore = %.2f" % (k,
          self.clash_dict[k]), file=out)
        if self.clash_dict_b_cutoff[k] is not None and self.b_factor_cutoff is not None:
          print("MODEL%s clashscore (B factor cutoff = %d) = %f" % \
            (k, self.b_factor_cutoff, self.clash_dict_b_cutoff[k]), file=out)

  def print_clashlist_old(self, out=sys.stdout):
    if self.fast:
      print("Bad Clashes >= 0.4 Angstrom - not available in fast=True mode", file=out)
      return
    for k in self.list_dict.keys():
      if k == '':
        print("Bad Clashes >= 0.4 Angstrom:", file=out)
        for result in self.list_dict[k] :
          print(result.format_old(), file=out)
      else:
        print("Bad Clashes >= 0.4 Angstrom MODEL%s" % k, file=out)
        for result in self.list_dict[k] :
          print(result.format_old(), file=out)

  def show(self, out=sys.stdout, prefix="", outliers_only=None, verbose=None):
    if (len(self.clash_dict) == 1):
      for result in self.list_dict[''] :
        print(prefix + str(result), file=out)
    else :
      for k in self.list_dict.keys():
        for result in self.list_dict[k] :
          print(prefix + str(result), file=out)
    self.show_summary(out=out, prefix=prefix)

  def as_JSON(self, addon_json={}):
    if not addon_json:
      addon_json = {}
    addon_json["validation_type"] = "clashscore"
    data = addon_json
    flat_results = []
    hierarchical_results = {}
    residue_clash_list = []
    summary_results = {}
    for k in sorted(self.list_dict.keys()):
      for result in self.list_dict[k]:
        flat_results.append(json.loads(result.as_JSON()))
        hier_result = json.loads(result.as_hierarchical_JSON())
        hierarchical_results = self.merge_dict(hierarchical_results, hier_result)

    data['flat_results'] = flat_results
    data['hierarchical_results'] = hierarchical_results

    for k in sorted(self.clash_dict.keys()):
      summary_results[k] = {"clashscore": self.clash_dict[k],
                            "num_clashes": len(self.list_dict[k])}
    data['summary_results'] = summary_results
    return json.dumps(data, indent=2)

  def as_coot_data(self):
    data = []
    for result in self.results :
      if result.is_outlier():
        data.append((result.atoms_info[0].id_str(),
          result.atoms_info[1].id_str(), result.overlap, result.xyz))
    return data

class probe_line_info(object): # this is parent
  def __init__(self, line, model_id=""):
    self.overlap_value = None
    self.model_id = model_id

  def is_similar(self, other):
    assert type(self) is type(other)
    return (self.srcAtom == other.srcAtom and self.targAtom == other.targAtom)

  def as_clash_obj(self, use_segids):
    assert self.overlap_value is not None
    atom1 = decode_atom_string(self.srcAtom,  use_segids, self.model_id)
    atom2 = decode_atom_string(self.targAtom, use_segids, self.model_id)
    if (self.srcAtom < self.targAtom):
      atoms = [ atom1, atom2 ]
    else:
      atoms = [ atom2, atom1 ]
    clash_obj = clash(
      atoms_info=atoms,
      overlap=self.overlap_value,
      probe_type=self.type,
      outlier=self.overlap_value <= -0.4,
      max_b_factor=max(self.sBval, self.tBval),
      xyz=(self.x,self.y,self.z))
    return clash_obj

class condensed_probe_line_info(probe_line_info):
  def __init__(self, line, model_id=""):
    super(condensed_probe_line_info, self).__init__(line, model_id)
    # What is in line:
    # name:pat:type:srcAtom:targAtom:dot-count:mingap:gap:spX:spY:spZ:spikeLen:score:stype:ttype:x:y:z:sBval:tBval:
    sp = line.split(":")
    self.type = sp[2]
    self.srcAtom = sp[3]
    self.targAtom = sp[4]
    self.min_gap = float(sp[6])
    self.gap = float(sp[7])
    self.x = float(sp[-5])
    self.y = float(sp[-4])
    self.z = float(sp[-3])
    self.sBval = float(sp[-2])
    self.tBval = float(sp[-1])
    self.overlap_value = self.gap

class raw_probe_line_info(probe_line_info):
  def __init__(self, line, model_id=""):
    super(raw_probe_line_info, self).__init__(line, model_id)
    self.name, self.pat, self.type, self.srcAtom, self.targAtom, self.min_gap, \
    self.gap, self.kissEdge2BullsEye, self.dot2BE, self.dot2SC, self.spike, \
    self.score, self.stype, self.ttype, self.x, self.y, self.z, self.sBval, \
    self.tBval = line.split(":")
    self.gap = float(self.gap)
    self.x = float(self.x)
    self.y = float(self.y)
    self.z = float(self.z)
    self.sBval = float(self.sBval)
    self.tBval = float(self.tBval)
    self.overlap_value = self.gap


class probe_clashscore_manager(object):
  def __init__(self,
               fast = False,
               condensed_probe=False,
               nuclear=False,
               largest_occupancy=10,
               b_factor_cutoff=None,
               use_segids=False,
               verbose=False,
               model_id=""):
    """
    Calculate probe (MolProbity) clashscore

    Args:
      nuclear (bool): When True use nuclear cloud x-H distances and vdW radii,
        otherwise use electron cloud x-H distances and vdW radii
      largest_occupancy (int)
      b_factor_cutoff (float)
      use_segids (bool)
      verbose (bool): verbosity of printout
      model_id (str): model ID number, used in json output
    """
    if fast and not condensed_probe:
      raise Sorry("Incompatible parameters: fast=True and condensed=False:\n"+\
        "There's no way to work fast without using condensed output.")

    self.b_factor_cutoff = b_factor_cutoff
    self.fast = fast
    self.condensed_probe = condensed_probe
    self.use_segids=use_segids
    self.model_id = model_id
    self.occupancy_frac = 0.1
    if largest_occupancy / 100 < self.occupancy_frac:
      self.occupancy_frac = largest_occupancy / 100
    self.nuclear = nuclear

  def put_group_into_dict(self, line_info, clash_hash, hbond_hash):
    key = line_info.targAtom+line_info.srcAtom
    if (line_info.srcAtom < line_info.targAtom):
      key = line_info.srcAtom+line_info.targAtom
    if line_info.type == "bo":
      if (line_info.overlap_value <= -0.4):
        if (key in clash_hash):
          if (line_info.overlap_value < clash_hash[key].overlap_value):
            clash_hash[key] = line_info
        else :
          clash_hash[key] = line_info
    elif (line_info.type == "hb"):
      if self.condensed_probe:
        hbond_hash[key] = line_info
      else: # not condensed
        if (key in hbond_hash):
          if (line_info.gap < hbond_hash[key].gap):
            hbond_hash[key] = line_info
        else :
          hbond_hash[key] = line_info

  def filter_dicts(self, new_clash_hash, new_hbond_hash):
    temp = []
    for k,v in six.iteritems(new_clash_hash):
      if k not in new_hbond_hash:
        temp.append(v.as_clash_obj(self.use_segids))
    return temp

  def process_raw_probe_output(self, probe_unformatted):
    new_clash_hash = {}
    new_hbond_hash = {}
    if self.condensed_probe:
      for line in probe_unformatted:
        try:
          line_storage = condensed_probe_line_info(line, self.model_id)
        except KeyboardInterrupt: raise
        except ValueError:
          continue # something else (different from expected) got into output
        self.put_group_into_dict(line_storage, new_clash_hash, new_hbond_hash)
    else: # not condensed
      previous_line = None
      for line in probe_unformatted:
        processed=False
        try:
          line_storage = raw_probe_line_info(line, self.model_id)
        except KeyboardInterrupt: raise
        except ValueError:
          continue # something else (different from expected) got into output

        if previous_line is not None:
          if line_storage.is_similar(previous_line):
            # modify previous line to store this one if needed
            previous_line.overlap_value = min(previous_line.overlap_value, line_storage.overlap_value)
          else:
            # seems like new group of lines, then dump previous and start new
            # one
            self.put_group_into_dict(previous_line, new_clash_hash, new_hbond_hash)
            previous_line = line_storage
        else:
          previous_line = line_storage
      if previous_line is not None:
        self.put_group_into_dict(previous_line, new_clash_hash, new_hbond_hash)
    return self.filter_dicts(new_clash_hash, new_hbond_hash)

  def get_condensed_clashes(self, lines):
    # Standalone faster parsing of output when only clashscore is needed.
    def parse_line(line):
      sp = line.split(':')
      return sp[3], sp[4], float(sp[7])
    def parse_h_line(line):
      sp = line.split(':')
      return sp[3], sp[4]

    clashes = set() # [(src, targ), (src, targ)]
    hbonds = [] # (src, targ), (targ, src)
    for l in lines:
      rtype = l[6:8]
      if rtype == 'bo':
        srcAtom, targAtom, gap = parse_line(l)
        if gap <= -0.4:
          # print l[:43], "good gap, saving", gap
          if (srcAtom, targAtom) not in clashes and (targAtom, srcAtom) not in clashes:
            clashes.add((srcAtom, targAtom))
            # print (srcAtom, targAtom)
      elif rtype == 'hb':
        srcAtom, targAtom = parse_h_line(l)
        hbonds.append((srcAtom, targAtom))
        hbonds.append((targAtom, srcAtom))
        prev_line = l
    hbonds_set = set(hbonds)
    n_clashes = 0
    # print "clashes", len(clashes)
    # print "hbonds", len(hbonds)
    for clash in clashes:
      if clash not in hbonds_set:
        n_clashes += 1
      # else:
        # print "skipping", clash
    return n_clashes

  # We have to take both the original data manager, which is from the model
  # without hydrogens, and the hydrogenated modified model because we need one
  # to construct a Probe2 program and the other to replace its model to run on.
  def run_probe_clashscore(self, data_manager, hydrogenated_model):
    self.n_clashes = 0
    self.n_clashes_b_cutoff = 0
    self.clashscore_b_cutoff = None
    self.bad_clashes = []
    self.clashscore = None
    self.n_atoms = 0
    self.natoms_b_cutoff = 0

    # Construct override parameters and then run probe2 using them and delete the resulting
    # temporary file.
    tempName = tempfile.mktemp()
    parser = iotbx.cli_parser.CCTBXParser(program_class=probe2.Program, logger=null_out())
    args = [
      "source_selection='(occupancy > {}) and not water'".format(self.occupancy_frac),
      "target_selection='occupancy > {}'".format(self.occupancy_frac),
      "use_neutron_distances={}".format(self.nuclear),
      "approach=once",
      "output.filename='{}'".format(tempName),
      "output.format=raw",
      "output.condensed={}".format(self.condensed_probe),
      "output.report_vdws=False",
      "ignore_lack_of_explicit_hydrogens=True",
    ]
    parser.parse_args(args)
    p2 = probe2.Program(data_manager, parser.working_phil.extract(),
                       master_phil=parser.master_phil, logger=null_out())
    p2.overrideModel(hydrogenated_model)
    dots, output = p2.run()
    probe_unformatted = output.splitlines()
    os.unlink(tempName)

    # Debugging facility, do not remove!
    # import random
    # pdb_string = hydrogenated_model.get_hierarchy().as_pdb_string()
    # tempdir = "tmp_for_probe_debug_%d" % random.randint(1000,9999)
    # while os.path.isdir(tempdir):
    #   tempdir = "tmp_for_probe_debug_%d" % random.randint(1000,9999)
    # os.mkdir(tempdir)
    # print ("Dumping info to %s" % tempdir)
    # with open(tempdir + os.sep + 'model.pdb', 'w') as f:
    #   f.write(pdb_string)
    # with open(tempdir + os.sep + 'probe_out.txt', 'w') as f:
    #   f.write('\n'.join(probe_unformatted))

    if not self.fast:
      temp = self.process_raw_probe_output(probe_unformatted)
      self.n_clashes = len(temp)

      # If we're not running fast, call probe2 again to get non-condensed output
      # including VDW contacts.  We make another temporary file for the output and
      # then delete it.  This option is for printing to file for coot usage.  The
      # no-VDWOUT option is used to speed up the parsing of the output.
      tempName = tempfile.mktemp()
      parser = iotbx.cli_parser.CCTBXParser(program_class=probe2.Program, logger=null_out())
      args = [
        "source_selection='(occupancy > {}) and not water'".format(self.occupancy_frac),
        "target_selection='occupancy > {}'".format(self.occupancy_frac),
        "use_neutron_distances={}".format(self.nuclear),
        "approach=once",
        "output.filename='{}'".format(tempName),
        "output.format=raw",
        "ignore_lack_of_explicit_hydrogens=True",
      ]
      parser.parse_args(args)
      p2 = probe2.Program(data_manager, parser.working_phil.extract(),
                         master_phil=parser.master_phil, logger=null_out())
      p2.overrideModel(hydrogenated_model)
      dots, output = p2.run()
      self.probe_unformatted = "\n".join(output.splitlines())
      os.unlink(tempName)
    else:
      self.n_clashes = self.get_condensed_clashes(probe_unformatted)

    # Find the number of non-water atoms that pass the occupancy test
    # and then use it to compute the clashscore.
    self.n_atoms = 0
    for a in hydrogenated_model.get_hierarchy().atoms():
      isWater = iotbx.pdb.common_residue_names_get_class(name=a.parent().resname) == "common_water"
      if a.occ > self.occupancy_frac and not isWater:
        self.n_atoms += 1
    if self.n_atoms == 0:
      self.clashscore = 0.0
    else:
      self.clashscore = (self.n_clashes * 1000) / self.n_atoms

    if not self.fast:
      # The rest is not necessary, we already got clashscore
      if self.b_factor_cutoff is not None:
        clashes_b_cutoff = 0
        for clash_obj in temp:
          if clash_obj.max_b_factor < self.b_factor_cutoff:
            clashes_b_cutoff += 1
        self.n_clashes_b_cutoff = clashes_b_cutoff
      used = []

      for clash_obj in sorted(temp):
        test_key = clash_obj.id_str_no_atom_name()
        test_key = clash_obj.id_str()
        if test_key not in used:
          used.append(test_key)
          self.bad_clashes.append(clash_obj)

      if self.b_factor_cutoff is not None:
        # Find the number of non-water atoms who pass the B factor test and the occupancy test
        self.natoms_b_cutoff = 0
        for a in hydrogenated_model.get_hierarchy().atoms():
          isWater = iotbx.pdb.common_residue_names_get_class(name=a.parent().resname) == "common_water"
          if  (a.b < self.b_factor_cutoff) and (a.occ > self.occupancy_frac) and not isWater:
            self.natoms_b_cutoff += 1
      self.clashscore_b_cutoff = None
      if self.natoms_b_cutoff == 0:
        self.clashscore_b_cutoff = 0.0
      else :
        self.clashscore_b_cutoff = \
          (self.n_clashes_b_cutoff*1000) / self.natoms_b_cutoff

def decode_atom_string(atom_str, use_segids=False, model_id=""):
  # Example:
  # ' A  49 LEU HD11B'
  if (not use_segids) or (len(atom_str) == 16):
    return atom_info(
      model_id=model_id,
      chain_id=atom_str[0:2],
      resseq=atom_str[2:6],
      icode=atom_str[6],
      resname=atom_str[7:10],
      altloc=atom_str[15],
      name=atom_str[11:15])
  else:
    return atom_info(
      model_id=model_id,
      chain_id=atom_str[0:4],
      resseq=atom_str[4:8],
      icode=atom_str[8],
      resname=atom_str[9:12],
      altloc=atom_str[17],
      name=atom_str[13:17])

def check_and_add_hydrogen(
        probe_parameters=None,
        data_manager_model=None,
        nuclear=False,
        keep_hydrogens=True,
        verbose=False,
        n_hydrogen_cut_off=0,
        do_flips=False,
        log=None):
  """
  If no hydrogens present, force addition for clashscore calculation.
  Use REDUCE to add the hydrogen atoms.

  Args:
    data_manager_model : Model from the data_manager
    nuclear (bool): When True use nuclear cloud x-H distances and vdW radii,
      otherwise use electron cloud x-H distances and vdW radii
    keep_hydrogens (bool): when True, if there are hydrogen atoms, keep them
    verbose (bool): verbosity of printout
    n_hydrogen_cut_off (int): when number of hydrogen atoms < n_hydrogen_cut_off
      force keep_hydrogens tp True

  Returns:
    (model): Model with hydrogens added
    (bool): True when the model was modified/replaced
  """
  if not log: log = sys.stdout
  assert probe_parameters
  assert data_manager_model
  if keep_hydrogens:
    elements = data_manager_model.get_hierarchy().atoms().extract_element()
    # strangely the elements can have a space when coming from phenix.clashscore
    # but no space when coming from phenix.molprobity
    h_count = elements.count('H')
    if h_count <= n_hydrogen_cut_off: h_count += elements.count(' H')
    if h_count <= n_hydrogen_cut_off: h_count += elements.count('D')
    if h_count <= n_hydrogen_cut_off: h_count += elements.count(' D')
    if h_count > n_hydrogen_cut_off:
      has_hd = True
    else:
      has_hd = False
    if not has_hd:
      if verbose:
        print("\nNo H/D atoms detected - forcing hydrogen addition!\n", file=log)
      keep_hydrogens = False

  # add hydrogen if needed
  if not keep_hydrogens:
    # Remove hydrogens and add them back in
    if verbose:
      print("\nTrimming and adding hydrogens...\n")
    reduce_add_h_obj = reduce_hydrogen.place_hydrogens(
      model = data_manager_model,
      use_neutron_distances=nuclear,
      n_terminal_charge="residue_one",
      exclude_water=True,
      stop_for_unknowns=False,
      keep_existing_H=False
    )
    reduce_add_h_obj.run()
    reduce_add_h_obj.show(log)
    missed_residues = set(reduce_add_h_obj.no_H_placed_mlq)
    if len(missed_residues) > 0:
      bad = ""
      for res in missed_residues:
        bad += " " + res
      raise Sorry("Restraints were not found for the following residues:"+bad)
    data_manager_model = reduce_add_h_obj.get_model()

    # Optimize H atoms with mmtbx.reduce
    if verbose:
      print("\nOptimizing H atoms with mmtbx.reduce...\n")
    opt = Optimizers.Optimizer(probe_parameters, do_flips, data_manager_model, modelIndex=None,
      fillAtomDump = False)

    # Re-process the model because we have removed some atoms that were previously
    # bonded.  Don't make restraints during the reprocessing.
    # We had to do this to keep from crashing on a call to pair_proxies when generating
    # mmCIF files, so we always do it for safety.
    data_manager_model.get_hierarchy().sort_atoms_in_place()
    data_manager_model.get_hierarchy().atoms().reset_serial()
    p = mmtbx.model.manager.get_default_pdb_interpretation_params()
    p.pdb_interpretation.allow_polymer_cross_special_position=True
    p.pdb_interpretation.clash_guard.nonbonded_distance_threshold=None
    p.pdb_interpretation.use_neutron_distances = nuclear
    p.pdb_interpretation.proceed_with_excessive_length_bonds=True
    p.pdb_interpretation.disable_uc_volume_vs_n_atoms_check=True
    # We need to turn this on because without it 1zz0.txt kept flipping the ring
    # in A TYR 214 every time we re-interpreted. The original interpretation done
    # by Hydrogen placement will have flipped them, so we don't need to do it again.
    p.pdb_interpretation.flip_symmetric_amino_acids=False
    #p.pdb_interpretation.sort_atoms=True
    data_manager_model.process(make_restraints=False, pdb_interpretation_params=p)

    return data_manager_model, True
  else:
    if verbose:
      print("\nUsing input model H/D atoms...\n")
    return data_manager_model, False

  def show(self, out=sys.stdout, prefix=""):
    if (self.n_outliers == 0):
      print(prefix+"No backwards Asn/Gln/His sidechains found.", file=out)
    else :
      for flip in self.results :
        print(prefix+flip.as_string(), file=out)


 *******************************************************************************


 *******************************************************************************
mmtbx/validation/comparama.py
from __future__ import absolute_import, division, print_function

import iotbx.phil
from libtbx import group_args
from libtbx.utils import null_out
from libtbx.test_utils import approx_equal
from mmtbx.validation.ramalyze import ramalyze, find_region_max_value
import math
import numpy as np
from collections import Counter
from scitbx.array_family import flex
import six
from six.moves import zip

master_phil_str = '''
comparama {
  nproc = 1
    .type = int
  show_labels = True
    .type = bool
    .help = show labels for outlier residues
  outlier_favored = lime
    .type = str
    .help = Color of outlier->favored arrows. None if not needed
  outlier_allowed = lime
    .type = str
    .help = Color of outlier->allowed arrows. None if not needed
  allowed_outlier = red
    .type = str
    .help = Color of allowed->outlier arrows. None if not needed
  allowed_favored = green
    .type = str
    .help = Color of allowed->favored arrows. None if not needed
  favored_outlier = red
    .type = str
    .help = Color of favored->outlier arrows. None if not needed
  favored_allowed = orange
    .type = str
    .help = Color of favored->allowed arrows. None if not needed
}
'''

def get_to_0_360(angle):
  if angle < 0:
    return angle + 360
  return angle

def get_distance(a1, a2):
  a = a1-a2
  if a < -180:
    a = a + 360
  if a > 180:
    a = a-360
  return a

class two_rama_points(object):
  def __init__(self, a, b):
    self.a = a
    self.b = b
    self.min_len = None
    self.best_xy_multipliers = None

  def length(self, abeg, aend):
    return math.sqrt((abeg[0]-aend[0])**2 + (abeg[1]-aend[1])**2)
  def get_xy_multipliers(self):
    if self.best_xy_multipliers is not None:
      self.min_length()
    return self.best_xy_multipliers

  def min_length(self, plot_ranges=((-180, 180),(-180, 180))):
    if self.min_len is not None:
      return self.min_len
    base_len = self.length(self.a, self.b)
    self.min_len = base_len
    # print("base_len", base_len)
    xspan = plot_ranges[0][1] - plot_ranges[0][0]
    yspan = plot_ranges[1][1] - plot_ranges[1][0]
    self.best_xy_multipliers = [0,0]
    for x in [-1,0,1]:
      for y in [-1,0,1]:
        new_x = self.b[0] + x*xspan
        new_y = self.b[1] + y*yspan
        tlen = self.length(self.a, (new_x, new_y))
        if tlen < self.min_len:
          self.min_len = tlen
          self.best_xy_multipliers = [x,y]
          # print("  min_len", min_len, best_xy_multipliers)
    return self.min_len

def determine_validation_change_text(r1, r2):
  rt1 = r1.ramalyze_type()
  rt2 = r2.ramalyze_type()
  # if r1.is_outlier() and not r2.is_outlier():
  #   return "fixed"
  # if not r1.is_outlier() and r2.is_outlier():
  #   return "broke"
  if rt1 == rt2:
    return rt1
  return "%s -> %s" % (rt1, rt2)


class rcompare(object):
  def __init__(self, model1, model2, params=None, log=null_out()):
    self.plots = None
    self.params = params
    if self.params is None:
      self.params = rcompare.get_default_params().comparama
    self.rama1 = ramalyze(model1.get_hierarchy(), out=null_out())
    self.rama2 = ramalyze(model2.get_hierarchy(), out=null_out())
    self.results = []
    # looping technique trying to recover when 1 or several residues are
    # missing
    i1 = i2 = 0
    self.skipped_1 = []
    self.skipped_2 = []
    while i1 < len(self.rama1.results) and i2 < len(self.rama2.results):
      r1 = self.rama1.results[i1]
      r2 = self.rama2.results[i2]
      if r1.id_str() == r2.id_str():
        # regular calculations
        diff = math.sqrt((r1.phi-r2.phi)**2 + (r1.psi-r2.psi)**2)
        v = determine_validation_change_text(r1, r2)
        diff2 = math.sqrt(get_distance(r1.phi, r2.phi)**2 +
            get_distance(r1.psi, r2.psi)**2)
        diff3 = two_rama_points((r1.phi, r1.psi), (r2.phi, r2.psi)).min_length()
        assert approx_equal(diff2, diff3), "%s, %s" % ((r1.phi, r1.psi), (r2.phi, r2.psi))
        self.results.append((r1.id_str(), diff2, r1.phi, r1.psi, r2.phi, r2.psi, v, r2.res_type, r1.score/100, r2.score/100))
        i1 += 1
        i2 += 1
      else:
        skip_1 = False
        # figure out what to skip
        if r1.chain_id == r2.chain_id:
          if r1.resseq_as_int() < r2.resseq_as_int():
            skip_1 = True
        else:
          if r1.resseq_as_int() > r2.resseq_as_int():
            skip_1 = True
        if skip_1:
          i1 += 1
          self.skipped_1.append(r1)
        else:
          i2 += 1
          self.skipped_2.append(r2)
    self.res_columns = None
    if len(self.results) > 0:
      self.res_columns = list(zip(*self.get_results()))

  def get_results(self):
    return self.results

  def get_results_as_vec3(self):
    r1 = flex.vec3_double()
    r2 = flex.vec3_double()
    assert len(self.results) > 0
    for r in self.results:
      r1.append((r[2], r[3], 0))
      r2.append((r[4], r[5], 0))
    return r1, r2

  def get_ramalyze_objects(self):
    return self.rama1, self.rama2

  def get_skipped(self):
    return self.skipped_1, self.skipped_2

  def get_number_results(self):
    if len(self.results) > 0:
      v1, v2 = rama_rescale(self.results)
      rv1 = [1-x for x in v1]
      rv2 = [1-x for x in v2]
      return group_args(
          mean_diff=np.mean(self.res_columns[1]),
          std_diff=np.std(self.res_columns[1]),
          sum_1 = np.sum(self.res_columns[-2]),
          sum_2 = np.sum(self.res_columns[-1]),
          n_res = len(self.res_columns[-1]),
          scaled_sum_1 = np.sum(v1),
          scaled_sum_2 = np.sum(v2),
          rev_scaled_sum_1 = np.sum(rv1),
          rev_scaled_sum_2 = np.sum(rv2),
          counts = Counter(self.res_columns[-4]),
          )
    return None

  def get_plots(self, wrap_arrows=True):
    if self.plots is not None:
      return self.plots
    self.plots = self.rama2.get_plots(
        show_labels=self.params.show_labels,
        point_style='bo',
        markersize=3,
        markeredgecolor="black",
        dpi=300,
        markerfacecolor="white")
    for pos, plot in six.iteritems(self.plots):
      # prepare data
      arrows_info = []
      if self.params.allowed_outlier is not None:
        arrs = [x for x in self.results if (x[-3]==pos and x[-4] == "Allowed -> OUTLIER")]
        arrs.sort(key=lambda x:x[1], reverse=True)
        arrows_info.append((arrs, self.params.allowed_outlier))
      if self.params.allowed_favored is not None:
        arrs = [x for x in self.results if (x[-3]==pos and x[-4] == "Allowed -> Favored")]
        arrs.sort(key=lambda x:x[1], reverse=True)
        arrows_info.append((arrs, self.params.allowed_favored))
      if self.params.favored_outlier is not None:
        arrs = [x for x in self.results if (x[-3]==pos and x[-4] == "Favored -> OUTLIER")]
        arrs.sort(key=lambda x:x[1], reverse=True)
        arrows_info.append((arrs, self.params.favored_outlier))
      if self.params.favored_allowed is not None:
        arrs = [x for x in self.results if (x[-3]==pos and x[-4] == "Favored -> Allowed")]
        arrs.sort(key=lambda x:x[1], reverse=True)
        arrows_info.append((arrs, self.params.favored_allowed))
      if self.params.outlier_favored is not None:
        arrs = [x for x in self.results if (x[-3]==pos and x[-4] == "OUTLIER -> Favored")]
        arrs.sort(key=lambda x:x[1], reverse=True)
        arrows_info.append((arrs, self.params.outlier_favored))
      if self.params.outlier_allowed is not None:
        arrs = [x for x in self.results if (x[-3]==pos and x[-4] == "OUTLIER -> Allowed")]
        arrs.sort(key=lambda x:x[1], reverse=True)
        arrows_info.append((arrs, self.params.outlier_allowed))

      for data, color in arrows_info:
        # print (len(data))
        if data and len(data) < 0: continue
        ad = [((x[2], x[3]),(x[4], x[5])) for x in data]
        add_arrows_on_plot(
            p=plot,
            arrows_data=ad,
            wrap_arrows=wrap_arrows,
            color=color)
    return self.plots

  @staticmethod
  def get_default_params():
    """
    Get extracted params
    """
    return iotbx.phil.parse(
          input_string=master_phil_str,
          process_includes=True).extract()

def rama_rescale(results):
  res1 = []
  res2 = []
  for (r1_id_str, diff2, r1_phi, r1_psi, r2_phi, r2_psi, v, r2_res_type,
      r1_score, r2_score) in results:
    for phi, psi, score, res in [(r1_phi, r1_psi, r1_score, res1),
                                 (r2_phi, r2_psi, r2_score, res2)]:
      max_value = find_region_max_value(r2_res_type, phi, psi)
      if max_value is None:
        res.append(score)
      else:
        res.append(score/max_value[1])
  return res1, res2

def breake_arrow_if_needed(abeg, aend, plot_ranges):
  eps = 1e-3
  tp = two_rama_points(abeg, aend)
  actual_len = tp.length(abeg, aend)
  min_len = tp.min_length()
  best_xy_multipliers = tp.get_xy_multipliers()
  result = []
  if best_xy_multipliers == [0,0]:
    return [(abeg,aend)]
  # Now we figure out how to brake it.
  result = [ [abeg, (0,0)], [(0,0), aend] ]
  ix = 0 if best_xy_multipliers[0] == -1 else 1
  iy = 0 if best_xy_multipliers[1] == -1 else 1
  if approx_equal(abeg[0], aend[0], eps, out=None):
    # case where x1 == x2
    result[0][1] = (abeg[0], plot_ranges[0][iy])
    result[1][0] = (abeg[0], plot_ranges[0][1-iy])
  elif best_xy_multipliers.count(0) == 1:
    # general case, 1 border crossing
    # y = ax + b
    n_aend = (aend[0]+360*best_xy_multipliers[0], aend[1]+360*best_xy_multipliers[1])
    a = (n_aend[1]-abeg[1]) / (n_aend[0] - abeg[0])
    b = n_aend[1] - a*n_aend[0]
    if best_xy_multipliers[0] != 0:
      # x wrapping, calculating y
      y = a*(plot_ranges[0][ix]) + b
      y = get_distance(y, 0)
      result[0][1] = (plot_ranges[0][ix],   y)
      result[1][0] = (plot_ranges[0][1-ix], y)
    else:
      # y wrapping, calculating x
      x = (plot_ranges[1][iy] - b) / a
      x = get_distance(x, 0)
      result[0][1] = (x, plot_ranges[1][iy])
      result[1][0] = (x, plot_ranges[1][1-iy])
  else:
    # both sides cutting. just go to the corner to make things simple
    result[0][1] = (plot_ranges[0][ix], plot_ranges[1][iy])
    result[1][0] = (plot_ranges[0][1-ix], plot_ranges[1][1-iy])
  return result

def add_arrows_on_plot(
    p,
    arrows_data,
    color='green',
    wrap_arrows=True,
    plot_ranges=[(-180, 180), (-180, 180)]):
  """
  p - pyplot
  arrows_data - [((x,y beginning), (x,y end)), ... ((xy),(xy))]
  wrap_arrows - draw shortest possible arrow - wrap around plot edges
  ranges - ranges of the plot
  """
  import matplotlib.patches as patches
  import matplotlib.lines as lines

  style="Simple,head_length=10,head_width=5,tail_width=1"
  for arrow in arrows_data:
    r = [(arrow[0], arrow[1])]
    if wrap_arrows:
      r = breake_arrow_if_needed(arrow[0], arrow[1], plot_ranges)
      for l_coors in r[:-1]:
        l = lines.Line2D(
            xdata = [l_coors[0][0], l_coors[1][0]],
            ydata = [l_coors[0][1], l_coors[1][1]],
            linewidth=1.7, color=color, zorder=10)
        p.plot.add_line(l)
    p.plot.add_patch(patches.FancyArrowPatch(
        r[-1][0],
        r[-1][1],
        arrowstyle=style,
        color = color,
        linewidth=0.5,
        zorder=10,
        ))


 *******************************************************************************
