

 *******************************************************************************
mmtbx/__init__.py
from __future__ import absolute_import, division, print_function

from libtbx.utils import Sorry
from libtbx.version import get_version
from scitbx.array_family import flex

__version__ = get_version()

class fmodels(object):
  """
  Container object for F_model values used during refinement.

  Attributes
  ----------
  fmodel_xray :
  fmodel_neutron :
  xray_scattering_dict :
  neutron_scattering_dict :
  neutron_refinement :
  """
  def __init__(self, fmodel_xray = None,
                     fmodel_neutron = None,
                     xray_scattering_dict = None,
                     neutron_scattering_dict = None,
                     neutron_refinement = None,
                     log = None):
    self.fmodel_x = fmodel_xray
    self.fmodel_n = fmodel_neutron
    self.xray_scattering_dict = xray_scattering_dict
    self.neutron_scattering_dict = neutron_scattering_dict
    self.neutron_refinement = neutron_refinement
    self.log = log
    self.create_target_functors()

  def pseudo_deep_copy(self):
    """
    Makes a deep copy of self.

    Returns
    -------
    mmtbx.fmodels
    """
    fmodel_n_dc = None
    if(self.fmodel_n is not None):
      fmodel_n_dc = self.fmodel_n.deep_copy()
    result = fmodels(fmodel_xray             = self.fmodel_x.deep_copy(),
                     fmodel_neutron          = fmodel_n_dc,
                     xray_scattering_dict    = self.xray_scattering_dict,
                     neutron_scattering_dict = self.neutron_scattering_dict,
                     neutron_refinement      = self.neutron_refinement,
                     log                     = self.log)
    result.update_xray_structure(xray_structure =
      self.fmodel_x.xray_structure.deep_copy_scatterers())
    return result

  def resolution_filter(self, d_min):
    """
    Returns a copy of self with a resolution filter applied to the x-ray and
    neutron maps above a given resolution.

    Parameters
    ----------
    d_min : float
        Reflections with resolutions <= d_min are removed.

    Returns
    -------
    mmtbx.fmodels

    See Also
    --------
    mmtbx.f_model.manager.resolution_filter
    """
    fmodel_n_dc = None
    if(self.fmodel_n is not None):
      fmodel_n_dc = self.fmodel_n.resolution_filter(d_min = d_min)
    result = fmodels(
      fmodel_xray             = self.fmodel_x.resolution_filter(d_min = d_min),
      fmodel_neutron          = fmodel_n_dc,
      xray_scattering_dict    = self.xray_scattering_dict,
      neutron_scattering_dict = self.neutron_scattering_dict,
      neutron_refinement      = self.neutron_refinement,
      log                     = self.log)
    return result

  def fmodel_xray(self, xray_structure = None):
    """
    ...
    """
    if(self.fmodel_x is not None):
      if(self.fmodel_n is not None):
        if(xray_structure is not None):
          self.fmodel_x.xray_structure = xray_structure
        # mrt: discard old scattering dictionary to avoid mixing xray and neutron
        self.fmodel_x.xray_structure._scattering_type_registry = None
        # XXX: xray tables could mix here
        # update possibly changed xray dictionary
        self.xray_scattering_dict = \
          self.fmodel_x.xray_structure.scattering_type_registry(custom_dict =
          self.xray_scattering_dict).as_type_gaussian_dict()
      #assert not self.fmodel_x.xray_structure.guess_scattering_type_neutron()
    return self.fmodel_x

  def fmodel_neutron(self, xray_structure = None):
    """
    ...
    """
    if(self.fmodel_n is not None):
      if(xray_structure is not None):
        self.fmodel_n.xray_structure = xray_structure
      # mrt: discard old scattering dictionary to avoid mixing xray and neutron
      self.fmodel_n.xray_structure._scattering_type_registry = None
      # update possibly changed xray dictionary
      self.neutron_scattering_dict = \
        self.fmodel_n.xray_structure.scattering_type_registry(custom_dict =
        self.neutron_scattering_dict, table="neutron").as_type_gaussian_dict()
      assert self.fmodel_n.xray_structure.guess_scattering_type_neutron()
    return self.fmodel_n

  def update_xray_structure(self, xray_structure = None,
                                  update_f_calc  = None,
                                  update_f_mask  = None,
                                  force_update_f_mask = False):
    """
    ...
    """
    if(self.fmodel_x is not None):
      self.fmodel_xray(xray_structure = xray_structure).update_xray_structure(
        xray_structure = xray_structure,
        update_f_calc  = update_f_calc,
        update_f_mask  = update_f_mask,
        force_update_f_mask = force_update_f_mask)
    if(self.fmodel_n is not None):
      self.fmodel_neutron(xray_structure=xray_structure).update_xray_structure(
        xray_structure = xray_structure,
        update_f_calc  = update_f_calc,
        update_f_mask  = update_f_mask,
        force_update_f_mask = force_update_f_mask)

  def show_short(self, log=None):
    """
    ...
    """
    if log is None: log = self.log
    if(self.fmodel_x is not None):
      prefix = ""
      if(self.fmodel_n is not None): prefix = "x-ray data"
      self.fmodel_xray().info().show_rfactors_targets_scales_overall(
        header = prefix, out = log)
    if(self.fmodel_n is not None):
      print(file=self.log)
      self.fmodel_neutron().info().show_rfactors_targets_scales_overall(
        header = "neutron data", out = log)

  def show_comprehensive(self, message = ""):
    """
    ...
    """
    from mmtbx.refinement import print_statistics
    print_statistics.make_sub_header("X-ray data", out = self.log)
    if(self.fmodel_x is not None):
      self.fmodel_xray().info().show_all(header = message, out = self.log)
    if(self.fmodel_n is not None):
      print_statistics.make_sub_header("Neutron data", out = self.log)
      self.fmodel_neutron().info().show_all(header = message, out = self.log)

  def update_all_scales(
        self,
        update_f_part1,
        remove_outliers     = True,
        params              = None,
        optimize_mask       = False,
        force_update_f_mask = False,
        nproc               = 1,
        log                 = None,
        apply_back_trace    = False,
        refine_hd_scattering=None):
    """
    ...
    """
    if log is None: log = self.log
    fast=True
    if(params.mode=="slow"): fast=False
    from mmtbx.refinement import print_statistics
    print_statistics.make_header("updating all scales", out = log)
    self.update_xray_structure(update_f_calc = True, update_f_mask = True,
      force_update_f_mask = force_update_f_mask)
    if([self.fmodel_x, self.fmodel_n].count(None)==0): apply_back_trace=False
    if(self.fmodel_x is not None):
      msg = None
      if(self.fmodel_n is not None):
        msg = "X-ray:"
        print(msg, file=log)
      self.fmodel_xray().update_all_scales(
        update_f_part1       = update_f_part1,
        remove_outliers      = remove_outliers,
        params               = params,
        fast                 = fast,
        log                  = log,
        show                 = True,
        optimize_mask        = optimize_mask,
        nproc                = nproc,
        apply_back_trace     = apply_back_trace,
        refine_hd_scattering = refine_hd_scattering)
      self.fmodel_x.show(log = log, suffix = msg)
    if(self.fmodel_n is not None):
      msg = "Neutron:"
      print(msg, file=log)
      self.fmodel_neutron().update_all_scales(
        update_f_part1       = update_f_part1,
        remove_outliers      = remove_outliers,
        params               = params,
        fast                 = fast,
        log                  = log,
        show                 = True,
        optimize_mask        = optimize_mask,
        nproc                = nproc,
        apply_back_trace     = apply_back_trace,
        refine_hd_scattering = refine_hd_scattering)
      self.fmodel_n.show(log = log, suffix = msg)

  def show_targets(self, log, text=""):
    """
    ...
    """
    prefix_x = ""
    if(self.fmodel_n is not None):
      prefix_x = "xray"
    self.fmodel_xray().info().show_targets(out = log, text = prefix_x+" "+text)
    if(self.fmodel_n is not None):
      self.fmodel_neutron().info().show_targets(out= log, text="neutron "+text)

  def update(self, target_name=None):
    """
    ...
    """
    if(self.fmodel_x is not None):
      self.fmodel_x.update(target_name=target_name)
    if(self.fmodel_n is not None):
      self.fmodel_n.update(target_name=target_name)

  def create_target_functors(self, alpha_beta=None):
    """
    ...
    """
    self.target_functor_xray = self.fmodel_xray().target_functor(
      alpha_beta = alpha_beta)
    self.target_functor_neutron = None
    if(self.fmodel_n is not None):
      self.target_functor_neutron = self.fmodel_neutron().target_functor()

  def prepare_target_functors_for_minimization(self):
    """
    ...
    """
    self.target_functor_xray.prepare_for_minimization()
    if (self.target_functor_neutron is not None):
      self.target_functor_neutron.prepare_for_minimization()

  def target_functions_are_invariant_under_allowed_origin_shifts(self):
    """
    ...
    """
    for f in [
          self.target_functor_xray,
          self.target_functor_neutron]:
      if (f is None): continue
      if (not f.target_function_is_invariant_under_allowed_origin_shifts()):
        return False
    return True

  def target_functor_result_xray(self, compute_gradients):
    """
    ...
    """
    fmx = self.fmodel_xray()
    return self.target_functor_xray(compute_gradients = compute_gradients)

  def target_functor_result_neutron(self, compute_gradients):
    """
    ...
    """
    result = None
    if(self.fmodel_n is not None):
      fmn = self.fmodel_neutron()
      result = self.target_functor_neutron(compute_gradients=compute_gradients)
    return result

  def target_and_gradients(self, compute_gradients, weights=None,
        u_iso_refinable_params = None, occupancy = False):
    """
    ...
    """
    tfx = self.target_functor_result_xray
    tfn = self.target_functor_result_neutron
    class tg(object):
      def __init__(self, fmodels):
        self.fmodels = fmodels
        tfx_r = tfx(compute_gradients = compute_gradients)
        wx=1.
        if(weights is not None): wx = weights.wx * weights.wx_scale
        self.target_work_xray = tfx_r.target_work()
        self.target_work_xray_weighted = self.target_work_xray * wx
        self.gradient_xray = None
        if(compute_gradients):
          # XXX discamb
          if self.fmodels.fmodel_xray().is_taam():
            #f0 = self.fmodels.fmodel_xray().xray_structure.scatterers()[0].flags
            #if f0.grad_occupancy():
            #gs = tfx_r.gradients_wrt_atomic_parameters()
            #if   f0.grad_site():
            #  sf = flex.vec3_double([g.site_derivatives for g in gs]).as_double()
            #elif f0.grad_u_iso():
            #  sf = flex.double([g.adp_derivatives[0] for g in gs])
            if occupancy:
              sel = flex.size_t()
              scs = self.fmodels.fmodel_xray().xray_structure.scatterers()
              sz = scs.size()
              sf_ = flex.double(sz,0)
              for i_seq, s in enumerate(scs):
                if s.flags.grad_occupancy():
                  sel.append(i_seq)
              sf = tfx_r.gradients_wrt_atomic_parameters()
              sf_ = sf_.set_selected(sel, sf)
              sf = sf_
            else:
              sf = tfx_r.gradients_wrt_atomic_parameters()
          # XXX discamb
          else:
            if(occupancy):
              sf = tfx_r.gradients_wrt_atomic_parameters(occupancy = occupancy)
            else:
              sf = tfx_r.gradients_wrt_atomic_parameters(
                u_iso_refinable_params = u_iso_refinable_params).packed()

          self.gradient_xray = sf
          self.gradient_xray_weighted = sf * wx
        if(fmodels.fmodel_neutron() is not None):
          wn=1
          if(weights is not None): wn = weights.wn * weights.wn_scale
          tfn_r = tfn(compute_gradients = compute_gradients)
          self.target_work_neutron = tfn_r.target_work()
          self.target_work_neutron_weighted = self.target_work_neutron * wn
          if(compute_gradients):
            if(occupancy):
              sf = tfn_r.gradients_wrt_atomic_parameters(occupancy = occupancy)
            else:
              sf = tfn_r.gradients_wrt_atomic_parameters(
                u_iso_refinable_params=u_iso_refinable_params).packed()
            self.gradient_neutron = sf
            self.gradient_neutron_weighted = sf * wn
      def target(self):
        if(self.fmodels.fmodel_neutron() is not None):
          if(weights is not None):
            result = (self.target_work_xray * weights.wx_scale + \
                      self.target_work_neutron * weights.wn_scale) * weights.wxn
          else:
            result = self.target_work_xray + self.target_work_neutron
        else: result = self.target_work_xray_weighted
        return result
      def gradients(self):
        result = None
        if(compute_gradients):
          if(self.fmodels.fmodel_neutron() is not None):
            if(weights is not None):
              result = (self.gradient_xray  * weights.wx_scale + \
                        self.gradient_neutron * weights.wn_scale) * weights.wxn
            else:
              result = self.gradient_xray + self.gradient_neutron
          else: result = self.gradient_xray_weighted
        return result
    result = tg(fmodels = self)
    return result

class map_names(object):
  """
  Class used for parsing and external display of map's name.

  Attributes
  ----------
  k : float
     Scale for F_obs.
  n : float
      Scale for F_model.
  ml_map : bool
  anomalous : bool
  anomalous_residual : bool
  phaser_sad_llg : bool
  f_obs_filled : bool
  """

  FC = ['fcalc','fcal','fc', 'fmodel','fmod','fm']
  DFC = ['dfcalc','dfcal','dfc', 'dfmodel','dfmod','dfm']
  FO = ['fobs','fob','fo']
  MFO = ['mfobs','mfob','mfo']
  FILLED = ['+filled','-filled','filled','+fill','-fill','fill']

  def __init__(self, map_name_string):
    s = map_name_string.lower()
    self.k = None
    self.n = None
    self.ml_map = None
    self.anomalous = False
    self.anomalous_residual = False
    self.phaser_sad_llg = False
    self.f_obs_filled = False
    for sym in ["~","!","@","#","$","%","^","&","*","(",")","=","<",">","?","/",
                ":",";","|","[","]","{","}",",","_"," "]:
      s = s.replace(sym,"")
    for tmp in self.FILLED:
      if tmp in s:
        s = s.replace(tmp,"")
        self.f_obs_filled = True
    if(s.count('ano')):
      if (s.count('resid')):
        self.anomalous_residual = True
      else :
        self.anomalous = True
    elif (s.count("sad") or s.count("llg")):
      self.phaser_sad_llg = True
    elif(s in self.FC):
      self.k = 0
      self.n = 1
      self.ml_map = False
    elif(s in self.DFC):
      self.k = 0
      self.n = 1
      self.ml_map = True
    elif(s in self.FO):
      self.k = 1
      self.n = 0
      self.ml_map = False
    elif(s in self.MFO):
      self.k = 1
      self.n = 0
      self.ml_map = True
    else:
      #
      found_D = False
      for tmp in self.DFC:
        if(tmp in s):
          found_D = True
          s = s.replace(tmp,"C")
      found_M = False
      for tmp in self.MFO:
        if(tmp in s):
          found_M = True
          s = s.replace(tmp,"O")
      if(not ([found_D,found_M].count(True) in [0,2])):
        self.error(map_name_string)
      if([found_D,found_M].count(True)==2): self.ml_map=True
      elif([found_D,found_M].count(True)==0): self.ml_map=False
      else: self.error(map_name_string)
      #
      if(not self.ml_map):
        for tmp in self.FC:
          if(tmp in s): s = s.replace(tmp,"C")
        for tmp in self.FO:
          if(tmp in s): s = s.replace(tmp,"O")
      #
      if(s.count("O")+s.count("C")!=2):
        self.error(map_name_string)
      for tmp in s:
        if(not (tmp in ["C","O"])):
          if(tmp.isalpha()): self.error(map_name_string)
      if(s.index("O")<s.index("C")):
        tmp = s[s.index("O")+1:s.index("C")]
        sign = None
        if(tmp.count("+")==1): sign="+"
        elif(tmp.count("-")==1): sign="-"
        else: self.error(map_name_string)
        po = s[:s.index("O")+tmp.index(sign)+1]
        pc = s[s.index("O")+tmp.index(sign)+1:]
        po = po.replace("O","")
        pc = pc.replace("C","")
        if(len(po)==0): self.k = 1.
        elif(len(po)==1 and po in ["+","-"]): self.k = float("%s1"%po)
        else: self.k = float(po)
        if(len(pc)==0): self.n = 1.
        elif(len(pc)==1 and pc in ["+","-"]): self.n = float("%s1"%pc)
        else: self.n = float(pc)
      elif(s.index("O")>s.index("C")):
        tmp = s[s.index("C")+1:s.index("O")]
        sign = None
        if(tmp.count("+")==1): sign="+"
        elif(tmp.count("-")==1): sign="-"
        else: self.error(map_name_string)
        po = s[:s.index("C")+tmp.index(sign)+1]
        pc = s[s.index("C")+tmp.index(sign)+1:]
        po = po.replace("C","")
        pc = pc.replace("O","")
        if(len(po)==0): self.n = 1.
        elif(len(po)==1 and po in ["+","-"]): self.n = float("%s1"%po)
        else: self.n = float(po)
        if(len(pc)==0): self.k = 1.
        elif(len(pc)==1 and pc in ["+","-"]): self.k = float("%s1"%pc)
        else: self.k = float(pc)
      else: raise RuntimeError("Error attempting to decode map name string "+
        "'%s'" % map_name_string)
    if(self.k is not None):
      self.k = float(self.k)
      self.n = float(self.n)
      #if self.n < 0: self.n *= -1

  def error(self, s):
    """
    Raises an exception for bad map names.
    """
    msg="""\n
Wrong map type requested: %s
  Allowed format is: %s
    where [p] and [q] are any numbers (optional),
          [m] and [D] indicate if the requested map is sigmaa (optional),
          Fo and Fc are Fobs and Fcalc,
          [filled] is for missing Fobs filled map.
  Examples: 2mFo-DFc, 3.2Fo-2.3Fc, mFobs-DFcalc, 2mFobs-DFcalc_filled, Fc,
            2mFobs-DFcalc_fill, anom, anom_diff, anomalous_difference, Fo
"""
    format = "[p][m]Fo+[q][D]Fc[filled]"
    raise Sorry(msg%(s,format))

  def format(self):
    """
    Formats a map name for external display.

    Examples
    --------
    >>> r = mmtbx.map_names(map_name_string="mFo-DFc ")
    >>> print r.format()
    mFobs-DFmodel
    """
    if (not self.anomalous) and (not self.phaser_sad_llg):
      if(abs(int(self.k)-self.k)<1.e-6): k = str(int(self.k))
      else: k = str(self.k)
      if(abs(int(self.n)-self.n)<1.e-6):
        sign = ""
        if(self.n>0): sign = "+"
        if(self.n==0): sign = "-"
        n = sign+str(int(self.n))
      else: n = str(self.n)
      if(k=="1" or k=="+1"): k = ""
      if(n=="1" or n=="+1"): n = "+"
      if(k=="-1"): k = "-"
      if(n=="-1"): n = "-"
      if(self.ml_map):
        result = k+"mFobs"+n+"DFmodel"
      else:
        result = k+"Fobs"+n+"Fmodel"
      if(self.f_obs_filled): result += "_filled"
      return result
    else:
      assert [self.k,self.n,self.ml_map].count(None) == 3
      assert [self.f_obs_filled].count(False)==1
      if (self.phaser_sad_llg):
        return "phaser_sad_llg"
      return "anomalous_difference"


 *******************************************************************************


 *******************************************************************************
mmtbx/arrays.py
from __future__ import absolute_import, division, print_function
from scitbx.array_family import flex
from libtbx import adopt_init_args
import boost_adaptbx.boost.python as bp
from six.moves import range
ext = bp.import_ext("mmtbx_f_model_ext")
from cctbx import miller

class init(object): # XXX PVA: Why Fobs, HL and r_free_flags are not here?
  def __init__(self,
               f_calc,
               f_masks=None,
               f_part1=None,
               f_part2=None,
               k_masks=None,
               k_isotropic_exp=None,
               k_isotropic=None,
               k_anisotropic=None):
    adopt_init_args(self, locals())
    if(self.f_masks is None):
      self.f_masks = [self.f_calc.customized_copy(
        data=flex.complex_double(f_calc.data().size(), 0))]
    else:
      if(not (type(self.f_masks) in [list, tuple])):
        self.f_masks = [self.f_masks]
      for fm in self.f_masks:
        assert self.f_calc.indices().all_eq(fm.indices())
    if(self.k_isotropic_exp is not None):
      assert self.k_isotropic_exp.size() == self.f_calc.indices().size()
    else: self.k_isotropic_exp = flex.double(f_calc.data().size(), 1)
    if(self.k_isotropic is not None):
      assert self.k_isotropic.size() == self.f_calc.indices().size()
    else: self.k_isotropic = flex.double(f_calc.data().size(), 1)
    if(self.k_anisotropic is not None):
      assert self.k_anisotropic.size() == self.f_calc.indices().size()
    else: self.k_anisotropic = flex.double(f_calc.data().size(), 1)
    if(self.k_masks is None):
      n=len(self.f_masks)
      self.k_masks = [flex.double(f_calc.data().size(), 0)]*n
    else:
      if(not (type(self.k_masks) in [list, tuple])):
        self.k_masks = [self.k_masks]
    if(self.f_part1 is not None):
      assert self.f_calc.indices().all_eq(self.f_part1.indices())
    else:
      self.f_part1 = self.f_calc.customized_copy(
        data=flex.complex_double(f_calc.data().size(), 0))
    if(self.f_part2 is not None):
      assert self.f_calc.indices().all_eq(self.f_part2.indices())
    else:
      self.f_part2 = self.f_calc.customized_copy(
        data=flex.complex_double(f_calc.data().size(), 0))
    # assemble f_bulk
    f_bulk_data = flex.complex_double(f_calc.data().size(), 0)
    assert len(self.k_masks) == len(self.f_masks)
    for i in range(len(self.k_masks)):
      f_bulk_data += self.k_masks[i]*self.f_masks[i].data()
    #
    self.data = ext.data(
      f_calc          = self.f_calc.data(),
      f_bulk          = f_bulk_data,
      k_isotropic_exp = self.k_isotropic_exp,
      k_isotropic     = self.k_isotropic,
      k_anisotropic   = self.k_anisotropic,
      f_part1         = self.f_part1.data(),
      f_part2         = self.f_part2.data())
    self.f_model = miller.array(miller_set=self.f_calc, data=self.data.f_model)
    self.f_model_no_aniso_scale = miller.array(
      miller_set=self.f_calc,
      data      =self.data.f_model_no_aniso_scale)

  def f_mask(self):
    assert len(self.f_masks)==1
    return self.f_masks[0]

  def k_mask(self):
    assert len(self.k_masks)==1
    return self.k_masks[0]

  def select(self, selection=None):
    if(selection is None): return self
    assert self.f_calc.indices().size() == selection.size()
    f_masks = [fm.select(selection=selection) for fm in self.f_masks]
    k_masks = [km.select(selection) for km in self.k_masks]
    return init(
      f_calc          = self.f_calc.select(selection),
      f_masks         = f_masks,
      k_isotropic_exp = self.k_isotropic_exp.select(selection),
      k_isotropic     = self.k_isotropic.select(selection),
      k_masks         = k_masks,
      k_anisotropic   = self.k_anisotropic.select(selection),
      f_part1         = self.f_part1.select(selection),
      f_part2         = self.f_part2.select(selection))

  def deep_copy(self):
    return self.select(selection=flex.bool(self.f_calc.indices().size(), True))

  def __getstate__(self):
    return {"args": (
      self.f_calc,
      self.f_masks,
      self.f_part1,
      self.f_part2,
      self.k_masks,
      self.k_isotropic_exp,
      self.k_isotropic,
      self.k_anisotropic)}

  def __setstate__(self, state):
    assert len(state) == 1
    self.__init__(*state["args"])

  # XXX PVA: returns new object, not itself updated. This is misleading!
  # See fmodel_kbu where this is done rigth.
  def update(self,
             f_calc=None,
             f_masks=None,
             k_isotropic_exp=None,
             k_isotropic=None,
             k_masks=None,
             k_anisotropic=None,
             f_part1=None,
             f_part2=None):
    if(f_calc is None):          f_calc          = self.f_calc
    if(f_masks is None):         f_masks         = self.f_masks
    if(k_isotropic_exp is None): k_isotropic_exp = self.k_isotropic_exp
    if(k_isotropic is None):     k_isotropic     = self.k_isotropic
    if(k_masks is None):         k_masks         = self.k_masks
    if(k_anisotropic is None):   k_anisotropic   = self.k_anisotropic
    if(f_part1 is None):         f_part1         = self.f_part1
    if(f_part2 is None):         f_part2         = self.f_part2
    self.__init__(
      f_calc          = f_calc,
      f_masks         = f_masks,
      k_isotropic_exp = k_isotropic_exp,
      k_isotropic     = k_isotropic,
      k_masks         = k_masks,
      k_anisotropic   = k_anisotropic,
      f_part1         = f_part1,
      f_part2         = f_part2)
    return self


 *******************************************************************************


 *******************************************************************************
mmtbx/clashes.py
from __future__ import absolute_import, division, print_function
import mmtbx.monomer_library
import mmtbx.monomer_library.pdb_interpretation
import sys
from cctbx.geometry_restraints.linking_class import linking_class
from six.moves import zip
origin_ids = linking_class()
from libtbx import adopt_init_args
import mmtbx.model


# XXX Need to support model_manager object input

# MARKED_FOR_DELETION_OLEG
# REASON: bad practices for obtaining GRM. remove_clashes should be used instead.
def from_xray_structure(
      clash_threshold,
      xray_structure,
      geometry_restraints_manager):
  # Find covalently bonded atoms to exclude them from non-bonded clash list
  bond_proxies_simple, asu = geometry_restraints_manager.get_all_bond_proxies(
      sites_cart = xray_structure.sites_cart())
  bonded_i_seqs = []
  for bp in bond_proxies_simple:
    bonded_i_seqs.append(bp.i_seqs)
  # Find all pairs within clash_threshold
  pair_asu_table = xray_structure.pair_asu_table(
    distance_cutoff=clash_threshold)
  pair_sym_table = pair_asu_table.extract_pair_sym_table()
  atom_pairs_i_seqs = pair_sym_table.simple_edge_list()
  # Get only non-bonded clashes
  return list(set(atom_pairs_i_seqs).difference(set(bonded_i_seqs)))

class from_pdb(object):

  def __init__(self, pdb_str,
      clash_threshold, remove_clash_guard=False,
      remove_max_reasonable_bond_distance=False,
      allow_polymer_cross_special_position=False):
    params = mmtbx.monomer_library.pdb_interpretation.master_params.extract()
    if remove_clash_guard:
      params.clash_guard.nonbonded_distance_threshold=None
      params.proceed_with_excessive_length_bonds=True
    if remove_max_reasonable_bond_distance:
      params.max_reasonable_bond_distance=None
    if allow_polymer_cross_special_position:
      params.allow_polymer_cross_special_position=True

    ppf = mmtbx.monomer_library.pdb_interpretation.process(
      mon_lib_srv              = mmtbx.monomer_library.server.server(),
      ener_lib                 = mmtbx.monomer_library.server.ener_lib(),
      raw_records              = pdb_str,
      params                   = params,
      strict_conflict_handling = True,
      force_symmetry           = True,
      log                      = None)
    self.atoms = list(ppf.all_chain_proxies.pdb_hierarchy.atoms())
    self._pairs = from_xray_structure(
      clash_threshold             = clash_threshold,
      xray_structure              = ppf.xray_structure(),
      geometry_restraints_manager = ppf.geometry_restraints_manager())
    self._ppf=ppf  #so that parents are remembered

  def format_clash_string(self, pair):
    a1 = self.atoms[pair[0]]
    a2 = self.atoms[pair[1]]
    l1,l2 = a1.pdb_label_columns(), a2.pdb_label_columns()
    dist = a1.distance(a2)
    fmt = "'%s' clash with '%s' distance %4.2f A"
    return fmt%(l1,l2,dist)

  def clashing_pairs(self):
    return self._pairs

  def show(self, log=None):
    if(log is None): log = sys.stdout
    for pair in self._pairs:
      print(self.format_clash_string(pair=pair), file=log)

# END_MARKED_FOR_DELETION_OLEG

main_chain_plus_cb=[
       "N","CA","C","O","CB",
       "O5'","C5'","C4'","O4'",
              "C1'","C2'","C3'",
              "O3'","P","OP1","OP2","OP3","O1P","O2P","O3P",
       "O2'" ]

class remove_clashes(object):
  # Remove side-chains/entire residues that are causing clashes or have
  #  bad bonds

  def __init__(self,
      model=None,
      non_bonded_deviation_threshold=0.4,
      bonded_deviation_threshold=0.2,
      max_fraction=None):

    adopt_init_args(self, locals())
    self.side_chains_removed=0
    self.residues_removed=0

    self.get_labels_and_proxies()
    if max_fraction:
      max_items=int(max_fraction*self.model.get_number_of_atoms())
    else:
      max_items=None
    self.worst_non_bonded_pairs_and_delta=self.get_bad_nonbonded(
       max_items=max_items)
    self.worst_bonded_pairs_and_delta=self.get_bad_bonded(
       max_items=max_items)

    self.get_remove_selection_string()
    # redo model after removing residues/side chains specified
    self.pare_model()


  def pare_model(self):
    if not self.remove_selection_string:
      self.new_model=self.model.deep_copy() # nothing to do
    else:
      sel1 = self.model.selection(string = " NOT (%s) " %(
         self.remove_selection_string))
      self.new_model=self.model.select(sel1)

  def get_remove_selection_string(self):
    # Now choose the worst side-chains or main chain to remove.
    # prioritize on shorter fragments and side chains first
    remove_residue_list=[]
    remove_side_chain_list=[]
    for [i,j,delta],threshold in zip(
       self.worst_non_bonded_pairs_and_delta+self.worst_bonded_pairs_and_delta,
        len(self.worst_non_bonded_pairs_and_delta)*
            [self.non_bonded_deviation_threshold]+
        len(self.worst_bonded_pairs_and_delta)*
            [self.bonded_deviation_threshold]):
      if delta > -threshold: continue

      i_is_shorter_fragment = \
         (self.segment_lengths[i] < self.segment_lengths[j])
      i_is_side_chain = not (self.is_main_chain_plus_cb(self.get_atomname(i)))
      j_is_side_chain = not (self.is_main_chain_plus_cb(self.get_atomname(j)))
      key_i=self.get_key(i)
      key_j=self.get_key(j)

      if (i_is_side_chain or j_is_side_chain):
        if not i_is_side_chain:  # take j
          if not key_j in remove_side_chain_list:remove_side_chain_list.append(
              key_j)
        elif not j_is_side_chain: #  take i
          if not key_i in remove_side_chain_list:remove_side_chain_list.append(
              key_i)
        elif i_is_shorter_fragment: # take i as shorter side chain
          if not key_i in remove_side_chain_list:remove_side_chain_list.append(
              key_i)
        else:
          if not key_j in remove_side_chain_list:remove_side_chain_list.append(
              key_j)
      else:  # neither is a side chain. Take shorter fragment
        if i_is_shorter_fragment: # take i whole residue
          if not key_i in remove_residue_list:remove_residue_list.append(key_i)
        else:
          if not key_j in remove_residue_list:remove_residue_list.append(key_j)

    # Take anything that is in remove_residue_list out of remove_side_chain_list
    new_remove_side_chain_list=[]
    for x in remove_side_chain_list:
      if not x in remove_residue_list:
        new_remove_side_chain_list.append(x)
    remove_side_chain_list=new_remove_side_chain_list
    self.side_chains_removed=len(remove_side_chain_list)
    self.residues_removed=len(remove_residue_list)

    remove_residue_selection_list=[]
    for x in remove_residue_list:
      chain_id,resseq=x.split(":")
      remove_residue_selection_list.append("(chain %s and resseq %s)" % (
         chain_id,resseq))
    if remove_residue_selection_list:
      remove_residue_selection_string=" ( %s ) " %( " OR ".join(
          remove_residue_selection_list))
    else:
      remove_residue_selection_string=""

    remove_side_chain_selection_list=[]
    for x in remove_side_chain_list:
      chain_id,resseq=x.split(":")
      remove_side_chain_selection_list.append(
        "chain %s and resseq %s" % (chain_id,resseq))
    if remove_side_chain_selection_list:
      side_chain_string=" ( NOT ( NAME %s ) )" % (
         " OR NAME ".join(main_chain_plus_cb))
      remove_side_chain_selection_string=" (%s AND ( %s  ) ) " %(
         side_chain_string, " OR ".join(remove_side_chain_selection_list))
    else:
      remove_side_chain_selection_string=""

    if remove_residue_selection_string and remove_side_chain_selection_string:
      self.remove_selection_string="%s OR %s" %(
        remove_residue_selection_string,remove_side_chain_selection_string)
    elif remove_residue_selection_string:
       self.remove_selection_string=remove_residue_selection_string
    else:
       self.remove_selection_string=remove_side_chain_selection_string

  def get_atomname(self,i):
    return self.model.get_atoms()[i].name

  def get_key(self,i):
      atom=self.model.get_atoms()[i]
      ag=atom.parent()
      rg=ag.parent()
      resseq_i=rg.resseq
      chain_id_i=rg.parent().id
      key_i="%s:%s" %(chain_id_i,resseq_i)
      return key_i

  def is_main_chain_plus_cb(self, atomname):
    if atomname.strip().upper() in main_chain_plus_cb:
      return True
    else:
      return False

  def get_bad_nonbonded(self,max_items=None):

    sites_cart=self.model.get_sites_cart()
    bad_nonbonded = []
    sorted_table, n_not_shown = self.pair_proxies.nonbonded_proxies.get_sorted(
        by_value="delta",
        sites_cart=sites_cart,
        site_labels=self.site_labels,
        max_items=max_items)
    for info in sorted_table:
      labels, i_seq, j_seq, delta, vdw_distance, sym_op_j, rt_mx = info
      if delta - vdw_distance  <= -self.non_bonded_deviation_threshold:
        bad_nonbonded.append([i_seq, j_seq, delta - vdw_distance])
    return bad_nonbonded

  def get_bad_bonded(self,max_items=None):

    # guess max items as percentage of sites_cart
    sites_cart=self.model.get_sites_cart()
    bad_bonded = []
    sorted_table, n_not_shown = self.pair_proxies.bond_proxies.get_sorted(
        by_value="residual",
        sites_cart=sites_cart,
        site_labels=self.site_labels,
        max_items=max_items,
        origin_id=origin_ids.get_origin_id('covalent geometry'))
    if sorted_table is not None:
      for restraint_info in sorted_table:
        (i_seq,j_seq,
            labels, distance_ideal, distance_model, slack, delta, sigma, weight,
            residual, sym_op_j, rt_mx) = restraint_info
        bad_bonded.append([i_seq,j_seq,delta])
    return bad_bonded


  def get_labels_and_proxies(self):
    # These are not really necessary
    self.site_labels = self.model.get_site_labels()

    self.pair_proxies = \
      self.model.get_restraints_manager().geometry.pair_proxies(
        sites_cart=self.model.get_sites_cart(),
        site_labels=self.site_labels)

    # Identify length of segments so we can prioritize on shorter segment
    self.get_segment_lengths()

  def get_segment_lengths(self):
    # Length of segment (residues) that each atom is in
    last_ca=None
    count_residues=0
    count_atoms=0
    self.segment_lengths=[]
    for atom in self.model.get_hierarchy().atoms():
      count_atoms+=1
      if atom.name.strip().upper() in ['CA','P']:
        count_residues+=1
        if last_ca is None or atom.distance(last_ca)< 4.5: #
           last_ca=atom
        else:
           self.segment_lengths+=count_atoms*[count_residues]  # count
           last_ca=None
           count_residues=0
           count_atoms=0
    if count_atoms:
      self.segment_lengths+=count_atoms*[count_residues]  # count
    assert len(self.segment_lengths) == self.model.get_number_of_atoms()


 *******************************************************************************


 *******************************************************************************
mmtbx/domains_from_pae.py
from __future__ import division, print_function
import sys
from scitbx.array_family import flex
from libtbx import group_args
from libtbx.utils import Sorry
from mmtbx.process_predicted_model import get_indices_as_ranges

##############################################################################
#################  domains_from_pae  by Tristan Croll#########################
##############################################################################
"""
This license applies to the routines: parse_pae_file,
domains_from_pae_matrix_networkx, domains_from_pae_matrix_igraph
in this file

These routines are from:
https://github.com/tristanic/pae_to_domains

MIT License

Copyright (c) 2021 Tristan Croll

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
"""


def parse_pae_file(pae_file = None, text = None):
    """Takes PAE file in PKL or JSON format and returns PAE matrix
     Alternative is text

    Arguments:
        *pae_file: file containing PAE matrix in PKL or JSON format
        text: text containing PAE matrix in JSON format

    Returns:
        pae_matrix as numpy array
    """
    import numpy
    import os

    if not text:
      _, ext = os.path.splitext(pae_file)

    if text or ext == '.json' or ext == '.jsn':

        import json
        if not text:
          try:
            with open(pae_file, 'rt') as f:
                data = json.load(f)
          except Exception:
            raise Sorry("Unable to read the json file: %s" %(pae_file))
        else:
          try:
            data = json.loads(text)
          except Exception:
            raise Sorry("Unable to read json text")

        if isinstance(data, dict) and 'pae' in data:
            # ColabFold 1.3 produces a JSON file different from AlphaFold database.
            matrix = numpy.array(data['pae'])
        elif isinstance(data, list):
            data = data[0]
            if 'residue1' in data.keys() and 'distance' in data.keys():
                # Support original JSON file format.
                r1, d = data['residue1'], data['distance']
                size = max(r1)
                matrix = numpy.empty((size, size))
                matrix.ravel()[:] = d
            elif 'predicted_aligned_error' in data.keys():
                # Support new AlphaFold database JSON file format.
                matrix = numpy.asarray(
                  data['predicted_aligned_error']).astype(float)
                matrix[matrix == 0] = 0.2
            else:
                raise Sorry("PAE data not detected in json file: %s" %(
                    pae_file))
        else:
            raise Sorry("Data in %s is not in a recognised format" %(
              pae_file))
    elif ext == '.pkl':
        # Support PKL format when running AlphaFold locally in ptm or multimer modes.
        import pickle as pkl
        data = []
        try:
            with open(pae_file, 'rb') as f_in:
                data.append(pkl.load(f_in))
        except Exception:
            raise Sorry("Unable to read the pkl file: %s" %(pae_file))

        try:
            matrix = numpy.asarray(data[0]['predicted_aligned_error'])
        except KeyError:
            raise Sorry("PAE data not detected in pkl file: %s" %(pae_file))
    else:
        raise Sorry("PAE file extension: '%s' not recognised" %(ext))


    return matrix


def domains_from_pae_matrix_networkx(pae_matrix, pae_power=1,
                                     pae_cutoff=5, graph_resolution=1, weight_by_ca_ca_distance=False,
                                     distance_power=1, distance_model=None):
    """
    Takes a predicted aligned error (PAE) matrix representing the predicted error in distances between each
    pair of residues in a model, and uses a graph-based community clustering algorithm to partition the model
    into approximately rigid groups.

    Arguments:

        * pae_matrix: a (n_residues x n_residues) numpy array. Diagonal elements should be set to some non-zero
          value to avoid divide-by-zero warnings
        * pae_power (optional, default=1): each edge in the graph will be weighted proportional to (1/pae**pae_power)
        * pae_cutoff (optional, default=5): graph edges will only be created for residue pairs with pae<pae_cutoff
        * graph_resolution (optional, default=1): regulates how aggressively the clustering algorithm is. Smaller values
          lead to larger clusters. Value should be larger than zero, and values larger than 5 are unlikely to be useful.
        * weight_by_ca_ca_distance (optional, default=False): adjust the edge weighting for each residue pair according
          to the distance between CA residues. If this is True, then `distance_model` must be provided.
        * distance_power (optional, default=1): If `weight_by_ca_ca_distance` is True, then edge weights will be multiplied
          by 1/distance**distance_power.
        * distance_model (optional, default=None): Model corresponding to the PAE
          matrix. Only needed if `weight_by_ca_ca_distances is True.
    Returns: a series of lists, where each list contains the indices of residues belonging to one cluster.
    """
    try:
        import networkx as nx
    except ImportError:
        raise Sorry(
            'ERROR: This method requires NetworkX (>=2.6.2) to be installed. Please install it using "pip install networkx" '
            'in a Python >=3.7 environment and try again.')
    import numpy
    weights = 1 / pae_matrix ** pae_power

    if weight_by_ca_ca_distance:
        if distance_model is None:
            raise Sorry('If weight_by_ca_ca_distance is True, distance_model must be provided!')
        weights *= weights_from_distance_matrix(distance_model, distance_power)

    g = nx.Graph()
    size = weights.shape[0]
    g.add_nodes_from(range(size))
    edges = numpy.argwhere(pae_matrix < pae_cutoff)
    sel_weights = weights[edges.T[0], edges.T[1]]
    wedges = [(i, j, w) for (i, j), w in zip(edges, sel_weights)]
    g.add_weighted_edges_from(wedges)

    from networkx.algorithms import community

    try:
      clusters = community.greedy_modularity_communities(g, weight='weight',
                                           resolution=graph_resolution)
      return clusters
    except Exception as e:
      pass # run again below without resolution

    # failed... try again without resolution
    try:
      clusters = community.greedy_modularity_communities(g, weight='weight')
      return clusters
    except Exception as e: #  failed
      pass # run again
    return None

def domains_from_pae_matrix_igraph(pae_matrix, pae_power=1, pae_cutoff=5,
                                   graph_resolution=1, weight_by_ca_ca_distance=False,
                                   distance_power=1, distance_model=None):
    """
    Takes a predicted aligned error (PAE) matrix representing the predicted error in distances between each
    pair of residues in a model, and uses a graph-based community clustering algorithm to partition the model
    into approximately rigid groups.

    Arguments:

        * pae_matrix: a (n_residues x n_residues) numpy array. Diagonal elements should be set to some non-zero
          value to avoid divide-by-zero warnings
        * pae_power (optional, default=1): each edge in the graph will be weighted proportional to (1/pae**pae_power)
        * pae_cutoff (optional, default=5): graph edges will only be created for residue pairs with pae<pae_cutoff
        * graph_resolution (optional, default=1): regulates how aggressively the clustering algorithm is. Smaller values
          lead to larger clusters. Value should be larger than zero, and values larger than 5 are unlikely to be useful.
        * weight_by_ca_ca_distance (optional, default=False): adjust the edge weighting for each residue pair according
          to the distance between CA residues. If this is True, then `distance_model` must be provided.
        * distance_power (optional, default=1): If `weight_by_ca_ca_distance` is True, then edge weights will be multiplied
          by 1/distance**distance_power.
        * distance_model (optional, default=None):  Model corresponding to the PAE
          matrix. Only needed if `weight_by_ca_ca_distances is True.

    Returns: a series of lists, where each list contains the indices of residues belonging to one cluster.
    """
    try:
        import igraph
    except ImportError:
        raise Sorry(
            'ERROR: This method requires python-igraph to be installed. Please install it using "pip install igraph" '
            'in a Python >=3.6 environment and try again.')
    import numpy
    weights = 1 / pae_matrix ** pae_power

    if weight_by_ca_ca_distance:
        if distance_model is None:
            raise Sorry('If weight_by_ca_ca_distance is True, distance_model must be provided!')
        weights *= weights_from_distance_matrix(distance_model, distance_power)

    g = igraph.Graph()
    size = weights.shape[0]
    g.add_vertices(range(size))
    edges = numpy.argwhere(pae_matrix < pae_cutoff)
    sel_weights = weights[edges.T[0], edges.T[1]]
    g.add_edges(edges)
    g.es['weight'] = sel_weights

    vc = g.community_leiden(weights='weight',
                            resolution_parameter=graph_resolution / 100, n_iterations=-1)
    membership = numpy.array(vc.membership)
    from collections import defaultdict
    clusters = defaultdict(list)
    for i, c in enumerate(membership):
        clusters[c].append(i)
    clusters = list(sorted(clusters.values(), key=lambda l: (len(l)), reverse=True))
    return clusters


##############################################################################
#################  end of domains_from_pae  by Tristan Croll##################
##############################################################################


def distance_matrix(sites_cart):
    from scitbx.matrix import col
    distances = flex.double()
    n = sites_cart.size()
    for i in range(n):
        diffs = (sites_cart - col(sites_cart[i])).norms()
        # Set diagonal terms to 1 to avoid divide-by-zero issues
        diffs[i] = 1
        distances.extend(diffs)
    flex_grid = flex.grid((n, n))
    distances.reshape(flex_grid)
    return distances


def flex_matrix_as_numpy_matrix(distances):
    import numpy
    shape = distances.all()
    matrix = numpy.empty(shape)
    matrix.ravel()[:] = list(distances.as_1d())
    return matrix


def weights_from_distance_matrix(distance_model, distance_power):
    flex_matrix = distance_matrix(
        distance_model.apply_selection_string("name CA").get_sites_cart()
    )
    numpy_matrix = flex_matrix_as_numpy_matrix(flex_matrix)
    return 1 / numpy_matrix ** distance_power


def cluster_as_selection(c, first_resno=None):
    # first_resno is residue number for selections corresponding to index of zero
    c = sorted(c)
    ranges = get_indices_as_ranges(c)
    selection_string = ""
    if first_resno is not None:
        offset = first_resno
    else:
        offset = 0
    for r in ranges:
        r_start = r.start + offset
        r_end = r.end + offset
        if not selection_string:
            selection_string = "(resseq %s:%s)" % (r_start, r_end)
        else:
            selection_string = "%s or (resseq %s:%s)" % (
                selection_string, r_start, r_end)
    return selection_string


def read_model(filename):
    from iotbx.data_manager import DataManager
    dm = DataManager()
    dm.set_overwrite(True)
    dm.process_model_file(filename)
    model = dm.get_model(filename)
    model.add_crystal_symmetry_if_necessary()
    return model


def get_domain_selections_from_pae_matrix(pae_matrix=None,
                                          pae_file=None,
                                          library=None,
                                          pae_power=None,
                                          pae_cutoff=None,
                                          graph_resolution=None,
                                          weight_by_ca_ca_distance=None,
                                          distance_power=None,
                                          distance_model=None,
                                          first_resno=None):
    if library is None:
        library = 'networkx'
    if library == 'networkx':
        f = domains_from_pae_matrix_networkx
    elif library == 'igraph':
        f = domains_from_pae_matrix_igraph
    else:
        raise Sorry('Unrecognised library "%s"!', library)

    # first_resno is residue number of residue with index of zero

    if pae_matrix is None:
        pae_matrix = parse_pae_file(pae_file)
    kwargs = {
        "pae_power": pae_power,
        "pae_cutoff": pae_cutoff,
        "graph_resolution": graph_resolution,
        "weight_by_ca_ca_distance": weight_by_ca_ca_distance,
        "distance_power": distance_power,
        "distance_model": distance_model
    }
    # Use default values when argument is None
    kwargs = {key: value for key, value in kwargs.items() if value is not None}
    clusters = f(pae_matrix, **kwargs)  # NOTE graph_resolution not used in 2.7

    new_clusters = []
    for c in clusters:
        new_clusters.append(sorted(c))
    clusters = sorted(new_clusters,
                      key=lambda c: c[0])
    selections = []
    for c in clusters:
        selections.append(cluster_as_selection(c, first_resno=first_resno))
    return selections


def write_pae_file(pae_matrix, file_name, range_to_keep=None):
    pae_as_list_of_lists = pae_matrix.tolist()
    if range_to_keep:
        n = range_to_keep.end + 1 - range_to_keep.start
        shape = (n, n)
        pae_1d = []
        for l in pae_as_list_of_lists[range_to_keep.start:range_to_keep.end + 1]:
            new_l = l[range_to_keep.start:range_to_keep.end + 1]
            pae_1d += new_l
    else:
        assert type(pae_matrix.tolist()[0][0]) == type(float(1))

        shape = tuple(pae_matrix.shape)

        # Flatten it out
        pae_1d = pae_matrix.flatten().tolist()

    # Read in to flex array
    flex_array = flex.float(pae_1d)

    from scitbx.array_family.flex import grid
    flex_grid = grid(shape)

    n, n = shape

    # Reshape the flex array
    flex_array.reshape(flex_grid)

    # Write out array to text file as json
    residues_1 = []
    residues_2 = []
    distances = []
    for i in range(n):
        ii = i + 1
        for j in range(n):
            jj = j + 1
            residues_1.append(ii)
            residues_2.append(jj)
            distances.append(float("%.2f" % (flex_array[i, j])))

    residue_dict = {"residue1": residues_1,
                    "residue2": residues_2,
                    "distance": distances,
                    "max_predicted_aligned_error": 0}
    values = [residue_dict]

    text = str(values).replace(" ", "").replace("'", '"')

    f = open(file_name, 'w')
    print(text, file=f)
    f.close()
    print("Wrote json file to %s" % (file_name))


if __name__ == '__main__':
    input_file_name = sys.argv[1]
    args = group_args(
        group_args_type='parameters',
        pae_file=input_file_name,
        library='networkx',
        pae_power=1.0,
        pae_cutoff=5.0,
        resolution=1.0,
        select_range=False)

    if args.select_range:
        range_to_keep = group_args(
            group_args_type='range to keep',
            start=283,  # starts with 0
            end=364,
        )
        pae_matrix = parse_pae_file(args.pae_file)
        write_pae_file(pae_matrix, "pae.json", range_to_keep=range_to_keep)
    else:
        pae_matrix = parse_pae_file(args.pae_file)
        selections = get_domain_selections_from_pae_matrix(
            pae_matrix=pae_matrix,
            pae_power=args.pae_power, pae_cutoff=args.pae_cutoff,
            graph_resolution=args.pae_graph_resolution, )
        print("Selections:")
        for s in selections:
            print(s)


 *******************************************************************************


 *******************************************************************************
mmtbx/f_model_info.py
from __future__ import absolute_import, division, print_function
from mmtbx.f_model.f_model_info import *

'''
This file is specifically to preserve backwards compatibility with earlier
versions of the Phenix GUI where the class for storing results in a pickle,
phenix.runtime.result_summary, has an import statement for this file.
Pickling an instance of result_summary stored the location of this file in the
cctbx hierarchy. The import statement in the result_summary class has been
moved outside of the class definition to prevent future issues. Moved to f_model.py
'''


 *******************************************************************************


 *******************************************************************************
mmtbx/find_peaks.py
from __future__ import absolute_import, division, print_function
from cctbx.array_family import flex
import sys
from libtbx import adopt_init_args
from cctbx import maptbx
import libtbx.phil
from libtbx.str_utils import format_value
from mmtbx.refinement import print_statistics

peak_search_params_str = """
peak_search_level = 1
  .type=int
max_peaks = 0
  .type=int
  .short_caption=Maximum peaks
interpolate = True
  .type=bool
min_distance_sym_equiv = None
  .type=float
  .short_caption=Minimum distance between symmetry-equivalent atoms
general_positions_only = False
  .type=bool
min_cross_distance = 1.8
  .type=float
  .short_caption=Minimum cross distance
min_cubicle_edge = 5
  .type=float
  .short_caption=Minimum edge length of cubicles used for \
    fast neighbor search
  .expert_level=2
"""

master_params = libtbx.phil.parse("""\
  use_sigma_scaled_maps = True
    .type=bool
    .help = Default is sigma scaled map, map in absolute scale is used \
            otherwise.
  grid_step = 0.6
    .type=float
    .help = Grid step for map sampling
  map_next_to_model
    .expert_level=2
    .style = noauto
  {
    min_model_peak_dist = 1.8
      .type=float
      .short_caption = Minimum distance from model
    max_model_peak_dist = 3.2
      .type=float
      .short_caption = Maximum distance from model
    min_peak_peak_dist = 1.8
      .type=float
      .short_caption = Minimum distance between peaks
    use_hydrogens = False
      .type = bool
      .short_caption = Use hydrogens
  }
  max_number_of_peaks = None
    .type=int
    .expert_level=1
  peak_search
    .expert_level=1
    .short_caption = Search settings
    .style = box auto_align
  {
    %s
  }
""" % peak_search_params_str)

class peaks_holder(object):
  def __init__(self, heights, sites, iseqs_of_closest_atoms = None):
    assert (len(heights) == len(sites))
    self.heights = heights
    self.sites = sites
    self.iseqs_of_closest_atoms = iseqs_of_closest_atoms

  def filter_by_secondary_map(self, map, min_value):
    n_deleted = 0
    k = 0
    while (k < len(self.sites)):
      map_value = map.tricubic_interpolation(self.sites[k])
      if (map_value < min_value):
        del self.sites[k]
        del self.heights[k]
        n_deleted += 1
      else :
        k += 1
    return n_deleted

  def sort(self, reverse=False):
    from scitbx.array_family import flex
    selection = flex.sort_permutation(self.heights, reverse=reverse)
    heights_sorted = self.heights.select(selection)
    sites_sorted = self.sites.select(selection)
    iseqs_sorted = None
    if (self.iseqs_of_closest_atoms is not None):
      iseqs_sorted = self.iseqs_of_closest_atoms.select(selection)
    self.heights = heights_sorted
    self.sites = sites_sorted
    self.iseqs_of_closest_atoms = iseqs_sorted

class manager(object):
  def __init__(self,
               map_cutoff,
               xray_structure,
               params = None,
               log = None,
               # Fourier map or...
               map_coeffs=None,
               # ...real-space map or...
               map_data = None,
               ):
    adopt_init_args(self, locals())
    # Make sure unique only-needed maps or map source is given
    assert [map_coeffs, map_data].count(None) == 1
    #
    self.mapped = False
    self.peaks_ = None
    if(self.log is None): self.log = sys.stdout
    if(self.params is None): self.params = master_params.extract()
    #
    crystal_symmetry = xray_structure.crystal_symmetry()
    crystal_gridding = maptbx.crystal_gridding(
      unit_cell        = crystal_symmetry.unit_cell(),
      space_group_info = crystal_symmetry.space_group_info(),
      symmetry_flags   = maptbx.use_space_group_symmetry,
      step             = self.params.grid_step)
    crystal_gridding_tags = maptbx.crystal_gridding_tags(
      gridding = crystal_gridding)
    #
    if(map_coeffs is not None):
      fft_map = map_coeffs.fft_map(
        crystal_gridding = crystal_gridding,
        symmetry_flags=maptbx.use_space_group_symmetry)
      if(self.params.use_sigma_scaled_maps):
        fft_map.apply_sigma_scaling()
      else:
        fft_map.apply_volume_scaling()
      fft_map_data = fft_map.real_map_unpadded()
    elif(map_data is not None):
      fft_map_data = map_data
    #
    max_number_of_peaks = self.params.max_number_of_peaks
    if(self.params.max_number_of_peaks is None):
      max_number_of_peaks = self.xray_structure.scatterers().size() * 5
    negative = False
    if(self.map_cutoff < 0):
      self.map_cutoff *= -1
      negative = True
      fft_map_data *= -1
    min_distance_sym_equiv = self.params.peak_search.min_distance_sym_equiv
    if(min_distance_sym_equiv is None):
      min_distance_sym_equiv = self.xray_structure.min_distance_sym_equiv()
    peak_search_parameters = maptbx.peak_search_parameters(
      peak_search_level      = self.params.peak_search.peak_search_level,
      max_peaks              = self.params.peak_search.max_peaks,
      peak_cutoff            = self.map_cutoff,
      interpolate            = self.params.peak_search.interpolate,
      min_distance_sym_equiv = min_distance_sym_equiv,
      general_positions_only = self.params.peak_search.general_positions_only,
      min_cross_distance     = self.params.peak_search.min_cross_distance,
      min_cubicle_edge       = self.params.peak_search.min_cubicle_edge)
    cluster_analysis = crystal_gridding_tags.peak_search(
      parameters = peak_search_parameters,
      map = fft_map_data).all(max_clusters = max_number_of_peaks)
    heights = cluster_analysis.heights()
    if(negative):
      heights *= -1.
    self.peaks_ = peaks_holder(heights = heights,
                               sites   = cluster_analysis.sites())
    if(self.log is not None):
      print("Number of peaks found (map cutoff=%s)= %s"%(
        format_value("%-5.2f", self.map_cutoff).strip(),
        format_value("%-12d", self.peaks_.sites.size())), file=self.log)

  def peaks(self):
    return self.peaks_

  def peaks_mapped(self):
    if(self.peaks_ is None): return None
    assert self.mapped == False
    max_dist = self.params.map_next_to_model.max_model_peak_dist
    min_dist = self.params.map_next_to_model.min_model_peak_dist
    if (min_dist is None):
      min_dist = 0.
    if (max_dist is None):
      max_dist = float(sys.maxsize)
    xray_structure = self.xray_structure.deep_copy_scatterers()
    use_selection = None
    if(not self.params.map_next_to_model.use_hydrogens):
      use_selection = ~xray_structure.hd_selection()
    initial_number_of_sites = self.peaks_.sites.size()
    if(self.log is not None):
      print("Filter by distance & map next to the model:", file=self.log)
    result = xray_structure.closest_distances(sites_frac = self.peaks_.sites,
      distance_cutoff = max_dist, use_selection = use_selection)
    smallest_distances_sq = result.smallest_distances_sq
    smallest_distances = result.smallest_distances
    in_box = smallest_distances_sq > 0
    not_too_far = smallest_distances_sq <= max_dist**2
    not_too_close = smallest_distances_sq >= min_dist**2
    selection = (not_too_far & not_too_close & in_box)
    iseqs_of_closest_atoms = result.i_seqs.select(selection)
    peaks = peaks_holder(
      heights                = self.peaks_.heights.select(selection),
      sites                  = result.sites_frac.select(selection),
      iseqs_of_closest_atoms = iseqs_of_closest_atoms)
    sd = flex.sqrt(smallest_distances_sq.select(in_box))
    d_min = flex.min_default(sd, 0)
    d_max = flex.max_default(sd, 0)
    if(self.log is not None):
      print("   mapped sites are within: %5.3f - %5.3f"%(d_min,d_max), file=self.log)
      print("   number of sites selected in [dist_min=%5.2f, " \
        "dist_max=%5.2f]: %d from: %d" % (min_dist, max_dist, peaks.sites.size(),
        initial_number_of_sites), file=self.log)
    smallest_distances = flex.sqrt(smallest_distances_sq.select(selection))
    d_min = flex.min_default(smallest_distances, 0)
    d_max = flex.max_default(smallest_distances, 0)
    if(self.log is not None):
      print("   mapped sites are within: %5.3f - %5.3f"%(d_min,d_max), file=self.log)
    self.mapped = True
    self.peaks_ = peaks
    return peaks

  def show_mapped(self, pdb_atoms):
    if(self.peaks_ is None): return None
    peaks = self.peaks()
    if(peaks.iseqs_of_closest_atoms is None):
      raise RuntimeError("iseqs_of_closest_atoms is None")
    scatterers = self.xray_structure.scatterers()
    assert scatterers.size() == pdb_atoms.size()
    assert peaks.sites.size() == peaks.heights.size()
    assert peaks.heights.size() == peaks.iseqs_of_closest_atoms.size()
    print(file=self.log)
    dist = self.xray_structure.unit_cell().distance
    for i in flex.sort_permutation(data=peaks.iseqs_of_closest_atoms):
      s = peaks.sites[i]
      h = peaks.heights[i]
      i_seq = peaks.iseqs_of_closest_atoms[i]
      sc = scatterers[i_seq]
      d = dist(s, sc.site)
      element = sc.element_symbol()
      print("peak= %8.3f closest distance to %s = %8.3f" % (
        h, pdb_atoms[i_seq].id_str(), d), file=self.log)
      assert d <= self.params.map_next_to_model.max_model_peak_dist
      assert d >= self.params.map_next_to_model.min_model_peak_dist

def show_highest_peaks_and_deepest_holes(fmodel,
                                         pdb_atoms,
                                         map_type,
                                         map_cutoff_plus,
                                         map_cutoff_minus,
                                         log = None):
  if(log is None): log = sys.stdout
  print_statistics.make_header(
    "residual map %s: highest peaks and deepst holes"%map_type, out = log)
  fp_params = master_params.extract()
  fp_params.map_next_to_model.min_model_peak_dist = 0.7
  fp_params.map_next_to_model.min_peak_peak_dist = 0.7
  fp_params.peak_search.min_cross_distance = 0.7
  fp_params.map_next_to_model.use_hydrogens = True
  for par in [(map_cutoff_plus,"peaks"), (map_cutoff_minus,"holes")]:
    print_statistics.make_sub_header(par[1], out = log)
    #
    from cctbx import maptbx
    e_map = fmodel.electron_density_map()
    crystal_symmetry = fmodel.xray_structure.crystal_symmetry()
    crystal_gridding = maptbx.crystal_gridding(
      unit_cell        = crystal_symmetry.unit_cell(),
      space_group_info = crystal_symmetry.space_group_info(),
      symmetry_flags   = maptbx.use_space_group_symmetry,
      step             = 0.6)
    coeffs = e_map.map_coefficients(
      map_type     = "mFobs-DFmodel",
      fill_missing = False,
      isotropize   = True)
    fft_map = coeffs.fft_map(crystal_gridding = crystal_gridding)
    fft_map.apply_sigma_scaling()
    map_data = fft_map.real_map_unpadded()
    #

    result = manager(map_data = map_data,
                     xray_structure = fmodel.xray_structure,
                     map_cutoff = par[0],
                     params     = fp_params,
                     log        = log)
    result.peaks_mapped()
    result.show_mapped(pdb_atoms = pdb_atoms)


 *******************************************************************************


 *******************************************************************************
mmtbx/grow_density.py
from __future__ import absolute_import, division, print_function
from cctbx.array_family import flex
import mmtbx.f_model
from mmtbx import utils
from iotbx import reflection_file_reader
from iotbx import reflection_file_utils
from six.moves import cStringIO as StringIO
import iotbx.phil
from iotbx import crystal_symmetry_from_any
from cctbx import adptbx
from libtbx.utils import Sorry
import os, math
from libtbx import group_args
from iotbx import pdb
from cctbx import xray
import mmtbx.refinement.minimization
import scitbx.lbfgs
from mmtbx import maps
import mmtbx.masks
from six.moves import zip
from six.moves import range
from iotbx import extract_xtal_data


master_params_str = """\
pdb_file_name = None
  .type = str
  .multiple = False
  .help = PDB file name (model only).
external_da_pdb_file_name = None
  .type = str
  .multiple = False
  .help = PDB file name (dummy atoms only).
reflection_file_name = None
  .type = str
  .help = File with experimental data (most of formats: CNS, SHELX, MTZ, etc).
data_labels = None
  .type = str
  .help = Labels for experimental data.
refine = *occupancies *adp *sites
  .type = choice(multi=True)
  .help = Parameters of DA to refine.
stop_reset_occupancies_at_macro_cycle = 5
  .type = int
  .help = Reset occupancies during first stop_reset_occupancies_at_macro_cycle \
          macro-cycles.
stop_reset_adp_at_macro_cycle = 5
  .type = int
  .help = Reset ADPs during first stop_reset_adp_at_macro_cycle macro-cycles.
start_filtering_at_macro_cycle=10
  .type = int
  .help = Macro-cycle number at which filtering takes off.
sphere
  .multiple=True
  {
    center = None
      .type = floats(size=3)
      .help = Approximate coordinates of the center of problem density.
    radius = None
      .type = float
      .help = Sphere radius of where the dummy atoms will be placed.
  }
mode = build *build_and_refine
  .type = choice(multi=False)
high_resolution = None
  .type = float
low_resolution = None
  .type = float
output_file_name_prefix = None
  .type = str
initial_occupancy = 0.1
  .type = float
  .help = Starting occupancy value for a newly placed DA
initial_b_factor = None
  .type = float
  .help = Starting B-factor value for a newly palced DA. Of None, then \
          determined automatically as mean B.
atom_gap = 0.7
  .type = float
  .help = Thje gap between the atoms in the grid.
  .expert_level = 2
overlap_interval = 0.25
  .type = float
  .help = Grid interval size - the gap between two grids.
  .expert_level = 2
atom_type = H
  .type = str
  .help = Atom to use to make grid. Nitrogen is a good choice
atom_name = " DA "
  .type = str
residue_name = DUM
  .type = str
  .help = Residue name for a DA
scattering_table = *n_gaussian wk1995 it1992 neutron
  .type = choice
  .help = Scattering table for structure factors calculations
number_of_refinement_cycles = 20
  .type = int
  .help = Number of refinement cycles
number_of_minimization_iterations = 25
  .type = int
  .help = Number of refinement iterations
filter
  {
    b_iso_min = 1.0
      .type = float
      .help = Min B-factor value to remove DA.
    b_iso_max = 100.0
      .type = float
      .help = Max B-factor value to remove DA.
    occupancy_min = 0.1
      .type = float
      .help = Min occupancy value to remove DA.
    occupancy_max = 10.0
      .type = float
      .help = Max occupancy value to remove DA.
    inside_sphere_scale = 1.2
      .type = float
      .help = DA with distance larger than sphere.radius*inside_sphere_scale \
              from sphere.center will be removed.
    da_model_min_dist = 1.
      .type = float
      .help = Max allowable distance between DA and any model atom.
  }
"""

def master_params():
  return iotbx.phil.parse(master_params_str, process_includes=False)

def create_da_xray_structures(xray_structure, params):
  def grid(sphere, gap, overlap):
    c = flex.double(sphere.center)
    x_start, y_start, z_start = c - float(sphere.radius)
    x_end, y_end, z_end       = c + float(sphere.radius)
    x_range = frange(c[0], c[0]+gap, overlap)
    y_range = frange(c[1], c[1]+gap, overlap)
    z_range = frange(c[2], c[2]+gap, overlap)
    return group_args(x_range = x_range, y_range = y_range, z_range = z_range)
  grids = []
  for sphere in params.sphere:
    grids.append(grid(sphere = sphere, gap = params.atom_gap,
      overlap = params.overlap_interval))
  initial_b_factor = params.initial_b_factor
  if(initial_b_factor is None):
    initial_b_factor = flex.mean(
      xray_structure.extract_u_iso_or_u_equiv()*adptbx.u_as_b(1.))
  da_xray_structures = []
  counter = 0
  for grid, sphere in zip(grids, params.sphere):
    cntr_g = 0 # XXX
    for x_start in grid.x_range:
      for y_start in grid.y_range:
        for z_start in grid.z_range:
          cntr_g += 1
          if(cntr_g>1): continue # XXX
          counter += 1
          new_center = flex.double([x_start, y_start, z_start])
          atom_grid = make_grid(
            center          = new_center,
            radius          = sphere.radius,
            gap             = params.atom_gap,
            occupancy       = params.initial_occupancy,
            b_factor        = initial_b_factor,
            atom_name       = params.atom_name,
            scattering_type = params.atom_type,
            resname         = params.residue_name)
          da_xray_structure = pdb_atoms_as_xray_structure(pdb_atoms = atom_grid,
            crystal_symmetry = xray_structure.crystal_symmetry())
          closest_distances_result = xray_structure.closest_distances(
            sites_frac      = da_xray_structure.sites_frac(),
            distance_cutoff = 5)
          selection = closest_distances_result.smallest_distances > 0
          selection &= closest_distances_result.smallest_distances < 1
          da_xray_structure = da_xray_structure.select(~selection)
          #print counter, da_xray_structure.scatterers().size()
          if(cntr_g==1): # XXX
            da_xray_structures.append(da_xray_structure)
  ###
  result = []
  for i, x1 in enumerate(da_xray_structures):
    for j, x2 in enumerate(da_xray_structures):
      if(x1 is not x2):
        closest_distances_result = x1.closest_distances(
          sites_frac      = x2.sites_frac(),
          distance_cutoff = 5) # XXX ???
        selection = closest_distances_result.smallest_distances > 0
        selection &= closest_distances_result.smallest_distances < params.atom_gap
        da_xray_structures[j] = x2.select(~selection)
  return da_xray_structures

def grow_density(f_obs,
                 r_free_flags,
                 xray_structure,
                 xray_structure_da,
                 params):
    if(xray_structure_da is not None):
      print("Using external DA...")
      da_xray_structures = [xray_structure_da]
    else:
      print("Start creating DAs...")
      da_xray_structures = create_da_xray_structures(xray_structure =
        xray_structure, params = params)
    n_da = 0
    for daxrs in da_xray_structures:
      n_da += daxrs.scatterers().size()
    print("Total number of dummy atoms:", n_da)
    #
    xray_structure_start = xray_structure.deep_copy_scatterers()
    #
    if(params.mode == "build_and_refine"):
      mask_params = mmtbx.masks.mask_master_params.extract()
      mask_params.ignore_hydrogens=True
      fmodel = mmtbx.f_model.manager(
        xray_structure = xray_structure_start,
        r_free_flags   = r_free_flags,
        target_name    = "ml",
        mask_params    = mask_params,
        f_obs          = f_obs)
      fmodel.update_all_scales()
      print("START R-work and R-free: %6.4f %6.4f"%(fmodel.r_work(),
        fmodel.r_free()))
    xray_structure_current = xray_structure_start.deep_copy_scatterers()
    da_sel_all = flex.bool(xray_structure_start.scatterers().size(), False)
    for i_model, da_xray_structure in enumerate(da_xray_structures):
      print("Model %d, adding %d dummy atoms" % (i_model,
        da_xray_structure.scatterers().size()))
      da_sel_refinable = flex.bool(xray_structure_current.scatterers().size(), False)
      xray_structure_current = xray_structure_current.concatenate(da_xray_structure)
      da_sel_all.extend(flex.bool(da_xray_structure.scatterers().size(), True))
      da_sel_refinable.extend(flex.bool(da_xray_structure.scatterers().size(), True))
      if(params.mode == "build_and_refine"):
        fmodel.update_xray_structure(update_f_calc=True, update_f_mask=False,
          xray_structure = xray_structure_current)
        #XXX print fmodel.r_work()
        #XXX assert 0 # update_f_mask=True would not work
        da_sel_all = refine_da(
          fmodel           = fmodel,
          selection        = da_sel_all,
          da_sel_refinable = da_sel_all,#XXXda_sel_refinable,
          params           = params)
        xray_structure_current = fmodel.xray_structure
    da_sel_all = flex.bool(xray_structure_start.scatterers().size(), False)
    da_sel_all.extend(flex.bool(xray_structure_current.scatterers().size()-
                            xray_structure_start.scatterers().size(), True))
    all_da_xray_structure = xray_structure_current.select(da_sel_all)
    ofn = params.output_file_name_prefix
    if(ofn is None):
      ofn = "DA.pdb"
    else:
      ofn = ofn+"_DA.pdb"
    ofn = open(ofn,"w")
    pdb_file_str = all_da_xray_structure.as_pdb_file()
    print(pdb_file_str, file=ofn)
    #
    if(params.mode == "build_and_refine"):
      map_type_obj = mmtbx.map_names(map_name_string = "2mFo-DFc")
      map_params = maps.map_and_map_coeff_master_params().fetch(
        maps.cast_map_coeff_params(map_type_obj)).extract()
      coeffs = maps.map_coefficients_from_fmodel(fmodel = fmodel,
        params = map_params.map_coefficients[0])
      lbl_mgr = maps.map_coeffs_mtz_label_manager(map_params =
        map_params.map_coefficients[0])
      mtz_dataset = coeffs.as_mtz_dataset(
        column_root_label = lbl_mgr.amplitudes(),
        label_decorator   = lbl_mgr)
      #
      f_calc_da = coeffs.structure_factors_from_scatterers(xray_structure =
        all_da_xray_structure).f_calc()
      mtz_dataset.add_miller_array(
        miller_array      = f_calc_da,
        column_root_label = "Fcalc_da")
      #
      mtz_object = mtz_dataset.mtz_object()
      output_map_file_name = "map_coeffs.mtz"
      if(params.output_file_name_prefix is not None):
        output_map_file_name = params.output_file_name_prefix+"_map_coeffs.mtz"
      mtz_object.write(file_name = output_map_file_name)
    print("Finished")

def refinery(fmodels, number_of_iterations, iselection, parameter):
  if(iselection.size()==0): return
  fmodels.fmodel_xray().xray_structure.scatterers().flags_set_grads(state=False)
  lbfgs_termination_params = scitbx.lbfgs.termination_parameters(
    max_iterations = number_of_iterations)
  if(parameter == "occupancies"):
    fmodels.fmodel_xray().xray_structure.scatterers().flags_set_grad_occupancy(
      iselection = iselection)
  if(parameter == "adp"):
    fmodels.fmodel_xray().xray_structure.scatterers().flags_set_grad_u_iso(
      iselection = iselection)
  if(parameter == "sites"):
    fmodels.fmodel_xray().xray_structure.scatterers().flags_set_grad_site(
      iselection = iselection)
  try:
    minimized = mmtbx.refinement.minimization.lbfgs(
      fmodels                  = fmodels,
      lbfgs_termination_params = lbfgs_termination_params,
      collect_monitor          = False)
  except Exception as e:
    print("ERROR: Refinement failed... ")
    print(str(e))
    print("... carry on")
    return
  xrs = fmodels.fmodel_xray().xray_structure
  fmodels.update_xray_structure(xray_structure = xrs, update_f_calc=True,
    update_f_mask=False)
  assert minimized.xray_structure is fmodels.fmodel_xray().xray_structure
  fmodels.fmodel_xray().xray_structure.scatterers().flags_set_grads(state=False)

def reset_occupancies(fmodels, selection, params):
  xrs = fmodels.fmodel_xray().xray_structure
  occ = xrs.scatterers().extract_occupancies()
  sel = occ < params.filter.occupancy_min
  sel &= selection
  occ = occ.set_selected(sel, params.filter.occupancy_min)
  sel = occ > params.filter.occupancy_max
  sel &= selection
  occ = occ.set_selected(sel, params.filter.occupancy_max)
  xrs.set_occupancies(occ)
  fmodels.update_xray_structure(xray_structure = xrs, update_f_calc=True,
    update_f_mask=False)

def reset_adps(fmodels, selection, params):
  xrs = fmodels.fmodel_xray().xray_structure
  b = xrs.extract_u_iso_or_u_equiv()*adptbx.u_as_b(1.)
  sel = b > params.filter.b_iso_max
  sel &= selection
  b = b.set_selected(sel, params.filter.b_iso_max)
  sel = b < params.filter.b_iso_min
  sel &= selection
  b = b.set_selected(sel, params.filter.b_iso_min)
  xrs = xrs.set_b_iso(values=b)
  fmodels.update_xray_structure(xray_structure = xrs, update_f_calc=True,
    update_f_mask=False)

def show_refinement_update(fmodels, selection, da_sel_refinable, prefix):
  fmt1 = "%s Rwork= %8.6f Rfree= %8.6f Number of: non-DA= %d DA= %d all= %d"
  print(fmt1%(prefix, fmodels.fmodel_xray().r_work(),
    fmodels.fmodel_xray().r_free(),
    selection.count(False),selection.count(True),
    fmodels.fmodel_xray().xray_structure.scatterers().size()))
  occ = fmodels.fmodel_xray().xray_structure.scatterers().extract_occupancies()
  occ_da = occ.select(selection)
  if(occ_da.size()>0):
    occ_ma = occ.select(~selection)
    print("         non-da: occ(min,max,mean)= %6.3f %6.3f %6.3f"%(
      flex.min(occ_ma),flex.max(occ_ma),flex.mean(occ_ma)))
    print("             da: occ(min,max,mean)= %6.3f %6.3f %6.3f"%(
      flex.min(occ_da),flex.max(occ_da),flex.mean(occ_da)))
    b = fmodels.fmodel_xray().xray_structure.extract_u_iso_or_u_equiv()*\
      adptbx.u_as_b(1.)
    b_da = b.select(selection)
    b_ma = b.select(~selection)
    print("         non-da: ADP(min,max,mean)= %7.2f %7.2f %7.2f"%(
      flex.min(b_ma),flex.max(b_ma),flex.mean(b_ma)))
    print("             da: ADP(min,max,mean)= %7.2f %7.2f %7.2f"%(
      flex.min(b_da),flex.max(b_da),flex.mean(b_da)))
    print("da_sel_refinable:", da_sel_refinable.size(), da_sel_refinable.count(True))

def refine_occupancies(fmodels, selection, da_sel_refinable, params, macro_cycle):
  refinery(
    fmodels              = fmodels,
    number_of_iterations = params.number_of_minimization_iterations,
    iselection           = da_sel_refinable.iselection(),
    parameter            = "occupancies")
  if(params.stop_reset_occupancies_at_macro_cycle > macro_cycle):
    reset_occupancies(fmodels = fmodels, selection = selection, params = params)
  if(params.start_filtering_at_macro_cycle <= macro_cycle):
    fmodels, selection, da_sel_refinable = filter_da(fmodels, selection, da_sel_refinable, params)
  show_refinement_update(fmodels, selection, da_sel_refinable, "occ(%2d):"%macro_cycle)
  return fmodels, selection, da_sel_refinable

def refine_adp(fmodels, selection, da_sel_refinable, params, macro_cycle):
  refinery(
    fmodels              = fmodels,
    number_of_iterations = params.number_of_minimization_iterations,
    iselection           = da_sel_refinable.iselection(),
    parameter            = "adp")
  if(params.stop_reset_adp_at_macro_cycle > macro_cycle):
    reset_adps(fmodels = fmodels, selection = selection, params = params)
  if(params.start_filtering_at_macro_cycle <= macro_cycle):
    fmodels, selection, da_sel_refinable = filter_da(fmodels, selection, da_sel_refinable, params)
  show_refinement_update(fmodels, selection, da_sel_refinable, "adp(%2d):"%macro_cycle)
  return fmodels, selection, da_sel_refinable

def refine_sites(fmodels, selection, da_sel_refinable, params, macro_cycle):
  refinery(
    fmodels              = fmodels,
    number_of_iterations = params.number_of_minimization_iterations,
    iselection           = da_sel_refinable.iselection(),
    parameter            = "sites")
  if(params.start_filtering_at_macro_cycle <= macro_cycle):
    fmodels, selection, da_sel_refinable = filter_da(fmodels, selection, da_sel_refinable, params)
  show_refinement_update(fmodels, selection, da_sel_refinable, "xyz(%2d):"%macro_cycle)
  return fmodels, selection, da_sel_refinable

def filter_da(fmodels, selection, da_sel_refinable, params):
  xrs = fmodels.fmodel_xray().xray_structure
  xrs_d = xrs.select(selection)
  xrs_m = xrs.select(~selection)
  #
  occ = xrs_d.scatterers().extract_occupancies()
  adp = xrs_d.extract_u_iso_or_u_equiv()*adptbx.u_as_b(1.)
  sel  = occ < params.filter.occupancy_max
  sel &= occ > params.filter.occupancy_min
  sel &= adp < params.filter.b_iso_max
  sel &= adp > params.filter.b_iso_min
  uc = xrs_d.unit_cell()
  sphere_defined = True
  if(len(params.sphere)==0): sphere_defined = False
  for sphere in params.sphere:
    if([sphere.center, sphere.radius].count(None) != 0): sphere_defined = False
  for i_seq, site_frac_d in enumerate(xrs_d.sites_frac()):
    if(sphere_defined):
      inside = False
      for sphere in params.sphere:
        if([sphere.center, sphere.radius].count(None)==0):
          dist = uc.distance(site_frac_d, uc.fractionalize(sphere.center))
          if(dist <= sphere.radius*params.filter.inside_sphere_scale):
            inside = True
            break
      if(not inside): sel[i_seq] = False
    for site_frac_m in xrs_m.sites_frac():
      dist = uc.distance(site_frac_d, site_frac_m)
      if(dist < params.filter.da_model_min_dist):
        sel[i_seq] = False
        break
  #
  sel_all = flex.bool(xrs_m.scatterers().size(),True)
  sel_all.extend(sel)
  da_sel_refinable = da_sel_refinable.select(sel_all)
  #
  xrs_d = xrs_d.select(sel)
  xrs = xrs_m.concatenate(xrs_d)
  selection = flex.bool(xrs_m.scatterers().size(), False)
  selection.extend(flex.bool(xrs_d.scatterers().size(), True))
  fmodels.update_xray_structure(xray_structure = xrs, update_f_calc=True,
    update_f_mask=False)
  assert selection.size() == da_sel_refinable.size()
  #XXXX
  da_sel_refinable = selection
  #XXXX
  return fmodels, selection, da_sel_refinable

def refine_da(fmodel, selection, params, da_sel_refinable):
  fmodels = mmtbx.fmodels(fmodel_xray = fmodel)
  show_refinement_update(fmodels, selection, da_sel_refinable, "  START:")
  nrm = params.number_of_refinement_cycles
  assert nrm >= params.stop_reset_occupancies_at_macro_cycle
  assert nrm >= params.stop_reset_adp_at_macro_cycle
  assert nrm >= params.start_filtering_at_macro_cycle
  for macro_cycle in range(nrm):
    if("occupancies" in params.refine):
      fmodels, selection, da_sel_refinable = refine_occupancies(
        fmodels     = fmodels,
        selection   = selection,
        da_sel_refinable = da_sel_refinable,
        params      = params,
        macro_cycle = macro_cycle)
    if("adp" in params.refine):
      fmodels, selection, da_sel_refinable = refine_adp(
        fmodels     = fmodels,
        selection   = selection,
        da_sel_refinable = da_sel_refinable,
        params      = params,
        macro_cycle = macro_cycle)
    if("sites" in params.refine):
      fmodels, selection, da_sel_refinable = refine_sites(
        fmodels     = fmodels,
        selection   = selection,
        da_sel_refinable = da_sel_refinable,
        params      = params,
        macro_cycle = macro_cycle)
    assert fmodel.xray_structure is fmodels.fmodel_xray().xray_structure
  return selection

def pdb_atoms_as_xray_structure(pdb_atoms, crystal_symmetry):
  xray_structure = xray.structure(crystal_symmetry = crystal_symmetry)
  unit_cell = xray_structure.unit_cell()
  for atom in pdb_atoms:
    scatterer = xray.scatterer(
      label           = atom.name,
      site            = unit_cell.fractionalize(atom.xyz),
      b               = atom.b,
      occupancy       = atom.occ,
      scattering_type = atom.element)
    xray_structure.add_scatterer(scatterer)
  return xray_structure

def make_grid(center, radius, gap, occupancy, b_factor, atom_name,
              scattering_type, resname):
  x_start, y_start, z_start = center - float(radius)
  x_end, y_end, z_end       = center + float(radius)
  x_range = frange(x_start, x_end, gap)
  y_range = frange(y_start, y_end, gap)
  z_range = frange(z_start, z_end, gap)
  result = []
  counter = 1
  for x in x_range:
    for y in y_range:
      for z in z_range:
        d = math.sqrt((x-center[0])**2+(y-center[1])**2+(z-center[2])**2)
        if(d < radius):
          atom = iotbx.pdb.hierarchy.atom_with_labels()
          atom.serial  = counter
          atom.name    = atom_name
          atom.resname = resname
          atom.resseq  = counter
          atom.xyz     = (x,y,z)
          atom.occ     = occupancy
          atom.b       = b_factor
          atom.element = scattering_type
          result.append(atom)
          counter += 1
  return result

def frange(start, end=None, inc=None):
    if end == None:
        end = start + 0.0
        start = 0.0
    if inc == None:
        inc = 1.0
    L = []
    while 1:
        next = start + len(L) * inc
        if inc > 0 and next >= end:
            break
        elif inc < 0 and next <= end:
            break
        L.append(next)
    return L

def reflection_file_server(crystal_symmetry, reflection_files):
  return reflection_file_utils.reflection_file_server(
    crystal_symmetry=crystal_symmetry,
    force_symmetry=True,
    reflection_files=reflection_files,
    err=StringIO())

def cmd_run(args, command_name):
  msg = """\

Tool for local improvement of electron density map.

How to use:
1: Run this command: phenix.grow_density
2: Copy, save into a file and edit the parameters shown between the
   lines *** below. Do not include *** lines.
3: Run the command with this parameters file:
   phenix.grow_density parameters.txt
"""
  if(len(args) == 0):
    print(msg)
    print("*"*79)
    master_params().show()
    print("*"*79)
    return
  else:
    if(not os.path.isfile(args[0]) or len(args)>1):
      print("Parameter file is expected at input. This is not a parameter file:\n", \
        args)
      print("Run phenix.grow_density without argumets for running instructions.")
      return
    processed_args = utils.process_command_line_args(args = args,
      master_params = master_params(), log = None)
    params = processed_args.params.extract()
    if(params.pdb_file_name is None):
      assert len(processed_args.pdb_file_names)==1
      params.pdb_file_name = processed_args.pdb_file_names[0]
    run(processed_args = processed_args, params = params)

def run(processed_args, params):
  if(params.scattering_table not in ["n_gaussian","wk1995",
     "it1992","neutron"]):
    raise Sorry("Incorrect scattering_table.")
  crystal_symmetry = None
  crystal_symmetries = []
  for f in [str(params.pdb_file_name), str(params.reflection_file_name)]:
    cs = crystal_symmetry_from_any.extract_from(f)
    if(cs is not None): crystal_symmetries.append(cs)
  if(len(crystal_symmetries) == 1): crystal_symmetry = crystal_symmetries[0]
  elif(len(crystal_symmetries) == 0):
    raise Sorry("No crystal symmetry found.")
  else:
    if(not crystal_symmetries[0].is_similar_symmetry(crystal_symmetries[1])):
      raise Sorry("Crystal symmetry mismatch between different files.")
    crystal_symmetry = crystal_symmetries[0]
  f_obs = None
  r_free_flags = None
  if(params.reflection_file_name is not None):
    reflection_file = reflection_file_reader.any_reflection_file(
      file_name = params.reflection_file_name, ensure_read_access = True)
    rfs = reflection_file_server(
      crystal_symmetry = crystal_symmetry,
      reflection_files = [reflection_file])
    parameters = extract_xtal_data.data_and_flags_master_params().extract()
    if(params.data_labels is not None):
      parameters.labels = [processed_args.data_labels]
    determine_data_and_flags_result = extract_xtal_data.run(
      reflection_file_server = rfs,
      parameters             = parameters,
      keep_going             = True)
    f_obs = determine_data_and_flags_result.f_obs
    r_free_flags = determine_data_and_flags_result.r_free_flags
    if(r_free_flags is None):
      r_free_flags=f_obs.array(data=flex.bool(f_obs.data().size(), False))
      test_flag_value=None
  xray_structure = iotbx.pdb.input(file_name =
    params.pdb_file_name).xray_structure_simple()
  xray_structure_da = None
  if(params.external_da_pdb_file_name is not None):
    xray_structure_da = iotbx.pdb.input(file_name =
      params.external_da_pdb_file_name).xray_structure_simple()
  if(f_obs is not None):
    f_obs = f_obs.resolution_filter(d_min = params.high_resolution,
      d_max = params.low_resolution)
    r_free_flags = r_free_flags.resolution_filter(d_min = params.high_resolution,
      d_max = params.low_resolution)
  #
  assert params.mode in ["build", "build_and_refine"]
  grow_density(f_obs             = f_obs,
               r_free_flags      = r_free_flags,
               xray_structure    = xray_structure,
               xray_structure_da = xray_structure_da,
               params            = params)


 *******************************************************************************


 *******************************************************************************
mmtbx/invariant_domain.py
from __future__ import absolute_import, division, print_function
from cctbx.array_family import flex
from scitbx.math import euler_angles_as_matrix
from scitbx.math import superpose
import sys
from six.moves import range

class find_domain(object):
  """This class tries to find pseudo invariant domain given a set of sites.
It requires an empirical parameter named match_radius to be set to a sensible
value. For large movements, having this value set to 2.5 is probably okai.
If the movements are more subtle, more problem dedicated software might be
the best course of action."""

  def __init__(self,
               set_a,
               set_b,
               initial_rms=0.5,
               match_radius=2.0,
               overlap_thres=0.75,
               minimum_size=25):
    assert set_a.size() == set_b.size()
    assert set_a.size() > 3
    assert minimum_size <= set_a.size()
    assert minimum_size <= set_b.size()
    assert initial_rms < match_radius

    self.matches = []
    self.set_a = set_a
    self.set_b = set_b
    self.n = set_a.size()
    self.max_iter = 10
    tmp_match = self.zipper(initial_rms,match_radius)
    self.matches = self.process( tmp_match,
                                 overlap_thres=overlap_thres,
                                 minimum_size=minimum_size)

  def show(self, out=None):
    if out is None:
      out = sys.stdout
    count=1
    print("%i invariant-like domains have been found. "% len(self.matches), file=out)
    if len(self.matches)>0:
      print("Listing transformations below.", file=out)
    else:
      print("Change settings in improve results.")

    for item in self.matches:
      r = item[1]
      t = item[2]
      rmsd = item[3]
      n = item[4]
      print(file=out)
      print("Operator set %i: "%(count), file=out)
      print(r.mathematica_form(label="r", one_row_per_line=True, format="%8.5f"), file=out)
      print(file=out)
      print(t.mathematica_form(label="t", format="%8.5f"), file=out)
      print(file=out)
      print("rmsd: %5.3f; number of sites: %i"%(rmsd,n), file=out)
      print(file=out)
      count += 1

  def process(self,matches, overlap_thres=0.75,minimum_size=10):
    #convert the booleans iubnto doubles
    used = flex.bool( len(matches), False )
    tmp_matches = []
    final_matches = []
    done = not bool(matches)
    while not done:
      # find the largest domain please
      size, index = self.find_largest( matches, used )
      if size>minimum_size:
        final_matches.append( matches[index] )
      # find all other sequences that share more than n% similarity
      sims = self.find_similar_matches( matches[index], matches, used, overlap_thres )
      used = used.set_selected( sims, True )
      if used.count( True ) == used.size():
        done = True
    return final_matches

  def find_similar_matches( self, target, matches, used, overlap_thres ):
    tmp_a =  flex.double(  self.set_a.size() , 0.0 ).set_selected( target[0], 1.0 )
    result = flex.size_t()
    for ii in range(len(matches) ):
      if not used[ii]:
        match = matches[ii][0]
        tmp_b = flex.double( self.set_a.size(), 0.0 ).set_selected( match, 1.0 )
        similar = flex.sum( tmp_a*tmp_b )/flex.sum( tmp_a )
        if similar > overlap_thres:
          result.append( ii )
    return( result )

  def pair_sites(self, r, t, cut_off):
    new_sites = r.elems*self.set_b+t.elems
    deltas = self.set_a - new_sites
    deltas = flex.sqrt( deltas.dot(deltas) )
    select = flex.bool( deltas < cut_off )
    tmp_a = self.set_a.select( select.iselection() )
    tmp_b = self.set_b.select( select.iselection() )
    return tmp_a, tmp_b, select

  def zipper(self,initial_rms, level):
    matches = []
    for jj in range( 1,self.n-1 ):
      ii = jj - 1
      kk = jj + 1
      #make triplets of sequence related sites
      xi = self.set_a[ii] ; xpi = self.set_b[ii]
      xj = self.set_a[jj] ; xpj = self.set_b[jj]
      xk = self.set_a[kk] ; xpk = self.set_b[kk]
      #get the lsq matrix
      ref = flex.vec3_double( [xi,xj,xk] )
      mov = flex.vec3_double( [xpi,xpj, xpk] )

      lsq = superpose.least_squares_fit(ref,mov)
      #here we have the rotation and translation operators
      r = lsq.r
      t = lsq.t
      rmsd = 10.0
      #we would like to know the rmsd on the coords used for superposition
      new_sites = lsq.other_sites_best_fit()
      deltas = ref - new_sites
      rmsd = deltas.rms_length()
      if rmsd < initial_rms:
        # please apply this rotation to the full set
        converged = False
        count=0
        match_size = 0
        previous_match_size = 0
        tmp_a = None
        tmp_b = None
        select = flex.bool()
        while not converged:
          previous_match_size = match_size
          tmp_a, tmp_b, select = self.pair_sites(r,t,level)
          #print count, tmp_a.size()
          match_size = tmp_a.size()
          if match_size <= previous_match_size:
            converged=True
            break
          if count>self.max_iter:
            converged=True
            break
          if tmp_b.size()>0:
            lsq = superpose.least_squares_fit(tmp_a,tmp_b)
            tmp_sites = lsq.other_sites_best_fit()
            rmsd = tmp_a.rms_difference(tmp_sites)
            r = lsq.r
            t = lsq.t
            count += 1
        if converged:

          matches.append( [select.deep_copy().iselection(),
                           r,
                           t,
                           rmsd,
                           select.deep_copy().iselection().size() ]  )
    return matches

  def find_largest(self,matches,used_flags=None):
    sizes = flex.double()
    if used_flags is None:
      used_flags = flex.bool( len(matches), False )
    for match in matches:
      sizes.append( match[0].size() )
    multi = flex.double( len(matches), 1 )
    multi = multi.set_selected( used_flags.iselection(), 0)
    sizes = sizes*multi
    max_size = flex.max( sizes )
    max_loc = flex.max_index( sizes )
    return max_size, max_loc

def exercise_core(n=10, verbose=0):
  from libtbx.test_utils import approx_equal
  import random
  # make two random sets of sites please
  c = euler_angles_as_matrix([random.uniform(0,360) for i in range(3)])
  set_1 = flex.vec3_double(flex.random_double(n*3)*10-2)
  set_2 = flex.vec3_double(flex.random_double(n*3)*10-2)
  set_3 = tuple(c)*set_2

  set_a = set_1.concatenate( set_2 )
  set_b = set_1.concatenate( set_3 )

  tmp = find_domain(set_a,
                    set_b,
                    initial_rms=0.02,
                    match_radius=0.03,
                    minimum_size=1)
  if (verbose):
    tmp.show()
  assert len(tmp.matches)==2
  assert approx_equal(tmp.matches[0][3],0,eps=1e-5)
  assert approx_equal(tmp.matches[0][4],n,eps=1e-5)

def exercise():
  import random
  if (1): # fixed random seed to avoid rare failures
    random.seed(0)
    flex.set_random_seed(0)
  verbose = "--verbose" in sys.argv[1:]
  for n in [4,10,20,100,200]:
    exercise_core(n=n, verbose=verbose)
  print("OK")

if( __name__ == "__main__"):
  exercise()


 *******************************************************************************


 *******************************************************************************
mmtbx/lattice.py

"""
Utility functions for analyzing lattice contacts.  Used in:

`Fraser JS, van den Bedem H, Samelson AJ, Lang PT, Holton JM, Echols N, Alber
T. Accessing protein conformational ensembles using room-temperature X-ray
crystallography. Proc Natl Acad Sci U S A. 2011 Sep 27;108(39):16247-52.
<http://www.ncbi.nlm.nih.gov/pubmed/21918110>`_
"""

from __future__ import absolute_import, division, print_function
import operator
import sys

def find_crystal_contacts(xray_structure,
                           pdb_atoms, # atom_with_labels, not atom!
                           selected_atoms=None,
                           distance_cutoff=3.5,
                           ignore_same_asu=True,
                           ignore_waters=True):
  from scitbx.array_family import flex
  sites_frac = xray_structure.sites_frac()
  unit_cell = xray_structure.unit_cell()
  pair_asu_table = xray_structure.pair_asu_table(
    distance_cutoff=distance_cutoff)
  pair_sym_table = pair_asu_table.extract_pair_sym_table()
  contacts = []
  if (selected_atoms is None):
    selected_atoms = flex.bool(len(pdb_atoms), True)
  for i_seq,pair_sym_dict in enumerate(pair_sym_table):
    if (not selected_atoms[i_seq]):
      continue
    site_i = sites_frac[i_seq]
    atom_i = pdb_atoms[i_seq]
    resname_i = atom_i.resname
    atmname_i = atom_i.name
    chainid_i = atom_i.chain_id
    for j_seq,sym_ops in pair_sym_dict.items():
      site_j = sites_frac[j_seq]
      atom_j = pdb_atoms[j_seq]
      resname_j = atom_j.resname
      atmname_j = atom_j.name
      chainid_j = atom_j.chain_id
      for sym_op in sym_ops:
        if sym_op.is_unit_mx():
          if ignore_same_asu :
            continue
          elif (chainid_i == chainid_j):
            continue
        if (resname_j in ["HOH","WAT"] and ignore_waters):
          continue
        site_ji = sym_op * site_j
        distance = unit_cell.distance(site_i, site_ji)
        contacts.append((i_seq, j_seq, sym_op, distance))
        #print resname_i, atmname_i, resname_j, atmname_j, str(sym_op), distance
  return contacts

def find_crystal_contacts_by_residue(xray_structure,
                                      pdb_hierarchy,
                                      **kwds):
  contacts_by_residue = {}
  atoms = list(pdb_hierarchy.atoms_with_labels())
  contacts = find_crystal_contacts(xray_structure, atoms, **kwds)
  for (i_seq, j_seq, sym_op, distance) in contacts :
    atom_rec = atoms[i_seq].fetch_labels()
    residue_key = (atom_rec.chain_id, atom_rec.resname, atom_rec.resid(),
      atom_rec.altloc)
    if (not residue_key in contacts_by_residue):
      contacts_by_residue[residue_key] = []
    contacts_by_residue[residue_key].append((j_seq, sym_op, distance))
  all_residues = []
  for chain in pdb_hierarchy.models()[0].chains():
    chain_id = chain.id
    for residue_group in chain.residue_groups():
      resid = residue_group.resid()
      for atom_group in residue_group.atom_groups():
        resname = atom_group.resname
        altloc = atom_group.altloc
        residue_key = (chain_id, resname, resid, altloc)
        residue_contacts = contacts_by_residue.get(residue_key, [])
        all_residues.append((residue_key, residue_contacts))
  return all_residues

def extract_closest_contacting_residues(residue_contacts,
                                         pdb_atoms):
  reduced_contacts = []
  for (residue_key, contacts) in residue_contacts :
    if (len(contacts) == 0):
      reduced_contacts.append((residue_key, None, None, None))
    else :
      contacts.sort(key=operator.itemgetter(2))
      (j_seq, sym_op, distance) = contacts[0]
      atom_rec = pdb_atoms[j_seq].fetch_labels()
      contact_key = (atom_rec.chain_id, atom_rec.resname, atom_rec.resid(),
        atom_rec.altloc)
      reduced_contacts.append((residue_key, contact_key, sym_op, distance))
  return reduced_contacts

def summarize_contacts_by_residue(residue_contacts,
                                   pdb_hierarchy,
                                   out=sys.stdout):
  from mmtbx.refinement.print_statistics import make_header
  summary = extract_closest_contacting_residues(residue_contacts,
    pdb_hierarchy.atoms())
  make_header("Crystal contacts by residue", out=out)
  print("  %-16s %-16s %-16s %-16s" % ("residue", "closest contact",
    "symop", "distance (A)"), file=out)
  print("-"*72, file=out)
  for (residue_key, contact_key, sym_op, distance) in summary :
    (chain_id, resname, resid, altloc) = residue_key
    id_str = "%s%5s %3s %s" % (chain_id, resid, resname, altloc)
    if (contact_key is None):
      print("  %-16s %-16s %-16s %-4s" % (id_str, "*","*","*"), file=out)
    else :
      (chain_id, resname, resid, altloc) = contact_key
      id_str_2 = "%s%5s %3s %s" % (chain_id, resid, resname, altloc)
      print("  %-16s %-16s %-16s %-4.2f" % (id_str, id_str_2, sym_op,
        distance), file=out)

def show_contacts(contacts, pdb_atoms):
  for contact in contacts :
    (i_seq, j_seq, sym_op, distance) = contact
    atom_i = pdb_atoms[i_seq]
    atom_j = pdb_atoms[j_seq]
    fmt_i = atom_i.id_str()[5:20]
    fmt_j = atom_j.id_str()[5:20]
    #fmt_i = "%-2s %4s %3s %4s" % (atom_i.chain_id, atom_i.resid(),
    #  atom_i.resname, atom_i.name)
    #fmt_j = "%-2s %4s %3s %4s" % (atom_j.chain_id, atom_j.resid(),
    #  atom_j.resname, atom_j.name)
    print("%s %s %5.2f %s" % (fmt_i,fmt_j,distance,str(sym_op)))

def show_contacts_for_pymol(contacts, pdb_atoms, object_name,
    distance_cutoff=3.5):
  for contact in contacts :
    (i_seq, j_seq, sym_op, distance) = contact
    atom_i = pdb_atoms[i_seq]
    atom_j = pdb_atoms[j_seq]
    s1 = "(%s and chain '%s' and resi %d and name %s)" % (object_name,
      atom_i.chain_id, atom_i.resseq_as_int(), atom_i.name)
    s2 = "((not %s) and (chain '%s' and resi %d and name %s))" % (
      object_name, atom_j.chain_id, atom_j.resseq_as_int(), atom_j.name)
    print("dist %s, %s within %.1f of %s" % (s1, s2, distance_cutoff+0.1, s1))

def apply_sym_op_to_pdb(pdb_hierarchy, sym_op, unit_cell):
  #import scitbx.matrix
  r = sym_op.r()
  t = sym_op.t()
  #rt = scitbx.matrix.rt((r.as_double(), t.as_double()))
  new_hierarchy = pdb_hierarchy.deep_copy()
  atoms = pdb_hierarchy.atoms()
  sites_frac = unit_cell.fractionalize(sites_cart=atoms.extract_xyz())
  new_sites = sites_frac * r.as_double() + t.as_double()
  atoms.set_xyz(unit_cell.orthogonalize(sites_frac=new_sites))
  return new_hierarchy

def apply_biological_unit(pdb_in):
  atoms = pdb_in.atoms()
  remark = pdb_in.remark_section()
  if (remark.size() == 0):
    raise Sorry("No REMARK records in this PDB file.")
  return pdb_out

if __name__ == "__main__" :
  pdb_file = sys.argv[1]
  import iotbx.pdb
  pdb_in = iotbx.pdb.input(pdb_file)
  pdb_hierarchy = pdb_in.construct_hierarchy()
  xrs = pdb_in.xray_structure_simple()
  residue_contacts = find_crystal_contacts_by_residue(xrs, pdb_hierarchy)
  summarize_contacts_by_residue(residue_contacts, pdb_hierarchy)


 *******************************************************************************


 *******************************************************************************
mmtbx/libtbx_refresh.py


 *******************************************************************************


 *******************************************************************************
mmtbx/map_tools.py

"""
This module contains a variety of functions related to map calculation, many
of which are access via the mmtbx.f_model.manager API.  It has some overlap
with the separate mmtbx.maps module.
"""

from __future__ import absolute_import, division, print_function
from cctbx.array_family import flex
from cctbx import miller
from cctbx import maptbx
from libtbx.utils import null_out
import mmtbx
import libtbx
import random
import boost_adaptbx.boost.python as bp
from six.moves import range
mmtbx_f_model_ext = bp.import_ext("mmtbx_f_model_ext")
import mmtbx.masks

def shelx_weight(
      f_obs,
      f_model,
      weight_parameter=None):
  if(weight_parameter is not None):
    sc = weight_parameter
  else:
    sc = flex.double()
    for sc_ in range(f_obs.data().size()):
      sc.append(random.choice([1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2]))
  f_m = abs(f_model).data()
  f_o = abs(f_obs).data()
  i_c = f_m * f_m
  i_f = f_o * f_o
  sig_f = f_obs.sigmas()
  assert sig_f.size() == f_o.size()
  w = 1./( sig_f*sig_f*sig_f*sig_f + (sc * i_f)*(sc * i_f) )
  sigma_i = 1/flex.sqrt(w)
  r1 = i_c*i_c/(i_c*i_c + sigma_i*sigma_i)
  r2 = 1 / (1 + sig_f*sig_f*sig_f*sig_f/i_c/i_c + sc*sc*i_f*i_f/i_c/i_c)
  return r1

class fo_fc_scales(object):
  def __init__(self,
               fmodel,
               map_type_str,
               acentrics_scale = 2.0,
               centrics_scale = 1.0):
    """
    Compute x and y for x*Fobs-y*Fmodel given map type string
    """
    self.fmodel = fmodel
    mnm = mmtbx.map_names(map_name_string = map_type_str)
    # R.Read, SIGMAA: 2mFo-DFc (acentrics) & mFo (centrics)
    centric_flags  = self.fmodel.f_obs().centric_flags().data()
    acentric_flags = ~centric_flags
    if(mnm.k != 0):
      self.fo_scale = flex.double(self.fmodel.f_obs().size(), 1.0)
    else: self.fo_scale = flex.double(self.fmodel.f_obs().size(), 0.0)
    if(mnm.n != 0):
      self.fc_scale = flex.double(self.fmodel.f_obs().size(), 1.0)
    else: self.fc_scale = flex.double(self.fmodel.f_obs().size(), 0.0)
    abs_kn = abs(mnm.k*mnm.n)
    if(mnm.k != abs(mnm.n) and abs_kn > 1.e-6):
      self.fo_scale.set_selected(acentric_flags, mnm.k)
      self.fo_scale.set_selected( centric_flags, max(mnm.k-centrics_scale,0.))
      self.fc_scale.set_selected(acentric_flags, mnm.n)
      self.fc_scale.set_selected( centric_flags, max(mnm.n-centrics_scale,0.))
    elif(mnm.k == abs(mnm.n) and abs_kn > 1.e-6):
      fo_scale_k = self.fo_scale*mnm.k
      self.fc_scale_n = self.fc_scale*mnm.n
      self.fo_scale.set_selected(acentric_flags, fo_scale_k*acentrics_scale)
      self.fo_scale.set_selected( centric_flags, fo_scale_k)
      self.fc_scale.set_selected(acentric_flags, self.fc_scale_n*acentrics_scale)
      self.fc_scale.set_selected( centric_flags, self.fc_scale_n)
    else:
      self.fo_scale *= mnm.k
      self.fc_scale *= mnm.n

class combine(object):
  def __init__(self,
               fmodel,
               map_type_str,
               fo_scale,
               fc_scale,
               use_shelx_weight,
               shelx_weight_parameter,
               map_calculation_helper=None):
    self.mch = map_calculation_helper
    self.mnm = mmtbx.map_names(map_name_string = map_type_str)
    self.fmodel = fmodel
    self.fc_scale = fc_scale
    self.fo_scale = fo_scale
    self.f_obs = None
    self.f_model = None
    self.use_shelx_weight = use_shelx_weight
    self.shelx_weight_parameter = shelx_weight_parameter
    if(not self.mnm.ml_map):
      self.f_obs = self.fmodel.f_obs().data()*fo_scale
    else:
      if(self.mch is None): self.mch = self.fmodel.map_calculation_helper()
      if(self.fmodel.hl_coeffs() is None):
        self.f_obs = self.mch.f_obs.data()*fo_scale*self.mch.fom
      else:
        cp = fmodel.combine_phases(map_calculation_helper = self.mch)
        self.f_obs = self.mch.f_obs.data()*fo_scale*cp.f_obs_phase_and_fom_source()

  def map_coefficients(self, f_model=None):
    def compute(fo,fc,miller_set):
      if(type(fo) == flex.double):
        fo = miller.array(miller_set=miller_set, data=fo).phase_transfer(
          phase_source = self.f_model).data()
      return miller.array(miller_set=miller_set, data=fo+fc)
    if(f_model is None):
      if(not self.mnm.ml_map):
        self.f_model = self.fmodel.f_model_scaled_with_k1()
        f_model_data = self.f_model.data()*self.fc_scale
      else:
        self.f_model = self.mch.f_model
        f_model_data = self.f_model.data()*self.fc_scale*self.mch.alpha.data()
    else:
      self.f_model = f_model
      if(not self.mnm.ml_map):
        f_model_data = self.f_model.data()*self.fc_scale
      else:
        f_model_data = self.f_model.data()*self.fc_scale*self.mch.alpha.data()
    # f_model_data may be multiplied by scales like "-1", so it cannot be
    # phase source !
    result = compute(fo=self.f_obs, fc=f_model_data,
      miller_set=self.fmodel.f_obs())
    if(self.use_shelx_weight):
      sw = shelx_weight(
        f_obs   = self.fmodel.f_obs(),
        f_model = self.fmodel.f_model_scaled_with_k1(),
        weight_parameter = self.shelx_weight_parameter)
      result = result.customized_copy(data = result.data()*sw)
    return result

class electron_density_map(object):

  def __init__(self,
               fmodel,
               map_calculation_helper = None):
    self.fmodel = fmodel
    self.anom_diff = None
    self.mch = map_calculation_helper
    if(self.fmodel.f_obs().anomalous_flag()):
      self.anom_diff = self.fmodel.f_obs().anomalous_differences()
      f_model = self.fmodel.f_model().as_non_anomalous_array().\
        merge_equivalents().array()
      fmodel_match_anom_diff, anom_diff_common = \
        f_model.common_sets(other =  self.anom_diff)
      assert anom_diff_common.indices().size()==self.anom_diff.indices().size()
      self.anom_diff = anom_diff_common.phase_transfer(
        phase_source = fmodel_match_anom_diff)

  def map_coefficients(self,
                       map_type,
                       acentrics_scale = 2.0,
                       centrics_pre_scale = 1.0,
                       exclude_free_r_reflections=False,
                       fill_missing=False,
                       fill_missing_method="f_model",
                       isotropize=True,
                       sharp=False,
                       pdb_hierarchy=None, # XXX required for map_type=llg
                       merge_anomalous=None,
                       use_shelx_weight=False,
                       shelx_weight_parameter=1.5):
    map_name_manager = mmtbx.map_names(map_name_string = map_type)
    # Special case #1: anomalous map
    if(map_name_manager.anomalous):
      if(self.anom_diff is not None):
        # Formula from page 141 in "The Bijvoet-Difference Fourier Synthesis",
        # Jeffrey Roach, METHODS IN ENZYMOLOGY, VOL. 374
        return miller.array(miller_set = self.anom_diff,
                            data       = self.anom_diff.data()/(2j))
      else: return None
    # Special case #2: anomalous residual map
    elif (map_name_manager.anomalous_residual):
      if (self.anom_diff is not None):
        return anomalous_residual_map_coefficients(
          fmodel=self.fmodel,
          exclude_free_r_reflections=exclude_free_r_reflections)
      else : return None
    # Special case #3: Phaser SAD LLG map
    elif (map_name_manager.phaser_sad_llg):
      if (pdb_hierarchy is None):
        raise RuntimeError("pdb_hierarchy must not be None when a Phaser SAD "+
          "LLG map is requested.")
      if (self.anom_diff is not None):
        return get_phaser_sad_llg_map_coefficients(
          fmodel=self.fmodel,
          pdb_hierarchy=pdb_hierarchy)
      else :
        return None
    # Special case #4: Fcalc map
    mnm = mmtbx.map_names(map_name_string = map_type)
    if(mnm.k==0 and abs(mnm.n)==1):
      if(fill_missing):
        return self.fmodel.xray_structure.structure_factors(
          d_min = self.fmodel.f_obs().d_min()).f_calc()
      else:
        return self.fmodel.f_obs().structure_factors_from_scatterers(
          xray_structure = self.fmodel.xray_structure).f_calc()
    #
    if(self.mch is None):
      self.mch = self.fmodel.map_calculation_helper()
    ffs = fo_fc_scales(
      fmodel          = self.fmodel,
      map_type_str    = map_type,
      acentrics_scale = acentrics_scale,
      centrics_scale  = centrics_pre_scale)
    fo_scale, fc_scale = ffs.fo_scale, ffs.fc_scale
    coeffs = combine(
      fmodel                 = self.fmodel,
      map_type_str           = map_type,
      fo_scale               = fo_scale,
      fc_scale               = fc_scale,
      map_calculation_helper = self.mch,
      use_shelx_weight       = use_shelx_weight,
      shelx_weight_parameter = shelx_weight_parameter).map_coefficients()
    r_free_flags = None
    # XXX the default scale array (used for the isotropize option) needs to be
    # calculated and processed now to avoid array size errors
    scale_default = 1. / (self.fmodel.k_isotropic()*self.fmodel.k_anisotropic())
    scale_array = coeffs.customized_copy(data=scale_default)
    if (exclude_free_r_reflections):
      if (coeffs.anomalous_flag()):
        coeffs = coeffs.average_bijvoet_mates()
      r_free_flags = self.fmodel.r_free_flags()
      if (r_free_flags.anomalous_flag()):
        r_free_flags = r_free_flags.average_bijvoet_mates()
        scale_array = scale_array.average_bijvoet_mates()
      coeffs = coeffs.select(~r_free_flags.data())
      scale_array = scale_array.select(~r_free_flags.data())
    scale=None
    if(isotropize):
      if(scale is None):
        if (scale_array.anomalous_flag()) and (not coeffs.anomalous_flag()):
          scale_array = scale_array.average_bijvoet_mates()
        scale = scale_array.data()
      coeffs = coeffs.customized_copy(data = coeffs.data()*scale)
    if(fill_missing):
      if(coeffs.anomalous_flag()):
        coeffs = coeffs.average_bijvoet_mates()
      coeffs = fill_missing_f_obs(
        coeffs = coeffs,
        fmodel = self.fmodel,
        method = fill_missing_method)
    if(sharp):
      ss = 1./flex.pow2(coeffs.d_spacings().data()) / 4.
      from cctbx import adptbx
      b = flex.mean(self.fmodel.xray_structure.extract_u_iso_or_u_equiv() *
        adptbx.u_as_b(1))/2
      k_sharp = 1./flex.exp(-ss * b)
      coeffs = coeffs.customized_copy(data = coeffs.data()*k_sharp)
    if (merge_anomalous) and (coeffs.anomalous_flag()):
      return coeffs.average_bijvoet_mates()
    return coeffs

  def fft_map(self,
              resolution_factor = 1/3.,
              symmetry_flags = None,
              map_coefficients = None,
              other_fft_map = None,
              map_type = None,
              force_anomalous_flag_false = None,
              acentrics_scale = 2.0,
              centrics_pre_scale = 1.0,
              use_all_data = True):
    if(map_coefficients is None):
      map_coefficients = self.map_coefficients(
        map_type           = map_type,
        acentrics_scale    = acentrics_scale,
        centrics_pre_scale = centrics_pre_scale)
      if(force_anomalous_flag_false):
        map_coefficients = map_coefficients.average_bijvoet_mates()
    if(force_anomalous_flag_false):
      map_coefficients = map_coefficients.average_bijvoet_mates()
    if(not use_all_data):
      map_coefficients = map_coefficients.select(self.fmodel.arrays.work_sel)
    if(other_fft_map is None):
      return map_coefficients.fft_map(
        resolution_factor = resolution_factor,
        symmetry_flags    = symmetry_flags)
    else:
      return miller.fft_map(
        crystal_gridding     = other_fft_map,
        fourier_coefficients = map_coefficients)

def resolve_dm_map(
      fmodel,
      map_coeffs,
      xrs,
      use_model_hl,
      fill,
      solvent_content=None,
      solvent_content_attenuator=0.1,
      mask_cycles  = 2,
      minor_cycles = 2,
      mask_from_model = None,  # use xrs as model_mask in density modification
      add_mask=None,  # alternative to mask_from_model: use xrs as added mask
      denmod_with_model = True, # use xrs in density modification
      input_text   = None):
  """
  Compute Resolve DM map
  """
  input_text="""
keep_missing
"""
  from solve_resolve.resolve_python import density_modify_in_memory
  if(solvent_content is None):
    import mmtbx.utils
    solvent_content = mmtbx.utils.f_000(xray_structure =
      fmodel.xray_structure).solvent_fraction-solvent_content_attenuator
  else:
    solvent_content = solvent_content-solvent_content_attenuator
  hl_model = None
  if(use_model_hl):
    hl_model = miller.set(crystal_symmetry=fmodel.f_obs().crystal_symmetry(),
      indices = fmodel.f_obs().indices(),
      anomalous_flag=False).array(
        data=fmodel.f_model_phases_as_hl_coefficients(
          map_calculation_helper=None))
  f_obs = fmodel.f_obs()
  f_obs = f_obs.array(
      data = f_obs.data(),
      sigmas = flex.double(f_obs.indices().size(), 1.0))  # must be present
  if(fill):
    data = f_obs.data()
    sigmas = f_obs.sigmas()
    complete_set = f_obs.complete_set()
    n_missing =  complete_set.indices().size() - f_obs.indices().size()
    complete_set = complete_set.array(
      data = flex.double(complete_set.indices().size(), 0.01), # must be > 0.0
      sigmas = flex.double(complete_set.indices().size(), -1.0))  # must be -1.0
    f_obs = f_obs.complete_with(other=complete_set)
  if(add_mask or mask_from_model):
    assert not (add_mask and mask_from_model)
    model_xrs = xrs # add_mask using xrs to define it. Could instead supply
                    #  a different xrs to define add_mask or model_mask
  else:
    model_xrs = None
  cmn=density_modify_in_memory.run(
    fp_sigfp            = f_obs.deep_copy(),
    hendrickson_lattman = hl_model,
    rad_mask            = max(2.5, f_obs.d_min()),
    map_coeffs_start    = map_coeffs,
    solvent_content     = solvent_content,
    mask_cycles         = mask_cycles,
    minor_cycles        = minor_cycles,
    xrs                 = xrs,
    model_xrs           = model_xrs,
    denmod_with_model   = denmod_with_model,
    add_mask            = add_mask,
    mask_from_model     = mask_from_model,
    verbose             = False,
    input_text          = input_text,
    out                 = null_out())
  result = cmn.map_coeffs_out_as_miller_array
  assert f_obs.indices().size()==result.indices().size()
  return result

class model_missing_reflections(object):
  def __init__(
        self,
        fmodel,
        coeffs):
    # XXX see f_model.py: duplication! Consolidate.
    self.fmodel = fmodel
    self.coeffs = coeffs
    crystal_gridding = fmodel.f_obs().crystal_gridding(
      d_min              = self.fmodel.f_obs().d_min(),
      resolution_factor  = 1./3)
    fft_map = miller.fft_map(
      crystal_gridding     = crystal_gridding,
      fourier_coefficients = self.coeffs)
    fft_map.apply_sigma_scaling()
    map_data = fft_map.real_map_unpadded()
    rho_atoms = flex.double()
    for site_frac in self.fmodel.xray_structure.sites_frac():
      rho_atoms.append(map_data.eight_point_interpolation(site_frac))
    rho_mean = flex.mean_default(rho_atoms.select(rho_atoms>0.5), 0.5)
    sel_exclude = rho_atoms > min(rho_mean/2., 1)
    sites_cart = fmodel.xray_structure.sites_cart()
    #
    fft_map = miller.fft_map(
      crystal_gridding     = crystal_gridding,
      fourier_coefficients = self.fmodel.f_model())
    fft_map.apply_sigma_scaling()
    map_data2 = fft_map.real_map_unpadded()
    #
    for i_seq, site_cart in enumerate(sites_cart):
      selection = maptbx.grid_indices_around_sites(
        unit_cell  = self.coeffs.unit_cell(),
        fft_n_real = map_data.focus(),
        fft_m_real = map_data.all(),
        sites_cart = flex.vec3_double([site_cart]),
        site_radii = flex.double([1.5]))
      cc = flex.linear_correlation(x=map_data.select(selection),
        y=map_data2.select(selection)).coefficient()
      if(cc<0.7): sel_exclude[i_seq] = False
    #
    del map_data, fft_map, rho_atoms
    self.d_min = fmodel.f_obs().d_min()
    cs = self.fmodel.f_obs().average_bijvoet_mates().complete_set(d_min=self.d_min)
    self.complete_set = cs.array(data = flex.double(cs.indices().size(), 0))
    self.xray_structure_cut = self.fmodel.xray_structure.select(sel_exclude)
    self.missing_set = self.complete_set.common_set(self.coeffs)
    #
    self.f_calc_missing = self.complete_set.structure_factors_from_scatterers(
      xray_structure = self.xray_structure_cut).f_calc()
    self.ss_missing = 1./flex.pow2(self.f_calc_missing.d_spacings().data()) / 4.
    mask_manager = mmtbx.masks.manager(
      miller_array      = self.f_calc_missing,
      miller_array_twin = None,
      mask_params       = None)
    self.f_mask_missing = mask_manager.shell_f_masks(
      xray_structure = self.xray_structure_cut,
      force_update   = True)
    self.zero_data = flex.complex_double(self.f_calc_missing.data().size(), 0)

  def get_missing_fast(self):
    k_sol = random.choice([i/100. for i in range(20,41)])
    b_sol = random.choice([i for i in range(20,85, 5)])
    f_calc = self.kick.randomize_struture_factors(
      map_coeffs=self.f_calc_missing, number_of_kicks=10)
    data = mmtbx_f_model_ext.core(
      f_calc        = f_calc.data(),
      shell_f_masks = [self.f_mask_missing[0].data(),],
      k_sols        = [k_sol,],
      b_sol         = b_sol,
      f_part1       = self.zero_data,
      f_part2       = self.zero_data,
      u_star        = [0,0,0,0,0,0],
      hkl           = self.f_calc_missing.indices(),
      uc            = self.f_calc_missing.unit_cell(),
      ss            = self.ss_missing)
    return miller.array(miller_set=self.f_calc_missing, data=data.f_model)

  def get_missing(self, deterministic=False):
    if(deterministic):
      xrs = self.xray_structure_cut
    else:
      sel = flex.random_bool(self.xray_structure_cut.scatterers().size(), 0.9)
      xrs = self.xray_structure_cut.select(sel)
    if(deterministic):
      # XXX This may result in very bad maps at low resolution
      #fm = mmtbx.f_model.manager(
      #  f_obs          = self.complete_set,
      #  xray_structure = xrs,
      #  k_sol          = 0.35,
      #  b_sol          = 46.0)
      #result = fm.f_model_no_scales()
      # This is slower but better
      # Never change this unless checked with CK case
      fm = self.fmodel.deep_copy()
      r = fm.update_all_scales(fast=False, refine_hd_scattering=False)
      fm = mmtbx.f_model.manager(
        f_obs          = self.complete_set,
        xray_structure = xrs,
        k_sol          = r.k_sol[0],
        b_sol          = r.b_sol[0],
        b_cart         = r.b_cart)
      result = fm.f_model_no_scales()
    else:
      if(random.choice([True, False])):
        k_sol = random.choice([i/100. for i in range(20,41)])
        b_sol = random.choice([i for i in range(20,85, 5)])
        fm = mmtbx.f_model.manager(
          f_obs          = self.complete_set,
          xray_structure = xrs,
          k_sol          = k_sol,
          b_sol          = b_sol)
        result = fm.f_model_no_scales()
      else:
        result = xrs.structure_factors(d_min = self.d_min).f_calc()
    return result

def fill_missing_f_obs_1(coeffs, fmodel):
  mro = model_missing_reflections(coeffs=coeffs, fmodel=fmodel)
  missing = mro.get_missing(deterministic=True)
  return coeffs.complete_with(other = missing, scale=True)

def fill_missing_f_obs_2(coeffs, fmodel):
  scale_to = fmodel.f_obs().average_bijvoet_mates()
  dsf = coeffs.double_step_filtration(
    vol_cutoff_plus_percent=5.0,
    vol_cutoff_minus_percent=5.0,
    scale_to=scale_to)
  return coeffs.complete_with(other = dsf, scale=True)

def fill_missing_f_obs_3(coeffs, fmodel):
  if(not libtbx.env.has_module("solve_resolve")): return coeffs
  mc_dm_filled = resolve_dm_map(
    fmodel       = fmodel,
    map_coeffs   = coeffs,
    pdb_inp      = None,
    use_model_hl = True,
    fill         = True)
  return coeffs.complete_with(other = mc_dm_filled, scale=True)

def fill_missing_f_obs(coeffs, fmodel, method):
  assert method in ["resolve_dm", "dsf", "f_model", None, False]
  if(method == "f_model"):
    return fill_missing_f_obs_1(coeffs=coeffs, fmodel=fmodel)
  elif(method == "dsf"):
    return fill_missing_f_obs_2(coeffs=coeffs, fmodel=fmodel)
  elif(method == "resolve_dm"):
    return fill_missing_f_obs_3(coeffs=coeffs, fmodel=fmodel)
  elif(method in [None, False]):
    return coeffs
  else:
    raise RuntimeError("Invalid arg of fill_missing_f_obs: method:"%(method))

def sharp_evaluation_target(sites_frac, map_coeffs, resolution_factor = 0.25):
  fft_map = map_coeffs.fft_map(resolution_factor=resolution_factor)
  fft_map.apply_sigma_scaling()
  map_data = fft_map.real_map_unpadded()
  target = 0
  cntr = 0
  for site_frac in sites_frac:
    mv = map_data.eight_point_interpolation(site_frac)
    if(mv >0):
      target += mv
      cntr += 1
  t_mean = target/cntr
  return t_mean

def sharp_map(sites_frac, map_coeffs, ss = None, b_sharp=None, b_min = -150,
              b_max = 150, step = 10):
  if(ss is None):
    ss = 1./flex.pow2(map_coeffs.d_spacings().data()) / 4.
  from cctbx import miller
  if(b_sharp is None):
    t=-1
    map_coeffs_best = None
    b_sharp_best = None
    for b_sharp in range(b_min,b_max,step):
      map_coeffs_ = map_coeffs.deep_copy()
      sc2 = flex.exp(b_sharp*ss)
      map_coeffs_ = map_coeffs_.customized_copy(data = map_coeffs_.data()*sc2)
      t_=sharp_evaluation_target(sites_frac=sites_frac, map_coeffs=map_coeffs_)
      if(t_>t):
        t=t_
        b_sharp_best = b_sharp
        map_coeffs_best = map_coeffs_.deep_copy()
    print("b_sharp:", b_sharp_best, t)
  else:
    scale = flex.exp(b_sharp*ss)
    map_coeffs_best = map_coeffs.customized_copy(data=map_coeffs.data()*scale)
    b_sharp_best = b_sharp
  return map_coeffs_best, b_sharp_best

def get_phaser_sad_llg_map_coefficients(
    fmodel,
    pdb_hierarchy,
    log=None,
    verbose=False):
  """
  Calculates an anomalous log-likelihood gradient (LLG) map using the SAD
  target in Phaser.  This is essentially similar to an anomalous difference-
  difference map, but more sensitive.
  """
  if (not libtbx.env.has_module("phaser")):
    raise Sorry("Phaser not available - required for SAD LLG maps.")
  from phaser.phenix_adaptors import sad_target
  assert (fmodel.f_model().anomalous_flag())
  f_obs = fmodel.f_obs().select(fmodel.f_obs().data()>0)
  r_free_flags = fmodel.r_free_flags().common_set(other=f_obs)
  data = sad_target.data_adaptor(
    f_obs=f_obs,
    r_free_flags=r_free_flags,
    verbose=True)
  if (verbose) and (log is not None):
    data.output.setPackagePhenix(log)
  else :
    data.output.setPackagePhenix(null_out())
  t = data.target(
    xray_structure=fmodel.xray_structure,
    pdb_hierarchy=pdb_hierarchy,
    log=null_out())
  t.set_f_calc(fmodel.f_model())
  map_coeffs = t.llg_map_coeffs()
  return map_coeffs

def anomalous_residual_map_coefficients(fmodel, weighted=False,
    exclude_free_r_reflections=True):
  """
  EXPERIMENTAL

  Calculates map coefficients showing the difference in anomalous scattering
  between F-obs and F-model.  Similar to the Phaser SAD LLG map, but appears
  to be less sensitive.
  """
  assert (fmodel.f_obs().anomalous_flag())
  f_obs_anom = fmodel.f_obs().anomalous_differences()
  f_model_anom = abs(fmodel.f_model()).anomalous_differences()
  if (weighted):
    mch = fmodel.map_calculation_helper()
    fom = fmodel.f_obs().customized_copy(
      data=mch.fom, sigmas=None).average_bijvoet_mates()
    alpha = mch.alpha.average_bijvoet_mates()
    fom = fom.common_set(other=f_obs_anom)
    alpha = alpha.common_set(other=f_obs_anom)
    f_obs_anom = f_obs_anom.customized_copy(data=f_obs_anom.data()*fom.data())
    f_model_anom = f_model_anom.customized_copy(
      data=f_model_anom.data()*alpha.data())
  anom_diff_diff = f_obs_anom.customized_copy(
    data=f_obs_anom.data() - f_model_anom.data())
  f_model = fmodel.f_model().as_non_anomalous_array().\
    merge_equivalents().array()
  fmodel_match_anom_diff, anom_diff_diff_common = \
    f_model.common_sets(other =  anom_diff_diff)
  assert (anom_diff_diff_common.indices().size() ==
          anom_diff_diff.indices().size())
  phases_tmp = miller.array(
    miller_set=anom_diff_diff_common,
    data=flex.double(anom_diff_diff_common.indices().size(), 1)
    ).phase_transfer(phase_source=fmodel_match_anom_diff)
  map_coeffs = miller.array(
    miller_set=anom_diff_diff_common,
    data = anom_diff_diff_common.data() * phases_tmp.data())
  if (exclude_free_r_reflections):
    r_free_flags = fmodel.r_free_flags().average_bijvoet_mates()
    r_free_flags, map_coeffs = r_free_flags.common_sets(map_coeffs)
    map_coeffs = map_coeffs.select(~(r_free_flags.data()))
  return miller.array(
    miller_set=map_coeffs,
    data=map_coeffs.data()/(2j))


 *******************************************************************************


 *******************************************************************************
mmtbx/model_vs_data.py
from __future__ import absolute_import, division, print_function
import sys, random
from cctbx.array_family import flex
from iotbx import pdb
from libtbx.utils import Sorry
from iotbx import reflection_file_utils
from libtbx.str_utils import format_value
import iotbx
from mmtbx import utils
from iotbx import pdb
from six.moves import cStringIO as StringIO
import mmtbx.model
import iotbx.phil
import mmtbx.f_model
from iotbx import extract_xtal_data

if(1):
  random.seed(0)
  flex.set_random_seed(0)

def show_header(l, log):
  print(l, "-"*(79-len(l)), file=log)

def reflection_file_server(crystal_symmetry, reflection_files):
  return reflection_file_utils.reflection_file_server(
    crystal_symmetry=crystal_symmetry,
    force_symmetry=True,
    reflection_files=reflection_files,
    err=StringIO())

msg="""\
Inputs:
  - File with reflection data (Fobs or Iobs), and R-free flags (optionally);
  - label(s) selecting which reflection data arrays should be used (in case
    there are multiple choices in input file, there is no need to provide labels
    otherwise);
  - PDB file with input model;
  - some other optional parameters.

Usage examples:
  1. phenix.model_vs_data model.pdb data.hkl
  2. phenix.model_vs_data model.pdb data.hkl f_obs_label="F" r_free_flags_label="FREE"
  3. phenix.model_vs_data model.pdb data.hkl scattering_table=neutron
  4. phenix.model_vs_data model.pdb data.hkl twin_law='h,-k,l+h'

"""

master_params_str="""\
f_obs_label = None
  .type = str
r_free_flags_label = None
  .type = str
scattering_table = wk1995  it1992  *n_gaussian  neutron
  .type = choice
high_resolution = None
  .type = float
twin_law = None
  .type = str
n_bins = 20
  .type = int
"""

def defaults(log, silent):
  if(not silent): print("Default params:\n", file=log)
  parsed = iotbx.phil.parse(master_params_str)
  if(not silent): parsed.show(prefix="  ", out=log)
  if(not silent): print(file=log)
  return parsed

def run(args,
        out = None,
        log = sys.stdout):
  if(len(args)==0) or (args == ["--help"]):
    print(msg, file=log)
    defaults(log=log, silent=False)
    return
  parsed = defaults(log=log, silent=True)
  #
  processed_args = utils.process_command_line_args(args = args,
    log = log, master_params = parsed)
  params = processed_args.params.extract()
  #
  reflection_files = processed_args.reflection_files
  if(len(reflection_files) == 0):
    raise Sorry("No reflection file found.")
  crystal_symmetry = processed_args.crystal_symmetry
  if(crystal_symmetry is None):
    raise Sorry("No crystal symmetry found.")
  if(len(processed_args.pdb_file_names) == 0):
    raise Sorry("No PDB file found.")
  pdb_file_names = processed_args.pdb_file_names
  #
  rfs = reflection_file_server(
    crystal_symmetry = crystal_symmetry,
    reflection_files = reflection_files)
  parameters = extract_xtal_data.data_and_flags_master_params().extract()
  if(params.f_obs_label is not None):
    parameters.labels = params.f_obs_label
  if(params.r_free_flags_label is not None):
    parameters.r_free_flags.label = params.r_free_flags_label
  if (params.high_resolution is not None):
    parameters.high_resolution = params.high_resolution
  determine_data_and_flags_result = extract_xtal_data.run(
    reflection_file_server = rfs,
    parameters             = parameters,
    keep_going             = True)
  f_obs = determine_data_and_flags_result.f_obs
  # Data
  show_header(l="Data:", log=log)
  f_obs.show_comprehensive_summary(prefix="  ", f = log)
  # R-free-flags
  show_header(l="R-free-flags:", log=log)
  r_free_flags = determine_data_and_flags_result.r_free_flags
  test_flag_value = determine_data_and_flags_result.test_flag_value
  if(r_free_flags is None):
    r_free_flags=f_obs.array(data=flex.bool(f_obs.data().size(), False))
    test_flag_value=None
    print("  not available", file=log)
  else:
    print("  flag value:", test_flag_value, file=log)
  # Model
  pdb_combined = iotbx.pdb.combine_unique_pdb_files(
    file_names=processed_args.pdb_file_names)
  pdb_combined.report_non_unique(out=log)
  if (len(pdb_combined.unique_file_names) == 0):
    raise Sorry("No coordinate file given.")
  raw_records = pdb_combined.raw_records
  try:
    pdb_inp = iotbx.pdb.input(source_info = None,
                              lines       = flex.std_string(raw_records))
  except ValueError as e :
    raise Sorry("Model format (PDB or mmCIF) error:\n%s" % str(e))
  model = mmtbx.model.manager(
    model_input      = pdb_inp,
    crystal_symmetry = crystal_symmetry,
    log              = StringIO())
  #
  scattering_table = params.scattering_table
  exptl_method = pdb_inp.get_experiment_type()
  if exptl_method.is_neutron():
    scattering_table = "neutron"
  model.setup_scattering_dictionaries(
    scattering_table = scattering_table,
    d_min            = f_obs.d_min())
  #
  # Model vs data
  #
  show_header(l="Model vs Data:", log=log)
  fmodel = mmtbx.f_model.manager(
    xray_structure = model.get_xray_structure(),
    f_obs          = f_obs,
    r_free_flags   = r_free_flags,
    twin_law       = params.twin_law)
  fmodel.update_all_scales(update_f_part1=True)
  fmodel.show(log=log, show_header=False, show_approx=False)
  print("  r_work: %6.4f"%fmodel.r_work(), file=log)
  if(test_flag_value is not None):
    print("  r_free: %6.4f"%fmodel.r_free(), file=log)
  else:
    print("  r_free: None", file=log)
  print(file=log)
  n_outl = f_obs.data().size() - fmodel.f_obs().data().size()
  print("  Number of F-obs outliers:", n_outl, file=log)
  #
  # Extract information from PDB file header and output (if any)
  #
  pub_r_work       = None
  pub_r_free       = None
  pub_high         = None
  pub_low          = None
  pub_sigma        = None
  pub_program_name = None
  pub_solv_cont    = None
  pub_matthews     = None
  published_results = pdb_inp.get_r_rfree_sigma(file_name=pdb_file_names[0])
  if(published_results is not None):
    pub_r_work = published_results.r_work
    pub_r_free = published_results.r_free
    pub_high   = published_results.high
    pub_low    = published_results.low
    pub_sigma  = published_results.sigma
  pub_program_name = pdb_inp.get_program_name()
  pub_solv_cont    = pdb_inp.get_solvent_content()
  pub_matthews     = pdb_inp.get_matthews_coeff()
  #
  show_header(l="Information extracted from PDB file header:", log=log)
  print("  program_name    : %-s"%format_value("%s",pub_program_name), file=log)
  print("  year            : %-s"%format_value("%s",pdb_inp.extract_header_year()), file=log)
  print("  r_work          : %-s"%format_value("%s",pub_r_work), file=log)
  print("  r_free          : %-s"%format_value("%s",pub_r_free), file=log)
  print("  high_resolution : %-s"%format_value("%s",pub_high), file=log)
  print("  low_resolution  : %-s"%format_value("%s",pub_low), file=log)
  print("  sigma_cutoff    : %-s"%format_value("%s",pub_sigma), file=log)
  print("  matthews_coeff  : %-s"%format_value("%s",pub_matthews), file=log)
  print("  solvent_cont    : %-s"%format_value("%s",pub_solv_cont), file=log)
  if(exptl_method is not None):
    print("  exptl_method    : %-s"%format_value("%s", exptl_method), file=log)
  #
  # Recompute R-factors using published cutoffs
  fmodel_cut = fmodel
  tmp_sel = flex.bool(fmodel.f_obs().data().size(), True)
  if(pub_sigma is not None and fmodel.f_obs().sigmas() is not None):
    tmp_sel &= fmodel.f_obs().data() > fmodel.f_obs().sigmas()*pub_sigma
  if(pub_high is not None and abs(pub_high-fmodel.f_obs().d_min()) > 0.03):
    tmp_sel &= fmodel.f_obs().d_spacings().data() > pub_high
  if(pub_low is not None and abs(pub_low-fmodel.f_obs().d_max_min()[0]) > 0.03):
    tmp_sel &= fmodel.f_obs().d_spacings().data() < pub_low
  if(tmp_sel.count(True) != tmp_sel.size() and tmp_sel.count(True) > 0):
    show_header(l="After applying resolution and sigma cutoffs:", log=log)
    fmodel = mmtbx.f_model.manager(
      xray_structure = model.get_xray_structure(),
      f_obs          = fmodel.f_obs().select(tmp_sel),
      r_free_flags   = fmodel.r_free_flags().select(tmp_sel),
      twin_law       = params.twin_law)
    fmodel.update_all_scales(update_f_part1=True)
    fmodel.show(log=log, show_header=False, show_approx=False)
    print("  r_work: %6.4f"%fmodel.r_work(), file=log)
    if(test_flag_value is not None):
      print("  r_free: %6.4f"%fmodel.r_free(), file=log)
    else:
      print("  r_free: None", file=log)
    print(file=log)
    n_outl = f_obs.data().size() - fmodel.f_obs().data().size()
    print("  Number of F-obs outliers:", n_outl, file=log)
  return fmodel



 *******************************************************************************


 *******************************************************************************
mmtbx/msa.py
from __future__ import absolute_import, division, print_function

from libtbx import easy_run
import libtbx.phil
import libtbx.load_env
from libtbx.utils import Sorry
from libtbx import adopt_init_args
import os
import sys
from six.moves import zip

# XXX this is more complex than ideal, in order to support long and
# potentially problematic "sequence names", which in some applications will
# actually be file paths + chain IDs.  to ensure that MUSCLE doesn't choke
# on these, the option is given to substitute numerical sequence names
# internally, while still allowing retrieval using the original names.
class align_pdb_residues(object):
  """
  Provides mapping between original residue numbers or IDs and a reference
  numbering, via multiple sequence alignment.  Depending on whether or not
  the insertion code was used in the original PDB files, it can use either
  the resseq or the resid as the lookup key.
  """
  def __init__(self,
                pdb_sequences,
                pdb_names,
                pdb_offsets=None,
                pdb_resids=None,
                reference_sequence=None,
                reference_sequence_name="reference_seq",
                reference_sequence_offset=0,
                reference_sequence_resids=None,
                reference_index=None,
                substitute_names=False,
                out=None):
    adopt_init_args(self, locals())
    n_models = len(pdb_sequences)
    assert (pdb_resids is not None) or (pdb_offsets is not None)
    assert ((n_models >= 1) and (n_models==len(pdb_names)))
    if (pdb_offsets is not None):
      assert (pdb_resids is None)
      assert (len(pdb_names) == len(pdb_offsets))
      assert (reference_sequence is None) or (reference_sequence_resids is None)
    else :
      assert (len(pdb_names) == len(pdb_resids))
      assert ((reference_sequence is None) or
              (reference_sequence_resids is not None))
    self.fasta_names = []
    self._name_lookup = None
    self.run_alignment(out=out)
    self.build_lookup_table()
    self.out = None # XXX required for pickling

  def run_alignment(self, out=None):
    use_pdb_sequence = False
    # build index of sequence names given to MUSCLE
    if self.substitute_names :
      self._name_lookup = {}
      for i, name in enumerate(self.pdb_names):
        self._name_lookup[name] = str(i)
        self.fasta_names.append(str(i))
    else :
      self.fasta_names = self.pdb_names
    # determine sequence for reference numbering
    if (self.reference_sequence is not None):
      assert (self.reference_index is None)
      assert ((self.reference_sequence_resids is not None) or
              (self.reference_sequence_offset is not None))
    else :
      i_ref = self.reference_index
      use_pdb_sequence = True
      if (i_ref is None):
        i_ref = self.reference_index = 0
      self.reference_sequence = self.pdb_sequences[i_ref]
      if (self.pdb_offsets is not None):
        self.reference_sequence_offset = self.pdb_offsets[i_ref]
      else :
        self.reference_sequence_resids = self.pdb_resids[i_ref]
      if self.substitute_names :
        self.reference_sequence_name = self.fasta_names[i_ref]
      else :
        self.reference_sequence_name = self.pdb_names[i_ref]
    fasta = "\n".join([ ">%s\n%s" % (n,s) for n,s in zip(self.fasta_names,
                        self.pdb_sequences) ])
    if (not use_pdb_sequence):
      ref_seq_fasta = ">%s\n%s" % (self.reference_sequence_name,
        self.reference_sequence)
      fasta = ref_seq_fasta + "\n" + fasta
    self.muscle_aln, errors = get_muscle_alignment(fasta, out=out)
    assert (self.muscle_aln is not None)

  # I am not proud of this.
  def build_lookup_table(self):
    # find sequences and determine equivalent numbering
    self._lookup_table = {}
    self._indices = {}
    self._resids_padded = {}
    i_ref = self.muscle_aln.names.index(self.reference_sequence_name)
    self._reference_alignment = self.muscle_aln.alignments[i_ref]
    assert (i_ref is not None)
    for i, name in enumerate(self.muscle_aln.names):
      if (i == i_ref):
        self._lookup_table[name] = None
        if (self.pdb_offsets is None):
          indices = {}
          resids_padded = [None] * self.get_alignment_size()
          h = k = 0
          for resi in self._reference_alignment :
            if (resi != '-'):
              resid = self.reference_sequence_resids[k]
              if (resid is not None):
                resid = resid.strip()
                indices[resid] = h
                resids_padded[h] = resid
              k += 1
            h += 1
          self._indices[name] = indices
          self._resids_padded[name] = resids_padded
      else :
        i_pdb = self.fasta_names.index(name)
        aln = self.muscle_aln.alignments[i]
        h = j = k = 0
        # case 1: index by resseq
        if (self.pdb_offsets is not None):
          new_resseqs = []
          for res1, res2 in zip(aln, self._reference_alignment):
            if (res2 != '-'):
              k += 1
            if (res1 == '-'):
              continue
            else :
              if (res2 == '-'):
                new_resseqs.append(None)
              else :
                new_resseqs.append(k - self.reference_sequence_offset)
              j += 1
          assert (j == len(new_resseqs))
          self._lookup_table[name] = new_resseqs
        # case 2: index by resid
        else :
          new_resids = {}
          indices = {}
          resids_padded = [None] * self.get_alignment_size()
          for res1, res2 in zip(aln, self._reference_alignment):
            resid1 = resid2 = None
            if (res2 != '-'):
              resid2 = self.reference_sequence_resids[k]
              if (resid2 is not None):
                resid2 = resid2.strip()
              k += 1
            if (res1 != '-'):
              resid1 = self.pdb_resids[i_pdb][j]
              if (resid1 is not None):
                resid1 = resid1.strip()
                new_resids[resid1] = resid2
                indices[resid1] = h
                resids_padded[h] = resid1
              j += 1
            h += 1
          assert (len(new_resids) == (len(self.pdb_resids[i_pdb]) -
                                      self.pdb_resids[i_pdb].count(None)))
          self._lookup_table[name] = new_resids
          self._indices[name] = indices
          self._resids_padded[name] = resids_padded

  def get_alignment_size(self):
    return len(getattr(self, "_reference_alignment", []))

  def write_file(self, file_name):
    f = open(file_name, "w")
    f.write(str(self.muscle_aln))
    f.close()

  def get_name_index(self, pdb_name):
    if (self._name_lookup is not None):
      return self._name_lookup[pdb_name]
    else :
      return pdb_name
      #return self.pdb_names.index(pdb_name)

  def get_residue_position(self, pdb_name, resseq=None, resid=None):
    if (resseq is not None):
      assert (resid is None)
      return self.convert_residue_number(pdb_name, resseq)
    else :
      assert (resid is not None)
      return self.convert_resid(pdb_name, resid)

  def get_resid_array_index(self, pdb_name, resid):
    assert (resid is not None)
    seq_name = self.get_name_index(pdb_name)
    indices = self._indices[seq_name]
    return indices[resid]

  def get_all_resids_at_index(self, index):
    resids = []
    for name in self.fasta_names : #self._resids_padded.keys():
      resids.append(self._resids_padded[name][index])
    return resids

  def get_reference_resid(self, index):
    if (self.pdb_offsets is None):
      resids = self._resids_padded[self.reference_sequence_name]
      return resids[index]
    else :
      return index + 1

  def convert_resid(self, pdb_name, resid):
    assert (self.pdb_resids is not None)
    assert (isinstance(resid, str))
    seq_name = self.get_name_index(pdb_name)
    if (self._lookup_table[seq_name] is None):
      return resid
    try :
      return self._lookup_table[seq_name][resid.strip()]
    except KeyError as e :
      raise RuntimeError("""\
Encountered IndexError attempting to convert residue ID!
Values:
  pdb_name = %s
  resid = %s
  seq_name = %s

Dump of full alignment:
  %s
""" % (pdb_name, resseq, seq_name, self.muscle_aln))

  def convert_residue_number(self, pdb_name, resseq):
    assert (self.pdb_offsets is not None)
    seq_name = self.get_name_index(pdb_name)
    assert isinstance(resseq, int)
    if (self._lookup_table[seq_name] is None):
      return resseq
    i_pdb = self.pdb_names.index(pdb_name)
    offset = self.pdb_offsets[i_pdb]
    i_res = resseq + offset - 1
    try :
      return self._lookup_table[seq_name][i_res]
    except IndexError as e :
      raise RuntimeError("""\
Encountered IndexError attempting to convert residue number!
Values:
  pdb_name = %s
  resseq = %s
  seq_name = %s
  i_res = %s

Dump of full alignment:
  %s
""" % (pdb_name, resseq, seq_name, i_res, self.muscle_aln))

def align_pdb_hierarchies(hierarchies,
                           hierarchy_names,
                           reference_hierarchy=None,
                           substitute_names=True,
                           log=None):
  """
  Convenience function: takes a collection of *single-model, single-chain* PDB
  hierarchies, plus optional reference hierarchy, extracts sequences and
  resseq offsets or resids, and generates alignment object.  By default, if
  any of the hierarchies contain atoms with insertion codes, the resid mapping
  will be used automatically.
  """
  #assert (reference_hierarchy is None)
  if (log is None):
    log = sys.stdout
  pdb_resids = []
  i = 0
  reference_index = None
  pdb_sequences = []
  def strip(string_or_none):
    if (string_or_none is None):
      return None
    return string_or_none.strip()
  for hierarchy, name in zip(hierarchies, hierarchy_names):
    assert (hierarchy.overall_counts().n_chains == 1)
    chain = hierarchy.only_model().only_chain()
    chain_seq = chain.as_padded_sequence(skip_insertions=False)
    pdb_sequences.append(chain_seq)
    resids = chain.get_residue_ids(skip_insertions=False)
    pdb_resids.append([ strip(resid) for resid in resids ])
    if (hierarchy is reference_hierarchy):
      reference_index = i
      print("  Using %s for sequence numbering" % name, file=log)
    elif (reference_hierarchy is None) and (reference_index is None):
      reference_index = i
      print("  Using %s for sequence numbering" % name, file=log)
  if (reference_index is not None):
    pdb_names = hierarchy_names
    msa_manager = align_pdb_residues(
      pdb_sequences=pdb_sequences,
      pdb_names=pdb_names,
      pdb_resids=pdb_resids,
      reference_index=reference_index,
      substitute_names=substitute_names,
      out=log)
  else :
    assert (reference_hierarchy.overall_counts().n_chains == 1)
    chain = reference_hierarchy.only_model().only_chain()
    reference_sequence = chain.as_padded_sequence(skip_insertions=False)
    reference_sequence_offset = reference_sequence_resids = None
    resids = chain.get_residue_ids(skip_insertions=False)
    reference_sequence_resids = [ strip(resid) for resid in resids ]
    msa_manager = align_pdb_residues(
      pdb_sequences=pdb_sequences,
      pdb_names=hierarchy_names,
      pdb_resids=pdb_resids,
      reference_sequence=reference_sequence,
      reference_sequence_resids=reference_sequence_resids,
      substitute_names=substitute_names,
      out=log)
  return msa_manager

def run_muscle(fasta_sequences, group_sequences=True):
  assert group_sequences # XXX this isn't actually optional!
  if not libtbx.env.has_module(name="muscle"):
    raise RuntimeError("MUSCLE not available or not configured.")
  exe_path = libtbx.env.under_build("muscle/exe/muscle")
  if (os.name == "nt"):
    exe_path += ".exe"
  if (not os.path.isfile(exe_path)):
    raise RuntimeError("muscle executable is not available.")
  assert isinstance(fasta_sequences, str)
  cmd = "%s -quiet -clw" % exe_path
  if group_sequences :
    cmd += " -group"
  #else :
  #  cmd += " -stable"
  muscle_out = easy_run.fully_buffered(cmd,
    stdin_lines=fasta_sequences)
  return muscle_out.stdout_lines, muscle_out.stderr_lines

def get_muscle_alignment(fasta_sequences, group_sequences=True, out=None):
  muscle_out, errors = run_muscle(fasta_sequences, group_sequences)
  from iotbx.bioinformatics import clustal_alignment_parse
  alignment, null = clustal_alignment_parse("\n".join(muscle_out))
  if (out is not None):
    print("\n".join(muscle_out), file=out)
  return alignment, errors

def get_muscle_alignment_ordered(sequences, out = None):

  from iotbx import bioinformatics
  from iotbx.pdb.amino_acid_codes import validate_sequence

  name_for = {}

  for ( i, seq ) in enumerate( sequences, start = 1 ):
    name = name_for.get( seq, "Chain_%d" % i )
    name_for[ seq ] = name

  alignment, errors = get_muscle_alignment(
    fasta_sequences = "\n".join(
      str( bioinformatics.sequence( name = name, sequence = seq.sequence ) )
      for ( seq, name ) in name_for.items()
      ),
    out = out,
    )

  # check for errors and handle:
  #   invalid characters in sequences
  if (len(errors) > 0):
    for error in errors:
      error = error.strip()
      if ('Invalid character' in error):
        for seq in name_for.keys():
          invalid = validate_sequence(
            seq.sequence, protein=True, strict_protein=False,
            nucleic_acid=True, strict_nucleic_acid=False)
          if (len(invalid) > 0):
            name_for.pop(seq)
        sequences = name_for.keys()
      elif (len(error) > 0):
        raise Sorry(error)

  lookup = dict( zip( alignment.names, alignment.alignments ))
  assert all( n in lookup for n in name_for.values() )

  return bioinformatics.clustal_alignment(
    names = [ seq.name for seq in sequences ],
    alignments = [ lookup[ name_for[ seq ] ] for seq in sequences ],
    program = alignment.program
    )

########################################################################
# PHENIX GUI ADAPTOR
master_phil = libtbx.phil.parse("""
muscle
  .caption = PHENIX includes the open-source multiple sequence alignment \
    program MUSCLE, written by Bob Edgar.  It can produce output identical in \
    format to CLUSTALW, which is suitable for input to the Sculptor model \
    preparation program.  You may provide all sequences in a single file, \
    or in as many different files as desired.  Alternately, you may provide \
    one or more PDB files from which the chain sequence(s) will be extracted \
    (note however that this is only useful for homogenous models).
  .style = caption_img:icons/custom/msa64.png caption_width:480
{
  seq_file = None
    .type = path
    .multiple = True
    .short_caption = Sequence or PDB file
    .style = file_type:seq,pdb use_list input_file
  output_file = None
    .type = path
    .style = bold file_type:aln new_file
  load_in_text_editor = True
    .type = bool
    .short_caption = Open alignment in text editor when complete
}""")

def run(args=(), params=None, out=sys.stdout):
  assert (params is not None)
  seq_files = params.muscle.seq_file
  output_file = params.muscle.output_file
  if (output_file is None) or (output_file == ""):
    output_file = os.path.join(os.getcwd(), "muscle.aln")
  import iotbx.pdb
  from iotbx.bioinformatics import any_sequence_format, sequence
  seqs = []
  for file_name in seq_files :
    if (file_name.endswith(".pdb") or file_name.endswith(".ent") or
        file_name.endswith(".pdb.gz") or file_name.endswith(".ent.gz")):
      pdb_in = iotbx.pdb.input(file_name)
      hierarchy = pdb_in.construct_hierarchy()
      first_model = hierarchy.models()[0]
      found_protein = False
      for chain in first_model.chains():
        if chain.is_protein():
          chain_seq = chain.as_padded_sequence()
          base_name = os.path.basename(file_name)
          seq_name = "%s_%s" % (os.path.splitext(base_name)[0], chain.id)
          seqs.append(sequence(chain_seq, seq_name))
          found_protein = True
      if (not found_protein):
        raise Sorry(("The PDB file %s does not contain any recognizable "+
          "protein chains.") % file_name)
    else :
      try :
        seq_objects, non_compliant = any_sequence_format(file_name,
          assign_name_if_not_defined=True)
        seqs.extend(seq_objects)
      except Exception as e :
        raise Sorry(("Error parsing '%s' - not a recognizable sequence "+
          "format.  (Original message: %s)") % (file_name, str(e)))
  if (len(seqs) < 2):
    raise Sorry("Need at least two valid sequences to run MUSCLE.")

  alignment = get_muscle_alignment_ordered( sequences = seqs )
  alistr = str( alignment )

  with open( output_file, "w" ) as ofile:
    ofile.write( alistr )
    ofile.write( "\n" )

  return ( output_file, alistr )

def validate_params(params):
  if (len(params.muscle.seq_file) == 0):
    raise Sorry("No sequence files provided!")


 *******************************************************************************


 *******************************************************************************
mmtbx/pdb_distances.py
from __future__ import absolute_import, division, print_function
import sys
import numpy
from six.moves import range


##########################################################################
##########################################################################
##########################################################################
#SECTION: IMPORTANT NOTES                                 BEGINNING
##########################################################################
##########################################################################
##########################################################################

#########################################
#Syntax: "phenix.python pdb_distances.py pdb-file.pdb > something.log"
#########################################

###########################
#######040610 PROGRAM FLOW:
        #"MASTER_Basepairs_bonds" is a list that carries pre-compiled basepair information regarding a denomination for the type of basepair interaction, residues invoved, number of bonds, and atoms involved in the bonds
                #Loaded in "SECTION: LIST DEFINITION"
                #"MASTER_Basepairs_bonds" contains as many lines as basepairs
        #FUNCTION "run(args)" is the program created by Ralf W. Grosse-Kunstle and provides a list of ATOM-to-ATOM distances calculated from a .pdb file. The rest of the program was created around UNCTION "run(args)"
                #Called from "Sub SECTION Calling run(args)" within "SECTION MAIN"
        #"First_List" Contains the filtered output from FUNCTION "run(args)". Complete list of lines carrying ATOM to ATOM distances below a rough cutoff of 5A.
                #The line "pair_asu_table = xray_structure.pair_asu_table(    distance_cutoff=5.0)" in FUNCTION "run(args)" carries the cutoff value used in FUNCTION "run(args)"

        #"MASTER_Basepairs" receives from "First_List" all the lines whose ATOM to ATOM distances are consistent with hydrogen bonds in basepair interactions, as defined by "MASTER_Basepairs_bonds".
                #The transfer of information from "First_List" to "MASTER_Basepairs" is performed in "Section: First sorting of basepair candidates" of FUNCTION "program".
                        #FUNCTION "program" is called from "Sub SECTION MAIN LOOP"
                #"MASTER_Basepairs" contains as many first-level sublists as lines in "MASTER_Basepairs_bonds" (see below). Therefore, all the information in every second-level sublist is consistent with the geometry of the basepair specified by the equivalent first-level sublist in "MASTER_Basepairs_bonds"
                #The information in each second-level sublist of "MASTER_Basepairs" merely involves pairs of ATOMs whose distance to one another is below a new CUTOFF value, dynamically assigned by the program at every run of "Sub SECTION MAIN LOOP". Therefore, in order to assign a 2-bond basepair at a particular CUTOFF value, there would have to be 2 second-level sublists somewhere in the "MASTER_Basepairs" first-level sublist corresponding to that basepair. Three lines would then be required for a three-bond basepair, although the assignment of these basepairs with only two bonds is allowed
        #"MASTER_Basepairs_summary" has the same number of first-level sublists as lines in "MASTER_Basepairs_bonds" and first-level sublists in "MASTER_Basepairs". "MASTER_Basepairs_summary" receives the content of "MASTER_Basepairs" and places it into a new set of second level sublists, each of which corresponds to  a single basepair candidate. Information for all possible bonds found relevant to the pair of residues in the basepair candidate are added onto these second-level sublists
                #All operations for the assigmnent of basepairs are carried out with "MASTER_Basepairs_summary"
                #Many second-level sublists in "MASTER_Basepairs_summary" will not be assigned to any basepairs
        #"new_list_end" will be used to filter out the information of "MASTER_Basepairs_summary" into a single-level list carrying only assigned basepairs. These basepairs will be ordered by RESIDUE (from smallest res# to largest res#)
                #Loaded in "SECTION ORDERING OUTPUT"
        #"run_cutoff_LISTS" will carry the same information as "new_list_end" but ordered by CUTOFF at which the basepair was identified. First level lists are the CUTOFF levels, according to the values stored in "run_cutoff". "run_cutoff_LISTS" will be used for STATISTICS
                #Loaded in SECTION STATISTICS
                        #"run_cutoff" is loaded in "Sub SECTION MAIN LOOP"
#######040610 PROGRAM FLOW:
###########################

#Note 012110:
  #By testing this script with different cutoffs for detection of basepairs, (see 'CUTOFF'), I have realized that different subsets of basepairs are identified when the resolution is changed from 3.2 to 3.6. At low cuttoff, only high-confidence basepairs are identified and very few of them get tagged as involved in MULTIPLE CONTACTS. As the cutoff is increased, the number of basepairs involved in MULTIPLE CONTACTS increases and so does the number of misidentified basepairs. This suggests that the best way to maximize the number of identified basepairs should be to start the run at low cuttoff, let's say 3.2 and perform successive runs at increasing cuttoffs, until 3.6 is reached. If the basepairs identified with high confidence are fixed, new basepairs can be identified with confidence at higher cutoffs by extension of preformed helices
##########################################################################
##########################################################################
##########################################################################
#SECTION: IMPORTANT NOTES                                 END
##########################################################################
##########################################################################
##########################################################################


##########################################################################
##########################################################################
##########################################################################
#SECTION: LIST DEFINITION                                 BEGINNING
##########################################################################
##########################################################################
##########################################################################
#"First_List" Contains the filtered output from FUNCTION "run(args)". Listof ATOM to ATOM distances below a rough cutoff of 5A

#"MASTER_Basepairs" master list: This list will contain lines from "First_List" that carry ATOM to ATOM distances that are consistent with basepair geometry, as established in "Basepair Lists SECTION" below.
        #"MASTER_Basepairs" will have as many sublists as established in "Basepair Lists SECTION" below. The basepair denomination for these sublists is (NOTE that this denomination will be carried separately in "MASTER_Basepairs_schemes" and also in "MASTER_Basepairs_summary[i][0]").
        #"MASTER_Basepairs" will have as many sub-sublists as basepair candidates are identified for a particular basepair scheme
###i = 0 in MASTER_Basepairs (I_AA)
###i = 1 in MASTER_Basepairs (II_AA)
###i = 2 in MASTER_Basepairs (III_GG)
###i = 3 in MASTER_Basepairs (IV_GG)
###i = 4 MASTER_Basepairs (V_AA)
###i = 5 in MASTER_Basepairs (VI_GG)
###i = 6 in MASTER_Basepairs (VII_GG)
###i = 7 in MASTER_Basepairs (VIII_AG)
###i = 8 in MASTER_Basepairs (VIII_GA)
###i = 9 in MASTER_Basepairs (IX_AG)
###i = 10 in MASTER_Basepairs (IX_GA)
###i = 11 in MASTER_Basepairs (X_AG)
###i = 12 in MASTER_Basepairs (X_GA)
###i = 13 in MASTER_Basepairs (XI_AG)
###i = 14 in MASTER_Basepairs (XI_GA)
###i = 15 in MASTER_Basepairs (XII_UU)
###i = 16 in MASTER_Basepairs (XIII_UU)
###i = 17 in MASTER_Basepairs (XIV_CC)
###i = 18 in MASTER_Basepairs (XV_CC)
###i = 19 in MASTER_Basepairs (XVII_CU)
###i = 20 in MASTER_Basepairs (XVII_UC)
###i = 21 in MASTER_Basepairs (XVIII_CU)
###i = 22 in MASTER_Basepairs (XVIII_UC)
###i = 23 in MASTER_Basepairs (XIX_CG_WC)
###i = 24 in MASTER_Basepairs (XIX_GC_WC)
###i = 25 in MASTER_Basepairs (XX_AU_WC)
###i = 26 in MASTER_Basepairs (XX_UA_WC)
###i = 27 in MASTER_Basepairs (XXI_AU)
###i = 28 in MASTER_Basepairs (XXI_UA)
###i = 29 in MASTER_Basepairs (XXII_CG)
###i = 30 in MASTER_Basepairs (XXII_GC)
###i = 31 in MASTER_Basepairs (XXIII_AU)
###i = 32 in MASTER_Basepairs (XXIII_UA)
###i = 33 in MASTER_Basepairs (XXIV_AU)
###i = 34 in MASTER_Basepairs (XXIV_UA)
###i = 35 in MASTER_Basepairs (XXV_AC)
###i = 36 in MASTER_Basepairs (XXV_CA)
###i = 37 in MASTER_Basepairs (XXVI_AC)
###i = 38 in MASTER_Basepairs (XXVI_CA)
###i = 39 in MASTER_Basepairs (XXVII_GU)
###i = 40 in MASTER_Basepairs (XXVII_UG)
###i = 41 in MASTER_Basepairs (XXVIII_GU)
###i = 42 in MASTER_Basepairs (XXVIII_UG)
###i = 44 in MASTER_Basepairs (XXX_CA)

MASTER_Basepairs = []
MASTER_Basepairs_excluded = [] #Will carry all the lines excluded from MASTER_Basepairs because they exceed CUTOFF value. Will be important while attempting to recover missing bonds. Will be important while attempting to recover missing bonds in "SECTION STATISTICS"
#THE BASEPAIR LISTS SECTION: The individual basepair lists in THE BASEPAIR LISTS SECTION start with a roman numeral that defines the basepair according to Saenger.
        #For basepairs formed with bases of different identitiy, two lists are created, depending on what the first base of the pair is (the one with the lower resid # ----> atom_i.resid()).
        #For every basepair in , a short list will also be created that contains:
                #1) number of bonds ---> n,
                #2) resid i (atom_i.resid())
                #3) resid j (atom_j.resid())
                #5) bond atom 1 for bond 1,
                #6) bond atom 2 for bond 1, ...
                #x-1) bond atom 1 for bond n,
                #x) bond atom 2 for bond n.
                #These lists will be appended to MASTER_Basepairs_bonds, which should also have a length of 43 like MASTER_Basepairs. Therefore, the relative positions for all basepair searching parameters in MASTER_Basepairs_bonds can be easily accessed by using their position in MASTER_Basepairs

MASTER_Basepairs_bonds = []


########################### Basepair Lists SECTION #################################
##### bonds[0] = # of bonds for current geometry
##### bonds[1] = Resid name for base 1
##### bonds[2] = Resid name for base 2
##### bonds[3] = atom name in bond #1 for base #1
##### bonds[4] = atom name in bond #1 for base #2
##### bonds[5] = atom name in bond #2 for base #1
##### bonds[6] = atom name in bond #2 for base #2
##### bonds[7] = atom name in bond #3 for base #1, NA if only 2 bonds for current geometry
##### bonds[8] = atom name in bond #3 for base #2, NA if only 2 bonds for current geometry
##### bonds[9] = average P-P distance for current geometry, NA if not determined
##### bonds[10] = standard deviation P-P distance for current geometry, NA if not determined
##### bonds[11] = average C1-C1 distance for current geometry, NA if not determined
##### bonds[12] = standard deviation C1-C1 distance for current geometry, NA if not determined


##### Homo purine
###i = 0 in MASTER_Basepairs (I_AA)
I_AA = []
#I Homo purine, Base-pairing pattern AA: AA_2
#Bond   A       A       Length Ave      Length Std      Attribute
#1      N1      N6      2.92    0.14    T
#2      N6      N1      3.06    0.14    T
bonds = [2, "A", "A", "N1", "N6", "N6", "N1", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 1 in MASTER_Basepairs (II_AA)
II_AA = []
#II Base-pairing pattern AA: AA_9
#Bond   A       A       Length Ave      Length Std      Attribute
#1      N6      N7      3.01    0.20    T
#2      N7      N6      2.89    0.21    T
bonds = [2, "A", "A", "N6", "N7", "N7", "N6", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 2 in MASTER_Basepairs (III_GG)
III_GG = []
#III Base-pairing pattern GG: GG_90
#Bond   G       G       Length Ave      Length Std      Attribute
#1      N1      O6      2.85    0.12    T
#2      O6      N1      2.88    0.12    T
bonds = [2, "G", "G", "N1", "O6", "O6", "N1", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 3 in MASTER_Basepairs (IV_GG)
IV_GG = []
#IV Base-pairing pattern GG: GG_109
#Bond   G       G       Length Ave      Length Std      Attribute
#1      N2      N3      2.96    0.29    T
#2      N3      N2      3.23    0.15    T
bonds = [2, "G", "G", "N2", "N3", "N3", "N2", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 4 MASTER_Basepairs (V_AA)
V_AA = [] #V Base-pairing pattern AA: AA_4
#Bond   A       A       Length Ave      Length Std      Attribute
#1      N1      N6      2.93    0.18    T
#2      N6      N7      3.07    0.17    T
#STATISTICAL DATA obtained from 2J02.pdb, 5 basepairs
bonds = [2, "A", "A", "N1", "N6", "N6", "N7", "NA", "NA", 12.63, 0.964, 12.20, 0.220]
MASTER_Basepairs_bonds.append(bonds)
###i = 5 in MASTER_Basepairs (VI_GG)
VI_GG = [] #VI Base-pairing pattern GG: GG_16
#Bond   G       G       Length Ave      Length Std      Attribute
#1      N1      O6      2.88    0.16    T
#2      N2      N7      2.91    0.15    T
bonds = [2, "G", "G", "N1", "O6", "N2", "N7", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 6 in MASTER_Basepairs (VII_GG)
VII_GG = [] #VII Base-pairing pattern GG: GG_21
#Bond   G       G       Length Ave      Length Std      Attribute
#1      N1      N7      2.93    0.17    T
#2      N2      O6      2.76    0.24    T
bonds = [2, "G", "G", "N1", "N7", "N2", "O6", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)

#####Hetero Purine
###i = 7 in MASTER_Basepairs (VIII_AG)
VIII_AG = [] #VIII (AG Imino). Base-pairing pattern AG: AG_3
#Bond   A       G       Length Ave      Length Std      Attribute
#1      N1      N1      2.88    0.15    T
#2      N6      O6      2.95    0.21    TI
#STATISTICAL DATA obtained from 2J02.pdb, 11 basepairs. Combined VIII_AG and VIII_GA. Needs to be revised
bonds = [2, "A", "G", "N1", "N1", "N6", "O6", "NA", "NA", 19.598, 1.085, 12.795, 0.222]
MASTER_Basepairs_bonds.append(bonds)
###i = 8 in MASTER_Basepairs (VIII_GA)
VIII_GA = [] #VIII (GA Imino). Base-pairing pattern GA: GA_3
#Bond   G       A       Length Ave      Length Std      Attribute
#1      N1      N1      2.88    0.15    T
#2      O6      N6      2.95    0.21    T
#STATISTICAL DATA obtained from 2J02.pdb, 11 basepairs. Combined VIII_AG and VIII_GA. Needs to be revised
bonds = [2, "G", "A", "N1", "N1", "O6", "N6", "NA", "NA", 19.598, 1.085, 12.795, 0.222]
MASTER_Basepairs_bonds.append(bonds)
###i = 9 in MASTER_Basepairs (IX_AG)
IX_AG = [] #IX Base-pairing pattern AG: AG_81
#Bond   A       G       Length Ave      Length Std      Attribute
#1      N7      N1      3.08    0.08    T
#2      N6      O6      2.70    0.09    T
bonds = [2, "A", "G", "N7", "N1", "N6", "O6", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 10 in MASTER_Basepairs (IX_GA)
IX_GA = [] #IX Base-pairing pattern GA: GA_81
#Bond   G       A       Length Ave      Length Std      Attribute
#1      N1      N7      3.08    0.08    T
#2      O6      N6      2.70    0.09    T
bonds = [2, "G", "A", "N1", "N7", "O6", "N6", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 11 in MASTER_Basepairs (X_AG)
X_AG = [] #X Base-pairing pattern AG: AG_38
#Bond   A       G       Length Ave      Length Std      Attribute
#1      N1      N2      3.02    0.19    T
#2      N6      N3      3.15    0.11    T
bonds = [2, "A", "G", "N1", "N2", "N6", "N3", "NA", "NA", 15.214, 0.258, 9.792, 0.410]
#STATISTICAL DATA obtained from 2J02.pdb, 2 basepairs. Combined X_AG and X_GA. Needs to be revised
MASTER_Basepairs_bonds.append(bonds)
###i = 12 in MASTER_Basepairs (X_GA)
X_GA = [] #X Base-pairing pattern GA: GA_38
#Bond   G       A       Length Ave      Length Std      Attribute
#1      N2      N1      3.02    0.19    T
#2      N3      N6      3.15    0.11    T
bonds = [2, "G", "A", "N2", "N1", "N3", "N6", "NA", "NA", 15.214, 0.258, 9.792, 0.410]
#STATISTICAL DATA obtained from 2J02.pdb, 2 basepairs. Combined X_AG and X_GA. Needs to be revised
MASTER_Basepairs_bonds.append(bonds)
###i = 13 in MASTER_Basepairs (XI_AG)
XI_AG = [] #XI (AG Sheared). Base-pairing pattern AG: AG_24
#Bond   A       G       Length Ave      Length Std      Attribute
#1      N7      N2      3.075           0.236           T
#2      N6      N3      3.132           0.227           T
#STATISTICAL DATA obtained from 2J02.pdb, 10 basepairs
bonds = [2, "A", "G", "N7", "N2", "N6", "N3", "NA", "NA", 16.04, 1.262, 9.512, 0.399]
MASTER_Basepairs_bonds.append(bonds)
###i = 14 in MASTER_Basepairs (XI_GA)
XI_GA = [] #XI (GA Sheared). Base-pairing pattern GA: GA_24
#Bond   G       A       Length Ave      Length Std      Attribute
#1      N2      N7      2.966           0.248           T
#2      N3      N6      3.201           0.225           T
#STATISTICAL DATA obtained from 2J02.pdb, 9 basepairs
bonds = [2, "G", "A", "N2", "N7", "N3", "N6", "NA", "NA", 15.45, 1.321, 9.523, 0.367]
MASTER_Basepairs_bonds.append(bonds)

#####Homo pyrimidine
###i = 15 in MASTER_Basepairs (XII_UU)
XII_UU = [] #XII Base-pairing pattern UU: UU_20
#Bond   U       U       Length Ave      Length Std      Attribute
#1      N3      O4      2.98    0.06    T
#2      O4      N3      2.74    0.05    T
bonds = [2, "U", "U", "N3", "O4", "O4", "N3", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 16 in MASTER_Basepairs (XIII_UU)
XIII_UU = [] #XIII Base-pairing pattern UU: UU_11
#Bond   U       U       Length Ave      Length Std      Attribute
#1      N3      O2      2.70    0.08    T
#2      O2      N3      2.77    0.15    T
bonds = [2, "U", "U", "N3", "O2", "O2", "N3", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 17 in MASTER_Basepairs (XIV_CC)
XIV_CC = [] #XIV Base-pairing pattern CC: CC_29
#Bond   C       C       Length Ave      Length Std      Attribute
#1      N3      N4      3.34    0.00    T
#2      N4      N3      2.09    0.00    T
bonds = [2, "C", "C", "N3", "N4", "N4", "N3", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 18 in MASTER_Basepairs (XV_CC)
XV_CC = [] #Base-pairing pattern CC: CC_6
#Bond   C       C       Length Ave      Length Std      Attribute
#1      N3      N3      2.92    0.04    P
#2      N4      O2      2.83    0.01    T
#3      O2      N4      3.12    0.07    T
bonds = [3, "C", "C", "N3", "N3", "N4", "O2", "O2", "N4", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)

#####Hetero pyrimidine
###i = 19 in MASTER_Basepairs (XVII_CU)
XVII_CU = [] #Base-pairing pattern CU: CU_35
#Bond   C       U       Length Ave      Length Std      Attribute
#1      N3      N3      2.98    0.05    T
#2      N4      O2      2.91    0.07    T
#3      O2      O4      3.18    0.06    P
bonds = [3, "C", "U", "N3", "N3", "N4", "O2", "O2", "O4", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 20 in MASTER_Basepairs (XVII_UC)
XVII_UC = [] #Base-pairing pattern UC: UC_35
#Bond   U       C       Length Ave      Length Std      Attribute
#1      N3      N3      2.98    0.05    T
#2      O2      N4      2.91    0.07    T
#3      O4      O2      3.18    0.06    P
bonds = [3, "U", "C", "N3", "N3", "O2", "N4", "O4", "O2", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 21 in MASTER_Basepairs (XVIII_CU)
XVIII_CU = [] #Base-pairing pattern CU: CU_36
#Bond   C       U       Length Ave      Length Std      Attribute
#1      N3      N3      3.16    0.06    T
#2      N4      O4      3.19    0.07    T
#STATISTICAL DATA obtained from 2J02.pdb and 2J01.pdb, 3 basepairs of the XVIII_CU and XVIII_UC types. Needs to be rechecked
bonds = [2, "C", "U", "N3", "N3", "N4", "O4", "NA", "NA", 17.339, 0.51, 8.967, 0.587]
MASTER_Basepairs_bonds.append(bonds)
###i = 22 in MASTER_Basepairs (XVIII_UC)
XVIII_UC = [] #Base-pairing pattern UC: UC_36
#Bond   U       C       Length Ave      Length Std      Attribute
#1      N3      N3      3.16    0.06    T
#2      O4      N4      3.19    0.07    T
#STATISTICAL DATA obtained from 2J02.pdb and 2J01.pdb, 3 basepairs of the XVIII_CU and XVIII_UC types. Need4 to be rechecked
bonds = [2, "U", "C", "N3", "N3", "O4", "N4", "NA", "NA", 17.339, 0.51, 8.967, 0.587]
MASTER_Basepairs_bonds.append(bonds)

#####Purine pyrimidine
###i = 23 in MASTER_Basepairs (XIX_CG_WC)
XIX_CG_WC = [] #XIX (Watson-Crick CG)
#Bond    C       G       Length Ave      Length Std      Attribute
#1       N3      N1      3.039           0.195           T
#2       O2      N2      2.931           0.300           T
#3       N4      O6      3.075           0.310           T
#STATISTICAL DATA obtained from 2J02.pdb, 143 basepairs
#bonds = [3, "C", "G", "N3", "N1", "O2", "N2", "N4", "O6", 18.58, 0.900, 10.69, 0.241]
#STATISTICAL DATA obtained from 4TNA.pdb, 6 basepairs
bonds = [3, "C", "G", "N3", "N1", "O2", "N2", "N4", "O6", 18.62, 1.145, 10.87, 0.436]
MASTER_Basepairs_bonds.append(bonds)
###i = 24 in MASTER_Basepairs (XIX_GC_WC)
XIX_GC_WC = [] #XIX (Watson-Crick GC)
#Bond    G       C       Length Ave      Length Std      Attribute
#1       N1      N3      3.005           0.206           T
#2       N2      O2      2.922           0.267           T
#3       O6      N4      3.029           0.326           T
#STATISTICAL DATA obtained from 2J02.pdb, 157 basepairs
#bonds = [3, "G", "C", "N1", "N3", "N2", "O2", "O6", "N4", 18.49, 0.742, 10.64, 0.257]
#STATISTICAL DATA obtained from 4TNA.pdb, 7 basepairs
bonds = [3, "G", "C", "N1", "N3", "N2", "O2", "O6", "N4", 17.89, 1.160, 10.69, 0.390]
MASTER_Basepairs_bonds.append(bonds)
###i = 25 in MASTER_Basepairs (XX_AU_WC)
XX_AU_WC = [] #XX (Watson-Crick AU). Base-pairing pattern AU: AU_2
#Bond   A       U       Length Ave      Length Std      Attribute
#1      N1      N3      2.927           0.216           T
#2      N6      O4      3.032           0.241           T
#STATISTICAL DATA obtained from 2J02.pdb, 21 basepairs
#bonds = [2, "A", "U", "N1", "N3", "N6", "O4", "NA", "NA", 18.68, 0.269, 10.67, 0.27]
#STATISTICAL DATA obtained from 4TNA.pdb, 7 basepairs (AU and UA combined)
bonds = [2, "A", "U", "N1", "N3", "N6", "O4", "NA", "NA", 18.46, 0.258, 10.726, 0.270]
MASTER_Basepairs_bonds.append(bonds)
###i = 26 in MASTER_Basepairs (XX_UA_WC)
XX_UA_WC = [] #XX (Watson-Crick UA). Base-pairing pattern UA: UA_2
#Bond   U       A       Length Ave      Length Std      Attribute
#1      N3      N1      2.974           0.188           T
#2      O4      N6      3.140           0.269           T
#STATISTICAL DATA obtained from 2J02.pdb, 30 basepairs
#bonds = [2, "U", "A", "N3", "N1", "O4", "N6", "NA", "NA", 18.60, 0.816, 10.55, 0.298]
#STATISTICAL DATA obtained from 4TNA.pdb, 7 basepairs (AU and UA combined)
bonds = [2, "U", "A", "N3", "N1", "O4", "N6", "NA", "NA", 18.46, 0.258, 10.726, 0.270]
MASTER_Basepairs_bonds.append(bonds)
###i = 27 in MASTER_Basepairs (XXI_AU)
XXI_AU = [] #XXI (AU Reversed Watson-Crick). Base-pairing pattern AU: AU_30
#Bond   A       U       Length Ave      Length Std      Attribute
#1      N1      N3      2.84    0.13    T
#2      N6      O2      2.94    0.17    T
#STATISTICAL DATA obtained from 2J02.pdb, 5 combined basepairs of the types XXI_AU and XXI_UA. Should be rechecked
bonds = [2, "A", "U", "N1", "N3", "N6", "O2", "NA", "NA", 16.73, 2.117, 10.97, 0.109]
MASTER_Basepairs_bonds.append(bonds)
###i = 28 in MASTER_Basepairs (XXI_UA)
XXI_UA = [] #XXI (UA Reversed Watson-Crick). Base-pairing pattern UA: UA_30
#Bond   U       A       Length Ave      Length Std      Attribute
#1      N3      N1      2.84    0.13    T
#2      O2      N6      2.94    0.17    T
#STATISTICAL DATA obtained from 2J02.pdb, 5 combined basepairs of the types XXI_AU and XXI_UA. Should be rechecked
bonds = [2, "U", "A", "N3", "N1", "O2", "N6", "NA", "NA", 16.73, 2.117, 10.97, 0.109]
MASTER_Basepairs_bonds.append(bonds)
###i = 29 in MASTER_Basepairs (XXII_CG)
XXII_CG = [] #XXII (CG Reversed Watson-Crick). Base-pairing pattern CG: CG_31
#Bond   C       G       Length Ave      Length Std      Attribute
#1      O2      N1      2.80    0.17    T
#2      N3      N2      2.86    0.18    T
#STATISTICAL DATA obtained from 2J01.pdb, 6 basepairs of the types XXII_CG and XXII_GC combined
bonds = [2, "C", "G", "O2", "N1", "N3", "N2", "NA", "NA", 17.404, 1.802, 10.35, 0.412]
MASTER_Basepairs_bonds.append(bonds)
###i = 30 in MASTER_Basepairs (XXII_GC)
XXII_GC = [] #XXII (GC Reversed Watson-Crick). Base-pairing pattern GC: GC_31
#Bond   G       C       Length Ave      Length Std      Attribute
#1      N1      O2      2.80    0.17    T
#2      N2      N3      2.86    0.18    T
bonds = [2, "G", "C", "N1", "O2", "N2", "N3", "NA", "NA", 17.404, 1.802, 10.35, 0.412]
MASTER_Basepairs_bonds.append(bonds)
###i = 31 in MASTER_Basepairs (XXIII_AU)
XXIII_AU = [] #XXIII (AU Hoogsteen). Base-pairing pattern AU: AU_29
#Bond   A       U       Length Ave      Length Std      Attribute
#1      N6      O4      3.05    0.15    T
#2      N7      N3      2.96    0.15    T
#STATISTICAL DATA obtained from 2J02.pdb, 4 basepairs of the type XXIII_UA. Should be rechecked
bonds = [2, "A", "U", "N6", "O4", "N7", "N3", "NA", "NA", 10.89, 0.436, 8.208, 0.260]
MASTER_Basepairs_bonds.append(bonds)
###i = 32 in MASTER_Basepairs (XXIII_UA)
XXIII_UA = [] #XXIII (UA Hoogsteen). Base-pairing pattern UA: UA_29
#Bond   U       A       Length Ave      Length Std      Attribute
#1      O4      N6      3.05    0.15    T
#2      N3      N7      2.96    0.15    T
#STATISTICAL DATA obtained from 2J02.pdb, 4 basepairs.
bonds = [2, "U", "A", "O4", "N6", "N3", "N7", "NA", "NA", 10.89, 0.436, 8.208, 0.260]
MASTER_Basepairs_bonds.append(bonds)
###i = 33 in MASTER_Basepairs (XXIV_AU)
XXIV_AU = [] #XXIV (AU Reversed Hoogsteen). Base-pairing pattern AU: AU_14
#Bond   A       U       Length Ave      Length Std      Attribute
#1      N6      O2      2.838           0.191           T
#2      N7      N3      2.943           0.048           T
#STATISTICAL DATA obtained from 2J02.pdb,7 basepairs
bonds = [2, "A", "U", "N6", "O2", "N7", "N3", "NA", "NA", 12.24, 1.077, 9.766, 0.205]
MASTER_Basepairs_bonds.append(bonds)
###i = 34 in MASTER_Basepairs (XXIV_UA)
XXIV_UA = [] #XXIV (UA Reversed Hoogsteen). Base-pairing patterMASTER_Basepairs_bonds.append(bonds)
#Bond   U       A       Length Ave      Length Std      Attribute
#1      O2      N6      3.002 SD        0.260           T
#2      N3      N7      2.935           0.249           T
#STATISTICAL DATA obtained from 2J02.pdb,11 basepairs
bonds = [2, "U", "A", "O2", "N6", "N3", "N7", "NA", "NA", 12.20, 1.556, 9.689, 0.334]
MASTER_Basepairs_bonds.append(bonds)
###i = 35 in MASTER_Basepairs (XXV_AC)
XXV_AC = [] #XXV (AC Reversed Hoogsteen). Base-pairing pattern AC: AC_7
#Bond   A       C       Length Ave      Length Std      Attribute
#1      N6      N3      3.355           0.198           T
#2      N7      N4      3.263           0.213           T
#STATISTICAL DATA obtained from 2J02.pdb, 3 basepairs(but compare to XXV_CA)
bonds = [2, "A", "C", "N6", "N3", "N7", "N4", "NA", "NA", 13.02, 0.814, 11.26, 0.228]
MASTER_Basepairs_bonds.append(bonds)
###i = 36 in MASTER_Basepairs (XXV_CA)
XXV_CA = [] #XXV (CA Reversed Hoogsteen). Base-pairing pattern CA: CA_7
#Bond   C       A       Length Ave      Length Std      Attribute
#1      N3      N6      3.067           0.202           T
#2      N4      N7      2.980           0.213           T
#STATISTICAL DATA obtained from 2J02.pdb, 5 basepairs
bonds = [2, "C", "A", "N3", "N6", "N4", "N7", "NA", "NA", 13.10, 1.713, 11.24, 0.149]
MASTER_Basepairs_bonds.append(bonds)
###i = 37 in MASTER_Basepairs (XXVI_AC)
XXVI_AC = [] #XXVI (AC Reversed Wobble). Base-pairing pattern AC: AC_42
#Bond   A       C       Length Ave      Length Std      Attribute
#1      N1      N4      3.01    0.08    T
#2      N6      N3      3.09    0.14    T
bonds = [2, "A", "C", "N1", "N4", "N6", "N3", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 38 in MASTER_Basepairs (XXVI_CA)
XXVI_CA = [] #XXVI (CA Reversed Wobble). Base-pairing pattern CA: CA_42
#Bond   C       A       Length Ave      Length Std      Attribute
#1      N4      N1      3.01    0.08    T
#2      N3      N6      3.09    0.14    T
bonds = [2, "C", "A", "N4", "N1", "N3", "N6", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 39 in MASTER_Basepairs (XXVII_GU)
XXVII_GU = [] #XXVII (Reversed GU Wobble). Base-pairing pattern GU: GU_29
#Bond   G       U       Length Ave      Length Std      Attribute
#1      N1      O4      2.99    0.07    T
#2      O6      N3      2.99    0.15    T
bonds = [2, "G", "U", "N1", "O4", "O6", "N3", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 40 in MASTER_Basepairs (XXVII_UG)
XXVII_UG = [] #XXVII (Reversed UG Wobble). Base-pairing pattern UG: UG_29
#Bond   U       G       Length Ave      Length Std      Attribute
#1      O4      N1      2.99    0.07    T
#2      N3      O6      2.99    0.15    T
bonds = [2, "U", "G", "O4", "N1", "N3", "O6", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 41 in MASTER_Basepairs (XXVIII_GU)
XXVIII_GU = [] #XXVIII (GU Wobble). Base-pairing pattern GU: GU_1
#Bond   G       U       Length Ave      Length Std      Attribute
#1      N1      O2      2.885           0.238           T
#2      O6      N3      2.846           0.213           T
#STATISTICAL DATA obtained from 2J02.pdb, 21 basepairs
bonds = [2, "G", "U", "N1", "O2", "O6", "N3", "NA", "NA", 18.45, 0.896, 10.44, 0.245]
MASTER_Basepairs_bonds.append(bonds)
###i = 42 in MASTER_Basepairs (XXVIII_UG)
XXVIII_UG = [] #XXVIII (UG Wobble). Base-pairing pattern UG: UG_1
#Bond   U       G       Length Ave      Length Std      Attribute
#1      O2      N1      2.932           0.214           T
#2      N3      O6      2.846           0.201           T
#STATISTICAL DATA obtained from 2J02.pdb, 22 basepairs
bonds = [2, "U", "G", "O2", "N1", "N3", "O6", "NA", "NA", 18.38, 0.807, 10.49, 0.213]
MASTER_Basepairs_bonds.append(bonds)

####ADDITIONAL BASEPAIRS, NOT IN SAENGER'S COMPILATION
###i = 43 in MASTER_Basepairs (XXIX_AC)
XXIX_AC = [] #XXIX (AC Wobble). Base-pairing pattern AC
#Bond   A       C       Length Ave      Length Std      Attribute
#1      N6      N3      unk             unk
#2      N1      O2      unk             unk
bonds = [2, "A", "C", "N6", "N3", "N1", "O2", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 44 in MASTER_Basepairs (XXX_CA)
XXIX_CA = [] #XXIX (CA Wobble). Base-pairing pattern CA
#Bond   C       A       Length Ave      Length Std      Attribute
#1      N3      N6      unk             unk
#2      O2      N1      unk             unk
bonds = [2, "A", "C", "N3", "N6", "O2", "N1", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 45 in MASTER_Basepairs (XXXI_GC)
XXX_GC = [] #XXX Base-pair between positions G1034 and C1028 of T. thermophilus 16S rRNA (E. coli numbering)
#Bond   G       C       Length Ave      Length Std      Attribute
#1      N2      N4      3.1             unk
#2      N3      N3      3.2             unk
#Not totally sure it is a real basepair
bonds = [2, "G", "C", "N2", "N4", "N3", "N3", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 46 in MASTER_Basepairs (XXXII_CG)
XXX_CG = [] #XXX Base-pair between positions G1034 and C1028 of T. thermophilus 16S rRNA (E. coli numbering)
#Bond   C       G       Length Ave      Length Std      Attribute
#1      N4      N2      3.1             unk
#2      N3      N3      3.2             unk
#Not totally sure it is a real basepair
bonds = [2, "C", "G", "N4", "N2", "N3", "N3", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 47 in MASTER_Basepairs (XXXIII_GG)
XXXI_GG = [] #XXXI Base-pair between positions G1030A and C1031 of T. thermophilus 16S rRNA (E. coli numbering)
#Bond   G       G       Length Ave      Length Std      Attribute
#1      N2      N1      2.9             unk
#2      N3      N2      3.4             unk
bonds = [2, "G", "G", "N2", "N1", "N3", "N2", "NA", "NA", "NA", "NA", "NA", "NA"]
MASTER_Basepairs_bonds.append(bonds)
###i = 48 in MASTER_Basepairs (XXXIV_A_PSU)
XXXIV_A_PSU = [] #present in 4TNA (1 base pair) and 2KYE (1 base pair 20 different NMR structures)
#Bond   A       U       Length Ave      Length Std      Attribute
#1      N1      N3      3.1             unk
#2      N6      O2      3.2             unk
bonds = [2, "A", "U", "N1", "N3", "N6", "O2", "NA", "NA", 16.942, 0.520, 11.109, 0.220]
MASTER_Basepairs_bonds.append(bonds)
###i = 49 in MASTER_Basepairs (XXXIV_PSU_A)
XXXIV_PSU_A = [] #present in 4TNA (1 base pair) and 2KYE (1 base pair 20 different NMR structures)
#Bond   U       A       Length Ave      Length Std      Attribute
#1      N3      N1      3.1             unk
#2      O2      N6      3.2             unk
bonds = [2, "U", "A", "N3", "N1", "O2", "N62", "NA", "NA", 16.942, 0.520, 11.109, 0.220]
MASTER_Basepairs_bonds.append(bonds)


########################### Basepair Lists SECTION #################################

#MASTER_Basepairs will be loaded after the sequential reading of First_List. This list is the output of FUNCTION "run(args)" with the lines carrying possible basepair bond information given the CUTOFF (search CUTOFF)
        #every MASTER_Basepair[i] below will be assigned as many [j] sublists as possible bonds are identified. MASTER_Basepair[i][j] will be composed of
                #k=0, atom_i.resid()
                #k=1, resname_i
                #k=2, atmname_i
                #k=3, atom_j.resid()
                #k=4, resname_j
                #k=5, atmname_j
                #k=6, distance


MASTER_Basepairs_excluded = [[], [], [], [], [], [], [], [], [], [],  [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]

#MASTER_Basepairs_schemes only carries strings with the names of the basepairing Schemes
MASTER_Basepairs_schemes = ["I_AA", "II_AA", "III_GG", "IV_GG", "V_AA", "VI_GG", "VII_GG", "VIII_AG", "VIII_GA", "IX_AG",  "IX_GA", "X_AG", "X_GA", "XI_AG", "XI_GA", "XII_UU", "XIII_UU", "XIV_CC", "XV_CC", "XVII_CU", "XVII_UC", "XVIII_CU", "XVIII_UC", "XIX_CG_WC", "XIX_GC_WC", "XX_AU_WC", "XX_UA_WC", "XXI_AU", "XXI_UA", "XXII_CG", "XXII_GC", "XXIII_AU", "XXIII_UA", "XXIV_AU", "XXIV_UA", "XXV_AC",  "XXV_CA", "XXVI_AC", "XXVI_CA", "XXVII_GU", "XXVII_UG", "XXVIII_GU", "XXVIII_UG", "XXIX_AC", "XXIX_CA", "XXX_GC", "XXX_CG", "XXXI_GG", "XXXIV_A_PSU", "XXXIV_PSU_A"]

#MASTER_Basepairs_summary: The lines in MASTER_Basepairs, carrying possible basepairing bonds, will be sorted into this list, once they have been confirmed as possible basepairs. Same format as MASTER_Basepairs
MASTER_Basepairs_summary = [[], [], [], [], [], [], [], [], [], [],  [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]


##########################################################################
##########################################################################
##########################################################################
#SECTION: LIST DEFINITION                                       END
##########################################################################
##########################################################################
##########################################################################


##########################################################################
##########################################################################
##########################################################################
#SECTION FUNCTIONS                                         BEGINNING
##########################################################################
##########################################################################
##########################################################################

################################################
################################################
#FUNCTION "run(args)"                BEGINNING
################################################
################################################
#FUNCTION run(args) was created by Ralf W. Grosse-Kunstle and provides a list of ATOM-to-ATOM distances calculated from a .pdb file.


def pair_sym_table_as_antons_master(
      master,
      unit_cell,
      pdb_atoms,
      sites_frac,
      pair_sym_table,
      reindexing_array,
      omit_symmetry_interactions=True):
  for table_i_seq,pair_sym_dict in enumerate(pair_sym_table):
    i_seq = reindexing_array[table_i_seq]
    site_i = sites_frac[i_seq]
    atom_i = pdb_atoms[i_seq]
    resname_i = atom_i.resname
    atmname_i = atom_i.name
    for table_j_seq,sym_ops in pair_sym_dict.items():
      j_seq = reindexing_array[table_j_seq]
      site_j = sites_frac[j_seq]
      atom_j = pdb_atoms[j_seq]
      resname_j = atom_j.resname
      atmname_j = atom_j.name
      for sym_op in sym_ops:
        if (omit_symmetry_interactions and not sym_op.is_unit_mx()):
          continue
        site_ji = sym_op * site_j
        distance = unit_cell.distance(site_i, site_ji)
        if atom_i.resid() != atom_j.resid():
           master.append([
             atom_i.resid(),
             resname_i,
             atmname_i,
             atom_j.resid(),
             resname_j,
             atmname_j,
             distance])



def run(args):
  assert len(args) == 1
  import iotbx.pdb
  pdb_inp = iotbx.pdb.input(file_name=args[0])
  crystal_symmetry = pdb_inp.crystal_symmetry()
  sites_cart = pdb_inp.atoms().extract_xyz()
  if (crystal_symmetry is not None):
    unit_cell = crystal_symmetry.unit_cell()
  else:
    from cctbx import crystal, uctbx
    unit_cell = uctbx.non_crystallographic_unit_cell(sites_cart=sites_cart)
    crystal_symmetry = crystal.symmetry(
      unit_cell=unit_cell, space_group_symbol="P1")
  sites_frac = unit_cell.fractionalization_matrix() * sites_cart
  #
  from cctbx.array_family import flex
  pair_sym_table = crystal_symmetry.special_position_settings() \
    .pair_asu_table(
      distance_cutoff=5.0,
      sites_frac=sites_frac).extract_pair_sym_table()
  pdb_atoms = pdb_inp.atoms_with_labels()
  master = [args[0]]
  pair_sym_table_as_antons_master(
    master=master,
    unit_cell=unit_cell,
    pdb_atoms=pdb_atoms,
    sites_frac=sites_frac,
    pair_sym_table=pair_sym_table,
    reindexing_array=flex.size_t_range(sites_frac.size()))
  #
  p_selection = pdb_inp.atoms().extract_name() == " P  "
  p_pair_sym_table = crystal_symmetry.special_position_settings() \
    .pair_asu_table(
      distance_cutoff=25.0,
      sites_frac=sites_frac.select(p_selection)).extract_pair_sym_table()
  pair_sym_table_as_antons_master(
    master=master,
    unit_cell=unit_cell,
    pdb_atoms=pdb_atoms,
    sites_frac=sites_frac,
    pair_sym_table=p_pair_sym_table,
    reindexing_array=p_selection.iselection())
  #
  c1_selection_1 = pdb_inp.atoms().extract_name() ==  " C1*"
  c1_pair_sym_table = crystal_symmetry.special_position_settings() \
    .pair_asu_table(
      distance_cutoff=22.0,
      sites_frac=sites_frac.select(c1_selection_1)).extract_pair_sym_table()
  pair_sym_table_as_antons_master(
    master=master,
    unit_cell=unit_cell,
    pdb_atoms=pdb_atoms,
    sites_frac=sites_frac,
    pair_sym_table=c1_pair_sym_table,
    reindexing_array=c1_selection_1.iselection())

  c1_selection_2 = pdb_inp.atoms().extract_name() ==  " C1'"
  c1_pair_sym_table = crystal_symmetry.special_position_settings() \
    .pair_asu_table(
      distance_cutoff=22.0,
      sites_frac=sites_frac.select(c1_selection_2)).extract_pair_sym_table()
  pair_sym_table_as_antons_master(
    master=master,
    unit_cell=unit_cell,
    pdb_atoms=pdb_atoms,
    sites_frac=sites_frac,
    pair_sym_table=c1_pair_sym_table,
    reindexing_array=c1_selection_2.iselection())

  return master
################################################
################################################
#FUNCTION "run(args)"                   END
################################################
################################################



################################################
################################################
#FUNCTION "CONTROL"               BEGINNING
################################################
################################################
#The bases identified with this function cannot be assigned to a single basepair. The lists of bases involved in MULTIPLE CONTACTS (sublists with more than 2 bases in list 'control', as determined in this FUNCTION, can be later deconvouluted by using the CONTINUITY OF HELICITY CRITERION or the ELIMINATION principle. Should the program not be able to deconvolute sublists with more than 2 bases in list 'control', the user will be prompted to check the assignement of these bases in the structure after displaying it in pymol
#Called from "FUNCTION Section: LOOKING FOR BASES INVOLVED IN MULTIPLE CONTACTS"
#FUNCTION "CONTROL" is executed before any basepair is assigned

def CONTROL(control, resid1, resid2, base1, base2):
    count = 0 #see avoiding repetitions
    collect = []
    position_control = ['none', 'none']
    if len(control) == 0: #First couple of bases in the 'control' list
        collect =[resid1, base1, resid2, base2]
    else:   #The 'control' list has already been appended. The next lines will determine which of the 2 bases from the current basepair candidate have been already identified in other possible basepairs
        for j in range (len(control)):
           if (resid1 in control[j]):
              count = count + 1
              position_control[0] = j  #see avoiding repetitions
           if (resid2 in control[j]):
              count = count + 1
              position_control[1] = j  #see avoiding repetitions
    if (count == 0) and (len(control) > 0):
        collect = [resid1, base1, resid2, base2]
#avoiding repetitions
#Both resid1 and resid2 from MASTER_Basepairs_summary[i][j] could be involved in a base pair interaction, other than the one described by MASTER_Basepairs_summary[i][j]. The following lines will avoid repetitions in the 'control' lines
    if count > 0:
        collect = [resid1, base1, resid2, base2]
        if ('none' not in position_control) and (position_control[1] < position_control[0]):
           position_control = [position_control[1], position_control[0]]
        for i in range (len(position_control)):
           if (position_control[i] != 'none'):
               for j in range (len(control[position_control[i]])/2):
                   if (resid1 not in control[position_control[i]][j*2]) and (resid2 not in control[position_control[i]][j*2]):
                      collect.append(control[position_control[i]][j*2])
                      collect.append(control[position_control[i]][(j*2)+1])
               control[position_control[i]] = []
    if len(collect) > 0:
        control.append(collect)
    return control
################################################
################################################
#FUNCTION "CONTROL"                   END
################################################
################################################


################################################
################################################
#FUNCTION "CONVERT"
################################################
################################################
#Converts a list composed of space-containing strings of the form ' resid # ', ' resid ', ' resid # ', ' resid ', etc into a string of the type 'resid' and 'resid #'
def CONVERT(list):
                   count = 2
                   resid = []
                   for i in range (len(list)):
                       resid.append([])
                   resid_count = 0
#                   print "resid", resid, "list", list
                   while count <= len(resid):
                       if list[resid_count] != []:
                           m=re.search(r'\d+', list[resid_count])
                           resid[resid_count] = m.group()
                           n=re.search(r'\w', list[resid_count + 1])
                           resid[resid_count + 1] = n.group()
                           count = count + 2
                           resid_count = resid_count + 2
                       else:
                           count = count + 2
                   return resid
################################################
################################################
#FUNCTION "CONVERT"
################################################
################################################


################################################
################################################
#FUNCTION "PYMOL_OUTPUT"                BEGINNING
################################################
################################################
def PYMOL_OUTPUT(list, h, pdb_file_main, From, file):
#list = new_list_end[b] or
#h = position within the original list from which LIST1 derives
          line = []
          if (h%5)/4 == 1:
             color = "blue"
          elif (h%5)/3 == 1:
             color = "cyan"
          elif (h%5)/2 == 1:
             color = "forest"
          elif (h%5)/1 == 1:
             color = "purple"
          else:
             color = "yellow"
          for k in range (len(file)):
              if file[k] != []:
                 for a in range (1, 3):
                     if From == 'basepairs':
                         m=re.search(r'\d+', list[2 * a])
                         resid = m.group()
                         RESID = str(resid)
                         line = [["color " + color + ", /" + pdb_file_main + "/*/*/" + RESID + '\n',"hide sticks, /" + pdb_file_main + "/*/*/" + RESID + '\n'],["color " + color + ", /" + pdb_file_main + "/*/*/" + RESID + '\n', "show sticks, /" + pdb_file_main + "/*/*/" + RESID + '\n']]
                         for l in range (len(line[k])):
                             file[k].write(line[k][l])
                     else:
                         m=re.search(r'\d+', list[2 * a])
                         resid = m.group()
                         RESID = str(resid)
                         line = "color " + color + ", /" + pdb_file_main + "/*/*/" + RESID + '\n'
                         file[k].write(line)
                         line1 = "show sticks, /" + pdb_file_main + "/*/*/" + RESID + '\n'
                         file[k].write(line1)

###############################################
################################################
#FUNCTION "PYMOL_OUTPUT"                   END
################################################
################################################

###############################################
################################################
#FUNCTION "CLOSE"                   END
################################################
################################################
def CLOSE(file):
   for h in range (len(file)):
      if file[h] != []:
          file[h].close()

###############################################
################################################
#FUNCTION "CLOSE"                   END
################################################
################################################


################################################
################################################
#FUNCTION "SEARCH"
################################################
################################################
def SEARCH(string, search):
   m=re.search(search, string)
   SEARCH = m.group()
   return SEARCH
################################################
################################################
#FUNCTION "SEARCH"
################################################
################################################

################################################
################################################
#FUNCTION "to_int"
################################################
################################################
#Searched in Internet. Converts strings with #s to integers
def to_int(in_str):
    """Converts a string to an integer"""
    out_num = 0

    if in_str[0] == "-":
        multiplier = -1
        in_str = in_str[1:]
    else:
        multiplier = 1
    for x in range(0, len(in_str)):
        out_num = out_num * 10 + ord(in_str[x]) - ord('0')
    return out_num * multiplier
################################################
################################################
#FUNCTION "to_int"
################################################
################################################

################################################
################################################
#FUNCTION "to_string"
################################################
################################################
#Searched in Internet. Converts #s to strings
def to_string(in_int):
    """Converts an integer to a string"""
    out_str = ""
    prefix = ""
    if in_int < 0:
        prefix = "-"
        in_int = -in_int
    while in_int / 10 != 0:
        out_str = chr(ord('0') + in_int % 10) + out_str
        in_int = in_int / 10
    out_str = chr(ord('0') + in_int % 10) + out_str
    return prefix + out_str
################################################
################################################
#FUNCTION "to_string"
################################################
################################################


################################################
################################################
#FUNCTION "ADD"
################################################
################################################
def ADD(SEARCH1, SEARCH2):
   A = SEARCH1
   A = to_string(A)
   B = SEARCH2
   B = to_string(B)
   a = SEARCH1 + 1
   a = to_string(a)
   b = SEARCH2 - 1
   b = to_string(b)
   c = SEARCH1 - 1
   c = to_string(c)
   d = SEARCH2 + 1
   d = to_string(d)
   return[a, b, c,d]
################################################
################################################
#FUNCTION "ADD"
################################################
################################################


################################################
################################################
#FUNCTION "SEQUENTIAL_READOUT"   BEGINNING
################################################
################################################
######Sequential readout of lines from lists 'control' and 'MASTER_Basepairs_summary'. Not necessary for execution
#LIST1 = MASTER_Basepairs_summary
#LIST2 = MASTER_Basepairs_schemes
#LIST3 = control
def SEQUENTIAL_READOUT(LIST1, LIST2, LIST3, print_control, line2):
    control_control = 1 #Initializing. This variable signals the presence of bases with possible multiple contacts
    print("\n##################################################\n##### SEQUENTIAL OUTPUT READOUT        BEGINNING")
    if print_control == 0: #Controls how much SEQUENTIAL READOUT is outputed
        print("##### LINES CARRYING POSSIBLE BASEPAIR INFORMATION", end=' ')
        print(line2)
        for i in range (len(LIST1)):
            line1 = "Basepairing scheme: " + LIST2[i]
            print(line1)
            for k in range (len(LIST1[i])):
                line1 = ''
                for l in range (len(LIST1[i][k])):
                    a = str(LIST1[i][k][l])
                    line1 = line1 + a + ' '
                print(line1)
        print("#####LINES CARRYING POSSIBLE BASEPAIR INFORMATION", end=' ')
        print(line2)
        print("##################################################")
#OUTPUT regarding MULTIPLE CONTACTS
    print("#################################################")
    print("####LIST OF BASES WITH MULTIPLE POSSIBLE CONTACTS", end=' ')
    print(line2)
    count1 = 0
    if LIST3 != []:
        line3 = ''
        list1 = []
        list = []
        count = 0
        a = ""
        for i in range (len(LIST3)):
            for j in range (len(LIST3[i])/2):
               if LIST3[i][j*2] != []:
                  count = count + 1
                  list.append(LIST3[i][j*2])
                  list.append(LIST3[i][(j*2)+1])
            if len(list) > 4: #Only sublists with more than two ATOMs should be printed
               count1 = count1 + 1
               list = CONVERT(list)
               for k in range (len(list)):
                  a = a + list[k]
                  if k%2 != 0:
                      a = a + ' '
               line3 = a
               list1.append(line3)
               a = ""
            list = []
        line1 = ""
        line2 = ""
        line3 = ""
    if count1 == 0:
        line1 = "NO BASES DETECTED WITH MULTIPLE POSSIBLE CONTACTS\n#################################################"
        print(line1)
        control_control = 0 #Bases with multiple contacts = 0. This will avoid unnecessary runs of ELIMINATION
    else:
        print("##### GROUPS OF BASES DETECTED WITH MULTIPLE POSSIBLE CONTACTS = ", count1)
        for i in range (len(list1)):
            print("GROUP", i + 1, "  ", end=' ')
            print(list1[i])
        print("####LIST OF BASES WITH MULTIPLE POSSIBLE CONTACTS\n#################################################")
    print("##################################################\n###### SEQUENTIAL OUTPUT READOUT          END\n##################################################\n")

    return control_control
#    import sys
#    sys.exit()
######Sequential readout of lines from lists 'control' and 'MASTER_Basepairs_summary'. Not necessary for execution
################################################
################################################
#FUNCTION "SEQUENTIAL_READOUT"   END
################################################
################################################



################################################
################################################
#FUNCTION "FIRST_APPEND"   BEGINNING
################################################
################################################
#This FUNCTION will be used to compare MASTER_Basepairs_summary to 'control' (already loaded), to determine what to do with base pairs involved in MULTIPLE CONTACTS accroding to the criteria chosen.
        #If the base pair is found to be involved in MULTIPLE CONTACTS, a '1' will be appended to its line in MASTER_Basepairs_summary
                #Having a '1' will not affect the assignment of 3-base basepairs for which all bonds have been identified
        #If the base pair is found not to be involved in MULTIPLE CONTACTS, a '0' will be appended to its line in MASTER_Basepairs_summary

def FIRST_APPEND(LIST1, LIST2, max_cutoff, cutoff):
#LIST1 = MASTER_Basepairs_summary[l][m]
#LIST2 = control

               for n in range (len(LIST2)):
                   append = '0'
                   if (len(LIST2[n]) > 4) and ((LIST1[2] in LIST2[n]) or (LIST1[4] in LIST2[n])):
                      append = '1'
                   if '1' in append:
#Base pair candidate is involved in MULTIPLE CONTACTS. Should be appended as 'no' unless it is a 3-base basepair with all bonds identified
                      if (LIST1[1] == 3) and (LIST1[6] == 3):
                          LIST1.append('yes')
                          LIST1.append('1')
                      if (LIST1[1] == 2) and (LIST1[6] == 2):
                          LIST1.append('no')
                          LIST1.append('1')
                      if (LIST1[1] == 3) and (LIST1[6] == 2):
                          LIST1.append('no')
                          LIST1.append('1')
                   elif (LIST1[2] in LIST2[n]) or (LIST1[4] in LIST2[n]):
#Base pair candidate is not involved in MULTIPLE CONTACTS. Should be appended as 'yes' unless it is a 3-base basepair with only 2 bonds identified. These basepair candidates will only be appended as 'yes' when the MAX_CUTOFF is reached
                      if (LIST1[1] == LIST1[6]):
                          LIST1.append('yes')
                          LIST1.append('0')
                      if (LIST1[1] == 3 and LIST1[6] == 2) and (cutoff != max_cutoff):
                          LIST1.append('no')
                          LIST1.append('0')
                      elif (LIST1[1] == 3 and LIST1[6] == 2) and (cutoff == max_cutoff):
                          LIST1.append('yes')
                          LIST1.append('0')

#               print "LIST1\n", LIST1
               return LIST1
################################################
################################################
#FUNCTION "FIRST_APPEND"   END
################################################
################################################


################################################
################################################
#FUNCTION "FUNCTION: BOND_PLACING"   BEGINING
################################################
################################################
#Called from FUNCTION "program"
def BOND_PLACING(LIST1, LIST2, LIST3):
#LIST1 = MASTER_Basepairs_bonds[i]
#LIST2 = MASTER_Basepairs_summary[i][k]
#LIST3 = atom
    #place the atoms involved in the new bond just detected (currently in MASTER_Basepairs[i][j][2] and MASTER_Basepairs[i][j][5]. Distance is in MASTER_Basepairs[i][j][6]) into the proper location within MASTER_Basepairs_summary[i][k] (See MASTER_Basepairs_summary SCHEME)  by first checking the location of these atoms in MASTER_Basepairs_bonds. There are several possibilities
#FOLLOW A LINE
#                                   if ('  12 ' in LIST2) and ('  23 ' in LIST2):
#                                      print "entering BOND_PLACING", "LIST1", LIST1, "\nLIST2", LIST2, "\nLIST3", LIST3
#                                      print "entering BOND_PLACING"
#FOLLOW A LINE

                                   if LIST1[3] == LIST3[0] and LIST1[4] == LIST3[1]:
                                       LIST2[7] = LIST3[0]
                                       LIST2[8] = LIST3[1]
                                       LIST2[9] = LIST3[2]
                                       LIST2[6] = LIST2[6] + 1
                                   elif LIST1[5] == LIST3[0] and LIST1[6] == LIST3[1]:
                                       LIST2[10] = LIST3[0]
                                       LIST2[11] = LIST3[1]
                                       LIST2[12] = LIST3[2]
                                       LIST2[6] = LIST2[6] + 1
                                   elif LIST1[7] == LIST3[0] and LIST1[8] == LIST3[1]:
                                       LIST2[13] = LIST3[0]
                                       LIST2[14] = LIST3[1]
                                       LIST2[15] = LIST3[2]
                                       LIST2[6] = LIST2[6] + 1
#FOLLOW A LINE
#                                   if ('  12 ' in LIST2) and ('  23 ' in LIST2):
#                                      print "exiting BOND_PLACING", "LIST1", LIST1, "\nLIST2", LIST2, "\nLIST3", LIST3
#                                      print "exiting BOND_PLACING"
#FOLLOW A LINE
                                   return LIST2
################################################
################################################
#FUNCTION "FUNCTION: BOND_PLACING"   BEGINING
################################################
################################################

################################################
################################################
#FUNCTION "C1-C1 DISTANCE CRITERION"    END
################################################
################################################
#IMPORTANT NOTE 050310: the C1* to C1* may be a much better criterion, as it seems to be more stable than P-P distance
def C1_C1_DISTANCE(LIST1, LIST2, A0, A1, A2, A3):
#LIST1 = MASTER_Basepairs_summary[i][j]
#LIST2 = MASTER_Basepairs_bonds[i]
#A0 = convert[0]
#A1 = convert[1]
#A2 = convert[2]
#A3 = convert[3]
    print("\n...using the C1\'-C1\' distance criterion")
    if LIST1[16] != []:
       diffP_P = LIST1[16] - LIST2[9]
    else:
       LIST1[16] = LIST2[9] #To avoid crashes when a phosphate is missing, like at the 5' end of the molecule, an average value is given. This will solve problems as long as there is an average value in MASTER_Basepairs_bonds, so it could create problems for rare basepairs, but one would not expect such basepairs at the beginning of helices.
       diffP_P = "NA"
    diffC1_C1 = LIST1[17] - LIST2[11]
    line = "The basepair formed by residues " + A0 + A1 + ":" + A2 + A3 + " displays a " + LIST1[0] + " geometry with a P-P distance of " + str(LIST1[16]) + " and a C1\'-C1\' distance of " + str(LIST1[17]) + ".\n     Empirically determined average C1\'-C1\' distance for this geometry = " + str(LIST2[11]) + " + SD = " + str(LIST2[12]) + "\n     Empirically determined average P-P distance for this geometry = " + str(LIST2[9]) + " + SD = " + str(LIST2[10])
    print(line)
    if abs(diffC1_C1) > 3 * LIST2[12]:
        LIST1.append('REMOVED')
        print("The C1\'-C1\' distance for this candidate basepair is more than 3 times the recorded standard deviation for this geometry. The basepair will be prevented from further processing by apending it as 'REMOVED'")
    LIST1[18:18] = [abs(diffC1_C1)]
    return LIST1
################################################
################################################
#FUNCTION "C1-C1 DISTANCE CRITERION"    END
################################################
################################################


################################################
################################################
#FUNCTION "Loop"             BEGINNING
################################################
################################################

def loop(MASTER_Basepairs_summary, control, loop_control, CUTOFF, positions):

#The first part of the loop will attempt to expand the initial assignment of legitimate basepairs by using the CONTINUITY OF HELICITY CRITERION. Basepairs for which only two bonds have been identified and tagged as 'no' in "SECTION: LOOKING FOR BASES INVOLVED IN MULTIPLE CONTACTS" will be included in this search. More info in FUNCTION Section: Final assignment of Basepairs.

#The second part of the loop will check whether any basepairs identified by the first part of the loop are involved in MULTIPLE CONTACTS. If so, they will be removed from this list and the existence of 'lone pairs' that could indicate additional basepairs will be checked. Assignment of 'lone pairs' as legitimate basepairs will be done if a 2-bond candidate basepair carrying the 2 basesof the lone pair exists. The criterion is called 'ELIMINATION'

   new_basepairs = 0
   CUTOFF_str = str(CUTOFF)
   run = 0 #This variable will count the number of runs of the loop
   count_found = 1 #This variable will ensure that the loop runs until no more additional 'no'-tagged basepairs can be identified as legitimate. Will be reinitialized to 0 within the loop. In addition this variable will keep track of how many additional basepairs are found in each run of the loop
   total_count = 0 #will keep track of the total amount of basepairs identified in the loop



#First part of LOOP:
   while count_found > 0:
        run = run + 1
#        if run == 1:
        count_found = 0 #Needs to be reinitialized at every run of the loop. Will keep track of all basepairs found by first part of the loop.
        run_str = to_string(run)
        line1 = "##################################\n\"Attempting to find additional basepairs by the CONTINUITY OF HELICITY CRITERION\"       Attempt # " + run_str + "\n##################################"
        print(line1)
        transient = []
        #First the positions of adjacent basepairs to those involved in MULTIPLE CONTACTS, are calculated and outputed
        for i in range (len(MASTER_Basepairs_summary)):
            for j in range (len(MASTER_Basepairs_summary[i])):
                MH_yes = 'y'
                if ((MASTER_Basepairs_summary[i][j][1] == 3) and (MASTER_Basepairs_summary[i][j][6] == 2) and (MAX_CUTOFF_str[0:3] not in CUTOFF_str[0:3])) or ('REMOVED' in MASTER_Basepairs_summary[i][j]):
                    MH_yes = 'n' #Three-bond-basepairs for which only two bonds have been identified are prevented from entering the rest of the loop lines until MAX_CUTOFF_str[0:3]  == CUTOFF_str[0:3]
                if len(MASTER_Basepairs_summary[i]) > 0:
                    if ('no' in MASTER_Basepairs_summary[i][j]) and ('yes' not in MASTER_Basepairs_summary[i][j]) and MH_yes == 'y':
                        m=re.search(r'\d+', MASTER_Basepairs_summary[i][j][2])
                        search1 = m.group()
                        SEARCH1 = to_int(search1) #convert string to integers
                        m=re.search(r'\d+', MASTER_Basepairs_summary[i][j][4])
                        search2 = m.group()
                        SEARCH2 = to_int(search2) #convert string to integers
                        list = [MASTER_Basepairs_summary[i][j][2], MASTER_Basepairs_summary[i][j][3], MASTER_Basepairs_summary[i][j][4], MASTER_Basepairs_summary[i][j][5]]
                        convert = CONVERT(list)
                        add = ADD(SEARCH1, SEARCH2)
                        line1 = "\nBasepair tagged for its involvement in MULTIPLE CONTACTS has been detected!!!!!!.\n  The basepair is formed by residues " + convert[0] + convert[1] + " and " + convert[2] + convert[3] + "\n    Attempting to determine whether it could be used to expand the length of an identified helical segment.\n      Searching for the following possible basepair candidates: " + add[0] + " and " + add[1] + " or " + add[2] + " and " + add[3]
                        print(line1)

#Second, the whole MASTER_Basepairs_summary is searched for the existence of either one of the adjacent basepairs calculated above. Done in FUNCTION HELICITY
                        where_from = 'Loop' #Used in SECTION HELICITY to determine what to do whether the program is coming from 'Loop' or 'CONTINUITY_HELICITY_CRITERION', or from '    print "AFTER LIST1", LIST1REMOVAL'
                        helicity = HELICITY(MASTER_Basepairs_summary, add[0], add[1], add[2], add[3], where_from)
                        count_helicity = 0 #Control output in this region
                        for t in range (0,2):
                            if helicity[t] != []:
                                count_helicity = count_helicity + 1  #Signals entering this 'if'
                                str(helicity[t][0])
                                str(helicity[t][1])

#One or the 2 adjacent basepairs is found
                                if count_helicity == 1: #One of the 2 possible adjacent basepairs is found. This is enought to assign the basepair whose assignemnt is in question by 'MH' criterion
                                    count_found = count_found + 1 #Will keep tract of how many basepairs are found. Serves also to keep the loop going
                                    line1  = "     #####\n     !!!!Found an adjacent basepair to residues " + convert[0] + convert[1] + " and "  + convert[2] + convert[3] +  "\n     This basepair is formed by residues " + helicity[t][0] + " and " +  helicity[t][1] + "\n     #####\n    Assigning " + convert[0] + convert[1] + " and "  + convert[2] + convert[3] + " as a legitimate basepair by the CONTINUITY OF HELICITY CRITERION\n"
                                    print(line1)
                                    MASTER_Basepairs_summary[i][j].append('yes')
                                    MASTER_Basepairs_summary[i][j].append('MH')  #'MH'Maximization of Helicity TAG
                                    transient = [MASTER_Basepairs_summary[i][j][2], MASTER_Basepairs_summary[i][j][4]]
                                    positions.append(transient)
                                    CUTOFF_str = repr(CUTOFF) #Converts 'float' to 'str'
                                    MASTER_Basepairs_summary[i][j].append(CUTOFF_str[0:3])
                                if count_helicity == 2: #The second adjacent basepair has been found. The basepair whose assignemnt is in question has already bee assigned when 'count_helicity == 1'. The program will only output the new finding
                                    line1  = "     #####\n     !!!!Found a second adjacent basepair to residues " + convert[0] + convert[1] + " and "  + convert[2] + convert[3] +  "\n     This basepair is formed by residues " + helicity[t][0] + " and " +  helicity[t][1] + "\n     #####\n"
                                    print(line1)

#Neither one of the possible adjacent basepairs was found by the CONTINUITY OF HELICITY CRITERION
                        if count_helicity == 0:
                            line1  = "                     Neither basepair was found!!!!!"
                            print(line1)

#Time to recap
        a = str(count_found)
        b = str(run_str )
        total_count = total_count + count_found
        line1 = "###### Number of basepairs identified by the CONTINUITY OF HELICITY CRITERION = " + a + "      Attempt # " + run_str
        print(line1)

#Second part of LOOP
        #Attempting to find additional basepairs by the 'ELIMINATION' criterion. running REMOVAL in 'semi-bulk' mode by using a terminal 'MH' as a tag.
        removal = []
        if (total_count > 0):
            mode_removal = 1 #"Run FUNCTION REMOVAL" in 'semi-bulk' mode
            CUTOFF_str = str(CUTOFF)
            print("\n##############################\n REMOVING the newly identified basepairs from the list of bases with possible mutliple contacts.", run_number_str, " Distance Cutoff = ", CUTOFF_str[0:3], "\n##############################\n ")
            removal = REMOVAL(MASTER_Basepairs_summary, control, convert[0], convert[2], CUTOFF, mode_removal, positions)
            MASTER_Basepairs_summary = removal[0]
            control = removal[1]
            new_basepairs = removal[2]
            count_found = new_basepairs #If additional basepairs have been assigned by 'ELIMINATION' the Loop should run again
            line1 = "\n\n##### Semi-Bulk Removal"
            print(line1)
            a = str(new_basepairs)
            line1 = a + " additional basepairs have been identified by ELIMINATION"
            print(line1)
            line1 = "##### Semi-Bulk Removal"
            print(line1)

        a = ''

#END OF LOOP
   return [MASTER_Basepairs_summary, control] #The original MASTER_Basepairs_summary and control lists are returned after modification

################################################
################################################
#FUNCTION "Loop"                        END
################################################
################################################


################################################
################################################
#FUNCTION "REMOVAL"                    BEGINNING
################################################
################################################
#Will remove newly identified basepairs from 'control' list, now in LIST2 within FUNCTION REMOVAL, carrying basepairs involved in MULTIPLE CONTACTS.
#After the newly identified basepairs are removed and the removed LIST2 sublists loaded in 'control_removed', the latterlist  is checked for the presence of 'lone pairs' that could represent additional basepairs
#Can be called from:
#1) "FUNCTION Section: Final assignment of Basepairs".
#2) "FUNCTION "Loop""

     #Uses FUNCTION HELICITY and

     #SUBFUNCTION "DEFINING_A_B"
          #Determines what 'a' and 'b' values are used in REMOVAL,
     #SUBFUNCTION "ACTUAL_REMOVAL"
          #Where lists LIST1 and LIST2 will be compared.


#'base_a' and 'base_b' = residues 1 and 2 of the current basepair (only if run in 'semi-bulk' mode)
#Two ways of executing this FUNCTION.
    #In 'semi-bulk mode' ('mode_removal' = 1). When called from "FUNCTION "Loop". It should only remove basepairs tagged as 'MH'
    #In 'bulk' mode ('mode_removal' = 0). For the whole LIST1 = MASTER_Basepairs_summary. When called from "FUNCTION Section: Final assignment of Basepairs"
       #Controlled by the argument 'mode_removal', which may be '1' ('semi-bulk' mode) or '0' ('bulk' mode). Peformed in SUBFUNCTION "DEFINING_A_B" above
       #Bases from legitimate basepairs that had been included in 'control' (LIST2), will be removed from this list. Removed 'control' sublists will be loaded into 'control_removed'.
       #In a second step, the remaining bases, loaded into 'control_removed', will be searched for the presence of 'lone pairs' that may be indicative of additional basepairs in "Searching for 'lone pairs' left over in 'control'"


#Identifying basepairs involved in MULTIPLE CONTACTS.
   #Most of the searching is performed in  ACTUAL_REMOVAL

#LIST1 = MASTER_Basepairs_summary
#LIST2 = control
def REMOVAL(LIST1, LIST2, base_a, base_b, CUTOFF, mode_removal, positions):
            CUTOFF_str = repr(CUTOFF) #Converts 'float' to 'str'
            new_basepairs = 0
            ACTUAL = []
            output_control = 0
            list = [] #Will carry the reminder of LIST2 sublists, once after "removal". Thenthe contents of "list" will be appendedto "control_removed". Has to be defined here and at the beginning of SUBFUNCTION "ACTUAL_REMOVAL"
            control_removed = [] #Will carry the control sublists, now in LIST2 within FUNCTION REMOVAL, after basepairs, identified as legitimate, have been removed

            for t in range (len(LIST2)):
               if len(LIST2[t]) > 4:

                   for u in range (len(LIST2[t])/2):
                      if LIST2[t][u*2] != []:
#                          print "LIST2[t][u*2]", LIST2[t][u*2]
                          for a in range (len(positions)):
#                              print "positions[a]", positions[a]
                              if LIST2[t][u*2] in positions[a]:
                                  line1 = "Residue" + LIST2[t][u*2] + "has been removed from the list of residues involved in MULTIPLE CONTACTS"
#                                  print line1
                                  LIST2[t][u*2] = []
                                  LIST2[t][(u*2)+1] = []


#Once all basepairs have been removed, the modified LIST2 ('control'), will be searched for lines with removed residues. The remaining residues will be transferred to 'control_removed' to search for 'lone pairs'.

            for l in range (len(LIST2)):
                 list = []
                 if LIST2 != []:
                    if [] in LIST2[l]:
#                       print "LIST2[l]", LIST2[l]
                       for m in range (len(LIST2[l])/2):
                          if LIST2[l][m*2] != []:
                             list.append(LIST2[l][m*2])
                             list.append(LIST2[l][(m*2)+1])
                       if list != []:
                          list.append(l) #This will indicate which line of LIST2 ('control') contains possible lone pairs
#                          print "Appending control_removed"
                          control_removed.append(list)
#                          print "control_removed", control_removed

#Searching for 'lone pairs' left over in 'control'
#Now the program searches through 'control_removed' to determine whether any "lone pairs" have been left after the removal process. This event should be indicated by the length of  the list 'control_removed', loaded above
            for i in range (len(control_removed)):
#               print "checking whether removed 'control' lines may carry \"lone pairs\" of bases which could correspond to actual basepairs"
               if len(control_removed[i]) == 5: #resid #1, resid1, resid #2, resid2, 't'or location in LIST2 (control), as calculated above
                   LIST2[control_removed[i][4]] = []
                   convert = CONVERT(control_removed[i])
                   line1 = "\n#####\nFound a lone pair of bases after removing newly identified basepairs. Bases are:\n" + convert[0] + convert[1] + " and " + convert[2] + convert[3] + "\nMatching against basepair list\n##### "
                   print(line1)
                   where_from = 'REMOVAL' #Used in SECTION HELICITY to determine what to do whether the program is coming from 'Loop' or 'CONTINUITY_HELICITY_CRITERION', or from 'REMOVAL'
                   a = "iiiiiiii" #Just to fill up the list of 'HELICITY''s arguments
                   b = "iiiiiiii" #Just to fill up the list of 'HELICITY''s arguments
                   helicity = HELICITY(LIST1, control_removed[i][0], control_removed[i][2], a, b, where_from)
                   line = ""
                   if helicity[2] != []:
                       if (LIST1[helicity[2][0]][helicity[2][1]][1] == 3 and LIST1[helicity[2][0]][helicity[2][1]][6] == 2) and (MAX_CUTOFF_str[0:3] not in CUTOFF_str[0:3]):
#candidate 3-base basepairs for which only 2 bases have been identified will be excluded from final assignment by 'ELIMINATION' until MAX_CUTOFF is reached.
                          line = "However, this is a 3-base basepairs for which only 2 bases have been so far identified, final assignment will await until final distance CUTTOFF is used"
                       else:
                          LIST1[helicity[2][0]][helicity[2][1]].append('yes')
                          LIST1[helicity[2][0]][helicity[2][1]].append('ELI')  #Assignment by 'Elimination' TAG.
                          CUTOFF_str = repr(CUTOFF) #Converts 'float' to 'str'
                          LIST1[helicity[2][0]][helicity[2][1]].append(CUTOFF_str[0:3])
                          new_basepairs = new_basepairs + 1
                       line1 = "     #####\n     Found!!! A basepair between residues " + convert[0] + convert[1] + " and " + convert[2] + convert[3] + " has been identified by the 'ELIMINATION' criterion\n     #####\n"
                   else:
                       line1 = "No basepair was identified"
                   print(line1)
                   print(line)
            return [LIST1, LIST2, new_basepairs]

################################################
################################################
#FUNCTION "REMOVAL"                    END
################################################
################################################


################################################
################################################
#FUNCTION "program"               BEGINNING
################################################
################################################

#This is the main function of the program. It started as the program itself but had to be converted to a function to allow its repetitive iteration
        #Called from 'MAIN'

#First_List = Contains the filtered output from FUNCTION "run(args)". List of ATOM to ATOM distances below a rough cutoff of 5A
def program(First_List, MASTER_Basepairs_summary, CUTOFF, pdb_file_main, control, run_number_str, CUTOFF_str, run_number, positions):

    First_List_smaller = []
    First_List_P = []
    First_List_C1 = []
    for a in range (len(First_List)):
       if First_List[a] != []:
          if (First_List[a][len(First_List[a]) - 1] < 5):
              First_List_smaller.append(First_List[a])
          if ' P  ' in First_List[a][2] and ' P  ' in First_List[a][5]:
              First_List_P.append(First_List[a])
          C1 = [' C1\'', ' C1*']
          if First_List[a][2] in C1 and First_List[a][5] in C1:
#FOLLOW A LINE
#              if ('  12 ' in First_List[a][0] and '  23 ' in First_List[a][3]):
#                 print "First_List[a] ", First_List[a]
#FOLLOW A LINE

              First_List_C1.append(First_List[a])
    First_List = First_List_smaller
    MASTER_Basepairs = [[], [], [], [], [], [], [], [], [], [],  [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]

#####################################
#FUNCTION "Program". Section: First sorting of basepair candidates          BEGINNING
#####################################
    line1 = "####################################\nFirst sorting of basepair candidates: Run # " + run_number_str + " Distance Cutoff = " + CUTOFF_str[0:3] + "\n#####################################"
    print(line1)
    collect = []
    for i in range (1, len(First_List)):
#FOLLOW A LINE
#        if ('  12 ' in First_List[i][0] and '  23 ' in First_List[i][3]):
#            print "/First sorting of basepair candidates:\nFirst_List[i] ", First_List[i]
#FOLLOW A LINE
        if First_List[i][0] != First_List[i][3]:
            for j in range(len(MASTER_Basepairs)):
               #Matching the bases to those of MASTER_Basepairs_bonds
               if (MASTER_Basepairs_bonds[j][1] in First_List[i][1]) and (MASTER_Basepairs_bonds[j][2] in First_List[i][4]):
                   count = 0
                   transient = MASTER_Basepairs_bonds[j][3:9] #Carries all the atoms involved in the basepair scheme
#FOLLOW A LINE
#                   if ('  12 ' in First_List[i][0] and '  23 ' in First_List[i][3]):
#                         print "\nFIRST SORTING: transient", transient, "\nMASTER_Basepairs_bonds[j]", MASTER_Basepairs_bonds[j]
#FOLLOW A LINE
                   #Matching the ATOMS to those of MASTER_Basepairs_bonds
                   for k in range ((len(transient)/2)):
#                       print "transient", transient
                       if (transient[2*k] in First_List[i][2]) and (transient[2*k + 1] in First_List[i][5]) and (First_List[i][6] > PREV_CUTOFF) and (First_List[i][6] <= CUTOFF):
                           collect = [First_List[i][0], First_List[i][1], First_List[i][2], First_List[i][3], First_List[i][4], First_List[i][5], First_List[i][6]]
                           MASTER_Basepairs[j].append(collect)
#FOLLOW A LINE
#                           if ('  12 ' in First_List[i][0] and '  23 ' in First_List[i][3]):
#                               print "\nSORTED IN: MASTER_Basepairs_schemes[j]", MASTER_Basepairs_schemes[j]
#                               print "MASTER_Basepairs[j][len(MASTER_Basepairs[j])-1]", MASTER_Basepairs[j][len(MASTER_Basepairs[j])-1]
#FOLLOW A LINE

                       elif (transient[2*k] in First_List[i][2]) and (transient[2*k + 1] in First_List[i][5]) and (First_List[i][6] > CUTOFF) and (MAX_CUTOFF_str[0:3] in CUTOFF_str[0:3]):
                           collect = [First_List[i][0], First_List[i][1], First_List[i][2], First_List[i][3], First_List[i][4], First_List[i][5], First_List[i][6], MASTER_Basepairs_schemes[j]]
                           MASTER_Basepairs_excluded[j].append(collect)
#FOLLOW A LINE
#                           if ('  12 ' in First_List[i][0] and '  23 ' in First_List[i][3]):
#                               for q in range (len(MASTER_Basepairs_excluded[j])):
#                                  print "\nSORTED OUT: MASTER_Basepairs_excluded[j][q]", MASTER_Basepairs_excluded[j][q]
#                               print "MASTER_Basepairs_excluded[j][len(MASTER_Basepairs_excluded[j])-1]", MASTER_Basepairs_excluded[j][len(MASTER_Basepairs_excluded[j])-1]
#FOLLOW A LINE
                       count = count + 1
                       collect = []

#At this point the list MASTER_Basepairs carries all the lines that are consistent with basepair interactions based only on the identity of the bases involved and the CUTOFF value. To learn which of these correspond to actual basepairs, the program has to determine how many lines per every two identified base candidates are contained in the corresponding basepairing Scheme sub-list. Done in "Second sorting of basepair candidates"
#####################################
#       FUNCTION "Program". Section: First sorting of basepair candidates                        END
#####################################


#####################################
#       FUNCTION "Program". Section: Second sorting of basepair candidates               BEGINNING
#####################################
#To find out how many lines of MASTER_Basepairs correspond to the same candidate basepair, the program will go over every MASTER_Basepairs sublist and assign it to a candidate basepair. This information will be appended to MASTER_Basepairs_summary, which can be accessed like the other MASTER_ lists.
        #MASTER_Basepairs_summary SCHEME: The sub-lists in MASTER_Basepairs_summary will be organized by basepairing scheme [i] and then by basepair candidate [j]. It will carry the following values:
                #position 0: "Basepairing Scheme" = MASTER_Basepairs_schemes[i]
                #position 1: "number of bonds expected" = MASTER_Basepairs_bonds[i][0],
                #position 2: "Base # for base 1" = MASTER_Basepairs[i][0][0],
                #position 3: "Base identity for base 1" = MASTER_Basepairs[i][0][1],
                #position 4: "Base # for base 2" = MASTER_Basepairs[i][0][3],
                #position 5: "Base identity for base 2" = MASTER_Basepairs[i][0][4],
                #position 6: "number of bonds found" = calculated below,
                #position 7: "atom 1 for bond 1 according to MASTER_Basepairs_bonds[i][3]" MASTER_Basepairs[i][0][2],

                #position 8: "atom 2 for bond 1 according to MASTER_Basepairs_bonds[i][4]" MASTER_Basepairs[i][0][5],
                #position 9: "Distance for H-bond 1 between atoms 1 and 2" MASTER_Basepairs[i+x][0][6],
                #position 10: "atom 1 for bond 2 according to MASTER_Basepairs_bonds[i][5]" MASTER_Basepairs[i+x][0][2],
                #position 11: "atom 2 for bond 2 according to MASTER_Basepairs_bonds[i][6]" MASTER_Basepairs[i+x][0][5],
                #position 12: "Distance for H-bond 2 between atoms 1 and 2" MASTER_Basepairs[i+x][0][6],
                #position 13: "atom 1 for bond 3 according to MASTER_Basepairs_bonds[i][7]" MASTER_Basepairs[i+x+2][0][2],
                #position 14: "atom 2 for bond 3 according to MASTER_Basepairs_bonds[i][8]" MASTER_Basepairs[i+x+s][0][5]]
                #position 15: "Distance for H-bond 3 between atoms 1 and 2" MASTER_Basepairs[i+x][0][6],
                        #NOTE: The latter three strings will only be present in three-bond basepairing schemes
                #position 16: "Phosphate-to-phosphate 'P-to-P' distance". Carried in 'First_List_P' and collected in 'FUNCTION Program. Section: LOOKING FOR BASES WITH POSSIBLE MULTIPLE CONTACTS'
                #position 17: "C1'-to-C1'" distance". Carried in 'First_List_C1' and collected in 'FUNCTION Program. Section: LOOKING FOR BASES WITH POSSIBLE MULTIPLE CONTACTS'
    line1 = "####################################\nSecond sorting of basepair candidates: Run # " + run_number_str + " Distance Cutoff = " + CUTOFF_str[0:3] + "\n#####################################"
    print(line1)
    for i in range(len(MASTER_Basepairs)): #'i' will correspond to the same level in both MASTER_Basepairs and MASTER_Basepairs_summary, the level of basepair scheme
        if len(MASTER_Basepairs[i]) > 0:
            for j in range (len(MASTER_Basepairs[i])):
#Detection of lines carrying consecutive bases. These should be not allowed to enter the assignment section, as they can give rise to FALSE basepairs
                resid = [[],[]]
                atom = [[],[],[]]
                m=re.search(r'\d+', MASTER_Basepairs[i][j][0])
                resid[0] = int(m.group())
                m=re.search(r'\d+', MASTER_Basepairs[i][j][3])
                resid[1] = int(m.group())
                m=re.search(r'\S+', MASTER_Basepairs[i][j][2])
                atom[0] = str(m.group())
                m=re.search(r'\S+', MASTER_Basepairs[i][j][5])
                atom[1] = str(m.group())
                atom[2] = MASTER_Basepairs[i][j][6]
                if (resid[1] - resid[0] > 1) or (resid[1] - resid[0] < -1):

#FOLLOW A LINE
#                    if ('  12 ' in MASTER_Basepairs[i][j][0]) and ('  23 ' in MASTER_Basepairs[i][j][3]):
#                        print "\nBefore Second sorting: MASTER_Basepairs[i][j]", MASTER_Basepairs_schemes[i], MASTER_Basepairs[i][j], "atom", atom
#                        print "i", i, "j", j, "MASTER_Basepairs_schemes[i]", MASTER_Basepairs_schemes[i], "MASTER_Basepairs_bonds[i]", MASTER_Basepairs_bonds[i]

#FOLLOW A LINE
                    if (len(MASTER_Basepairs_summary[i]) > 0):
                        found = 0 #Will control that the program does not execute the next 'elif' if it has executed this 'if'
                        for k in range (len(MASTER_Basepairs_summary[i])):

#This basepair has already been detected, new bond for the basepair. Here we should distinguish between RUN 1 and later RUNs, as during the latter the program will run into pre-loaded MASTER_Basepairs_summary[i][k] lines
                           if (MASTER_Basepairs[i][j][0] in MASTER_Basepairs_summary[i][k][2]) and (MASTER_Basepairs[i][j][3] in MASTER_Basepairs_summary[i][k][4]):
                              found = 1 #Will control that the program does not execute the next 'elif' if it has executed this 'if'
                              if (atom[0] != MASTER_Basepairs_summary[i][k][7]) and (atom[0] != MASTER_Basepairs_summary[i][k][10]) and (atom[0] !=  MASTER_Basepairs_summary[i][k][13]):
########Checking that MASTER_Basepairs[i][j] and MASTER_Basepairs_summary[i][k] contain the same residues but that the former brings new ATOMS
#                                  MASTER_Basepairs_summary[i][k][6] = MASTER_Basepairs_summary[i][k][6] + 1  #One new bond added
                              #Bond ATOMS and distances will be placed at their proper position in "FUNCTION BOND_PLACING"
                                  MASTER_Basepairs_summary[i][k] = BOND_PLACING(MASTER_Basepairs_bonds[i], MASTER_Basepairs_summary[i][k], atom)
#FOLLOW A LINE
#                                  if ('  12 ' in MASTER_Basepairs[i][j][0]) and ('  23 ' in MASTER_Basepairs[i][j][3]):
#                                      print "new bond for the basepair: MASTER_Basepairs_summary[i][k]", MASTER_Basepairs_summary[i][k]
#FOLLOW A LINE


#New basepair detected for a basepairing Scheme for which other basepairs have been identified
                           elif (found == 0) and (k == len(MASTER_Basepairs_summary[i]) - 1):
                               collect = [MASTER_Basepairs_schemes[i], MASTER_Basepairs_bonds[i][0], MASTER_Basepairs[i][j][0], MASTER_Basepairs[i][j][1], MASTER_Basepairs[i][j][3], MASTER_Basepairs[i][j][4], 0, [], [], [], [], [], [], [], [], [], [], []]
                               MASTER_Basepairs_summary[i].append(collect)
                       #Bond ATOMS and distances will be placed at their proper position in "FUNCTION BOND_PLACING"
                               MASTER_Basepairs_summary[i][len(MASTER_Basepairs_summary[i])-1] = BOND_PLACING(MASTER_Basepairs_bonds[i], MASTER_Basepairs_summary[i][len(MASTER_Basepairs_summary[i])-1], atom)
#FOLLOW A LINE
#                               if ('  12 ' in MASTER_Basepairs[i][j][0]) and ('  23 ' in MASTER_Basepairs[i][j][3]):
#                                  print "New basepair detected for a basepairing Scheme: MASTER_Basepairs_summary[i][k]", MASTER_Basepairs_summary[i][k]
#FOLLOW A LINE

#First basepair for this basepairing Scheme
                    else: ######
                        collect = [MASTER_Basepairs_schemes[i], MASTER_Basepairs_bonds[i][0], MASTER_Basepairs[i][j][0], MASTER_Basepairs[i][j][1], MASTER_Basepairs[i][j][3], MASTER_Basepairs[i][j][4], 0, [], [], [], [], [], [], [], [], [], [], []]
                        MASTER_Basepairs_summary[i].append(collect)
                        #Bond ATOMS and distances will be placed at their proper position in "FUNCTION BOND_PLACING"
                        MASTER_Basepairs_summary[i][0] = BOND_PLACING(MASTER_Basepairs_bonds[i], MASTER_Basepairs_summary[i][0], atom)
#FOLLOW A LINE
#                        if ('  12 ' in MASTER_Basepairs[i][j][0]) and ('  23 ' in MASTER_Basepairs[i][j][3]):
#                            print "First basepair for this basepairing Scheme: MASTER_Basepairs_summary[i][0]", MASTER_Basepairs_summary[i][0]
#FOLLOW A LINE
#####################################
#       FUNCTION "Program". Section: Second sorting of basepair candidates                END
#####################################

########################################################
#       FUNCTION "Program". Section: LOOKING FOR BASES WITH POSSIBLE MULTIPLE CONTACTS
########################################################
#The following code is designed to identify bases from basepair candidates carried in MASTER_Basepairs_summary, which are involved in MULTIPLE CONTACTS. These bases will not be assigned to base pairs, except if they belong to 3-bond basepairs for which all 3 bonds have been identified.
    #DEFINITION of MULTIPLE CONTACTS: These are ATOM-TO-ATOM contacts identified by 'iotbx.pdb' which are below the CUTOFF and that do not necessarily reflect basepairing interactions but obscure the assignment of such interactions. Indeed, a large part of all the contacts identified in "SECTION: First sorting of basepair candidates" are not from actual base pairs but from stacked bases. In addition, some of these multiple interactions could be used to detect base triples (NOT CLEAR HOW TO DO THIS YET).

#Due to the importance of these MULTIPLE CONTACTS, this section is devoted to find them. After detecting the existence of MULTIPLE CONTACTS, the program will attempt to remove as  many bases as possible from them by using the "CONTINUITY OF HELICITY CRITERION" in the next section.

    #Searching for these bases has to be done before the assignment of legitimate basepairs, so that the complete list of multiply involved bases can be used during this assignment in SECTION: Final assignment of Basepairs
   #In addition to the code contained in this section, function 'CONTROL' is in charge of performing this search. The list of bases involved in MULTIPLE CONTACTS will be loaded into 'control' by function 'CONTROL'.


    print("#####################################\nSearching for bases with possible MULTIPLE CONTACTS Run #", run_number_str, " Distance Cutoff = ", CUTOFF_str[0:3], "\n#####################################")
    for i in range (len(MASTER_Basepairs_summary)):
        if len(MASTER_Basepairs_summary[i]) > 0:
            for j in range (len(MASTER_Basepairs_summary[i])):
                append = '0' #Initializing. Will control what to append
                if MASTER_Basepairs_summary[i][j] != [] and MASTER_Basepairs_summary[i][j][16] == []:
                    for r in range (len(First_List_P)):
                        if First_List_P[r][0] in MASTER_Basepairs_summary[i][j] and First_List_P[r][3] in MASTER_Basepairs_summary[i][j][4]:
                           MASTER_Basepairs_summary[i][j][16] = First_List_P[r][6]
                    C1_compare = 22
                    for s in range (len(First_List_C1)):
                        if First_List_C1[s][0] in MASTER_Basepairs_summary[i][j] and First_List_C1[s][3] in MASTER_Basepairs_summary[i][j][4]:
                            if First_List_C1[s][6] < C1_compare:
                                C1_compare = First_List_C1[s][6]
                    MASTER_Basepairs_summary[i][j][17] = C1_compare


                if (MASTER_Basepairs_summary[i][j][6] >= 2) and ('REMOVED' not in MASTER_Basepairs_summary[i][j]):
#Now that the C1-C1 distances have been added to the basepair candidates, this distance can be used to perform a first discrimination by weeding out basepairs with C1-C1 distances that are more than 3X the precalculated average C1-Ca. This will be done in FUNCTION C1_C1_DISTANCE. Candidate basepairs with 'bad' C1-C1 distances will be appended as 'REMOVED'
                    if (MASTER_Basepairs_bonds[i][9] != 'NA'):
                        list = [MASTER_Basepairs_summary[i][j][2], MASTER_Basepairs_summary[i][j][3], MASTER_Basepairs_summary[i][j][4], MASTER_Basepairs_summary[i][j][5]]
                        convert = CONVERT(list)
                        MASTER_Basepairs_summary[i][j] = C1_C1_DISTANCE(MASTER_Basepairs_summary[i][j], MASTER_Basepairs_bonds[i], convert[0], convert[1], convert[2], convert[3])


#Calling FUNCTION "CONTROL" to perform an initial assessment of whether the identified bases might be involved in MULTIPLE CONTACTS
                    control = CONTROL(control, MASTER_Basepairs_summary[i][j][2], MASTER_Basepairs_summary[i][j][4], MASTER_Basepairs_summary[i][j][3], MASTER_Basepairs_summary[i][j][5])

######ASSIGNING '1' or '0' to candidate basepairs(see also explanation for DISTANCE criterion)######
#Once the 'control' list has been loaded, MASTER_Basepairs_summary has to be compared to 'control' in order to properly tag all basepairs candidates prior to their assignment as legitimate basepairs.
        #Performed in #This FUNCTION will be used to compare MASTER_Basepairs_summary to 'control' (already loaded), to determine what to do with bases with possible multiple contacts according to the criteria chosen.
                #If the base pair is found to be possibly involved in MULTIPLE CONTACTS, a '1' will be appended to its line in MASTER_Basepairs_summary
                        #Having a '1' will not affect the assignment of 3-base basepairs for which all bonds have been identified
                #If the base pair is found not to be involved in MULTIPLE CONTACTS, a '0' will be appended to its line in MASTER_Basepairs_summary

    for l in range (len(MASTER_Basepairs_summary)):
        for m in range (len(MASTER_Basepairs_summary[l])):
            if ('D' not in MASTER_Basepairs_summary[l][m]) and ('MH' not in MASTER_Basepairs_summary[l][m]) and ('ELI' not in MASTER_Basepairs_summary[l][m]):
               first_append = FIRST_APPEND(MASTER_Basepairs_summary[l][m], control, MAX_CUTOFF_str[0:3], CUTOFF_str[0:3])
               MASTER_Basepairs_summary[l][m] = first_append
######ASSIGNING '1' or '0' to candidate basepairs(see also explanation for DISTANCE criterion)######

#Printing output.
    line1 = "AFTER SEARCHING FOR BASES WITH POSSIBLE MULTIPLE CONTACTS: Run # " + run_number_str + " Distance Cutoff = " + CUTOFF_str[0:3]
#    print_control = 1
    print_control = 0
    control_control = SEQUENTIAL_READOUT(MASTER_Basepairs_summary, MASTER_Basepairs_schemes, control, print_control, line1) #control_control = 0 signals that no bases with possible multiple contacts have been found. Will avoid unnecesary runs of some parts of the program
########################################################
#       FUNCTION "Program". Section: LOOKING FOR BASES WITH POSSIBLE MULTIPLE CONTACTS                 END
########################################################



################################################
#       FUNCTION "Program". Section: Final assignment of Basepairs BEGINNING
################################################

#Performed by following three different criteria:
        #First: DISTANCE criterion
        #Second: CONTINUITY OF HELICITY CRITERION
        #Third: ELIMINATION criterion

#1) DISTANCE CRITERION (Performed in #######SUBSECTION Initial Assignment):
                #First: The numeral '0' (not involved in MULTIPLE CONTACTS), or '1' (involved in MULTIPLE CONTACTS), will be appended to all candidate basepairs after comparing MASTER_Basepairs_summary to 'control'.
                #Second: All possible legitimate basepairs will be identified based on the DISTANCE criterion. These include:
                        #3-bond basepairs for which all 3 bonds have been detected, regardless of whether they carry a '0' or '1' tag. A 'yes' TAG will be appended to these basepairs. In addition, a 'D' (Distance) TAG
                        #2-bond basepairs which carry a '0', (i.e not involved in MULTIPLE CONTACTS). A 'yes' tag will be appended to these basepairs. In addition, a 'D' (Distance) TAG will be appended to these basepairs.
                        #3-bond basepairs for which only 2 bonds have been identified, which carry a '0', (i.e not involved in MULTIPLE CONTACTS). A 'yes' tag will be appended to these basepairs but only when MAX_CUTOFF_str[0:3] in CUTOFF_str[0:3]. In addition, a 'D' (Distance) TAG will be appended to these basepairs.
                                #The reason to keep these basepairs unassigned until the last run is that the missing third bond may be identified at MAX_CUTTOF, making their assignment stronger
                        #All other basepairs will be tagged as 'no'. This tag can be reversed by the next two CRITERIA
                        #After one round of "Initial Assignment" 'no'-tagged basepairs (only 2-bond basepairs) will be run through FUNCTION "REMOVAL". This will allow the identification of furhter basepairs through the "ELIMINATION" criterion.

#2)"ELIMINATION" criterion. As important as detecting basepairs possibly involved in MULTIPLE CONTACTS, is to remove the ones assigned as legitimate from this list ('control'). This is done by FUNCTION REMOVAL as follows.
                #The removal of newly assigned, legitimate basepairs would have left the length of these sub-lists intact but the strings corresponding to the newly assigned basepairs will carry now '[]'. Therefore if only 2 bases are left in some of these sublists after the removal of newly assigned basepairs from them, these leftover bases may identify a lone pair that has been removed of multiple interactions.
                #Any new basepair identified by this method will be appended a 'ELI'(Elimination) TAG at position (16 + ('run_number' - 1)*4). For STATISTICS.


#2) CONTINUITY OF HELICITY CRITERION
        #A 'no' string  can be reversed in Loop "Attempting to find additional basepairs by the CONTINUITY OF HELICITY CRITERION" if an adjacent basepair is found. This would  assume that the continuity of helical segments is a strong parameter while determining the likelihood of a given basepair
                #The involvement of many basepairs in MULTIPLE CONTACTS (see definition of MULTIPLE CONTACTS), which makes it impossible to accurately assign basepairs with less than 3 bonds, severely limits the ability of using distance data to accurately assign base pairs. At medium cutoffs about only half the basepairs can be identified.
#This proportion increases with lower cutoff distances (more restricitive search) but then, other basepairs with bonds longer than the cutoff are missed. Therefore, if no other criterion was used only 3-bond basepairs and a few other isolated basepairs, not involved in mutiple interactions could be identified by this program.
                #To improve this, I have written, "SUBSECTION Loop", containing the loop named "Attempting to find additional basepairs by the CONTINUITY OF HELICITY CRITERION". This loop will attempt to expand the initial assignment of legitimate basepairs by including basepairs for which only two bonds have been identified and tagged as 'no' in "SECTION: LOOKING FOR BASES INVOLVED IN MULTIPLE CONTACTS".
                #It works as follows:
                        #If an immediately adjacent legitimate basepair can be found for the current 'no'-tagged basepair candidate, the current basepair would be retagged as a 'yes', or legitimate, basepair. This is based on the idea that retagging the current basepair would expand an already established helical element
                        #The loop "Attempting to find additional basepairs by the CONTINUITY OF HELICITY CRITERION" will run until no more additional basepairs are found. This will ensure that every newly identified legitimate basepair can be used to increase the chances that other basepairs can be assigned as legitimate.
                        #Any additional basepairs identified in this section will be removed from the 'control' list carrying basepairs involved in MULTIPLE CONTACTS.
                        #NOTE:The actual loop is contained within FUNCTION "Loop", which is called from this section
                        #Any new basepair identified by the loop will be appended a 'MH'(Maximization of Helicity) TAG at position (16 + ('run_number' - 1)*4). For STATISTICS
                        #After one round of "Initial Assignment" 'no' tagged basepairs will be run through FUNCTION "REMOVAL". This will allow the identification of furhter basepairs through the "ELIMINATION" criterion.


#TAGS APPENDED: Every run, the following TAGS will be appended at the end of every line in MASTER_Basepairs_summary
               # 1) Position 13: TAG = 'none', globally added before MASTER_Basepairs_summary is searched for bases involved in MULTIPLE CONTACTS by comparing to list 'control'
               # 2) Position 13: TAG = 'yes' or 'no', which mean that the basepair candidate can be accurately assigned or not, depending on whether it is involved in MULTIPLE CONTACTS with other possible basepairs. For 3-bond basepairs for which all 3 bonds have been identified, a 'no' tag will be bypassed and the basepair will be considered unambiguously assigned in SECTION "Final assignment of Basepairs". The 'no' tag will then be reversed
               # 3) Position 14: TAG = can take the values '1' or '0', depending on whether the basepair was detected in MULTIPLE CONTACTS or not. For STATISTICS purposes only.
               # 5) Position 15: TAG = CUTOFF
               # 6) Position 16: TAG = can take the values 'D' (Distance), 'MH'(Maximization of Helicity), or 'ELI' (Elimination), to keep a record of how the basepair was identified (see SECTION: Final assignment of Basepairs). For STATISTICS only.
              # 7 and further: Position 17 through 20 (4 positions/run) will be appended in Run 2, and so on
                     #The actual position of the TAGs in every run can be calculated as: (13 + ('run_number' - 1)*4), in which the initial value for 'run_number' is = 1
#######SUBSECTION Initial Assignment

    convert = []
    new_basepairs = 0
    print("##############################\n##### Initial Basepair Assignment. Run #", run_number_str, " Distance Cutoff = ", CUTOFF_str[0:3], "PREV_CUTOFF", PREV_CUTOFF, "\n##############################")
    transient = []
    if (MAX_CUTOFF_str[0:3] in CUTOFF_str[0:3]):
        print("MAX_CUTOFF_str[0:3] ", MAX_CUTOFF_str[0:3], "CUTOFF_str[0:3] ", CUTOFF_str[0:3])
    if CUTOFF_str[0:3] in MAX_CUTOFF_str[0:3]:
        print("CUTOFF_str[0:3] ", CUTOFF_str[0:3], "MAX_CUTOFF_str[0:3] ", MAX_CUTOFF_str[0:3])
    for i in range (len(MASTER_Basepairs_summary)):
        print("Basepairing scheme:", MASTER_Basepairs_schemes[i])
        for j in range (len(MASTER_Basepairs_summary[i])):
                if (len(MASTER_Basepairs_summary[i]) > 0) and ('D' not in MASTER_Basepairs_summary[i][j]) and ('MH' not in MASTER_Basepairs_summary[i][j]) and ('ELI' not in MASTER_Basepairs_summary[i][j]) and ('REMOVED' not in MASTER_Basepairs_summary[i][j]):

#basepairs with 3 bonds, all identified
                    if (MASTER_Basepairs_summary[i][j][1] == 3) and (MASTER_Basepairs_summary[i][j][6] == 3):
                        #if (('D' not in MASTER_Basepairs_summary[i][j]) and ('MH' not in MASTER_Basepairs_summary[i][j]) and ('ELI' not in MASTER_Basepairs_summary[i][j]) and ('REMOVED' not in MASTER_Basepairs_summary[i][j])):
                            list = [MASTER_Basepairs_summary[i][j][2], MASTER_Basepairs_summary[i][j][3], MASTER_Basepairs_summary[i][j][4], MASTER_Basepairs_summary[i][j][5]]
                            convert = CONVERT(list)
                            print("----->High confidence 3-bond basepair formed by residues " + convert[1] + convert[0] + " and " + convert[3] + convert[2])
                            length_1 = len(MASTER_Basepairs_summary[i][j]) - 1
                            MASTER_Basepairs_summary[i][j].append('D') #Distance 'D' TAG
                            MASTER_Basepairs_summary[i][j].append(CUTOFF_str[0:3])
                            transient = [MASTER_Basepairs_summary[i][j][2], MASTER_Basepairs_summary[i][j][4]]
                            positions.append(transient)
                            new_basepairs = new_basepairs + 1

                            if ('no' in MASTER_Basepairs_summary[i][j]):
                                print("    The bases in this basepair may be participating in an interaction with other bases. However, the detection of 3 bonds, makes its assignment as a 3-bond basepair highly probable.")

#basepairs with 3 bonds for which only 2 bonds were identified
                    elif ((MASTER_Basepairs_summary[i][j][1] == 3) and (MASTER_Basepairs_summary[i][j][6] == 2) and (MAX_CUTOFF_str[0:3] in CUTOFF_str[0:3])):
#These basepairs are excluded from 'DISTANCE' and 'ELIMINATION' criteria (MAX_CUTOFF_str[0:3] in CUTOFF_str[0:3]), so they should not arrive here with either 'D' or 'ELI'
                        if ('yes' in  MASTER_Basepairs_summary[i][j]):
                            list = [MASTER_Basepairs_summary[i][j][2], MASTER_Basepairs_summary[i][j][3], MASTER_Basepairs_summary[i][j][4], MASTER_Basepairs_summary[i][j][5]]
                            convert = CONVERT(list)
                            print("----->At least 2 bonds identified out of 3 expected for the base pair formed by residues " + convert[1] + convert[0] + " and " + convert[3] + convert[2])
                            MASTER_Basepairs_summary[i][j].append('D') #Distance 'D' TAG
                            MASTER_Basepairs_summary[i][j].append(CUTOFF_str[0:3])
                            transient = [MASTER_Basepairs_summary[i][j][2], MASTER_Basepairs_summary[i][j][4]]
                            positions.append(transient)
                            new_basepairs = new_basepairs + 1

#basepairs with 2 bonds
                    elif (MASTER_Basepairs_summary[i][j][1] == 2) and (MASTER_Basepairs_summary[i][j][6] == 2):
                        #if (('D' not in MASTER_Basepairs_summary[i][j]) and ('MH' not in MASTER_Basepairs_summary[i][j]) and ('ELI' not in MASTER_Basepairs_summary[i][j]) and ('REMOVED' not in MASTER_Basepairs_summary[i][j])):
                            if ('yes' in  MASTER_Basepairs_summary[i][j]):
                                list = [MASTER_Basepairs_summary[i][j][2], MASTER_Basepairs_summary[i][j][3], MASTER_Basepairs_summary[i][j][4], MASTER_Basepairs_summary[i][j][5]]
                                convert = CONVERT(list)
                                print("----->2-bond basepair identified formed by residues " + convert[1] + convert[0] + " and " + convert[3] + convert[2])
                                MASTER_Basepairs_summary[i][j].append('D') #Distance 'D' TAG
                                MASTER_Basepairs_summary[i][j].append(CUTOFF_str[0:3])
                                transient = [MASTER_Basepairs_summary[i][j][2], MASTER_Basepairs_summary[i][j][4]]
                                positions.append(transient)
                                new_basepairs = new_basepairs + 1
    print("##### END of Initial Basepair Assignment. Run #", run_number_str, " Distance Cutoff = ", CUTOFF_str[0:3], "\n##############################")

#The newly identified basepairs should be removed from 'control' (list of bases involved in MULTIPLE CONTACTS)
    if new_basepairs > 0 and control_control == 1:
        print("\n##############################\n##### Searching for additional basepairs by the ELIMINATION criterion. Run # ", run_number_str, " Distance Cutoff = ", CUTOFF_str[0:3])
        mode_removal = 0
        removal = REMOVAL(MASTER_Basepairs_summary, control, convert[0], convert[2], CUTOFF, mode_removal, positions)
        MASTER_Basepairs_summary = removal[0]
        control = removal[1]
        new_basepairs = removal[2]

#Printing output.
        a = str(new_basepairs)
        line1 = "##### Number of additional basepairs identified by ELIMINATION = " + a + "  Run # " + run_number_str +  " Distance Cutoff = " + CUTOFF_str[0:3]
        print(line1)
        print("############################## ")
        line2 = "AFTER Bulk Removal: Run # " + run_number_str + " Distance Cutoff = " + CUTOFF_str[0:3] #This line will be outputed in FUNCTION "SEQUENTIAL_READOUT"
        print_control = 0
        SEQUENTIAL_READOUT(MASTER_Basepairs_summary, MASTER_Basepairs_schemes, control, print_control, line2)
    elif new_basepairs == 0 and control_control == 1:
        print("##############################\n NO newly identified basepairs to be removed from the list BASES WITH POSSIBLE MULTIPLE CONTACTS.\n ", "Run # ", run_number_str, " Distance Cutoff = ", CUTOFF_str[0:3])
        print("############################## ")
    else:
        print("##############################\n NO BASES WITH POSSIBLE MULTIPLE CONTACTS have been identified.\nAssignement of furhter additional basepairs by the ELIMINATION criterion not possible\n ", "Run # ", run_number_str, " Distance Cutoff = ", CUTOFF_str[0:3])
        print("############################## ")

######SUBSECTION Initial Assignment

######SUBSECTION Loop
#######Loop "Attempting to find additional basepairs by the CONTINUITY OF HELICITY CRITERION"
#see more info on this loop above

#FIRST LOOP RUN, searching for basepairs that would increase helicity

    if control_control == 1:
        print("\n##############################\n##### SEARCHING FOR ADDITIONAL BASEPAIRS BY USING THE CONTINUITY OF HELICITY CRITERIUM /nRun #", run_number_str, " Distance Cutoff = ", CUTOFF_str[0:3])

        loop_output = []
        loop_control = 1 #Allow multiple runs of the loop

#calling FUNCTION "Loop"
        loop_output = loop(MASTER_Basepairs_summary, control, loop_control, CUTOFF, positions)
        MASTER_Basepairs_summary = loop_output[0]
        control = loop_output[1]
#Calling FUNCTION "SEQUENTIAL_READOUT"
        line2 = "AFTER SEARCHING FOR ADDITIONAL BASEPAIRS BY USING THE CONTINUITY OF HELICITY CRITERIUM: Run # " + run_number_str + " Distance Cutoff = " + CUTOFF_str[0:3]
        print_control = 0
        SEQUENTIAL_READOUT(MASTER_Basepairs_summary, MASTER_Basepairs_schemes, control, print_control, line2)
        print("\n##############################")
    else:
        print("\n##############################\nBYPASSING THE SEARCH FOR ADDITIONAL BASEPAIRS BY USING THE CONTINUITY OF HELICITY CRITERIUM /nRun #", run_number_str, " Distance Cutoff = ", CUTOFF_str[0:3], "\n##############################")

################################################
#       FUNCTION "Program". Section: Final assignment of Basepairs       END
################################################

################################################
################################################
#FUNCTION "program"                     END
################################################
################################################

##########################################################################
##########################################################################
##########################################################################
#SECTION FUNCTIONS                                                 END
##########################################################################
##########################################################################
##########################################################################


##########################################################################
##########################################################################
##########################################################################
#SECTION HELICITY                        BEGINNING
##########################################################################
##########################################################################
##########################################################################
#Searches for matches to a potential basepair candidate within MASTER_Basepairs_summary (named LIST within this FUNCTION)
def HELICITY(LIST, a0, a1, a2, a3, where_from):
                        a0 = str(a0)
                        a1 = str(a1)
                        a2 = str(a2)
                        a3 = str(a3)
                        line1 = "            Searching for possible basepairs between residues " + a0 + " and " +  a1

                        print(line1)
                        if ('iiiiiiiiiiiiiii' in a2) and ('iiiiiiiiiiiiiii' in a3):
                              line1 = line1 + a2 + " and " + a3 + "\n"

#Called from FUNCTION "Loop"
#LIST = MASTER_Basepairs_summary
                        match_found1 = []
                        match_found2 = []
                        match_position = []
                        for r in range (len(LIST)):
                            for s in range (len(LIST[r])):
                                m=re.search(r'\d+', LIST[r][s][2])
                                match1 = m.group()
                                m=re.search(r'\d+', LIST[r][s][4])
                                match2 = m.group()
#                                print "LIST[r][s]", LIST[r][s], "\n    match1", match1, "match2", match2, "a0", a0, "a1", a1, "a2", a2, "a3", a3
                                if (('Loop' in where_from) and ('REMOVED' not in LIST[r][s]) and ('yes' in LIST[r][s]) and ((match1 in a0) and (match2 in a1) or (match2 in a0) and (match1 in a1))):
                                   match_found1 = [a0, a1]
                                   match_position = [r, s]
#                                   print "match found: a0", a0, "a1", a1
                                elif (('Loop' in where_from) and ('REMOVED' not in LIST[r][s]) and ('yes' in LIST[r][s]) and ((match1 in a2) and (match2 in a3) or (match1 in a3) and (match2 in a2))):
                                   match_found2 = [a2, a3]
                                   match_position = [r, s]
#                                   print "match found: a2", a2, "a3", a3
                                elif ('REMOVAL' in where_from) and ('REMOVED' not in LIST[r][s]) and ('no' in LIST[r][s]) and ((match1 in a0) and (match2 in a1) or (match2 in a0) and (match1 in a1)):
                                   match_found1 = [a0, a1]
                                   match_position = [r, s]
#                                   print "where_from ", where_from, "LIST[r][s] ", LIST[r][s],  "\na0", a0, "a1", a1, "match1 ", match1, "match2 ", match2

                        return[match_found1, match_found2, match_position]
##########################################################################
##########################################################################
##########################################################################
#SECTION HELICITY                                 END
##########################################################################
##########################################################################
##########################################################################




##########################################################################
##########################################################################
##########################################################################
#SECTION MAIN                                    BEGINNING
##########################################################################
##########################################################################
##########################################################################

##########################################################################
#Sub SECTION Calling run(args)
##########################################################################
#FUNCTION run(args) was created by Ralf W. Grosse-Kunstle and provides a list of ATOM-to-ATOM distances calculated from a .pdb file.

First_List = run(args=sys.argv[1:]) #Will carry the output of FUNCTION run(args)
pdb_file = First_List[0] #Name of the file read in
First_List[0] = []
##########################################################################
#Sub SECTION Calling run(args)
##########################################################################

##########################################################################
#Sub SECTION file name
##########################################################################
import os
dir =  os.getcwd()
import re
ma=re.compile(r'\/+')
iterator = ma.finditer(dir)
aaa=[]
for match in iterator:
   aaa.append(match.span())
directory = dir[0:aaa[len(aaa)-1][1]]
FILE = dir[aaa[len(aaa)-1][1]:len(dir)]
mm=re.search(r'\.', pdb_file)
START = mm.start()
pdb_file_main = pdb_file[0:START]
##########################################################################
#Sub SECTION file name
##########################################################################

##########################################################################
#Sub SECTION MAIN LOOP                        BEGINNING
##########################################################################
run_number = 1 #In addition to signaling the Run #, this variable will allow differential execution of certain events depending on whether the program is in 'first run' at the lowest CUTOFF, or later runs at higher CUTOFFs
run_cutoff = [] #Will register all CUTOFF values used in each run

CUTOFF = 3.0
CUTOFF_str = repr(CUTOFF) #Converts 'float' to 'str'
run_cutoff.append(run_number)
run_cutoff.append(CUTOFF_str[0:3])
run_cutoff_LISTS = [] #Will carry all the assigned basepairs, ordered by CUTOFF value at which they have been assigned. Used in STATISTICS
MAX_CUTOFF = 3.6
PREV_CUTOFF = 0.0 #Initializing
increment = 0.2
        #ARGUMENTS
                #First_List = Contains the filtered output from FUNCTION "run(args)". Listof ATOM to ATOM distances below a rough cutoff of 5A
                #CUTOFF = Defined above. CUTOFF to assign ATOM-to-ATOM distances that are compatible with basepair formation. CUTOFF has a Dynamic value, increasing by the value of 'increment' at every new execution of FUNCTION "Program"
                #pdb_file_main = main part of the input .pdb file withou the extension
                #file = name of the .pml file for PYMOL
                #file1 = name of the output file for the run
positions = [] #Will store the base #s of the basepairs identified for their easy removal in FUNCTION REMOVAL
while CUTOFF < MAX_CUTOFF:
    if run_number > 1:
        CUTOFF = CUTOFF + increment
        CUTOFF_str = repr(CUTOFF) #Converts 'float' to 'str'
        run_cutoff.append(run_number)
        run_cutoff.append(CUTOFF_str[0:3])
#    else:
    control = [] #The list of bases involved in MULTIPLE CONTACTS will be loaded into 'control' by function 'CONTROL'. See FUNCTION Section: LOOKING FOR BASES INVOLVED IN MULTIPLE CONTACTS:

    print("\n############################################################################################\n############################################################################################\n###### Run # ", run_number, "DISTANCE CUTOFF = ", CUTOFF, "\n############################################################################################\n############################################################################################\n")
    run_number_str = to_string(run_number)
    MAX_CUTOFF_str = str(MAX_CUTOFF) #Converts 'float' to 'str'

#Executing FUNCTION "program", main part of the program:
    #It started as the program itself but had to be converted to a function to allow its repetitive iteration
    program(First_List, MASTER_Basepairs_summary, CUTOFF, pdb_file_main, control, run_number_str, CUTOFF_str, run_number, positions)

    run_number = run_number + 1
    PREV_CUTOFF = CUTOFF
##########################################################################
#Sub SECTION MAIN LOOP                          END
##########################################################################

##########################################################################
##########################################################################
##########################################################################
#SECTION MAIN                                   END
##########################################################################
##########################################################################
##########################################################################


##########################################################################
##########################################################################
##########################################################################
#SECTION DETECTION OF FALSE BASEPAIRS                  BEGININNG
##########################################################################
##########################################################################
##########################################################################

#I have observed that in addition to predicting basepairs, the CONTINUITY OF HELICITY CRITERION could be used to detect some false onse. For example, I have observed that false 3-bond basepairs could be erroneously assigned by bases that are closely stacked to real basepairs. This is not the situation for 2-bond basepairs candidates and 3-bond basepair candidates, for which only 2 bonds have been detected, because these are excluded from being falsely assigned by the MULTIPLE INTERACTION CRITERION. Interestinlgy, I have observed that after assignment of additional basepairs by the CONTINUITY OF HELICITY CRITERION, new 2-bond basepairs may be assigned which share a base with the false basepair. Therefore, a 'MH'-tagged basepair could be used to detect a false 3-bond basepair, with which it shares a base, and to remove it from the final basepair list. However, before this is done, the non-'MH'-tagged basepair should be run through checked for its possible involvement in a helical element (see below). In addition, this observation also indicates that in other cases, in which the exclusion of a erroneously assigned basepair is not as stright forward (for example, 2 D-tagged, 3-bond basepairs that may share one base), subjecting the two basepairs to the "CONTINUITY OF HELICITY CRITERION" may be used to tell the legitimate basepair from the false one.


#I have also observed that some false Distance-identified 2-bond basepairs can escape the MULTIPLE INTERACTION CRITERION if they lie adjacent to a 3-bond basepair for which all bonds have been identified. This is not surprising, as the latter basepairs are excluded from re-entering the pool of bases involved in  MULTIPLE CONTACTS. In this case, running the CONTINUITY OF HELICITY CRITERION should identify the false basepair. However, the possibility of allowing these basepairs to enter the pool of bases involved in  MULTIPLE CONTACTS should be considered.
        #Note 021010: Indeed, allowing 3-bond basepairs for which all bonds have been identified to enter the  pool of bases involved in  MULTIPLE CONTACTS, prevents some of these bases from being erroneoulsy assigned as legitimate basepairs.


####FUNCTIONING
#In the MAIN part of the section MASTER_Basepairs_summary_bak will be compared to MASTER_Basepairs_summary to detect false basepairs as described above. Several criteria will be cyclically used at different levels of discrimination stringency to decide which one of the debated basepairs is LEGITIMATE and which one is FALSE. Initially a list of base-sharing basepairs is defined. There are 2 categories:
        #Basepairs which share a single base. These are the most abundant. They are removed from the list first.
        #Basepairs which share both bases. In this case the 2 bases clearly form a basepair, only the right geometry has to be inferred. These are removed from the list once the basepairs which share a single base have been processed.


        #"C1-C1 and P-P DISTANCE CRITERION": Performed only at HIGH STRINGENCY and before any of the other criteria is used. Will allow initial assignement of FALSE basepairs independently of the rest of the loop.
                #Performed in SECTION FUNCTION "C1_C1 DISTANCE CRITERION". As of 050310 version, I have added statistical information regarding the C1-C1 and P-P distance. The information was empirically generated by the program and then fed back into into it by means of the list "MASTER_Basepairs_bonds". Historically, while this information was not necessary for the prediction of basepairs in tRNA (4TNA.pdb), 16S rRNA was more of a challenge, in particular regions in which the structure is a little bit disordered, the program has problems telling real basepairs from false ones due to the noise brought about by stacking interactions. The quality of this information should be improved as more structures are processed by the program, specially large ones. IMPORTANT: only really well refined structures should be used to get statistical data, as this information is highly sensitive to the quality of the structure!!!!!
        #"CONTINUITY_HELICITY_CRITERION CRITERION": Initially this criterion was the only one used to identify FALSE basepairs and worked well enough with tRNA. However, tests with 16S rRNA made it clear that this criterion by itself was not enough.
                #The debated basepairs are sent to SECTION FUNCTION "CONTINUITY_HELICITY_CRITERION" to check whether they could form part of a helical element. The debated basepairs will be given a number from "0" to "2", based on the number of helical elements that are continuous with the proposed basepair.
                        #Score
        #"CUTOFF_CRITERION": Performed in SECTION FUNCTION "CUTOFF_CRITERION". Not very valuable and possibly targeted for removal after implementing the "C1-C1 DISTANCE CRITERION".
        #"BOND_CRITERION": Perofrmed in SECTION FUNCTION "BOND_CRITERION". Not very valuable and possibly targeted for removal after implementing the "C1-C1 DISTANCE CRITERION".

#At HIGH STRINGENCY, basepairs with this distance falling beyond the C1-C1 distance average +/- standard deviation are not considered further. Other criteria not considered at this stringency
#At LOW STRINGENCY, the C1-C1 distance average +/- standard deviation values could be used to weigh the rest of the scores, coming from the other discrimination criteria


#The recorded score after submitting the debated basepairs to all the above criteria determines which one of the them is defined as LEGITIMATE and which one as FALSE. A 'LEGITIMATE' tag will be appended to this basepair and a 'FALSE' tag to the other
             #If both basepairs have equal scores associated to them, the 'yes' tag of both will be replaced with a 'no' and an 'UNDETERMINED' tag will be appended to both

#################################
#####SECTION FUNCTION "CONTINUITY_HELICITY_CRITERION"          BEGINNING
###### The following lines are adapted from FUNCTION LOOP. It was too complicated to run again the program through FUNCTION LOOP, so i have adapted the relevant lines of FUNCTION LOOP to check the amount of helix continuity afforded by each of the possible false basepairs.
#Called below

def CONTINUITY_HELICITY_CRITERION(MASTER_Basepairs_summary_line, MASTER_Basepairs_summary, CUTOFF):
                        m=re.search(r'\d+', MASTER_Basepairs_summary_line[2])
                        search1 = m.group()
                        SEARCH1 = to_int(search1) #convert string to integers
                        m=re.search(r'\d+', MASTER_Basepairs_summary_line[4])
                        search2 = m.group()
                        SEARCH2 = to_int(search2) #convert string to integers
                        list = [MASTER_Basepairs_summary_line[2], MASTER_Basepairs_summary_line[3], MASTER_Basepairs_summary_line[4], MASTER_Basepairs_summary_line[5]]
                        convert = CONVERT(list)
                        add = ADD(SEARCH1, SEARCH2)
                        line = "\n...using the CONTINUITY OF HELICITY CRITERION criterion for " + convert[0] + convert[1] + " and " + convert[2] + convert[3]
                        print(line)
                        line1 = "\nChecking whether the basepair formed by residues " + convert[0] + convert[1] + " and " + convert[2] + convert[3] + " could be involved in a helical element\n    Searching for the following possible basepair candidates: " + add[0] + " and " + add[1] + " or " + add[2] + " and " + add[3]
                        print(line1)

#the whole MASTER_Basepairs_summary is searched for the existence of either one of the adjacent basepairs calculated above. Done in FUNCTION HELICITY
                        where_from = 'Loop' #Used in SECTION HELICITY to determine what to do whether the program is coming from 'Loop' or 'CONTINUITY_HELICITY_CRITERION', or from 'REMOVAL'
                        helicity = HELICITY(MASTER_Basepairs_summary, add[0], add[1], add[2], add[3], where_from)
                        count_helicity = 0 #Control output in this region
                        for t in range (0,2):
                            if helicity[t] != []:
                                count_helicity = count_helicity + 1  #Signals entering this 'if'
                                str(helicity[t][0])
                                str(helicity[t][1])
#                                count_found = count_found + 1 #Will keep tract of how many basepairs are found. Serves also to keep the loop going

#One or the 2 adjacent basepairs is found
                                if count_helicity == 1: #One of the 2 possible adjacent basepairs is found. This is enought to assign the basepair whose assignemnt is in question by 'MH' criterion
#                                    MASTER_Basepairs_summary_line.append('MH')  #'MH'Maximization of Helicity TAG
                                    line1  = "     #####\n     !!!!Found an adjacent basepair to residues " + convert[0] + convert[1] + " and "  + convert[2] + convert[3] +  "\n     This basepair is formed by residues " + helicity[t][0] + " and " +  helicity[t][1] + "\n     #####\n"
                                    print(line1)
                                if count_helicity == 2: #The second adjacent basepair has been found. The basepair whose assignemnt is in question has already bee assigned when 'count_helicity == 1'. The program will only output the new finding
                                    line1  = "     #####\n     !!!!Found a second adjacent basepair to residues " + convert[0] + convert[1] + " and "  + convert[2] + convert[3] +  "\n     This basepair is formed by residues " + helicity[t][0] + " and " +  helicity[t][1] + "     \n     #####\n"
                                    print(line1)
#Neither one of the possible adjacent basepairs was found by the CONTINUITY OF HELICITY CRITERION
                        if count_helicity == 0:
                            line1  = "                     Neither basepair was found!!!!!"
                            print(line1)
                        return [MASTER_Basepairs_summary_line, count_helicity]
#####SECTION FUNCTION "CONTINUITY_HELICITY_CRITERION"                END
#################################


#################################
#####SECTION FUNCTION "CUTOFF_CRITERION"          BEGINNING
def CUTOFF_CRITERION(LIST1, LIST2, A0, A1, A2, A3, A4, A5, A6, A7):
#LIST1 = MASTER_Basepairs_summary[i][j]
#LIST2 = MASTER_Basepairs_summary[l][m]
#A0 = convert[0]
#A1 = convert[1]
#A2 = convert[2]
#A3 = convert[3]
#A4 = convert[4]
#A5 = convert[5]
#A6 = convert[6]
#A7 = convert[7]
    score = [0, 0]
    print("\n...using the CUTOFF criterion")
    #The CUTOFF string has to be converted back into an 'float' number
    float_1 = float(LIST1[len(LIST1) - 1][0]) + float(LIST1[len(LIST1) - 1][2])/10
    float_2 = float(LIST2[len(LIST2) - 1][0]) + float(LIST2[len(LIST2) - 1][2])/10
    line = "\n     The basepair formed by residues " + A0 + A1 + ":" + A2 + A3 + " and with a " + LIST1[0] + " geometry was assigned at a CUTOFF value of " + LIST1[len(LIST1) - 1]  + " whereas the basepair formed by residues " + A4 + A5 + ":" + A6 + A7 + "and with a " + LIST2[0] + " geometry was assigned at a CUTOFF value of " + LIST2[len(LIST2) - 1]
    if float_1 < float_2:
        print(line)
        line1 = "          therefore the " + A0 + A1 + ":" + A2 + A3 + " basepair, with a " + LIST1[0] + " geometry, fares better than the " + A4 + A5 + ":" + A6 + A7 + " basepair, with a " +  LIST2[0] + " geometry by the CUTOFF criterion"
        print(line1)
        score = [2, 0]

    elif float_1 > float_2:
        print(line)
        line1 = "          therefore the " + A4 + A5 + ":" + A6 + A7 + " basepair, with a " + LIST2[0] + " geometry, fares better than the " + A0 + A1 + ":" + A2 + A3 + " basepair, with a " +  LIST1[0] + " geometry by the CUTOFF criterion"
        print(line1)
        score = [0 , 2]
    else:
        print("\n     Both basepairs have been assigned at the same CUTOFF value of " + LIST2[len(LIST2) - 1])

    return score

#####SECTION FUNCTION "CUTOFF_CRITERION"          END
#################################

#################################
#####SECTION FUNCTION "BOND_CRITERION"          BEGINNING
def BOND_CRITERION(LIST1, LIST2, A0, A1, A2, A3, A4, A5, A6, A7):
#LIST1 = MASTER_Basepairs_summary[i][j]
#LIST2 = MASTER_Basepairs_summary[l][m]
#A0 = convert[0]
#A1 = convert[1]
#A2 = convert[2]
#A3 = convert[3]
#A4 = convert[4]
#A5 = convert[5]
#A6 = convert[6]
#A7 = convert[7]
     score = [0, 0]
                #number of bonds criterion. The larger this number, the more chances that the basepair is correctly assigned
     print("\n...using the BOND criterion")
     if LIST1[6] > LIST2[6]:
         line = "     therefore the " + A0 + A1 + ":" + A2 + A3 + " basepair fares better under the 'NUMBER-OF-BONDS CRITERION' than the " + A4 + A5 + ":" + A6 + A7 + "basepair."
         score[0] = 1
         score[1] = 0
     elif LIST1[6] < LIST2[6]:
         line = "     therefore the " + A4 + A5 + ":" + A6 + A7 + " basepair fares better under the 'NUMBER-OF-BONDS CRITERION' than the " + A0 + A1 + ":" + A2 + A3 + "basepair."
         score[1] = 1
         score[0] = 0
     else:
         line = "     therefore the 'NUMBER-OF-BONDS CRITERION' cannot be used to sort out basepair legitimacy"
     A = str(LIST1[6])
     B = str(LIST2[6])
     line1 = A + " bonds have been identified for the " + A0 + A1 + ":" + A2 + A3 + " basepair and " + B + " bonds have been identified for the " + A4 + A5 + ":" + A6 + A7 + " basepair."
     print(line1)
     print(line)

     return score

#####SECTION FUNCTION "BOND_CRITERION"          END
#################################

#################################
#####SECTION FUNCTION "DECISSIONS"              BEGINNING
def DECISSION(LIST1, LIST2, A0, A1, A2, A3, A4, A5, A6, A7, removed, undetermined, two_base_shared):
#LIST1 and LIST2 can either be MASTER_Basepairs_summary[i][j] or MASTER_Basepairs_summary[l][m] depending on their scores
#A0 = convert[0]
#A1 = convert[1]
#A2 = convert[2]
#A3 = convert[3]
#A4 = convert[4]
#A5 = convert[5]
#A6 = convert[6]
#A7 = convert[7]
    if undetermined == 'n':
        if two_base_shared == "n":
           line = "The basepair formed by residues " + A4 + A5 + ":" + A6 + A7 + " and with geometry " + LIST2[0] + " will be assigned as 'FALSE'. Please inspect this basepair and its surroundings for the presence of additional misassigned basepairs"
           LIST2.append('FALSE')
        else:
           line = "The basepair formed by residues " + A4 + A5 + ":" + A6 + A7 + " and with geometry " + LIST2[0] + " will be assigned as 'WRONG'. Please inspect this basepair and its surroundings for the presence of additional misassigned basepairs"
           LIST2.append('WRONG')
        print(line)
        line = "The basepair formed by residues " + A0 + A1 + ":" + A2 + A3 + " and with geometry " + LIST1[0] + " will be assigned as 'LEGITIMATE'"
        print(line)
        print("################")
        LIST1[19:19] = ['LEGITIMATE']
    else:
        line = "\nBasepairs, " + convert[0] + convert[1] + ":" + convert[2] + convert[3] + " with geometry " + MASTER_Basepairs_summary[i][j][0] + " and " + convert[4] + convert[5] + ":" + convert[6] + convert[7] +  " with geometry " + MASTER_Basepairs_summary[l][m][0] + " cannot be deconvoluted. Their assignment as legitimate basepairs remains 'UNDETERMINED'"
        print(line)
        print("################")
        LIST1.append('UNDETERMINED')
        LIST2.append('UNDETERMINED')
    removed = removed + 1

    return [LIST1, LIST2, removed]
#####SECTION FUNCTION "DECISSIONS"              END
#################################

######################################################
#####MAIN part of SECTION DETECTION OF FALSE BASEPAIRS
#MASTER_Basepairs_summary_bak = MASTER_Basepairs_summary
print("SEARCHING FOR ERRONEOUSLY ASSIGNED BASEPAIRS")

CONTINUITY_HELICITY_CRITERION_output = [] #Will collect the output of 'FUNCTION CONTINUITY_HELICITY_CRITERION'
while_control = 'y' #Will end the next 'while' loop when == 'n'
run_while = 1
stringency = 'HIGH' #Will signal whether the next 'while' loop will be executed with 'HIGH' or 'LOW' stringency
single_base_shared = 'n' #When 'y', it will signal that deconvolution of basepairs sharing one base can start
two_base_shared = 'n' #When 'y', it will signal that deconvolution of basepairs sharing two bases can start. Requires prior deconvolution of basepairs sharing a single base

###############
#'FALSE-BASEPAIR-ASSIGNMENT LOOP'             BEGINNING
MODE = "Single base sharing"
while while_control == 'y':

    removed = 0 #This variable will keep track of how many basepairs sharing bases are deconvoluted in this loop
    if two_base_shared == 'y':
       line = "deconvoluting candidate basepairs sharing both bases\n"
       run_while = 1
    else:
       line = "deconvoluting candidate basepairs sharing one base\n"
    print("#######################################################\n#####SEARCHING FOR ERRONEOUSLY ASSIGNED BASEPAIRS: STRINGENCY ", stringency, " Run =", run_while, "\nMODE =", MODE, "\n#######################################################\n")
#, line, "\nremoved", removed, " while_control", while_control, " two_base_shared", two_base_shared, " single_base_shared", single_base_shared, " stringency", stringency
    for i in range (len(MASTER_Basepairs_summary)):
        if (len(MASTER_Basepairs_summary[i]) > 0):
            for j in range (len(MASTER_Basepairs_summary[i])):


                if (('REMOVED' not in MASTER_Basepairs_summary[i][j]) and ('FALSE' not in MASTER_Basepairs_summary[i][j]) and ('WRONG' not in MASTER_Basepairs_summary[i][j]) and ('UNDETERMINED' not in MASTER_Basepairs_summary[i][j])) and (('D' in MASTER_Basepairs_summary[i][j]) or ('MH' in MASTER_Basepairs_summary[i][j]) or ('ELI' in MASTER_Basepairs_summary[i][j])):
                    for l in range (len(MASTER_Basepairs_summary)):
                        if (len(MASTER_Basepairs_summary[l]) > 0):
                            for m in range (len(MASTER_Basepairs_summary[l])):
                                 if (('REMOVED' not in MASTER_Basepairs_summary[i][j]) and ('FALSE' not in MASTER_Basepairs_summary[l][m]) and ('WRONG' not in MASTER_Basepairs_summary[l][m]) and ('UNDETERMINED' not in MASTER_Basepairs_summary[l][m])) and (('D' in MASTER_Basepairs_summary[l][m]) or ('MH' in MASTER_Basepairs_summary[l][m]) or ('ELI' in MASTER_Basepairs_summary[l][m])):
                                     list = [MASTER_Basepairs_summary[i][j][2], MASTER_Basepairs_summary[i][j][3], MASTER_Basepairs_summary[i][j][4], MASTER_Basepairs_summary[i][j][5], MASTER_Basepairs_summary[l][m][2], MASTER_Basepairs_summary[l][m][3], MASTER_Basepairs_summary[l][m][4], MASTER_Basepairs_summary[l][m][5]]
                                     convert = CONVERT(list)
                                     SCORE = [[],[],[]] #To keep track of the performance of the debated basepairs in each of the three CRITERIA that will be used to sort out their legitimacy. FORMAT: 'SCORE[0] will be used for CONTINUITY_HELICITY_CRITERION', 'SCORE[1] will be used for CUTOFF_CRITERION', and 'SCORE[2] will be used for BOND_CRITERION'
                                     score = [[],[]] #'score[0]' and 'score[1]' will carry a value representative of how the debated basepairs fare under all criteria used in the loop+
                                     single_base_shared = 'n' ##When 'y', it will signal that deconvolution of basepairs sharing one base can start
                                     if (MASTER_Basepairs_summary[i][j][2] in MASTER_Basepairs_summary[l][m]) and (MASTER_Basepairs_summary[i][j][4] not in MASTER_Basepairs_summary[l][m]):
                                        single_base_shared = 'y'
#                                        print "single_base_shared = 'y' MASTER_Basepairs_summary[i][j]", MASTER_Basepairs_summary[i][j], "\nMASTER_Basepairs_summary[l][m]", MASTER_Basepairs_summary[l][m]
                                     if (MASTER_Basepairs_summary[i][j][2] not in MASTER_Basepairs_summary[l][m]) and (MASTER_Basepairs_summary[i][j][4] in MASTER_Basepairs_summary[l][m]):
                                        single_base_shared = 'y'
#                                        print "single_base_shared = 'y' MASTER_Basepairs_summary[i][j]", MASTER_Basepairs_summary[i][j], "\nMASTER_Basepairs_summary[l][m]", MASTER_Basepairs_summary[l][m]

######CASE 1: Only one base is shared. Most of the times this is the case.
#In this case there are three criteria that could be used to sort out which of the two is 'LEGITIMATE' and which one is 'FALSE'.
                                     if single_base_shared == 'y':
                                        line = "\n################\nBasepair, " + convert[0] + convert[1] + ":" + convert[2] + convert[3] + " with geometry " + MASTER_Basepairs_summary[i][j][0] + " and basepair " + convert[4] + convert[5] + ":" + convert[6] + convert[7] + " with geometry " + MASTER_Basepairs_summary[l][m][0] + " share one base!!!!!!\nAttempting to deconvolute these two basepairs with " + stringency + " stringency by"
                                        print(line)

        #CRITERION #1: CONTINUITY_HELICITY_CRITERION: Checking helicity of the basepair. This is done in 'FUNCTION CONTINUITY_HELICITY_CRITERION', where the basepair will be assigned a number between '0' and '2' depending on the number of helical elements that are continuous with it. HIGHEST PRIORITY!!
                                        CONTINUITY_HELICITY_CRITERION_output = CONTINUITY_HELICITY_CRITERION(MASTER_Basepairs_summary[i][j], MASTER_Basepairs_summary, CUTOFF)
                                        MASTER_Basepairs_summary[i][j] = CONTINUITY_HELICITY_CRITERION_output[0]
                                        SCORE[0].append(CONTINUITY_HELICITY_CRITERION_output[1] * 4)
                                        CONTINUITY_HELICITY_CRITERION_output = CONTINUITY_HELICITY_CRITERION(MASTER_Basepairs_summary[l][m], MASTER_Basepairs_summary, CUTOFF)
                                        MASTER_Basepairs_summary[l][m] = CONTINUITY_HELICITY_CRITERION_output[0]
                                        SCORE[0].append(CONTINUITY_HELICITY_CRITERION_output[1] * 4)
                                        print("Number of helical elements continuous with basepair, ", convert[0], convert[1], ":", convert[2], convert[3], " = ", int(SCORE[0][0]/4))
                                        print("Number of helical elements continuous with basepair, ", convert[4], convert[5], ":", convert[6], convert[7], " = ", int(SCORE[0][1]/4))

#                                        print "AFTER CONTINUITY_HELICITY_CRITERION SCORE", SCORE, "\nscore", score
        #CRITERION #2: CUTOFF_CRITERION: the smaller the CUTOFF value, the more chances that the basepair is correctly assigned. MEDIUM PRIORITY.
                                   #The cutoff string is at MASTER_Basepairs_summary[i][j][len(MASTER_Basepairs_summary[i][j]) - 1] and MASTER_Basepairs_summary[l][m][len(MASTER_Basepairs_summary[l][m]) - 1]

                                        SCORE[1] = CUTOFF_CRITERION(MASTER_Basepairs_summary[i][j], MASTER_Basepairs_summary[l][m], convert[0], convert[1], convert[2], convert[3], convert[4], convert[5], convert[6], convert[7])
#                                        print "AFTER CUTOFF_CRITERION SCORE", SCORE, "\nscore", score
        #CRITERION #3: BOND_CRITERION: The larger the number of bonds between two bases, the more chances that the basepair is correctly assigned. Should have the lowest priority. LOWEST PRIORITY.
                                        SCORE[2] = BOND_CRITERION(MASTER_Basepairs_summary[i][j], MASTER_Basepairs_summary[l][m], convert[0], convert[1], convert[2], convert[3], convert[4], convert[5], convert[6], convert[7])

####CASE 2: both bases are shared, the distances between ATOMS for these bases is compatible with more than 1 basepair geometry. SHOULD ONLY BE PERFORMED AFTER ALL BASEPAIRS WHICH SHARE A SINGLE BASE HAVE BEEN SORTED OUT
        #In this case there are two criteria that could be used to sort out which of the two geometries is 'LEGITIMATE' and which one is 'FALSE':
                                     if (two_base_shared == 'y') and (MASTER_Basepairs_summary[i][j][2] == MASTER_Basepairs_summary[l][m][2]) and (MASTER_Basepairs_summary[i][j][4] ==  MASTER_Basepairs_summary[l][m][4]) and not ((i == l) and (j == m)):
                                         SCORE[0] = [0, 0]
        #CRITERION #1: CUTOFF_CRITERION: the smaller the CUTOFF value, the more chances that the basepair is correctly assigned.
                                   #The cutoff string is at MASTER_Basepairs_summary[i][j][len(MASTER_Basepairs_summary[i][j]) - 1] and MASTER_Basepairs_summary[l][m][len(MASTER_Basepairs_summary[l][m]) - 1]
#                                         print "MASTER_Basepairs_summary[i][j]", MASTER_Basepairs_summary[i][j], "/nMASTER_Basepairs_summary[l][m]", MASTER_Basepairs_summary[l][m]
                                         line = "\n################\nTwo possible geometries for the basepair formed by " + convert[0] + convert[1] + ":" + convert[2] + convert[3] + ", i.e.: " + MASTER_Basepairs_summary[i][j][0] + " and " + MASTER_Basepairs_summary[l][m][0]
                                         print(line)
                                         SCORE[1] = CUTOFF_CRITERION(MASTER_Basepairs_summary[i][j], MASTER_Basepairs_summary[l][m], convert[0], convert[1], convert[2], convert[3], convert[4], convert[5], convert[6], convert[7])
        #CRITERION #2: BOND_CRITERION: The larger the number of bonds between two bases, the more chances that the basepair is correctly assigned. Should have the lowest priority
                                         SCORE[2] = BOND_CRITERION(MASTER_Basepairs_summary[i][j], MASTER_Basepairs_summary[l][m], convert[0], convert[1], convert[2], convert[3], convert[4], convert[5], convert[6], convert[7])

#####DECISIONS
#The 'FALSE-BASEPAIR-ASSIGNMENT LOOP' will run at highest stringency until no more FALSE basepairs can be removed, i.e, when 'removed' = 0 and 'while_control' = 'n'.
        #Highest stringency means that in order to assign a pair of debated basepairs as LEGITIMATE and FALSE, the former has to fare better than the latter under all three CRITERIA
        #Therefore, to be assigned as LEGITIMATE and FALSE, one of the debated basepairs must have a score of '3' and the other a score of '0'
                                     if SCORE != [[],[],[]]:
#                                         print "\nENTERING DECISSIONS removed", removed, " while_control", while_control, " two_base_shared", two_base_shared, " single_base_shared", single_base_shared, " stringency", stringency, "score", score
                                         print("CALCULATING SCORE", SCORE, end=' ')
                                         score[0] = SCORE[0][0] + SCORE[1][0] + SCORE[2][0]
                                         score[1] = SCORE[0][1] + SCORE[1][1] + SCORE[2][1]
                                         print("CALCULATING score", score)
                                         decission = []
                                         if (stringency == 'HIGH'):
                                             undetermined = 'n'
                                             if score[0] >= 7:
                                                 decission = DECISSION(MASTER_Basepairs_summary[i][j], MASTER_Basepairs_summary[l][m], convert[0], convert[1], convert[2], convert[3], convert[4], convert[5], convert[6], convert[7], removed, undetermined, two_base_shared)
                                             elif score[1] >= 7:
                                                 decission = DECISSION(MASTER_Basepairs_summary[l][m], MASTER_Basepairs_summary[i][j], convert[4], convert[5], convert[6], convert[7], convert[0], convert[1], convert[2], convert[3], removed, undetermined, two_base_shared)
#                                             print "END DECISSIONS with stringency HIGH: removed", removed, "MASTER_Basepairs_summary[i][j] ", MASTER_Basepairs_summary[i][j], "\nMASTER_Basepairs_summary[l][m] ", MASTER_Basepairs_summary[l][m]
                                         elif (stringency == 'LOW'):
                                             if score[0] > score[1]:
#                                                 print "score[0] > score[1]"
                                                 undetermined = 'n'
                                                 decission = DECISSION(MASTER_Basepairs_summary[i][j], MASTER_Basepairs_summary[l][m], convert[0], convert[1], convert[2], convert[3], convert[4], convert[5], convert[6], convert[7], removed, undetermined, two_base_shared)
                                             elif score[0] < score[1]:
#                                                 print "score[0] < score[1]"
                                                 undetermined = 'n'
                                                 decission = DECISSION(MASTER_Basepairs_summary[l][m], MASTER_Basepairs_summary[i][j], convert[4], convert[5], convert[6], convert[7], convert[0], convert[1], convert[2], convert[3], removed, undetermined, two_base_shared)
                                             else:
                                                 undetermined = 'y'
                                                 decission = DECISSION(MASTER_Basepairs_summary[l][m], MASTER_Basepairs_summary[i][j], convert[0], convert[1], convert[2], convert[3], convert[4], convert[5], convert[6], convert[7], removed, undetermined, two_base_shared)

                                         if decission != []:
                                             MASTER_Basepairs_summary[i][j] = decission[0]
                                             MASTER_Basepairs_summary[l][m] = decission[1]
                                             removed = decission[2]
#                                         print "END DECISSIONS with stringency LOW: removed", removed, "MASTER_Basepairs_summary[i][j] ", MASTER_Basepairs_summary[i][j], "\nMASTER_Basepairs_summary[l][m] ", MASTER_Basepairs_summary[l][m], "\nscore[1]", score[1], "score[0]", score[0]


#Loop control:
    print("Number of succesful deconvolutions =", removed)
    if two_base_shared == 'y':
       while_control = 'n' #Ends the 'while' loop
    if stringency == 'LOW' and removed == 0:
       two_base_shared = 'y' #Now the program will search for basepairs with both bases shared and then exit the loop
       MODE = "Two base sharing"
    if stringency == 'HIGH' and removed == 0:
       stringency = 'LOW'
       run_while = 0
#Loop control:

    run_while = run_while + 1


#'FALSE-BASEPAIR-ASSIGNMENT LOOP'             END

#####MAIN part of SECTION DETECTION OF FALSE BASEPAIRS
######################################################
print_control = 0
line1 = " AFTER DETECTION OF FALSE BASEPAIRS"
SEQUENTIAL_READOUT(MASTER_Basepairs_summary, MASTER_Basepairs_schemes, control, print_control, line1)
##########################################################################
##########################################################################
##########################################################################
#SECTION DETECTION OF FALSE BASEPAIRS                  END
##########################################################################
##########################################################################
##########################################################################




##########################################################################
##########################################################################
##########################################################################
#SECTION ORDERING OUTPUT                             BEGINNING
##########################################################################
##########################################################################
##########################################################################

print("\n##########################################################################\nORDERING OUTPUT\n##########################################################################\n")
new_list_end = [] #Will collect all legitimate basepairs from MASTER_Basepairs_summary, from largest first residue to smallest
max = []
yes_count = 1 #This will allow to enter the next 'while'where it will be made '0'
while (yes_count > 0):
    yes_count = 0 #Initializing, will count the # of 'yes' lines in left in MASTER_Basepairs_summary after they are sequentially removed by this loop
    for i in range (len(MASTER_Basepairs_summary)):
       if (len(MASTER_Basepairs_summary[i]) > 0):
          for j in range (len(MASTER_Basepairs_summary[i])):
              if len(MASTER_Basepairs_summary[i][j]) > 0:
                   if 'yes' in MASTER_Basepairs_summary[i][j]:
                      if yes_count == 0: #Initializing 'min' and 'max'
                          count = 1
                          m=re.search(r'\d+', MASTER_Basepairs_summary[i][j][2])
                          search1 = m.group()
                          compare_to = to_int(search1)
                          max = [compare_to, i, j]
                          yes_count = 1
                      else:
                          m=re.search(r'\d+', MASTER_Basepairs_summary[i][j][2])
                          search1 = m.group()
                          transient = to_int(search1)
                          if transient >= max[0]:
                              max = [transient, i, j]
                          yes_count = yes_count + 1
    if yes_count > 0:
            new_list_end.append(MASTER_Basepairs_summary[max[1]][max[2]])
            MASTER_Basepairs_summary[max[1]][max[2]] = []

for i in range (len(new_list_end)):
   print(new_list_end[i])
##########################################################################
##########################################################################
##########################################################################
#SECTION ORDERING OUTPUT                             END
##########################################################################
##########################################################################
##########################################################################


##########################################################################
##########################################################################
##########################################################################
#SECTION STATISTICS                                  BEGINNING
##########################################################################
##########################################################################
##########################################################################

#Now the list of baseipairs has been ordered and contained in 'new_list_end'. The list will be outputed in several different ways, under two main categories: text output and PYMOL files.
##################################
#####SECTION FUNCTION "ADD_SPACES"
def ADD_SPACES(STRING, NUM):
#STRING = new_list_end[count_for][l]
#NUM = key[l]

    result = NUM - len(STRING)
    space = " "
    STRING = space*result + STRING
    return STRING
#####SECTION FUNCTION "ADD_SPACES"
##################################

###################################
#####SECTION FUNCTION "DIFF_CALC"
def DIFF_CALC(ARR1, ARR2, LIST1, LIST2):
#ARR1 = new_list_end_floats[b,:]
#ARR2 = STATS[c,:]
#LIST1 = can be 'new_list_end[b]',falsely_assigned[b], wrongly_assigned[b]
#LIST2 = MASTER_Basepairs_bonds[c]
#    print "ARR1", ARR1, "ARR2", ARR2, "\LIST1", LIST1
    diff_1 = str(ARR1[0] - ARR2[0])
    diff_2 = str(ARR1[1] - ARR2[2])
    diff_3 = str(ARR1[2] - ARR2[4])
#    diff_P_dist = str(ARR1[3] - ARR2[6])
#    diff_C1_dist = str(ARR1[4] - ARR2[8])
    if LIST2[9] != "NA":
        diff_P_dist = str(ARR1[3] - LIST2[9])
        diff_C1_dist = str(ARR1[4] - LIST2[11])
    else:
        diff_P_dist = str(ARR1[3] - ARR2[6])
        diff_C1_dist = str(ARR1[4] - ARR2[8])
    list = [diff_1[:6], diff_2[:6], diff_3[:6], diff_P_dist[:6], diff_C1_dist[:6]]
    key = 5
    for l in range (len(list)):
        if list[l] == '0.0':
           list[l] = '0.000'
        if list[l][0] != "-":
           list[l] = ' ' + list[l][0:5]
        if len(list[l]) < key:
           list[l] = ADD_SPACES(list[l], key)
#To format the distances so that the ones carrying a '*' do not jutt out
    if len(LIST1[9]) == 5:
        LIST1[9] = LIST1[9] + ' '
    if len(LIST1[12]) == 5:
        LIST1[12] = LIST1[12] + ' '
    if len(LIST1[15]) == 5:
        LIST1[15] = LIST1[15] + ' '
    print(LIST1[0], "", LIST1[2], " ", LIST1[3], " ", LIST1[4], " ", LIST1[5], "    ", LIST1[1], "    ", LIST1[6], "  ", LIST1[7], " ", LIST1[8], LIST1[9], list[0], LIST1[16], list[3], LIST1[17], list[4])
    print("                                                     ", LIST1[10], " ", LIST1[11], LIST1[12], list[1])
    print("                                                     ", LIST1[13], " ", LIST1[14], LIST1[15], list[2])

    return list
#####SECTION FUNCTION "DIFF_CALC"
##################################


key = [] #Will carry information to format the output
        #OUTPUT FIELDS
                #position 0: "Basepairing Scheme"                                     STRING of 9 characters
key.append(11)
                #position 1: "number of bonds expected"                               STRING of 1 character
key.append(1)
                #position 2: "Base # for base 1"                                      STRING of 4 characters
key.append(4)
                #position 3: "Base identity for base 1"                               STRING of 3 characters
key.append(3)
                #position 4: "Base # for base 2"                                      STRING of 4 characters
key.append(4)
                #position 5: "Base identity for base 2"                               STRING of 3 characters
key.append(3)
                #position 6: "number of bonds found"                                  STRING of 1 character
key.append(1)
                #position 7: "atom 1 for bond 1"                                      STRING of 2 characters
key.append(2)
                #position 8: "atom 2 for bond 1"                                      STRING of 2 characters
key.append(2)
                #position 9: distance for H-bond 1                                    STRING of 4 characters
key.append(4)
                #position 10: "atom 1 for bond 2"                                     STRING of 2 characters
key.append(2)
                #position 11: "atom 2 for bond 2"                                     STRING of 2 characters
key.append(2)
                #position 12: distance for H-bond 2                                   STRING of 4 characters
key.append(4)
                #position 13: "atom 1 for bond 3"                                     STRING of 2 characters
key.append(2)
                #position 14: "atom 2 for bond 3"                                     STRING of 2 characters
key.append(2)
                #position 15: distance for H-bond 2                                   STRING of 4 characters
key.append(4)
                #position 16 "P-P bond distance"                                      STRING of 4 characters
key.append(5)
                #position 17 "Information regarding involvement in MULTIPLE CONTACTS", carried in new_list_end[count_for][len(new_list_end[count_for])-3], or at new_list_end[count_for][len(new_list_end[count_for])-4] if the basepair was assigned by 'MH' or 'ELI'. The information in this string is either '0' (not involved in MULTIPLE CONTACTS) or '1' (involved in MULTIPLE CONTACTS) but will be switched to 'NO' or 'YES', which will make more sense during statistics                                                                       STRING of 3 characters
key.append(3)
                #position 18 "Method used to assign the basepair", carried in new_list_end[count_for][len(new_list_end[count_for])-2]                                              STRING of 3 characters
key.append(3)
                #position 19 "CUTOFF", carried in new_list_end[count_for][len(new_list_end[count_for])-1]                                              STRING of 3 characters
key.append(3)

###########################FORMATTING 'new_list_end'############################################
print("######################################\n##########ORDERED RAW OUTPUT##########        BEGINNING\n######################################")
print("############### '*' notes bonds with length larger tan maximum distance cutoff ###############")
list = []
count_for = len(new_list_end) - 1 #Controld the next 'while'
new_list_end_floats = numpy.zeros(5 * len(new_list_end)).reshape(len(new_list_end), 5) #Will have as many lines as 'new_list_end', so it can be accessed just like 'new_list_end, but will carry only the bond- and P-distances as float, so they can be used in statistical operations
while count_for > -1:
           line1 = ''

####Finding Missing Bonds
#To allow STATISTICS regarding bond distances in the assigned basepairs, the missing bonds for 3-bond basepairs with only 2 bonds identified need to be found.
           missing = []
           if (new_list_end[count_for][1] == 3) and (new_list_end[count_for][6] == 2):
# and ('FALSE' not in new_list_end[count_for]) and ('UNDETERMINED' not in new_list_end[count_for]):
               print("Basepair formed by residues", new_list_end[count_for][2], " and", new_list_end[count_for][4], "is missing one bond. Searching for missing bond")
               missing_control = "0"
#               print "new_list_end[count_for]", new_list_end[count_for]
          #Let's first find the basepair scheme
               for i in range (len(MASTER_Basepairs_schemes)):
                   if MASTER_Basepairs_schemes[i] == new_list_end[count_for][0]:
#                       print "MASTER_Basepairs_schemes[i] ", MASTER_Basepairs_schemes[i], "len(MASTER_Basepairs_schemes)", len(MASTER_Basepairs_schemes), "MASTER_Basepairs_bonds[i] ", MASTER_Basepairs_bonds[i], "len(MASTER_Basepairs_bonds)", len(MASTER_Basepairs_bonds), "len(MASTER_Basepairs_excluded)", len(MASTER_Basepairs_excluded)
#                       for q in range (len(MASTER_Basepairs_excluded[i])):
#                           print MASTER_Basepairs_excluded[i][q]
          #Let's now find the location of the missing bonds in new_list_end[count_for] and in MASTER_Basepairs_bonds[i]. Append the location and the identity of the participating ATOMS to the list 'missing'
                       for j in range (len(new_list_end[count_for])):
                           if new_list_end[count_for][j] == []:
                               if j == 7:
                                  missing.append(j)
                                  missing.append(MASTER_Basepairs_bonds[i][3])
                                  missing.append(j+1)
                                  missing.append(MASTER_Basepairs_bonds[i][4])
                                  break
                               elif j == 10:
                                  missing.append(j)
                                  missing.append(MASTER_Basepairs_bonds[i][5])
                                  missing.append(j+1)
                                  missing.append(MASTER_Basepairs_bonds[i][6])
                                  break
                               elif j == 13:
                                  missing.append(j)
                                  missing.append(MASTER_Basepairs_bonds[i][7])
                                  missing.append(j+1)
                                  missing.append(MASTER_Basepairs_bonds[i][8])
                                  break
          #Let's now find the line in MASTER_Basepairs that carries the missing bond
#                       print "missing", missing
                       for k in range (len(MASTER_Basepairs_excluded[i])):
#                           print "MASTER_Basepairs_excluded[i][k]", MASTER_Basepairs_excluded[i][k]
                           if (new_list_end[count_for][2] == MASTER_Basepairs_excluded[i][k][0]) and (new_list_end[count_for][4] == MASTER_Basepairs_excluded[i][k][3]):
                               if (missing[1] in MASTER_Basepairs_excluded[i][k][2]) and (missing[3] in MASTER_Basepairs_excluded[i][k][5]):
                                  new_list_end[count_for][missing[0]] = MASTER_Basepairs_excluded[i][k][2]
                                  new_list_end[count_for][missing[2]] = MASTER_Basepairs_excluded[i][k][5]
                                  new_list_end[count_for][missing[2]+1] = MASTER_Basepairs_excluded[i][k][6]
                                  missing_control = "1"
                       if missing_control == "0": #This will prevent the program from crashing if a bond remains missing. Following these basepairs can lead to the identification of new basepair geometries
                             new_list_end[count_for][missing[0]] = "1"
                             new_list_end[count_for][missing[2]] = "1"
                             new_list_end[count_for][missing[2]+1] = "11"
                             #The newly found bond will be marked for output a few lines below
####Finding Missing Bonds



#DELETING UNWANTED SPACES and SAVING THE DISTANCE AS FLOATS IN 'new_list_end_floats' FOR FUTURE MATHEMATICAL OPERATIONS
        #For basepairs with only 2 bonds identified, positions 13, 14, and 15 will have []. Let's replace them with a string carrying '0', or '00' for new_list_end[count_for][15], so 'FUNCTION CONVERT' will work properly (see also next explanation)
#           print "\nnew_list_end[count_for]", new_list_end[count_for]
#           print "count_for", count_for, "new_list_end_floats[count_for]", new_list_end_floats[count_for]
           if "11" != new_list_end[count_for][15] and "11" != new_list_end[count_for][12] and "11" != new_list_end[count_for][9]:
               if new_list_end[count_for][13] == []: #Basepairs with only 2 bonds
                   print("new_list_end[count_for]", new_list_end[count_for])
                   new_list_end[count_for][13] = '0'
                   new_list_end[count_for][14] = '0'
                   new_list_end[count_for][15] = '00'
                   new_list_end_floats[count_for,:] = new_list_end_floats[count_for,:] + [new_list_end[count_for][9], new_list_end[count_for][12], 0, new_list_end[count_for][16], new_list_end[count_for][17]]
               else:
                   print("new_list_end[count_for]", new_list_end[count_for])
                   new_list_end_floats[count_for,:] = new_list_end_floats[count_for,:] + [new_list_end[count_for][9], new_list_end[count_for][12], new_list_end[count_for][15], new_list_end[count_for][16], new_list_end[count_for][17]]
                   a3 = str(new_list_end[count_for][15])
                   new_list_end[count_for][15] = a3[:5]
               a1 = str(new_list_end[count_for][9])
               new_list_end[count_for][9] = a1[:5]
               a2 = str(new_list_end[count_for][12])
               new_list_end[count_for][12] = a2[:5]
               a3 = str(new_list_end[count_for][16])
               if new_list_end[count_for][16] < 10:
                   new_list_end[count_for][16] = ' ' + a3[:5]
               if new_list_end[count_for][16] >= 10:
                   new_list_end[count_for][16] = a3[:6]
               a4 = str(new_list_end[count_for][17])
               new_list_end[count_for][17] = a4[:6]
        #positions 2, 4, 7 and 8, 10 and 11, 13 and 14,  have lefthand spaces. They need to be removed. Strings "new_list_end[count_for][7]" and above are introduced twice to make 'FUNCTION CONVERT' work properly. First time through "FUNCTION CONVERT" the 'number' part of the string is extracted and the second time the 'word' part of the string is extracted

               list = [new_list_end[count_for][2], new_list_end[count_for][3], new_list_end[count_for][4], new_list_end[count_for][5], new_list_end[count_for][7], new_list_end[count_for][7], new_list_end[count_for][8], new_list_end[count_for][8], new_list_end[count_for][10], new_list_end[count_for][10], new_list_end[count_for][11], new_list_end[count_for][11], new_list_end[count_for][13], new_list_end[count_for][13], new_list_end[count_for][14], new_list_end[count_for][14]]

               convert = CONVERT(list)
               new_list_end[count_for][2] = convert[0]
               new_list_end[count_for][4] = convert[2]
               new_list_end[count_for][7] = convert[5] + convert[4]
               new_list_end[count_for][8] = convert[7] + convert[6]
               new_list_end[count_for][10] = convert[9] + convert[8]
               new_list_end[count_for][11] = convert[11] + convert[10]
               new_list_end[count_for][13] = convert[13] + convert[12]
               new_list_end[count_for][14] = convert[15] + convert[14]
               if new_list_end[count_for][13] == '00':
                   new_list_end[count_for][13] = '--'
                   new_list_end[count_for][14] = '--'
                   new_list_end[count_for][15] = '  -- '
               if new_list_end[count_for][7] == '11' or new_list_end[count_for][10] == '11' or new_list_end[count_for][13] == '11':
                   new_list_end[count_for][13] = 'NF'
                   new_list_end[count_for][14] = 'NF'
                   new_list_end[count_for][15] = '  NF '

#DELETING UNWANTED SPACES

#ADDING DESIRED SPACES
#Adding spaces for format. In conjuction with 'SECTION FUNCTION ADD_SPACES'
           #Formatting most of the info carried in 'new_list_end'
           for l in range (0,17): #For most of the info carried in 'new_list_end'
              a = str(new_list_end[count_for][l])
              if len(a) < key[l]:
                 new_list_end[count_for][l] = ADD_SPACES(a, key[l])
              else:
                 new_list_end[count_for][l] = a
              if missing != [] and l == missing[2]+1 and missing_control == "1":
#                 print "[missing[2]+1] ", [missing[2]+1], "and l", l
                 new_list_end[count_for][missing[2]+1] = new_list_end[count_for][missing[2]+1] + '*'
#                 print "AFTER append new_list_end[count_for][missing[2]+1]", new_list_end[count_for][missing[2]+1]
              line1 = line1 + new_list_end[count_for][l] + ' '
           #Formatting the method ('MH', 'D', 'ELI'), carried in new_list_end[count_for][len(new_list_end[count_for])-3], after appending the 'MULTIPLE CONTACTS' information, right above
           if ('FALSE' in new_list_end[count_for]) or ('WRONG' in new_list_end[count_for]):
               new_list_end[count_for][len(new_list_end[count_for])-3] = ADD_SPACES(new_list_end[count_for][len(new_list_end[count_for])-3], key[17])
               line1 = line1 + new_list_end[count_for][len(new_list_end[count_for])-3] + ' '
           else:
               new_list_end[count_for][len(new_list_end[count_for])-2] = ADD_SPACES(new_list_end[count_for][len(new_list_end[count_for])-2], key[17])
               line1 = line1 + new_list_end[count_for][len(new_list_end[count_for])-2] + ' '
           #Formatting the cutoff, carried in new_list_end[count_for][len(new_list_end[count_for])-2], after appending the 'MULTIPLE CONTACTS' information, right above
           if ('FALSE' in new_list_end[count_for]) or ('WRONG' in new_list_end[count_for]):
                line1 = line1 + new_list_end[count_for][len(new_list_end[count_for])-2] + ' '
           else:
                line1 = line1 + new_list_end[count_for][len(new_list_end[count_for])-1] + ' '
           #Formatting the MULTIPLE CONTACTS information ('0' or '1'), carried in new_list_end[count_for][len(new_list_end[count_for])-3], or at new_list_end[count_for][len(new_list_end[count_for])-4] if the basepair was assigned by 'MH' or 'ELI'. To make it simpler later, the '0' or '1' will be switched to 'NO' or 'YES' and the formatted string will be appended at the end of 'new_list_end[count_for]'
           if ('FALSE' in new_list_end[count_for]) or ('WRONG' in new_list_end[count_for]) or ('UNDETERMINED' in new_list_end[count_for]):
               place = 4
               if ('FALSE' in new_list_end[count_for]):
                  ending = ' FALSE'
               elif ('WRONG' in new_list_end[count_for]):
                  ending = 'WRONG'
               elif ('UNDETERMINED' in new_list_end[count_for]):
                  ending = ' UNDETERMINED'
           else:
               place = 3
               ending = ''
           if '  D' in new_list_end[count_for]:
               if '0' in str(new_list_end[count_for][len(new_list_end[count_for])- place]):
                  a = 'NO'
               if '1' in str(new_list_end[count_for][len(new_list_end[count_for])- place]):
                  a = 'YES'
               new_list_end[count_for].append(ADD_SPACES(a, key[16]))
           else:
               if '0' in str(new_list_end[count_for][len(new_list_end[count_for])- place -1]):
                  a = 'NO'
               if '1' in str(new_list_end[count_for][len(new_list_end[count_for])- place -1]):
                  a = 'YES'
               new_list_end[count_for].append(ADD_SPACES(a, key[16]))
           line1 = line1 + new_list_end[count_for][len(new_list_end[count_for])-1] + ending
           print(line1)
#           print new_list_end[count_for]
           count_for = count_for - 1
print("############### '*' notes bonds with length larger tan maximum distance cutoff ###############")
print("######################################\n##########ORDERED RAW OUTPUT##########       END\n######################################")
#ADDING DESIRED SPACES

#print "AFTER loading new_list_end_floats \nlen(new_list_end)", len(new_list_end), "new_list_end_floats.ndim", new_list_end_floats.ndim, "new_list_end_floats.shape", new_list_end_floats.shape, "count_for", count_for
###########################FORMATTING 'new_list_end'############################################

#################Transferring the list of ORDERED and FORMATTED basepairs to run_cutoff_LISTS, in which they will be further order by CUTOFF. Also 'FALSE', 'LEGITIMATE', and 'UNDETERMINED' basepairs get transferred to corresponding lists
LEGITIMATE_tagged = [] #Will collect basepairs with a 'LEGITIMATE' tag from MASTER_Basepairs_summary, from largest first residue to smallest
falsely_assigned = [] #Will collect FALSE basepairs from MASTER_Basepairs_summary, from largest first residue to smallest
wrongly_assigned = [] #Will collect basepairs with 'WRONG' geometry from MASTER_Basepairs_summary, from largest first residue to smallest
undetermined = [] #Will collect 'UNDETERMINED' basepairs from MASTER_Basepairs_summary, from largest first residue to smallest
missing_bond = [] #Will collect basepairs with missing bonds. Tagged as " miss"
file = [[], []]
#pymol0 = directory + "/" + pdb_file_main + "_all-basepairs_REVERSED" + "_PYMOL" + "_script.pml"
pymol0 = dir + "/" + pdb_file_main + "_all-basepairs_REVERSED" + "_PYMOL" + "_script.pml"
file[0] = open(pymol0, 'w')
line0 = "load " + pdb_file + '\n' + "color white, /" + pdb_file_main + "/*" + '\n' + "set stick_ball, on" + '\n' + "set stick_ball_ratio, 2.00000" + '\n' + "show sticks" + '\n'
file[0].write(line0)
#pymol1 = directory + "/" + pdb_file_main + "_all-basepairs" + "_PYMOL" + "_script.pml"
pymol1 = dir + "/" + pdb_file_main + "_all-basepairs" + "_PYMOL" + "_script.pml"
file[1] = open(pymol1, 'w')
line1 = "load " + pdb_file + '\n' + "color white, /" + pdb_file_main + "/*" + '\n' + "set stick_ball, on" + '\n' + "set stick_ball_ratio, 2.00000" + '\n'
file[1].write(line1)
From = 'basepairs'

basepair_count = 0
for h in range (len(run_cutoff)/2):
   run_cutoff_LISTS.append([])
for b in range (len(new_list_end)):
   if 'REMOVED' not in new_list_end[b] and 'FALSE' not in new_list_end[b] and 'WRONG' not in new_list_end[b] and 'UNDETERMINED' not in new_list_end[b] and " miss" not in new_list_end[b]:
       PYMOL_OUTPUT(new_list_end[b], b, pdb_file_main, From, file)
   if ('LEGITIMATE' in new_list_end[b]):
       LEGITIMATE_tagged.append(new_list_end[b])
#      LEGITIMATE_tagged[len(LEGITIMATE_tagged)-1].append(b) #For easy future access to the corresponding line in 'new_list_end'
   elif ('FALSE' in new_list_end[b]):
      falsely_assigned.append(new_list_end[b])
      falsely_assigned[len(falsely_assigned)-1].append(b) #For easy future access to the corresponding line in 'new_list_end'
   elif ('WRONG' in new_list_end[b]):
      wrongly_assigned.append(new_list_end[b])
      wrongly_assigned[len(wrongly_assigned)-1].append(b) #For easy future access to the corresponding line in 'new_list_end'
   elif ('UNDETERMINED' in new_list_end[b]):
      undetermined.append(new_list_end[b])
      undetermined[len(undetermined)-1].append(b) #For easy future access to the corresponding line in 'new_list_end'
   elif(" miss" in new_list_end[b]):
      missing_bond.append(new_list_end[b])
      missing_bond[len(missing_bond)-1].append(b) #For easy future access to the corresponding line in 'new_list_end'
   for h in range (len(run_cutoff_LISTS)):
      if run_cutoff[h*2 + 1] in new_list_end[b] and ('REMOVED' not in new_list_end[b]) and ('FALSE' not in new_list_end[b]) and ('WRONG' not in new_list_end[b]) and ('UNDETERMINED' not in new_list_end[b]) and " miss" not in new_list_end[b]:
           run_cutoff_LISTS[h].append(new_list_end[b])
           basepair_count = basepair_count + 1

CLOSE(file)

#################Transferring the list of ORDERED and FORMATTED basepairs to run_cutoff_LISTS, in which they will be further order by CUTOFF. Also 'FALSE', 'LEGITIMATE', and 'UNDETERMINED' basepairs get transferred to corresponding lists


#########################
########STATISTICS OUTPUT              BEGINNING
print("\n\n#######################################################\n##################### STATISTICS ######################\n#######################################################\n")
print("###### A total of", basepair_count, "basepairs has been identified")
##### 1) PRINTING OUTPUT BY CUTOFF    BEGINNING
file = [[], []]
for h in range (len(run_cutoff)/2):
   print("\n\nBASEPAIRS FULLY IDENTIFIED AT DISTANCE CUTOFF = ", run_cutoff[h*2 + 1])
   print("===============================================================================")
   print("   Scheme   Resid  Base  Resid  Base Expect. Found ATOM ATOM  DIST. Method M.I.")
   print("             #1     #1    #2     #2   Bonds  Bonds  #1   #2")
   print("===============================================================================")

   #For PYMOL output
   pymol1 = dir + "/" + pdb_file_main + "_basepairs-at-cutoff_" + run_cutoff[h*2 + 1] + "_PYMOL" + "_script.pml"
   file[1] = open(pymol1, 'w')
   line1 = "load " + pdb_file + '\n' + "color white, /" + pdb_file_main + "/*" + '\n' + "set stick_ball, on" + '\n' + "set stick_ball_ratio, 2.00000" + '\n'
   file[1].write(line1)
   #For PYMOL output

   for a in range (len(run_cutoff_LISTS[h])):

      PYMOL_OUTPUT(run_cutoff_LISTS[h][a], a, pdb_file_main, From, file)

      print(run_cutoff_LISTS[h][a][0], "", run_cutoff_LISTS[h][a][2], " ", run_cutoff_LISTS[h][a][3], " ", run_cutoff_LISTS[h][a][4], " ", run_cutoff_LISTS[h][a][5], "    ", run_cutoff_LISTS[h][a][1], "    ", run_cutoff_LISTS[h][a][6], "  ", run_cutoff_LISTS[h][a][7], " ", run_cutoff_LISTS[h][a][8], run_cutoff_LISTS[h][a][9], "  ", run_cutoff_LISTS[h][a][len(run_cutoff_LISTS[h][a])-3], run_cutoff_LISTS[h][a][len(run_cutoff_LISTS[h][a])-1])
      print("                                                     ", run_cutoff_LISTS[h][a][10], " ", run_cutoff_LISTS[h][a][11], run_cutoff_LISTS[h][a][12])
      print("                                                     ", run_cutoff_LISTS[h][a][13], " ", run_cutoff_LISTS[h][a][14], run_cutoff_LISTS[h][a][15])
   print("===============================================================================")
   print("Number of basepairs identified at CUTOFF =", run_cutoff[h*2 + 1], "is", len(run_cutoff_LISTS[h]))
print("############### notes bonds with length larger tan maximum distance cutoff ###############")
CLOSE(file)
##### 1) PRINTING OUTPUT BY CUTOFF    END

##### 2) LIST OF LEGITIMATE BASEPAIRS
if LEGITIMATE_tagged != []:
    file = [[], []]
    pymol1 = dir + "/" + pdb_file_main + "_LEGITIMATED-basepairs" + "_PYMOL" + "_script.pml"
    file[1] = open(pymol1, 'w')
    line1 = "load " + pdb_file + '\n' + "color white, /" + pdb_file_main + "/*" + '\n' + "set stick_ball, on" + '\n' + "set stick_ball_ratio, 2.00000" + '\n'
    file[1].write(line1)
    color = 'RED'
    From = ''

    print("\n\nLIST OF LEGITIMATE BASEPAIRS FOUND TO SHARE BASES WITH FALSELY ASSIGNED BASEPAIRS")
    print("===============================================================================")
    print("   Scheme   Resid  Base  Resid  Base Expect. Found ATOM ATOM  DIST. Method M.I.")
    print("             #1     #1    #2     #2   Bonds  Bonds  #1   #2")
    print("===============================================================================")
    for h in range (len(LEGITIMATE_tagged)):
       PYMOL_OUTPUT(LEGITIMATE_tagged[h], h, pdb_file_main, From, file)
       print(LEGITIMATE_tagged[h][0], "", LEGITIMATE_tagged[h][2], " ", LEGITIMATE_tagged[h][3], " ", LEGITIMATE_tagged[h][4], " ", LEGITIMATE_tagged[h][5], "    ", LEGITIMATE_tagged[h][1], "    ", LEGITIMATE_tagged[h][6], "  ", LEGITIMATE_tagged[h][7], " ", LEGITIMATE_tagged[h][8], LEGITIMATE_tagged[h][9], "  ", LEGITIMATE_tagged[h][len(LEGITIMATE_tagged[h])-4], LEGITIMATE_tagged[h][len(LEGITIMATE_tagged[h])-2])
       print("                                                     ", LEGITIMATE_tagged[h][10], " ", LEGITIMATE_tagged[h][11], LEGITIMATE_tagged[h][12])
       print("                                                     ", LEGITIMATE_tagged[h][13], " ", LEGITIMATE_tagged[h][14], LEGITIMATE_tagged[h][15])
    print("===============================================================================")
    print("############### '*' notes bonds with length larger tan maximum distance cutoff ###############")
    print("Number of LEGITIMATE BASEPAIRS FOUND TO SHARE BASES WITH FALSELY ASSIGNED BASEPAIRS =", len(LEGITIMATE_tagged))
    CLOSE(file)

##### 3) LIST OF FALSE BASEPAIRS
if falsely_assigned != []:
    file = [[], []]
    pymol1 = dir + "/" + pdb_file_main + "_FALSE-basepairs" + "_PYMOL" + "_script.pml"
    file[1] = open(pymol1, 'w')
    line1 = "load " + pdb_file + '\n' + "color white, /" + pdb_file_main + "/*" + '\n' + "set stick_ball, on" + '\n' + "set stick_ball_ratio, 2.00000" + '\n'
    file[1].write(line1)
    color = 'RED'
    From = ''

    print("\n\nLIST OF FALSE BASEPAIRS")
    print("===============================================================================")
    print("   Scheme   Resid  Base  Resid  Base Expect. Found ATOM ATOM  DIST. Method M.I.")
    print("             #1     #1    #2     #2   Bonds  Bonds  #1   #2")
    print("===============================================================================")
    for h in range (len(falsely_assigned)):
       PYMOL_OUTPUT(falsely_assigned[h], h, pdb_file_main, From, file)
       print(falsely_assigned[h][0], "", falsely_assigned[h][2], " ", falsely_assigned[h][3], " ", falsely_assigned[h][4], " ", falsely_assigned[h][5], "    ", falsely_assigned[h][1], "    ", falsely_assigned[h][6], "  ", falsely_assigned[h][7], " ", falsely_assigned[h][8], falsely_assigned[h][9], "  ", falsely_assigned[h][len(falsely_assigned[h])-5], falsely_assigned[h][len(falsely_assigned[h])-2])
       print("                                                     ", falsely_assigned[h][10], " ", falsely_assigned[h][11], falsely_assigned[h][12])
       print("                                                     ", falsely_assigned[h][13], " ", falsely_assigned[h][14], falsely_assigned[h][15])
    print("===============================================================================")
    print("############### '*' notes bonds with length larger tan maximum distance cutoff ###############")
    print("Number of FALSELY ASSIGNED BASEPAIRS =", len(falsely_assigned))
    CLOSE(file)

##### 4) LIST OF BASEPAIRS WITH WRONG GEOMETRY
if wrongly_assigned != []:
    file = [[], []]
    pymol1 = dir + "/" + pdb_file_main + "_WRONG-basepairs" + "_PYMOL" + "_script.pml"
    file[1] = open(pymol1, 'w')
    line1 = "load " + pdb_file + '\n' + "color white, /" + pdb_file_main + "/*" + '\n' + "set stick_ball, on" + '\n' + "set stick_ball_ratio, 2.00000" + '\n'
    file[1].write(line1)
    color = 'RED'
    From = ''

    print("\n\nLIST OF BASEPAIRS WITH WRONG GEOMETRY")
    print("===============================================================================")
    print("   Scheme   Resid  Base  Resid  Base Expect. Found ATOM ATOM  DIST. Method M.I.")
    print("             #1     #1    #2     #2   Bonds  Bonds  #1   #2")
    print("===============================================================================")
    for h in range (len(wrongly_assigned)):
       PYMOL_OUTPUT(wrongly_assigned[h], h, pdb_file_main, From, file)
       print(wrongly_assigned[h][0], "", wrongly_assigned[h][2], " ", wrongly_assigned[h][3], " ", wrongly_assigned[h][4], " ", wrongly_assigned[h][5], "    ", wrongly_assigned[h][1], "    ", wrongly_assigned[h][6], "  ", wrongly_assigned[h][7], " ", wrongly_assigned[h][8], wrongly_assigned[h][9], "  ", wrongly_assigned[h][len(wrongly_assigned[h])-5], wrongly_assigned[h][len(wrongly_assigned[h])-2])
       print("                                                     ", wrongly_assigned[h][10], " ", wrongly_assigned[h][11], wrongly_assigned[h][12])
       print("                                                     ", wrongly_assigned[h][13], " ", wrongly_assigned[h][14], wrongly_assigned[h][15])
    print("===============================================================================")
    print("############### '*' notes bonds with length larger tan maximum distance cutoff ###############")
    print("Number of BASEPAIRS WITH WRONG GEOMETRY =", len(wrongly_assigned))
    CLOSE(file)


##### 5) LIST OF UNDETERMINED BASEPAIRS BEGINNING
if undetermined != []:
    file = [[], []]
    pymol1 = dir + "/" + pdb_file_main + "_UNDETERMINED-basepairs" + "_PYMOL" + "_script.pml"
    file[1] = open(pymol1, 'w')
    line1 = "load " + pdb_file + '\n' + "color white, /" + pdb_file_main + "/*" + '\n' + "set stick_ball, on" + '\n' + "set stick_ball_ratio, 2.00000" + '\n'
    file[1].write(line1)
    color = 'RED'
    From = ''
    print("\n\nLIST OF BASE-SHARING BASEPAIRS THAT CANNOT BE DECONVOLUTED")
    print("===============================================================================")
    print("   Scheme   Resid  Base  Resid  Base Expect. Found ATOM ATOM  DIST. Method M.I.")
    print("             #1     #1    #2     #2   Bonds  Bonds  #1   #2")
    print("===============================================================================")
    for h in range (len(undetermined)):
      PYMOL_OUTPUT(undetermined[h], h, pdb_file_main, From, file)
      print(undetermined[h][0], "", undetermined[h][2], " ", undetermined[h][3], " ", undetermined[h][4], " ", undetermined[h][5], "    ", undetermined[h][1], "    ", undetermined[h][6], "  ", undetermined[h][7], " ", undetermined[h][8], undetermined[h][9], "  ", undetermined[h][len(undetermined[h])-4], undetermined[h][len(undetermined[h])-1])
      print("                                                     ", undetermined[h][10], " ", undetermined[h][11], undetermined[h][12])
      print("                                                     ", undetermined[h][13], " ", undetermined[h][14], undetermined[h][15])
    print("===============================================================================")
    print("Number of BASE-SHARING BASEPAIRS THAT CANNOT BE DECONVOLUTED =", len(undetermined))
    print("############### '*' notes bonds with length larger tan maximum distance cutoff ###############")
    CLOSE(file)
##### 5) LIST OF UNDETERMINED BASEPAIRS END

##### 6) LIST OF MISSING-BOND BASEPAIRS BEGINNING
if missing_bond != []:
    file = [[], []]
    pymol1 = dir + "/" + pdb_file_main + "_MISSING-BOND-basepairs" + "_PYMOL" + "_script.pml"
    file[1] = open(pymol1, 'w')
    line1 = "load " + pdb_file + '\n' + "color white, /" + pdb_file_main + "/*" + '\n' + "set stick_ball, on" + '\n' + "set stick_ball_ratio, 2.00000" + '\n'
    file[1].write(line1)
    color = 'RED'
    From = ''
    print("\n\nLIST OF MISSING-BOND BASEPAIRS")
    print("===============================================================================")
    print("   Scheme   Resid  Base  Resid  Base Expect. Found ATOM ATOM  DIST. Method M.I.")
    print("             #1     #1    #2     #2   Bonds  Bonds  #1   #2")
    print("===============================================================================")
    for h in range (len(missing_bond)):
      PYMOL_OUTPUT(missing_bond[h], h, pdb_file_main, From, file)
      print(missing_bond[h][0], "", missing_bond[h][2], " ", missing_bond[h][3], " ", missing_bond[h][4], " ", missing_bond[h][5], "    ", missing_bond[h][1], "    ", missing_bond[h][6], "  ", missing_bond[h][7], " ", missing_bond[h][8], missing_bond[h][9], "  ", missing_bond[h][len(missing_bond[h])-4], missing_bond[h][len(missing_bond[h])-1])
      print("                                                     ", missing_bond[h][10], " ", missing_bond[h][11], missing_bond[h][12])
      print("                                                     ", missing_bond[h][13], " ", missing_bond[h][14], missing_bond[h][15])
    print("===============================================================================")
    print("Number of MISSING-BOND BASEPAIRS =", len(missing_bond))
    print("############### 'NF' notes Not-found bond ###############")
    CLOSE(file)
##### 6) LIST OF MISSING-BOND BASEPAIRS END



##### 7) LIST OF BASEPAIRS BY GEOMETRY BEGINNIG
###BOND STATISTICS. Important arrays:
        # new_list_end_floats = numpy.zeros(4 * len(new_list_end)).reshape(len(new_list_end), 4) #Will have as many lines as 'new_list_end', so it can be accessed just like 'new_list_end, but will carry only the bond- and P-distances as float, so they can be used in statistical operations
                #Defined in Subsection "DELETING UNWANTED SPACES and SAVING THE DISTANCE AS FLOATS" above
        # STATS, see definition below


list = []
STATS = numpy.zeros(11 * len(MASTER_Basepairs_schemes)).reshape(len(MASTER_Basepairs_schemes), 11) #can be accessed with MASTER_Basepairs_schemes indexes. Will store:
                #STATS[a,0] = AVERAGE for bond-1's length for the GEOMETRY specified by the 'MASTER_Basepairs_schemes[a]'
                #STATS[a,1] = STANDARD DEVIATION for bond-1's length for the GEOMETRY specified by the 'MASTER_Basepairs_schemes[a]'
                #STATS[a,2] = AVERAGE for bond-2's length for the GEOMETRY specified by the 'MASTER_Basepairs_schemes[a]'
                #STATS[a,3] = STANDARD DEVIATION for bond-2's length for the GEOMETRY specified by the 'MASTER_Basepairs_schemes[a]'
                #STATS[a,4] = AVERAGE for bond-3's length for the GEOMETRY specified by the 'MASTER_Basepairs_schemes[a]'
                #STATS[a,5] = STANDARD DEVIATION for bond-3's length for the GEOMETRY specified by the 'MASTER_Basepairs_schemes[a]'
                #STATS[a,6] = AVERAGE for P-P distance for the GEOMETRY specified by the 'MASTER_Basepairs_schemes[a]'
                #STATS[a,7] = STANDARD DEVIATION for P-P distance for the GEOMETRY specified by the 'MASTER_Basepairs_schemes[a]'
                #STATS[a,8] = AVERAGE for C1'-C1' distance for the GEOMETRY specified by the 'MASTER_Basepairs_schemes[a]'
                #STATS[a,9] = STANDARD DEVIATION for C1'-C1' distance for the GEOMETRY specified by the 'MASTER_Basepairs_schemes[a]'
                #STATS[a,] = N, nuber of basepairs found of the GEOMETRY specified by the 'MASTER_Basepairs_schemes[a]'
for a in range (len(MASTER_Basepairs_schemes)):
    SUM = []
    counter = 0
#Calculating mean and standard deviation
    for e in range (len(new_list_end)):
        if  MASTER_Basepairs_schemes[a] in new_list_end[e][0] and 'REMOVED' not in new_list_end[e] and 'FALSE' not in new_list_end[e] and 'WRONG' not in new_list_end[e] and 'UNDETERMINED' not in new_list_end[e] and 'miss' not in new_list_end[e]:
            SUM.append(new_list_end_floats[e,:])
            counter = counter + 1
    if SUM != []:
        SUM = numpy.array(SUM)
        SUM.reshape(counter, 5)
        STATS[a,] = [numpy.mean(SUM[:,0]), numpy.std(SUM[:,0]), numpy.mean(SUM[:,1]), numpy.std(SUM[:,1]), numpy.mean(SUM[:,2]), numpy.std(SUM[:,2]), numpy.mean(SUM[:,3]), numpy.std(SUM[:,3]), numpy.mean(SUM[:,4]), numpy.std(SUM[:,4]), SUM.shape[0]]

#SUMMARY STATISTICS BY BASEPAIR
print("n\n")
for a in range (len(MASTER_Basepairs_schemes)):
    print(MASTER_Basepairs_schemes[a], end=' ')
    list = [str(STATS[a,0]), str(STATS[a,1]), str(STATS[a,2]), str(STATS[a,3]), str(STATS[a,4]), str(STATS[a,5]), str(STATS[a,6]), str(STATS[a,7]), str(STATS[a,8]), str(STATS[a,9]), str(STATS[a,10])]
    if list[0] != '0.0':
        print("STATISTICAL VALUES CALCULATED WITHIN STRUCTURE")
        print("Bond1: MEAN =", list[0][:5], "SD =", list[1][:5], "Bond2: MEAN = ", list[2][:5], "SD =", list[3][:5], "Bond3: MEAN =", list[4][:5], "SD =", list[5][:5], "P-to-P distance =", list[6][:5], "SD =", list[7][:5], "C1\'-to-C1\' distance =", list[8][:5], "SD =", list[9][:5], "No Basepairs =", list[7][:1])
    else:
        print("No basepairs of this geometry were found")

#WHOLE LIST
list = []
print("\n\nLIST OF BASEPAIRS ORDERED BY GEOMETRY")
for c in range (len(MASTER_Basepairs_schemes)):
    geometry_counter = 0 #Will count all basepairs within 'new_list_end[b]' with 'MASTER_Basepairs_schemes[c]' geometry
    geometry_counter_real = 0 #Will count only non-FALSE, non-UNDETERMINED basepairs within 'new_list_end[b]' with 'MASTER_Basepairs_schemes[c]' geometry
#TRUE BASEPAIRS
    for b in range (len(new_list_end)):
        m=re.search(r'\S+', new_list_end[b][0])
        scheme = str(m.group())
        if MASTER_Basepairs_schemes[c] == scheme:
           geometry_counter = geometry_counter + 1
           if geometry_counter == 1:
              print("=======================================================================================================")
              print("   Scheme   Resid  Base  Resid  Base Expect. Found  ATOM ATOM  DIST.  DIFF.  DIST   DIFF.  DIST   DIFF.")
              print("             #1     #1    #2     #2   Bonds  Bonds   #1   #2    BOND         P-P          C1'-C1'")
              print("=======================================================================================================")
           if 'REMOVED' not in new_list_end[b] and 'FALSE' not in new_list_end[b] and 'WRONG' not in new_list_end[b] and 'UNDETERMINED' not in new_list_end[b]:
               geometry_counter_real = geometry_counter_real + 1
               list = DIFF_CALC(new_list_end_floats[b,:], STATS[c,:], new_list_end[b], MASTER_Basepairs_bonds[c])

    if geometry_counter_real > 0 and 'REMOVED' not in new_list_end[b] and 'FALSE' not in new_list_end[b] and 'WRONG' not in new_list_end[b] and 'UNDETERMINED' not in new_list_end[b]:
         list = [str(STATS[c,0]), str(STATS[c,1]), str(STATS[c,2]), str(STATS[c,3]), str(STATS[c,4]), str(STATS[c,5]), str(STATS[c,6]), str(STATS[c,7]), str(STATS[c,8]), str(STATS[c,9])]
         print("PRECALCULATED STATISTICAL DATA:", "\nP-to-P distance =", MASTER_Basepairs_bonds[c][9], " SD =", MASTER_Basepairs_bonds[c][10], "\nC1\'-to-C1\' distance =", MASTER_Basepairs_bonds[c][11], " SD =", MASTER_Basepairs_bonds[c][12])
         print("STATISTICAL DATA FROM CURRENT STRUCTURE:", "\nBond1: MEAN =", list[0][:5], "SD =", list[1][:5], "\nBond2: MEAN =", list[2][:5], " SD =", list[3][:5], "\nBond3: MEAN =", list[4][:5], " SD =", list[5][:5], "\nP-to-P distance =", list[6][:5], " SD =", list[7][:5], "\nC1\'-to-C1\' distance =", list[8][:5], " SD =", list[9][:5])
         print("NUMBER OF BASEPAIRS WITH", MASTER_Basepairs_schemes[c], "geometry =", geometry_counter_real, "\n")

#FALSE BASEPAIRS
    false_counter = 0
    for b in range (len(falsely_assigned)):
        if MASTER_Basepairs_schemes[c] in falsely_assigned[b][0]:
            if b == 0:
               print("LIST FALSE BASEPAIRS WITH", MASTER_Basepairs_schemes[c], "geometry:")
            false_counter = false_counter + 1
            list = DIFF_CALC(new_list_end_floats[falsely_assigned[b][len(falsely_assigned[b])-1],:], STATS[c,:], falsely_assigned[b], MASTER_Basepairs_bonds[c])
    if false_counter > 0:
        print("NUMBER OF FALSE BASEPAIRS WITH", MASTER_Basepairs_schemes[c], "geometry =", false_counter, "\n")

#UNDETERMINED BASEPAIRS
    undetermined_counter = 0
    for b in range (len(undetermined)):
        if MASTER_Basepairs_schemes[c] in undetermined[b][0]:
            if b == 0:
                print("LIST OF UNDETERMINED BASEPAIRS WITH", MASTER_Basepairs_schemes[c], "geometry:")
            undetermined_counter = undetermined_counter + 1
            list = DIFF_CALC(new_list_end_floats[undetermined[b][len(undetermined[b])-1],:], STATS[c,:], undetermined[b], MASTER_Basepairs_bonds[c])
    if undetermined_counter > 0:
        print("NUMBER OF UNDETERMINED BASEPAIRS WITH", MASTER_Basepairs_schemes[c], "geometry =", undetermined_counter, "\n")



print("===============================================================================")
#### 7) LIST OF BASEPAIRS BY GEOMETRY END




print("\n\n#######################################################\n##################### STATISTICS ######################\n#######################################################\n")
#########################################################################
##########################################################################
##########################################################################
#SECTION STATISTICS                                  END
##########################################################################
##########################################################################
##########################################################################


##########################################################################
##########################################################################
##########################################################################
#SECTION FINAL OUTPUT
##########################################################################
##########################################################################
##########################################################################


#OUTPUT REGARDING BASES WITH POSSIBLE MULTIPLE CONTACTS
#print "\n\nThe following group of bases have been identified as possible basepairs but cannot be assigned to a single basepair interaction\nPlease visually check the assignment in Pymol.\n##### These bases will be displayed in RED #####."
file = [[], []]
#pymol1 = directory + "/" + pdb_file_main + "_UNASSIGNABLE-basepairs" + "_PYMOL" + "_script.pml"
pymol1 = dir + "/" + pdb_file_main + "_UNASSIGNABLE-basepairs" + "_PYMOL" + "_script.pml"
file[1] = open(pymol1, 'w')
line1 = "load " + pdb_file + '\n' + "color white, /" + pdb_file_main + "/*" + '\n' + "set stick_ball, on" + '\n' + "set stick_ball_ratio, 2.00000" + '\n'
file[1].write(line1)

for i in range (len(control)):
    line1 = [] #will carry messages for printing out
    if len(control[i]) > 4:
       processed_control = [] #Will shuttle control[i] sub-lists with more than 4 elements. These are arrays of candidate basepairs possibly involved in MULTIPLE CONTACTS that could not be assigned as legitimate basepairs
       list = [] #Will carry formatted basepairs, so that they can be properly outputed
       for j in range (len(control[i])):
           if control[i][j] != []:
              processed_control.append(control[i][j])
       for u in range (len(processed_control)): #To format the 2 strings for the current basepair, resid and resid #, into residresid #
           if u%2 == 0:
              search = r'\d+'
           else:
              search = r'\w+'
           formatted = SEARCH(processed_control[u], search)
           list.append(formatted)
       for v in range (len(list)/2):
           line = "color red, /" + pdb_file_main + "/*/*/" + list[v*2] + '\n'
           file[1].write(line)
           line = "show sticks, /" + pdb_file_main + "/*/*/" + list[v*2] + '\n'
           file[1].write(line)
           line1.append(list[v*2] + list[(v*2)+1])


print("########################################################\n#################### END OF PROGRAM ####################\n########################################################")

import sys
sys.exit()

#STILL TO DO:
#v.041910 #The following basepairs in Thermus 16S rRNA cannot be assigned.
        #Between A978 and G1316 of the X_AG geometry.
        #No helical elements continuous with the basepair
        #Involved in multiple interactions with A1318 via A978
        #Distances between A`1318/N1 and A`978/C8 = 4.3, A`1318/C2 and A`978/C4 = 4.2
                #These distances could be used to deconvolute these basepairs. May not work in other cases, though
        #Between G1139 and G1142 of the VI_GG geometry
        #No helical elements continuous with the basepair
        #Involved in multiple interactions with G1134 via G1142
        #Distance between G`1142/N9 and G`1134/N2 = 3.7
                #This distance
        #Problems with residue G1030A in Thermus 16S rRNA (2J02)

        #SOLVED: Does not detect basepair between C490 and C444. This one should be a three bond basepair with 2 bonds identified
        #Could it recognize base to backbone interactions?:
                #G481 06 to A451 O2* ---> 3.3A
                #G481 N2 to A451 O1P ---> 3.3A
                #G481 N1 to A451 O1P ---> 3.1A
                #Note that A451 is also pretty close to G481 backbone
                #See also the interaction between G485's N1 and N2 with A448's O2P

        #note that G447 and A487 form a sheared basepair in which the G's N3 and the A's N6 are more than 3.7A apart. The distance from A's N6 to 447's O2* is 3.0A, though. Is this a common theme in sheared basepairs?
                #Sheared between A441 and G493, from A's N6 to G's O2* 2.8A, whereas from A's N6 to G's N3 3.7A
        #SOLVED 051210: Wrong helical element running from C456 to C458 and from G474 to G476. At maximum CUTOFF of 3.6 only two basepairs should have been identified
                #3-bond base pair with all bonds identified between C456 and G475
                #3-bond base pair with  only two bonds identified between C455 and G476

        #051510:Possible new basepair between A441 and G493 (involving A`441/N6 and G`493/O2*, and A`441/N7 and G`493/N2)
        #051610 (BUILD pdb_distances_051210.py): Basepair between 978 and 1316 of the X_AG geometry cannot be deconvoluted due to a stacking noise from A 1318, which is detected as II_AA. C1-C1 distance should be able to take care of this.
                # You clicked /2J02_16SrRNA//A/G`1316/N2 -> (pk1)
                # You clicked /2J02_16SrRNA//A/A`978/N1 -> (pk1)
                # You clicked /2J02_16SrRNA//A/G`1316/N3 -> (pk1)
                # You clicked /2J02_16SrRNA//A/A`978/N6 -> (pk1)
        #051610 (SOLVED) (BUILD pdb_distances_051210.py): Basepair between G1022 and C1007 (C1-C1 = 11.2) cannot be deconvoluted from stacking noise between C1008 and G1022 (C1-C1 = 8.3). C1-C1 distances to sort out this noise before attempting deconvolution by MH??????
        #Basepairs C444:G490 and G445:C489 are not identified. Instead a basepair between C444 and C489 appears to be identified

        # Base-pair between positions G1030A and C1031 of T. thermophilus 16S rRNA not recognized, probably because of consecutive bases involved
###i = 47 in MASTER_Basepairs (XXXIII_GG)
#XXXI_GG = [] #XXXIII Base-pair between positions G1030A and C1031 of T. thermophilus 16S rRNA (E. coli numbering)
#Bond   G       G       Length Ave      Length Std      Attribute
#1      N2      N1      2.9             unk
#2      N3      N2      3.4             unk
#bonds = [2, "G", "G", "N2", "N1", "N3", "N2", "NA", "NA", "NA", "NA", "NA", "NA"]
#1GMASTER_Basepairs_bonds.append(bonds)

        #TRIPLE involving bases 64, 68, and 101. the program detects the a basepair between 69 and 101 and appends that between 68 and 64 as FALSE
#v.042110
#From Ralf 050310
#    Hi Anton,

#    > but how is 'neighboring' defined? in terms of distance? in terms of residue
#    > number?

#    Simply the relative positions in the list of residues.
#    Maybe I should have said "consecutive residues".
#    I was thinking that could tell you if you have a base-pair interaction
#    or a stacking interaction.
#    But I'm at the limit of my biochemistry knowledge here.

#    > how would this distinguish between atoms forming a basepair and
#    > stacking atoms? unless you have some other way to tell them apart
#    > neighboring bases within 5A can be part of a stacking interaction, or even
#    > be part of a neighboring helix.

#    From the hierarchical iotbx.pdb objects you can easily tell if
#    atoms are in different chains.

#    > thanks for the explanation on the objects. however, I am far from
#    > understanding how you build them. What is the name of the type of
#    > hierarchical objects that you use? so i can read something more about them.

#    We have thousands of objects of this kind. It is impossible to write
#    documentation for all of them. The way to work with an object (after
#    somebody told you they have the information you need) is to insert
#    "print dir(atom)" or "help(atom)" into the script and run it. It will
#    show you all the things the object contains. We use long variable
#    names to make them as self-explanatory as possible.

#    > maybe, I could try to finish the program the way I am writing it and later
#    > on translate it into objects, like you suggested. i need to learn how they
#    > work first.

#    Yes, sounds like a good idea.
#    But what you do here...

#              master.append([
#                atom_i.resid(),
#                resname_i,
#                atmname_i,
#                atom_j.resid(),
#                resname_j,
#                atmname_j,
#                distance])

#    "throw away everything I don't understand" and "create a list
#    of anonymous numbers", is causing a lot of problems. I think it will
#    pay to play with the atom objects a little, and to start right here
#    making meaningful objects where each number has a name someone else
#    can understand. A simple start would be

#     class atom_pair:

#       def __init__(self, atom_i, atom_j, distance):
#         self.atom_i = atom_i
#         self.atom_j = atom_j
#         self.distance = distance

#       def atoms_are_in_same_chain(self):
#         # returns True or False

#       def atoms_are_in_consecutive_residues(self)
#         # returns True or False

#    and

#     master.append(atom_pair(atom_i, atom_j, distance))

#    Method names like above would tell me what you need and I could help
#    filling in the details.

#    Ralf


 *******************************************************************************
