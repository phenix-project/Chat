

 *******************************************************************************
xfel/cxi/cspad_ana/mod_ledge.py
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# $Id$

# XXX Better dark generation: are the bright pixels really due to
# cosmic radiation?

# Does it even make sense to run this on more than one core?  NO IT DOES NOT!

from __future__ import absolute_import, division, print_function

import math
import numpy

from pypdsdata import xtc


from scitbx.array_family import flex

from xfel.cxi.cspad_ana import common_mode
from xfel.cxi.cspad_ana import cspad_tbx

from collections import deque
from six.moves import range
from six.moves import zip


def _summed_area_table(f, m, n):
  """The _summed_area_table() function calculates the summed-area tables
  required for the denominator in the expansion of the normalised
  correlation coefficient.

  @param f Two-dimensional intensity image
  @param m Height of the template, in pixels
  @param n Width of the template, in pixels
  @return  The energy of @f as a function of the template position
  """

  from scitbx.array_family import flex

  s = flex.double(flex.grid(f.focus()[0] + 2 * m,
                            f.focus()[1] + 2 * n))
  s.matrix_paste_block_in_place(block=f, i_row=m, i_column=n)

  # Treat the first row and the first column as special cases, then
  # fill in the rest of matrix.
  for u in range(1, s.focus()[0]):
    s[u, 0] += s[u - 1, 0]
  for v in range(1, s.focus()[1]):
    s[0, v] += s[0, v - 1]
  for u in range(1, s.focus()[0]):
    for v in range(1, s.focus()[1]):
      s[u, v] += s[u - 1, v] + s[u, v - 1] - s[u - 1, v - 1]

  # Note the off-by-one discrepancy with respect to Lewis (1995), due
  # to the padding.
  e = flex.double(flex.grid(f.focus()[0] + m - 1, f.focus()[1] + n - 1))
  e[0, 0] = s[m, n]
  for u in range(1, e.focus()[0]):
    e[u, 0] = s[u + m, n] - s[u, n]
  for v in range(1, e.focus()[1]):
    e[0, v] = s[m, v + n] - s[m, v]
  e[1:e.focus()[0], 1:e.focus()[1]] = \
    s[(m + 1):(e.focus()[0] + m ), (n + 1):(e.focus()[1] + n)] - \
    s[    (1):(e.focus()[0]),      (n + 1):(e.focus()[1] + n)] - \
    s[(m + 1):(e.focus()[0] + m),      (1):(e.focus()[1])]     + \
    s[    (1):(e.focus()[0]),          (1):(e.focus()[1])]

  return e


def lewis(template, img):
  """The lewis() function computes the normalised cross-correlation
  (NCC) of an image, @p img, and a template, @p template.  Both image
  and template must be two-dimensional, real, and finite.  The image
  must be larger than the template, which in turn should contain more
  than one pixel, and the template must have positive variance.  The
  function returns the correlation coefficients in the range [-1, +1].
  See Lewis, J. P. (1995) "Fast Template Matching", Vision Interface,
  120-123.

  @note This function should be equivalent to MATLAB's normxcorr2()
        function.

  @param img      Two-dimensional intensity image
  @param template Two-dimensional intensity template
  @return         Correlation coefficients
  """

  import math
  import numpy
  from sys import float_info

  from scitbx import fftpack
  from scitbx.array_family import flex

  # Assert that image and template are two-dimensional arrays, and
  # that the template is no larger than image.  Assert that template
  # is not flat.  XXX Check for real and finite, too?
  assert len(img.focus()) == 2 \
    and  len(template.focus()) == 2
  assert img.focus()[0] >= template.focus()[0] \
    and  img.focus()[1] >= template.focus()[1]
  assert template.sample_standard_deviation() > 0

  # For conformance with MATLAB's normxcorr2() and geck 342320: for
  # numerical robustness, ensure that both image and template are
  # always non-negative.
  img_nn = img - min(0, flex.min(img))
  template_nn = template - min(0, flex.min(template))

  # Calculate the terms of the denominator of gamma.  Must guard
  # against negative variance of the image due to inaccuracies in the
  # one-pass formula.
  img_sum = _summed_area_table(
    img_nn, template_nn.focus()[0], template_nn.focus()[1])
  img_ssq = _summed_area_table(
    flex.pow2(img_nn), template_nn.focus()[0], template_nn.focus()[1])

  f_sigma = (img_ssq - img_sum * img_sum /
             (template_nn.focus()[0] * template_nn.focus()[1]))
  f_sigma.set_selected(f_sigma < 0, 0)
  f_sigma = flex.sqrt(f_sigma)
  t_sigma = (template_nn - flex.mean(template_nn)).norm()

  gamma_denominator =  f_sigma * t_sigma

  # Zero-pad the image to permit partial overlap of template and
  # image, and embed the time-reversed template in a zero-padded array
  # of the same size.  Zero-padding means the entire template is
  # always overlapping the image, and terms involving the template
  # mean cancel in the expansion of the numerator of gamma.
  #
  # Note: the NCC demands the template to be time-reversed, which can
  # be accomplished by conjugation in the frequency domain.  An
  # implementation following that approach would however require
  # special care to be taken for the first rows and columns:
  #
  #   from numpy import roll
  #   t_embed.matrix_paste_block_in_place(
  #     block=template_nn,
  #     i_row=full[0] - template_nn.focus()[0],
  #     i_column=full[1] - template_nn.focus()[1])
  #   t_embed = flex.double(roll(
  #     roll(t_embed.as_numpy_array(), 1, axis=0), 1, axis=1))
  #
  # Calculate correlation in frequency domain.  XXX Could use spatial
  # domain calculation in cases where it's faster (see MATLAB's
  # implementation).
  full = (img_nn.focus()[0] + template_nn.focus()[0] - 1,
          img_nn.focus()[1] + template_nn.focus()[1] - 1)

  f_embed = flex.double(flex.grid(full))
  f_embed.matrix_paste_block_in_place(
    block=img_nn, i_row=0, i_column=0)
  f_prime = flex.complex_double(
    reals=f_embed, imags=flex.double(flex.grid(full)))

  t_embed = flex.double(flex.grid(full))
  t_embed.matrix_paste_block_in_place(
    block=template_nn.matrix_rot90(2), i_row=0, i_column=0)
  t_prime = flex.complex_double(
    reals=t_embed, imags=flex.double(flex.grid(full)))

  fft = fftpack.complex_to_complex_2d(full)
  fft.forward(f_prime)
  fft.forward(t_prime)
  gamma_numerator = f_prime * t_prime
  fft.backward(gamma_numerator)
  gamma_numerator = flex.real(gamma_numerator) / (fft.n()[0] * fft.n()[1]) \
                    - img_sum * flex.mean(template_nn)

  # For conformance with MATLAB: set the NCC to zero in regions where
  # the image has zero variance over the extent of the template.  If,
  # due to small variances in the image or the template, a correlation
  # coefficient falls outside the range [-1, 1], set it to zero to
  # reflect the undefined 0/0 condition.
  tol = math.sqrt(math.ldexp(
      float_info.epsilon,
      math.frexp(flex.max(flex.abs(gamma_denominator)))[1] - 1))
  sel = gamma_denominator <= tol
  gamma = gamma_numerator.set_selected(sel, 0) / \
          gamma_denominator.set_selected(sel, 1)
  gamma.set_selected(flex.abs(gamma) > 1 + math.sqrt(float_info.epsilon), 0)

  return gamma


class ring_buffer(deque):
  """Better named history_buffer?
  """

  def __init__(self, maxlen=1024):
    """Must ensure to call the constructor with maxlen other than @c
    None.  The maxlen parameter requires Python 2.6 or later.
    """
    assert maxlen is not None
    super(ring_buffer, self).__init__(maxlen=maxlen)


  def frequency(self):
    """If time is in seconds, returns the frequency, in Hz, of the
    value.
    """
    dt = self[-1][0] - self[0][0]
    if dt <= 0:
      return 0
    return (len(self) - 1) / dt


  def push(self, time, value):
    if len(self) == 0 or self[-1][1] != value:
      self.append((time, value))


class mod_ledge(common_mode.common_mode_correction):

  def __init__(
      self, address, display, mat_path, table_path, template_path=None, **kwds):
    """The mod_average class constructor stores the parameters passed
    from the pyana configuration file in instance variables.  All
    parameters, except @p address are optional, and hence need not be
    defined in pyana.cfg.

    @param address Address string XXX Que?!
    """

    super(mod_ledge, self).__init__(address=address, **kwds)

    # XXX Should be forced to false if graphics unavailable (e.g. when
    # running on the cluster).
    self._display = cspad_tbx.getOptBool(display)

    # Use line buffering to allow tailing the output in real time.
    # XXX Make name configurable.
    self._mat_path = cspad_tbx.getOptString(mat_path)
    self._table_path = cspad_tbx.getOptString(table_path)

    # Ring buffers for history-keeping.
    self._history_injector_xyz = ring_buffer()
    self._history_spectrometer_xyz = ring_buffer()
    self._history_energy = ring_buffer()

    # Get the template for image registration.
    if template_path is not None:
      from libtbx import easy_pickle
      self._template = easy_pickle.load(template_path)
    else:
      self._template = None

    # Optionally, initialise the viewer with a custom callback, see
    # mod_view.  The regions of interest are communicated to the
    # viewer through a shared multiprocessing array.  XXX This needs a
    # bit more thought to come up with a sensible interface.
    if self._display:
      from .mod_view import _xray_frame_process
      from multiprocessing import Array, Process, Manager
      from rstbx.viewer import display

      manager = Manager()
      self._queue = manager.Queue()
      self._proc = Process(
        target=_xray_frame_process, args=(self._queue, True, 0))
      self._roi = Array('i', 15 * 4, lock=False) # XXX Make variable!
      display.user_callback = self._callback
      self._proc.start()


  def _callback(self, dc, wxpanel, wx):
    # Draws a rectangle with the given top left corner, and with the
    # given size.  The current pen is used for the outline and the
    # current brush for filling the shape.
    #
    # These are like in wxPython, (x, y, width, height).  Origin is at
    # top left corner, width increases to the left, height downwards.
    # Log it!

    dc.SetBrush(wx.TRANSPARENT_BRUSH)
    dc.SetPen(wx.Pen('red'))
    for i in range(0, len(self._roi), 4):
      roi = self._roi[i:i + 4]

      # XXX Figure out what's going on here--subtract one because it's
      # inclusive?!
      tl = wxpanel._img.image_coords_as_screen_coords(
        roi[0] - 0.5, roi[1] - 0.5)
      br = wxpanel._img.image_coords_as_screen_coords(
        roi[0] + roi[2], roi[1] + roi[3])
      dc.DrawRectangle(tl[0], tl[1], br[0] - tl[0], br[1] - tl[1])

      """
      tl = wxpanel._img.image_coords_as_screen_coords(
        roi[0] + roi[2] - 0.5, roi[1] - 0.5)
      br = wxpanel._img.image_coords_as_screen_coords(
        roi[0] - 0.5 + roi[2] + 10, roi[1] - 0.5 + 10)
      dc.DrawRectangle(tl[0], tl[1], br[0] - tl[0], br[1] - tl[1])

      tl = wxpanel._img.image_coords_as_screen_coords(
        roi[0] - 0.5, roi[1] - 0.5 + roi[3])
      br = wxpanel._img.image_coords_as_screen_coords(
        roi[0] - 0.5 + 10, roi[1] - 0.5 + roi[3] + 10)
      dc.DrawRectangle(tl[0], tl[1], br[0] - tl[0], br[1] - tl[1])
      """


  def _reset_counters(self):
    self._I0 = flex.double()
    self._fee_before = flex.double()
    self._fee_after = flex.double()
    self._hit = flex.bool()

    self._injector_plus_current = flex.double()
    self._injector_plus_voltage = flex.double()
    self._injector_minus_current = flex.double()
    self._injector_minus_voltage = flex.double()

    self._injector_micos_xyz = flex.vec3_double()
    self._injector_rough_xyz = flex.vec3_double()
    self._repetition_rate = flex.double()
    self._timestamp = flex.double()
    self._spectrometer_xyz = flex.vec3_double()
    self._energy = flex.double()

    self._acq_apd_integral = flex.double()
    self._acq_opto_diode_integral = flex.double()

#    self._injector_x = flex.double()
#    self._injector_y = flex.double()
#    self._injector_z = flex.double()

#    self._spectrometer_x = flex.double()
#    self._spectrometer_y = flex.double()
#    self._spectrometer_z = flex.double()


  @staticmethod
  def _filtered_stats(function, iterable):
    """The _filtered_stats() computes first- and second-order statistics
    on an array, after first applying a filter.

    @param function Function which returns @c True for those elements
                    of @p iterable which should be excluded in the
                    returned statistics, or @c None to include all
                    data
    @param iterable An iterable sequence of data items
    @return         A four-tuple of mean, standard deviation,
                    effective sample size, and number of rejected
                    samples
    """

    filtered_data = list(filter(function, iterable))
    if len(filtered_data) == 0:
      return (0, 0, 0, 0)

    stats = flex.mean_and_variance(flex.double(filtered_data))
    mean = stats.mean()
    if len(filtered_data) > 1:
      stddev = stats.unweighted_sample_standard_deviation()
    else:
      stddev = 0

    return (mean,
            stddev,
            len(filtered_data),
            len(iterable) - len(filtered_data))


  def beginjob(self, evt, env):
    """The beginjob() function does one-time initialisation from
    event- or environment data.  It is called at an XTC configure
    transition.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    from os import makedirs, path

    super(mod_ledge, self).beginjob(evt, env)
    self._reset_counters()
    self._nframes = 0

    # XXX Don't really want to store the stream, but how to properly
    # close it on exit?
    self._table_path = cspad_tbx.pathsubst(self._table_path, evt, env)
    if not path.isdir(path.dirname(self._table_path)):
      makedirs(path.dirname(self._table_path))

    self._stream_table = open(self._table_path, mode='wb', buffering=1)

    # Initial camera settings requested by Rolf.  The Andor's region
    # of interest is defined by width, height, orgX, and orgY,
    # independent of the binning.  The camera has model number
    # DO936N-M0W-#BN.  Note that the configuration may change on an
    # event-basis.  XXX Does the width really correspond to X, and the
    # height to Y?
    config = cspad_tbx.getConfig(self.address, env)
    if config is not None:
      self.logger.info("Initial effective frame size: %dx%d" %
                       (config.width()  // config.binX(),
                        config.height() // config.binY()))
      self.logger.info("Initial region of interest: (%d, %d) - (%d, %d)" % \
                         (config.orgX(),
                          config.orgY(),
                          config.orgX() + config.width(),
                          config.orgY() + config.height()))

      if config.highCapacity() >= 0 and config.highCapacity() <= 1:
        self.logger.info(
          "Initial output mode: %s" % \
            ("high sensitivity", "high capacity")[config.highCapacity()])
      if config.readoutSpeedIndex() >= 0 and config.readoutSpeedIndex() <= 3:
        self.logger.info(
          "Initial pixel readout rate: %s" % \
            ("5 MHz", "3 MHz", "1 MHz", "50 kHz")[config.readoutSpeedIndex()])
      if config.gainIndex() >= 0 and config.gainIndex() <= 2:
        self.logger.info(
          "Initial pre-amplifier gain: %s" % \
            ("1x", "2x", "4x")[config.gainIndex()])


  def event(self, evt, env):
    """The event() function is called for every L1Accept transition.

    For now, log error and set bogus value to allow stuff to continue
    -- must check for the bogosity later

    XXX The dead time of the detector complicates checking how often
    things are updated!  Move this to the ring buffer?

    @param evt Event data object, a configure object
    @param env Environment object
    """

    from pyana.event import Event
    from acqiris_ext import acqiris_integrate, apd_hitfind

    super(mod_ledge, self).event(evt, env)
    if evt.status() != Event.Normal:
      pass # XXX return -- Never skip because arrays will end up
           # different length, so ignore this?

    # Get the time of the event, in fractional seconds since the
    # epoch.  This is needed for all subsequent history-keeping, and
    # is hence determined first.  XXX Is history-keeping even
    # justified?
    time = cspad_tbx.evt_time(evt)
    if time is None:
      time = float('nan')
    else:
      time = time[0] + time[1] / 1e3
    self._timestamp.append(time)

    # The repetition rate is currently just used for sanity checking.
    repetition_rate = cspad_tbx.evt_repetition_rate(evt)
    if repetition_rate is None:
      repetition_rate = float('nan')
    self._repetition_rate.append(repetition_rate)

    # Get the I0.  No need to warn about it here, it will be done once
    # the image is written out.
    I0 = cspad_tbx.evt_pulse_energy(evt)
    if I0 is None:
      I0 = float('nan')
    self._I0.append(I0)

    # Get the FEE energy.  Average the two readings before and after
    # attenuation separately.  XXX What are the units?  It look like
    # it could be mJ?
    fee_before = 0.5 * sum(evt.getFeeGasDet()[0:2])
    if fee_before is None:
      fee_before = float('nan')
    self._fee_before.append(fee_before)

    fee_after = 0.5 * sum(evt.getFeeGasDet()[2:4])
    if fee_after is None:
      fee_after = float('nan')
    self._fee_after.append(fee_after)

    # XXX Just a check: this is what xtcexplorer does:
    fee_energy = evt.get(xtc.TypeId.Type.Id_FEEGasDetEnergy)
    if fee_energy is not None:
      assert evt.getFeeGasDet()[0] == fee_energy.f_11_ENRC \
        and  evt.getFeeGasDet()[1] == fee_energy.f_12_ENRC \
        and  evt.getFeeGasDet()[2] == fee_energy.f_21_ENRC \
        and  evt.getFeeGasDet()[3] == fee_energy.f_22_ENRC

    """
    # For Bill: expect 84240 data points for r0054
    #
    # grep "^BILL_POINT" | cut -d' ' -f2,3,4,5,6 > t.dat
    # gnuplot> m=0.1 ; k=-0.01e-8; f(x) = k * x + m
    # gnuplot> fit f(x) "t.dat" using ($3):($5) via k,m
    if not hasattr(self, '_gmd_seqno'):
      self._gmd_seqno = 0
    gmd = evt.get(key=xtc.TypeId.Type.Id_GMD)
    if gmd is None:
      return
    acq_apd = evt.getAcqValue('SxrEndstation-0|Acqiris-1', 0, env)
    if acq_apd is not None and acq_apd.waveform() is not None:
      w = acq_apd.waveform()
      baseline = numpy.mean(w[0:(w.shape[0] / 5)])
      peak = numpy.min(w[(w.shape[0] / 5):w.shape[0]])
      self._gmd_seqno += 1
      print "BILL_POINT %d %s %s %s %s" % (self._gmd_seqno,
                                           repr(gmd.fBgValuePerSample),
                                           repr(gmd.fCorrectedSumPerPulse),
                                           repr(gmd.fRelativeEnergyPerPulse),
                                           repr(peak - baseline))
    return
    """

    """
    # XXX Record injector motion--note that they cannot be added--see
    # Ray's email.
    injector_micos_xyz = cspad_tbx.env_pv3_get(
      env,
      ['SXR:EXP:MZM:%02d:ENCPOSITIONGET' % i for i in [1, 2, 3]])
    if injector_micos_xyz is None:
      self.logger.error("No micos injector motor positions")
      injector_micos_xyz = (float('nan'), float('nan'), float('nan'))
    self._injector_micos_xyz.append(injector_micos_xyz)

    injector_rough_xyz = cspad_tbx.env_pv3_get(
      env,
      ['SXR:EXP:MMS:%02d.RBV' % i for i in [1, 2, 3]])
    if injector_rough_xyz is None:
      self.logger.error("No rough injector motor positions")
      injector_rough_xyz = (float('nan'), float('nan'), float('nan'))
    self._injector_rough_xyz.append(injector_rough_xyz)

    # Injector power supplies XXX There is a third PSU, no?
    #
    # The -5kV supply
    # SXR:EXP:SHV:VHS6:CH0:VoltageMeasure
    # SXR:EXP:SHV:VHS6:CH0:CurrentMeasure
    #
    # The plus 5kV supply
    # SXR:EXP:SHV:VHS2:CH0:VoltageMeasure
    # SXR:EXP:SHV:VHS2:CH0:CurrentMeasure
    injector_plus_current = cspad_tbx.env_pv1_get(
      env, 'SXR:EXP:SHV:VHS6:CH0:CurrentMeasure')
    if injector_plus_current is None:
      self.logger.error("No plus-motor current")
      injector_plus_current = -1
    self._injector_plus_current.append(injector_plus_current)

    injector_plus_voltage = cspad_tbx.env_pv1_get(
      env, 'SXR:EXP:SHV:VHS6:CH0:VoltageMeasure')
    if injector_plus_voltage is None:
      self.logger.error("No plus-motor voltage")
      injector_plus_voltage = -1
    self._injector_plus_voltage.append(injector_plus_voltage)

    injector_minus_current = cspad_tbx.env_pv1_get(
      env, 'SXR:EXP:SHV:VHS2:CH0:CurrentMeasure')
    if injector_minus_current is None:
      self.logger.error("No minus-motor current")
      injector_minus_current = -1
    self._injector_minus_current.append(injector_minus_current)

    injector_minus_voltage = cspad_tbx.env_pv1_get(
      env, 'SXR:EXP:SHV:VHS2:CH0:VoltageMeasure')
    if injector_minus_voltage is None:
      self.logger.error("No minus-motor voltage")
      injector_minus_voltage = -1
    self._injector_minus_voltage.append(injector_minus_voltage)
    """

    """
    # The spectrometer motor positions are just used for sanity
    # checking.
    spectrometer_xyz = cspad_tbx.env_spectrometer_xyz_sxr(env)
    if spectrometer_xyz is None:
      self.logger.error("No spectrometer motor positions")
      spectrometer_xyz = (float('nan'), float('nan'), float('nan'))
    self._spectrometer_xyz.append(spectrometer_xyz)
    """

    # Get the pulse energy after monochromator, and fall back on the
    # pre-monochromator energy if the former is absent.  Record in
    # list for mean and stddev.  XXX Verify that the wavelength after
    # the monochromator is updated at around 1 Hz.
    #
    # For the publication an offset and scale were calibrated.
    wavelength = cspad_tbx.env_wavelength_sxr(evt, env)
    if wavelength is None:
      wavelength = cspad_tbx.evt_wavelength(evt)
    if wavelength is None:
      energy = float('nan')
    else:
      energy = 12398.4187 / wavelength
    self._energy.append(energy)
    self._history_energy.push(time, energy) # XXX Not necessary?!

    """
    # Laser shutters XXX need to sort out laser numbering XXX Laser
    # power stuff? XXX Position of polarizer/analyser
    shutters = cspad_tbx.env_laser_shutters(env)
    #print "Got shutters", shutters
    """

    # Read out the diode traces from the via the Acqiris.  XXX In any
    # case, the APD and the more sensitive Opto Diode in the monitor
    # tank (i.e. the transmission diode) should be anti-correlated, so
    # check it!  The entire trace always covers 10 us.  XXX Could this
    # be figured out from xtc.TypeId.Type.Id_AcqConfig?
    #
    # XXX This appears to be suboptimal: look at the
    # skewness-transform for the APD to sort this out.
    acq_apd = evt.getAcqValue('SxrEndstation-0|Acqiris-1', 0, env)
    acq_apd_integral = float('nan')
    if acq_apd is not None:
      waveform = acq_apd.waveform()
      if waveform is not None:
        # With a 40k-point trace, one should integrate from 18200 to
        # 18400.
        waveform = waveform.flatten()
        nmemb = len(waveform) // 200
        if nmemb > 0:
          acq_apd_integral = acqiris_integrate(
            flex.double(waveform), 91 * nmemb, 100 * nmemb, nmemb)
    self._acq_apd_integral.append(acq_apd_integral)

    if evt.expNum() == 208:
      # Opto diode address for L632.
      acq_opto_diode = evt.getAcqValue('SxrEndstation-0|Acqiris-1', 1, env)
    elif evt.expNum() == 363:
      # Opto diode address for LB68.
      acq_opto_diode = evt.getAcqValue('SxrEndstation-0|Acqiris-2', 2, env)
    acq_opto_diode_integral = float('nan')
    if acq_opto_diode is not None:
      waveform = acq_opto_diode.waveform()
      if waveform is not None:
        # With a 40k-point trace, one should integrate from 16000 to
        # 24000.  With a 20k-point trace, a suitable integration
        # region is bounded by 8000 and 12000.  There is no need for
        # thresholding, because the integral of the Opto Diode will
        # not be used for hit finding.  XXX What are the "misses" we
        # record on the Opto Diode?  XXX The direct beam is completely
        # gone after it hits the sample, because soft X-rays.
        waveform = waveform.flatten()
        nmemb = len(waveform) // 5
        if nmemb > 0:
          acq_opto_diode_integral = acqiris_integrate(
            flex.double(waveform), 2 * nmemb, 4 * nmemb, nmemb)
    self._acq_opto_diode_integral.append(acq_opto_diode_integral)

    # Sanity check: verify that the timestamps for the two Acqiris
    # traces are similar enough.
    if acq_apd is not None and acq_opto_diode is not None:
      assert \
          len(acq_apd.timestamps()) == len(acq_opto_diode.timestamps()) and \
          numpy.any(numpy.abs(acq_apd.timestamps() -
                              acq_opto_diode.timestamps())) < 1e-6

    #self.logger.info("DIODE INTEGRALS: %f %f %f" % (I0, acq_apd_integral, acq_opto_diode_integral))

    """
    import matplotlib.pyplot as plt

    hit_array_apd = apd_hitfind(
      flex.double(acq_apd.waveform()),
      len(acq_apd.waveform()) // 5)
    hit_array_opto_diode = apd_hitfind(
      flex.double(acq_opto_diode.waveform()),
      len(acq_opto_diode.waveform()) // 5)

    fig = plt.figure()
    ax = fig.add_subplot(111)
    #ax.plot(
    #  range(len(acq_apd.timestamps())), acq_apd.waveform())
    ax.plot(
      range(len(acq_opto_diode.timestamps())), acq_opto_diode.waveform()[0, :])
    plt.show()

    fig = plt.figure()
    ax = fig.add_subplot(111)
    #ax.plot(
    #  acq_apd.timestamps()[0:len(hit_array_apd)], hit_array)
    ax.plot(
      acq_opto_diode.timestamps()[0:len(hit_array_opto_diode)], hit_array)
    plt.show()
    """

    # Determine whether the beam hit the sample, and register the
    # outcome.  If not using any diodes for hit-finding, every shot is
    # assumed to be a hit.  XXX Unfortunately, this crucial piece is
    # very unreliable.  The threshold for the APD needs to be
    # verified--inspect all the histograms.  XXX hitfind_flags is
    # probable better as a module parameter.
#    hitfind_flags = 0x3
    hitfind_flags = 0
    hit = False
    if not hitfind_flags:
      hit = True
    elif hitfind_flags & 0x1 and acq_apd_integral > 0.2:
      hit = True
    self._hit.append(hit)

    # Always proceed all the way through (even if some shots have
    # invalid values of e.g. I0) because images are precious.  XXX
    # Must reset counters before returning!  XXX What about skipping
    # all of the above if display is True?
    if self.cspad_img is not None:
      self._nframes += 1

      """
      # The spectrometer should not move!
      t = (self._spectrometer_xyz -
           self._spectrometer_xyz.mean()).rms_length()
      print "Spectrometer displacement", t

      # Fine/rough motor position deviations from the mean.  See Ray's
      # email.
      t = (self._injector_micos_xyz -
           self._injector_micos_xyz.mean()).rms_length()
      print "Injector micos displacement", t

      t = (self._injector_rough_xyz -
           self._injector_rough_xyz.mean()).rms_length()
      print "Injector rough displacement", t

      # Injector motor position means and deviations
      if self._injector_plus_current.size() > 1:
        t = flex.mean_and_variance(self._injector_plus_current)
        print "Injector plus current mean %10e stddev %10e" % \
            (t.mean(), t.unweighted_sample_standard_deviation())
      if self._injector_plus_voltage.size() > 1:
        t = flex.mean_and_variance(self._injector_plus_voltage)
        print "Injector plus voltage mean %10e stddev %10e" % \
            (t.mean(), t.unweighted_sample_standard_deviation())

      if self._injector_minus_current.size() > 1:
        t = flex.mean_and_variance(self._injector_minus_current)
        print "Injector minus current mean %10e stddev %10e" % \
            (t.mean(), t.unweighted_sample_standard_deviation())
      if self._injector_minus_voltage.size() > 1:
        t = flex.mean_and_variance(self._injector_minus_voltage)
        print "Injector minus voltage mean %10e stddev %10e" % \
            (t.mean(), t.unweighted_sample_standard_deviation())

      """

      # Energy statistics are collected from all shots, regardless of
      # whether they are hits or not.  Since this statistic mentions
      # the frame number, it should be reported first.  XXX The energy
      # should have a really small standard deviation.  Check
      # self._energy.size() and self._history_energy.frequency() XXX
      # verify that it works for one data point.
      (energy_mean, energy_stddev, energy_nmemb, n) = self._filtered_stats(
        lambda x: not math.isnan(x) and x > 0, self._energy)
      if n > 0:
        self.logger.warning("%d shots have undefined energy" % n)

      (I0_mean, I0_stddev, I0_nmemb, n) = self._filtered_stats(
        lambda x: not math.isnan(x), self._I0)
      if n > 0:
        self.logger.warning("%d shots have undefined I0" % n)

      self.logger.info(
        "Frame %d: E=%.3f+/-%.3f (N=%d) I0=%.0f+/-%.0f (N=%d)" %
        (self._nframes,
         energy_mean, energy_stddev, energy_nmemb,
         I0_mean, I0_stddev, I0_nmemb))

      # Sanity check: unless changed while integrating the frame, the
      # repetition rate should have a standard deviation of zero.
      dt = self._timestamp[-1] - self._timestamp[0]
      rr_mean = rr_observed = rr_stddev = 0
      if dt > 0:
        rr_observed = (len(self._timestamp) - 1) / dt
        rr = [x for x in self._repetition_rate if not math.isnan(x) and x > 0]
        if len(rr) > 1:
          rr_stats = flex.mean_and_variance(flex.double(rr))
          rr_mean = rr_stats.mean()
          rr_stddev = rr_stats.unweighted_sample_standard_deviation()
      self.logger.info(
        "Repetition rate: %.3f Hz (observed), %.3f+/-%.3f Hz (expected)" %
        (rr_observed, rr_mean, rr_stddev))

      # Compare observed and configured exposure time.
      config = cspad_tbx.getConfig(self.address, env)
      exposure_time = 0
      if config is not None and dt > 0 and len(self._timestamp) > 0:
        exposure_time = dt * (len(self._timestamp) + 1) / len(self._timestamp)
      self.logger.info(
        "Exposure time: %.3f s (observed), %.3f s (configured)" %
        (exposure_time, config.exposureTime()))

      # Compute the leading dead time, the time between starting the
      # readout of the previous frame and the arrival of the shot
      # immediately following it.  This is an interesting statistic,
      # no matter what.  XXX Maybe look at its distribution?
      dead_time = 0
      if rr_observed > 0 and hasattr(self, '_previous_readout_time'):
        dead_time = \
            self._timestamp[0] - self._previous_readout_time - 1 / rr_observed
        if math.isnan(dead_time):
          dead_time = 0
      self.logger.info("Dead time: %.3f s" % dead_time)
      self._previous_readout_time = self._timestamp[-1]

      assert time == self._timestamp[-1] # XXX ZAP once one run survives it!

      # Flag blank images (i.e. images that had no hits), because
      # these may interesting for background subtraction.
      hits = self._hit.count(True)
      self.logger.info("Hit rate: %d/%d (%.2f%%)" %
                       (hits, self._hit.size(), 100 * hits / self._hit.size()))
      if hits == 0:
        self.logger.info("Frame %d is blank" % self._nframes)

      # Get the normalisation factor by summing up I0 for all hits.
      # Invalid and non-positive values of I0 are treated as zeroes.
      # XXX Make this kind of summing a function of its own.
      I0 = sum([x for x in self._I0.select(self._hit) if not math.isnan(x) and x > 0])
      I0_all = sum([x for x in self._I0 if not math.isnan(x) and x > 0])

      fee_before_all = sum([x for x in self._fee_before if not math.isnan(x) and x > 0])
      fee_after_all = sum([x for x in self._fee_after if not math.isnan(x) and x > 0])

      # Register the template to the image and locate the regions of
      # interest based on the registration parameters.  XXX Should
      # also give contrast: fit 2D-Gaussian to peak and report its
      # standard deviations and fit?
      if self._template is not None:
        gamma = lewis(self._template, self.cspad_img)
        p = flex.max_index(gamma)
        peak = (p // gamma.focus()[1] - self._template.focus()[0] + 1,
                p % gamma.focus()[1] - self._template.focus()[1] + 1)

        #"""
        ### REFERENCE CHECK ###
        from os.path import dirname, isdir, join
        from scipy import io

        mat_dirname = dirname(cspad_tbx.pathsubst(
          self._mat_path, evt, env, frame_number=self._nframes))
        if not isdir(mat_dirname):
          makedirs(mat_dirname)

        io.savemat(
          file_name=join(mat_dirname, 'cross-check-%05d.mat' % self._nframes),
          mdict=dict(
            image=self.cspad_img.as_numpy_array(),
            template=self._template.as_numpy_array(),
            gamma=gamma.as_numpy_array(),
            peak=numpy.array(peak)),
          appendmat=False,
          do_compression=True,
          oned_as='column')

        return
        ### REFERENCE CHECK ###
        #"""
      else:
        # Alternative: position everything with respect to the frame
        # origin.
        peak = (0, 0)

      # XXX Come up with a better way to handle the offsets!  They
      # really do depend on the template, and should therefore be
      # packaged with it.
      self.logger.info("Template registration anchor point (%d, %d)" %
                       (peak[0], peak[1]))

      roi = []
      if evt.expNum() == 208:
        # Regions of interest for L632 (experiment number 208).  XXX
        # Could perhaps migrate the template matching here instead?

        # The left, middle, and right manganese signals.  XXX Extend the
        # rightmost ROI three pixels in upward direction (see runs 145
        # and onwards, also note narrower slit)?
        roi.append((peak[0] + 59, peak[1] - 24, 12, 5))
        roi.append((peak[0] + 61, peak[1] + 28, 12, 4))
        roi.append((peak[0] + 61, peak[1] + 79, 12, 5))

        # Two background regions between the manganese spots, with the
        # same total area as the signal.
        roi.append((peak[0] + 62, peak[1] +  1, 8, 8))
        roi.append((peak[0] + 63, peak[1] + 51, 8, 8))

        # The left and right direct reflections from the Si substrate
        # (i.e. the areas between the zone plates).  These were the
        # features used for template registration.
        roi.append((peak[0], peak[1],      40, 10))
        roi.append((peak[0], peak[1] + 50, 40,  9))

        # Spot between the direct reflections.  XXX What is this?
        roi.append((peak[0] + 1, peak[1] + 23, 22, 13))

        # The horizontal slit, where the direct reflection occurs.  This
        # is fixed.  XXX Verify this!
        roi.append((22, 0, 41, 128))

        # Background stripe, below the manganese spots.  This is fixed
        # to the bottom of the detector.
        roi.append((104, 0, 20, 128))

      elif evt.expNum() == 363:
        # Regions of interest for LB68 (experiment number 363).
        # 0-pixel are active, 255-pixel are inactive
        from scipy.misc import imread

        # Dec 5, 2013 (09:00 - 21:00): initial estimates from r0010
        """
        roi.append((peak[0] +  14, peak[1] + 138 + 23, 25, 50 - 25))
        roi.append((peak[0] +  45, peak[1] + 138 + 23, 25, 50 - 25))
        roi.append((peak[0] +  78, peak[1] + 137 + 23, 25, 50 - 25))
        roi.append((peak[0] + 111, peak[1] + 137 + 23, 25, 50 - 25))
        roi.append((peak[0] + 144, peak[1] + 137 + 23, 25, 50 - 25))
        roi.append((peak[0] + 177, peak[1] + 136 + 23, 25, 50 - 25))
        roi.append((peak[0] + 210, peak[1] + 136 + 23, 25, 50 - 25))
        roi.append((peak[0] + 243, peak[1] + 136 + 23, 25, 50 - 25))
        roi.append((peak[0] + 278, peak[1] + 135 + 23, 25, 50 - 25))
        roi.append((peak[0] + 312, peak[1] + 135 + 23, 25, 50 - 25))
        roi.append((peak[0] + 344, peak[1] + 135 + 23, 25, 50 - 25))
        roi.append((peak[0] + 376, peak[1] + 135 + 23, 25, 50 - 25))
        roi.append((peak[0] + 408, peak[1] + 135 + 23, 25, 50 - 25))
        roi.append((peak[0] + 442, peak[1] + 135 + 23, 25, 50 - 25))
        roi.append((peak[0] + 475, peak[1] + 135 + 23, 25, 50 - 25))
        """

        # Dec 6, 2013 (09:00 - 21:00): rough estimates
        """
        roi.append((peak[0] + 0, peak[1] +  25, 512,  25)) # bkg
        roi.append((peak[0] + 0, peak[1] + 135, 512,  25)) # oxygen
        roi.append((peak[0] + 0, peak[1] + 160, 512,  25)) # signal
        roi.append((peak[0] + 0, peak[1] + 300, 512, 130)) # zeroth order
        """

        # Dec 7, 2013 (09:00 - 21:00): overlap between oxygen and
        # signal.  Will loose some signal.
        """
        roi.append((peak[0] + 0, peak[1] +  25, 512,  25)) # bkg
        roi.append((peak[0] + 0, peak[1] + 135, 512,  50)) # oxygen
        roi.append((peak[0] + 0, peak[1] + 185, 512,  40)) # signal
        roi.append((peak[0] + 0, peak[1] + 270, 512, 170)) # zeroth order
        """

        """
        # Dec 7 2013 (09:00 - 21:00): binary masks stored in PNG
        # images.

        roi.append((peak[0] + 0, peak[1] +  25, 512,  25)) # bkg
        roi.append((peak[0] + 0, peak[1] + 135, 512,  25)) # oxygen

        #roi_image = flex.float(
        #  imread('/reg/neh/home1/hattne/myrelease/LB68-r0039-max-mask.png',
        #         flatten=True))
        #roi_image = flex.float(
        #  imread('/reg/neh/home1/hattne/myrelease/LB68-r0039-std-mask.png',
        #         flatten=True))
        roi_image = flex.float(
          imread('/reg/neh/home1/hattne/myrelease/LB68-r0052-avg-mask.png',
                 flatten=True))
        roi_image = (255 - roi_image)

        #roi.append((0, 0, self.cspad_img.focus()[0], self.cspad_img.focus()[1]))
        roi.append(roi_image)

        roi.append((peak[0] + 0, peak[1] + 270, 512, 170)) # zeroth order
        """

        # Dec 9, 2013 (09:00 - 21:00)
        #"""
        roi.append((peak[0] + 0, peak[1] +  25, 512,  25)) # bkg
        roi.append((peak[0] + 0, peak[1] + 135, 512,  25)) # oxygen
        #roi.append((peak[0] + 0, peak[1] + 160, 512,  25)) # signal
        roi_image = flex.float(
          imread('/reg/neh/home1/hattne/myrelease/LB68-r0067-max-mask.png',
                 flatten=True))
        roi.append(roi_image)

        roi.append((peak[0] + 0, peak[1] + 240, 512, 180)) # zeroth order
        #"""

      else:
        self.logger.error(
          "No regions of interest for %s (experiment number %d)" % (
            env.experiment(), evt.expNum()))

      # Clip the regions of interest to the actual image.  If the ROI
      # does not overlap with the image at all, set its width and
      # height to zero.  XXX Do the integration here as well?
      for i in range(len(roi)):
        if not isinstance(roi[i], tuple):
          continue

        r = roi[i]
        if    r[0] + r[2] < 0 or r[0] >= self.cspad_img.focus()[0] or \
              r[1] + r[3] < 0 or r[1] >= self.cspad_img.focus()[1]:
          roi[i] = (r[0], r[1], 0, 0)
          continue

        r = roi[i]
        if r[0] < 0:
          roi[i] = (0, r[1], r[2] + r[0], r[3])

        r = roi[i]
        if r[1] < 0:
          roi[i] = (r[0], 0, r[2], r[3] + r[1])

        r = roi[i]
        if r[0] + r[2] > self.cspad_img.focus()[0]:
          roi[i] = (r[0], r[1], self.cspad_img.focus()[0] - r[0], r[3])

        r = roi[i]
        if r[1] + r[3] > self.cspad_img.focus()[1]:
          roi[i] = (r[0], r[1], r[2], self.cspad_img.focus()[1] - r[1])

      # Sum up intensities in all regions of interest, and keep track
      # of the actual number of pixels summed.  The common_mode module
      # takes care of dark-subtraction.  XXX Would like to estimate
      # sigma for spot, like in spotfinder/LABELIT.
      I = flex.double(len(roi))
      I_nmemb = flex.int(len(roi))
      for i in range(len(roi)):
        if isinstance(roi[i], flex.float):
          sel = roi[i].as_1d() < 128
          I[i] = flex.sum(self.cspad_img.as_1d().select(sel))
          I_nmemb[i] = sel.count(True)
          continue

        if roi[i][2] <= 0 or roi[i][3] <= 0:
          I[i] = 0
          I_nmemb[i] = 0
        else:
          I[i] = flex.sum(self.cspad_img.matrix_copy_block(
              i_row=roi[i][0],
              i_column=roi[i][1],
              n_rows=roi[i][2],
              n_columns=roi[i][3]))
          I_nmemb[i] = roi[i][2] * roi[i][3]
          """
          # Sanity check: white out the region of interest.
          self.cspad_img.matrix_paste_block_in_place(
            block=flex.double(flex.grid(roi[i][2], roi[i][3])),
            i_row=roi[i][0],
            i_column=roi[i][1])
          """

      acq_apd_sum = sum(
        [x for x in self._acq_apd_integral.select(self._hit) if not math.isnan(x) and x > 0])
      acq_opto_diode_sum = sum(
        [x for x in self._acq_opto_diode_integral.select(self._hit) if not math.isnan(x) and x > 0])

      acq_apd_sum_all = sum(
        [x for x in self._acq_apd_integral if not math.isnan(x) and x > 0])
      acq_opto_diode_sum_all = sum(
        [x for x in self._acq_opto_diode_integral if not math.isnan(x) and x > 0])

      # Append the data point to the stream: shots, hits, energy, and
      # I.  XXX OrderedDict requires Python 2.7, could fall back on
      # regular Dict at the price of non-deterministic column order.
      from collections import OrderedDict
      csv_dict = OrderedDict([
        ('n_frames', self._hit.size()),
        ('n_hits', hits),
        ('I0', I0),
        ('I0_all', I0_all),
        ('fee_before_all', fee_before_all),
        ('fee_after_all', fee_after_all),
        ('energy_mean', energy_mean),
        ('acq_apd_sum', acq_apd_sum),
        ('acq_apd_sum_all', acq_apd_sum_all),
        ('acq_opto_diode_sum', acq_opto_diode_sum),
        ('acq_opto_diode_sum_all', acq_opto_diode_sum_all)])
      for (i, item) in enumerate(zip(roi, I, I_nmemb)):
        key = 'roi_' + ('bkg', 'oxygen', 'manganese', 'zeroth_order')[i]
        csv_dict['%s_nmemb' % key] = item[2]

        if isinstance(item[0], tuple):
          csv_dict['%s_ss_start' % key] = item[0][0]
          csv_dict['%s_fs_start' % key] = item[0][1]
          csv_dict['%s_ss_size' % key] = item[0][2]
          csv_dict['%s_fs_size' % key] = item[0][3]
        else:
          csv_dict['%s_ss_start' % key] = 0
          csv_dict['%s_fs_start' % key] = 0
          csv_dict['%s_ss_size' % key] = item[0].focus()[0]
          csv_dict['%s_fs_size' % key] = item[0].focus()[1]

        csv_dict['%s_I' % key] = item[1]

      # XXX assert that keys match up with what's in the file already?
      # Or exploit the error-reporting mechanism already implemented?
      # Write the header.  XXX How to control the order of the
      # columns?
      if not hasattr(self, '_csv'):
        from csv import DictWriter
        self._csv = DictWriter(self._stream_table, list(csv_dict.keys()))
        self._csv.writerow({key: key for key in csv_dict.keys()})
      self._csv.writerow(csv_dict)

      # Output the non-normalised image and all other relevant data to
      # a binary MATLAB file.  XXX What if scipy is not available?
      from os import makedirs, path
      from scipy import io

      mat_path = cspad_tbx.pathsubst(
        self._mat_path, evt, env, frame_number=self._nframes)
      if not path.isdir(path.dirname(mat_path)):
        makedirs(path.dirname(mat_path))

      io.savemat(
        file_name=mat_path,
        mdict=dict(
          DATA=self.cspad_img.as_numpy_array(),
          DIODES=numpy.array((acq_apd_sum, acq_apd_sum_all,
                              acq_opto_diode_sum, acq_opto_diode_sum_all)),
          ENERGY=energy_mean,
          HITS=numpy.array((hits, self._hit.size())),
          I0=numpy.array((I0, I0_all)),
          INTENSITIES=numpy.array(I),
          ROIS=numpy.array([r for r in roi if isinstance(r, tuple)])),
        appendmat=False,
        do_compression=True,
        oned_as='column')

      # Optionally update the image in the viewer.  See mod_view.
      if self._display:
        from time import localtime, strftime

        # Copy over regions of interest to shared multiprocessing
        # array.  XXX Flip to honour wxPython convention.
        for i in range(len(roi)):
          if not isinstance(roi[i], tuple):
            continue
          self._roi[4 * i + 0] = roi[i][1]
          self._roi[4 * i + 1] = roi[i][0]
          self._roi[4 * i + 2] = roi[i][3]
          self._roi[4 * i + 3] = roi[i][2]

        time_str = strftime("%H:%M:%S", localtime(evt.getTime().seconds()))
        title = "r%04d@%s: frame %d on %s" \
            % (evt.run(), time_str, self._nframes, self.address)

        # XXX No distance in the Andor experiment.  So don't bother
        # with the fictional beam center, distance, and saturation
        # value?  See also mod_average.endjob()
        img_obj = (dict(BEAM_CENTER=(0, 0),
                        DATA=self.cspad_img,
                        DETECTOR_ADDRESS=self.address,
                        DISTANCE=10, # XXX Evil kludge to keep dxtbx happy!
                        PIXEL_SIZE=13.5e-3, # XXX Hard-coded, again!
                        SATURATED_VALUE=10000,
                        TIME_TUPLE=cspad_tbx.evt_time(evt),
                        WAVELENGTH=12398.4187 / energy),
                   title)

        while not self._queue.empty():
          if not self._proc.is_alive():
            evt.setStatus(Event.Stop)
            return
        while True:
          try:
            self._queue.put(img_obj, timeout=1)
            break
          except Exception: #Queue.Full:
            pass

      self._reset_counters()
      return


  #signature for pyana:
  #def endjob(self, env):

  #signature for psana:
  #def endjob(self, evt, env):

  def endjob(self, obj1, obj2=None):
    """The endjob() function terminates the viewer process by sending
    it a @c None object, and waiting for it to finish.

    @param evt Event object (psana only)
    @param env Environment object
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    # Close the stream.
    self._stream_table.close()

    self.logger.info("XXX We gotta get out into space")

    # Optionally, wait for the viewer to shut down, see mod_view.
    if self._display:
      try:
        self._queue.put(None)
      except Exception:
        pass
      self._proc.join()


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_mar.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# $Id$

from __future__ import absolute_import, division, print_function

import logging, os

from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import skip_event_flag
from scitbx.array_family import flex
from six.moves import range

class mod_mar(object):
  def __init__(self, address, directory, beam_x = None, beam_y = None, template = None):
    """
    @param address     Address string XXX Que?!
    @param directory   Directory portion of the MAR image paths
    @param beam_x      Beam center x in pixels. Uses dxtbx beam center if not specified.
    @param beam_y      Beam center y in pixels. Uses dxtbx beam center if not specified.
    @param template    Simple template for filtering files processed. If @param template
                       is in the file name, the file is accepted
    """
    self._logger = logging.getLogger(self.__class__.__name__)
    self._logger.setLevel(logging.INFO)

    # This is needed as a key for the image data.
    self._address     = cspad_tbx.getOptString(address)
    self._directory   = cspad_tbx.getOptString(directory)

    # Save the beam center and image template
    self._beam_x      = cspad_tbx.getOptInteger(beam_x)
    self._beam_y      = cspad_tbx.getOptInteger(beam_y)
    self._template    = cspad_tbx.getOptString(template)

    # At the moment, we cannot use multiprocessing for this anyway.
    # This has to be an absolute path to the image.
    self._path = None
    self._mccd_name = None


  def begincalibcycle(self, evt, env):
    # This is all very specific to the XPP experiments, so don't worry
    # so much about hardcoded stuff.

    from os.path import join
    from pypdsdata import xtc

    ctrl_config = env.getConfig(xtc.TypeId.Type.Id_ControlConfig)
    if ctrl_config is None:
      return

    # This is for the L650 experiment and beyond
    path_d = {
      'filename':"",
      'directory_part1':"",
      'directory_part2':""
    }

    for i in range(ctrl_config.npvLabels()): # interface change for psana: used to add 1 to the return value of npvLabels()
      try:
        pv = ctrl_config.pvLabel(i)
      except AttributeError:
        # interface change for psana
        pv = ctrl_config.pvLabels()[i]

      if pv.name() in path_d:
        path_d[pv.name()] = pv.value()

    if path_d['filename'] != "":
      # directory_part1 contains two parts, including where it came from and a user specific part.
      # Example: /data/blctl/G78/test/A1. We need only 'test/A1' from that string, but 'G78' varies.
      path_d['directory_part1'] = os.sep.join(path_d['directory_part1'].split(os.sep)[4:])
      self._path = join(self._directory,
                        path_d['directory_part1'],
                        path_d['directory_part2'],
                        path_d['filename'] + '.mccd')
      self._mccd_name  = path_d['filename']
      return

    # The value of this control must be of integer type.  If that is
    # not the case, the error will be caught during access.  This is
    # for the L748 experiment.
    for i in range(ctrl_config.npvControls() + 1):
      if not hasattr(ctrl_config, 'pvControl'): continue
      pv = ctrl_config.pvControl(i)
      if pv is None or pv.name() != 'marccd_filenumber':
        continue
      self._path = join(
        self._directory, 'xpp74813_image_%06d.mccd' % int(round(pv.value())))
      return


  def beginjob(self, evt, env):
    self._fmt = None


  def event(self, evt, env):
    import dxtbx.format.Registry
    from os.path import exists
    from time import sleep

    # Nop if there is no image.  For experiments configured to have
    # exactly one event per calibration cycle, this should never
    # happen.
    if self._path is None:
      evt.put(skip_event_flag(), "skip_event")
      return

    # Skip this event if the template isn't in the path
    if self._template is not None and not True in [t in self._path for t in self._template.split(',')]:
      evt.put(skip_event_flag(), "skip_event")
      return

    if "phi" in self._path:
      evt.put(skip_event_flag(), "skip_event")
      return

    # Wait for the image to appear in the file system, probing for it
    # at exponentially increasing delays.
    t = 1
    t_tot = 0

    if not exists(self._path):
      self._logger.info("Waiting for path %s"%self._path)

    while not exists(self._path):
      if t_tot > 1:
        self._logger.info("Timeout waiting for path %s"%self._path)
        evt.put(skip_event_flag(), "skip_event")
        self._logger.info("Image not found:  %s"%self._path)
        return
      sleep(t)
      t_tot += t
      t *= 2

    # Find a matching Format object and instantiate it using the
    # given path.  If the Format object does not understand the image,
    # try determining a new format.  XXX Emits "Couldn't create a
    # detector model for this image".
    if self._fmt is None:
      self._fmt = dxtbx.format.Registry.get_format_class_for_file(self._path)
      if self._fmt is None:
        evt.put(skip_event_flag(), "skip_event")
        return

    img = self._fmt(self._path)
    if img is None:
      self._fmt = dxtbx.format.Registry.get_format_class_for_file(self._path)
      if self._fmt is None:
        evt.put(skip_event_flag(), "skip_event")
        return
      img = self._fmt(self._path)
      if img is None:
        evt.put(skip_event_flag(), "skip_event")
        return

    self._logger.info(
      "Reading %s using %s" % (self._path, self._fmt.__name__))

    # Get the raw image data and convert to double precision floating
    # point array.  XXX Why will img.get_raw_data() not work, like it
    # does in print_header?
    db = img.get_detectorbase()
    db.readHeader()
    db.read()
    data = db.get_raw_data().as_double()

    # Get the pixel size and store it for common_mode.py
    detector = img.get_detector()[0]
    ps = detector.get_pixel_size()
    assert ps[0] == ps[1]
    pixel_size = ps[0]
    evt.put(ps[0],"marccd_pixel_size")
    evt.put(detector.get_trusted_range()[1],"marccd_saturated_value")
    evt.put(detector.get_distance(),"marccd_distance")

    # If the beam center isn't provided in the config file, get it from the
    # image.  It will probably be wrong.
    if self._beam_x is None or self._beam_y is None:
      self._beam_x, self._beam_y = detector.get_beam_centre_px(img.get_beam().get_s0())
      self._beam_x = int(round(self._beam_x))
      self._beam_y = int(round(self._beam_y))

    # Crop the data so that the beam center is in the center of the image
    maxy, maxx = data.focus()

    minsize = min([self._beam_x,self._beam_y,maxx-self._beam_x,maxy-self._beam_y])

    data = data[self._beam_y-minsize:self._beam_y+minsize,self._beam_x-minsize:self._beam_x+minsize]
    evt.put((minsize,minsize),"marccd_beam_center")

    evt.put(flex.int([0,0,minsize*2,minsize*2]),"marccd_active_areas")

    # Store the image in the event.
    evt.put(data, self._address)
    # Store the .mmcd file name in the event
    evt.put(self._mccd_name, "mccd_name")

    evt_time = cspad_tbx.evt_time(evt) # tuple of seconds, milliseconds
    timestamp = cspad_tbx.evt_timestamp(evt_time) # human readable format
    self._logger.info("converted  %s to pickle with timestamp %s" %(self._path, timestamp))

    # This should not be necessary as the machine is configured with
    # one event per calibration cycle.
    self._path = None

  #signature for pyana:
  #def endjob(self, env):

  #signature for psana:
  #def endjob(self, evt, env):

  def endjob(self, obj1, obj2=None):
    """
    @param evt Event object (psana only)
    @param env Environment object
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    pass


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_onecolor_spectrum_filter.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#

''' Filters shots from FEE spectrometer '''
from __future__ import absolute_import, division, print_function
from libtbx.phil import parse
from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import skip_event_flag
from xfel.cxi.spectra_filter import spectra_filter, phil_scope

class mod_onecolor_spectrum_filter(object):
  def __init__(self,
               phil_file = None,
               selected_filter = None):
    """The mod_onecolor_spectrum_filter class constructor stores the parameters passed
    from the psana configuration file in instance variables.

    @param phil_file Filter parameters. See cxi/spectra_filter.py
    @param selected_filter Which of the filters in phil_file to apply
    """
    self.params = phil_scope.fetch(parse(file_name = phil_file)).extract()
    self.filter = spectra_filter(self.params)
    self.selected_filter = selected_filter
    self.n_accepted = 0
    self.n_total = 0

  def beginjob(self, evt, env):
    pass

  def event(self,evt,evn):
    """The event() function puts a "skip_event" object with value @c
    True into the event if the shot is to be skipped.

    @param evt Event data object, a configure object
    @param env Environment object
    """
    if (evt.get("skip_event")):
      return
    self.n_total += 1
    ts = cspad_tbx.evt_timestamp(cspad_tbx.evt_time(evt))

    accept = self.filter.filter_event(evt, self.selected_filter)[0]
    if not accept:
      print("Skipping event", ts, ": didn't pass filter", self.selected_filter)
      evt.put(skip_event_flag(), "skip_event")
      return

    print("Accepting event", ts, ": passed filter", self.selected_filter)
    self.n_accepted += 1

  def endjob(self, obj1, obj2=None):
    """
    @param evt Event object (psana only)
    @param env Environment object
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    print("Accepted %d of %d shots"%(self.n_accepted, self.n_total))


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_param.py
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# $Id$
"""Simple analysis of per-shot parameters of a run

XXX mod_param must be run as a single process--guard against it!
"""
from __future__ import absolute_import, division, print_function

__version__ = "$Revision$"

import logging
import math

from xfel.cxi.cspad_ana import cspad_tbx


class mod_param(object):
  """Class for simple analysis of certain per-shot parameters of a
  run.  Currently attenuation, sample-detector distance, and
  wavelength are considered.  Since attenuation and distance is not
  expected to vary, their values are output only when changed.  For
  the wavelength, the average and standard deviation is computed.
  Note that incompleteness due to lacking image data is not detected,
  because it may slow down this module considerably.
  """


  def __init__(self):
    """The mod_param class constructor initialises the logger.
    """

    self.m_logger = logging.getLogger(__name__)
    self.m_logger.setLevel(logging.INFO)


  def __del__(self):
    logging.shutdown()


  def beginjob(self, evt, env):
    """The beginjob() function does one-time initialisation from
    event- or environment data.  It is called at an XTC configure
    transition.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    self.m_nfail = 0
    self.m_nmemb = 0

    self.m_no_detz       = 0
    self.m_no_sifoil     = 0
    self.m_no_wavelength = 0

    self.m_detz       = None
    self.m_detz_max   = None
    self.m_detz_min   = None
    self.m_sifoil     = None
    self.m_sifoil_max = None
    self.m_sifoil_min = None


  def event(self, evt, env):
    """The event() function is called for every L1Accept transition.
    The event() function does not log shots skipped due to
    incompleteness in order to keep the output streams clean.
    Instead, the number of skipped shots is reported by endjob().

    @param evt Event data object, a configure object
    @param env Environment object
    """

    if (evt.get("skip_event")):
      return

    # XXX This hardcodes the address for the front detector!
    detz = cspad_tbx.env_detz('CxiDs1-0|Cspad-0', env)
    if (detz is None):
      self.m_no_detz += 1

    sifoil = cspad_tbx.env_sifoil(env)
    if (sifoil is None):
      self.m_no_sifoil += 1

    timestamp = cspad_tbx.evt_timestamp(cspad_tbx.evt_time(evt))
    if (timestamp is None):
      self.m_no_timestamp += 1

    wavelength = cspad_tbx.evt_wavelength(evt)
    if (wavelength is None):
      self.m_no_wavelength += 1

    if (detz       is None or
        sifoil     is None or
        timestamp  is None or
        wavelength is None):
      self.m_nfail += 1
      return

    if (detz != self.m_detz):
      if (self.m_detz is None):
        self.m_logger.info("%s: initial detz     % 8.4f" %
                           (timestamp, detz))
      else:
        self.m_logger.info("%s: detz     % 8.4f -> % 8.4f" %
                           (timestamp, self.m_detz, detz))

      self.m_detz = detz
      if (self.m_detz_max is None or detz > self.m_detz_max):
        self.m_detz_max = detz
      if (self.m_detz_min is None or detz < self.m_detz_min):
        self.m_detz_min = detz

    if (sifoil != self.m_sifoil):
      if (self.m_sifoil is None):
        self.m_logger.info("%s: initial Si-foil  % 8d" %
                           (timestamp, sifoil))
      else:
        self.m_logger.info("%s: Si-foil  % 8d -> % 8d" %
                           (timestamp, self.m_sifoil, sifoil))

      self.m_sifoil = sifoil
      if (self.m_sifoil_max is None or sifoil > self.m_sifoil_max):
        self.m_sifoil_max = sifoil
      if (self.m_sifoil_min is None or sifoil < self.m_sifoil_min):
        self.m_sifoil_min = sifoil

    # Accumulate the sum and the squared sum of the shifted the
    # wavelength.  The shift is taken as the first wavelength
    # encountered.  This may be more accurate than accumulating raw
    # values [Chan et al. (1983) Am. Stat. 37, 242-247].
    if (self.m_nmemb == 0):
      self.m_wavelength_shift  = wavelength
      self.m_wavelength_sum    = (wavelength - self.m_wavelength_shift)
      self.m_wavelength_sumsq  = (wavelength - self.m_wavelength_shift)**2
      self.m_nmemb             = 1
    else:
      self.m_wavelength_sum   += (wavelength - self.m_wavelength_shift)
      self.m_wavelength_sumsq += (wavelength - self.m_wavelength_shift)**2
      self.m_nmemb            += 1

  #signature for pyana:
  #def endjob(self, env):

  #signature for psana:
  #def endjob(self, evt, env):

  def endjob(self, obj1, obj2=None):
    """The endjob() function finalises the mean and standard deviation
    calculations, and reports on the total number of skipped shots.

    @param evt Event object (psana only)
    @param env Environment object
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    if (self.m_nmemb > 0):
      # The shift has to be added back to the average wavelength, but
      # not the standard deviation.
      avg_wavelength     = self.m_wavelength_sum / self.m_nmemb
      stddev_wavelength  = self.m_wavelength_sumsq \
          -                self.m_wavelength_sum * avg_wavelength
      avg_wavelength    += self.m_wavelength_shift

      if (stddev_wavelength < 0):
        stddev_wavelength = 0
      elif (self.m_nmemb > 1):
        stddev_wavelength = math.sqrt(stddev_wavelength / (self.m_nmemb - 1))
      else:
        stddev_wavelength = math.sqrt(stddev_wavelength / (self.m_nmemb - 0))

      self.m_logger.info("Det-z:      min % 12.6f, max   % 12.6f" %
                         (self.m_detz_min, self.m_detz_max))
      self.m_logger.info("Si-foil:    min % 12d, max   % 12d" %
                         (self.m_sifoil_min, self.m_sifoil_max))
      self.m_logger.info("Wavelength: mu  % 12.6f, sigma % 12.6f" %
                         (avg_wavelength, stddev_wavelength))

    self.m_logger.info("%5d images processed, %5d images skipped" %
                       (self.m_nmemb, self.m_nfail))
    self.m_logger.info("No det-z:       %5d" % self.m_no_detz)
    self.m_logger.info("No attenuation: %5d" % self.m_no_sifoil)
    self.m_logger.info("No wavelength:  %5d" % self.m_no_wavelength)


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_pulnix_dump.py
# -*- mode: python; indent-tabs-mode: nil; python-indent: 2; tab-width: 8 -*-
#
# $Id$

"""XXX"""

from __future__ import absolute_import, division, print_function

import os
import logging

from . import cspad_tbx


class mod_pulnix_dump(object):
  """Class for dumping simple intensity images into an uncompressed
  tarball.  At 120 Hz it is inconvenient to dump every single image to
  its own file.
  """

  def __init__(self,
               address,
               out_dirname=None,
               out_basename=None):
    """The mod_pulnix_dump class constructor stores the parameters passed
    from the pyana configuration file in instance variables.

    @param address      Address string XXX Que?!
    @param out_dirname  Directory portion of output image pathname
    @param out_basename Filename prefix of output image pathname
    """

    self._logger = logging.getLogger(self.__class__.__name__)
    self._logger.setLevel(logging.WARNING)

    self._address = cspad_tbx.getOptString(address)
    self._out_basename = cspad_tbx.getOptString(out_basename)
    self._out_dirname = cspad_tbx.getOptString(out_dirname)


  def beginjob(self, evt, env):
    """The beginjob() function is called at an XTC configure transition.
    It opens per-process tar files to avoid clobbering while
    multiprocessing.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    from collections import deque
    from tarfile import open as opentar

    # The maxlen parameter specifies for how many shots the direction
    # of the principal component must be stable before it can be
    # considered a clear view of the capillary.
    self._angles = deque(maxlen=10)

    if not os.path.isdir(self._out_dirname):
      os.makedirs(self._out_dirname)

    if env.subprocess() >= 0:
      basename = '%sp%02d-r%04d.tar' % (
        self._out_basename, env.subprocess(), evt.run())
    else:
      basename = '%sr%04d.tar' % (
        self._out_basename, evt.run())

    self._tar = opentar(os.path.join(self._out_dirname, basename), 'w')


  def event(self, evt, env):
    """The event() function is called for every L1Accept transition.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    from math import atan2
    from numpy import cov, mean, roll, std
    from numpy.linalg import eig
    from scipy.misc import toimage
    from tempfile import mkstemp

    value = evt.getFrameValue(self._address)
    if value is None:
      self._logger.info("event(): no image data, shot skipped")
      return

    seqno = cspad_tbx.evt_seqno(evt)
    if seqno is None:
      self._logger.warning("event(): no sequence number, shot skipped")
      return

    # Heuristic thresholds to determine whether image is blank or not.
    # XXX Better moved to a separate filtering module?

    # Normalize the image, compute its principal components, and sort
    # them by descending modulus of the corresponding eigenvalue.
    img_norm = (value.data() - mean(value.data())) / std(value.data())
    (w, v) = eig(cov(img_norm))
    if abs(w[0]) < abs(w[1]):
      w = roll(w, 1)
      v = roll(v, 1, 1)

    # For a clear view of the capillary the direction of the principal
    # compenents should be stable over time.  A standard deviation of
    # 0.1 radians corresponds to 5.7 degrees.  Three standard
    # deviations around the mean account for almost 100% of the area
    # under a Gaussian curve.
    #
    # XXX This works perfectly for r0047 and r0210 from cxi78513, but
    # has problems with r0090, r0098, r0099, r0109, r0197, r0267,
    # r0273, r0274, r0281, r0287, and probably others.
    visible = False
    self._angles.append(atan2(v[0, 0], v[0, 1]))
    if len(self._angles) == self._angles.maxlen:
      a_mean = mean(self._angles)
      a_std = std(self._angles)
      if a_std < 0.1 and abs(self._angles[-1] - a_mean) < 3 * a_std:
        visible = True

    if not visible:
      self._logger.info("event(): image blank, shot skipped")
      return

    # Write image to temporary image file.  Add image file to tar
    # archive, and remove temporary file.
    (tmp_fd, tmp_path) = mkstemp()
    tmp_stream = os.fdopen(tmp_fd, 'wb')
    img = toimage(value.data())
    img.save(tmp_stream, format='PNG')
    tmp_stream.close()

    arcname = '%s%s.png' % (self._out_basename, seqno)
    self._tar.add(tmp_path, arcname=arcname)
    os.unlink(tmp_path)

    self._logger.info("event(): archived %s" % arcname)

  #signature for pyana:
  #def endjob(self, env):

  #signature for psana:
  #def endjob(self, evt, env):

  def endjob(self, obj1, obj2=None):
    """The endjob() function closes the tar file opened in beginjob().

    @param evt Event object (psana only)
    @param env Environment object
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    self._tar.close()


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_radial_average.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# $Id: mod_radial_average.py

"""Calculate the radial average of the images in a stream, for each image.
"""
from __future__ import absolute_import, division, print_function

__version__ = "$Revision$"

from xfel.cxi.cspad_ana import common_mode
from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import skip_event_flag
from xfel.cxi.cspad_ana import rayonix_tbx
import os

class mod_radial_average(common_mode.common_mode_correction):
  """Class for calulating radial average of images
  """

  def __init__(self,
               address,
               out_dirname  = None,
               out_basename = None,
               xtal_target  = None,
               two_theta_low = None,
               two_theta_high = None,
               **kwds):
    """The mod_radial_average class constructor stores the parameters passed
    from the pyana configuration file in instance variables.  All
    parameters, except @p address are optional, and hence need not be
    defined in pyana.cfg.

    @param address      Address string XXX Que?!
    @param out_dirname  Optional directory portion of output average pathname
    @param out_basename Optional filename prefix of output average pathname
    @param xtal_target  Phil file with target paramters, including metrology corrections
    @param two_theta_low Optional two theta value of interest
    @param two_theta_high Optional two theta value of interest
    """

    super(mod_radial_average, self).__init__(address=address, **kwds)
    self.m_xtal_target          = cspad_tbx.getOptString(xtal_target)

    self._basename = cspad_tbx.getOptString(out_basename)
    self._dirname = cspad_tbx.getOptString(out_dirname)
    self._two_theta_low = cspad_tbx.getOptFloat(two_theta_low)
    self._two_theta_high = cspad_tbx.getOptFloat(two_theta_high)

    if self._dirname is not None or self._basename is not None:
      assert self._dirname is not None and self._basename is not None

  def beginjob(self, evt, env):
    """The beginjob() function does one-time initialisation from
    event- or environment data.  It is called at an XTC configure
    transition.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    super(mod_radial_average, self).beginjob(evt, env)

  def event(self, evt, env):
    """The event() function is called for every L1Accept transition.
    @param evt Event data object, a configure object
    @param env Environment object
    """

    super(mod_radial_average, self).event(evt, env)
    if (evt.get("skip_event")):
      return

    # This module only applies to detectors for which a distance is
    # available.
    distance = cspad_tbx.env_distance(self.address, env, self._detz_offset)
    if distance is None:
      self.nfail += 1
      self.logger.warning("event(): no distance, shot skipped")
      evt.put(skip_event_flag(), "skip_event")
      return

    # See r17537 of mod_average.py.
    device = cspad_tbx.address_split(self.address)[2]
    if device == 'Cspad':
      pixel_size = cspad_tbx.pixel_size
      saturated_value = cspad_tbx.cspad_saturated_value
    elif device == 'marccd':
      pixel_size = 0.079346
      saturated_value = 2**16 - 1
    elif device == 'Rayonix':
      pixel_size = rayonix_tbx.get_rayonix_pixel_size(self.bin_size)
      saturated_value = rayonix_tbx.rayonix_saturated_value

    d = cspad_tbx.dpack(
      active_areas=self.active_areas,
      address=self.address,
      beam_center_x=pixel_size * self.beam_center[0],
      beam_center_y=pixel_size * self.beam_center[1],
      data=self.cspad_img.iround(), # XXX ouch!
      distance=distance,
      pixel_size=pixel_size,
      saturated_value=saturated_value,
      timestamp=self.timestamp,
      wavelength=self.wavelength,
      xtal_target=self.m_xtal_target)

    from xfel.command_line.radial_average import run
    args = [
      "file_path=XTC stream",
      "xfel_target=%s"%self.m_xtal_target,
      "verbose=False"
    ]

    t = self.timestamp
    s = t[0:4] + t[5:7] + t[8:10] + t[11:13] + t[14:16] + t[17:19] + t[20:23]

    if self._dirname is not None:
      dest_path = os.path.join(self._dirname, self._basename + s + ".txt")
      args.append("output_file=%s"%dest_path)

    self.logger.info("Calculating radial average for image %s"%s)
    xvals, results = run(args, d)

    evt.put(xvals, "cctbx.xfel.radial_average.xvals")
    evt.put(results, "cctbx.xfel.radial_average.results")

    def get_closest_idx(data, val):
      from scitbx.array_family import flex
      deltas = flex.abs(data - val)
      return flex.first_index(deltas, flex.min(deltas))

    if self._two_theta_low is not None:
      i_low = results[get_closest_idx(xvals, self._two_theta_low)]
      evt.put(i_low, "cctbx.xfel.radial_average.two_theta_low")

    if self._two_theta_high is not None:
      i_high = results[get_closest_idx(xvals, self._two_theta_high)]
      evt.put(i_high, "cctbx.xfel.radial_average.two_theta_high")

  #signature for pyana:
  #def endjob(self, env):

  #signature for psana:
  #def endjob(self, evt, env):

  def endjob(self, obj1, obj2=None):
    """The endjob() function logs the number of processed shots.

    @param evt Event object (psana only)
    @param env Environment object
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    super(mod_radial_average, self).endjob(env)
    if (env.subprocess() >= 0):
      self.logger.info("Subprocess %02d: processed %d shots" %
                       (env.subprocess(), self.nshots))
    else:
      self.logger.info("Processed %d shots" % self.nshots)


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_spectrum_filter.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#

''' Filters shots from FEE spectrometer that are two color and flags xtc
    stream events as being a two color event or not.
'''
from __future__ import absolute_import, division, print_function
from six.moves import range
from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import skip_event_flag
import numpy as np
from libtbx import easy_pickle
from psana import *

class mod_spectrum_filter(object):
  def __init__(self,address,
               peak_one_position_min,
               peak_one_position_max,
               peak_two_position_min,
               peak_two_position_max,
               peak_one_width,
               peak_two_width,
               peak_ratio = 0.4,
               normalized_peak_to_noise_ratio = 0.4,
               spectrometer_dark_path = None,
               iron_edge_position = None):
    """The mod_spectrum_filter class constructor stores the parameters passed
    from the psana configuration file in instance variables.

    @param address Full data source address of the FEE device
    @param peak_one_position_min the minimum x coordinate in pixel units
     of the first peak position on the detector
    @param peak_one_position_max the maximum x coordinate in pixel units
     of the first peak position on the detector
    @param peak_two_position_min the minimum x coordinate in pixel units
     of the second peak position on the detector
    @param peak_two_position_max the maximum x coordinate in pixel units
     of the second peak position on the detector
    @param peak_one_width the width in pixels of the first peak
    @param peak_two_width the width in pixels of the second peak
    @param peak_ratio the ratio of the two peak heights
    @param normalized_peak_to_noise_ratio ratio of the normalized integrated
      peak to the normalized integrated regions between the peaks
    @param spectrometer_dark_path path to pickle file of FEE dark
      if None then no dark is subtracted from the spectrum
    @param iron_edge_position the position in pixels of the iron edge if absorbing
      iron foil is used in the experiment if None this is not used as a filtering parameter
    """
    #self.logger = logging.getLogger(self.__class__.__name__)
    #self.logger.setLevel(logging.INFO)
    #self.logging = logging

    self.src = Source('%s'%address)
    if spectrometer_dark_path is not None:
      self.dark = easy_pickle.load(spectrometer_dark_path)
    else:
      self.dark = None
    self.peak_one_position_min = int(peak_one_position_min)
    self.peak_one_position_max = int(peak_one_position_max)
    self.peak_two_position_min = int(peak_two_position_min)
    self.peak_two_position_max = int(peak_two_position_max)
    self.peak_one_width = int(peak_one_width)
    self.peak_two_width = int(peak_two_width)
    self.normalized_peak_to_noise_ratio = float(normalized_peak_to_noise_ratio)
    self.peak_ratio = float(peak_ratio)
    if iron_edge_position is not None:
      self.iron_edge_position = int(iron_edge_position)
    else:
      self.iron_edge_position = None

    self.ntwo_color = 0
    self.nnodata = 0
    self.nshots = 0
    self.naccepted= 0

  def beginjob(self, evt, env):
    pass

  def event(self,evt,evn):
    """The event() function puts a "skip_event" object with value @c
    True into the event if the shot is to be skipped.

    @param evt Event data object, a configure object
    @param env Environment object
    """
    #import pdb; pdb.set_trace()
    if (evt.get("skip_event")):
      return
    # check if FEE data is one or two dimensional
    data = evt.get(Camera.FrameV1, self.src)
    if data is None:
      one_D = True
      data = evt.get(Bld.BldDataSpectrometerV1, self.src)
    else:
      one_D = False
    # get event timestamp
    timestamp = cspad_tbx.evt_timestamp(cspad_tbx.evt_time(evt)) # human readable format

    if data is None:
      self.nnodata +=1
      #self.logger.warning("event(): No spectrum data")
      evt.put(skip_event_flag(),"skip_event")


    if timestamp is None:
      evt.put(skip_event_flag(),"skip_event")
      self.logger.warning("event(): No TIMESTAMP, skipping shot")

    elif data is not None:
      self.nshots +=1
      # get data as array and split into two half to find each peak
      if one_D:
        data = np.array(data.hproj().astype(np.float64))
        if 'dark' in locals():
          data = data - self.dark
        spectrum = data
        spectrum1 = data[:data.shape[0]//2]
        spectrum2 = data[data.shape[0]//2:]
      else:
        data = np.array(data.data16().astype(np.float64))
        if 'dark' in locals():
          data = data - self.dark
        data_split1 = data[:,:data.shape[1]//2]
        data_split2 = data[:,data.shape[1]//2:]
        # make a 1D trace of entire spectrum and each half to find peaks
        spectrum  = np.sum(data,0)/data.shape[0]
        spectrum1 = np.sum(data_split1,0)/data_split1.shape[0]
        spectrum2 = np.sum(data_split2,0)/data_split2.shape[0]

      peak_one = np.max(spectrum1)
      peak_two = np.max(spectrum2)
      peak_one_position = np.argmax(spectrum1)
      peak_two_position = np.argmax(spectrum2) + len(spectrum2)
    # define the limits of the regions between the two peaks
      peak_one_lower_limit = self.peak_one_position_min - self.peak_one_width
      peak_one_upper_limit = self.peak_one_position_max + self.peak_one_width
      peak_two_lower_limit = self.peak_two_position_min - self.peak_two_width
      peak_two_upper_limit = self.peak_two_position_max + self.peak_two_width

      # the x-coordinate of the weighted center of peak region
      weighted_peak_one_positions = []
      for i in range(peak_one_lower_limit,peak_one_upper_limit):
        weighted_peak_one_positions.append(spectrum[i]*i)

      weighted_sum_peak_one = sum(weighted_peak_one_positions)
      weighted_peak_one_center_position = weighted_sum_peak_one//sum(spectrum[peak_one_lower_limit:peak_one_upper_limit])

      weighted_peak_two_positions = []
      for i in range(peak_two_lower_limit,peak_two_upper_limit):
        weighted_peak_two_positions.append(spectrum[i]*i)

      weighted_sum_peak_two = sum(weighted_peak_two_positions)
      weighted_peak_two_center_position = weighted_sum_peak_two//sum(spectrum[peak_two_lower_limit:peak_two_upper_limit])
    # normalized integrated peaks
      int_peak_one = np.sum(spectrum[peak_one_lower_limit:self.peak_one_position_max])/len(spectrum[peak_one_lower_limit:self.peak_one_position_max])
      int_peak_two = np.sum(spectrum[peak_two_lower_limit:self.peak_two_position_max])/len(spectrum[peak_two_lower_limit:self.peak_two_position_max])
    # normalized integrated regions between the peaks
      int_left_region = np.sum(spectrum[0:peak_one_lower_limit])/len(spectrum[0:peak_one_lower_limit])
      int_middle_region = np.sum(spectrum[peak_one_upper_limit:peak_two_lower_limit])/len(spectrum[peak_one_upper_limit:peak_two_lower_limit])
      int_right_region = np.sum(spectrum[peak_two_upper_limit:])/len(spectrum[:peak_two_upper_limit])
      # now to do the filtering
      if peak_one/peak_two < self.peak_ratio or peak_one/peak_two > 1/self.peak_ratio:
        print("event(): too low")
        evt.put(skip_event_flag(), "skip_event")
        return
      if weighted_peak_two_center_position < self.peak_two_position_min or weighted_peak_two_center_position > self.peak_two_position_max:
        print("event(): out of range high energy peak")
        evt.put(skip_event_flag(), "skip_event")
        return
      if weighted_peak_one_center_position < self.peak_one_position_min or weighted_peak_one_center_position > self.peak_one_position_max:
        print("event(): out of range low energy peak")
        evt.put(skip_event_flag(), "skip_event")
        return
      if not one_D and (int_left_region/int_peak_one > self.normalized_peak_to_noise_ratio):
        print("event(): noisy left of low energy peak")
        evt.put(skip_event_flag(), "skip_event")
        return
      if not one_D and (int_middle_region/int_peak_one > self.normalized_peak_to_noise_ratio):
        print("event(): noisy middle")
        evt.put(skip_event_flag(), "skip_event")
        return
      if not one_D and (int_middle_region/int_peak_two > self.normalized_peak_to_noise_ratio):
        print("event(): noisy middle")
        evt.put(skip_event_flag(), "skip_event")
        return
      if not one_D and (int_right_region/int_peak_two > self.normalized_peak_to_noise_ratio):
        print("event(): noisy right of high energy peak")
        evt.put(skip_event_flag(), "skip_event")
        return
      #iron edge at 738 pixels on FFE detetor
      if one_D and (spectrum[self.iron_edge_position]>=spectrum[weighted_peak_one_center_position]):
        print("event(): peak at iron edge")
        evt.put(skip_event_flag(), "skip_event")
        return

      if one_D and (spectrum[self.iron_edge_position]>=spectrum[weighted_peak_two_center_position]):
        print("event(): peak at iron edge")
        evt.put(skip_event_flag(), "skip_event")
        return

      #self.logger.info("TIMESTAMP %s accepted" %timestamp)
      self.naccepted += 1
      self.ntwo_color += 1
      print("%d Two Color shots"  %self.ntwo_color)

  def endjob(self, obj1, obj2=None):
    """
    @param evt Event object (psana only)
    @param env Environment object
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    #self.logger.info("Saw %d shots, two_color %d, nodata %d " % (self.nshots, self.naccepted, self.nnodata))

  #def __del__(self):
    #logging.shutdown()


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_spectrum_filter_remote_only.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#

''' Filters shots from FEE spectrometer that are two color and flags xtc
    stream events as being a two color event or not.
'''
from __future__ import absolute_import, division, print_function
from six.moves import range
from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import skip_event_flag
import numpy as np
from libtbx import easy_pickle
from psana import *

class mod_spectrum_filter_remote_only(object):
  def __init__(self,
               address,
               peak_one_range_min,
               peak_one_range_max,
               peak_two_range_min,
               peak_two_range_max,
               peak_ratio = 0.4,
               normalized_peak_to_noise_ratio = 0.4,
               spectrometer_dark_path = None,
               metal_edge_px_position = None,
               metal_edge_eV = None,
               forbidden_range_eV=20):
    """The mod_spectrum_filter class constructor stores the parameters passed
    from the psana configuration file in instance variables.

    @param address Full data source address of the FEE device
    @param peak_one_range_min the minimum x coordinate in pixel units
     of the first peak range on the detector
    @param peak_one_range_max the maximum x coordinate in pixel units
     of the first peak range on the detector
    @param peak_two_range_min the minimum x coordinate in pixel units
     of the second peak range on the detector
    @param peak_two_range_max the maximum x coordinate in pixel units
     of the second peak range on the detector
    @param peak_ratio the ratio of the two peak heights
    @param normalized_peak_to_noise_ratio ratio of the normalized integrated
      peak to the normalized integrated regions between the peaks
    @param spectrometer_dark_path path to pickle file of FEE dark
      if None then no dark is subtracted from the spectrum
    @param metal_edge_position the position in pixels of the metal edge
      if used to absorb photons in experiment if set to None it is not used as a filtering parameter
    @param percent_eV_range range in percent of metal edge region
      used as +/- a percent of the given metal edge if set to None is is not used as a filtering parameter
    """
    #self.logger = logging.getLogger(self.__class__.__name__)
    #self.logger.setLevel(logging.INFO)
    #self.logging = logging

    self.src = Source('%s'%address)
    if spectrometer_dark_path is not None:
      self.dark = easy_pickle.load(spectrometer_dark_path)
    else:
      self.dark = None
    self.peak_one_range_min = int(peak_one_range_min)
    self.peak_one_range_max = int(peak_one_range_max)
    self.peak_two_range_min = int(peak_two_range_min)
    self.peak_two_range_max = int(peak_two_range_max)
    self.normalized_peak_to_noise_ratio = float(normalized_peak_to_noise_ratio)
    self.peak_ratio = float(peak_ratio)
    self.forbidden_range_eV = int(forbidden_range_eV)
    if metal_edge_px_position is not None:
      self.metal_edge_px_position = int(metal_edge_px_position)
    else:
      self.metal_edge_px_position = None

    if metal_edge_eV is not None:
      self.metal_edge_eV = int(metal_edge_eV)
    else:
      self.metal_edge_eV = None
    self.ntwo_color = 0
    self.nnodata = 0
    self.nshots = 0
    self.naccepted= 0

  def beginjob(self, evt, env):
    pass

  def event(self,evt,evn):
    """The event() function puts a "skip_event" object with value @c
    True into the event if the shot is to be skipped.

    @param evt Event data object, a configure object
    @param env Environment object
    """
    #import pdb; pdb.set_trace()
    if (evt.get("skip_event")):
      return
    # check if FEE data is one or two dimensional
    data = evt.get(Camera.FrameV1, self.src)
    if data is None:
      one_D = True
      data = evt.get(Bld.BldDataSpectrometerV1, self.src)
    else:
      one_D = False
    # get event timestamp
    timestamp = cspad_tbx.evt_timestamp(cspad_tbx.evt_time(evt)) # human readable format

    if data is None:
      self.nnodata +=1
      #self.logger.warning("event(): No spectrum data")
      evt.put(skip_event_flag(),"skip_event")

    if timestamp is None:
      evt.put(skip_event_flag(),"skip_event")
      #self.logger.warning("event(): No TIMESTAMP, skipping shot")

    elif data is not None:
      self.nshots +=1
      # get data as array and split into two half to find each peak
      if one_D:
        # filtering out outlier spikes in FEE data
        data = np.array(data.hproj().astype(np.float64))
        for i in range(len(data)):
          if data[i]>1000000000:
            data[i]=data[i]-(2**32)
        if self.dark is not None:
          data = data - self.dark
        spectrum = data
        spectrum1 = data[:data.shape[0]//2]
        spectrum2 = data[data.shape[0]//2:]
      else:
        data = np.array(data.data16().astype(np.int32))
        if self.dark is not None:
          data = data - self.dark
        data = np.double(data)
        data_split1 = data[:,:data.shape[1]//2]
        data_split2 = data[:,data.shape[1]//2:]
        # make a 1D trace of entire spectrum and each half to find peaks
        spectrum  = np.sum(data,0)/data.shape[0]
        spectrum1 = np.sum(data_split1,0)/data_split1.shape[0]
        spectrum2 = np.sum(data_split2,0)/data_split2.shape[0]
      if not one_D:
        # the x-coordinate of the weighted center of peak region
        weighted_peak_one_positions = []
        for i in range(self.peak_one_range_min,self.peak_one_range_max):
          weighted_peak_one_positions.append(spectrum[i]*i)
        weighted_sum_peak_one = np.sum(weighted_peak_one_positions)
        weighted_peak_one_center_position = weighted_sum_peak_one/np.sum(spectrum[self.peak_one_range_min:self.peak_one_range_max])

        weighted_peak_two_positions = []
        for i in range(self.peak_two_range_min,self.peak_two_range_max):
          weighted_peak_two_positions.append(spectrum[i]*i)
        weighted_sum_peak_two = np.sum(weighted_peak_two_positions)
        weighted_peak_two_center_position = weighted_sum_peak_two/np.sum(spectrum[self.peak_two_range_min:self.peak_two_range_max])

        # normalized integrated regions between the peaks
        #int_left_region = np.sum(spectrum[weighted_peak_one_center_position+len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2:(weighted_peak_two_center_position-len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2)])
        int_left_region = np.sum(spectrum[:weighted_peak_two_center_position/2])

        #int_left_region_norm = np.sum(spectrum[weighted_peak_one_center_position+len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2:(weighted_peak_two_center_position-len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2)])/len(spectrum[weighted_peak_one_center_position+len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2:(weighted_peak_two_center_position-len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2)])
        int_left_region_norm = np.sum(spectrum[:weighted_peak_two_center_position/2])/len(spectrum[:weighted_peak_two_center_position/2])

        int_right_region = np.sum(spectrum[self.peak_two_range_max:])

        int_right_region_norm = np.sum(spectrum[self.peak_two_range_max:])/len(spectrum[self.peak_two_range_max:])

        # normalized integrated peaks
        int_peak_one = np.sum(spectrum[(weighted_peak_one_center_position-len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2):(weighted_peak_one_center_position+len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2)])

        int_peak_one_norm = np.sum(spectrum[(weighted_peak_one_center_position-len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2):(weighted_peak_one_center_position+len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2)])/len(spectrum[(weighted_peak_one_center_position-len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2):(weighted_peak_one_center_position+len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2)])

        int_peak_two = np.sum(spectrum[(weighted_peak_two_center_position-len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2):(weighted_peak_two_center_position+len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2)])

        int_peak_two_norm = np.sum(spectrum[(weighted_peak_two_center_position-len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2):(weighted_peak_two_center_position+len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2)])/len(spectrum[(weighted_peak_two_center_position-len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2):(weighted_peak_two_center_position+len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2)])

      if not one_D:
        if int_peak_one_norm/int_peak_two_norm > self.peak_ratio:
          print("event(): inflection peak too high")
          evt.put(skip_event_flag(), "skip_event")
          return
        if int_left_region_norm > self.normalized_peak_to_noise_ratio*int_peak_two_norm:
          print("event(): noisy left of low energy peak")
          evt.put(skip_event_flag(), "skip_event")
          return
        if int_right_region_norm > self.normalized_peak_to_noise_ratio*int_peak_two_norm:
          print("event(): noisy right of high energy peak")
          evt.put(skip_event_flag(), "skip_event")
          return
      #self.logger.info("TIMESTAMP %s accepted" %timestamp)
      self.naccepted += 1
      self.ntwo_color += 1
      print("%d Remote shot"  %self.ntwo_color)
      print("%s Remote timestamp" %timestamp)
  def endjob(self, obj1, obj2=None):
    """
    @param evt Event object (psana only)
    @param env Environment object
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    #self.logger.info("Saw %d shots, two_color %d, nodata %d " % (self.nshots, self.naccepted, self.nnodata))

  #def __del__(self):
    #logging.shutdown()


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_spectrum_filter_v2.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#

''' Filters shots from FEE spectrometer that are two color and flags xtc
    stream events as being a two color event or not.
'''
from __future__ import absolute_import, division, print_function
from six.moves import range
from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import skip_event_flag
import numpy as np
from libtbx import easy_pickle
from psana import *

class mod_spectrum_filter_v2(object):
  def __init__(self,
               address,
               peak_one_range_min,
               peak_one_range_max,
               peak_two_range_min,
               peak_two_range_max,
               peak_ratio = 0.4,
               normalized_peak_to_noise_ratio = 0.4,
               spectrometer_dark_path = None,
               metal_edge_px_position = None,
               metal_edge_eV = None,
               forbidden_range_eV=20):
    """The mod_spectrum_filter class constructor stores the parameters passed
    from the psana configuration file in instance variables.

    @param address Full data source address of the FEE device
    @param peak_one_range_min the minimum x coordinate in pixel units
     of the first peak range on the detector
    @param peak_one_range_max the maximum x coordinate in pixel units
     of the first peak range on the detector
    @param peak_two_range_min the minimum x coordinate in pixel units
     of the second peak range on the detector
    @param peak_two_range_max the maximum x coordinate in pixel units
     of the second peak range on the detector
    @param peak_ratio the ratio of the two peak heights
    @param normalized_peak_to_noise_ratio ratio of the normalized integrated
      peak to the normalized integrated regions between the peaks
    @param spectrometer_dark_path path to pickle file of FEE dark
      if None then no dark is subtracted from the spectrum
    @param metal_edge_position the position in pixels of the metal edge
      if used to absorb photons in experiment if set to None it is not used as a filtering parameter
    @param percent_eV_range range in percent of metal edge region
      used as +/- a percent of the given metal edge if set to None is is not used as a filtering parameter
    """
    #self.logger = logging.getLogger(self.__class__.__name__)
    #self.logger.setLevel(logging.INFO)
    #self.logging = logging

    self.src = Source('%s'%address)
    if spectrometer_dark_path is not None:
      self.dark = easy_pickle.load(spectrometer_dark_path)
    else:
      self.dark = None
    self.peak_one_range_min = int(peak_one_range_min)
    self.peak_one_range_max = int(peak_one_range_max)
    self.peak_two_range_min = int(peak_two_range_min)
    self.peak_two_range_max = int(peak_two_range_max)
    self.normalized_peak_to_noise_ratio = float(normalized_peak_to_noise_ratio)
    self.peak_ratio = float(peak_ratio)
    self.forbidden_range_eV = int(forbidden_range_eV)
    if metal_edge_px_position is not None:
      self.metal_edge_px_position = int(metal_edge_px_position)
    else:
      self.metal_edge_px_position = None

    if metal_edge_eV is not None:
      self.metal_edge_eV = int(metal_edge_eV)
    else:
      self.metal_edge_eV = None
    self.ntwo_color = 0
    self.nnodata = 0
    self.nshots = 0
    self.naccepted= 0

  def beginjob(self, evt, env):
    pass

  def event(self,evt,evn):
    """The event() function puts a "skip_event" object with value @c
    True into the event if the shot is to be skipped.

    @param evt Event data object, a configure object
    @param env Environment object
    """
    #import pdb; pdb.set_trace()
    if (evt.get("skip_event")):
      return
    # check if FEE data is one or two dimensional
    data = evt.get(Camera.FrameV1, self.src)
    if data is None:
      one_D = True
      data = evt.get(Bld.BldDataSpectrometerV1, self.src)
    else:
      one_D = False
    # get event timestamp
    timestamp = cspad_tbx.evt_timestamp(cspad_tbx.evt_time(evt)) # human readable format

    if data is None:
      self.nnodata +=1
      #self.logger.warning("event(): No spectrum data")
      evt.put(skip_event_flag(),"skip_event")

    if timestamp is None:
      evt.put(skip_event_flag(),"skip_event")
      #self.logger.warning("event(): No TIMESTAMP, skipping shot")

    elif data is not None:
      self.nshots +=1
      # get data as array and split into two half to find each peak
      if one_D:
        # filtering out outlier spikes in FEE data
        data = np.array(data.hproj().astype(np.float64))
        for i in range(len(data)):
          if data[i]>1000000000:
            data[i]=data[i]-(2**32)
        if self.dark is not None:
          data = data - self.dark
        spectrum = data
        spectrum1 = data[:data.shape[0]//2]
        spectrum2 = data[data.shape[0]//2:]
      else:
        data = np.array(data.data16().astype(np.int32))
        if self.dark is not None:
          data = data - self.dark
        data_split1 = data[:,:data.shape[1]//2]
        data_split2 = data[:,data.shape[1]//2:]
        # make a 1D trace of entire spectrum and each half to find peaks
        spectrum  = np.sum(data,0)/data.shape[0]
        spectrum1 = np.sum(data_split1,0)/data_split1.shape[0]
        spectrum2 = np.sum(data_split2,0)/data_split2.shape[0]
      if not one_D:
        # the x-coordinate of the weighted center of peak region
        weighted_peak_one_positions = []
        for i in range(self.peak_one_range_min,self.peak_one_range_max):
          weighted_peak_one_positions.append(spectrum[i]*i)
        weighted_sum_peak_one = sum(weighted_peak_one_positions)
        weighted_peak_one_center_position = weighted_sum_peak_one//sum(spectrum[self.peak_one_range_min:self.peak_one_range_max])
        weighted_peak_two_positions = []
        for i in range(self.peak_two_range_min,self.peak_two_range_max):
          weighted_peak_two_positions.append(spectrum[i]*i)
        weighted_sum_peak_two = sum(weighted_peak_two_positions)
        weighted_peak_two_center_position = weighted_sum_peak_two//sum(spectrum[self.peak_two_range_min:self.peak_two_range_max])
        # normalized integrated regions between the peaks
        int_left_region_norm = np.sum(spectrum[0:(weighted_peak_one_center_position-len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2)])/len(spectrum[0:(weighted_peak_one_center_position-len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2)])

        int_middle_region_norm = np.sum(spectrum[(weighted_peak_one_center_position+len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2):(weighted_peak_two_center_position-len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2)])/len(spectrum[(weighted_peak_one_center_position+len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2):(weighted_peak_two_center_position-len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2)])

        int_right_region_norm = np.sum(spectrum[(weighted_peak_two_center_position+len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2):])/len(spectrum[(weighted_peak_two_center_position+len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2):])

        # normalized integrated peaks
        int_peak_one_norm = np.sum(spectrum[(weighted_peak_one_center_position-len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2):(weighted_peak_one_center_position+len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2)])/len(spectrum[(weighted_peak_one_center_position-len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2):(weighted_peak_one_center_position+len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2)])

        int_peak_two_norm = np.sum(spectrum[(weighted_peak_two_center_position-len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2):(weighted_peak_two_center_position+len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2)])/len(spectrum[(weighted_peak_two_center_position-len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2):(weighted_peak_two_center_position+len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2)])

      else:
        # convert eV range of iron edge (7112 eV) to pixels:
        metal_edge_max=(self.forbidden_range_eV/2.)/0.059+738
        metal_edge_min=(-self.forbidden_range_eV/2.)/0.059+738
        int_metal_region = np.sum(spectrum[metal_edge_min:metal_edge_max])
        #peak one region integrate:
        int_peak_one=np.sum(spectrum[0:metal_edge_min])
        int_peak_two=np.sum(spectrum[metal_edge_max:])
      # now to do the filtering
      if not one_D:
        if min(int_peak_one_norm,int_peak_two_norm)/max(int_peak_one_norm,int_peak_two_norm) < self.peak_ratio:
          print("event(): too low")
          evt.put(skip_event_flag(), "skip_event")
          return
        if (np.argmax(spectrum2)+len(spectrum2)) > (weighted_peak_two_center_position+(len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2)) or (np.argmax(spectrum2)+len(spectrum2)) < (weighted_peak_two_center_position-(len(spectrum[self.peak_two_range_min:self.peak_two_range_max])/2)):
          print("event(): out of range high energy peak")
          evt.put(skip_event_flag(), "skip_event")
          return
        if np.argmax(spectrum1) > (weighted_peak_one_center_position+(len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2)) or np.argmax(spectrum1) < (weighted_peak_one_center_position-(len(spectrum[self.peak_one_range_min:self.peak_one_range_max])/2)):
          print("event(): out of range low energy peak")
          evt.put(skip_event_flag(), "skip_event")
          return
        if int_left_region_norm/int_peak_one_norm > self.normalized_peak_to_noise_ratio:
          print("event(): noisy left of low energy peak")
          evt.put(skip_event_flag(), "skip_event")
          return
        if int_middle_region_norm/int_peak_one_norm > self.normalized_peak_to_noise_ratio:
          print("event(): noisy middle")
          evt.put(skip_event_flag(), "skip_event")
          return
        if int_middle_region_norm/int_peak_one_norm > self.normalized_peak_to_noise_ratio:
          print("event(): noisy middle")
          evt.put(skip_event_flag(), "skip_event")
          return
        if int_right_region_norm/int_peak_two_norm > self.normalized_peak_to_noise_ratio:
          print("event(): noisy right of high energy peak")
          evt.put(skip_event_flag(), "skip_event")
          return
      else:
      # filter for cxih8015
      #iron edge at 738 pixels on FFE detetor
        if int_metal_region>=0.10*int_peak_two:
          print("event(): high intensity at metal edge")
          evt.put(skip_event_flag(), "skip_event")
          return
        if int_metal_region>=0.10*int_peak_one:
          print("event(): high intensity at metal edge")
          evt.put(skip_event_flag(), "skip_event")
          return
        if min(int_peak_one,int_peak_two)/max(int_peak_one,int_peak_two) < self.peak_ratio:
          print("event(): peak ratio too low")
          evt.put(skip_event_flag(), "skip_event")
          return
      #self.logger.info("TIMESTAMP %s accepted" %timestamp)
      self.naccepted += 1
      self.ntwo_color += 1
      print("%d Two Color shots"  %self.ntwo_color)

  def endjob(self, obj1, obj2=None):
    """
    @param evt Event object (psana only)
    @param env Environment object
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    #self.logger.info("Saw %d shots, two_color %d, nodata %d " % (self.nshots, self.naccepted, self.nnodata))

  #def __del__(self):
    #logging.shutdown()


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_view.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# $Id$
"""Interactive image viewer for CSPad images

XXX Known issues, wishlist:

  * Is it slow?  Yes!

  * Radial distribution plot, requested by Jan F. Kern

  * Coordinate and resolution tool-tips in sub-pixel zoom.  Can we
    choose the colour automagically?
"""
from __future__ import absolute_import, division, print_function

__version__ = "$Revision$"

import multiprocessing
import thread
import threading
import time

from dxtbx.format.FormatPYunspecified import FormatPYunspecified
from iotbx.detectors.npy import NpyImage
from rstbx.viewer.frame import XrayFrame
from xfel.cxi.cspad_ana import common_mode, cspad_tbx
from xfel.cxi.cspad_ana import skip_event_flag

class _ImgDict(NpyImage):
  """Minimal iotbx detector class for in-memory dictionary
  representation of NpyImage images.  This class must be defined at
  the top level of the module so that it can be pickled.
  """

  def __init__(self, data, parameters):
    # self.vendortype is required to guess the beam centre convention.

    super(_ImgDict, self).__init__('')
    self.parameters = parameters
    self.vendortype = 'npy_raw'
    self.bin_safe_set_data(data)

  def readHeader(self):
    pass

  def read(self):
    pass


class _Format(FormatPYunspecified):
  """Minimal Format class for in-memory dxtbx representation of
  FormatPYunspecified images.  This class must be defined at the top
  level of the module so that it can be pickled.
  """

  def __init__(self, **kwargs):
    """The _Format constructor builds a dxtbx Format instance from the
    supplied keyworded arguments.  It should be equivalent to the
    FormatPYunspecified constructor.
    """

    from copy import deepcopy

    from dxtbx.model.beam import Beam, BeamFactory
    from dxtbx.model.detector import Detector, DetectorFactory
    from dxtbx.model.goniometer import Goniometer, GoniometerFactory
    from dxtbx.model import Scan, ScanFactory

    from spotfinder.applications.xfel import cxi_phil
    from iotbx.detectors.cspad_detector_formats import detector_format_version

    # From Format.__init__().
    self._goniometer_factory = GoniometerFactory
    self._detector_factory = DetectorFactory
    self._beam_factory = BeamFactory
    self._scan_factory = ScanFactory

    # From FormatPYunspecified._start().  Split the keyworded
    # arguments into a parameter dictionary suitable for
    # iotbx.detectors.npy.NpyImage and a separate image data object.
    parameters = dict(
      BEAM_CENTER_X=kwargs['PIXEL_SIZE'] * kwargs['BEAM_CENTER'][0],
      BEAM_CENTER_Y=kwargs['PIXEL_SIZE'] * kwargs['BEAM_CENTER'][1],
      CCD_IMAGE_SATURATION=kwargs['SATURATED_VALUE'],
      DISTANCE=kwargs['DISTANCE'],
      OSC_RANGE=0,
      OSC_START=0,
      PIXEL_SIZE=kwargs['PIXEL_SIZE'],
      SATURATED_VALUE=kwargs['SATURATED_VALUE'],
      SIZE1=kwargs['DATA'].focus()[0],
      SIZE2=kwargs['DATA'].focus()[1],
      WAVELENGTH=kwargs['WAVELENGTH'])
    self.detectorbase = _ImgDict(kwargs['DATA'], parameters)

    # Attempt to apply tile translations only for detectors that
    # support it.
    version_lookup = detector_format_version(
      kwargs['DETECTOR_ADDRESS'],
      kwargs['TIME_TUPLE'][0])
    if version_lookup is not None:
      params = cxi_phil.cxi_versioned_extract(
        "distl.detector_format_version=" + version_lookup)

      # Necessary to keep the phil parameters for subsequent calls to
      # get_tile_manager().
      horizons_phil = params.persist.commands

      self.detectorbase.translate_tiles(horizons_phil)
      self.detectorbase.horizons_phil_cache = deepcopy(horizons_phil)

    # From Format.setup().
    goniometer_instance = self._goniometer()
    assert(isinstance(goniometer_instance, Goniometer))
    self._goniometer_instance = goniometer_instance

    detector_instance = self._detector()
    assert(isinstance(detector_instance, Detector))
    self._detector_instance = detector_instance

    beam_instance = self._beam()
    assert(isinstance(beam_instance, Beam))
    self._beam_instance = beam_instance

    scan_instance = self._scan()
    assert(isinstance(scan_instance, Scan))
    self._scan_instance = scan_instance


class _XrayFrameThread(threading.Thread):
  """The _XrayFrameThread class allows MainLoop() to be run as a
  thread, which is necessary because all calls to wxPython must be
  made from the same thread that originally imported wxPython.

  This is all based on "Running MainLoop in a separate thread",
  http://wiki.wxpython.org/MainLoopAsThread.
  """

  def __init__(self):
    """The thread is started automatically on initialisation.
    self.run() will initialise self.frame and release self._init_lock.
    """

    super(_XrayFrameThread, self).__init__()
    self.setDaemon(1)
    self._init_lock = threading.Lock()
    self._next_semaphore = threading.Semaphore()
    self._start_orig = self.start
    self._frame = None
    self.start = self._start_local

    self._init_lock.acquire()
    self.start()


  def _start_local(self):
    """The _start_local() function calls the run() function through
    self._start_orig, and exists only after self._init_lock has been
    released.  This eliminates a race condition which could cause
    updates to be sent to a non-existent frame.
    """

    self._start_orig()
    self._init_lock.acquire()


  def run(self):
    """The run() function defines the frame and starts the main loop.
    self._init_lock is released only when all initialisation is done.

    Whatever thread is the current one when wxWindows is initialised
    is what it will consider the "main thread." For wxPython 2.4 that
    happens when wxPython.wx is imported the first time.  For 2.5 it
    will be when the wx.App object is created.
    """

    import wx
    from wxtbx import bitmaps
    app = wx.App(0)
    self._bitmap_pause = bitmaps.fetch_icon_bitmap('actions', 'stop')
    self._bitmap_run = bitmaps.fetch_icon_bitmap('actions', 'runit')
    self._frame = XrayFrame(None, -1, "X-ray image display", size=(800, 720))

    self._frame.Bind(wx.EVT_IDLE, self.OnIdle)

    self.setup_toolbar(self._frame.toolbar)
    self._frame.Show()

    self._init_lock.release()
    app.MainLoop()

    # Avoid deadlock where the send_data() function is waiting for the
    # semaphore after the frame has closed.
    self._next_semaphore.release()


  def send_data(self, img, title):
    """The send_data() function updates the wxPython application with
    @p img and @p title by sending it an ExternalUpdateEvent().  The
    function blocks until the event is processed."""

    from rstbx.viewer.frame import ExternalUpdateEvent

    event = ExternalUpdateEvent()
    event.img = img
    event.title = title
    if self.isAlive():
      try:
        # Saturating the event queue makes the whole caboodle
        # uselessly unresponsive.  Therefore, block until idle events
        # are processed.
        while self.isAlive() and not self._run_pause.IsToggled():
          pass
        self._frame.AddPendingEvent(event)
        self._is_idle = False
        while self.isAlive() and not self._is_idle:
          pass
      except Exception:
        pass


  def setup_toolbar(self, toolbar):
    import wx
    from wxtbx import icons

    toolbar.ClearTools()

    btn = toolbar.AddLabelTool(
      id=wx.ID_ANY,
      label="Settings",
      bitmap=icons.advancedsettings.GetBitmap(),
      shortHelp="Settings",
      kind=wx.ITEM_NORMAL)
    self._frame.Bind(wx.EVT_MENU, self._frame.OnShowSettings, btn)

    btn = toolbar.AddLabelTool(
      id=wx.ID_ANY,
      label="Zoom",
      bitmap=icons.search.GetBitmap(),
      shortHelp="Zoom",
      kind=wx.ITEM_NORMAL)
    self._frame.Bind(wx.EVT_MENU, self._frame.OnZoom, btn)

    # Reset the normal bitmap after the tool has been created, so that
    # it will update on the next event.  See also OnPauseRun()
    self._run_pause = toolbar.AddCheckLabelTool(
      id=wx.ID_ANY,
      label="Run/Pause",
      bitmap=self._bitmap_run,
      shortHelp="Run/Pause")
    self._run_pause.SetNormalBitmap(self._bitmap_pause)
    self._frame.Bind(wx.EVT_MENU, self.OnPauseRun, self._run_pause)


  def OnIdle(self, event):
    self._is_idle = True
    event.RequestMore()


  def OnPauseRun(self, event):
    if self._run_pause.IsToggled():
      self._run_pause.SetNormalBitmap(self._bitmap_run)
    else:
      self._run_pause.SetNormalBitmap(self._bitmap_pause)


  def stop(self):
    from wx import CloseEvent
    self._frame.AddPendingEvent(CloseEvent())


def _xray_frame_process(queue, linger=True, wait=None):
  """The _xray_frame_process() function starts the viewer in a
  separate thread.  It then continuously reads data from @p queue and
  dispatches update events to the viewer.  The function returns when
  it reads a @c None object from @p queue or when the viewer thread
  has exited.
  """

  from Queue import Empty
  import rstbx.viewer

  # Start the viewer's main loop in its own thread, and get the
  # interface for sending updates to the frame.
  thread = _XrayFrameThread()
  send_data = thread.send_data

  while True:
    try:
      payload = queue.get(timeout=1)

      if payload is None:
        if linger:
          thread.join()
        else:
          thread.stop()
        return

      if not thread.isAlive():
        thread.join()
        return

      if wait is not None:
        time.sleep(wait)

      # All kinds of exceptions--not just PyDeadObjectError--may occur
      # if the viewer process exits during this call.  XXX This may be
      # dangerous!
      try:
        send_data(rstbx.viewer.image(payload[0]), payload[1])
      except Exception:
        pass
    except Empty:
      pass


class mod_view(common_mode.common_mode_correction):
  """XXX
  """

  def __init__(self,
               address,
               n_collate   = None,
               n_update    = 120,
               common_mode_correction = "none",
               wait=None,
               photon_counting=False,
               sigma_scaling=False,
               **kwds):
    """The mod_view class constructor XXX.

    @param address         Full data source address of the DAQ device
    @param calib_dir       Directory with calibration information
    @param common_mode_correction The type of common mode correction to apply
    @param dark_path       Path to input average dark image
    @param dark_stddev     Path to input standard deviation dark
                           image, required if @p dark_path is given
    @param wait            Minimum time (in seconds) to wait on the current
                           image before moving on to the next
    @param n_collate       Number of shots to average, or <= 0 to
                           average all shots
    @param n_update        Number of shots between updates
    """

    super(mod_view, self).__init__(
      address=address,
      common_mode_correction=common_mode_correction,
      **kwds)

    self.detector = cspad_tbx.address_split(address)[0]
    self.nvalid   = 0
    self.ncollate = cspad_tbx.getOptInteger(n_collate)
    self.nupdate  = cspad_tbx.getOptInteger(n_update)
    self.photon_counting = cspad_tbx.getOptBool(photon_counting)
    self.sigma_scaling = cspad_tbx.getOptBool(sigma_scaling)
    if (self.ncollate is None):
      self.ncollate = self.nupdate
    if (self.ncollate > self.nupdate):
      self.ncollate = self.nupdate
      self.logger.warning("n_collate capped to %d" % self.nupdate)

    linger = True # XXX Make configurable
    wait = cspad_tbx.getOptFloat(wait)

    # Create a managed FIFO queue shared between the viewer and the
    # current process.  The current process will produce images, while
    # the viewer process will consume them.
    manager = multiprocessing.Manager()
    self._queue = manager.Queue()
    self._proc = multiprocessing.Process(
      target=_xray_frame_process, args=(self._queue, linger, wait))
    self._proc.start()

    self.n_shots = 0


  def event(self, evt, env):
    """The event() function is called for every L1Accept transition.
    XXX Since the viewer is now running in a parallel process, the
    averaging here is now the bottleneck.

    @param evt Event data object, a configure object
    @param env Environment object
    """
    from pyana.event import Event

    self.n_shots += 1

    super(mod_view, self).event(evt, env)
    if evt.status() != Event.Normal or evt.get('skip_event'): # XXX transition
      return

    # Get the distance for the detectors that should have it, and set
    # it to NaN for those that should not.
    if self.detector == 'CxiDs1' or \
       self.detector == 'CxiDsd' or \
       self.detector == 'XppGon':
      distance = cspad_tbx.env_distance(self.address, env, self._detz_offset)
      if distance is None:
        self.nfail += 1
        self.logger.warning("event(): no distance, shot skipped")
        evt.put(skip_event_flag(), "skip_event")
        return
    else:
      distance = float('nan')

    if not self._proc.is_alive():
      evt.setStatus(Event.Stop)

    # Early return if the next update to the viewer is more than
    # self.ncollate shots away.  XXX Since the common_mode.event()
    # function does quite a bit of processing, the savings are
    # probably not so big.
    next_update = (self.nupdate - 1) - (self.nshots - 1) % self.nupdate
    if (self.ncollate > 0 and next_update >= self.ncollate):
      return

    if self.sigma_scaling:
      self.do_sigma_scaling()

    if self.photon_counting:
      self.do_photon_counting()

    # Trim the disabled section from the Sc1 detector image.  XXX This
    # is a bit of a kludge, really.
#    if (self.address == "CxiSc1-0|Cspad2x2-0"):
#      self.cspad_img = self.cspad_img[185:2 * 185, :]

    # Update the sum of the valid images, starting a new collation if
    # appropriate.  This guarantees self.nvalid > 0.
    if (self.nvalid == 0 or self.ncollate > 0 and self.nvalid >= self.ncollate):
      self.img_sum = self.cspad_img
      self.nvalid  = 1
    else:
      self.img_sum += self.cspad_img
      self.nvalid  += 1

    # Update the viewer to display the current average image, and
    # start a new collation, if appropriate.
    if (next_update == 0):
      from time import localtime, strftime

      time_str = strftime("%H:%M:%S", localtime(evt.getTime().seconds()))
      title = "r%04d@%s: average of %d last images on %s" \
          % (evt.run(), time_str, self.nvalid, self.address)

      # See also mod_average.py.
      device = cspad_tbx.address_split(self.address)[2]
      if device == 'Cspad':
        beam_center = self.beam_center
        pixel_size = cspad_tbx.pixel_size
        saturated_value = cspad_tbx.cspad_saturated_value

      elif device == 'marccd':
        beam_center = tuple(t // 2 for t in self.img_sum.focus())
        pixel_size = 0.079346
        saturated_value = 2**16 - 1

      # Wait for the viewer process to empty the queue before feeding
      # it a new image, and ensure not to hang if the viewer process
      # exits.  Because of multithreading/multiprocessing semantics,
      # self._queue.empty() is unreliable.
      fmt = _Format(BEAM_CENTER=beam_center,
                      DATA=self.img_sum / self.nvalid,
                      DETECTOR_ADDRESS=self.address,
                      DISTANCE=distance,
                      PIXEL_SIZE=pixel_size,
                      SATURATED_VALUE=saturated_value,
                      TIME_TUPLE=cspad_tbx.evt_time(evt),
                      WAVELENGTH=self.wavelength)

      while not self._queue.empty():
        if not self._proc.is_alive():
          evt.setStatus(Event.Stop)
          return
      while True:
        try:
          self._queue.put((fmt, title), timeout=1)
          break
        except Exception:
          pass

      if (self.ncollate > 0):
        self.nvalid = 0

  #signature for pyana:
  #def endjob(self, env):

  #signature for psana:
  #def endjob(self, evt, env):

  def endjob(self, obj1, obj2=None):
    """The endjob() function terminates the viewer process by sending
    it a @c None object, and waiting for it to finish.

    @param evt Event object (psana only)
    @param env Environment object
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    super(mod_view, self).endjob(env)
    try:
      self._queue.put(None)
    except Exception:
      pass
    self.logger.info("endjob(): end of stream")
    self._proc.join()


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/mod_xes.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# $Id$
"""First- and second-order statistics for CS-PAD images

The mod_xes user analysis module performs the following sequence of
analysis:  dark correction (using average dark previously computed
by mod_average); removal of inactive pixels; common mode correction;
removal of pixels with high stddev ("hot pixels"); removal of
noise < 5 ADUs; selection of scan rows known to contain the
spectrum; summation-reduction so as to form a single spectrum.

XXX mod_xes must be run as a single process--guard against it!
"""
from __future__ import absolute_import, division, print_function

__version__ = "$Revision$"

import math
import os

from libtbx import easy_pickle
import scitbx.math
from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import average_tbx


class mod_xes(average_tbx.average_mixin):
  """Class for generating first- and second-order statistics within
  the pyana framework

  XXX Maybe this module should be renamed to mod_stat12, mod_sstat or
  some such?
  """


  def __init__(self,
               address,
               pickle_dirname=".",
               pickle_basename="",
               roi=None,
               **kwds):
    """The mod_average class constructor stores the parameters passed
    from the pyana configuration file in instance variables.  All
    parameters, except @p address are optional, and hence need not be
    defined in pyana.cfg.

    @param address         Full data source address of the DAQ device
    @param pickle_dirname  Directory portion of output pickle file
                           XXX mean, mu?
    @param pickle_basename Filename prefix of output pickle file
                           image XXX mean, mu?
    """
    super(mod_xes, self).__init__(
      address=address,
      **kwds
    )
    self.pickle_dirname = cspad_tbx.getOptString(pickle_dirname)
    self.pickle_basename = cspad_tbx.getOptString(pickle_basename)
    self.roi = cspad_tbx.getOptROI(roi)

  def event(self, evt, env):
    """The event() function is called for every L1Accept transition.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    super(mod_xes, self).event(evt, env)
    if (evt.get("skip_event")):
      return

    if self.roi is not None:
      pixels = self.cspad_img[self.roi[2]:self.roi[3], self.roi[0]:self.roi[1]]
      dark_mask = self.dark_mask[self.roi[2]:self.roi[3], self.roi[0]:self.roi[1]]
      pixels = pixels.as_1d().select(dark_mask.as_1d())
    else:
      pixels = self.cspad_img.as_1d().select(self.dark_mask.as_1d())
    stats = scitbx.math.basic_statistics(pixels.as_double())
    s, ms = cspad_tbx.evt_time(evt)
    evt_time = s + ms/1000
    self.stats_logger.info("SKEWNESS %.3f %s" %(evt_time, stats.skew))
    self.stats_logger.info("KURTOSIS %.3f %s" %(evt_time, stats.kurtosis))

    #if self._nmemb % 1000 == 0 or math.log(self._nmemb, 2) % 1 == 0:
      #self.endjob(env)

  #signature for pyana:
  #def endjob(self, env):

  #signature for psana:
  #def endjob(self, evt, env):

  def endjob(self, obj1, obj2=None):
    """The endjob() function finalises the mean and standard deviation
    images and writes them to disk.

    @param evt Event object (psana only)
    @param env Environment object
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    stats = super(mod_xes, self).endjob(env)
    if stats is None:
      return

    if stats['nmemb'] > 0:
      if (self.pickle_dirname  is not None or
          self.pickle_basename is not None):
        if (not os.path.isdir(self.pickle_dirname)):
          os.makedirs(self.pickle_dirname)
        d = dict(
          sum_img = stats['mean_img'],
          sumsq_img = stats['std_img'],
          nmemb = stats['nmemb'],
          sifoil = self.sifoil,
        )
        pickle_path = os.path.join(self.pickle_dirname,
                                   self.pickle_basename+str(env.subprocess())+".pickle")
        easy_pickle.dump(pickle_path, d)
        self.logger.info("Pickle written to %s" % self.pickle_dirname)

    if stats['nfail'] == 0:
      self.logger.info("%d images processed" % stats['nmemb'])
    else:
      self.logger.warning(
        "%d images processed, %d failed" % (stats['nmemb'], stats['nfail']))


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/parse_calib.py
from __future__ import absolute_import, division, print_function
from six.moves import range
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# XXX Module summary here
#
# XXX Zap excessive documentation
#
# $Id$

# XXX Move imports into functions?
import glob
import math
import numpy
import os
import re


class Section(object):
  """Class for a section, or a pair of ASIC:s, or a two-by-one.  A
  quadrant (and to some extent a section) is really an imaginary
  object--the detector only reads out the ASIC:s.  The metrology
  should be accurate to +/- one pixel, usually better but occasionally
  worse.  XXX Is it a section, a two-by-one, or a sensor?

  This class adopts a matrix-oriented coordinate system.  The origin
  is in the top left corner, the first coordinate increases downwards,
  the second coordinate increases to the right, and the third
  coordinate increases towards the viewer.  In this right-handed
  coordinate system, a rotation by a positive angle is
  counter-clockwise in the plane of the two first coordinates.

  @note There are a few hard coded numbers throughout this class.
  """


  # The size of a quadrant, in pixels.  Mikhail S. Dubrovin
  # empirically found that 850-by-850 pixels are enough to accommodate
  # any possibly section misalignment.
  q_size = (850, 850)


  def __init__(self, angle = 0, center = (0, 0)):
    """By default, an untransformed section is standing up, with its
    long side along the first (vertical) coordinate.  The two
    194-by-185 pixel ASIC:s of the section are separated by a
    three-pixel gap.

    XXX This is the metrology convention.  The data in the XTC stream
    are rotated by 90 degrees clockwise.
    """

    self.angle  = angle
    self.center = center
    self.size   = (2 * 194 + 3, 185)


  def corners(self, right = True):
    """The corners() function returns an array of the four corners of
    the section, in counter-clockwise order.  Each vertex is a
    two-dimensional array of the plane components.  XXX Maybe better
    named corners_section()?

    @param right @c True to restrict rotations to right angles
    @return      Coordinates of the four corners, in counter-clockwise
                 order
    """

    # The coordinates of the corners of the untransformed section, in
    # counter-clockwise order starting at the upper, left corner.
    coords = [[-0.5 * self.size[0], -0.5 * self.size[1]],
              [-0.5 * self.size[0], +0.5 * self.size[1]],
              [+0.5 * self.size[0], +0.5 * self.size[1]],
              [+0.5 * self.size[0], -0.5 * self.size[1]]]

    # Determine the cosine and sine of the rotation angle, rounded to
    # a multiple of 90 degrees if appropriate.
    if (right):
      a = math.radians(90.0 * round(self.angle / 90.0))
    else:
      a = math.radians(self.angle)
    c = math.cos(a)
    s = math.sin(a)

    # Apply plane rotation, and translation.
    for i in range(len(coords)):
      p         = coords[i]
      coords[i] = [c * p[0] - s * p[1] + self.center[0],
                   s * p[0] + c * p[1] + self.center[1]]
    return (coords)


  def corners_asic(self):
    """The corners_asic() function returns a list of pixel indices
    which position the two ASIC:s on the detector, in order from top
    to bottom by the "standing up" convention.  Each list of indices
    gives the coordinates of the top, left and bottom, right corners
    of the section's ASIC:s in "spotfinder format".
    """
    a = int(round(self.angle / 90.0)) % 4
    c = self.corners(True)

    if (a == 0):
      # The section is "standing up", and the top left corner is given
      # by the first corner.
      ul0 = [int(round(c[a][0])), int(round(c[a][1]))]
      ul1 = [ul0[0] + 194 + 3, ul0[1]]
      dlr = [194, 185]
    elif (a == 2):
      # The section is "standing up", and the top left corner is given
      # by the third corner.
      ul1 = [int(round(c[a][0])), int(round(c[a][1]))]
      ul0 = [ul1[0] + 194 + 3, ul1[1]]
      dlr = [194, 185]
    elif (a == 1):
      # The section is "laying down", and the top left corner is given
      # by the second corner.
      ul0 = [int(round(c[a][0])), int(round(c[a][1]))]
      ul1 = [ul0[0], ul0[1] + 194 + 3]
      dlr = [185, 194]
    elif (a == 3):
      # The section is "laying down", and the top left corner is given
      # by the forth corner.
      ul1 = [int(round(c[a][0])), int(round(c[a][1]))]
      ul0 = [ul1[0], ul1[1] + 194 + 3]
      dlr = [185, 194]

    coords = [
      [ul0[0], ul0[1], ul0[0] + dlr[0], ul0[1] + dlr[1]],
      [ul1[0], ul1[1], ul1[0] + dlr[0], ul1[1] + dlr[1]]]
    return (coords)


  def qrotate(self, angle):
    """ The qrotate() function rotates the section counter-clockwise
    by @p angle degrees around the centre of its quadrant.  The
    rotation angle is rounded to an integer multiple of 90 degrees
    prior to transformation.  Rotation around the quadrant centre
    changes the location and the orientation of the section.

    @param angle Rotation angle, in degrees
    """

    q = int(round(angle / 90.0)) % 4
    a = 90.0 * q

    if (q == 0):
      pass
    elif (q == 1):
      self.srotate(a)
      self.center = (self.q_size[1] - self.center[1],
                     0              + self.center[0])
    elif (q == 2):
      self.srotate(a)
      self.center = (self.q_size[0] - self.center[0],
                     self.q_size[1] - self.center[1])
    elif (q == 3):
      self.srotate(a)
      self.center = (0              + self.center[1],
                     self.q_size[0] - self.center[0])


  def srotate(self, angle):
    """The srotate() function rotates the section counter-clockwise by
    @p angle degrees around its centre.  Rotation within the quadrant
    only changes the orientation of the section.

    @param angle Rotation angle, in degrees
    """

    self.angle = self.angle + angle


  def translate(self, displacement):
    """The translate() function displaces the section.

    @param displacement Two-dimensional array of the additive
                        displacement
    """

    self.center = (self.center[0] + displacement[0],
                   self.center[1] + displacement[1])


def fread_vector(stream):
  """The fread_vector() function reads a vector from the stream
  pointed to by @p stream and returns it as a numpy array.

  @param stream Stream object
  @return       Tensor as numpy array
  """

  return (numpy.array(
      [float(d) for d in re.split(r"\s+", stream.readline()) if len(d) > 0]))


def fread_matrix(stream):
  """The fread_matrix() function reads a vector or matrix from the
  stream pointed to by @p stream and returns it as a numpy array.

  @param stream Stream object
  @return       Tensor as numpy array
  """

  A = fread_vector(stream)
  while (True):
    v = fread_vector(stream)
    if (v.shape[0] == 0):
      return (A)
    A = numpy.vstack((A, v))


def fread_tensor3(stream):
  """The fread_tensor3() function reads a tensor of rank no greater
  than 3 from the stream pointed to by @p stream and returns it as a
  numpy array.

  @param stream Stream object
  @return       Tensor as numpy array
  """

  T = fread_matrix(stream)
  while (True):
    A = fread_matrix(stream)
    if (len(A.shape) < 2 or A.shape[0] == 0 or A.shape[1] == 0):
      return (T)
    T = numpy.dstack((T, A))


def calib2tensor3(dirname, component):
  """The calib2tensor3() function reads the latest calibration tensor
  for @p component from @p dirname.  Any obsoleted calibration data is
  ignored.  The function returns the tensor as a numpy array on
  successful completion.

  @param dirname   Directory with calibration information
  @param component Kind of calibration data sought
  @return          Tensor as numpy array
  """

  basename = "*-end.data"
  path     = glob.glob(os.path.join(dirname, component, basename))[-1]
  stream   = open(path, "r")
  T        = fread_tensor3(stream)
  stream.close()
  return (T)

def v2calib2sections(filename):
  """The v2calib2sections() function reads calibration information
  stored in new style SLAC calibration file and returns a
  two-dimensional array of Section objects.  The first index in the
  returned array identifies the quadrant, and the second index
  identifies the section within the quadrant.

  @param dirname Directory with calibration information
  @return        Section objects
  """

  from serialtbx.detector.cspad import read_slac_metrology
  from scitbx.matrix import sqr
  from xfel.cxi.cspad_ana.cspad_tbx import pixel_size

  # metro is a dictionary where the keys are levels in the detector
  # hierarchy and the values are 'basis' objects
  metro = read_slac_metrology(filename)

  # 90 degree rotation to get into same frame
  reference_frame = sqr((0,-1, 0, 0,
                         1, 0, 0, 0,
                         0, 0, 1, 0,
                         0, 0, 0, 1))

  d = 0
  d_basis = metro[(d,)]

  sections = []
  for q in range(4):
    sections.append([])
    q_basis = metro[(d,q)]
    for s in range(8):
      if not (d,q,s) in metro:
        continue

      s_basis = metro[(d,q,s)]

      # collapse the transformations from the detector center to the quadrant center
      # to the sensor center
      transform = reference_frame * \
                  d_basis.as_homogenous_transformation() * \
                  q_basis.as_homogenous_transformation() * \
                  s_basis.as_homogenous_transformation()

      # an homologous transformation is a 4x4 matrix, with a 3x3 rotation in the
      # upper left corner and the translation in the right-most column. The last
      # row is 0,0,0,1
      ori = sqr((transform[0],transform[1],transform[2],
                 transform[4],transform[5],transform[6],
                 transform[8],transform[9],transform[10]))
      angle = ori.r3_rotation_matrix_as_x_y_z_angles(deg=True)[2]

      # move the reference of the sensor so its relative to the upper left of the
      # detector instead of the center of the detector
      center = (1765/2)+(transform[3]/pixel_size),(1765/2)+(transform[7]/pixel_size)

      sections[q].append(Section(angle, center))

  return sections

def calib2sections(dirname):
  """The calib2sections() function reads calibration information
  stored in the directory tree beneath @p dirname and returns a
  two-dimensional array of Section objects.  The first index in the
  returned array identifies the quadrant, and the second index
  identifies the section within the quadrant.

  @param dirname Directory with calibration information
  @return        Section objects
  """

  if os.path.isfile(dirname):
    return v2calib2sections(dirname)

  # Get centres of the sections, and apply corrections.
  s_cen = calib2tensor3(dirname, "center") \
      +   calib2tensor3(dirname, "center_corr")

  # Get the rotation of sections, and apply corrections.  Note that
  # sections 0, 1 and 4, 5 are antiparallel!
  s_rot = calib2tensor3(dirname, "rotation") \
      +   calib2tensor3(dirname, "tilt")

  # Get the margin, gap, and shift adjustments of the sections within
  # each quadrant.
  s_mgs = calib2tensor3(dirname, "marg_gap_shift")

  # Get offsets of the quadrants, and apply corrections.
  q_off = calib2tensor3(dirname, "offset") \
      +   calib2tensor3(dirname, "offset_corr")

  # Get rotation of the quadrants, and apply corrections.
  q_rot = calib2tensor3(dirname, "quad_rotation") \
      +   calib2tensor3(dirname, "quad_tilt")

  # The third coordinate is ignored for now, even though optical
  # measurement gives a variation in Z up to 0.6 mm.
  sections = []
  for q in range(s_cen.shape[0]):
    sections.append([])
    for s in range(s_cen.shape[1]):
      sec = Section()
      sec.translate((s_mgs[0, 0] + s_cen[q, s, 0],
                     s_mgs[1, 0] + s_cen[q, s, 1]))
      sec.srotate(s_rot[q, s])
      sec.qrotate(q_rot[q])
      sec.translate((s_mgs[0, 1] + q_off[0, q],
                     s_mgs[1, 1] + q_off[1, q]))

      # XXX I still don't understand this bit!
      if (q == 0):
        sec.translate((-s_mgs[0, 2] + s_mgs[0, 3],
                       -s_mgs[1, 2] - s_mgs[1, 3]))
      elif (q == 1):
        sec.translate((-s_mgs[0, 2] - s_mgs[0, 3],
                       +s_mgs[1, 2] - s_mgs[1, 3]))
      elif (q == 2):
        sec.translate((+s_mgs[0, 2] - s_mgs[0, 3],
                       +s_mgs[1, 2] + s_mgs[1, 3]))
      elif (q == 3):
        sec.translate((+s_mgs[0, 2] + s_mgs[0, 3],
                       -s_mgs[1, 2] + s_mgs[1, 3]))

      sections[q].append(sec)
  return (sections)


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/pixel_histograms.py
from __future__ import absolute_import, division, print_function
import math
import os

from libtbx import easy_pickle
from scitbx.array_family import flex
import scitbx.math
from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import common_mode
from six.moves import range

class pixel_histograms(common_mode.common_mode_correction):

  def __init__(self,
               address,
               pickle_dirname=".",
               pickle_basename="hist",
               roi=None,
               hist_min=None,
               hist_max=None,
               n_slots=None,
               **kwds):
    """

    @param address         Address string XXX Que?!
    @param pickle_dirname     Directory portion of output pickle file
                           XXX mean, mu?
    @param pickle_basename    Filename prefix of output pickle file
                           image XXX mean, mu?
    @param calib_dir       Directory with calibration information
    @param dark_path       Path to input dark image
    @param dark_stddev     Path to input dark standard deviation
    """
    super(pixel_histograms, self).__init__(
      address=address,
      **kwds
    )
    self.pickle_dirname = cspad_tbx.getOptString(pickle_dirname)
    self.pickle_basename = cspad_tbx.getOptString(pickle_basename)
    self.hist_min = cspad_tbx.getOptFloat(hist_min)
    self.hist_max = cspad_tbx.getOptFloat(hist_max)
    self.n_slots = cspad_tbx.getOptInteger(n_slots)
    self.histograms = {}
    self.dimensions = None
    self.roi = cspad_tbx.getOptROI(roi)
    self.values = flex.long()

    self.sigma_scaling = False
    if self.hist_min is None: self.hist_min = -50
    if self.hist_max is None: self.hist_max = 150
    if self.n_slots is None: self.n_slots = 200

  def beginjob(self, evt, env):
    super(pixel_histograms, self).beginjob(evt, env)


  def event(self, evt, env):
    """The event() function is called for every L1Accept transition.
    Once self.nshots shots are accumulated, this function turns into
    a nop.

    @param evt Event data object, a configure object
    @param env Environment object
    """

    super(pixel_histograms, self).event(evt, env)

    if (evt.get("skip_event")):
      return

    if self.sigma_scaling:
      flex_cspad_img = self.cspad_img.as_double()
      flex_cspad_img_sel = flex_cspad_img.as_1d().select(self.dark_mask.as_1d())
      flex_dark_stddev = self.dark_stddev.select(self.dark_mask.as_1d()).as_double()
      assert flex_dark_stddev.count(0) == 0
      flex_cspad_img_sel /= flex_dark_stddev
      flex_cspad_img.as_1d().set_selected(self.dark_mask.as_1d().iselection(), flex_cspad_img_sel)
      self.cspad_img = flex_cspad_img.iround()

    pixels = self.cspad_img.deep_copy()
    dimensions = pixels.all()
    if self.roi is None:
      self.roi = (0, dimensions[1], 0, dimensions[0])

    for i in range(self.roi[2], self.roi[3]):
      for j in range(self.roi[0], self.roi[1]):
        if (i,j) not in self.histograms:
          self.histograms[(i,j)] = flex.histogram(flex.double(), self.hist_min, self.hist_max, self.n_slots)
        self.histograms[(i,j)].update(pixels[i,j])

    self.nmemb += 1
    if 0 and math.log(self.nmemb, 2) % 1 == 0:
      self.endjob(env)
    print(self.nmemb)

  #signature for pyana:
  #def endjob(self, env):

  #signature for psana:
  #def endjob(self, evt, env):

  def endjob(self, obj1, obj2=None):
    """The endjob() function finalises the mean and standard deviation
    images and writes them to disk.
    @param evt Event object (psana only)
    @param env Environment object
    """

    if obj2 is None:
      env = obj1
    else:
      evt = obj1
      env = obj2

    super(pixel_histograms, self).endjob(env)

    d = {
      "nmemb": self.nmemb,
      "histogram": self.histograms,
    }

    pickle_path = os.path.join(self.pickle_dirname,
                               self.pickle_basename+str(env.subprocess())+".pickle")
    easy_pickle.dump(pickle_path, d)
    self.logger.info(
      "Pickle written to %s" % self.pickle_dirname)

    if (self.nfail == 0):
      self.logger.info(
        "%d images processed" % self.nmemb)
    else:
      self.logger.warning(
        "%d images processed, %d failed" % (self.nmemb, self.nfail))


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/rayonix_tbx.py
from __future__ import absolute_import, division, print_function
# Utility functions for the Rayonix Detector.
from dxtbx.format.cbf_writer import add_frame_specific_cbf_tables
from scitbx.array_family import flex

from serialtbx.detector.rayonix import rayonix_min_trusted_value, rayonix_max_trusted_value, get_rayonix_pixel_size
from serialtbx.detector.rayonix import get_data_from_psana_event, rayonix_saturated_value, get_rayonix_detector_dimensions # import dependency

def get_rayonix_cbf_handle(tiles, metro, timestamp, cbf_root, wavelength, distance, bin_size, detector_size, verbose = True, header_only = False):
  # set up the metrology dictionary to include axis names, pixel sizes, and so forth
  dserial = None
  dname = None
  detector_axes_names = [] # save these for later
  pixel_size = get_rayonix_pixel_size(bin_size)
  for key in sorted(metro):
    basis = metro[key]
    if len(key) == 1:
      assert dserial is None # only one detector allowed for now
      dserial = key[0]

      dname = "AXIS_D%d"%dserial #XXX check if DS1 is here
      for a in ["_X","_Y","_Z","_R"]: detector_axes_names.append(dname+a)
      basis.equipment_component = "detector_arm"
      basis.depends_on = dname+"_X"
      basis.include_translation = False # don't include the translation in the rotation axis offset below, instead it will be
                                        # included in the axis_settings table below
      basis.pixel_size = (pixel_size,pixel_size)
      basis.dimension = detector_size
      basis.trusted_range = (rayonix_min_trusted_value, rayonix_max_trusted_value)
    else:
      assert False # shouldn't be reached as it would indicate a hierarchy for this detector
    basis.axis_name = detector_axes_names[-1]

  # the data block is the root cbf node
  from dxtbx.format.FormatCBFMultiTile import cbf_wrapper
  import os
  cbf=cbf_wrapper()
  cbf.new_datablock(os.path.splitext(os.path.basename(cbf_root))[0].encode())

  # Each category listed here is preceded by the imageCIF description taken from here:
  # http://www.iucr.org/__data/iucr/cifdic_html/2/cif_img.dic/index.html

  """Data items in the DIFFRN category record details about the
   diffraction data and their measurement."""
  cbf.add_category("diffrn",["id"])
  cbf.add_row(["Rayonix"])

  """Data items in the DIFFRN_SOURCE category record details of
  the source of radiation used in the diffraction experiment."""
  cbf.add_category("diffrn_source", ["diffrn_id","source","type"])
  cbf.add_row(["Rayonix","xfel","LCLS Endstation"])

  """Data items in the DIFFRN_DETECTOR category describe the
   detector used to measure the scattered radiation, including
   any analyser and post-sample collimation."""
  cbf.add_category("diffrn_detector", ["diffrn_id","id","type","details","number_of_axes"])
  cbf.add_row(["Rayonix","MFX_Rayonix","Rayonix",".",str(len(detector_axes_names))])

  """Data items in the DIFFRN_DETECTOR_AXIS category associate
     axes with detectors."""
  # Note, does not include the fast and the slow axes
  cbf.add_category("diffrn_detector_axis",["detector_id","axis_id"])
  for name in detector_axes_names:
    cbf.add_row(["MFX_Rayonix",name])

  """Data items in the DIFFRN_DETECTOR_ELEMENT category record
   the details about spatial layout and other characteristics
   of each element of a detector which may have multiple elements."""
  cbf.add_category("diffrn_detector_element",["id","detector_id"])

  cbf.add_row(["ELE_Rayonix", "MFX_Rayonix"])

  """Data items in the DIFFRN_DATA_FRAME category record
   the details about each frame of data."""
  cbf.add_category("diffrn_data_frame",["id","detector_element_id","array_id","binary_id"])

  cbf.add_row(["FRAME1","ELE_Rayonix","ARRAY_Rayonix","1"])

  if not header_only:
    add_frame_specific_cbf_tables(cbf, wavelength, timestamp, [metro[k].trusted_range for k in metro.keys()])

  """Data items in the AXIS category record the information required
     to describe the various goniometer, detector, source and other
     axes needed to specify a data collection.  The location of each
     axis is specified by two vectors: the axis itself, given as a unit
     vector, and an offset to the base of the unit vector.  These vectors
     are referenced to a right-handed laboratory coordinate system with
     its origin in the sample or specimen"""
  # More detail here: http://www.iucr.org/__data/iucr/cifdic_html/2/cif_img.dic/Caxis.html
  # Note we also use two new columns not in the latest imageCIF dictionary: rotation and rotation_axis.
  # We use them to specify an translation and a rotation in a single axis to describe a change in setting
  # when moving from one frame (say a quadrant) to another (say a sensor)
  cbf.add_category("axis",["id","type","equipment","depends_on","vector[1]","vector[2]","vector[3]",
                                                                "offset[1]","offset[2]","offset[3]",
                                                                "equipment_component"])

  # Keep a list of rows to add to the scan frame axis table
  axis_settings = []
  # keep a list of rows to add to the scan axis table
  axis_names = []

  # Create a series of axis describing frame shifts from each level of the detector to the next
  cbf.add_row( "AXIS_SOURCE  general     source   .        0  0  1 . . . .".split())                           ; axis_names.append("AXIS_SOURCE")
  cbf.add_row( "AXIS_GRAVITY general     gravity  .        0 -1  0 . . . .".split())                           ; axis_names.append("AXIS_GRAVITY")
  cbf.add_row(("%s_Z         translation detector .        0  0  1 . . . detector_arm"%(dname)).split())       ; axis_names.append("%s_Z"%dname)
  cbf.add_row(("%s_Y         translation detector %s_Z     0  1  0 . . . detector_arm"%(dname,dname)).split()) ; axis_names.append("%s_Y"%dname)
  cbf.add_row(("%s_X         translation detector %s_Y     1  0  0 . . . detector_arm"%(dname,dname)).split()) ; axis_names.append("%s_X"%dname)

  root_basis = metro[(0,)]

  axis_settings.append(["AXIS_SOURCE" ,"FRAME1","0","0"])
  axis_settings.append(["AXIS_GRAVITY","FRAME1","0","0"])
  axis_settings.append([dname+"_X"    ,"FRAME1","0",str(root_basis.translation[0])])
  axis_settings.append([dname+"_Y"    ,"FRAME1","0",str(root_basis.translation[1])])
  axis_settings.append([dname+"_Z"    ,"FRAME1","0",str(root_basis.translation[2])])

  for key in sorted(metro):
    basis = metro[key]
    cbf.add_frame_shift(basis, axis_settings)
    axis_names.append(basis.axis_name)

    dim_pixel = basis.pixel_size
    dim_readout = basis.dimension

    # Add the two vectors for each asic that describe the fast and slow directions pixels should be laid out in real space
    offset_fast = -dim_pixel[0]*((dim_readout[0]) / 2)
    offset_slow = +dim_pixel[1]*((dim_readout[1]) / 2)

    aname = "D%d"%key

    cbf.add_row(["AXIS_"+ aname + "_S", "translation","detector",basis.axis_name,"0", "-1","0",str(offset_fast),str(offset_slow),"0.0", "detector_asic"])
    cbf.add_row(["AXIS_"+ aname + "_F", "translation","detector","AXIS_"+aname +"_S","1","0","0","0","0","0.0", "detector_asic"])
    axis_names.append("AXIS_"+ aname + "_F"); axis_names.append("AXIS_"+ aname + "_S")
    axis_settings.append(["AXIS_"+ aname + "_F","FRAME1","0","0"])
    axis_settings.append(["AXIS_"+ aname + "_S","FRAME1","0","0"])

  """Data items in the DIFFRN_SCAN_AXIS category describe the settings of
     axes for particular scans.  Unspecified axes are assumed to be at
     their zero points."""
  # leave all the settings zero. Levels with settings are set below.
  cbf.add_category("diffrn_scan_axis",["axis_id","scan_id","angle_start","angle_range","angle_increment",
                                       "displacement_start","displacement_range","displacement_increment"])
  for name in axis_names:
    cbf.add_row([name,"SCAN1","0","0","0","0","0","0"])

  """Data items in the DIFFRN_SCAN_FRAME_AXIS category describe the
     settings of axes for particular frames.  Unspecified axes are
     assumed to be at their zero points."""
  cbf.add_category("diffrn_scan_frame_axis",["axis_id","frame_id","angle","displacement"])
  for row in axis_settings:
    cbf.add_row(row)

  """Data items in the ARRAY_STRUCTURE_LIST category record the size
     and organization of each array dimension.
     The relationship to physical axes may be given."""

  # find the asic sizes
  for tilekey in metro.keys():
    b = metro[tilekey]
    if not "x_dim" in locals():
      x_dim = b.dimension[0]
      y_dim = b.dimension[1]
    else:
      assert x_dim == b.dimension[0] and y_dim == b.dimension[1]

  z_dim = len(metro)
  root_key = (0,); assert root_key in metro; assert z_dim == 1 # single panel monolithic

  cbf.add_category("array_structure_list",["array_id","index","dimension","precedence","direction","axis_set_id"])
  cbf.add_row(["ARRAY_Rayonix","1","%d"%(x_dim),"1","increasing",dname+"_F"])
  cbf.add_row(["ARRAY_Rayonix","2","%d"%y_dim,"2","increasing",dname+"_S"])

  """Data items in the ARRAY_STRUCTURE_LIST_SECTION category identify
     the dimension-by-dimension start, end and stride of each section of an
     array that is to be referenced."""
  # no array sections in monolithic rayonix
  #cbf.add_category("array_structure_list_section",["array_id","id","index","start","end"])

  """Data items in the ARRAY_STRUCTURE_LIST_AXIS category describe
     the physical settings of sets of axes for the centres of pixels that
     correspond to data points described in the
     ARRAY_STRUCTURE_LIST category."""
  cbf.add_category("array_structure_list_axis",["axis_set_id","axis_id","displacement","displacement_increment"])
  cbf.add_row([dname+"_F",dname+"_F","0.0",str(metro[root_key].pixel_size[0])])
  cbf.add_row([dname+"_S",dname+"_S","0.0",str(metro[root_key].pixel_size[1])])

  if not header_only:
    add_data_to_cbf(cbf, tiles)

  return cbf

from dxtbx.format.FormatCBFCspad import FormatCBFFullStillInMemory
class FormatCBFRayonixInMemory(FormatCBFFullStillInMemory):
  """Mixin class for Rayonix in memory"""

def get_dxtbx_from_params(params, detector_size):
  """ Build a dxtbx format object for the Rayonix based on input paramters (beam center and binning) """
  from serialtbx.detector import basis
  from scitbx.matrix import col
  fake_distance = 100
  null_ori = col((0,0,1)).axis_and_angle_as_unit_quaternion(0, deg=True)
  if params.override_beam_x is None and params.override_beam_y is None:
    metro = {(0,): basis(null_ori, col((0, 0, 0)))}
  elif params.override_beam_x is not None and params.override_beam_y is not None:
    # compute the offset from the origin given the provided beam center override
    pixel_size = get_rayonix_pixel_size(params.bin_size)
    image_center = col(detector_size)/2
    override_center = col((params.override_beam_x, params.override_beam_y))
    delta = (image_center-override_center)*pixel_size
    metro = {(0,): basis(null_ori, col((delta[0], -delta[1], 0)))} # note the -Y
  else:
    assert False, "Please provide both override_beam_x and override_beam_y or provide neither"
  cbf = get_rayonix_cbf_handle(None, metro, None, "test", None, fake_distance, params.bin_size, detector_size, verbose = True, header_only = True)
  base_dxtbx = FormatCBFRayonixInMemory(cbf)
  return base_dxtbx

def format_object_from_data(base_dxtbx, data, distance, wavelength, timestamp, address, round_to_int=True):
  """
  Given a preloaded dxtbx format object and raw data, assemble the tiles
  and set the distance.
  @param base_dxtbx A header only dxtbx format object
  @param data rayonix byte array from XTC stream
  @param distance Detector distance (mm)
  @param wavelength Shot wavelength (angstroms)
  @param timestamp Human readable timestamp
  @param address Detector address, put in CBF header
  """
  import numpy as np
  from xfel.cftbx.detector import cspad_cbf_tbx
  cbf = cspad_cbf_tbx.copy_cbf_header(base_dxtbx._cbf_handle, skip_sections=True)
  rayonix_img = FormatCBFRayonixInMemory(cbf)
  cbf.set_datablockname((address + "_" + timestamp).encode())

  if round_to_int:
    data = flex.double(data.astype(np.float64)).iround()
  else:
    data = flex.double(data.astype(np.float64))

  n_asics = data.focus()[0] * data.focus()[1]
  add_frame_specific_cbf_tables(cbf, wavelength,timestamp,
    [(rayonix_min_trusted_value, rayonix_max_trusted_value)]*n_asics)

  # Set the distance, I.E., the length translated along the Z axis
  cbf.find_category(b"diffrn_scan_frame_axis")
  cbf.find_column(b"axis_id")
  cbf.find_row(b"AXIS_D0_Z") # XXX discover the Z axis somehow, don't use D0 here
  cbf.find_column(b"displacement")
  cbf.set_value(b"%f"%(-distance))

  # Explicitly reset the detector object now that the distance is set correctly
  rayonix_img._detector_instance = rayonix_img._detector()

  # Explicitly set up the beam object now that the tables are all loaded correctly
  rayonix_img._beam_instance = rayonix_img._beam()

  # Get the data and add it to the cbf handle.
  add_data_to_cbf(cbf,data)

  return rayonix_img

def add_data_to_cbf(cbf, data, verbose = False):
  """
  Given a cbf handle, add the raw data and the necessary tables to support it
  """
  import pycbf

  cbf.find_category(b"diffrn_data_frame")
  while True:
    try:
      cbf.find_column(b"array_id")
      array_name = cbf.get_value().decode()
      cbf.next_row()
    except Exception as e:
      assert "CBF_NOTFOUND" in str(e)
      break

  assert len(data.focus()) == 2
  if isinstance(data,flex.int):
    data_is_int = True
  elif isinstance(data,flex.double):
    data_is_int = False
  else:
    raise TypeError("Ints or doubles are required")

  """ Data items in the ARRAY_STRUCTURE category record the organization and
  encoding of array data in the ARRAY_DATA category."""
  cbf.add_category("array_structure",["id","encoding_type","compression_type","byte_order"])
  if data_is_int:
    cbf.add_row([array_name,"signed 32-bit integer","packed","little_endian"])
  else:
    cbf.add_row([array_name,"signed 64-bit real IEEE","packed","little_endian"])

  """ Data items in the ARRAY_DATA category are the containers for the array data
  items described in the category ARRAY_STRUCTURE. """
  cbf.add_category("array_data",["array_id","binary_id","data"])

  if verbose:
    print("Compressing tiles...", end=' ')

  focus = data.focus()

  cbf.add_row([array_name,"1"])

  binary_id = 1
  elements = len(data)
  data = data.copy_to_byte_str()
  byteorder = b"little_endian"
  dimfast = focus[1]
  dimmid = focus[0]
  dimslow = 1
  padding = 0

  if data_is_int:
    elsize = 4
    elsigned = 1

    cbf.set_integerarray_wdims_fs(\
      pycbf.CBF_PACKED,
      binary_id,
      data,
      elsize,
      elsigned,
      elements,
      byteorder,
      dimfast,
      dimmid,
      dimslow,
      padding)
  else:
    elsize = 8

    cbf.set_realarray_wdims_fs(\
      pycbf.CBF_CANONICAL,
      binary_id,
      data,
      elsize,
      elements,
      byteorder,
      dimfast,
      dimmid,
      dimslow,
      padding)


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/xes_faster_histograms.py
"""Specialized version of xes_histograms.
1) no support for background region of interest
2) photon_counting method only; uses integrated area under 1-photon Gaussian
3) no support for >1 photon
4) no multiprocessing
5) no output of gain map; no input gain correction
6) Fixed constraints for ratio of peak widths 1-photon: 0-photon
7) Fixed constraints for ratio of 1-photon gain: 0-photon sigma
8) 60-fold speed improvement over xes_histograms.py; takes 7 seconds.
"""
from __future__ import absolute_import, division, print_function

import os
import sys
import math

from libtbx import easy_pickle
import iotbx.phil
from scitbx.array_family import flex
import scitbx.math

from xfel.command_line import view_pixel_histograms
from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import xes_finalise
from scitbx.lstbx import normal_eqns
from scitbx.lstbx import normal_eqns_solving
from scitbx.math import curve_fitting

from xfel.cxi.cspad_ana.xes_histograms import master_phil_str
from six.moves import range

master_phil_str = master_phil_str + """
xes {
  fudge_factor {
    gain_to_sigma = 6.75
      .type = float
      .help = On the assumption that one-mean is zero_mean + zero_sigma * gain_to_sigma
      .help = with gain_to_sigma being a constant for the pixel array detector.
      .help = approx 6.75 for LB67 r0100
      .help = manually optimized for LG36: r0025,6.0 / r0080,5.8 (MnCl2) / r0137,5.9 (PSII solution)
  }
  fit_limits = (20,150)
    .type = ints(size=2)
    .help = x-Limits for histogram fitting, relative to the presumably -50 ADU histogram origin
    .help = 20,150 used for LG36
}
"""
Usage = """cctbx.python xes_faster_histograms.py output_dirname=[outdir] \
   roi=0:388,99:126 [datadir]/hist_r0[run_no].pickle run=[run_no] fudge_factor.gain_to_sigma=5.9
   ...converts histogram.pickle file (described elsewhere) into spectrum by fitting
      0- and 1-photon Gaussians to histograms representing each pixel on the XES spectrometer."""

def run(args):
  if len(args)==0: print(Usage); exit()
  processed = iotbx.phil.process_command_line(
    args=args, master_string=master_phil_str)
  args = processed.remaining_args
  work_params = processed.work.extract().xes
  processed.work.show()
  assert len(args) == 1
  output_dirname = work_params.output_dirname
  roi = cspad_tbx.getOptROI(work_params.roi)
  bg_roi = cspad_tbx.getOptROI(work_params.bg_roi)
  gain_map_path = work_params.gain_map
  estimated_gain = work_params.estimated_gain

  print(output_dirname)
  if output_dirname is None:
    output_dirname = os.path.join(os.path.dirname(args[0]), "finalise")
    print(output_dirname)
  hist_d = easy_pickle.load(args[0])
  if len(hist_d)==2:
    hist_d = hist_d['histogram']
  pixel_histograms = faster_methods_for_pixel_histograms(
    hist_d, work_params)

  result = xes_from_histograms(
    pixel_histograms, output_dirname=output_dirname,
    gain_map_path=gain_map_path, estimated_gain=estimated_gain,
    roi=roi, run=work_params.run)

class xes_from_histograms(object):

  def __init__(self, pixel_histograms, output_dirname=".", gain_map_path=None,
               gain_map=None, estimated_gain=30,roi=None,run=None):

    self.sum_img = flex.double(flex.grid(370,391), 0) # XXX define the image size some other way?
    gain_img = flex.double(self.sum_img.accessor(), 0)

    assert [gain_map, gain_map_path].count(None) > 0
    if gain_map_path is not None:
      d = easy_pickle.load(gain_map_path)
      gain_map = d["DATA"]

    mask = flex.int(self.sum_img.accessor(), 0)

    start_row = 370
    end_row = 0
    print(len(pixel_histograms.histograms))

    pixels = list(pixel_histograms.pixels())
    n_pixels = len(pixels)
    if roi is not None:
      for k, (i, j) in enumerate(reversed(pixels)):
        if (   i < roi[2]
            or i > roi[3]
            or j < roi[0]
            or j > roi[1]):
          del pixels[n_pixels-k-1]

    if gain_map is None:
      fixed_func = pixel_histograms.fit_one_histogram
    else:
      def fixed_func(pixel):
        return pixel_histograms.fit_one_histogram(pixel, n_gaussians=1)

    chi_squared_list=flex.double()

    for i, pixel in enumerate(pixels):
      #print i,pixel
      LEG = False
      start_row = min(start_row, pixel[0])
      end_row = max(end_row, pixel[0])
      n_photons = 0

      try:
          if LEG: gaussians, two_photon_flag = pixel_histograms.fit_one_histogram(pixel)
          alt_gaussians = pixel_histograms.fit_one_histogram_two_gaussians(pixel)
      except ZeroDivisionError:
          print("HEY DIVIDE BY ZERO")
          #pixel_histograms.plot_combo(pixel, gaussians)
          mask[pixel] = 1
          continue
      except RuntimeError as e:
          print("Error fitting pixel %s" %str(pixel))
          print(str(e))
          mask[pixel] = 1
          continue

      hist = pixel_histograms.histograms[pixel]

      if not LEG:
        gs = alt_gaussians[1].params
        fit_photons = gs[0] * gs[2] * math.sqrt(2.*math.pi)
        n_photons = int(round(fit_photons,0))
        fit_interpretation=pixel_histograms.multiphoton_and_fit_residual(
                     pixel_histograms.histograms[pixel], alt_gaussians)
        multi_photons = fit_interpretation.get_multiphoton_count()
        total_photons = n_photons + multi_photons

        if False and n_photons< 0: # Generally, do not mask negative values; if fit is still OK
          print("\n%d pixel %s altrn %d photons from curvefitting"%( i,pixel,n_photons ))
          pixel_histograms.plot_combo(pixel, alt_gaussians,
                                      interpretation=fit_interpretation)
          mask[pixel]=1 # do not mask out negative pixels if the Gaussian fit is good
          continue

        chi_squared_list.append(fit_interpretation.chi_squared())
        suspect = False # don't know the optimal statistical test.  Histograms vary primarily by total count & # photons
        if total_photons <= 3:
          if fit_interpretation.chi_squared() > 2.5 or fit_interpretation.quality_factor < 5: suspect=True
        elif 3 < total_photons <= 10:
          if fit_interpretation.chi_squared() > 5 or fit_interpretation.quality_factor < 10: suspect=True
        elif 10 < total_photons <= 33:
          if fit_interpretation.chi_squared() > 10 or fit_interpretation.quality_factor < 20: suspect=True
        elif 33 < total_photons <= 100:
          if fit_interpretation.chi_squared() > 20 or fit_interpretation.quality_factor < 20: suspect=True
        elif 100 < total_photons <= 330:
          if fit_interpretation.chi_squared() > 30 or fit_interpretation.quality_factor < 25: suspect=True
        elif 330 < total_photons <= 1000:
          if fit_interpretation.chi_squared() > 40 or fit_interpretation.quality_factor < 30: suspect=True
        elif 1000 < total_photons:
          if fit_interpretation.chi_squared() > 50 or fit_interpretation.quality_factor < 30: suspect=True

        if suspect:
          print("\n%d pixel %s Bad quality 0/1-photon fit"%(i,pixel),fit_interpretation.quality_factor)
          print("   with chi-squared %10.5f"%fit_interpretation.chi_squared())
          print("   Suspect",suspect)
          print("%d fit photons, %d total photons"%(n_photons,total_photons))
          #pixel_histograms.plot_combo(pixel, alt_gaussians,
          #                            interpretation=fit_interpretation)
          mask[pixel]=1
          continue

        self.sum_img[pixel] = n_photons + multi_photons

    mask.set_selected(self.sum_img == 0, 1)
    unbound_pixel_mask = xes_finalise.cspad_unbound_pixel_mask()
    mask.set_selected(unbound_pixel_mask > 0, 1)
    bad_pixel_mask = xes_finalise.cspad2x2_bad_pixel_mask_cxi_run7()
    mask.set_selected(bad_pixel_mask > 0, 1)

    for row in range(self.sum_img.all()[0]):
      self.sum_img[row:row+1,:].count(0)

    spectrum_focus = self.sum_img[start_row:end_row,:]
    mask_focus = mask[start_row:end_row,:]

    spectrum_focus.set_selected(mask_focus > 0, 0)

    xes_finalise.filter_outlying_pixels(spectrum_focus, mask_focus)

    print("Number of rows: %i" %spectrum_focus.all()[0])
    print("Estimated no. photons counted: %i" %flex.sum(spectrum_focus))
    print("Number of images used: %i" %flex.sum(
      pixel_histograms.histograms.values()[0].slots()))

    d = cspad_tbx.dpack(
      address='CxiSc1-0|Cspad2x2-0',
      data=spectrum_focus,
      distance=1,
      ccd_image_saturation=2e8, # XXX
    )
    if run is not None: runstr="_%04d"%run
    else: runstr=""
    cspad_tbx.dwritef(d, output_dirname, 'sum%s_'%runstr)


    plot_x, plot_y = xes_finalise.output_spectrum(
      spectrum_focus.iround(), mask_focus=mask_focus,
      output_dirname=output_dirname, run=run)
    self.spectrum = (plot_x, plot_y)
    self.spectrum_focus = spectrum_focus
    xes_finalise.output_matlab_form(spectrum_focus, "%s/sum%s.m" %(output_dirname,runstr))
    print(output_dirname)
    print("Average chi squared is",flex.mean(chi_squared_list),"on %d shots"%flex.sum(hist.slots()))

SIGMAFAC = 1.15
class faster_methods_for_pixel_histograms(view_pixel_histograms.pixel_histograms):

  def __init__(self,hist_dict,work_params):
    self.work_params = work_params
    super(faster_methods_for_pixel_histograms,self
      ).__init__(hist_dict,work_params.estimated_gain)

  def plot_combo(self, pixel, gaussians,
                         window_title=None, title=None,
                         log_scale=False, normalise=False, save_image=False, interpretation=None):
    histogram = self.histograms[pixel]
    from matplotlib import pyplot
    from xfel.command_line.view_pixel_histograms import hist_outline
    slots = histogram.slots().as_double()
    if normalise:
      normalisation = (flex.sum(slots) + histogram.n_out_of_slot_range()) / 1e5
      print("normalising by factor: ", normalisation)
      slots /= normalisation
    bins, data = hist_outline(histogram)
    if log_scale:
      data.set_selected(data == 0, 0.1) # otherwise lines don't get drawn when we have some empty bins
      pyplot.yscale("log")
    pyplot.plot(bins, data, '-k', linewidth=2)
    pyplot.plot(bins, data/1000., '-k', linewidth=2)
    pyplot.suptitle(title)
    data_min = min([slot.low_cutoff for slot in histogram.slot_infos() if slot.n > 0])
    data_max = max([slot.low_cutoff for slot in histogram.slot_infos() if slot.n > 0])
    pyplot.xlim(data_min, data_max+histogram.slot_width())
    pyplot.xlim(-50, 100)
    pyplot.ylim(-10, 40)
    x = histogram.slot_centers()
    for g in gaussians:
      print("Height %7.2f mean %4.1f sigma %3.1f"%(g.params))
      pyplot.plot(x, g(x), linewidth=2)

    if interpretation is not None:
      interpretation.plot_multiphoton_fit(pyplot)
      interpretation.plot_quality(pyplot)
    pyplot.show()

  @staticmethod
  def multiphoton_and_fit_residual(histogram,gaussians):

    class per_pixel_analysis:

      def __init__(OO):

        #OK let's figure stuff out about the multiphoton residual, after fitting with 0 + 1 photons
        # only count the residual for x larger than one_mean + 3*zero_sigma
        x = histogram.slot_centers()
        y_calc = flex.double(x.size(), 0)
        for g in gaussians:
          y_calc += g(x)
        xfloor = gaussians[1].params[1] + 3.*gaussians[0].params[2]
        selection = (histogram.slot_centers()>xfloor)
        OO.fit_xresid = histogram.slot_centers().select(selection)
        OO.fit_yresid = histogram.slots().as_double().select(selection) - y_calc.select(selection)
        OO.xweight = (OO.fit_xresid - gaussians[0].params[1])/(gaussians[1].params[1] - gaussians[0].params[1])
        OO.additional_photons = flex.sum( OO.xweight * OO.fit_yresid )

        #Now the other half of the data; the part supposedly fit by the 0- and 1-photon gaussians
        OO.qual_xresid = histogram.slot_centers().select(~selection)
        ysignal = histogram.slots().as_double().select(~selection)
        OO.qual_yresid = ysignal - y_calc.select(~selection)
        # Not sure how to treat weights for channels with zero observations; default to 1
        _variance = ysignal.deep_copy().set_selected(ysignal==0., 1.)
        OO.weight = 1./_variance
        OO.weighted_numerator = OO.weight * (OO.qual_yresid * OO.qual_yresid)
        OO.sumsq_signal = flex.sum(ysignal * ysignal)
        OO.sumsq_residual = flex.sum(OO.qual_yresid * OO.qual_yresid)

      def get_multiphoton_count(OO):
        # XXX insert a test here as to whether the analysis has been carried out
        #   far enough along x-axis to capture all the high multi-photon signal
        #   if not, raise an exception
        return int(round(OO.additional_photons,0))

      def plot_multiphoton_fit(OO,plotter):
        print("counted %.0f multiphoton photons on this pixel"%OO.additional_photons)
        plotter.plot(OO.fit_xresid, 10*OO.xweight, "b.")
        plotter.plot(OO.fit_xresid,OO.fit_yresid,"r.")

      def plot_quality(OO,plotter):
        plotter.plot(OO.qual_xresid,OO.qual_yresid/10.,"m.")
        print(OO.sumsq_signal,OO.sumsq_residual, OO.quality_factor, math.sqrt(OO.sumsq_signal))

      def chi_squared(OO):
        return flex.sum(OO.weighted_numerator)/len(OO.weighted_numerator)

    E = per_pixel_analysis()
    E.quality_factor = E.sumsq_signal/E.sumsq_residual
    return E

  def fit_one_histogram_two_gaussians(self,pixel):
    histogram = self.histograms[pixel]
    fitted_gaussians = []
    GAIN_TO_SIGMA = self.work_params.fudge_factor.gain_to_sigma
    low_idx = self.work_params.fit_limits[0]
    high_idx = self.work_params.fit_limits[1]

    slot_centers = histogram.slot_centers()
    free_x = slot_centers[low_idx:high_idx]
    #print list(free_x)
    slots = histogram.slots().as_double()
    free_y = slots[low_idx:high_idx]

    # zero_mean = 0. # originally intended mean=0
    maxidx = flex.max_index(free_y) # but if dark subtraction (pedstal correction) is off
    zero_mean = free_x[maxidx] # use this non-zero maximum instead

    zero_amplitude = flex.max(free_y)
    assert 1./zero_amplitude #guard against division by zero
    total_population = flex.sum(free_y)
    zero_sigma = self.estimated_gain / GAIN_TO_SIGMA
    one_amplitude = 0.001
    helper = self.per_pixel_helper_factory(initial_estimates =
      (zero_mean, 1.0, zero_sigma, one_amplitude),
      GAIN_TO_SIGMA=GAIN_TO_SIGMA,
      free_x = free_x,
      free_y = free_y/zero_amplitude) # put y values on 0->1 scale for normal eqn solving
    helper.restart()
    iterations = normal_eqns_solving.levenberg_marquardt_iterations(
          non_linear_ls = helper,
          n_max_iterations = 7,
          gradient_threshold = 1.E-3)
    #print "current values after iterations", list(helper.x),

    fitted_gaussians = helper.as_gaussians()
    for item in fitted_gaussians: item.params = (item.params[0] * zero_amplitude,
                                  item.params[1], item.params[2]) # convert back to full scale
    return fitted_gaussians

  @staticmethod
  def per_pixel_helper_factory(initial_estimates,GAIN_TO_SIGMA,free_x,free_y):

      from xfel.vonHamos import gaussian_fit_inheriting_from_non_linear_ls

      class per_pixel_helper(gaussian_fit_inheriting_from_non_linear_ls, normal_eqns.non_linear_ls_mixin):
        def __init__(pfh):
          super(per_pixel_helper, pfh).__init__(n_parameters=4)
          pfh.x_0 = flex.double(initial_estimates)
          pfh.restart()
          pfh.set_cpp_data(free_x,free_y,gain_to_sigma=GAIN_TO_SIGMA,sigmafac=SIGMAFAC)

        def restart(pfh):
          pfh.x = pfh.x_0.deep_copy()
          pfh.old_x = None

        def step_forward(pfh):
          pfh.old_x = pfh.x.deep_copy()
          pfh.x += pfh.step()

        def step_backward(pfh):
          assert pfh.old_x is not None
          pfh.x, pfh.old_x = pfh.old_x, None

        def parameter_vector_norm(pfh):
          return pfh.x.norm()

        def build_up(pfh, objective_only=False):
          pfh.reset()
          #rely on C++ and go directly for add_equation singular
          pfh.access_cpp_build_up_directly(objective_only, current_values = pfh.x)

        def as_gaussians(pfh):
          return [curve_fitting.gaussian( a = pfh.x[1], b = pfh.x[0], c = pfh.x[2] ),
                  curve_fitting.gaussian( a = pfh.x[3], b = pfh.x[0] + pfh.x[2] * GAIN_TO_SIGMA,
                                          c = pfh.x[2] * SIGMAFAC )]

      value = per_pixel_helper()
      return value


if __name__ == '__main__':
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/xes_finalise.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import os
import glob

from libtbx import easy_pickle
from scitbx.array_family import flex
import scitbx.matrix
from xfel.cxi.cspad_ana import cspad_tbx
from math import sin, atan
from six.moves import zip

def cspad_unbound_pixel_mask():
  # Every 10th pixel along the diagonal from the top left hand corner are not
  # bonded, hence we ignore them in the summed spectrum
  mask = flex.int(flex.grid(370, 391), 0)
  for section_offset in ((0,0), (0, 197), (185, 197), (185, 0)):
    for i in range(19):
      mask[section_offset[0] + i * 10, section_offset[1] + i * 10] = 1
  return mask

def cspad2x2_bad_pixel_mask_cxi_run7():
  bad_pixels = set((
    (279,289), (277,254), (273,295), (268,247), (269,260), (281,383), (268,57),
    (262,184), (291,206), (275,106), (261,28), (286,189), (265,304), (284,277)))
  mask = flex.int(flex.grid(370, 391), 0)
  for bad_pixel in bad_pixels:
    mask[bad_pixel] = 1
  return mask


class xes_finalise(object):

  def __init__(self,
               runs,
               output_dirname=".",
               roi=None):
    avg_basename="avg_"
    stddev_basename="stddev"
    self.sum_img = None
    self.sumsq_img = None
    self.nmemb = 0
    self.roi = cspad_tbx.getOptROI(roi)
    self.unbound_pixel_mask = cspad_unbound_pixel_mask()
    for i_run, run in enumerate(runs):
      run_scratch_dir = run
      result = finalise_one_run(run_scratch_dir)
      if result.sum_img is None: continue
      if self.sum_img is None:
        self.sum_img = result.sum_img
        self.sumsq_img = result.sumsq_img
      else:
        self.sum_img += result.sum_img
        self.sumsq_img += result.sumsq_img
      self.nmemb += result.nmemb

    self.avg_img = self.sum_img.as_double() / self.nmemb
    self.stddev_img = flex.sqrt((self.sumsq_img.as_double() - self.sum_img.as_double() * self.avg_img) / (self.nmemb - 1))

    self.mask = flex.int(self.sum_img.accessor(), 0)
    self.mask.set_selected(self.sum_img == 0, 1)
    self.mask.set_selected(self.unbound_pixel_mask > 0, 1)

    if (output_dirname is not None and
        avg_basename is not None):
      if (not os.path.isdir(output_dirname)):
        os.makedirs(output_dirname)
      d = cspad_tbx.dpack(
        address='CxiSc1-0|Cspad2x2-0',
        data=self.avg_img,
        distance=1,
      )
      cspad_tbx.dwritef(d, output_dirname, avg_basename)
      d = cspad_tbx.dpack(
        address='CxiSc1-0|Cspad2x2-0',
        data=self.sum_img,
        distance=1,
      )
      cspad_tbx.dwritef(d, output_dirname, "sum_")
      if 1:
        output_image(self.avg_img, "%s/avg.png" %output_dirname)
        output_image(self.avg_img, "%s/avg_inv.png" %output_dirname, invert=True)

      if 1:
        output_matlab_form(self.sum_img, "%s/sum.m" %output_dirname)
        output_matlab_form(self.avg_img, "%s/avg.m" %output_dirname)
        output_matlab_form(self.stddev_img, "%s/stddev.m" %output_dirname)

    if (stddev_basename is not None):
      d = cspad_tbx.dpack(
        address='CxiSc1-0|Cspad2x2-0',
        data=self.stddev_img,
        distance=1,
      )
      cspad_tbx.dwritef(d, output_dirname, stddev_basename)

      # XXX we should really figure out automatically the area where the spectrum is
      #write an integrated spectrum from lines 186-227
      #spectrum_focus = self.sum_img.as_numpy_array()[186:228,:]
      img = self.sum_img
      if self.roi is None:
        spectrum_focus = img
        mask_focus = self.mask
      else:
        slices = (slice(self.roi[2],self.roi[3]), slice(self.roi[0],self.roi[1]))
        spectrum_focus = img[slices]
        mask_focus = self.mask[slices]
      if False:
        from matplotlib import pylab
        pylab.imshow(spectrum_focus.as_numpy_array())
        pylab.show()

    output_spectrum(spectrum_focus, mask_focus=mask_focus,
                    output_dirname=output_dirname)

    print("Total number of images used from %i runs: %i" %(i_run+1, self.nmemb))

def filter_outlying_pixels(spectrum_focus, mask_focus):
  for i in range(spectrum_focus.all()[0]):
    for j in range(spectrum_focus.all()[1]):
      neighbouring_pixels = flex.double()
      for ii in range(i-1, i+2):
        for jj in range(j-1, j+2):
          if ii == i and jj == j: continue
          try:
            if mask_focus[ii, jj] > 0: continue
            neighbouring_pixels.append(spectrum_focus[ii, jj])
          except IndexError:
            continue
      if len(neighbouring_pixels) == 0: continue
      if spectrum_focus[i, j] > 2 * flex.mean(neighbouring_pixels):
        print("Bad pixel: (%i, %i)" %(i, j))
        spectrum_focus[i, j] = 0
        mask_focus[i,j] = 1

def get_spectrum(spectrum_focus, mask_focus=None):
  spectrum = flex.sum(spectrum_focus, axis=0).as_double()
  # take care of columns where one or more pixels are inactive
  # and/or flagged as a "hot" - in this case the sum is over fewer rows and
  # will introduce artefacts into the spectrum
  if 1 and mask_focus is not None:
    # estimate values for bad pixels as the mean value of
    # neighbouring pixels
    for j in range(spectrum_focus.all()[1]):
      for i in range(spectrum_focus.all()[0]):
        if mask_focus[i,j] > 0:
          neighbouring_pixels = flex.double()
          for ii in range(i-1, i+2):
            for jj in range(j-1, j+2):
              if ii == i and jj == j: continue
              try:
                if mask_focus[ii, jj] > 0: continue
                neighbouring_pixels.append(spectrum_focus[ii, jj])
              except IndexError:
                continue
          if len(neighbouring_pixels) == 0: continue
          spectrum[j] += flex.mean(neighbouring_pixels)
          print("bad pixel at (%i,%i): estimating value from neighbouring pixels: %.0f" %(
            i,j, flex.mean(neighbouring_pixels)))

  if 0 and mask_focus is not None:
    # Upweight columns that contain bad pixels based on the
    # relative strength of the signal on the rows containing
    # bad pixels
    scales = flex.sum(spectrum_focus, axis=1).as_double()
    scales /= flex.sum(scales)
    for j in range(spectrum_focus.all()[1]):
      sum_column_weights = 0
      for i in range(spectrum_focus.all()[0]):
        if mask_focus[i,j] == 0:
          sum_column_weights += scales[i]
      if sum_column_weights < 1:
        print("pixels missing from column %i" %j)
        print(sum_column_weights)
        if sum_column_weights > 0:
          spectrum[j] *= (1/sum_column_weights)

  # Elongated pixels span the gap between a pair of ASICs.
  # These are 2.5x the width of normal pixels, consequently by excluding these
  # pixels we will have a 5 pixel gap in our spectrum.
  # For further details see:
  #   https://confluence.slac.stanford.edu/download/attachments/112107361/PixelPitch.pdf
  omit_col = True
  if omit_col is True:
    #omit_columns = [181,193,194,195,196,197,378] # run 4
    omit_columns = set([193,194,195,196,197]) # run 5
    for column_i in range(1, spectrum.size()-1):
      if (column_i in omit_columns
          or column_i+1 in omit_columns
          or column_i-1 in omit_columns): continue
    plot_x = flex.int(range(spectrum.size()))
    plot_y = spectrum.deep_copy()
    for i in reversed(sorted(omit_columns)):
      del plot_x[i]
      del plot_y[i]
    plot_x += 1
  else:
    plot_x = flex.double(range(1,len(spectrum)+1))
    plot_y = spectrum

  return plot_x, plot_y

#Written by Muhamed Amin
def plot_energy(plot_x):
    plot_E=[x * 0.1109 for x in plot_x]
    plot_E1=[(y/2)-(95*0.1109) for y in plot_E]
    E=[1.2398e+004/(2*0.9601*sin(atan(500/(z+50)))) for z in plot_E1]
    return E

def output_spectrum(spectrum_focus, mask_focus=None, output_dirname=".", run=None):
  plot_x, plot_y = get_spectrum(spectrum_focus, mask_focus=mask_focus)
  if run is not None:  runstr = "_%04d"%run
  else: runstr=""
  spec_plot(plot_x,plot_y,spectrum_focus,
            os.path.join(output_dirname, "spectrum%s"%runstr)+ ".png")
  plot_E=plot_energy(plot_x)
  spec_plot(plot_E,plot_y,spectrum_focus,
            os.path.join(output_dirname, "spectrum_E%s"%runstr)+ ".png")
  f = open(os.path.join(output_dirname, "spectrum%s.txt"%runstr), "wb")
  print("\n".join(["%i %f" %(x, y) for x, y in zip(plot_x, plot_y)]), file=f)
  f.close()

  return plot_x, plot_y

  ## first moment analysis
  ## XXX columns of interest for CXI run 5
  #numerator = 0
  #denominator = 0
  #for i in range(150, 270):
    #numerator += spectrum[i] * i
    #denominator += spectrum[i]
  #first_moment = numerator/denominator
  #print "first moment: ", first_moment

def output_matlab_form(flex_matrix, filename):
  f = open(filename, "wb")
  #print >> f, "%% number of images = %i" %(self.nmemb)
  print(scitbx.matrix.rec(
    flex_matrix, flex_matrix.focus()).matlab_form(one_row_per_line=True), file=f)
  f.close()

def output_image(flex_img, filename, invert=False, scale=False):
  try:
    import PIL.Image as Image
  except ImportError:
    import Image
  flex_img = flex_img.deep_copy()
  flex_img -= flex.min(flex_img)
  if scale:
    img_max_value = 2**16
    scale = img_max_value/flex.max(flex_img)
    flex_img = flex_img.as_double() * scale
    flex_img = flex_img
  if invert:
    img_max_value = 2**16
    flex_img = img_max_value - flex_img # invert image for display
  dim = flex_img.all()
  #easy_pickle.dump("%s/avg_img.pickle" %output_dirname, flex_img)
  byte_str = flex_img.slice_to_byte_str(0,flex_img.size())
  try:
    im = Image.fromstring(mode="I", size=(dim[1],dim[0]), data=byte_str)
  except NotImplementedError:
    im = Image.frombytes(mode="I", size=(dim[1],dim[0]), data=byte_str)
  im = im.crop((0,185,391,370))
  #im.save("avg.tiff", "TIFF") # XXX This does not work (phenix.python -Qnew option)
  im.save(filename, "PNG")

class finalise_one_run(object):

  def __init__(self, scratch_dir):
    pickle_dirname = "pickle"
    pickle_basename = "pkl_"
    self.sum_img = None
    self.sumsq_img = None
    self.nmemb = 0
    path_pattern = "%s/%s/%ss[0-9][0-9]-[0-9].pickle" %(
      scratch_dir, pickle_dirname, pickle_basename)
    g = glob.glob(path_pattern)
    if len(g) == 0:
      print("No files found matching pattern: %s" %path_pattern)
      return
    for path in g:
      try:
        d = easy_pickle.load(file_name=path)
      except EOFError:
        print("EOFError: skipping %s:" %path)
        continue
      if self.sum_img is None:
        self.sum_img = d["sum_img"]
        self.sumsq_img = d["sumsq_img"]
      else:
        self.sum_img += d["sum_img"]
        self.sumsq_img += d["sumsq_img"]
      self.nmemb += d["nmemb"]
      print("Read %d images from %s" % (d["nmemb"], path))
      #print "Si foil length: %s" %(d["sifoil"])

    print("Number of images used: %i" %self.nmemb)
    assert self.nmemb > 0

class first_moment_analysis:
  def __init__(self, x, y):
    self.x = flex.double(list(x)) # sometimes integer array must be coerced to double
    self.y = y
    self.calculate()

  def calculate(self):
    self.first_moment=self.x[0]
    pass
    return

  def as_trace(self):
    return (self.first_moment,self.first_moment), (flex.min(self.y), flex.max(self.y))

def spec_plot(x, y, img, file_name, figure_size=(10,5), transparent=False):
  import matplotlib
  import matplotlib.figure
  import matplotlib.cm
  from matplotlib.backends.backend_agg import FigureCanvasAgg
  figure_size = None

  F = first_moment_analysis(x,y)

  fig = matplotlib.figure.Figure(figure_size, 144, linewidth=0,
      facecolor="white")
  if transparent :
    self.figure.figurePatch.set_alpha(0.0)
  canvas = FigureCanvasAgg(fig)
  p = fig.add_subplot(211)
  p.set_position([0.1,0.3,0.8,0.6])
  p.plot(x, y, '-')
  fm = F.as_trace()
  p.plot(fm[0],fm[1],"r-")
  p.set_xlim(x[0],x[-1])
  p.set_xlabel("position")
  p.set_ylabel("intensity")
  p.set_title("X-ray emission spectrum, first moment=%.2f"%(F.first_moment))
  p2 = fig.add_subplot(212)
  p2.set_position([0.1, 0.05, 0.8, 0.2])
  im=p2.imshow(img.as_numpy_array(), cmap='spectral')
  im.set_interpolation("none")
  position=fig.add_axes([0.91,0.1,0.015,0.35])
#  p2.imshow(img.as_numpy_array(), cmap=matplotlib.cm.gist_yarg)
  p3=fig.colorbar(im, orientation='vertical', cax=position)
#  p2.set_position([0.1, 0.05, 0.8, 0.2])
  canvas.draw()
  fig.savefig(file_name, dpi=200, format="png")


if __name__ == '__main__':
  import sys
  args = sys.argv[1:]
  assert len(args) >= 2
  scratch_dir = args[0]
  runs = args[1:]
  mod_xes_mp_finalise(scratch_dir, runs)


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cspad_ana/xes_histograms.py
from __future__ import absolute_import, division, print_function
import os
import sys

from libtbx import easy_mp
from libtbx import easy_pickle
import iotbx.phil
from scitbx.array_family import flex
import scitbx.math

from xfel.command_line import view_pixel_histograms # XXX
from xfel.cxi.cspad_ana import cspad_tbx
from xfel.cxi.cspad_ana import xes_finalise
from six.moves import range
from six.moves import zip


master_phil_str = """
xes {
  output_dirname = None
    .type = path
    .help = Directory for output files.
  roi = None
    .type = str
    .help = Region of interest for signal.
  bg_roi = None
    .type = str
    .help = Region of interest for background.
  gain_map = None
    .type = path
    .help = "Path to a gain map that will be used instead of"
            "fitting the one photon peak to estimate the gain."
  estimated_gain = 30
    .type = float
    .help = The approximate position of the one photon peak.
  nproc = Auto
    .type = int
    .help = Number of processors to use.
  photon_threshold = 2/3
    .type = float
    .help = "Threshold for counting photons (as a fraction of"
            "the distance between the zero and one photon peaks)."
  method = *photon_counting sum_adu
    .type = choice
    .help = "Method for summing up the individual images to obtain the final 2D"
            "spectrum. Either attempt to count individual photons, or sum up the"
            "ADU values for each pixel."
  run = None
    .type = int
    .help = The run number, used for output file names.
}
"""

master_phil = iotbx.phil.parse(master_phil_str)


def run(args):
  processed = iotbx.phil.process_command_line(
    args=args, master_string=master_phil_str)
  args = processed.remaining_args
  work_params = processed.work.extract().xes
  processed.work.show()
  assert len(args) == 1
  output_dirname = work_params.output_dirname
  roi = cspad_tbx.getOptROI(work_params.roi)
  bg_roi = cspad_tbx.getOptROI(work_params.bg_roi)
  gain_map_path = work_params.gain_map
  estimated_gain = work_params.estimated_gain
  nproc = work_params.nproc
  photon_threshold = work_params.photon_threshold
  method = work_params.method
  print(output_dirname)
  if output_dirname is None:
    output_dirname = os.path.join(os.path.dirname(args[0]), "finalise")
    print(output_dirname)
  hist_d = easy_pickle.load(args[0])
  if len(hist_d)==2:
    hist_d = hist_d['histogram']
  pixel_histograms = view_pixel_histograms.pixel_histograms(
    hist_d, estimated_gain=estimated_gain)
  result = xes_from_histograms(
    pixel_histograms, output_dirname=output_dirname,
    gain_map_path=gain_map_path, estimated_gain=estimated_gain,
    method=method, nproc=nproc,
    photon_threshold=photon_threshold, roi=roi, run=work_params.run)

  if bg_roi is not None:
    bg_outdir = os.path.normpath(output_dirname)+"_bg"
    bg_result = xes_from_histograms(
      pixel_histograms, output_dirname=bg_outdir,
      gain_map_path=gain_map_path, estimated_gain=estimated_gain,
      method=method, nproc=nproc,
      photon_threshold=photon_threshold, roi=bg_roi)

    from xfel.command_line.subtract_background import subtract_background
    signal = result.spectrum
    background = bg_result.spectrum
    signal = (signal[0].as_double(), signal[1])
    background = (background[0].as_double(), background[1])
    signal_x, background_subtracted = subtract_background(signal, background, plot=True)
    f = open(os.path.join(output_dirname, "background_subtracted.txt"), "wb")
    print("\n".join(["%i %f" %(x, y)
                           for x, y in zip(signal_x, background_subtracted)]), file=f)
    f.close()

  else:
    from xfel.command_line import smooth_spectrum
    from scitbx.smoothing import savitzky_golay_filter
    x, y = result.spectrum[0].as_double(), result.spectrum[1]
    x, y = smooth_spectrum.interpolate(x, y)
    x, y_smoothed = savitzky_golay_filter(
      x, y, 20, 4)
    smooth_spectrum.estimate_signal_to_noise(x, y, y_smoothed)


class xes_from_histograms(object):

  def __init__(self, pixel_histograms, output_dirname=".", gain_map_path=None,
               gain_map=None, method="photon_counting", estimated_gain=30,
               nproc=None, photon_threshold=2/3, roi=None,run=None):
    assert method in ("sum_adu", "photon_counting")
    self.sum_img = flex.double(flex.grid(370,391), 0) # XXX define the image size some other way?
    gain_img = flex.double(self.sum_img.accessor(), 0)

    assert [gain_map, gain_map_path].count(None) > 0
    if gain_map_path is not None:
      d = easy_pickle.load(gain_map_path)
      gain_map = d["DATA"]

    two_photon_threshold = photon_threshold + 1

    mask = flex.int(self.sum_img.accessor(), 0)

    start_row = 370
    end_row = 0
    print(len(pixel_histograms.histograms))

    pixels = list(pixel_histograms.pixels())
    n_pixels = len(pixels)
    if roi is not None:
      for k, (i, j) in enumerate(reversed(pixels)):
        if (   i < roi[2]
            or i > roi[3]
            or j < roi[0]
            or j > roi[1]):
          del pixels[n_pixels-k-1]

    if gain_map is None:
      fixed_func = pixel_histograms.fit_one_histogram
    else:
      def fixed_func(pixel):
        return pixel_histograms.fit_one_histogram(pixel, n_gaussians=1)
    results = None
    if nproc is None: nproc = easy_mp.Auto
    nproc = easy_mp.get_processes(nproc)
    print("nproc: ", nproc)

    stdout_and_results = easy_mp.pool_map(
      processes=nproc,
      fixed_func=fixed_func,
      args=pixels,
      func_wrapper="buffer_stdout_stderr")
    results = [r for so, r in stdout_and_results]

    gains = flex.double()

    for i, pixel in enumerate(pixels):
      start_row = min(start_row, pixel[0])
      end_row = max(end_row, pixel[0])
      n_photons = 0
      if results is None:
        # i.e. not multiprocessing
        try:
          gaussians = pixel_histograms.fit_one_histogram(pixel)
        except RuntimeError as e:
          print("Error fitting pixel %s" %str(pixel))
          print(str(e))
          mask[pixel] = 1
          continue
      else:
        gaussians = results[i]
      hist = pixel_histograms.histograms[pixel]
      if gaussians is None:
        # Presumably the peak fitting failed in some way
        print("Skipping pixel %s" %str(pixel))
        continue
      zero_peak_diff = gaussians[0].params[1]
      if gain_map is None:
        try:
          view_pixel_histograms.check_pixel_histogram_fit(hist, gaussians)
        except view_pixel_histograms.PixelFitError as e:
          print("PixelFitError:", str(pixel), str(e))
          mask[pixel] = 1
          continue
        gain = gaussians[1].params[1] - gaussians[0].params[1]
        gain_img[pixel] = gain
        gain_ratio = gain/estimated_gain
      else:
        gain = gain_map[pixel]
        if gain == 0:
          print("bad gain!!!!!", pixel)
          continue
        gain = 30/gain
        gain_ratio = 1/gain
      gains.append(gain)

      #for g in gaussians:
        #sigma = abs(g.params[2])
        #if sigma < 1 or sigma > 10:
          #print "bad sigma!!!!!", pixel, sigma
          #mask[pixel] = 1
          #continue
      if method == "sum_adu":
        sum_adu = 0
        one_photon_cutoff, two_photon_cutoff = [
          (threshold * gain + zero_peak_diff)
          for threshold in (photon_threshold, two_photon_threshold)]
        i_one_photon_cutoff = hist.get_i_slot(one_photon_cutoff)
        slots = hist.slots().as_double()
        slot_centers = hist.slot_centers()
        slots -= gaussians[0](slot_centers)
        for j in range(i_one_photon_cutoff, len(slots)):
          center = slot_centers[j]
          sum_adu += slots[j] * (center - zero_peak_diff) * 30/gain

        self.sum_img[pixel] = sum_adu
      elif method == "photon_counting":
        one_photon_cutoff, two_photon_cutoff = [
          (threshold * gain + zero_peak_diff)
          for threshold in (photon_threshold, two_photon_threshold)]
        i_one_photon_cutoff = hist.get_i_slot(one_photon_cutoff)
        i_two_photon_cutoff = hist.get_i_slot(two_photon_cutoff)
        slots = hist.slots()
        for j in range(i_one_photon_cutoff, len(slots)):
          if j == i_one_photon_cutoff:
            center = hist.slot_centers()[j]
            upper = center + 0.5 * hist.slot_width()
            n_photons += int(round((upper - one_photon_cutoff)/hist.slot_width() * slots[j]))
          elif j == i_two_photon_cutoff:
            center = hist.slot_centers()[j]
            upper = center + 0.5 * hist.slot_width()
            n_photons += 2 * int(round((upper - two_photon_cutoff)/hist.slot_width() * slots[j]))
          elif j < i_two_photon_cutoff:
            n_photons += int(round(slots[j]))
          else:
            n_photons += 2 * int(round(slots[j]))
        self.sum_img[pixel] = n_photons

    stats = scitbx.math.basic_statistics(gains)
    print("gain statistics:")
    stats.show()

    mask.set_selected(self.sum_img == 0, 1)
    unbound_pixel_mask = xes_finalise.cspad_unbound_pixel_mask()
    mask.set_selected(unbound_pixel_mask > 0, 1)
    bad_pixel_mask = xes_finalise.cspad2x2_bad_pixel_mask_cxi_run7()
    mask.set_selected(bad_pixel_mask > 0, 1)

    for row in range(self.sum_img.all()[0]):
      self.sum_img[row:row+1,:].count(0)

    spectrum_focus = self.sum_img[start_row:end_row,:]
    mask_focus = mask[start_row:end_row,:]

    spectrum_focus.set_selected(mask_focus > 0, 0)

    xes_finalise.filter_outlying_pixels(spectrum_focus, mask_focus)

    print("Number of rows: %i" %spectrum_focus.all()[0])
    print("Estimated no. photons counted: %i" %flex.sum(spectrum_focus))
    print("Number of images used: %i" %flex.sum(
      pixel_histograms.histograms.values()[0].slots()))

    d = cspad_tbx.dpack(
      address='CxiSc1-0|Cspad2x2-0',
      data=spectrum_focus,
      distance=1,
      ccd_image_saturation=2e8, # XXX
    )
    if run is not None: runstr="_%04d"%run
    else: runstr=""
    cspad_tbx.dwritef(d, output_dirname, 'sum%s_'%runstr)

    if gain_map is None:
      gain_map = flex.double(gain_img.accessor(), 0)
      img_sel = (gain_img > 0).as_1d()
      d = cspad_tbx.dpack(
        address='CxiSc1-0|Cspad2x2-0',
        data=gain_img,
        distance=1
      )
      cspad_tbx.dwritef(d, output_dirname, 'raw_gain_map_')
      gain_map.as_1d().set_selected(img_sel.iselection(), 1/gain_img.as_1d().select(img_sel))
      gain_map /= flex.mean(gain_map.as_1d().select(img_sel))
      d = cspad_tbx.dpack(
        address='CxiSc1-0|Cspad2x2-0',
        data=gain_map,
        distance=1
      )
      cspad_tbx.dwritef(d, output_dirname, 'gain_map_')

    plot_x, plot_y = xes_finalise.output_spectrum(
      spectrum_focus.iround(), mask_focus=mask_focus,
      output_dirname=output_dirname, run=run)
    self.spectrum = (plot_x, plot_y)
    self.spectrum_focus = spectrum_focus

    xes_finalise.output_matlab_form(spectrum_focus, "%s/sum%s.m" %(output_dirname,runstr))
    print(output_dirname)

if __name__ == '__main__':
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/cxi/cxi_cc.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# $Id$

from __future__ import absolute_import, division, print_function
from six.moves import range

import math
from libtbx.str_utils import format_value
from iotbx import mtz
from cctbx import crystal
from scitbx.array_family import flex
from cctbx.miller import binned_data

# A possible development path 9/11/2015 NKS
# can we first filter on the outliers to avoid outliers in the I - <I> plot and check that in
# then figure out what our target function is: delta or SpSig?
# figure out what happens to these statistics when confronted with known normal distributions
# thus settling the issues of factors of two and root(two)
# then figure out some empirical correction that fits the plot.  Doesn't have to be sdfac/sdb/adadd just has to be smooth.

def scale_factor(self, other, weights=None, cutoff_factor=None,
                   use_binning=False):
    """
    The analytical expression for the least squares scale factor.

    K = sum(w * yo * yc) / sum(w * yc^2)

    If the optional cutoff_factor argument is provided, only the reflections
    whose magnitudes are greater than cutoff_factor * max(yo) will be included
    in the calculation.
    """
    assert not use_binning or self.binner() is not None
    if use_binning: assert cutoff_factor is None
    assert other.size() == self.data().size()
    if not use_binning:
      if self.data().size() == 0: return None
      obs = self.data()
      calc = other.data()
      if cutoff_factor is not None:
        assert cutoff_factor < 1
        sel = obs >= flex.max(self.data()) * cutoff_factor
        obs = obs.select(sel)
        calc = calc.select(sel)
        if weights is not None:
          weights = weights.select(sel)
      if weights is None:
        return flex.sum(obs*calc) / flex.sum(flex.pow2(calc))
      else:
        return flex.sum(weights * obs * calc) \
             / flex.sum(weights * flex.pow2(calc))
    results = []
    for i_bin in self.binner().range_all():
      sel = self.binner().selection(i_bin)
      weights_sel = None
      if weights is not None:
        weights_sel = weights.select(sel)
      results.append(
        scale_factor(self.select(sel),other.select(sel), weights_sel))
    return binned_data(binner=self.binner(), data=results, data_fmt="%7.4f")

def split_sigma_test(self, other, scale, use_binning=False, show_plot=False):
  """
  Calculates the split sigma ratio test by Peter Zwart:
  ssr = sum( (Iah-Ibh)^2 ) / sum( sigma_ah^2 + sigma_bh^2)

  where Iah and Ibh are merged intensities for a given hkl from two halves of
  a dataset (a and b). Likewise for sigma_ah and sigma_bh.

  ssr (split sigma ratio) should approximately equal 1 if the errors are correctly estimated.
  """

  assert other.size() == self.data().size()
  assert (self.indices() == other.indices()).all_eq(True)
  assert not use_binning or self.binner() is not None

  if use_binning:
    results = []
    for i_bin in self.binner().range_all():
      sel = self.binner().selection(i_bin)
      i_self = self.select(sel)
      i_other = other.select(sel)
      scale_rel = scale.data[i_bin]
      if i_self.size() == 0:
        results.append(None)
      else:
        results.append(split_sigma_test(i_self,i_other,scale=scale_rel,show_plot=show_plot))
    return binned_data(binner=self.binner(), data=results, data_fmt="%7.4f")

  a_data = self.data(); b_data = scale * other.data()
  a_sigmas = self.sigmas(); b_sigmas = scale * other.sigmas()

  if show_plot:
    """
    # Diagnostic use of the (I - <I>) / sigma distribution, should have mean=0, std=1
    a_variance = a_sigmas * a_sigmas
    b_variance = b_sigmas * b_sigmas
    mean_num = (a_data/ (a_variance) ) + (b_data/ (b_variance) )
    mean_den = (1./ (a_variance) ) + (1./ (b_variance) )
    mean_values = mean_num / mean_den

    delta_I_a = a_data - mean_values
    normal_a = delta_I_a / (a_sigmas)
    stats_a = flex.mean_and_variance(normal_a)
    print "\nA mean %7.4f std %7.4f"%(stats_a.mean(),stats_a.unweighted_sample_standard_deviation())
    order_a = flex.sort_permutation(normal_a)

    delta_I_b = b_data - mean_values
    normal_b = delta_I_b / (b_sigmas)
    stats_b = flex.mean_and_variance(normal_b)
    print "B mean %7.4f std %7.4f"%(stats_b.mean(),stats_b.unweighted_sample_standard_deviation())
    order_b = flex.sort_permutation(normal_b)
    # plots for debugging
    from matplotlib import pyplot as plt
    plt.plot(range(len(order_a)),normal_a.select(order_a),"b.")
    plt.plot(range(len(order_b)),normal_b.select(order_b),"r.")
    plt.show()
    """
    from cctbx.examples.merging.sigma_correction import ccp4_model
    Correction = ccp4_model()
    Correction.plots(a_data, b_data, a_sigmas, b_sigmas)
    #a_new_variance,b_new_variance = Correction.optimize(a_data, b_data, a_sigmas, b_sigmas)
    #Correction.plots(a_data, b_data, flex.sqrt(a_new_variance), flex.sqrt(b_new_variance))

  n = flex.pow(a_data - b_data,2)
  d = flex.pow(a_sigmas,2)+flex.pow(b_sigmas,2)

  return flex.sum(n)/flex.sum(d)

def r1_factor(self, other, scale_factor=None, assume_index_matching=False,
                use_binning=False):
    r"""Get the R1 factor according to this formula

    .. math::
       R1 = \dfrac{\sum{||F| - k|F'||}}{\sum{|F|}}

    where F is self.data() and F' is other.data() and
    k is the factor to put F' on the same scale as F"""
    assert not use_binning or self.binner() is not None
    assert other.indices().size() == self.indices().size()
    if not use_binning:
      if self.data().size() == 0: return None
      if (assume_index_matching):
        o, c = self, other
      else:
        o, c = self.common_sets(other=other, assert_no_singles=True)
      o  = flex.abs(o.data())
      c = flex.abs(c.data())
      if (scale_factor is None):
        den = flex.sum(c * c)
        if (den != 0):
          c *= (flex.sum(o * c) / den)
      elif (scale_factor is not None):
        c *= scale_factor
      return flex.sum(flex.abs(o - c)) / flex.sum(o)
    results = []
    for i_bin in self.binner().range_all():
      sel = self.binner().selection(i_bin)
      results.append(r1_factor(self.select(sel),
        other.select(sel), scale_factor.data[i_bin], assume_index_matching))
    return binned_data(binner=self.binner(), data=results, data_fmt="%7.4f")

def r_split(self, other, assume_index_matching=False, use_binning=False):
    # Used in Boutet et al. (2012), which credit it to Owen et al
    # (2006).  See also R_mrgd_I in Diederichs & Karplus (1997)?
    # Barends cites Collaborative Computational Project Number 4. The
    # CCP4 suite: programs for protein crystallography. Acta
    # Crystallogr. Sect. D-Biol. Crystallogr. 50, 760-763 (1994) and
    # White, T. A. et al. CrystFEL: a software suite for snapshot
    # serial crystallography. J. Appl. Cryst. 45, 335341 (2012).

    if not use_binning:
      assert other.indices().size() == self.indices().size()
      if self.data().size() == 0:
        return None

      if assume_index_matching:
        (o, c) = (self, other)
      else:
        (o, c) = self.common_sets(other=other, assert_no_singles=True)

      # The case where the denominator is less or equal to zero is
      # pathological and should never arise in practice.
      den = flex.sum(flex.abs(o.data() + c.data()))
      assert den > 0
      return math.sqrt(2) * flex.sum(flex.abs(o.data() - c.data())) / den

    assert self.binner is not None
    results = []
    for i_bin in self.binner().range_all():
      sel = self.binner().selection(i_bin)
      results.append(r_split(self.select(sel), other.select(sel),
        assume_index_matching=assume_index_matching,
        use_binning=False))
    return binned_data(binner=self.binner(), data=results, data_fmt='%7.4f')

def binned_correlation(self,other,include_negatives=False):
    results = []
    bin_count = []
    for i_bin in self.binner().range_all():
      sel = self.binner().selection(i_bin)
      if sel.count(True)==0:
        results.append(0.)
        bin_count.append(0.)
        continue
      result_tuple = correlation(self.select(sel),other.select(sel),include_negatives)
      results.append(result_tuple[2])
      bin_count.append(result_tuple[3])
      # plots for debugging
      #from matplotlib import pyplot as plt
      #plt.plot(flex.log(self.select(sel).data()),flex.log(other.select(sel).data()),"b.")
      #plt.show()

    return binned_data(binner=self.binner(), data=results, data_fmt="%7.4f"),\
           binned_data(binner=self.binner(), data=bin_count, data_fmt="%7d")

def correlation(self,other, include_negatives=False):
    N = 0
    sum_xx = 0
    sum_xy = 0
    sum_yy = 0
    sum_x = 0
    sum_y = 0
    for idx in range(self.indices().size()):

      assert self.indices()[idx]==other.indices()[idx]
      I_r = other.data()[idx]
      I_o = self.data()[idx]
      #assert I_r >= 0. or I_o >= 0.
      #why does this go from 81% to 33% when uniform selection is made?

      if not include_negatives and (I_r < 0. or I_o < 0.): continue
      #print "%15s %15s %10.0f %10.0f"%(
    #self.indices()[idx], self.indices()[idx],
    #self.data()[idx], other.data()[idx],)
      N      += 1
      sum_xx += I_r**2
      sum_yy += I_o**2
      sum_xy += I_r * I_o
      sum_x  += I_r
      sum_y  += I_o
    # Linearly fit I_r to I_o, i.e. find slope and offset such that
    # I_o = slope * I_r + offset, optimal in a least-squares sense.
    if N < 2:  return 0,0,0,N
    slope = (N * sum_xy - sum_x * sum_y) / (N * sum_xx - sum_x**2)
    offset = (sum_xx * sum_y - sum_x * sum_xy) / (N * sum_xx - sum_x**2)
    corr  = (N * sum_xy - sum_x * sum_y) / (math.sqrt(N * sum_xx - sum_x**2) *
             math.sqrt(N * sum_yy - sum_y**2))
    return slope,offset,corr,N

def load_cc_data(params,reindexing_op,output):
  if reindexing_op != "h,k,l":
    print("""Recalculating after reindexing the new data with %s
     (it is necessary to pick which indexing choice gives the sensible CC iso):"""%reindexing_op)
  try:
    data_SR = mtz.object(params.scaling.mtz_file)
    have_iso_ref = True
  except RuntimeError:
    data_SR = None
    have_iso_ref = False
  data_d0 = mtz.object(params.output.prefix+"_s0_"+params.scaling.algorithm+".mtz")
  data_d1 = mtz.object(params.output.prefix+"_s1_"+params.scaling.algorithm+".mtz")
  data_d2 = mtz.object(params.output.prefix+"_s2_"+params.scaling.algorithm+".mtz")

  uniform = []
  for idx,item in enumerate([data_SR,data_d0,data_d1,data_d2]):
    if not have_iso_ref and idx == 0:
      uniform.append(None)
      continue
    #item.show_summary()
    print("-------------------------------", file=output)
    for array in item.as_miller_arrays():
       this_label = array.info().label_string().lower()
       print(this_label, params.scaling.mtz_column_F)
       if this_label.find("fobs")>=0:
         print(this_label,array.observation_type(), file=output)
         uniform.append(array.as_intensity_array())
         break
       if this_label.find("iobs")>=0:
         """This test is added for the use case of unmerged anomalous data without
            an isomorphous reference (in other words, mark1).  Without this,
            the unmerged reflections are not picked up by the cc comparison.
            Indicates that this section probably has to be reanalyzed and redesigned.
         """
         print(this_label,array.observation_type(), file=output)
         uniform.append(array.as_intensity_array())
         break
       if this_label.find("imean")>=0:
         print(this_label,array.observation_type(), file=output)
         uniform.append(array.as_intensity_array())
         break
       if this_label.find(params.scaling.mtz_column_F)==0:
         print(this_label,array.observation_type(), file=output)
         uniform.append(array.as_intensity_array())
         break

  # If necesssary, generate Bijvoet mates for the isomorphous
  # reference.
  if have_iso_ref \
     and not params.merge_anomalous \
     and not uniform[0].anomalous_flag():
      uniform[0] = uniform[0].generate_bijvoet_mates()

  for x in [1,2,3]:
    # reindex the experimental data
    uniform[x] = uniform[x].change_basis(reindexing_op).map_to_asu()

  d_max_min = uniform[1].d_max_min()
  if have_iso_ref:
    sgi = uniform[0].space_group_info()
  else:
    sgi = params.target_space_group

  for x in [0,1,2,3]:
    if not have_iso_ref and x == 0:
      continue
    print("%6d indices:"%uniform[x].size(),{0:"Reference intensities",
                     1:"Merged structure factors",
                     2:"Semi-dataset 1",
                     3:"Semi-dataset 2"}[x], file=output)
    uniform[x] = uniform[x].customized_copy(
      crystal_symmetry = crystal.symmetry(unit_cell=uniform[1].unit_cell(),
                                          space_group_info=sgi),
      ).resolution_filter(d_min=uniform[1].d_min(),d_max=params.d_max,
      ).complete_array(d_min=uniform[1].d_min(),d_max=params.d_max).map_to_asu()
  print("%6d indices: An asymmetric unit in the resolution interval %.2f - %.2f Angstrom"%(
     uniform[1].size(),d_max_min[0],uniform[1].d_min()), file=output)

  if have_iso_ref:
    uniform[0] = uniform[0].common_set(uniform[1])
    assert len(uniform[0].indices()) == len(uniform[1].indices())
  uniform[2] = uniform[2].common_set(uniform[1])
  uniform[3] = uniform[3].common_set(uniform[1])
  print("-------------------------------", file=output)
  NBIN = params.output.n_bins
  for x in [0, 1, 2, 3]:
    if not have_iso_ref and x == 0:
      continue
    uniform[x].setup_binner(n_bins=NBIN)

  for x in range(len(uniform[1].indices())):
    if have_iso_ref:
      assert uniform[0].indices()[x] == uniform[1].indices()[x]
    assert uniform[1].indices()[x] == uniform[2].indices()[x]
    assert uniform[2].indices()[x] == uniform[3].indices()[x]

  cutoff = math.exp(params.scaling.log_cutoff or -100.)

  selected_uniform = []
  if have_iso_ref:
    if params.include_negatives:
      uniformA = (uniform[0].sigmas() > 0.).__and__(uniform[1].sigmas() > 0.)
    elif params.scaling.log_cutoff is None:
      uniformA = (uniform[0].data() > 0.).__and__(uniform[1].data() > 0.)
    else:
      uniformA = (uniform[0].data() > cutoff).__and__(uniform[1].data() > cutoff)
    for x in [0, 1]:
      selected_uniform.append(uniform[x].select(uniformA))
      selected_uniform[x].setup_binner(
        d_max=(params.d_max or 100000), d_min=params.d_min, n_bins=NBIN)

  else:
    selected_uniform = [None, None]

  if params.include_negatives:
      uniformB = (uniform[2].sigmas() > 0.).__and__(uniform[3].sigmas() > 0.)
  elif params.scaling.log_cutoff is None:
      uniformB = (uniform[2].data() > 0.).__and__(uniform[3].data() > 0.)
  else:
      uniformB = (uniform[2].data() > cutoff).__and__(uniform[3].data() > cutoff)

  for x in [2, 3]:
    selected_uniform.append(uniform[x].select(uniformB))
    selected_uniform[x].setup_binner(
      d_max=(params.d_max or 100000), d_min=params.d_min, n_bins=NBIN)

  return uniform, selected_uniform, have_iso_ref

def run_cc(params,reindexing_op,output):
  uniform, selected_uniform, have_iso_ref = load_cc_data(params, reindexing_op, output)
  NBIN = params.output.n_bins

  if have_iso_ref:
    slope, offset, corr_iso, N_iso = correlation(
      selected_uniform[1], selected_uniform[0], params.include_negatives)
    print("C.C. iso is %.1f%% on %d indices" % (
      100 * corr_iso, N_iso), file=output)

  slope,offset,corr_int,N_int = correlation(selected_uniform[2],selected_uniform[3], params.include_negatives)
  print("C.C. int is %.1f%% on %d indices"%(100.*corr_int, N_int), file=output)

  if have_iso_ref:
    binned_cc_ref, binned_cc_ref_N = binned_correlation(
      selected_uniform[1], selected_uniform[0], params.include_negatives)
    #binned_cc_ref.show(f=output)

    ref_scale = scale_factor(
      selected_uniform[1], selected_uniform[0],
      weights=flex.pow(selected_uniform[1].sigmas(), -2),
      use_binning=True)
    #ref_scale.show(f=output)

    ref_riso = r1_factor(
      selected_uniform[1], selected_uniform[0],
      scale_factor=ref_scale, use_binning=True)
    #ref_riso.show(f=output)

    ref_scale_all = scale_factor(
      selected_uniform[1], selected_uniform[0],
      weights=flex.pow(selected_uniform[1].sigmas(), -2))

    ref_riso_all = r1_factor(
      selected_uniform[1], selected_uniform[0],
      scale_factor=ref_scale_all)

  binned_cc_int,binned_cc_int_N = binned_correlation(
    selected_uniform[2], selected_uniform[3], params.include_negatives)
  #binned_cc_int.show(f=output)

  oe_scale = scale_factor(selected_uniform[2],selected_uniform[3],
    weights = flex.pow(selected_uniform[2].sigmas(),-2)
            + flex.pow(selected_uniform[3].sigmas(),-2),
    use_binning=True)
  #oe_scale.show(f=output)

  oe_rint = r1_factor(selected_uniform[2],selected_uniform[3],
                       scale_factor = oe_scale, use_binning=True)
  #oe_rint.show(f=output)

  oe_rsplit = r_split(selected_uniform[2], selected_uniform[3],
                      use_binning=True)

  oe_scale_all = scale_factor(selected_uniform[2],selected_uniform[3],
    weights = flex.pow(selected_uniform[2].sigmas(),-2)
            + flex.pow(selected_uniform[3].sigmas(),-2),)

  oe_rint_all = r1_factor(selected_uniform[2],selected_uniform[3],
                       scale_factor = oe_scale_all)
  oe_rsplit_all = r_split(selected_uniform[2], selected_uniform[3])
  if have_iso_ref:
    print("R factors Riso = %.1f%%, Rint = %.1f%%"%(100.*ref_riso_all, 100.*oe_rint_all), file=output)
  else:
    print("R factor Rint = %.1f%%"%(100.*oe_rint_all), file=output)

  split_sigma_data = split_sigma_test(selected_uniform[2],selected_uniform[3],
                                      scale=oe_scale,use_binning=True,show_plot=False)
  split_sigma_data_all = split_sigma_test(selected_uniform[2],selected_uniform[3],
                                          scale=oe_scale_all,use_binning=False,show_plot=False)

  print(file=output)
  if reindexing_op == "h,k,l":
    print("Table of Scaling Results:", file=output)
  else:
    print("Table of Scaling Results Reindexing as %s:"%reindexing_op, file=output)

  from libtbx import table_utils
  table_header = ["","","","CC"," N","CC"," N","R","R","R","Scale","Scale","SpSig"]
  table_header2 = ["Bin","Resolution Range","Completeness","int","int","iso","iso","int","split","iso","int","iso","Test"]
  table_data = []
  table_data.append(table_header)
  table_data.append(table_header2)

  items = binned_cc_int.binner.range_used()

  # XXX Make it clear what the completeness here actually is!
  cumulative_counts_given = 0
  cumulative_counts_complete = 0
  for bin in items:
    table_row = []
    table_row.append("%3d"%bin)
    table_row.append("%-13s"%binned_cc_int.binner.bin_legend(i_bin=bin,show_bin_number=False,show_bin_range=False,
                                                 show_d_range=True, show_counts=False))
    table_row.append("%13s"%binned_cc_int.binner.bin_legend(i_bin=bin,show_bin_number=False,show_bin_range=False,
                                                 show_d_range=False, show_counts=True))
    cumulative_counts_given += binned_cc_int.binner._counts_given[bin]
    cumulative_counts_complete += binned_cc_int.binner._counts_complete[bin]
    table_row.append("%.1f%%"%(100.*binned_cc_int.data[bin]))
    table_row.append("%7d"%(binned_cc_int_N.data[bin]))

    if have_iso_ref and binned_cc_ref.data[bin] is not None:
      table_row.append("%.1f%%" % (100 * binned_cc_ref.data[bin]))
    else:
      table_row.append("--")

    if have_iso_ref and binned_cc_ref_N.data[bin] is not None:
      table_row.append("%6d" % (binned_cc_ref_N.data[bin]))
    else:
      table_row.append("--")

    if oe_rint.data[bin] is not None:
      table_row.append("%.1f%%"%(100.*oe_rint.data[bin]))
    else:
      table_row.append("--")

    if oe_rsplit.data[bin] is not None:
      table_row.append("%.1f%%" % (100 * oe_rsplit.data[bin]))
    else:
      table_row.append("--")

    if have_iso_ref and ref_riso.data[bin] is not None:
      table_row.append("%.1f%%" % (100 * ref_riso.data[bin]))
    else:
      table_row.append("--")

    if oe_scale.data[bin] is not None:
      table_row.append("%.3f"%oe_scale.data[bin])
    else:
      table_row.append("--")

    if have_iso_ref and ref_scale.data[bin] is not None:
      table_row.append("%.3f" % ref_scale.data[bin])
    else:
      table_row.append("--")

    if split_sigma_data.data[bin] is not None:
      table_row.append("%.4f" % split_sigma_data.data[bin])
    else:
      table_row.append("--")

    table_data.append(table_row)
  table_data.append([""]*len(table_header))

  table_row = [format_value("%3s",   "All"),
               format_value("%-13s", "                 "),
               format_value("%13s",  "[%d/%d]"%(cumulative_counts_given,
                                                cumulative_counts_complete)),
               format_value("%.1f%%", 100 * corr_int),
               format_value("%7d", N_int)]

  if have_iso_ref:
    table_row.extend((format_value("%.1f%%", 100 * corr_iso),
                      format_value("%6d", N_iso)))
  else:
    table_row.extend(("--", "--"))

  table_row.extend((format_value("%.1f%%", 100 * oe_rint_all),
                    format_value("%.1f%%", 100 * oe_rsplit_all)))
  if have_iso_ref:
    table_row.append(format_value("%.1f%%", 100 * ref_riso_all))
  else:
    table_row.append("--")

  table_row.append(format_value("%.3f", oe_scale_all))
  if have_iso_ref:
    table_row.append(format_value("%.3f", ref_scale_all))
  else:
    table_row.append("--")

  if split_sigma_data_all is not None:
    table_row.append("%.1f" % split_sigma_data_all)
  else:
    table_row.append("--")

  table_data.append(table_row)

  print(file=output)
  print(table_utils.format(table_data,has_header=2,justify='center',delim=" "), file=output)
  print("""CCint is the CC-1/2 defined by Diederichs; correlation between odd/even images.
  Similarly, Scale int and R int are the scaling factor and scaling R factor between odd/even images.
  "iso" columns compare the whole XFEL dataset to the isomorphous reference.""", file=output)

  print("""Niso: result vs. reference common set""", end=' ', file=output)
  if params.include_negatives:
    print("""including negative merged intensities (set by phil parameter).""", file=output)
  elif params.scaling.log_cutoff is None:
    print(file=output)
  else:
    print("""with intensites < %7.2g filtered out (controlled by
    scaling.log_cutoff phil parameter set to %5.1f)"""%(math.exp(params.scaling.log_cutoff),
    params.scaling.log_cutoff), file=output)

  if have_iso_ref:
    assert N_iso == flex.sum(flex.double([x for x in binned_cc_ref_N.data if x is not None]))
  assert N_int == flex.sum(flex.double([x for x in binned_cc_int_N.data if x is not None]))

  if params.scaling.show_plots:
    from matplotlib import pyplot as plt
    plt.plot(flex.log(selected_uniform[-2].data()),
             flex.log(selected_uniform[-1].data()), 'r.')
    plt.show()
    if have_iso_ref:
      plt.plot(flex.log(selected_uniform[0].data()),
               flex.log(selected_uniform[1].data()), 'r.')
      plt.show()
  print(file=output)


 *******************************************************************************


 *******************************************************************************
xfel/cxi/data_utils.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import math
import sys
from dials.array_family import flex
from six.moves import cPickle as pickle
from rstbx.dials_core.integration_core import show_observations

class reduction(object):
  """Reduced data container.  Merges the following concepts:

  filename : the integration pickle
  experiment : the dxtbx-experimental model
  HKL : the original-index miller set taken from joined database
  """
  def __init__(self, filename, experiment, HKL, i_sigi, measurements, params):
    from libtbx import adopt_init_args
    adopt_init_args(self, locals())
    self.stash_type = None
    self.stash_res_filter = None

    from dxtbx.model import DetectorFactory
    self.dummy_detector = DetectorFactory.simple(
      sensor = DetectorFactory.sensor("PAD"),
      distance = 100,
      beam_centre = [1000, 1000],
      fast_direction = "+x",
      slow_direction = "+y",
      pixel_size = [0.2,0.2],
      image_size = [2000,2000],
      )
    # simple view of post-integration, no longer need to know detector

  def get_delta_psi_deg(self):
    from scitbx import matrix
    if self.stash_type is None:
       self.stash_type = self.experiment.crystal.get_space_group().type()
    UC = self.experiment.crystal.get_unit_cell()
    from dials.algorithms.spot_prediction \
      import StillsDeltaPsiReflectionPredictor
    S = StillsDeltaPsiReflectionPredictor(
      self.experiment.beam, \
      self.dummy_detector, \
      self.experiment.crystal.get_A(), \
      UC, \
      self.stash_type, \
      10.0) #dummy value for dmin

    length = len(self.HKL)
    R= flex.reflection_table.empty_standard(length)
    R['miller_index'] = self.HKL
    S.for_reflection_table(R,matrix.sqr(self.experiment.crystal.get_A()))
    degrees = (180./math.pi)*R["delpsical.rad"]
    return degrees

  def get_two_theta_deg(self):
    wavelength=self.experiment.beam.get_wavelength()
    UC = self.experiment.crystal.get_unit_cell()
    two_theta = UC.two_theta(
      miller_indices=self.HKL, wavelength=wavelength, deg=True)
    return two_theta

  def get_imposed_res_filter(self, out):
    if self.stash_res_filter is not None:  return self.stash_res_filter
    if self.params.significance_filter.apply is True: #------------------------------------

      print("Step 5. Frame by frame resolution filter", file=out)
      # Apply an I/sigma filter ... accept resolution bins only if they
      #   have significant signal; tends to screen out higher resolution observations
      #   if the integration model doesn't quite fit
      N_obs_pre_filter = self.i_sigi.size()
      N_bins_small_set = N_obs_pre_filter // self.params.significance_filter.min_ct
      N_bins_large_set = N_obs_pre_filter // self.params.significance_filter.max_ct

      # Ensure there is at least one bin.
      N_bins = max(
        [min([self.params.significance_filter.n_bins,N_bins_small_set]),
         N_bins_large_set, 1]
      )
      print("Total obs %d Choose n bins = %d"%(N_obs_pre_filter,N_bins), file=out)
      bin_results = show_observations(self.measurements, out=sys.stdout, n_bins=N_bins)

      if True: # no fuller kapton -- not implemented here,
               # but code can and should be borrowed from cxi.merge
        acceptable_resolution_bins = [
          bin.mean_I_sigI > self.params.significance_filter.sigma for bin in bin_results]
        acceptable_nested_bin_sequences = [i for i in range(len(acceptable_resolution_bins))
                                           if False not in acceptable_resolution_bins[:i+1]]
        N_acceptable_bins = max(acceptable_nested_bin_sequences) + 1
        imposed_res_filter = float(bin_results[N_acceptable_bins-1].d_range.split()[2])
        print("New resolution filter at %7.2f"%imposed_res_filter,self.filename, file=out)
        print("N acceptable bins",N_acceptable_bins, file=out)
      print("Old n_obs: %d, new n_obs: %d"%(N_obs_pre_filter,self.measurements.size()), file=out)
      # Finished applying the binwise I/sigma filter---------------------------------------
    else:
      imposed_res_filter=None
    self.stash_res_filter = imposed_res_filter
    return imposed_res_filter


 *******************************************************************************


 *******************************************************************************
xfel/cxi/display_powder_arcs.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import math
import mmtbx.programs.fmodel
import mmtbx.utils
import iotbx.pdb.fetch
from iotbx import pdb
from cctbx.array_family import flex


def is_pinwheel_region(dx,dy):
  angledeg = math.atan2(dy,dx)*180./math.pi
  return (30.<angledeg<60.) or (120.<angledeg<150.) or \
         (-30.>angledeg>-60.) or (-120.>angledeg>-150.)

def get_mmtbx_icalc(code,d_min, anomalous_flag = False):

    #pdbtools.run([file, "high_resolution=%f"%self.d_min, "label=f-obs-nks"
    #              "k_sol=0.35", "b_sol=60", "b_cart=1 2 -3 0 0 0", "--f_model", "r_free=0.1"])
    # get xray_structure

    pdb_url = iotbx.pdb.fetch.fetch(id=code)
    pdb_input = pdb.input(source_info=None,lines=pdb_url.readlines())

    xray_structure = pdb_input.xray_structure_simple()

    phil2 = mmtbx.programs.fmodel.master_phil
    params2 = phil2.extract()
    # adjust the cutoff of the generated intensities to assure that
    # statistics will be reported to the desired high-resolution limit
    # even if the observed unit cell differs slightly from the reference.
    ISO_ALLOWANCE = 0.1 # isomorphous recip cell volume changes no more than 10%
    params2.high_resolution = d_min / math.pow( (1.+ISO_ALLOWANCE),(1./3.) )
    params2.output.type = "real"
    if True :
      params2.fmodel.k_sol = 0.35
      params2.fmodel.b_sol = 46
    f_model = mmtbx.utils.fmodel_from_xray_structure(
      xray_structure = xray_structure,
      f_obs          = None,
      add_sigmas     = True,
      params         = params2).f_model
    i_model = f_model.as_intensity_array()


    return  i_model.map_to_asu()

def apply_gaussian_noise(image,params):
    from scitbx.random import variate,normal_distribution
    import numpy
    G = variate(normal_distribution(mean=2.0,sigma=0.5))
    gaussian_noise = flex.double(G(image.linearintdata.size()))
    #image.linearintdata += flex.int(gaussian_noise.as_numpy_array().astype(numpy.int32))
    image.linearintdata += gaussian_noise

def superimpose_powder_arcs(image,params):

    #put in some random noise to improve display
    image.read()
    apply_gaussian_noise(image,params)

    pxlsz = image.pixel_size
    distance = image.distance
    wavelength = image.wavelength
    eps = 0.01
    beamx = eps+image.beamx/pxlsz
    beamy = eps+image.beamy/pxlsz
    print(beamx,beamy)

    image.read()
    data = image.linearintdata
    detector_d = flex.double()
    detector_radius = flex.double()

    for x in range(data.focus()[0]):
      dx = x-beamx
      dx_sq = dx*dx;

      for y in range(data.focus()[1]):
        dy = y-beamy
        radius = math.sqrt(dx_sq+dy*dy)*pxlsz
        detector_radius.append(radius)
        theta = 0.5 * math.atan(radius/distance)
        resol_d = wavelength/(2.*math.sin(theta))

        if is_pinwheel_region(dx,dy):
          detector_d.append(resol_d)
        else:
          detector_d.append(0.0)
    print(detector_d.size())
    d_order_image = flex.sort_permutation(detector_d,reverse=True)

    #code = "2oh5"
    code = params.viewer.powder_arcs.code
    d_min = 2.0
    sf = get_mmtbx_icalc(code,d_min)
    sf.show_summary()

    cell = sf.unit_cell()
    millers = sf.indices()
    FUDGE_FACTOR = 0.02
    intensities = FUDGE_FACTOR*sf.data()
    spot_d = cell.d(millers)
    spot_two_theta = cell.two_theta(millers,wavelength=wavelength)
    spot_radius = distance * flex.tan(spot_two_theta)

    d_order_spots = flex.sort_permutation(spot_d,reverse=True)
    print(list(d_order_spots))

    spot_ptr_min = 0
    spot_ptr_max = 1

    THREESIGMA=1.5
    SIGMA = 0.20
    SCALE = 0.003
    for x in range(50000):#len(detector_d)):
      if x%10000==0: print(x)
      if detector_d[d_order_image[x]]>2.1:
        pxl_radius = detector_radius[d_order_image[x]]
        #print "pixel radius",pxl_radius,"resolution",detector_d[d_order_image[x]]
        if True:# data[d_order_image[x]]>0:
          data[d_order_image[x]]=0
          # increase spot_ptr_max???
          while spot_radius[d_order_spots[spot_ptr_max]]-THREESIGMA < pxl_radius:
            spot_ptr_max += 1
            print(x, pxl_radius,"increase spot_ptr_max" , spot_ptr_max, spot_radius[d_order_spots[spot_ptr_max]]-THREESIGMA)
          while spot_radius[d_order_spots[spot_ptr_min]]+THREESIGMA < pxl_radius:
            spot_ptr_min += 1
            print(x, pxl_radius,"increase spot_ptr_min" , spot_ptr_min)
          for sidx in range(spot_ptr_min,spot_ptr_max):
            spot_rad = spot_radius[d_order_spots[sidx]]
            delta_rad = spot_rad-pxl_radius
            #gaussian distribution with SIGMA=1 (1 pixel sigma)
            #divide through by spot_radius since energy has to be spread around increasing ring
            #  circumference
            #print "changing data ",x,d_order_image[x]
            data[d_order_image[x]] += int(SCALE*(1./(math.sqrt(2.*math.pi*SIGMA*SIGMA)))*math.exp(
              -0.5*delta_rad*delta_rad/(SIGMA*SIGMA))*intensities[d_order_spots[sidx]]/spot_rad)


 *******************************************************************************


 *******************************************************************************
xfel/cxi/display_spots.py
# -*- mode: python; coding: utf-8; indent-tabs-mode: nil; python-indent: 2 -*-
#
# $Id$

from __future__ import absolute_import, division, print_function
from six.moves import range

import os, sys, copy
from iotbx.detectors.npy import NpyImage

class empty:
  pass

parameters=empty()

def ImageFactory(filename):
  global parameters
  if type(filename)==type("") and os.path.isfile(filename):
    from dxtbx.format.FormatPYunspecified import FormatPYunspecified

    assert FormatPYunspecified.understand(filename)
    I = FormatPYunspecified(filename).get_detectorbase()

  else:
    print("This is not a file; assume the data are in the defined dictionary format")
    I = NpyImage(filename, source_data=parameters.horizons_phil.indexing.data)
  I.readHeader(parameters.horizons_phil)

  I.translate_tiles(parameters.horizons_phil)

  #from xfel.cxi.display_powder_arcs import apply_gaussian_noise
  #apply_gaussian_noise(I,parameters.horizons_phil)
  if (parameters.horizons_phil.viewer.powder_arcs.show):
    from xfel.cxi.display_powder_arcs import superimpose_powder_arcs
    superimpose_powder_arcs(I,parameters.horizons_phil)
  return I


from iotbx import detectors
detectors.ImageFactory = ImageFactory

from spotfinder.diffraction.imagefiles import FileName
FileName.exts.append("pickle")

from spotfinder.applications.xfel import cxi_phil

show_bg_boxes=True
class wrapper_of_callback(object):
  def __init__(self, info=None):
    if info != None:
      self.spotfinder = info.S
      self.frames     = info.frames
      self.Files      = info.Files
      self.phil_params= info.phil_params

  def display(self,path):
    import wx
    from rstbx.viewer.frame import XrayFrame

    app   = wx.App(0)
    frame = XrayFrame(None, -1, "X-ray image display", size=(1200,1080))
    frame.SetSize((1024,780))
    frame.load_image(path)
    frame.Show()
    app.MainLoop()

  def display_with_callback(self,path):
    from rstbx.viewer       import display
    display.user_callback = self.user_callback
    self.display(path)

  def user_callback(self,dc,wxpanel,wx):
    # arguments are a wx Device Context, an Xray Frame, and the wx Module itself
    """
    for spot in self.spotfinder.images[self.frames[0]]["spots_total"]:
      for pxl in spot.bodypixels:
        x,y = wxpanel._img.image_coords_as_screen_coords(
          pxl.y,
          pxl.x)
        dc.SetPen(wx.Pen('red'))
        dc.SetBrush(wx.RED_BRUSH)
        dc.DrawCircle(x,y,1)

      x,y = wxpanel._img.image_coords_as_screen_coords(
        spot.ctr_mass_y(),
        spot.ctr_mass_x())
      dc.SetPen(wx.Pen('green'))
      dc.SetBrush(wx.GREEN_BRUSH)
      dc.DrawCircle(x,y,1)
    """
    """
    for spot in self.spotfinder.images[self.frames[0]]["lo_pass_resolution_spots"]:
      for pxl in spot.bodypixels:
        x,y = wxpanel._img.image_coords_as_screen_coords(
          pxl.y,
          pxl.x)
        dc.SetPen(wx.Pen('yellow'))
        dc.DrawCircle(x,y,1)
    """
    for spot in self.spotfinder.images[self.frames[0]]["spots_inlier"]:
      for pxl in spot.bodypixels:
        x,y = wxpanel._img.image_coords_as_screen_coords(
          pxl.y,
          pxl.x)
        dc.SetPen(wx.Pen('cyan'))
        dc.DrawCircle(x,y,1)

    if  show_bg_boxes:
      imgobj = self.Files.imageindex(self.frames[0])
      from iotbx.detectors.npy import tile_manager
      aa = tile_manager(self.phil_params).effective_tiling_as_flex_int(
        reapply_peripheral_margin = True, beam = (
        imgobj.beamx/imgobj.pixel_size,imgobj.beamy/imgobj.pixel_size))

      dc.SetPen(wx.Pen('orange'))
      dc.SetBrush(wx.Brush('red', wx.TRANSPARENT))
      for i in range(0, len(aa), 4):
        p = wxpanel._img.image_coords_as_screen_coords(aa[i + 1], aa[i + 0])
        p2 = wxpanel._img.image_coords_as_screen_coords(aa[i + 3]-1, aa[i + 2]-1)
        dc.DrawRectangle(x=p[0], y=p[1], width=p2[0]-p[0],height=p2[1]-p[1])

    if False:
      x, y = wxpanel._img.image_coords_as_screen_coords(850, 850)

      dc.SetPen(wx.Pen('red'))
      dc.SetBrush(wx.Brush('red', wx.TRANSPARENT))
      for i in range(24):
        r, s = wxpanel._img.image_coords_as_screen_coords(850 + (i + 1) * 25, 850)
        dc.DrawCircle(x, y, r  - x)

def view_raw_image(path, *command_line, **kwargs):
  args = [path,
          "viewer.powder_arcs.show=False",
          "viewer.powder_arcs.code=3n9c",
         ]

  horizons_phil = cxi_phil.cxi_versioned_extract(
                    copy.deepcopy(args),list(command_line))

  global parameters
  parameters.horizons_phil = horizons_phil

  if horizons_phil.viewer.calibrate_pdb.code is not None:
    from rstbx.viewer.calibration import pdb_code_wrapper
    pdb_code_wrapper(horizons_phil).display(path)
    return
  elif horizons_phil.viewer.calibrate_unitcell.unitcell is not None:
    from rstbx.viewer.calibration import unit_cell_wrapper
    unit_cell_wrapper(horizons_phil).display(path)
    return
  elif horizons_phil.viewer.calibrate_silver==True:
    from rstbx.viewer.calibration import sb_wrapper
    sb_wrapper(horizons_phil).display(path)
    return

  wrapper_of_callback().display(path)

def run_one(path, *command_line, **kwargs):
  args = ["distl.image=%s"%path,
          "distl.res.outer=2.1",
          "distl.detector_format_version=CXI 5.1",
          ]

  horizons_phil = cxi_phil.cxi_versioned_extract(
                    copy.deepcopy(args),list(command_line))

  global parameters
  parameters.horizons_phil = horizons_phil

  from spotfinder.applications import signal_strength
  info = signal_strength.run_signal_strength(horizons_phil)

  if kwargs.get("display",False):

    work = wrapper_of_callback(info)
    work.display_with_callback(path)

def run_one_index_core(horizons_phil):
  global parameters
  parameters.horizons_phil = horizons_phil

  from rstbx.new_horizons.index import pre_indexing_validation,pack_names,new_horizons_state
  pre_indexing_validation(horizons_phil)
  imagefile_arguments = pack_names(horizons_phil)
  info = new_horizons_state(horizons_phil,imagefile_arguments)

  try:
    info.process()
  except Exception as e:
    e.info = info
    raise e

  info.S = info.spotfinder_results
  return info

def run_one_index(path, *arguments, **kwargs):

  assert arguments[0].find("target=")==0
  target = arguments[0].split("=")[1]

  from xfel.phil_preferences import load_cxi_phil
  if "--nodisplay" in arguments[1:]:
    display = False
    arguments = list(arguments)
    arguments.remove("--nodisplay")
  else:
    display = True

  args = ["indexing.data=%s"%path,
          "beam_search_scope=0.5",
          "lepage_max_delta = 3.0",
          "spots_pickle = None",
          "subgroups_pickle = None",
          "refinements_pickle = None",
          "rmsd_tolerance = 5.0",
          "mosflm_rmsd_tolerance = 5.0",
          "difflimit_sigma_cutoff=2.0",
          #"indexing.verbose_cv=True",
          "indexing.open_wx_viewer=%s"%display
          ] + list(arguments[1:])

  horizons_phil = load_cxi_phil(target, args)

  info = run_one_index_core(horizons_phil)
  info.Files = info.organizer.Files
  info.phil_params = info.horizons_phil

  # The spotfinder view within cxi.index is an anachronism; no useful purpose anymore
  # therefore remove this option within cxi.index:
  return
  work = wrapper_of_callback(info)

  if kwargs.get("display",False):
      import wx
      from rstbx.viewer       import display
      from rstbx.viewer.frame import XrayFrame
      display.user_callback = work.user_callback

      app   = wx.App(0)
      frame = XrayFrame(None, -1, "X-ray image display", size=(1200,1080))
      frame.SetSize((1024,780))
      frame.load_image(path)
      frame.Show()
      app.MainLoop()

if __name__ == "__main__":
  function = sys.argv[1] # either view, spots, or index
  selector = {"view":view_raw_image, "spots":run_one, "index":run_one_index}
  files = [arg for arg in sys.argv[2:] if os.path.isfile(arg)]
  arguments = [arg for arg in sys.argv[2:] if not os.path.isfile(arg)]
  for file in files:
    selector[function](file, *arguments, **({'display':True}))


 *******************************************************************************


 *******************************************************************************
xfel/cxi/gfx/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/cxi/gfx/status_plot.py
from __future__ import absolute_import, division, print_function

########################################################################
# (excerpted and paraphrased) email from JH 11-23-11:
#
# fields:
# - DAQ det-z in mm (floating point in range [-400, 10])
# - mm of Si-foil in the beam (integer, in range [0, 20460], typically
#   less than 1000)
# - Wavelength, should be tightly distributed (floating point)
#
# The det-z from the data acquisition system (field 5) is related to the
# sample--detector distance.  Because we may want to change the exact
# relationship during the experiment, plotting the det-z value from the DAQ
# should give more consistent results (and I'd think we'd only like to know
# whether it changes or not).  If it changes, it could (semi-)continuously
# vary over the entire permitted range.
#
# The thickness of the attenuation Si-foil is integer (actually we can only
# ever see certain values, ask again if you'd like to know how this works).
# These aren't continuous changes, and they'll typically jump back and forth
# as the sheets of foil are inserted and removed.
#
# I expect the wavelength to have a sigma of around 0.001 A (even though we
# could see deviations as large as 0.1 A).  Basically, whatever wavelength we
# see for the first shout shouldn't change much.
########################################################################

import wxtbx.plots
import wx
import time

def format_time(x, pos=None):
  lt = time.localtime(x)
  return time.strftime("%H:%M:%S", lt)

def draw_plot (figure, t, det_z, laser01, laser04, laser04_power, si_foil,
               wavelength, bragg=None, run_id=None) :
  assert (len(t) == len(si_foil))
  import matplotlib.ticker as ticker
  left, width = 0.1, 0.8
  rect1 = [left, 0.65, width, 0.15] # detector Z
  rect2 = [left, 0.50, width, 0.15] # mm of Si-foil
  rect3 = [left, 0.35, width, 0.15] # wavelength
  #rect4 = [left, 0.05, width, 0.4] # bragg spots
  rect5 = [left, 0.20, width, 0.15] # Laser #1 & #4
  rect6 = [left, 0.05, width, 0.15] # Laser #4 power
  ax1 = figure.add_axes(rect1)
  ax2 = figure.add_axes(rect2, sharex=ax1)
  ax3 = figure.add_axes(rect3, sharex=ax1)
  #ax4 = figure.add_axes(rect4, sharex=ax1)
  ax51 = figure.add_axes(rect5, sharex=ax1)
  ax52 = ax51.twinx()
  ax6 = figure.add_axes(rect6, sharex=ax1)
  ax1.grid(True, color="0.75")
  ax2.grid(True, color="0.75")
  ax3.grid(True, color="0.75")
  #ax4.grid(True, color="0.75")
  ax51.grid(True, color="0.75")
  ax52.grid(True, color="0.75")
  ax6.grid(True, color="0.75")
  try :
    ax1.plot(t, det_z, linewidth=1, color=[1.0, 0.0, 0.0])
  except ValueError as e :
    print("error plotting det_z:", e)
    print(len(t), len(det_z))
  try :
    ax2.plot(t, si_foil, linewidth=1, color=[0.2, 0.4, 0.6])
  except ValueError as e :
    print("error plotting si_foil:", e)
    print(len(t), len(si_foil))
  ax3.plot(t, wavelength, linewidth=1, color=[0.0,1.0,0.5])
  #ax4.bar(t, bragg, facecolor=[0.0,0.5,1.0], edgecolor=[0.0,0.5,1.0])
  #ax4.plot(t, bragg, linewidth=1, color=[0.0,0.5,1.0])
  ax51.plot(t, laser01, linewidth=1, color=[1.0,0.0,0.5])
  ax52.plot(t, laser04, linewidth=1, color=[0.0,1.0,0.5])
  ax6.plot(t, laser04_power, linewidth=1, color=[0.0,1.0,0.5])
  ax1.set_ylabel("Detector Z (mm)")
  ax2.set_ylabel("Si foil (mm)")
  ax3.set_ylabel("Wavelength (A)")
  #ax4.set_ylabel("Bragg spots")
  ax51.set_ylabel("Laser #1", color=[1.0,0.0,0.5])
  ax52.set_ylabel("Laser #4", color=[0.0,1.0,0.5])
  ax6.set_ylabel("Laser #4 power")
  ax1.set_xlabel("Time")
  for ax in ax1, ax2, ax3, ax51, ax52, ax6: #, ax4:
    if (ax is not ax6) :
      for label in ax.get_xticklabels():
        label.set_visible(False)
      ax.get_yticklabels()[0].set_visible(False)
  ax6.xaxis.set_major_formatter(ticker.FuncFormatter(format_time))
  if (run_id is not None) :
    ax1.set_title("CXI experiment status - run %d" % run_id)
  else :
    ax1.set_title("CXI experiment status - RUN NUMBER NOT SET")
  figure.autofmt_xdate()

UPDATE_ID = wx.NewId()
RUN_NUMBER_ID = wx.NewId()
SAVE_IMG_ID = wx.NewId()

class UpdateEvent (wx.PyEvent) :
  """
  Contains new data for plotting (one x series and three y series).
  """
  def __init__ (self, t, det_z, laser01, laser04, laser04_power, si_foil,
                wavelength) :
    self.data = (t, det_z, laser01, laser04, laser04_power, si_foil, wavelength)
    wx.PyEvent.__init__(self)
    self.SetEventType(UPDATE_ID)

class RunNumberEvent (wx.PyEvent) :
  """
  Sets the current run ID, which will appear in the frame title bar and the
  plot.
  """
  def __init__ (self, run_id) :
    self.run_id = run_id
    wx.PyEvent.__init__(self)
    self.SetEventType(RUN_NUMBER_ID)

class SaveImageEvent (wx.PyEvent) :
  """
  Signals that an image of the current plot should be saved to disk (the file
  name will be something like rID_status.png, where ID is the run number.
  """
  def __init__ (self) :
    wx.PyEvent.__init__(self)
    self.SetEventType(SAVE_IMG_ID)

class StatusFrame (wxtbx.plots.plot_frame) :
  show_controls_default = False
  def __init__ (self, *args, **kwds) :
    self.run_id = None
    wxtbx.plots.plot_frame.__init__(self, *args, **kwds)
    self.Connect(-1, -1, UPDATE_ID, self.OnUpdate)
    self.Connect(-1, -1, RUN_NUMBER_ID, self.OnSetRunNumber)
    self.Connect(-1, -1, SAVE_IMG_ID, self.OnSaveImage)

  def create_plot_panel (self) :
    return StatusPlot(
      parent=self,
      figure_size=(16,10))

  def OnUpdate (self, event) :
    t, det_z, laser01, laser04, laser04_power, si_foil, wavelength = event.data
    self.plot_panel.show_plot(t, det_z, laser01, laser04, laser04_power,
                              si_foil, wavelength, run_id=self.run_id)

  def OnSetRunNumber (self, event) :
    self.run_id = event.run_id
    self.SetTitle("CXI experiment status - run %d" % self.run_id)

  def OnSaveImage (self, event) :
    print("StatusFrame.OnSaveImage(event)")
    assert (self.run_id is not None)
    file_name = "r%d_status.png" % self.run_id
    self.plot_panel.figure.savefig(file_name, format="png")
    print("SAVED PLOT: %s" % file_name)

class StatusPlot (wxtbx.plots.plot_container) :
  def show_plot (self, *args, **kwds) :
    self.figure.clear()
    draw_plot(self.figure, *args, **kwds)
    self.canvas.draw()
    self.parent.Refresh()

def draw_plot_to_file (file_name, *args, **kwds) :
  """
  For offline plotting, which turns out to be necessary when running the
  status plot through pyana.
  """
  import matplotlib
  import matplotlib.figure
  from matplotlib.backends.backend_agg import FigureCanvasAgg
  figure = matplotlib.figure.Figure((16,10))
  canvas = FigureCanvasAgg(figure)
  draw_plot(figure, *args, **kwds)
  canvas.draw()
  figure.savefig(file_name, format="png")


 *******************************************************************************


 *******************************************************************************
xfel/cxi/gfx/trials_plot.py
from __future__ import absolute_import, division, print_function

#-----------------------------------------------------------------------
# more-or-less real-time plotting of Bragg peak count and XES detector
# skewness.
#-----------------------------------------------------------------------

from xfel.cxi.gfx import status_plot
import wxtbx.plots
from scitbx.array_family import flex
import libtbx.phil
from libtbx.utils import Usage, Sorry
import wx
import matplotlib.ticker as ticker
import time
import os
import operator
import math
from six.moves import range

master_phil = libtbx.phil.parse("""
  trial_id = None
    .type = int
  t_wait = 8000
    .type = int
  hit_cutoff = 16
    .type = int
  average_window = 1000
    .type = int
  n_points = 1000
    .type = int
  display_time = 1800
    .type = int
  run_num = None
    .type = int
  run_min = None
    .type = int
  run_max = None
    .type = int
""")

class Run (object):
  def __init__(self, runId):
    self.runId = runId
    self.hits_count = 0
    self.bragg_times = flex.double()
    self.braggs = flex.double()
    self.culled_braggs = flex.double()
    self.culled_bragg_times = flex.double()
    self.hit_rates_times = flex.double()
    self.hit_rates = flex.double()

    self.distances = flex.double()
    self.culled_distances = flex.double()

    self.sifoils = flex.double()
    self.culled_sifoils = flex.double()

    self.wavelengths = flex.double()
    self.culled_wavelengths = flex.double()

    self.indexed = flex.bool()
    self.culled_indexed = flex.bool()

  def width(self):
    return max(self.bragg_times)-min(self.bragg_times)

  def min(self):
    return min(self.bragg_times)

  def max(self):
    return max(self.bragg_times)

  def cull_braggs(self, count):
    if count == 0:
      window = 0
    else:
      window = len(self.bragg_times)/count

    if count <= 0 or window < 1:
      self.culled_braggs = self.braggs
      self.culled_bragg_times = self.bragg_times
      self.culled_distances = self.distances
      self.culled_sifoils = self.sifoils
      self.culled_wavelengths = self.wavelengths
      self.culled_indexed = self.indexed
      return

    self.culled_braggs = flex.double()
    self.culled_bragg_times = flex.double()
    self.culled_distances = flex.double()
    self.culled_sifoils = flex.double()
    self.culled_wavelengths = flex.double()
    self.culled_indexed = flex.bool()

    for i in range(count):
      braggs =  self.braggs[i*int(window):(i+1)*int(window)]
      idx = int(i*window) + flex.max_index(braggs)
      self.culled_braggs     .append(self.braggs[idx])
      self.culled_bragg_times.append(self.bragg_times[idx])
      self.culled_distances  .append(self.distances[idx])
      self.culled_sifoils    .append(self.sifoils[idx])
      self.culled_wavelengths.append(self.wavelengths[idx])
      self.culled_indexed    .append(self.indexed[idx])

  def recalc_hits(self, windowLen, hit_cutoff):

    self.hit_rates_times = flex.double()
    self.hit_rates = flex.double()
    self.hits_count = 0
    if len(self.braggs) <= 0 or windowLen <= 0: return

    for i in range(int(math.floor(len(self.braggs)/windowLen))):
      window = self.braggs[i*windowLen:(i+1)*windowLen]
      isel = (window >= hit_cutoff).iselection()
      ratio = float(len(isel)) / float(windowLen)
      self.hit_rates.append(ratio*100)
      self.hit_rates_times.append(self.bragg_times[(i*windowLen)+int(math.floor((windowLen/2)))])
      self.hits_count += len(isel)

class TrialsPlotFrame (wxtbx.plots.plot_frame) :
  show_controls_default = False
  def __init__ (self, *args, **kwds) :
    wxtbx.plots.plot_frame.__init__(self, *args, **kwds)

    label = wx.StaticText(self.toolbar, -1, " Zoom: ", style=wx.ALIGN_CENTER)
    self.toolbar.AddControl(label)

    self.zoomSlider = wx.Slider(self.toolbar, size= (250, -1), minValue=0, maxValue=99)
    self.toolbar.AddControl(self.zoomSlider)
    self.Bind(wx.EVT_SCROLL, self.OnZoom, self.zoomSlider)

    label = wx.StaticText(self.toolbar, -1, " Pan: ", style=wx.ALIGN_CENTER)
    self.toolbar.AddControl(label)

    self.panSlider = wx.Slider(self.toolbar, size= (250, -1), minValue=1, maxValue=100)
    self.panSlider.SetValue(50)
    self.panSlider.Disable()
    self.toolbar.AddControl(self.panSlider)
    self.Bind(wx.EVT_SCROLL, self.OnPan, self.panSlider)

    self.timerCheck = wx.CheckBox(self.toolbar, -1, "Auto load new data")
    self.timerCheck.SetValue(True)
    self.toolbar.AddControl(self.timerCheck)

    self.timelockCheck = wx.CheckBox(self.toolbar, -1, "Display last %s minutes: ")
    self.timelockCheck.SetValue(False)
    self.timelockCheck.newly_checked = False
    self.toolbar.AddControl(self.timelockCheck)
    self.timelockCheck.Bind(wx.EVT_CHECKBOX, self.OnTimeLockCheck, self.timelockCheck)

    self.zoom = 100
    self.pan = 50

    self.full_data_load = True

  def create_plot_panel (self) :
    return TrialsPlot(
      parent=self,
      figure_size=(16,10))

  def show_plot(self):
    self.plot_panel.show_plot()

  def set_params(self, params):
    self.params = params
    self.trial_id = params.trial_id

    self._timer = wx.Timer(owner=self)
    self.Bind(wx.EVT_TIMER, self.OnTimer)
    self._timer.Start(params.t_wait)

    self.timelockCheck.SetLabel("Display last %s minutes: "%(self.params.display_time/60))

  def OnTimeLockCheck(self, event):
    self.timelockCheck.newly_checked = True

  def OnTimer(self, event):
    if(self.timerCheck.GetValue()):
      if self.load_data() or self.timelockCheck.newly_checked:
        if self.timelockCheck.GetValue():
          self.panSlider.Disable()
          self.zoomSlider.Disable()
          if self.total_width > 0:
            self.zoom = 100 * self.params.display_time / self.total_width
            self.pan = 100

            if self.zoom > 100: self.zoom = 100

        else:
          self.zoomSlider.Enable()

        self.timelockCheck.newly_checked = False
        self.show_plot()
      else:
        print("No new data")

  def OnZoom(self, event):
    self.zoom = 100-event.GetPosition()
    if self.zoom < 100:
      self.panSlider.Enable()
    else:
      self.panSlider.Disable()

    self.cull_braggs()

    self.plot_panel.show_plot()

  def OnPan(self, event):
    self.pan = event.GetPosition()
    self.plot_panel.show_plot()

  # Returns true if new data was loaded, otherwise false
  def load_data (self):
    ttop = time.time()
    print("Loading data...")
    assert (self.trial_id is not None)

    import cxi_xdr_xes.cftbx.cspad_ana.db as cxidb
    db=cxidb.dbconnect()
    assert(db is not None and db.open)

    # retrieve the run IDs in this trial
    #t1 = time.time()
    cursor = db.cursor()
    #cursor.execute("SELECT DISTINCT(run) FROM %s WHERE trial = %s"%(cxidb.table_name,self.trial_id))
    cmd = "SELECT DISTINCT(run) FROM %s WHERE trial = %s"
    if self.params.run_num is not None:
      extra = " AND run = %s"%self.params.run_num
    elif self.params.run_min is not None and self.params.run_max is not None:
      extra = " AND run >= %s AND run <= %s"%(self.params.run_min, self.params.run_max)
    else:
      extra = " ORDER BY run DESC LIMIT 5"
    cursor.execute(cmd%(cxidb.table_name,self.trial_id) + extra)
    #t2 = time.time()
    #print "Runs queried in %.2fs" % (t2 - t1)

    if(self.full_data_load):
      self.runs = []
    if(len(self.runs) > 5):
      self.runs = self.runs[-5:]

    new_data = False

    for runId in cursor.fetchall():
      if self.full_data_load:
        run = Run(int(runId[0]))
        self.runs.append(run)
      else:
        foundit=False
        for runtest in self.runs:
          if runtest.runId == int(runId[0]):
            foundit = True
            run = runtest
            break
        if not foundit:
          print("New run: %s"%runId)
          run = Run(int(runId[0]))
          self.runs.append(run)

      #t1 = time.time()
      #print "Loading data from run %s" % (run.runId)
      if self.full_data_load or not hasattr(run, "latest_entry_id"):
        print("Full load")
        cursor.execute("SELECT id, eventstamp, hitcount, distance, sifoil, wavelength, indexed FROM %s \
          WHERE trial = %s AND run = %s ORDER BY eventstamp"%(cxidb.table_name,self.trial_id,run.runId))
      else:
        print("Partial load")
        cursor.execute("SELECT id, eventstamp, hitcount, distance, sifoil, wavelength, indexed FROM %s \
          WHERE trial = %s AND run = %s AND id > %s ORDER BY eventstamp"%(cxidb.table_name,self.trial_id,run.runId,run.latest_entry_id ))

      #t2 = time.time()
      #print "Query ran in %.2fs" % (t2 - t1)

      ids = flex.int()

      for id, eventstamp, hitcount, distance, sifoil, wavelength, indexed in cursor.fetchall():
        run.bragg_times.append(float(eventstamp))
        run.braggs.append(int(hitcount))
        ids.append(id)

        run.distances.append(float(distance))
        run.sifoils.append(float(sifoil))
        run.wavelengths.append(float(wavelength))
        run.indexed.append(bool(indexed))

      if len(ids) > 0:
        run.latest_entry_id = max(ids)
        new_data = True
        run.recalc_hits(self.params.average_window, self.params.hit_cutoff)


    self.total_width = 0
    for run in self.runs:
      perm = flex.sort_permutation(run.hit_rates_times)
      run.hit_rates_times = run.hit_rates_times.select(perm)
      run.hit_rates = run.hit_rates.select(perm)

      self.total_width += run.width()

    self.cull_braggs()

    #self.full_data_load = False #always do a full load
    self.runs.sort(key=operator.attrgetter('runId'))
    tbot = time.time()
    print("Data loaded in %.2fs" % (tbot - ttop))
    return new_data

  def cull_braggs(self):
    for run in self.runs:
      run.cull_braggs(int(self.params.n_points*run.width()/self.total_width))


class TrialsPlot (wxtbx.plots.plot_container) :
  def show_plot(self):
    """
    We assume that the runs are sorted in ascending order and that they do not overlap in time

    The final graph will have a number of time units displayed equal to a percentage
    of the total number of time units in all the runs, not including gaps.  Each run
    may or may not be displayed depending on the zoom and pan.

    total_width: how many time units total in these runs, not including gaps
    xmin: smallest time value in the earliest run
    xmax: largest time value in the lastest run
    newwidth: number of time units to display on xaxis total over all runs
    newmid: a point on the total time units scale within the zoom and pan limits. Usually
     the center, but since it could cause x to fall out of bounds it is adjusted
    newxmin: how many time units on the left of the graph are not displayed, OR, the first
     time unit to be displayed
    newxmax: the first time unit not to be displayed
    """
    runs = self.GetParent().runs
    if len(runs) <= 0: return

    t1 = time.time()
    total_width = self.GetParent().total_width

    newwidth = total_width * (self.GetParent().zoom / 100)
    newmid = total_width * (self.GetParent().pan/100)
    newxmin = newmid - (newwidth/2)
    newxmax = newxmin + newwidth

    if newxmin < 0:
      newxmin = 0
      newxmax = newwidth
    elif newxmax > total_width:
      newxmax = total_width
      newxmin = newxmax - newwidth

    assert newxmin >= 0 and newxmin <= total_width

    #print "**** Zoom: %s, pan: %s, total_width: %s, newwidth: %s, newmid: %s, newxmin: %s, newxmax: %s" \
    #  %(self.GetParent().zoom,self.GetParent().pan,total_width,newwidth,newmid,newxmin,newxmax)

    left = 0
    width_so_far = 0
    self.figure.clear()
    braggsmax = max(flex.max(r.culled_braggs) for r in runs)
    braggsmin = min(flex.min(r.culled_braggs) for r in runs)
    distsmax = max(flex.max(r.culled_distances) for r in runs)
    distsmin = min(flex.min(r.culled_distances) for r in runs)
    sifomax = max(flex.max(r.culled_sifoils) for r in runs)
    sifomin = min(flex.min(r.culled_sifoils) for r in runs)
    wavemax = max(flex.max(r.culled_wavelengths) for r in runs)
    wavemin = min(flex.min(r.culled_wavelengths) for r in runs)

    #above tricks don't work for hit rates as they can be empty if the run is new
    goodruns = []
    for run in runs:
      if len(run.hit_rates) > 0: goodruns.append(run)
    if len(goodruns) > 0:
      hitsmax = max(flex.max(r.hit_rates) for r in goodruns)
      hitsmin = min(flex.min(r.hit_rates) for r in goodruns)
    else:
      hitsmax = hitsmin = 0

    first_run = True
    for run in runs:
      right = left + run.width()

      if right < newxmin or left > newxmax:
        left += run.width()
        #print "Not showing run %s"%run.runId
        continue

      if left < newxmin:
        xmin = run.min() + (newxmin - left)
      else:
        xmin = run.min()

      if right > newxmax:
        xmax = run.min() + (newxmax - left)
      else:
        xmax = run.max()

      #print "Run: %s, run.width(): %s, left: %s, right: %s, run.min(): %s, run.max(): %s, xmin: %s, xmax: %s, width_so_far: %s, xmax-xmin: %s" \
        #%(run.runId,run.width(),left,right,run.min(),run.max(),xmin,xmax,width_so_far,xmax-xmin)

      ax1 = self.figure.add_axes([0.05+(0.9*width_so_far/newwidth), 0.05, 0.9*(xmax-xmin)/newwidth, 0.4])
      ax2 = self.figure.add_axes([0.05+(0.9*width_so_far/newwidth), 0.45, 0.9*(xmax-xmin)/newwidth, 0.2], sharex=ax1)
      ax3 = self.figure.add_axes([0.05+(0.9*width_so_far/newwidth), 0.65, 0.9*(xmax-xmin)/newwidth, 0.1], sharex=ax1)
      ax4 = self.figure.add_axes([0.05+(0.9*width_so_far/newwidth), 0.75, 0.9*(xmax-xmin)/newwidth, 0.1], sharex=ax1)
      ax5 = self.figure.add_axes([0.05+(0.9*width_so_far/newwidth), 0.85, 0.9*(xmax-xmin)/newwidth, 0.1], sharex=ax1)
      left += run.width()
      width_so_far += (xmax-xmin)

      ax1.grid(True, color="0.75")
      ax2.grid(True, color="0.75")
      ax3.grid(True, color="0.75")
      ax4.grid(True, color="0.75")
      ax5.grid(True, color="0.75")
      ax1.plot(run.culled_bragg_times.select(run.culled_indexed),
               run.culled_braggs.select(run.culled_indexed), 'd', color=[0.0,1.0,0.0])
      ax1.plot(run.culled_bragg_times.select(~run.culled_indexed),
               run.culled_braggs.select(~run.culled_indexed), 'd', color=[0.0,0.5,1.0])
      ax2.plot(run.hit_rates_times, run.hit_rates, 'o-', color=[0.0,1.0,0.0])
      ax3.plot(run.culled_bragg_times, run.culled_wavelengths, '^', color=[0.8,0.0,0.2])
      ax4.plot(run.culled_bragg_times, run.culled_sifoils, '<', color=[0.8,0.0,0.2])
      ax5.plot(run.culled_bragg_times, run.culled_distances, '>', color=[0.8,0.0,0.2])
      ax1.set_ylabel("# of Bragg spots")
      ax2.set_ylabel("Hit rate (%)")
      ax3.set_ylabel("WaveL")
      ax4.set_ylabel("SiFoils(mm)")
      ax5.set_ylabel("Dist (mm)")
      ax1.set_xlim(xmin, xmax)
      ax1.set_ylim(braggsmin, braggsmax)
      ax2.set_ylim(hitsmin, hitsmax)
      ax3.set_ylim(wavemin, wavemax)
      ax4.set_ylim(sifomin-10, sifomax+10)
      ax5.set_ylim(distsmin-3, distsmax+3)
      ax1.set_xlabel("Time")
      for ax in ax1, ax2, ax3, ax4, ax5:
        if (ax is not ax1) :
          for label in ax.get_xticklabels():
            label.set_visible(False)
        ax.get_yticklabels()[0].set_visible(False)
        if not first_run:
          ax.get_yaxis().set_visible(False)

      ax1.xaxis.set_major_formatter(ticker.FuncFormatter(status_plot.format_time))
      ax3.yaxis.set_major_formatter(ticker.FormatStrFormatter("%.3f"))
      ax5.yaxis.set_major_formatter(ticker.FormatStrFormatter("%.0f"))
      ax5.set_title("%d:%d/%d:%.1f%% I:%d"%(run.runId, run.hits_count, len(run.braggs), 100*run.hits_count/len(run.braggs),run.indexed.count(True)))

      labels = ax1.get_xticklabels()
      for label in labels:
        label.set_rotation(30)

      first_run = False

    self.figure.autofmt_xdate()
    self.canvas.draw()
    self.parent.Refresh()

    t2 = time.time()
    print("Plotted in %.2fs" % (t2 - t1))

def run (args) :
  user_phil = []
  # TODO: replace this stuff with iotbx.phil.process_command_line_with_files
  # as soon as I can safely modify it
  for arg in args :
    #if (os.path.isdir(arg)) :
      #user_phil.append(libtbx.phil.parse("""status_dir=\"%s\"""" % arg))
    #elif (not "=" in arg) :
    if (not "=" in arg) :
      try :
        user_phil.append(libtbx.phil.parse("""trial_id=%d""" % int(arg)))
      except ValueError as e :
        raise Sorry("Unrecognized argument '%s'" % arg)
    else :
      try :
        user_phil.append(libtbx.phil.parse(arg))
      except RuntimeError as e :
        raise Sorry("Unrecognized argument '%s' (error: %s)" % (arg, str(e)))
  params = master_phil.fetch(sources=user_phil).extract()
  if (params.trial_id is None) :
    master_phil.show()
    raise Usage("trial_id must be defined (either trial_id=XXX, or the integer "+
      "ID alone).")
  assert (params.t_wait is not None) and (params.t_wait > 0)
  assert (params.hit_cutoff is not None) and (params.hit_cutoff > 0)
  assert (params.average_window is not None) and (params.average_window > 0)
  assert (params.n_points is not None) # zero or less means display all the points
  assert (params.display_time is not None) and (params.display_time > 0)

  app = wx.App(0)
  frame = TrialsPlotFrame(None, -1, "Detector status for trial %d" %
      params.trial_id)
  frame.set_params(params)
  frame.load_data()
  frame.show_plot()
  frame.Show()
  app.MainLoop()


 *******************************************************************************
