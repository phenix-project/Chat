

 *******************************************************************************
iotbx/map_tools.py
"""
  Manager for files containing map coefficients (either as complex arrays or
  separate F, PHI, and optional FOM).  Once an MTZ file or collection of
  Miller arrays is loaded, individual maps can be quickly retrieved, and it
  includes built-in recognition of standard map labels used in PHENIX (and
  CCP4).Routines to calculate maps from map coefficients of various types
"""
from __future__ import absolute_import, division, print_function

from libtbx.math_utils import ifloor, iceil
from libtbx import adopt_init_args
import os
from six.moves import zip

MAP_TYPE_F_OBS = 1
MAP_TYPE_DIFF = 2
MAP_TYPE_ANOMALOUS = 4
MAP_TYPE_ISO_DIFF = 8
MAP_TYPE_NEUTRON = 16
MAP_TYPE_F_CALC = 32
MAP_TYPE_FILLED = 64

standard_map_names = {
  '2FOFCWT' : '2mFo-DFc',
  '2FOFCWT_no_fill' : '2mFo-DFc_no_fill',
  'FOFCWT' : 'mFo-DFc',
  'ANOM' : 'anomalous',
}

class MapNotFound(RuntimeError):
  pass

class server(object):
  """
  Manager for files containing map coefficients (either as complex arrays or
  separate F, PHI, and optional FOM).  Once an MTZ file or collection of
  Miller arrays is loaded, individual maps can be quickly retrieved, and it
  includes built-in recognition of standard map labels used in PHENIX (and
  CCP4).
  """
  def __init__(self, file_name=None, miller_arrays=()):
    assert (file_name is not None) or (len(miller_arrays) > 0)
    self.file_name = file_name
    self.miller_arrays = miller_arrays
    if (len(self.miller_arrays) == 0):
      from iotbx import file_reader
      f = file_reader.any_file(file_name, force_type="hkl")
      f.assert_file_type("hkl")
      self.miller_arrays = f.file_server.miller_arrays
    assert (len(self.miller_arrays) > 0)
    self.map_coeffs = []
    self.array_labels = []
    self.data_arrays = []
    self.phase_arrays = []
    self.weight_arrays = []
    self.rfree_arrays = []
    self.fcalc_arrays = []
    self._pdb_cache = {}
    from iotbx.reflection_file_utils import looks_like_r_free_flags_info
    for array in self.miller_arrays :
      labels = array.info().label_string()
      self.array_labels.append(labels)
      if (array.is_xray_amplitude_array() or array.is_xray_intensity_array()):
        if (labels.startswith("FC")):
          self.fcalc_arrays.append(array)
        else :
          self.data_arrays.append(array)
      elif (array.is_complex_array()):
        if (labels.startswith("FC") or labels.startswith("FMODEL") or
            labels.startswith("F-model")):
          self.fcalc_arrays.append(array)
        else :
          self.map_coeffs.append(array)
      elif (array.is_real_array()):
        if (labels.startswith("PH")):
          self.phase_arrays.append(array)
        elif (labels.startswith("FOM")):
          self.weight_arrays.append(array)
      elif (array.is_bool_array()):
        self.rfree_arrays.append(array)
      elif (array.is_integer_array()):
        if (looks_like_r_free_flags_info(array.info())):
          self.rfree_arrays.append(array)

  def is_phenix_refine_maps(self):
    return "2FOFCWT,PH2FOFCWT" in self.array_labels

  def is_cctbx_maps(self):
    return "2mFoDFc,P2mFoDFc" in self.array_labels

  def is_resolve_map(self):
    return ((("FP" in self.array_labels) or ("FP,SIGFP" in self.array_labels))
          and ("PHIM" in self.array_labels) and ("FOMM" in self.array_labels))

  def is_ccp4_style_map(self):
    return (("FWT,PHWT" in self.array_labels) or
            ("FWT,PHIFWT" in self.array_labels))

  def is_ccp4_style_difference_map(self):
    return ("DELFWT,PHDELWT" in self.array_labels)

  def is_solve_map(self):
    return ((("FP" in self.array_labels) or ("FP,SIGFP" in self.array_labels))
          and ("PHIB" in self.array_labels) and ("FOM" in self.array_labels))

  def is_anomalous_map(self):
    for array in self.map_coeffs :
      if array.info().label_string().startswith("ANOM"):
        return True
    return False

  def contains_map_coefficients(self):
    return (len(self.map_coeffs) > 0)

  def contains_amplitude_and_phase(self):
    return (len(self.data_arrays) > 0) and (len(self.phase_arrays) > 0)

  def get_amplitudes_and_phases(self,
                                 f_label=None,
                                 phi_label=None,
                                 fom_label=None):
    f_array, phi_array, fom_array = None, None, None
    allowed_arrays = self.data_arrays + self.map_coeffs
    if (f_label in ["FC", "FMODEL", "F-model"]):
      allowed_arrays += self.fcalc_arrays
    for array in allowed_arrays :
      if (f_array is None):
        labels = array.info().labels
        label_string = array.info().label_string()
        if (f_label is not None):
          if (label_string == f_label) or (labels[0] == f_label):
            f_array = array
        elif (labels[0] == "FP"):
          f_array = array
        elif (labels[0] == "F"):
          f_array = array
    for array in self.phase_arrays :
      if (phi_array is None):
        labels = array.info().label_string()
        if (phi_label is not None):
          if (labels == phi_label):
            phi_array = array
            continue
        elif (labels == "PHIM"):
          phi_array = array
        elif (labels == "PHIB") and (not "PHIM" in self.array_labels):
          phi_array = array
        elif (labels == "PHIC") and not (("PHIM" in self.array_labels) or
                                         ("PHIB" in self.array_labels)):
          phi_array = array
        elif (labels == "PHI"):
          phi_array = array
    for array in self.weight_arrays :
      if (fom_array is None):
        labels = array.info().label_string()
        if (fom_label is not None):
          if (labels == fom_label):
            fom_array = array
        elif (labels == "FOMM"):
          fom_array = array
        elif (labels == "FOM"):
          fom_array = array
    return (f_array, phi_array, fom_array)

  def get_phenix_maps(self, keep_column_labels=False):
    f_maps = []
    diff_maps = []
    default_is_filled = None
    all_maps = []
    unknown_maps = []
    have_names = []
    for array in self.map_coeffs :
      labels = array.info().label_string()
      if (labels.endswith("_no_fill")):
        default_is_filled = True
      elif (labels.endswith("_fill")):
        default_is_filled = False
      if labels.startswith("2FOFCWT") or labels.startswith("2mFoDFc"):
        f_maps.append(array)
      elif labels.startswith("FOFCWT") or labels.startswith("mFoDFc"):
        diff_maps.append(array)
      elif labels.startswith("ANOM"):
        map_type = MAP_TYPE_ANOMALOUS
        map_name = "anomalous"
        if ("neutron" in labels):
          map_type |= MAP_TYPE_NEUTRON
          map_name += "_neutron"
        if (keep_column_labels):
          map_name = labels.split(",")[0]
        elif (map_name in have_names):
          map_name += "_%d" % (have_names.count(map_name) + 1)
        have_names.append(map_name)
        all_maps.append(map_info(map_coeffs=array,
          map_type=map_type,
          map_name=map_name))
      elif (labels.startswith("F-model")) or (labels.startswith("FC")):
        continue
      else :
        unknown_maps.append(array)
    unfilled_maps = []
    filled_maps = []
    for array in diff_maps :
      labels = array.info().label_string()
      map_type = MAP_TYPE_DIFF
      map_name = "mFo-DFc"
      if ("neutron" in labels):
        map_type |= MAP_TYPE_NEUTRON
        map_name += "_neutron"
      if (keep_column_labels):
        map_name = labels.split(",")[0]
      elif (map_name in have_names):
       map_name += "_%d" % (have_names.count(map_name) + 1)
      have_names.append(map_name)
      all_maps.append(map_info(map_coeffs=array,
        map_type=map_type,
        map_name=map_name))
    for array in f_maps :
      labels = array.info().label_string()
      map_name = "2mFo-DFc"
      map_type = MAP_TYPE_F_OBS
      if ("neutron" in labels):
        map_name += "_neutron"
        map_type |= MAP_TYPE_NEUTRON
      if labels.endswith("_no_fill"):
        map_name += "_no_fill"
      elif labels.endswith("_fill") or labels.endswith("_filled"):
        map_type |= MAP_TYPE_FILLED
      elif (default_is_filled):
        map_type |= MAP_TYPE_FILLED
      else :
        map_name += "_no_fill"
      if (keep_column_labels):
        map_name = labels.split(",")[0]
      elif (map_name in have_names):
       map_name += "_%d" % (have_names.count(map_name) + 1)
      have_names.append(map_name)
      all_maps.append(map_info(map_coeffs=array,
        map_type=map_type,
        map_name=map_name))
    return all_maps

  def get_resolve_map(self):
    map_coeffs, null_value = self.get_ccp4_maps()
    if (map_coeffs is None):
      map_coeffs = self._convert_amplitudes_and_phases(f_label="FP",
        phi_label="PHIM", fom_label="FOMM", weighted=True)
    from cctbx.miller import array_info
    info = array_info(labels=["FWT","PHWT"])
    return map_coeffs.set_info(info)

  def get_solve_map(self):
    map_coeffs = self._convert_amplitudes_and_phases(f_label="FP",
      phi_label="PHIB", fom_label="FOM", weighted=True)
    from cctbx.miller import array_info
    info = array_info(labels=["FWT","PHWT"])
    return map_coeffs.set_info(info)

  def get_ccp4_maps(self):
    f_map = diff_map = None
    for array in self.map_coeffs :
      labels = array.info().label_string()
      if (labels.startswith("FWT,")):
        f_map = array
      elif (labels.startswith("DELFWT,")):
        diff_map = array
    return (f_map, diff_map)

  def get_anomalous_map(self):
    for array in self.map_coeffs :
      labels = array.info().label_string()
      if (labels.upper().startswith("ANOM")):
        return array
    return None

  def _convert_amplitudes_and_phases(self, f_label=None, phi_label=None,
      fom_label=None, weighted=True):
    f, phi, fom = self.get_amplitudes_and_phases(f_label, phi_label, fom_label)
    if (f is None) or ((not f.is_complex_array()) and (phi is None)):
      raise MapNotFound(("Couldn't find amplitude or phase arrays in %s.\n"+
        "File contents:\n%s\nExpected labels: F=%s PHI=%s FOM=%s") %
        (self.file_name, "\n".join(self.array_labels), str(f_label),
         str(phi_label), str(fom_label)))
    return combine_f_phi_and_fom(f=f, phi=phi, fom=fom, weighted=weighted)

  def convert_any_map(self, f_label, phi_label, fom_label, **kwds):
    map_coeffs = self._convert_amplitudes_and_phases(f_label, phi_label,
      fom_label)
    return self._write_ccp4_map(map_coeffs, **kwds)

  def convert_complex_map(self, map_label, **kwds):
    map_coeffs = self._convert_amplitudes_and_phases(f_label=map_label)
    return self._write_ccp4_map(map_coeffs, **kwds)

  def convert_resolve_map(self, **kwds):
    map_coeffs = self.get_resolve_map()
    kwds['map_coeffs'] = map_coeffs
    kwds['simple_file_name'] = True
    return self._write_ccp4_map(**kwds)

  def convert_solve_map(self, **kwds):
    map_coeffs = self.get_solve_map()
    kwds['map_coeffs'] = map_coeffs
    kwds['simple_file_name'] = True
    return self._write_ccp4_map(**kwds)

  def convert_ccp4_map(self, **kwds):
    map_coeffs_list = list(self.get_ccp4_maps())
    files = []
    #kwds['simple_file_name'] = True
    for map_coeffs in map_coeffs_list :
      if (map_coeffs is None) : continue
      kwds['map_coeffs'] = map_coeffs
      output_file = self._write_ccp4_map(**kwds)
      files.append(os.path.abspath(output_file))
    return files

  def convert_anomalous_map(self, **kwds):
    map_coeffs = self.get_anomalous_map()
    if (map_coeffs is not None):
      kwds['map_coeffs'] = map_coeffs
      return self._write_ccp4_map(**kwds)
    return None

  def convert_phenix_maps(self, file_base, **kwds):
    all_maps = self.get_phenix_maps()
    files = []
    for map in all_maps :
      file_name = "%s_%s.ccp4" % (file_base, map.map_name)
      kwds['output_file'] = file_name
      self._write_ccp4_map(map.map_coeffs, **kwds)
      files.append(os.path.abspath(file_name))
    return files

  def convert_all_map_coefficients(self, **kwds):
    for map_coeffs in self.map_coeffs :
      labels = map_coeffs.info().label_string()
      map_file = self.convert_complex_map(labels, **kwds)
      yield map_file, labels

  def get_pdb_file(self, file_name):
    pdb_in, mtime = self._pdb_cache.get(file_name, (None, 0))
    if (pdb_in is None) or (os.path.getmtime(file_name) > mtime):
      import iotbx.pdb
      pdb_in = iotbx.pdb.input(file_name)
      self._pdb_cache[file_name] = (pdb_in, os.path.getmtime(file_name))
    return pdb_in

  def _write_ccp4_map(self,
                       map_coeffs,
                       pdb_file=None,
                       output_file=None,
                       simple_file_name=False,
                       resolution_factor=0.25,
                       sigma_scaling=True,
                       force=False,
                       use_standard_map_names=False):
    sites_cart = None
    if (pdb_file is not None):
      pdb_in = self.get_pdb_file(pdb_file)
      sites_cart = pdb_in.atoms().extract_xyz()
    if (output_file is None):
      if (simple_file_name):
        output_file = os.path.splitext(self.file_name)[0] + ".ccp4"
      elif (use_standard_map_names):
        lab1 = map_coeffs.info().labels[0]
        map_name = standard_map_names.get(lab1, lab1)
        output_file = os.path.splitext(self.file_name)[0] + "_%s.ccp4" % \
          map_name
      else :
        lab1 = map_coeffs.info().labels[0]
        output_file = os.path.splitext(self.file_name)[0] + "_%s.ccp4" % lab1
    if (not force) and (os.path.exists(output_file) and
        os.path.getmtime(output_file) < os.path.getmtime(self.file_name)):
      return output_file
    fft_map = map_coeffs.fft_map(resolution_factor=resolution_factor)
    if (sigma_scaling):
      fft_map.apply_sigma_scaling()
    else :
      fft_map.apply_volume_scaling()
    map_data = fft_map.real_map()
    write_ccp4_map(
      sites_cart=sites_cart,
      unit_cell=map_coeffs.unit_cell(),
      map_data=map_data,
      n_real=fft_map.n_real(),
      file_name=output_file)
    return output_file

  def auto_open_maps(self, use_filled=True):
    maps = []
    if (self.is_phenix_refine_maps()) or (self.is_cctbx_maps()):
      maps = self.get_phenix_maps()
    elif (self.is_ccp4_style_map()):
      f_map, diff_map = self.get_ccp4_maps()
      if (f_map is not None):
        maps.append(map_info(f_map, MAP_TYPE_F_OBS, "2mFo-DFc"))
      if (diff_map is not None):
        maps.append(map_info(diff_map, MAP_TYPE_DIFF, "mFo-DFc"))
    elif (self.is_solve_map()):
      f_map = self.get_solve_map()
      maps.append(map_info(f_map, MAP_TYPE_F_OBS, "FP,PHIB,FOM"))
    elif (self.is_resolve_map()):
      f_map = self.get_resolve_map()
      maps.append(map_info(f_map, MAP_TYPE_F_OBS, "FP,PHIM,FOMM"))
    elif (self.is_anomalous_map()):
      anom_map = self.get_anomalous_map()
      maps.append(map_info(anom_map, MAP_TYPE_ANOMALOUS, "anomalous"))
    return maps

class map_info(object):
  def __init__(self, map_coeffs, map_type, map_name=None):
    self.map_coeffs = map_coeffs
    self.map_type = map_type
    self.map_name = map_name

  def is_fobs_map(self):
    return (self.map_type & MAP_TYPE_F_OBS)

  def is_difference_map(self):
    return (self.map_type & MAP_TYPE_DIFF)

  def is_anomalous_map(self):
    return (self.map_type & MAP_TYPE_ANOMALOUS)

  def is_neutron_map(self):
    return (self.map_type & MAP_TYPE_NEUTRON)

  def is_filled_map(self):
    return (self.map_type & MAP_TYPE_FILLED)

  def fft_map(self, resolution_factor):
    map_coeffs = self.map_coeffs.unique_under_symmetry()
    map = map_coeffs.fft_map(resolution_factor=resolution_factor)
    map.apply_sigma_scaling()
    return map

  def __getattr__(self, name):
    return getattr(self.map_coeffs, name)

def write_ccp4_map(sites_cart, unit_cell, map_data, n_real, file_name,
    buffer=10):
  import iotbx.mrcfile
  from cctbx import sgtbx
  from scitbx.array_family import flex
  if sites_cart is not None :
    frac_min, frac_max = unit_cell.box_frac_around_sites(
      sites_cart=sites_cart,
      buffer=buffer)
  else :
    frac_min, frac_max = (0.0, 0.0, 0.0), (1.0, 1.0, 1.0)
  gridding_first = tuple([ifloor(f*n) for f,n in zip(frac_min,n_real)])
  gridding_last = tuple([iceil(f*n) for f,n in zip(frac_max,n_real)])
  space_group = sgtbx.space_group_info("P1").group()
  iotbx.mrcfile.write_ccp4_map(
    file_name=file_name,
    unit_cell=unit_cell,
    space_group=space_group,
    gridding_first=gridding_first,
    gridding_last=gridding_last,
    map_data=map_data,
    labels=flex.std_string(["iotbx.map_conversion.write_ccp4_map_box"]))

def write_dsn6_map(sites_cart, unit_cell, map_data, n_real, file_name,
    buffer=10):
  import iotbx.dsn6
  from cctbx import sgtbx
  from scitbx.array_family import flex
  if sites_cart is not None :
    frac_min, frac_max = unit_cell.box_frac_around_sites(
      sites_cart=sites_cart,
      buffer=buffer)
  else :
    frac_min, frac_max = (0.0, 0.0, 0.0), (1.0, 1.0, 1.0)
  gridding_first = tuple([ifloor(f*n) for f,n in zip(frac_min,n_real)])
  gridding_last = tuple([iceil(f*n) for f,n in zip(frac_max,n_real)])
  print("n_real:", n_real)
  print("gridding start:", gridding_first)
  print("gridding end:", gridding_last)
  iotbx.dsn6.write_dsn6_map(
    file_name=file_name,
    unit_cell=unit_cell,
    gridding_first=gridding_first,
    gridding_last=gridding_last,
    map_data=map_data)

########################################################################
# convenience functions
def convert_resolve_map(mtz_file, **kwds):
  server_ = server(file_name=mtz_file)
  return server_.convert_resolve_map(**kwds)

def convert_solve_map(mtz_file, **kwds):
  server_ = server(file_name=mtz_file)
  return server_.convert_solve_map(**kwds)

def convert_ccp4_map(mtz_file, **kwds):
  server_ = server(file_name=mtz_file)
  return server_.convert_ccp4_map(**kwds)

def convert_any_map_coeffs(mtz_file, **kwds):
  server_ = server(file_name=mtz_file)
  return server_.convert_any_map(**kwds)

def convert_phenix_maps(file_base, **kwds):
  server_ = server(file_name=file_base+".mtz")
  return server_.convert_phenix_maps(file_base, **kwds)

def convert_map_coefficients(map_coefficients,
                              mtz_file,
                              **kwds):
  """
  Convert the map coefficients in an MTZ file created by phenix.maps or
  phenix.refine (and related applications).  The parameter block expected in
  map_coefficients is actually in mmtbx/maps/__init__.py, but the I/O belongs
  here.
  """
  outputs = []
  server_ = server(file_name=mtz_file)
  for map in map_coefficients :
    array_label = map.mtz_label_amplitudes + "," + map.mtz_label_phases
    try :
      map_file = server_.convert_any_map(array_label, None, None, **kwds)
    except MapNotFound :
      continue
    outputs.append((map_file, map.map_type))
  return outputs

class write_ccp4_maps_wrapper(object):
  """
  Callable object for running map conversions in parallel (probably overkill,
  but anything we can do to take advantage of multiple CPUs is worth trying).
  """
  def __init__(self, pdb_hierarchy, map_coeffs, output_files,
      resolution_factor):
    adopt_init_args(self, locals())

  def run(self):
    sites_cart = self.pdb_hierarchy.atoms().extract_xyz()
    for map_coeffs, file_name in zip(self.map_coeffs, self.output_files):
      if (map_coeffs is None):
        continue
      fft_map = map_coeffs.fft_map(resolution_factor=self.resolution_factor)
      write_ccp4_map(
        sites_cart=sites_cart,
        unit_cell=map_coeffs.unit_cell(),
        map_data=fft_map.real_map(),
        n_real=fft_map.n_real(),
        file_name=file_name)

def write_map_coeffs(fwt_coeffs, delfwt_coeffs, file_name, anom_coeffs=None,
    fmodel_coeffs=None):
  """
  Convenience function for writing out 2mFo-DFc and mFo-DFc (and, optionally,
  anomalous difference) map coefficients with predefined labels, which will
  be recognized by the auto-open function in Coot.
  """
  import iotbx.mtz
  decorator = iotbx.mtz.label_decorator(phases_prefix="PH")
  mtz_dataset = fwt_coeffs.as_mtz_dataset(
    column_root_label="2FOFCWT",
    label_decorator=decorator)
  mtz_dataset.add_miller_array(
    miller_array=delfwt_coeffs,
    column_root_label="FOFCWT",
    label_decorator=decorator)
  if (anom_coeffs is not None):
    mtz_dataset.add_miller_array(
      miller_array=anom_coeffs,
      column_root_label="ANOM",
      label_decorator=decorator)
  if (fmodel_coeffs is not None):
    mtz_dataset.add_miller_array(
      miller_array=fmodel_coeffs,
      column_root_label="F-model",
      label_decorator=decorator)
  mtz_object = mtz_dataset.mtz_object()
  mtz_object.write(file_name=file_name)
  del mtz_object

# FIXME this is probably too much code duplication...
# used by phenix.composite_omit_map, etc.
def write_map_coefficients_generic(map_coeffs, map_types, file_name):
  import iotbx.mtz
  mtz_dataset = None
  for map_type, map_coeffs in zip(map_types, map_coeffs):
    if map_coeffs.anomalous_flag():
      map_coeffs = map_coeffs.average_bijvoet_mates()
    dec = iotbx.mtz.label_decorator(phases_prefix="PH")
    if (map_type == "2mFo-DFc"):
      label = "2FOFCWT"
    elif (map_type == "mFo-DFc"):
      label = "FOFCWT"
    elif (map_type == "mFo"):
      label = "FWT"
      dec = iotbx.mtz.ccp4_label_decorator()
    elif (map_type in ["anom", "anomalous"]):
      label = "ANOM"
    else :
      label = map_type
    if (mtz_dataset is None):
      mtz_dataset = map_coeffs.as_mtz_dataset(
        column_root_label=label,
        label_decorator=dec)
    else :
      mtz_dataset.add_miller_array(
        miller_array=map_coeffs,
        column_root_label=label,
        label_decorator=dec)
  mtz_dataset.mtz_object().write(file_name)

def get_map_summary(map, resolution_factor=0.25):
  info = []
  real_map = map.real_map_unpadded()
  n_grid_points = real_map.size()
  map.apply_volume_scaling()
  stats_vol = map.statistics()
  info.append(("Grid points (with resolution_factor=%g)" % resolution_factor,
    str(n_grid_points)))
  info.append(("Min value (volume-scaled)", "%.2f" % stats_vol.min()))
  info.append(("Max value (volume-scaled)", "%.2f" % stats_vol.max()))
  info.append(("Mean value (volume-scaled)", "%.2f" % stats_vol.mean()))
  info.append(("Sigma (volume-scaled)", "%.2f" % stats_vol.sigma()))
  map.apply_sigma_scaling()
  stats_sigma = map.statistics()
  info.append(("Min value (sigma-scaled)", "%.2f" % stats_sigma.min()))
  info.append(("Max value (sigma-scaled)", "%.2f" % stats_sigma.max()))
  from cctbx import maptbx
  more_stats = maptbx.more_statistics(map.real_map(False))
  info.append(("Skewness", "%.2f" % more_stats.skewness()))
  return info

class auto_convert_map_coefficients(object):
  def __init__(self, mtz_file, pdb_file=None, resolution_factor=0.25):
    self.f_map = None
    self.diff_map = None
    self.anom_map = None
    self.f_map_type = None
    map_server = server(file_name=mtz_file)
    if (map_server.is_phenix_refine_maps()):
      self.f_map = map_server.convert_complex_map(
        map_label="2FOFCWT,PH2FOFCWT",
        pdb_file=pdb_file,
        use_standard_map_names=True,
        resolution_factor=resolution_factor)
      self.f_map_type = "2mFo-DFc"
      self.diff_map = map_server.convert_complex_map(
        map_label="FOFCWT,PHFOFCWT",
        pdb_file=pdb_file,
        use_standard_map_names=True,
        resolution_factor=resolution_factor)
    elif (map_server.is_ccp4_style_map()):
      self.f_map = map_server.convert_complex_map(
        map_label="FWT,PHWT",
        pdb_file=pdb_file,
        use_standard_map_names=True,
        resolution_factor=resolution_factor)
      self.f_map_type = "fwt" # could be 2mFo-DFc, or resolve Fobs map
      if (map_server.is_ccp4_style_difference_map()):
        self.f_map_type = "2mFo-DFc" # now we know it's definitely 2mFo-DFc
        self.diff_map = map_server.convert_complex_map(
          map_label="DELFWT,PHDELWT",
          pdb_file=pdb_file,
          use_standard_map_names=True,
          resolution_factor=resolution_factor)
    if (map_server.is_anomalous_map()):
      self.anom_map = map_server.convert_anomalous_map(
        pdb_file=pdb_file,
        use_standard_map_names=True,
        resolution_factor=resolution_factor)

def combine_f_phi_and_fom(f, phi, fom=None, weighted=True):
  info = f.info()
  if (f.anomalous_flag()):
    f = f.average_bijvoet_mates()
  f = f.map_to_asu().set_info(info)
  if (f.is_complex_array()):
    return f
  else :
    assert f.is_real_array()
    if (weighted) and (fom is not None):
      f, fom = f.common_sets(other=fom.map_to_asu())
      f = f * fom
    f, phi = f.common_sets(other=phi.map_to_asu())
    # XXX is deg always True?
    return f.phase_transfer(phi,
      deg=True).customized_copy(sigmas=None).set_info(info)


 *******************************************************************************


 *******************************************************************************
iotbx/merging_statistics.py
"""
Routines for calculating common metrics of data quality based on merging of
redundant observations.
"""

from __future__ import absolute_import, division, print_function

import six
import sys
from math import sqrt
from six.moves import cStringIO as StringIO

from iotbx import data_plots
from libtbx.str_utils import make_sub_header, format_value
from libtbx.utils import Sorry, null_out
from libtbx import group_args, Auto
from scitbx.array_family import flex


citations_str = """\
  Diederichs K & Karplus PA (1997) Nature Structural Biology 4:269-275
    (with erratum in: Nat Struct Biol 1997 Jul;4(7):592)
  Weiss MS (2001) J Appl Cryst 34:130-135.
  Dauter, Z. (2006). Acta Cryst. D62, 867-876.
  Karplus PA & Diederichs K (2012) Science 336:1030-3."""

sigma_filtering_phil_str = """
sigma_filtering = *auto xds scala scalepack
  .type = choice
  .short_caption = Sigma(I) filtering convention
  .help = Determines how data are filtered by SigmaI and I/SigmaI.  XDS \
    discards reflections whose intensity after merging is less than -3*sigma, \
    Scalepack uses the same cutoff before merging, and SCALA does not do any \
    filtering.  Reflections with negative SigmaI will always be discarded.
"""

merging_params_str = """
high_resolution = None
  .type = float
  .input_size = 64
low_resolution = None
  .type = float
  .input_size = 64
n_bins = 10
  .type = int
  .short_caption = Number of resolution bins
  .input_size = 64
  .style = spinner
reflections_per_bin = None
  .type = int(value_min=1)
  .help = "Minimum number of reflections per bin in counting_sorted binning"
binning_method = *volume counting_sorted
  .type = choice
  .help = "Use equal-volume bins or bins with approximately equal numbers of reflections per bin."
extend_d_max_min = False
  .type = bool
  .expert_level = 2
anomalous = False
  .type = bool
  .short_caption = Keep anomalous pairs separate in merging statistics
%s
use_internal_variance = True
  .type = bool
  .short_caption = Use internal variance of the data in the calculation of the merged sigmas
eliminate_sys_absent = True
  .type = bool
  .short_caption = Eliminate systematically absent reflections before computation of merging statistics.
cc_one_half_significance_level = None
  .type = float(value_min=0, value_max=None)
cc_one_half_method = *half_dataset sigma_tau
  .type = choice
""" % sigma_filtering_phil_str

class StatisticsError(RuntimeError):
  '''Custom error class, so that these errors can be caught and dealt with appropriately.'''

class model_based_arrays(object):
  """
  Container for observed and calculated intensities, along with the selections
  for work and free sets; these should be provided by mmtbx.f_model.  It is
  assumed (or hoped) that the resolution range of these arrays will be
  the same as that of the unmerged data, but the current implementation does
  not force this.
  """
  def __init__(self, f_obs, i_obs, i_calc, work_sel, free_sel):
    assert (i_obs.data().size() == i_calc.data().size() ==
            work_sel.data().size() == free_sel.data().size())
    assert (len(f_obs.data()) <= len(i_obs.data()))
    self.f_obs = f_obs
    self.i_obs = i_obs.common_set(other=self.f_obs)
    self.i_calc = i_calc.common_set(other=self.f_obs)
    self.work_sel = work_sel.common_set(other=self.f_obs)
    self.free_sel = free_sel.common_set(other=self.f_obs)

  def cc_work_and_free(self, other):
    """
    Given a unique array of arbitrary resolution range, extract the equivalent
    reflections from the observed and calculated intensities, and calculate
    CC and R-factor for work and free sets.  Currently, these statistics will
    be None if there are no matching reflections.
    """
    assert (self.i_obs.is_similar_symmetry(other))
    i_obs_sel = self.i_obs.common_set(other=other)
    f_obs_sel = self.f_obs.common_set(other=other)
    i_calc_sel = self.i_calc.common_set(other=other)
    work_sel = self.work_sel.common_set(other=other)
    free_sel = self.free_sel.common_set(other=other)
    if (len(i_obs_sel.data()) == 0) : # XXX should this raise an error?
      return [None] * 4
    i_obs_work = i_obs_sel.select(work_sel.data())
    i_calc_work = i_calc_sel.select(work_sel.data())
    i_obs_free = i_obs_sel.select(free_sel.data())
    i_calc_free = i_calc_sel.select(free_sel.data())
    f_obs_work = f_obs_sel.select(work_sel.data())
    f_obs_free = f_obs_sel.select(free_sel.data())
    if (len(f_obs_work.data()) > 0) and (len(f_obs_free.data()) > 0):
      cc_work = flex.linear_correlation(i_obs_work.data(),
        i_calc_work.data()).coefficient()
      cc_free = flex.linear_correlation(i_obs_free.data(),
        i_calc_free.data()).coefficient()
      r_work = f_obs_work.r1_factor(i_calc_work.f_sq_as_f())
      r_free = f_obs_free.r1_factor(i_calc_free.f_sq_as_f())
      return cc_work, cc_free, r_work, r_free
    return [None] * 4

def get_filtering_convention(i_obs, sigma_filtering=Auto):
  info = i_obs.info()
  if sigma_filtering in [Auto, "auto"]:
    if info is not None and info.source_type == "xds_ascii":
      sigma_filtering = "xds"
    elif info is not None and info.source_type == "ccp4_mtz":
      sigma_filtering = "scala"
    elif info is not None and info.source_type == "scalepack_no_merge_original_index":
      sigma_filtering = "scalepack"
    else: # XXX default to the most conservative method
      sigma_filtering = "scala"
  return sigma_filtering

class filter_intensities_by_sigma(object):
  """
  Wrapper for filtering intensities based on one of several different
  conventions:

    - in XDS, reflections where I < -3*sigmaI after merging are deleted from
      both the merged and unmerged arrays
    - in Scalepack, the filtering is done before merging
    - SCALA and AIMLESS do not do any filtering

  note that ctruncate and cctbx.french_wilson (any others?) do their own
  filtering, e.g. discarding I < -4*sigma in cctbx.french_wilson.
  """
  def __init__(self, array, sigma_filtering=Auto, use_internal_variance=True):
    sigma_filtering = get_filtering_convention(array, sigma_filtering)
    assert (sigma_filtering in ["scala","scalepack","xds", None])
    self.n_rejected_before_merge = self.n_rejected_after_merge = 0
    merge = array.merge_equivalents(use_internal_variance=use_internal_variance)
    array_merged = merge.array()
    reject_sel = None
    self.observed_criterion_sigma_I = None
    if (sigma_filtering == "xds"):
      self.observed_criterion_sigma_I = -3
      reject_sel = (array_merged.data() < -3*array_merged.sigmas())
      self.n_rejected_after_merge = reject_sel.count(True)
      bad_data = array_merged.select(reject_sel)
      array = array.delete_indices(other=bad_data)
      # and merge again...
      merge = array.merge_equivalents(
        use_internal_variance=use_internal_variance)
      array_merged = merge.array()
    elif (sigma_filtering == "scalepack"):
      self.observed_criterion_sigma_I = -3
      reject_sel = (array.data() < -3* array.sigmas())
      self.n_rejected_before_merge = reject_sel.count(True)
      array = array.select(~reject_sel)
      merge = array.merge_equivalents(
        use_internal_variance=use_internal_variance)
      array_merged = merge.array()
    elif (sigma_filtering == "scala") or (sigma_filtering is None):
      pass
    else :
      raise ValueError("Unrecognized sigmaI filtering convention '%s'." %
        sigma_filtering)
    self.array = array
    self.merge = merge
    self.array_merged = array_merged

class merging_stats(object):
  """
  Calculate standard merging statistics for (scaled) unmerged data.  Usually
  these statistics will consider I(+) and I(-) as observations of the same
  reflection, but these can be kept separate instead if desired.

  Reflections with negative sigmas will be discarded, and depending on the
  program we're trying to mimic, excessively negative intensities.
  """
  def __init__(self,
      array,
      d_max_min=None,
      model_arrays=None,
      anomalous=False,
      debug=None,
      sigma_filtering="scala",
      use_internal_variance=True,
      cc_one_half_significance_level=None,
      cc_one_half_method='half_dataset',
      anomalous_probability_plot=False,
      anomalous_probability_plot_expected_delta=0.9,
      ):
    import cctbx.miller
    assert cc_one_half_method in ('half_dataset', 'sigma_tau')
    self.cc_one_half_method = cc_one_half_method
    assert (array.sigmas() is not None)
    non_negative_sel = array.sigmas() >= 0
    self.n_neg_sigmas = non_negative_sel.count(False)
    positive_sel = array.sigmas() > 0
    self.n_zero_sigmas = positive_sel.count(False) - self.n_neg_sigmas
    array = array.select(positive_sel)
    # calculate CC(anom) first, because the default behavior is to switch to
    # non-anomalous data for the rest of the analyses
    self.cc_anom, n_anom_pairs = array.half_dataset_anomalous_correlation(
      return_n_pairs=True)
    self.cc_anom_significance = None
    self.cc_anom_critical_value = None
    try:
      self.r_anom = array.r_anom()
    except AssertionError:
      self.r_anom = None
    array = array.customized_copy(anomalous_flag=anomalous).map_to_asu()
    array = array.sort("packed_indices")
    filter = filter_intensities_by_sigma(
      array=array,
      sigma_filtering=sigma_filtering,
      use_internal_variance=use_internal_variance)
    if (d_max_min is None):
      d_max_min = array.d_max_min()
    self.d_max, self.d_min = d_max_min
    self.observed_criterion_sigma_I = filter.observed_criterion_sigma_I
    array = filter.array
    merge = filter.merge
    array_merged = filter.array_merged
    self.n_rejected_before_merge = filter.n_rejected_before_merge
    self.n_rejected_after_merge = filter.n_rejected_after_merge
    self.n_obs = array.indices().size()
    self.n_uniq = array_merged.indices().size()
    complete_set = array_merged.complete_set().resolution_filter(
      d_min=self.d_min, d_max=self.d_max)
    if (self.n_uniq == 0):
      complete_set = cctbx.miller.build_set(
        crystal_symmetry=array_merged,
        anomalous_flag=anomalous,
        d_min=self.d_min).resolution_filter(d_min=self.d_min, d_max=self.d_max)
    n_expected = len(complete_set.indices())
    if (n_expected == 0):
      raise StatisticsError(
          "No reflections within specified resolution range (%g - %g)" % (self.d_max, self.d_min))
    self.completeness = min(self.n_uniq / n_expected, 1.)
    self.anom_completeness = None
    self.anom_signal = None
    self.delta_i_mean_over_sig_delta_i_mean = None
    self.anom_probability_plot_all_data = None
    self.anom_probability_plot_expected_delta = None
    # TODO also calculate when anomalous=False, since it is customary to
    # calculate merging statistics with F+ and F- treated as redundant
    # observations even when we're going to keep them separate.
    if (anomalous):
      self.anom_completeness = array_merged.anomalous_completeness(
        d_max=self.d_max, d_min=self.d_min)
      self.anom_signal = array_merged.anomalous_signal()
      anomalous_differences = array_merged.anomalous_differences()
      nonzero_array = anomalous_differences.select(anomalous_differences.sigmas() > 0)
      if nonzero_array.size():
        self.delta_i_mean_over_sig_delta_i_mean = flex.mean(
          flex.abs(nonzero_array.data()))/flex.mean(nonzero_array.sigmas())
      if anomalous_probability_plot:
        self.anom_probability_plot_all_data = array_merged.anomalous_probability_plot()
        self.anom_probability_plot_expected_delta = array_merged.anomalous_probability_plot(
          expected_delta=anomalous_probability_plot_expected_delta
        )

    redundancies = merge.redundancies().data()
    self.redundancies = {}
    self.mean_redundancy = 0
    self.i_mean = 0
    self.sigi_mean = 0
    self.i_over_sigma_mean = 0
    self.i_mean_over_sigi_mean = 0
    self.unmerged_i_over_sigma_mean = 0
    self.cc_one_half = 0
    self.cc_one_half_n_refl = 0
    self.cc_one_half_significance = None
    self.cc_one_half_critical_value = None
    self.cc_one_half_sigma_tau = 0
    self.cc_one_half_sigma_tau_n_refl = 0
    self.cc_one_half_sigma_tau_significance = None
    self.cc_one_half_sigma_tau_critical_value = None
    self.cc_star = 0
    self.r_merge = self.r_meas = self.r_pim = None
    for x in sorted(set(redundancies)):
      self.redundancies[x] = redundancies.count(x)
    if (self.n_uniq > 0):
      self.mean_redundancy = flex.mean(redundancies.as_double())
      self.i_mean = flex.mean(array_merged.data())
      self.sigi_mean = flex.mean(array_merged.sigmas())
      nonzero_array = array_merged.select(array_merged.sigmas() > 0)
      i_over_sigma = nonzero_array.data() / nonzero_array.sigmas()
      self.i_over_sigma_mean = flex.mean(i_over_sigma)
      self.i_mean_over_sigi_mean = self.i_mean/self.sigi_mean
      nonzero_array = array.select(array.sigmas() > 0)
      unmerged_i_over_sigma = nonzero_array.data() / nonzero_array.sigmas()
      self.unmerged_i_over_sigma_mean = flex.mean(unmerged_i_over_sigma)
      self.r_merge = merge.r_merge()
      self.r_meas = merge.r_meas()
      self.r_pim = merge.r_pim()
      self.cc_one_half, self.cc_one_half_n_refl = cctbx.miller.compute_cc_one_half(
        unmerged=array, return_n_refl=True)
      self.cc_one_half_sigma_tau, self.cc_one_half_sigma_tau_n_refl = \
        array.cc_one_half_sigma_tau(return_n_refl=True)
      if cc_one_half_significance_level is not None:
        def compute_cc_significance(r, n, p):
          # https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient#Testing_using_Student.27s_t-distribution
          if r == -1 or n <= 2:
            significance = False
            critical_value = 0
          else:
            from scitbx.math import distributions
            dist = distributions.students_t_distribution(n-2)
            t = dist.quantile(1-p)
            critical_value = t/sqrt(n - 2 + t**2)
            significance = r > critical_value
          return significance, critical_value
        self.cc_one_half_significance, self.cc_one_half_critical_value = \
          compute_cc_significance(
            self.cc_one_half, self.cc_one_half_n_refl,
            cc_one_half_significance_level)
        self.cc_anom_significance, self.cc_anom_critical_value = \
          compute_cc_significance(
            self.cc_anom, n_anom_pairs, cc_one_half_significance_level)
        self.cc_one_half_sigma_tau_significance, self.cc_one_half_sigma_tau_critical_value = \
          compute_cc_significance(
            self.cc_one_half_sigma_tau, array_merged.size(),
            cc_one_half_significance_level)
      if (self.cc_one_half == 0):
        self.cc_star = 0
      elif (self.cc_one_half < -0.999):
        self.cc_star = float("-inf")
      else :
        mult = 1.
        if (self.cc_one_half < 0):
          mult = -1.
        self.cc_star = mult * sqrt((2*abs(self.cc_one_half)) /
                                   (1 + self.cc_one_half))
    self.cc_work = self.cc_free = self.r_work = self.r_free = None
    self.has_cc_work = False
    if (model_arrays is not None) and (self.n_uniq > 0):
      self.has_cc_work = True
      self.cc_work, self.cc_free, self.r_work, self.r_free = \
        model_arrays.cc_work_and_free(array_merged)

  @property
  def anom_half_corr(self):
    return getattr(self, "cc_anom", None)

  def format(self):
    if self.cc_one_half_method == 'sigma_tau':
      cc_one_half = self.cc_one_half_sigma_tau
      cc_one_half_significance = self.cc_one_half_sigma_tau_significance
    else:
      cc_one_half = self.cc_one_half
      cc_one_half_significance = self.cc_one_half_significance
    return "%6.2f %6.2f %6d %6d   %5.2f %6.2f  %8.1f  %6.1f  %s  %s  %s  %s  % 5.3f%s  % 5.3f%s" % (
      self.d_max,
      self.d_min,
      self.n_obs,
      self.n_uniq,
      self.mean_redundancy,
      self.completeness*100,
      self.i_mean,
      self.i_over_sigma_mean,
      format_value("% 7.3f", self.r_merge),
      format_value("% 7.3f", self.r_meas),
      format_value("% 7.3f", self.r_pim),
      format_value("% 7.3f", self.r_anom) if self.r_anom is not None else "",
      cc_one_half,
      "*" if cc_one_half_significance else "",
      self.cc_anom,
      "*" if self.cc_anom_significance else "")

  def format_for_model_cc(self):
    return "%6.2f  %6.2f  %6d  %6.2f  %6.2f  %5.3f  %5.3f   %s   %s  %s  %s"%(
      self.d_max, self.d_min, self.n_uniq,
      self.completeness*100, self.i_over_sigma_mean,
      self.cc_one_half, self.cc_star,
      format_value("%5.3f", self.cc_work), format_value("%5.3f", self.cc_free),
      format_value("%5.3f", self.r_work), format_value("%5.3f", self.r_free))

  def format_for_gui(self):
    return [ "%.2f - %.2f" % (self.d_max, self.d_min),
             str(self.n_obs),
             str(self.n_uniq),
             "%.1f" % self.mean_redundancy,
             "%.1f %%" % (self.completeness * 100),
             "%.1f" % self.i_over_sigma_mean,
             "%.3f" % self.r_merge,
             "%.3f" % self.r_meas,
             "%.3f" % self.r_pim,
             "%.3f" % self.cc_one_half ]

  def format_for_cc_star_gui(self):
      return [ "%.2f - %.2f" % (self.d_max, self.d_min),
             str(self.n_uniq),
             "%.1f %%" % (self.completeness * 100),
             "%.1f" % self.i_over_sigma_mean,
             "%.3f" % self.cc_one_half,
             "%.3f" % self.cc_star,
              format_value("%5.3f", self.cc_work),
              format_value("%5.3f", self.cc_free),
              format_value("%5.3f", self.r_work),
              format_value("%5.3f", self.r_free) ]

  def as_dict(self):
    d = {
      'd_star_sq_min': 1/self.d_min**2,
      'd_star_sq_max': 1/self.d_max**2,
      'n_obs': self.n_obs,
      'n_uniq': self.n_uniq,
      'multiplicity': self.mean_redundancy,
      'completeness': self.completeness*100,
      'i_mean': self.i_mean,
      'i_over_sigma_mean': self.i_over_sigma_mean,
      'r_merge': self.r_merge,
      'r_meas': self.r_meas,
      'r_pim': self.r_pim,
      'r_anom': self.r_anom,
      'cc_one_half': self.cc_one_half,
      'cc_one_half_significance': self.cc_one_half_significance,
      'cc_one_half_critical_value': self.cc_one_half_critical_value,
      'cc_anom': self.cc_anom,
      'anom_completeness': self.anom_completeness,
      'anom_signal': self.anom_signal,
      'anom_probability_plot_all_data':
        self.anom_probability_plot_all_data._asdict()
        if self.anom_probability_plot_all_data is not None else None,
      'anom_probability_plot_expected_delta':
        self.anom_probability_plot_expected_delta._asdict()
        if self.anom_probability_plot_expected_delta is not None else None,
      'delta_i_mean_over_sig_delta_i_mean': self.delta_i_mean_over_sig_delta_i_mean,
    }
    if (self.cc_work is not None):
      d.update({
        'cc_star': self.cc_star,
        'cc_work': self.cc_work,
        'cc_free': self.cc_free,
        'r_work': self.r_work,
        'r_free': self.r_free
      })
    return d

  def table_data(self):
    table = [(1/self.d_min**2), self.n_obs, self.n_uniq, self.mean_redundancy,
            self.completeness*100, self.i_mean, self.i_over_sigma_mean,
            self.r_merge, self.r_meas, self.r_pim, self.r_anom, self.cc_one_half,
            self.cc_one_half_significance, self.cc_one_half_critical_value,
            self.cc_anom]
    if self.has_cc_work:
      table.extend([self.cc_star, self.cc_work, self.cc_free, self.r_work,
        self.r_free])
    return table

  def show_summary(self, out=sys.stdout, prefix=""):
    print(prefix+"Resolution: %.2f - %.2f" % (self.d_max, self.d_min), file=out)
    print(prefix+"Observations: %d" % self.n_obs, file=out)
    print(prefix+"Unique reflections: %d" % self.n_uniq, file=out)
    print(prefix+"Multiplicity: %.1f" % self.mean_redundancy, file=out)
    print(prefix+"Completeness: %.2f%%" % (self.completeness*100), file=out)
    print(prefix+"Mean intensity: %.1f" % self.i_mean, file=out)
    print(prefix+"Mean I/sigma(I): %.1f" % self.i_over_sigma_mean, file=out)
    # negative sigmas are rejected before merging
    if (self.n_neg_sigmas > 0):
      print(prefix+"SigI < 0 (rejected): %d observations" % \
        self.n_neg_sigmas, file=out)
    # excessively negative intensities can be rejected either before or after
    # merging, depending on convention used
    if (self.n_rejected_before_merge > 0):
      print(prefix+"I < -3*SigI (rejected): %d observations" % \
        self.n_rejected_before_merge, file=out)
    if (self.n_rejected_after_merge > 0):
      print(prefix+"I < -3*SigI (rejected): %d reflections" % \
        self.n_rejected_after_merge, file=out)
    print(prefix+"R-merge: %5.3f" % self.r_merge, file=out)
    print(prefix+"R-meas:  %5.3f" % self.r_meas, file=out)
    print(prefix+"R-pim:   %5.3f" % self.r_pim, file=out)

  def show_anomalous_probability_plot(self, out=sys.stdout, prefix=""):
    if (
      self.anom_probability_plot_all_data is not None and
      self.anom_probability_plot_all_data.slope is not None
      ):
      print(prefix+"Anomalous probability plot (all data):", file=out)
      print(prefix+"  slope:     %5.3f" % self.anom_probability_plot_all_data.slope, file=out)
      print(prefix+"  intercept: %5.3f" % self.anom_probability_plot_all_data.intercept, file=out)
      print(prefix+"  n_pairs:   %d" % self.anom_probability_plot_all_data.n_pairs, file=out)
    if (
      self.anom_probability_plot_expected_delta is not None and
      self.anom_probability_plot_expected_delta.slope is not None
      ):
      print(
        prefix+"Anomalous probability plot (expected delta = %g):"
        % self.anom_probability_plot_expected_delta.expected_delta,
        file=out,
      )
      print(prefix+"  slope:     %5.3f" % self.anom_probability_plot_expected_delta.slope, file=out)
      print(prefix+"  intercept: %5.3f" % self.anom_probability_plot_expected_delta.intercept, file=out)
      print(prefix+"  n_pairs:   %d" % self.anom_probability_plot_expected_delta.n_pairs, file=out)

class dataset_statistics(object):
  """
  Container for overall and by-shell merging statistics, plus a table_data
  object suitable for displaying graphs (or outputting loggraph format).
  """
  def __init__(self,
      i_obs,
      crystal_symmetry=None,
      d_min=None,
      d_max=None,
      anomalous=False,
      n_bins=10,
      reflections_per_bin=None,
      binning_method='volume',
      debug=False,
      file_name=None,
      model_arrays=None,
      sigma_filtering=Auto,
      use_internal_variance=True,
      eliminate_sys_absent=True,
      d_min_tolerance=1.e-6,
      extend_d_max_min=False,
      cc_one_half_significance_level=None,
      cc_one_half_method='half_dataset',
      assert_is_not_unique_set_under_symmetry=True,
      log=None):
    self.file_name = file_name
    if (log is None) : log = null_out()
    assert (i_obs.sigmas() is not None)
    info = i_obs.info()
    sigma_filtering = get_filtering_convention(i_obs, sigma_filtering)
    if (crystal_symmetry is None):
      assert (i_obs.space_group() is not None)
      crystal_symmetry = i_obs.crystal_symmetry()
    self.crystal_symmetry = crystal_symmetry
    i_obs = i_obs.customized_copy(
      crystal_symmetry=crystal_symmetry).set_info(info)
    if (
      assert_is_not_unique_set_under_symmetry and
      (
        (anomalous and i_obs.as_anomalous_array().is_unique_set_under_symmetry()) or
        i_obs.as_non_anomalous_array().is_unique_set_under_symmetry()
      )
    ):
      raise Sorry(
        "The data in %s are already merged.  Only unmerged (but scaled) data "
        "may be used in this program." %
        i_obs.info().label_string()
      )
    d_min_cutoff = d_min
    d_max_cutoff = d_max
    if (d_min is not None):
      d_min_cutoff *= (1-d_min_tolerance)
      if (d_max is not None):
        assert (d_max > d_min)
    if (d_max is not None):
      d_max_cutoff *= 1+d_min_tolerance
    i_obs = i_obs.resolution_filter(
      d_min=d_min_cutoff,
      d_max=d_max_cutoff).set_info(info)
    if (i_obs.size() == 0):
      raise Sorry("No reflections left after applying resolution cutoffs.")
    i_obs.show_summary(f=log)
    self.anom_extra = ""
    if (not anomalous):
      i_obs = i_obs.customized_copy(anomalous_flag=False).set_info(info)
      self.anom_extra = " (non-anomalous)"
    overall_d_max_min = None
    # eliminate_sys_absent() before setting up binner to ensure consistency
    # between reported overall d_min/max and d_min/max for resolution bins"
    if eliminate_sys_absent:
      i_obs = i_obs.eliminate_sys_absent()
    if extend_d_max_min :
      i_obs.setup_binner(
        n_bins=n_bins,
        d_max=d_max_cutoff,
        d_min=d_min_cutoff)
      overall_d_max_min = d_max_cutoff, d_min_cutoff
    else :
      if binning_method == 'volume':
        i_obs.setup_binner(n_bins=n_bins)
      elif binning_method == 'counting_sorted':
        i_obs.setup_binner_counting_sorted(
          n_bins=n_bins, reflections_per_bin=reflections_per_bin
        )
    self.overall = merging_stats(i_obs,
      d_max_min=overall_d_max_min,
      model_arrays=model_arrays,
      anomalous=anomalous,
      debug=debug,
      sigma_filtering=sigma_filtering,
      use_internal_variance=use_internal_variance,
      cc_one_half_significance_level=cc_one_half_significance_level,
      cc_one_half_method=cc_one_half_method,
      anomalous_probability_plot=True,
    )
    self.bins = []
    title = "Intensity merging statistics"
    column_labels = ["1/d**2","N(obs)","N(unique)","Multiplicity","Completeness",
        "Mean(I)", "Mean(I/sigma)", "R-merge", "R-meas", "R-pim", "R-anom", "CC1/2",
        "CC(anom)"]
    graph_names = ["Reflection counts", "Multiplicity", "Completeness",
        "Mean(I)", "Mean(I/sigma)", "R-factors", "CC1/2", "CC(anom)"]
    graph_columns = [[0,1,2],[0,3],[0,4],[0,5],[0,6],[0,7,8,9],[0,11],[0,13]]
    if cc_one_half_significance_level is not None:
      column_labels.extend(["CC1/2 significance", "CC1/2 critical value"])
      graph_names.extend(["CC1/2 significance", "CC1/2 critical value"])
      graph_columns[-2] = [0,10,12]
    #--- CC* mode
    if (model_arrays is not None):
      title = "Model quality and intensity merging statistics"
      column_labels.extend(["CC*", "CC(work)", "CC(free)", "R-work", "R-free"])
      graph_names.extend(["CC*", "Model R-factors"])
      graph_columns.extend([[0,11,14,15],[0,16,17]])
    #---
    self.table = data_plots.table_data(
      title=title,
      column_labels=column_labels,
      graph_names=graph_names,
      graph_columns=graph_columns,
      x_is_inverse_d_min=True,
      force_exact_x_labels=True)
    for bin in i_obs.binner().range_used():
      sele_unmerged = i_obs.binner().selection(bin)
      bin_stats = merging_stats(i_obs.select(sele_unmerged),
        d_max_min=i_obs.binner().bin_d_range(bin),
        model_arrays=model_arrays,
        anomalous=anomalous,
        debug=debug,
        sigma_filtering=sigma_filtering,
        use_internal_variance=use_internal_variance,
        cc_one_half_significance_level=cc_one_half_significance_level,
        cc_one_half_method=cc_one_half_method)
      self.bins.append(bin_stats)
      self.table.add_row(bin_stats.table_data())

    self.cc_one_half_overall = flex.mean_weighted(
      flex.double(b.cc_one_half for b in self.bins),
      flex.double(b.cc_one_half_n_refl for b in self.bins))
    self.cc_one_half_sigma_tau_overall = flex.mean_weighted(
      flex.double(b.cc_one_half_sigma_tau for b in self.bins),
      flex.double(b.cc_one_half_sigma_tau_n_refl for b in self.bins))

  @property
  def signal_table(self):
    column_labels = ["1/d**2","N(obs)","N(unique)","Multiplicity","Completeness",
        "Mean(I)", "Mean(I/sigma)", ]
    graph_names = ["Reflection counts", "Multiplicity", "Completeness",
        "Mean(I)", "Mean(I/sigma)",]
    graph_columns = [[0,1,2],[0,3],[0,4],[0,5],[0,6],]
    table = data_plots.table_data(
      title="Statistics for multiplicity, completeness, and signal",
      column_labels=column_labels,
      graph_names=graph_names,
      graph_columns=graph_columns,
      column_formats=["%6.2f","%6d","%6d","%5.2f","%6.2f","%8.1f","%6.1f"],
      x_is_inverse_d_min=True,
      force_exact_x_labels=True)
    for bin in self.bins :
      data = bin.table_data()
      table.add_row(data[0:7])
    return table

  @property
  def quality_table(self):
    column_labels = ["1/d**2", "R-merge", "R-meas", "R-pim", "CC1/2",
                     "CC(anom)"]
    graph_columns = [[0,1,2,3],[0,4],[0,5]]
    graph_names = ["R-factors", "CC1/2", "CC(anom)"]
    table = data_plots.table_data(
      title="Statistics for dataset consistency",
      column_labels=column_labels,
      column_formats=["%6.2f","%5.3f", "%5.3f", "%5.3f", "%5.3f", "%5.3f"],
      graph_names=graph_names,
      graph_columns=graph_columns,
      x_is_inverse_d_min=True,
      force_exact_x_labels=True)
    for bin in self.bins :
      data = bin.table_data()
      table.add_row([ data[0] ] + data[7:10] + [ data[11] ] + [ data[-1] ])
    return table

  @property
  def cc_anom_table(self):
    column_labels = ["d_min", "CC(anom)"]
    graph_columns = [[0,1]]
    graph_names = ["CC(anom)"]
    table = data_plots.table_data(
      title="Half-dataset anomalous correlation",
      column_labels=column_labels,
      column_formats=["%6.2f", "%5.3f"],
      graph_names=graph_names,
      graph_columns=graph_columns,
      x_is_inverse_d_min=True,
      force_exact_x_labels=True)
    for bin in self.bins :
      data = bin.table_data()
      table.add_row([ (1/bin.d_min**2), bin.cc_anom ])
    return table

  def show_loggraph(self, out=None):
    if (out is None) : out = sys.stdout
    print("", file=out)
    print(self.table.format_loggraph(), file=out)
    print("", file=out)

  def show(self, out=None, header=True):
    if (out is None) : out = sys.stdout
    if (header):
      make_sub_header("Merging statistics", out=out)
    self.overall.show_summary(out)
    print("", file=out)
    if self.overall.anom_probability_plot_all_data is not None:
      self.overall.show_anomalous_probability_plot(out)
      print("", file=out)
    print("Multiplicities%s:" % self.anom_extra, file=out)
    n_obs = sorted(self.overall.redundancies.keys())
    for x in n_obs :
      print("  %d : %d" % (x, self.overall.redundancies[x]), file=out)
    print("", file=out)
    print("""\
  Statistics by resolution bin:
 d_max  d_min   #obs  #uniq   mult.  %%comp       <I>  <I/sI>    r_mrg   r_meas    r_pim   r_anom   %scc1/2   cc_ano""" %(
      ' ' if self.overall.cc_one_half_significance is not None else ''), file=out)
    for bin_stats in self.bins :
      print(bin_stats.format(), file=out)
    print(self.overall.format(), file=out)

  def show_cc_star(self, out=None):
    make_sub_header("CC* and related statistics", out=out)
    print("""\
 d_max   d_min  n_uniq  compl. <I/sI>  cc_1/2    cc* cc_work cc_free r_work r_free""", file=out)
    for k, bin in enumerate(self.bins):
      print(bin.format_for_model_cc(), file=out)
    print(self.overall.format_for_model_cc(), file=out)

  def as_dict(self):
    d = {}
    for bin_stats in self.bins:
      for k, v in bin_stats.as_dict().items():
        d.setdefault(k, [])
        d[k].append(v)
    d['overall'] = self.overall.as_dict()
    return d

  def as_json(self, file_name=None, indent=None):
    import json
    json_str = json.dumps(self.as_dict(), indent=indent)
    if file_name is not None:
      with open(file_name, 'w') as f:
        print(json_str, file=f)
    return json_str

  def extract_outer_shell_stats(self):
    """
    For compatibility with iotbx.logfiles (which should probably now be
    deprecated) and phenix.table_one
    """
    shell = self.bins[-1]
    return group_args(
      d_max_min=(shell.d_max, shell.d_min),
      n_refl=shell.n_uniq,
      n_refl_all=shell.n_obs,
      completeness=shell.completeness,
      multiplicity=shell.mean_redundancy, # XXX bad
      r_sym=shell.r_merge,
      r_meas=shell.r_meas,
      cc_one_half=shell.cc_one_half,
      cc_star=shell.cc_star,
      i_over_sigma=shell.i_over_sigma_mean)

  def as_cif_block(self, cif_block=None):
    import iotbx.cif.model
    if cif_block is None:
      cif_block = iotbx.cif.model.block()

    from collections import OrderedDict

    mmcif_to_name_reflns = OrderedDict([
      ('d_resolution_high', 'd_min'),
      ('d_resolution_low', 'd_max'),
      ('pdbx_CC_half', 'cc_one_half'),
      ('number_obs', 'n_uniq'),
      ('pdbx_number_measured_all', 'n_obs'),
      ('pdbx_Rmerge_I_obs', 'r_merge'),
      ('pdbx_Rpim_I_all', 'r_pim'),
      ('pdbx_Rrim_I_all', 'r_meas'),
      ('pdbx_netI_over_sigmaI', 'i_over_sigma_mean'),
      ('pdbx_netI_over_av_sigmaI', 'i_mean_over_sigi_mean'),
      ('pdbx_redundancy', 'mean_redundancy'),
      ('percent_possible_obs', 'completeness'),
    ])

    mmcif_to_name_reflns_shell = OrderedDict([
      ('d_res_high', 'd_min'),
      ('d_res_low', 'd_max'),
      ('pdbx_CC_half', 'cc_one_half'),
      ('number_unique_obs', 'n_uniq'),
      ('number_measured_obs', 'n_obs'),
      ('Rmerge_I_obs', 'r_merge'),
      ('pdbx_Rpim_I_all', 'r_pim'),
      ('pdbx_Rrim_I_all', 'r_meas'),
      ('pdbx_netI_over_sigmaI_obs', 'i_over_sigma_mean'),
      ('meanI_over_sigI_obs', 'i_mean_over_sigi_mean'),
      ('pdbx_redundancy', 'mean_redundancy'),
      ('percent_possible_obs', 'completeness'),
    ])

    for k, v in six.iteritems(mmcif_to_name_reflns):
      value = self.overall.__getattribute__(v)
      if 'percent' in k:
        value *= 100
      cif_block['_reflns.' + k] = value

    header = ['_reflns_shell.pdbx_ordinal'] + [
      '_reflns_shell.' + k for k in mmcif_to_name_reflns_shell.keys()]
    reflns_shell_loop = iotbx.cif.model.loop(header=header)
    for i, bin_stats in enumerate(self.bins):
      stats_d = bin_stats.as_dict()
      values = [bin_stats.__getattribute__(v) * 100
                if 'percent' in k else bin_stats.__getattribute__(v)
                for k, v in six.iteritems(mmcif_to_name_reflns_shell)]
      reflns_shell_loop.add_row([i+1] + values)
    cif_block.add_loop(reflns_shell_loop)

    return cif_block

  def as_remark_200(self, wavelength=None):
    from libtbx.test_utils import approx_equal
    synchrotron = wl = "NULL"
    if (wavelength is not None):
      out = StringIO()
      # XXX somewhat risky...
      if (not approx_equal(wavelength, 1.5418, eps=0.01, out=out) and
          not approx_equal(wavelength, 0.7107, eps=0.01, out=out)):
        synchrotron = "Y"
      else :
        synchrotron = "N"
      wl = "%.4f" % wavelength
    lines = []
    lines.append("")
    lines.append("EXPERIMENTAL DETAILS")
    lines.append(" EXPERIMENT TYPE                : X-RAY DIFFRACTION")
    lines.append(" DATE OF DATA COLLECTION        : NULL")
    lines.append(" TEMPERATURE           (KELVIN) : NULL")
    lines.append(" PH                             : NULL")
    lines.append(" NUMBER OF CRYSTALS USED        : NULL")
    lines.append("")
    lines.append(" SYNCHROTRON              (Y/N) : NULL")
    lines.append(" RADIATION SOURCE               : NULL")
    lines.append(" BEAMLINE                       : NULL")
    lines.append(" X-RAY GENERATOR MODEL          : NULL")
    lines.append(" MONOCHROMATIC OR LAUE    (M/L) : M")
    lines.append(" WAVELENGTH OR RANGE        (A) : %s" % wl)
    lines.append(" MONOCHROMATOR                  : NULL")
    lines.append(" OPTICS                         : NULL")
    lines.append("")
    lines.append(" DETECTOR TYPE                  : NULL")
    lines.append(" DETECTOR MANUFACTURER          : NULL")
    lines.append(" INTENSITY-INTEGRATION SOFTWARE : NULL")
    lines.append(" DATA SCALING SOFTWARE          : NULL")
    lines.append("")
    lines.append("OVERALL.")
    comp_overall = format_value("%.1f", self.overall.completeness * 100)
    mult_overall = format_value("%.1f", self.overall.mean_redundancy)
    rmerg_overall = format_value("%.5f", self.overall.r_merge)
    s2n_overall = format_value("%.4f", self.overall.i_over_sigma_mean)
    lines.append(" COMPLETENESS FOR RANGE     (%%) : %s" % comp_overall)
    lines.append(" DATA REDUNDANCY                : %s" % mult_overall)
    lines.append(" R MERGE                    (I) : %s" % rmerg_overall)
    lines.append(" R SYM                      (I) : NULL")
    lines.append(" <I/SIGMA(I)> FOR THE DATA SET  : %s" % s2n_overall)
    lines.append("")
    lines.append("IN THE HIGHEST RESOLUTION SHELL.")
    bin_stats = self.bins[-1]
    d_max = format_value("%.2f", bin_stats.d_max)
    d_min = format_value("%.2f", bin_stats.d_min)
    comp_lastbin = format_value("%.1f", bin_stats.completeness * 100)
    mult_lastbin = format_value("%.1f", bin_stats.mean_redundancy)
    rmerg_lastbin = format_value("%.5f", bin_stats.r_merge)
    s2n_lastbin = format_value("%.4f", bin_stats.i_over_sigma_mean)
    lines.append(" HIGHEST RESOLUTION SHELL, RANGE HIGH (A) : %s" % d_min)
    lines.append(" HIGHEST RESOLUTION SHELL, RANGE LOW  (A) : %s" % d_max)
    lines.append(" COMPLETENESS FOR SHELL     (%%) : %s" % comp_lastbin)
    lines.append(" DATA REDUNDANCY IN SHELL       : %s" % mult_lastbin)
    lines.append(" R MERGE FOR SHELL          (I) : %s" % rmerg_lastbin)
    lines.append(" R SYM FOR SHELL            (I) : NULL")
    lines.append(" <I/SIGMA(I)> FOR SHELL         : %s" % s2n_lastbin)
    lines.append("")
    remark_lines = [ "REMARK 200 %s" % line for line in lines ]
    return "\n".join(remark_lines)

  def show_model_vs_data(self, out=None, prefix=""):
    assert (self.overall.cc_work is not None)
    if (out is None) : out = sys.stdout
    outer_shell = self.bins[-1]
    print(prefix + "Merging statistics and CC*:", file=out)
    print(prefix + "  Resolution      : %.3f - %.3f (%.3f - %.3f)" % (
      self.overall.d_max, self.overall.d_min, outer_shell.d_max,
      outer_shell.d_min), file=out)
    print(prefix + "  Mean(I/sigmaI)  : %6.3f (%.3f)" % (
      self.overall.i_over_sigma_mean, outer_shell.i_over_sigma_mean), file=out)
    print(prefix + "  Multiplicity    :  %4.2f  (%.2f)" % (
      self.overall.mean_redundancy, outer_shell.mean_redundancy), file=out)
    print(prefix + "  R-merge         :  %5.3f (%.3f)" % (
      self.overall.r_merge, outer_shell.r_merge), file=out)
    print(prefix + "  R-meas          :  %5.3f (%.3f)" % (
      self.overall.r_meas, outer_shell.r_meas), file=out)
    print(prefix + "  R-pim           :  %5.3f (%.3f)" % (
      self.overall.r_pim, outer_shell.r_pim), file=out)
    print(prefix + "  CC1/2           :  %5.3f (%.3f)" % (
      self.overall.cc_one_half, outer_shell.cc_one_half), file=out)
    print(prefix + "  CC*             :  %5.3f (%.3f)" % (
      self.overall.cc_star, outer_shell.cc_star), file=out)
    print(prefix + "  CC(work)        :  %6.4f (%.4f)" % (
      self.overall.cc_work, outer_shell.cc_work), file=out)
    if (self.overall.cc_free is not None):
      print(prefix + "  CC(free)        :  %6.4f (%.4f)" % (
        self.overall.cc_free, outer_shell.cc_free), file=out)
    else :
      print(prefix + "  CC(free)        :  not available", file=out)

  def estimate_d_min(self,
      min_i_over_sigma=0,
      min_cc_one_half=0,
      max_r_merge=sys.maxsize,
      max_r_meas=sys.maxsize,
      min_cc_anom=-1,
      min_completeness=0):
    """
    Determine approximate resolution cutoffs based on a variety of metrics.
    Numbers are assumed to be fractional, not percentage values, except for
    the completeness which will be treated as a percent if the cutoff is
    greater than 1.

    :param min_i_over_sigma: minimum Mean(I/sigmaI) for outer shell
    :param min_cc_one_half: minimum CC1/2 for outer shell
    :param max_r_merge: maximum R-merge for outer shell
    :param max_r_meas: maximum R-meas for outer shell
    :param min_cc_anom: minimum CC(anom) for outer shell
    :param min_completeness: minimum completeness for outer shell
    :returns: Python float representing d_min for the outermost acceptable
      resolution bin, or None if no bins meet the given criteria
    """
    if ([min_i_over_sigma,min_cc_one_half,max_r_merge,max_r_meas,min_cc_anom,
          min_completeness].count(None) == 6):
      return None

    if (min_completeness is not None and min_completeness > 1):
      min_completeness /= 100.

    last_bin = None
    for bin in self.bins:
      if (
        ((min_i_over_sigma is not None) and
         (bin.i_over_sigma_mean < min_i_over_sigma)) or
        ((min_cc_one_half is not None) and
         (bin.cc_one_half < min_cc_one_half)) or
        (
          max_r_merge is not None and
          bin.r_merge is not None and
          bin.r_merge > max_r_merge
        ) or
        (
          max_r_meas is not None and
          bin.r_meas is not None and bin.r_meas > max_r_meas
        ) or
        (min_cc_anom is not None and bin.cc_anom < min_cc_anom) or
        (min_completeness is not None and bin.completeness < min_completeness)
      ):
        break
      last_bin = bin

    if last_bin is None:
      return None
    else:
      return last_bin.d_min

  def show_estimated_cutoffs(self, out=sys.stdout, prefix=""):
    print("", file=out)
    print("", file=out)
    def format_d_min(value):
      if (value is None):
        return "(use all data)" #% self.d_min_overall
      return "%7.3f" % value
    make_sub_header("Resolution cutoff estimates", out=out)
    print(prefix + "  resolution of all data          : %7.3f" % \
      self.overall.d_min, file=out)
    cc_one_half_cut = self.estimate_d_min(min_cc_one_half=0.33)
    i_over_sigma_cut = self.estimate_d_min(min_i_over_sigma=2.0)
    r_merge_cut = self.estimate_d_min(max_r_merge=0.5)
    r_meas_cut = self.estimate_d_min(max_r_meas=0.5)
    cc_anom_cut = self.estimate_d_min(min_cc_anom=0.3)
    completeness_cut_conservative = self.estimate_d_min(min_completeness=0.9)
    completeness_cut_permissive = self.estimate_d_min(min_completeness=0.5)
    print(prefix + "  based on CC(1/2) >= 0.33        : %s" % \
      format_d_min(cc_one_half_cut), file=out)
    print(prefix + "  based on mean(I/sigma) >= 2.0   : %s" % \
      format_d_min(i_over_sigma_cut), file=out)
    print(prefix + "  based on R-merge < 0.5          : %s" % \
      format_d_min(r_merge_cut), file=out)
    print(prefix + "  based on R-meas < 0.5           : %s" % \
      format_d_min(r_meas_cut), file=out)
    print(prefix + "  based on completeness >= 90%%    : %s" % \
      format_d_min(completeness_cut_conservative), file=out)
    print(prefix + "  based on completeness >= 50%%    : %s" % \
      format_d_min(completeness_cut_permissive), file=out)
    print("", file=out)
    print("NOTE: we recommend using all data out to the CC(1/2) limit", file=out)
    print("for refinement.", file=out)


def select_data(file_name, data_labels, log=None,
    assume_shelx_observation_type_is=None, allow_amplitudes=None, anomalous=None):
  if (log is None) : log = null_out()
  from iotbx import reflection_file_reader
  hkl_in = reflection_file_reader.any_reflection_file(file_name)
  print("Format:", hkl_in.file_type(), file=log)
  miller_arrays = hkl_in.as_miller_arrays(merge_equivalents=False,
    assume_shelx_observation_type_is=assume_shelx_observation_type_is,
    anomalous=anomalous,
  )
  if ((hkl_in.file_type() == "shelx_hklf") and (not "=" in file_name)
       and assume_shelx_observation_type_is is None):
    print("WARNING: SHELX file is assumed to contain intensities", file=log)
  i_obs = None
  all_i_obs = []
  for array in miller_arrays :
    labels = array.info().label_string()
    if (labels == data_labels):
      i_obs = array
      break
    elif (array.is_xray_intensity_array()):
      all_i_obs.append(array)
  # if no intensities...try again with amplitudes
  if (hkl_in.file_type() == "shelx_hklf" or allow_amplitudes):
    if (i_obs is None and len(all_i_obs)==0):
      for array in miller_arrays :
        if (array.is_xray_amplitude_array()):
          all_i_obs.append(array.f_as_f_sq())
  if (i_obs is None):
    if (len(all_i_obs) == 0):
      raise Sorry("No intensities found in %s." % file_name)
    elif (len(all_i_obs) > 1):
      raise Sorry("Multiple intensity arrays - please specify one:\n%s" %
        "\n".join(["  labels=%s"%a.info().label_string() for a in all_i_obs]))
    else :
      i_obs = all_i_obs[0]
  if hkl_in.file_type() == 'ccp4_mtz':
    # need original miller indices otherwise we don't get correct anomalous
    # merging statistics
    mtz_object = hkl_in.file_content()
    if "M_ISYM" in mtz_object.column_labels():
      indices = mtz_object.extract_original_index_miller_indices()
      i_obs = i_obs.customized_copy(indices=indices, info=i_obs.info())
  if (not i_obs.is_xray_intensity_array()):
    raise Sorry("%s is not an intensity array." % i_obs.info().label_string())
  return i_obs


 *******************************************************************************


 *******************************************************************************
iotbx/mtrix_biomt.py
"""
Routines to read and write PDB-style BIOMTR records
"""

from __future__ import absolute_import, division, print_function
from scitbx import matrix
import iotbx.pdb
from libtbx.utils import Sorry
from six import string_types
from six.moves import range, zip
from cctbx.array_family import flex

class container(object):

  def __init__(self):
    self.r=[]
    self.t=[]
    self.coordinates_present=[]
    self.serial_number=[]

  @classmethod
  def from_lists(cls, rl, tl, snl, cpl):
    assert len(rl) == len(tl) == len(snl) == len(cpl)
    result = cls()
    for r,t,sn,cp in zip(rl,tl,snl,cpl):
      ignore_transform = r.is_r3_identity_matrix() and t.is_col_zero()
      result.add(
        r=r, t=t,
        coordinates_present=(cp or ignore_transform),
        serial_number=sn)
    return result

  def add(self, r, t, serial_number, coordinates_present=False):
    self.r.append(r)
    self.t.append(t)
    self.coordinates_present.append(coordinates_present)
    self.serial_number.append(serial_number)

  def validate(self, eps=1e-4):
    raise_sorry = False
    for i, r in enumerate(self.r):
      if(not r.is_r3_rotation_matrix(rms_tolerance=eps)):
        raise_sorry = True
        print ('  ERROR: matrix with serial number %s is not proper' % self.serial_number[i])
    if raise_sorry:
      raise Sorry("One or more rotation matrices is not proper. See the log for details.")
    present = False
    for (r,t,n,cp) in zip(self.r,self.t,self.serial_number,
                            self.coordinates_present):
      if(not (r.is_r3_identity_matrix() and t.is_col_zero())):
        present = present or cp
    return present

  def as_pdb_string(self):
    return format_MTRIX_pdb_string(
      rotation_matrices=self.r,
      translation_vectors=self.t,
      serial_numbers=self.serial_number,
      coordinates_present_flags=self.coordinates_present)

  def format_BIOMT_pdb_string(self):
    '''
    BIOMT data sample
    REMARK 350   BIOMT1   1  1.000000  0.000000  0.000000        0.00000
    '''
    lines = []
    fmt1="REMARK 350   BIOMT1  %2d%10.6f%10.6f%10.6f     %10.5f"
    fmt2="REMARK 350   BIOMT2  %2d%10.6f%10.6f%10.6f     %10.5f"
    fmt3="REMARK 350   BIOMT3  %2d%10.6f%10.6f%10.6f     %10.5f"
    for sn_, r_, t_ in zip(self.serial_number, self.r, self.t):
      lines.append(fmt1%(int(sn_), r_[0],r_[1],r_[2], t_[0]))
      lines.append(fmt2%(int(sn_), r_[3],r_[4],r_[5], t_[1]))
      lines.append(fmt3%(int(sn_), r_[6],r_[7],r_[8], t_[2]))
    return "\n".join(lines)

  def is_empty(self):
    return len(self.r) == 0

def parse_MTRIX_BIOMT_records_cif(cif_block, recs='mtrix'):
  # this is temporarily work-around. Whole thing for matrices need to be
  # rewritten to be able to carry all the info from mmCIF, see
  # https://pdb101.rcsb.org/learn/guide-to-understanding-pdb-data/biological-assemblies#Anchor-Biol

  assert recs in ['mtrix', 'biomt']
  block_name = '_struct_ncs_oper' if recs=='mtrix' else '_pdbx_struct_oper_list'
  rots = []
  trans = []
  serial_number = []
  coordinates_present = []
  ncs_oper = cif_block.get('%s.id' % block_name)
  if ncs_oper is not None:
    for i,sn in enumerate(ncs_oper):
      # filter everything for X0 and P here because they represent some other translations
      # not related to whole molecule reproduction in BIOMT:
      # http://mmcif.wwpdb.org/dictionaries/mmcif_pdbx_v50.dic/Items/_pdbx_struct_oper_list.id.html
      if recs=='biomt' and (sn == "P" or sn == "X0" or sn == "H"):
        continue
      serial_number.append((sn,i))
      if recs == 'mtrix':
        code_loop_or_item = cif_block.get('%s.code' % block_name)
        if isinstance(code_loop_or_item, flex.std_string):
          coordinates_present.append(code_loop_or_item[i] == 'given')
        else:
          coordinates_present.append(code_loop_or_item == 'given')
      else:
        coordinates_present.append(False) # no way to figure out
      r = [(cif_block.get('%s.matrix[%s][%s]' %(block_name,x,y)))
        for x,y in ('11', '12', '13', '21', '22', '23', '31','32', '33')]
      if not isinstance(r[0], string_types):
        r = [elem[i] for elem in r]
      try:
        rots.append(matrix.sqr([float(r_elem) for r_elem in r]) )
        t = [(cif_block.get('%s.vector[%s]' %(block_name, x)))
          for x in '123']
        if not isinstance(t[0], string_types):
          t = [elem[i] for elem in t]
        trans.append(matrix.col([float(t_elem) for t_elem in t]))
      except ValueError:
        raise Sorry("Error in %s information. Likely '?' instead of a number." % block_name)
  if recs=='mtrix':
    # sort records by serial number
    serial_number.sort()
    items_order = [i for (_,i) in serial_number]
    trans = [trans[i] for i in items_order]
    rots = [rots[i] for i in items_order]
    coordinates_present = [coordinates_present[i] for i in items_order]
    serial_number = [j for (j,_) in serial_number]
  return rots, trans, serial_number, coordinates_present

def process_BIOMT_records_cif(cif_block):
  rots, trans, serial_number, coordinates_present = parse_MTRIX_BIOMT_records_cif(cif_block, 'biomt')
  return container.from_lists(rots, trans, serial_number, coordinates_present)

def process_MTRIX_records_cif(cif_block):
  rots, trans, serial_number, coordinates_present = parse_MTRIX_BIOMT_records_cif(cif_block, 'mtrix')
  return container.from_lists(rots, trans, serial_number, coordinates_present)

def process_BIOMT_records_pdb(lines):
  '''(pdb_data,boolean,float) -> group of lists
  extract REMARK 350 BIOMT information, information that provides rotation matrices
  and translation  data, required for generating  a complete multimer from the asymmetric unit.

  BIOMT data sample:
  REMARK 350   BIOMT1   1  1.000000  0.000000  0.000000        0.00000
  REMARK 350   BIOMT2   1  0.000000  1.000000  0.000000        0.00000
  REMARK 350   BIOMT3   1  0.000000  0.000000  1.000000        0.00000
  REMARK 350   BIOMT1   2  0.559048 -0.789435  0.253492        0.30000
  REMARK 350   BIOMT2   2  0.722264  0.313528 -0.616470        0.00100
  REMARK 350   BIOMT3   2  0.407186  0.527724  0.745457        0.05000

  The data from every 3 lines will be combined to a list
  [[x11,x12,x13,x21,x22,x23,x31,x32,x33],[x1,x2,x3]]
  the first component, with the 9 numbers are the rotation matrix,
  and the second component, with the 3 numbers is the translation information
  x is the serial number of the operation

  for x=2 the transformation_data will be
  [[0.559048,-0.789435,0.253492,0.722264,0.313528,-0.616470,0.407186,0.527724,0.745457][0.30000,0.00100,0.05000]]

  the result is a list of libtx group_arg constructs, each one contains
    pdb_inp.process_BIOMT_records()[1].values              # give a list containing float type numbers
    pdb_inp.process_BIOMT_records()[1].coordinates_present # True when transformatin included in pdb file
    pdb_inp.process_BIOMT_records()[1].serial_number       # is an integer

  @author: Youval Dar (2013)
  '''
  source_info = lines # XXX
  if not source_info:
    return container()                # check if any BIOMT info is available
    # collecting the data from the remarks. Checking that we are collecting only data
    # and not part of the remarks header by verifying that the 3rd component contains "BIOMT"
    # and that the length of that component is 6
  biomt_data = [[float(_x_elem) for _x_elem in  x.split()[3:]] for x in source_info if (
    x.split()[2].find('BIOMT') > -1) and (len(x.split()[2]) == 6)]
  # test that there is no missing data
  if len(biomt_data)%3 != 0:
    raise RuntimeError(
      "Improper or missing set of PDB BIOMAT records. Data length = %s" % \
      str(len(biomt_data)))
  # test that the length of the data match the serial number, that there are no missing records
  # temporary workaround, could be plain text over there instead of
  # expected number of records, see 5l93
  try:
    temp = int(source_info[-1].split()[3])
  except ValueError:
    temp = 0
  if len(biomt_data)/3.0 != temp:
    raise RuntimeError(
      "Missing record sets in PDB BIOMAT records \n" + \
      "Actual number of BIOMT matrices: {} \n".format(len(biomt_data)/3.0)+\
      "expected according to serial number: {} \n".format(temp))
  rots = []
  trans = []
  serial_number = []
  coordinates_present = []
  for i in range(len(biomt_data)//3):
    # i is the group number in biomt_data
    # Each group composed from 3 data sets.
    j = 3*i;
    rotation_data = \
      biomt_data[j][1:4] + biomt_data[j+1][1:4] + biomt_data[j+2][1:4]
    translation_data = \
      [biomt_data[j][-1], biomt_data[j+1][-1], biomt_data[j+2][-1]]
    rots.append(matrix.sqr(rotation_data))
    trans.append(matrix.col(translation_data))
    # For BIOMT the first transform is the identity matrix
    ignore_transform = rots[-1].is_r3_identity_matrix() and trans[-1].is_col_zero()
    coordinates_present.append(ignore_transform)
    serial_number.append(i+1)
  return container.from_lists(rots, trans, serial_number, coordinates_present)

def process_MTRIX_records_pdb(lines):
  """
  Read MTRIX records from a pdb file
  """
  storage = {}
  for line in lines:
    if (line.startswith("MTRIX") and line[5:6] in ["1", "2", "3"]):
      r = read_mtrix_record(line=line)
      stored = storage.get(r.serial_number)
      if (stored is None):
        values = [[None]*9,[None]*3]
        done = [None]*3
        present = [None]*3
        # Create a dictionary record for a serial number
        stored = storage[r.serial_number] = (values, done, present)
      else:
        values, done, present = stored
      for i_col,v in enumerate(r.r):
        values[0][(r.n-1)*3+i_col] = v
      values[1][r.n-1] = r.t
      done[r.n-1] = r.n
      present[r.n-1] = r.coordinates_present
  rots = []
  trans = []
  serial_number = []
  coordinates_present = []
  for sn in sorted(storage.keys()):
    values, done, present = storage[sn]
    serial_number.append(sn)
    rots.append(matrix.sqr(values[0]))
    trans.append(matrix.col(values[1]))
    if (sorted(done) != [1,2,3] or len(set(present)) != 1):
      raise RuntimeError("Improper set of PDB MTRIX records")
    ignore_transform = rots[-1].is_r3_identity_matrix() and trans[-1].is_col_zero()
    coordinates_present.append(present[0]==1 or ignore_transform)
  return container.from_lists(rots, trans, serial_number, coordinates_present)

def format_MTRIX_pdb_string(rotation_matrices, translation_vectors,
      serial_numbers=None, coordinates_present_flags=None):
  '''
  MTRIX data sample
  REMARK 350   BIOMT1   1  1.000000  0.000000  0.000000        0.00000
  MTRIX1   1  1.000000  0.000000  0.000000        0.00000    1
  MTRIX2   1  0.000000  1.000000  0.000000        0.00000    1
  MTRIX3   1  0.000000  0.000000  1.000000        0.00000    1
  MTRIX1   2  0.496590 -0.643597  0.582393        0.00000
  MTRIX2   2  0.867925  0.376088 -0.324443        0.00000
  MTRIX3   2 -0.010221  0.666588  0.745356        0.00000
  '''
  assert len(rotation_matrices) == len(translation_vectors)
  if(serial_numbers is None): serial_numbers = range(0, len(rotation_matrices))
  if(coordinates_present_flags is None):
    coordinates_present_flags = [False]*len(rotation_matrices)
  lines = []
  fmt1="MTRIX1  %2d%10.6f%10.6f%10.6f     %10.5f    %s"
  fmt2="MTRIX2  %2d%10.6f%10.6f%10.6f     %10.5f    %s"
  fmt3="MTRIX3  %2d%10.6f%10.6f%10.6f     %10.5f    %s"
  for sn_, r_, t_, p_ in zip(serial_numbers, rotation_matrices,
                             translation_vectors, coordinates_present_flags):
    flag = " "
    if p_: flag="1"
    lines.append(fmt1%(int(sn_), r_[0],r_[1],r_[2], t_[0], flag))
    lines.append(fmt2%(int(sn_), r_[3],r_[4],r_[5], t_[1], flag))
    lines.append(fmt3%(int(sn_), r_[6],r_[7],r_[8], t_[2], flag))
  return "\n".join(lines)

class read_mtrix_record(iotbx.pdb.read_scale_record):

  __slots__ = iotbx.pdb.read_scale_record.__slots__ + [
    "serial_number", "coordinates_present"]

  def __init__(O, line):
    iotbx.pdb.read_scale_record.__init__(O, line=line, source_info="")
    O.serial_number = line[7:10]
    O.coordinates_present = (len(line) >= 60 and line[59] != " ")


 *******************************************************************************


 *******************************************************************************
iotbx/option_parser.py
"""Option parser tools for GUI
"""
from __future__ import absolute_import, division, print_function
from iotbx import crystal_symmetry_from_any
from cctbx import crystal
from cctbx import sgtbx
from cctbx import uctbx
from libtbx.option_parser import libtbx_option_parser, OptionError, make_option

class option_parser(libtbx_option_parser):

  def __init__(self, usage=None, description=None, more_help=None):
    libtbx_option_parser.__init__(self,
      usage=usage, description=description, more_help=more_help)
    self.symmetry_callback = symmetry_callback()

  def enable_unit_cell(self):
    self.add_option(make_option(None, "--unit_cell",
      action="callback",
      type="string",
      callback=self.symmetry_callback,
      help="External unit cell parameters",
      metavar="10,10,20,90,90,120|FILENAME"))
    self.symmetry_callback.is_enabled = True
    return self

  def enable_space_group(self):
    self.add_option(make_option(None, "--space_group",
      action="callback",
      type="string",
      callback=self.symmetry_callback,
      help="External space group symbol",
      metavar="P212121|FILENAME"))
    self.symmetry_callback.is_enabled = True
    return self

  def enable_symmetry(self):
    self.add_option(make_option(None, "--symmetry",
      action="callback",
      type="string",
      callback=self.symmetry_callback,
      help="External file with symmetry information",
      metavar="FILENAME"))
    self.symmetry_callback.is_enabled = True
    return self

  def enable_symmetry_comprehensive(self):
    self.enable_unit_cell()
    self.enable_space_group()
    self.enable_symmetry()
    return self

  def enable_resolution(self, default=None):
    self.add_option(make_option(None, "--resolution",
      action="store",
      default=default,
      type="float",
      help="High resolution limit (minimum d-spacing, d_min)",
      metavar="FLOAT"))
    return self

  def enable_low_resolution(self, default=None):
    self.add_option(make_option(None, "--low_resolution",
      action="store",
      default=default,
      type="float",
      help="Low resolution limit (maximum d-spacing, d_max)",
      metavar="FLOAT"))
    return self

  def enable_resolutions(self, default_low=None, default_high=None):
    self.enable_resolution(default=default_high)
    self.enable_low_resolution(default=default_low)
    return self

  def process(self, args=None, nargs=None, min_nargs=None, max_nargs=None):
    result = libtbx_option_parser.process(self,
      args=args, nargs=nargs, min_nargs=min_nargs, max_nargs=max_nargs)
    result.symmetry = self.symmetry_callback.get()
    return result

iotbx_option_parser = option_parser

class symmetry_callback(object):

  def __init__(self):
    self.is_enabled = False
    self.unit_cell = None
    self.space_group_info = None

  def __call__(self, option, opt, value, parser):
    if (opt == "--unit_cell"):
      unit_cell = None
      try: unit_cell = uctbx.unit_cell(value)
      except Exception: pass
      if (unit_cell is not None):
        self.unit_cell = unit_cell
      else:
        crystal_symmetry = crystal_symmetry_from_any.extract_from(value)
        if (   crystal_symmetry is None
            or crystal_symmetry.unit_cell() is None):
          raise OptionError("cannot read parameters: " + value, opt)
        self.unit_cell = crystal_symmetry.unit_cell()
    elif (opt == "--space_group"):
      space_group_info = None
      space_group_info = sgtbx.space_group_info(symbol=value)
      try: space_group_info = sgtbx.space_group_info(symbol=value)
      except Exception: pass
      if (space_group_info is not None):
        self.space_group_info = space_group_info
      else:
        crystal_symmetry = crystal_symmetry_from_any.extract_from(value)
        if (   crystal_symmetry is None
            or crystal_symmetry.space_group_info() is None):
          raise OptionError("unknown space group: " + value, opt)
        self.space_group_info = crystal_symmetry.space_group_info()
    elif (opt == "--symmetry"):
      crystal_symmetry = crystal_symmetry_from_any.extract_from(value)
      if (   crystal_symmetry is None
          or crystal_symmetry.space_group_info() is None):
        raise OptionError("cannot read symmetry: " + value, opt)
      if (crystal_symmetry.unit_cell() is not None):
        self.unit_cell = crystal_symmetry.unit_cell()
      if (crystal_symmetry.space_group_info() is not None):
        self.space_group_info = crystal_symmetry.space_group_info()
    else:
      raise RuntimeError("Programming error.")

  def get(self):
    return crystal.symmetry(
      unit_cell=self.unit_cell,
      space_group_info=self.space_group_info)


 *******************************************************************************


 *******************************************************************************
iotbx/phases.py
"""Tool to write miller array containing phases and amplitudes in XTALVIEW format"""
from __future__ import absolute_import, division, print_function
from cctbx.array_family import flex
import math
from six.moves import zip

def miller_array_as_phases_phs(self,
      out,
      scale_amplitudes=True,
      phases=None,
      phases_deg=None,
      figures_of_merit=None):
  """http://www.sdsc.edu/CCMS/Packages/XTALVIEW/xtalviewfaq.html"""
  if (phases is not None): assert phases_deg is False or phases_deg is True
  if (self.is_complex_array()):
    amplitudes = self.amplitudes().data()
  else:
    amplitudes = self.data()
  if (scale_amplitudes):
    amplitudes_max = flex.max(amplitudes)
    if (amplitudes_max > 0):
      amplitudes = (9999.99/amplitudes_max) * amplitudes
  assert len(" %7.2f" % flex.min(amplitudes)) == 8
  assert len(" %7.2f" % flex.max(amplitudes)) == 8
  if (phases is None):
    phases = self.phases(deg=True).data()
  else:
    if (hasattr(phases, "data")):
      phases = phases.data()
    if (not phases_deg):
      phases = phases * (180/math.pi)
  assert len(" %7.2f" % flex.min(phases)) == 8
  assert len(" %7.2f" % flex.max(phases)) == 8
  if (figures_of_merit is None):
    for h,a,p in zip(self.indices(),amplitudes,phases):
      print("%4d%4d%4d" % h + " %7.2f"%a + " %7.2f"%1 + " %7.2f"%p, file=out)
  else:
    if (hasattr(figures_of_merit, "data")):
      assert figures_of_merit.indices().all_eq(self.indices())
      figures_of_merit = figures_of_merit.data()
    assert len(" %7.2f" % flex.min(figures_of_merit)) == 8
    assert len(" %7.2f" % flex.max(figures_of_merit)) == 8
    for h,a,p,f in zip(self.indices(),amplitudes,phases,figures_of_merit):
      print("%4d%4d%4d" % h + " %7.2f"%a + " %7.2f"%f + " %7.2f"%p, file=out)


 *******************************************************************************


 *******************************************************************************
iotbx/phil.py
"""
Tools for processing inputs using the phil control language
"""

from __future__ import absolute_import, division, print_function

import os
import sys

from cctbx import sgtbx
from cctbx import uctbx
import libtbx.phil.command_line
from libtbx.utils import Sorry, Usage, import_python_object
from libtbx.phil import tokenizer
from libtbx import Auto

class unit_cell_converters(object):

  phil_type = "unit_cell"

  def __str__(self): return self.phil_type

  def from_words(self, words, master):
    s = libtbx.phil.str_from_words(words=words)
    if (s is None): return None
    if (s is Auto): return Auto
    return uctbx.unit_cell(str(s))

  def as_words(self, python_object, master):
    if (python_object is None):
      return [tokenizer.word(value="None")]
    if (python_object is Auto):
      return [tokenizer.word(value="Auto")]
    return [tokenizer.word(value="%.10g" % v)
      for v in python_object.parameters()]

class space_group_converters(object):

  phil_type = "space_group"

  def __str__(self): return self.phil_type

  def from_words(self, words, master):
    symbol = libtbx.phil.str_from_words(words)
    if (symbol is None): return None
    if (symbol is Auto): return Auto
    return sgtbx.space_group_info(symbol=str(symbol))

  def as_words(self, python_object, master):
    if (python_object is None):
      return [tokenizer.word(value="None")]
    if (python_object is Auto):
      return [tokenizer.word(value="Auto")]
    return [tokenizer.word(value=str(python_object), quote_token='"')]

class atom_selection_converters(libtbx.phil.qstr_converters):

  phil_type = "atom_selection"

  def __str__(self): return self.phil_type

  def from_words(self, words, master):
    if (len(words) == 1):
      word = words[0]
      if (word.quote_token is not None):
        return word.value # mainly for backward compatibility
    return libtbx.phil.qstr_converters.from_words(self, words, master)

default_converter_registry = libtbx.phil.extended_converter_registry(
  additional_converters=[
    unit_cell_converters,
    space_group_converters,
    atom_selection_converters])

def parse(
      input_string=None,
      source_info=None,
      file_name=None,
      converter_registry=None,
      process_includes=False):
  if (converter_registry is None):
    converter_registry = default_converter_registry
  return libtbx.phil.parse(
    input_string=input_string,
    source_info=source_info,
    file_name=file_name,
    converter_registry=converter_registry,
    process_includes=process_includes)

def read_default(
      caller_file_name,
      params_extension=".params",
      converter_registry=None,
      process_includes=True):
  if (converter_registry is None):
    converter_registry = default_converter_registry
  return libtbx.phil.read_default(
    caller_file_name=caller_file_name,
    params_extension=params_extension,
    converter_registry=converter_registry,
    process_includes=process_includes)

def process_command_line(args, master_string, parse=None):
  if (parse is None):
    import iotbx.phil
    parse = iotbx.phil.parse
  return libtbx.phil.process_command_line(
    args=args, master_string=master_string, parse=parse)

class process_command_line_with_files(object):
  def __init__(self,
                args,
                master_phil=None,
                master_phil_string=None,
                pdb_file_def=None,
                reflection_file_def=None,
                map_file_def=None,
                cif_file_def=None,
                seq_file_def=None,
                pickle_file_def=None,
                ncs_file_def=None,
                directory_def=None,
                integer_def=None,
                float_def=None,
                space_group_def=None,
                unit_cell_def=None,
                usage_string=None):
    assert (master_phil is not None) or (master_phil_string is not None)
    if (master_phil_string is not None):
      assert (master_phil is None)
      import iotbx.phil
      master_phil = iotbx.phil.parse(input_string=master_phil_string,
        process_includes=True)
    if (usage_string is not None):
      if (len(args) == 0) or ("--help" in args):
        raise Usage("""
%s

Full parameters:

%s""" % (usage_string, master_phil.as_str(prefix="  ", attributes_level=1)))
    self.master = master_phil
    self.pdb_file_def = pdb_file_def
    self.reflection_file_def = reflection_file_def
    self.cif_file_def = cif_file_def
    self.cif_objects = []
    self.map_file_def = map_file_def
    self.seq_file_def = seq_file_def
    self.pickle_file_def = pickle_file_def
    self.ncs_file_def = ncs_file_def
    self.directory_def = directory_def
    self.integer_def = integer_def
    self.float_def = float_def
    self.space_group_def = space_group_def
    self.unit_cell_def = unit_cell_def
    self._type_counts = {}
    self._cache = {}
    cai=libtbx.phil.command_line.argument_interpreter(master_phil=self.master)
    self.unused_args = []
    self.work = cai.process_and_fetch(
       args=args,
       custom_processor=self)

  def get_file_type_count(self, file_type):
    return self._type_counts.get(file_type, 0)

  def __call__(self, arg):
    file_arg = None
    is_shelx_file = False
    if (os.path.isfile(arg)):
      file_arg = arg
    # handle SHELX format hack
    elif (arg.endswith("=hklf3") or arg.endswith("=hklf4") or
          arg.endswith("=amplitudes") or arg.endswith("=intensities")):
      base_arg = "".join(arg.split("=")[:-1])
      if (base_arg != "") and os.path.isfile(base_arg):
        file_arg = arg
        is_shelx_file = True
    if (file_arg is not None):
      from iotbx import file_reader
      f = file_reader.any_file(os.path.abspath(file_arg),
        raise_sorry_if_not_expected_format=True)
      if (f.file_type is not None):
        if (not f.file_type in self._type_counts):
          self._type_counts[f.file_type] = 0
        self._type_counts[f.file_type] += 1
        self._cache[f.file_name] = f
      file_def_name = None
      if (f.file_type == "pdb") and (self.pdb_file_def is not None):
        file_def_name = self.pdb_file_def
      elif (f.file_type == "hkl") and (self.reflection_file_def is not None):
        file_def_name = self.reflection_file_def
      elif (f.file_type == "ccp4_map") and (self.map_file_def is not None):
        file_def_name = self.map_file_def
      elif (f.file_type == "cif") and (self.cif_file_def is not None):
        file_def_name = self.cif_file_def
        self.cif_objects.append((f.file_name, f.file_object.model()))
      elif (f.file_type == "seq") and (self.seq_file_def is not None):
        file_def_name = self.seq_file_def
      elif (f.file_type == "ncs") and (self.ncs_file_def is not None):
        file_def_name = self.ncs_file_def
      elif (f.file_type == "pkl") and (self.pickle_file_def is not None):
        file_def_name = self.pickle_file_def
      if (file_def_name is not None):
        file_name = f.file_name
        if (is_shelx_file):
          file_name = file_arg
        return libtbx.phil.parse("%s=%s" % (file_def_name, file_name))
      else :
        return False
    elif (os.path.isdir(arg)):
      if (self.directory_def is not None):
        return libtbx.phil.parse("%s=%s" % (self.directory_def, arg))
    else :
      int_value = float_value = None
      if (self.integer_def is not None):
        try :
          int_value = int(arg)
        except ValueError:
          pass
        else :
          return libtbx.phil.parse("%s=%d" % (self.integer_def, int_value))
      if (self.float_def is not None):
        try :
          float_value = float(arg)
        except ValueError:
          pass
        else :
          return libtbx.phil.parse("%s=%g" % (self.float_def, float_value))
      if (self.space_group_def is not None):
        try :
          space_group_info = sgtbx.space_group_info(arg)
        except RuntimeError : # XXX should really be ValueError
          pass
        else :
          return libtbx.phil.parse("%s=%s" % (self.space_group_def,
            space_group_info))
      if (self.unit_cell_def is not None) and (arg.count(",") >= 2):
        try :
          uc_params = tuple([ float(x) for x in arg.split(",") ])
          unit_cell = uctbx.unit_cell(uc_params)
        except Exception : # XXX should really be ValueError
          pass
        else :
          return libtbx.phil.parse("%s=%s" % (self.unit_cell_def,
            ",".join([ "%g"%x for x in unit_cell.parameters() ])))
    return self.process_other(arg)

  def process_other(self, arg):
    self.unused_args.append(arg)
    return True

  def get_cached_file(self, file_name):
    return self._cache.get(file_name, None)

  def get_file(self, file_name, force_type=None):
    if file_name is None:
      return None
    input_file = self._cache.get(file_name)
    if (input_file is None):
      from iotbx import file_reader
      input_file = file_reader.any_file(file_name,
        force_type=force_type,
        raise_sorry_if_errors=True)
    elif (force_type is not None):
      input_file.assert_file_type(force_type)
    return input_file

# Utilities for Phenix GUI
class setup_app_generic(object):
  def __init__(self, master_phil_path,
     top_level_scopes_to_remove = None):
    self.top_level_scopes_to_remove = top_level_scopes_to_remove
    master_phil = self.load_from_cache_if_possible(master_phil_path)
    if master_phil is None :
      raise Sorry("Couldn't start program using specified phil object (%s)!" %
        master_phil_path)
    if hasattr(master_phil, "__call__"):
      master_phil = master_phil()
    elif isinstance(master_phil, str):
      master_phil = parse(master_phil, process_includes=True)
    else :
      assert type(master_phil).__name__ == "scope"
    self.master_phil = master_phil

  def __call__(self, args):
    (working_phil, options, unused_args) = self.parse_command_line_phil_args(
      args=args,
      master_phil=self.master_phil,
      command_name="phenix",
      usage_opts=["[model.pdb]", "[data.mtz]"],
      app_options=None,
      top_level_scopes_to_remove = self.top_level_scopes_to_remove,
      home_scope="")
    return (self.master_phil,working_phil,options, unused_args)

  # TODO probably redundant, replace with process_command_line or similar?
  def parse_command_line_phil_args(self, args, master_phil, command_name, usage_opts,
      app_options, home_scope, top_level_scopes_to_remove = None, log=sys.stdout):
    sources = []
    unused_args = []
    interpreter = master_phil.command_line_argument_interpreter(
      home_scope=home_scope)
    for arg in args :
      if os.path.isfile(arg):
        try :
          user_phil = parse(file_name=arg)
          if top_level_scopes_to_remove:
            # ----------------------------------------------------------------
            # Backwards compatibility for modules with top-level scope removed
            if user_phil and user_phil.objects and \
                (len(user_phil.objects) == 1) and \
                (user_phil.objects[0].name in top_level_scopes_to_remove):
              print("REMOVING TOP-LEVEL SCOPE '%s'" %(user_phil.objects[0].name ))
              user_phil.objects = user_phil.objects[0].objects
            # ----------------------------------------------------------------

          sources.append(user_phil)
        except Exception as e :
          unused_args.append(os.path.abspath(arg))
      elif arg != "" and not arg.startswith("-"):
        try :
          params = interpreter.process(arg=arg)
        except RuntimeError :
          print("%s does not appear to be a parameter definition" % arg, file=log)
          unused_args.append(arg)
        else :
          sources.append(params)
      elif arg != "" :
        unused_args.append(arg)
    try :
      working_phil = master_phil.fetch(sources=sources,
         skip_incompatible_objects=True)
    except Exception as e :
      print("Error incorporating parameters from user-specified file(s):", file=log)
      print(str(e), file=log)
      print("Will revert to default parameters for this run.", file=log)
      working_phil = master_phil.fetch()

    assert working_phil is not None
    if "--debug" in args :
      working_phil.show()
    cmdline_opts = None
    return (working_phil, cmdline_opts, unused_args)

  def load_from_cache_if_possible(self, phil_path):
    import libtbx.load_env
    full_path = os.path.join(abs(libtbx.env.build_path), "phil_cache",
      "%s.phil" % phil_path)
    if (os.path.exists(full_path)):
      return parse(file_name=full_path)
    else :
      return import_python_object(
        import_path=phil_path,
        error_prefix="",
        target_must_be="",
        where_str="").object


 *******************************************************************************


 *******************************************************************************
iotbx/poscar.py
"http://cms.mpi.univie.ac.at/vasp/vasp/POSCAR_file.html"
from __future__ import absolute_import, division, print_function

from libtbx import slots_getstate_setstate
from six.moves import range
from six.moves import zip

class reader(slots_getstate_setstate):

  __slots__ = """
    title
    scale
    lattice_vectors
    types
    type_counts
    sites
  """.split()

  def __init__(O, lines, source_info=None):
    assert len(lines) >= 7
    O.title = lines[0]
    scale_str = lines[1].split()
    assert len(scale_str) == 1
    O.scale = float(scale_str[0])
    O.lattice_vectors = []
    for i in [2,3,4]:
      vec_str = lines[i].split()
      assert len(vec_str) == 3
      vec = [float(_) for _ in vec_str]
      O.lattice_vectors.append(vec)
    i_type_counts = 5
    type_counts_str = lines[i_type_counts].split()
    assert len(type_counts_str) > 0
    _ = type_counts_str[0]
    try:
      int(_)
    except ValueError:
      O.types = type_counts_str
      i_type_counts = 6
      type_counts_str = lines[i_type_counts].split()
      assert len(type_counts_str) == len(O.types)
    else:
      O.types = None
    O.type_counts = [int(_) for _ in type_counts_str]
    key = lines[i_type_counts+1].strip()
    if (key != "Direct"):
      from libtbx.utils import Sorry
      if (source_info is not None):
        from libtbx.str_utils import show_string
        s = " of %s" % show_string(source_info)
      else:
        s = ""
      raise Sorry('POSCAR "%s" is not supported (line %d%s).'
        % (key, i_type_counts+1+1, s))
    n_sites = sum(O.type_counts)
    assert len(lines) >= i_type_counts+2+n_sites
    O.sites = []
    for i in range(i_type_counts+2, i_type_counts+2+n_sites):
      site_str = lines[i].split()
      assert len(site_str) >= 3
      site = [float(_) for _ in site_str[:3]]
      O.sites.append(site)

  def unit_cell(O):
    from scitbx import matrix
    vecs = [matrix.col(_) for _ in O.lattice_vectors]
    params = [_.length() for _ in vecs]
    for i in range(3):
      params.append(vecs[(i+1)%3].angle(vecs[(i+2)%3], deg=True))
    from cctbx import uctbx
    return uctbx.unit_cell(params)

  def make_up_types_if_necessary(O, default="Si"):
    if (O.types is None):
      O.types = [default]*len(O.type_counts)
    return O

  def scatterers(O, u_iso=0):
    assert O.types is not None
    from cctbx import xray
    from cctbx.array_family import flex
    result = flex.xray_scatterer()
    sites = iter(O.sites)
    for type,count in zip(O.types, O.type_counts):
      for _ in range(count):
        result.append(xray.scatterer(
          label="%s%d"%(type, len(result)+1),
          scattering_type=type,
          site=next(sites),
          u=u_iso))
    assert len(result) == len(O.sites)
    return result

  def xray_structure(O, u_iso=0):
    from cctbx import xray
    from cctbx import crystal
    return xray.structure(
      crystal_symmetry=crystal.symmetry(
        unit_cell=O.unit_cell(), space_group_symbol="P1"),
      scatterers=O.scatterers(u_iso=u_iso))


 *******************************************************************************


 *******************************************************************************
iotbx/pymol.py
"""Tools for interfacing with Pymol
"""
from __future__ import absolute_import, division, print_function
from libtbx import adopt_init_args

class pml_stick(object):

  def __init__(self, begin, end, colors=None, width=None):
    if (colors is None):
      colors = [[1,0,0]]*2
    if (width is None):
      width = 0.1
    adopt_init_args(self, locals())

def pml_write(f, label, sticks):
  print("from pymol.cgo import *", file=f)
  print("from pymol import cmd", file=f)
  print("obj = [", file=f)
  for stick in sticks:
    print("CYLINDER,", "%.6g, %.6g, %.6g," % tuple(stick.begin), end=' ', file=f)
    print("%.6g, %.6g, %.6g," % tuple(stick.end), end=' ', file=f)
    print("%.6g," % stick.width, end=' ', file=f)
    print("%.6g, %.6g, %.6g," % tuple(stick.colors[0]), end=' ', file=f)
    print("%.6g, %.6g, %.6g," % tuple(stick.colors[1]), file=f)
  print("]", file=f)
  print('cmd.load_cgo(obj, "%s")' % label, file=f)


 *******************************************************************************


 *******************************************************************************
iotbx/ranges.py
"""Tools for manipulating ranges
"""
from __future__ import absolute_import, division, print_function
from six.moves import range
from six.moves import zip
def range_parser( txt ):
  splitter = " "
  if "," in txt:
    splitter = ","
  txt_ranges = txt.split(splitter)
  ranges = []
  for range_ in txt_ranges:
    tmp = range_.split("-")
    assert len(tmp)<=2
    if len(tmp)==2:
      ranges.append( [int(tmp[0]),int(tmp[1])]  )
    if len( tmp) == 1:
      ranges.append( [ int(tmp[0]) ] )
  return ranges

def range_to_list(range_):
  result = []
  for item in range_:
    if len(item)==1:
      result.append( item[0] )
    if len(item)==2:
      for ii in range(item[0],item[1]+1):
        result.append( ii )
  return result



class serial_file_name_handler(object):
  def __init__(self, base_name, wildcard="#"):
    self.user_base_name = base_name
    self.wildcard = wildcard
    self.base, self.extension, self.n = self.process_user_input()

  def process_user_input(self):
    at_serial = False
    at_extension = False
    p1=""
    p3=""
    n=0
    for ii in self.user_base_name:
      if not at_serial:
        if (ii == self.wildcard):
          at_serial=True
        if not at_extension:
          if not at_serial:
            p1+=ii
      if at_serial:
        n += 1
        if (ii!=self.wildcard):
          at_serial=False
          at_extension=True
      if at_extension:
        p3 += ii
    if len(p3)==0:
      n = n
    else:
      n = n-1
    return p1,p3,n


  def name_from_number(self, serial_id=1):
    id = str(serial_id)
    return self.base+(self.n-len(id))*"0"+id+self.extension

  def names_from_range_list(self, range_list):
    result = []
    for id in range_list:
      result.append( self.name_from_number( id ) )
    return result

  def names_from_range(self, range_txt):
    result = []
    range_list = range_to_list( range_parser( range_txt ) )
    result = self.names_from_range_list( range_list )
    return result



def tst_file_names():
  base="lysozyme_1_####.img"
  obj = serial_file_name_handler(base)
  assert obj.base=="lysozyme_1_"
  assert obj.extension == ".img"
  assert obj.n == 4
  assert obj.name_from_number( 23 ) == "lysozyme_1_0023.img"

  base="lysozyme_1.####"
  obj = serial_file_name_handler(base)
  assert obj.base=="lysozyme_1."
  assert obj.extension == ""
  assert obj.n == 4
  assert obj.name_from_number( 23 ) == "lysozyme_1.0023"

  base = "######.lysozyme_1.img"
  obj = serial_file_name_handler(base)
  assert obj.base==""
  assert obj.extension == ".lysozyme_1.img"
  assert obj.n == 6
  assert obj.name_from_number( 23 ) == "000023.lysozyme_1.img"
  lst = obj.names_from_range_list( [1,2,3] )
  assert lst[0] == "000001.lysozyme_1.img"
  assert lst[1] == "000002.lysozyme_1.img"
  assert lst[2] == "000003.lysozyme_1.img"
  assert len(lst)==3


  base = "######.lysozyme_1.img"
  obj = serial_file_name_handler(base)
  range_txt = "1-3"
  name_list =  obj.names_from_range(range_txt)
  assert name_list[0] == '000001.lysozyme_1.img'
  assert name_list[1] == '000002.lysozyme_1.img'
  assert name_list[2] == '000003.lysozyme_1.img'

def tst_ranges():
  range_txt="1-9,10,11,13-16"
  result = range_to_list( range_parser( range_txt ) )
  tst = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16]
  for ii, jj in zip(result,tst):
    assert(ii==jj)
  range_txt="1-9 10 11 13-16"
  result = range_to_list( range_parser( range_txt ) )
  tst = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16]
  for ii, jj in zip(result,tst):
    assert(ii==jj)

if __name__ == "__main__":
  tst_ranges()
  tst_file_names()
  print("OK")


 *******************************************************************************


 *******************************************************************************
iotbx/reflection_file_converter.py
"""Low-level tools for manipulation of reflection data files
"""
from __future__ import absolute_import, division, print_function
import iotbx.mtz
import iotbx.cns.miller_array
import iotbx.scalepack.merge
from iotbx import reflection_file_reader
from iotbx import reflection_file_utils
from iotbx.option_parser import option_parser
from cctbx import crystal
from cctbx import sgtbx
from cctbx.array_family import flex
from libtbx.utils import Sorry, date_and_time, plural_s
from six.moves import cStringIO as StringIO
import random
import os

def remove_anomalous_suffix_if_necessary(
   miller_array=None,
   column_root_label=None):
  # if the miller array is anomalous and the suffix is "(+)", remove it
  if column_root_label and column_root_label.endswith("(+)") and \
       miller_array.anomalous_flag():
    column_root_label=column_root_label[:-3]
  return column_root_label

def run(
      args,
      command_name="phenix.reflection_file_converter",
      simply_return_all_miller_arrays=False):
  command_line = (option_parser(
    usage="%s [options] reflection_file ..." % command_name,
    description="Example: %s w1.sca --mtz ." % command_name)
    .enable_symmetry_comprehensive()
    .option(None, "--weak_symmetry",
      action="store_true",
      default=False,
      help="symmetry on command line is weaker than symmetry found in files")
    .enable_resolutions()
    .option(None, "--label",
      action="store",
      type="string",
      help="Substring of reflection data label or number",
      metavar="STRING")
    .option(None, "--non_anomalous",
      action="store_true",
      default=False,
      help="Averages Bijvoet mates to obtain a non-anomalous array")
    .option(None, "--r_free_label",
      action="store",
      type="string",
      help="Substring of reflection data label or number",
      metavar="STRING")
    .option(None, "--r_free_test_flag_value",
      action="store",
      type="int",
      help="Value in R-free array indicating assignment to free set.",
      metavar="FLOAT")
    .option(None, "--generate_r_free_flags",
      action="store_true",
      default=False,
      help="Generates a new array of random R-free flags"
           " (MTZ and CNS output only).")
    .option(None, "--use_lattice_symmetry_in_r_free_flag_generation",
      dest="use_lattice_symmetry_in_r_free_flag_generation",
      action="store_true",
      default=True,
      help="group twin/pseudo symmetry related reflections together"
           " in r-free set (this is the default).")
    .option(None, "--no_lattice_symmetry_in_r_free_flag_generation",
      dest="use_lattice_symmetry_in_r_free_flag_generation",
      action="store_false",
      help="opposite of --use-lattice-symmetry-in-r-free-flag-generation")
    .option(None, "--r_free_flags_fraction",
      action="store",
      default=0.10,
      type="float",
      help="Target fraction free/work reflections (default: 0.10).",
      metavar="FLOAT")
    .option(None, "--r_free_flags_max_free",
      action="store",
      default=2000,
      type="int",
      help="Maximum number of free reflections (default: 2000).",
      metavar="INT")
    .option(None, "--r_free_flags_format",
      choices=("cns", "ccp4", "shelx"),
      default="cns",
      help="Convention for generating R-free flags",
      metavar="cns|ccp4")
    .option(None, "--output_r_free_label",
      action="store",
      type="string",
      help="Label for newly generated R-free flags (defaults to R-free-flags)",
      default="R-free-flags",
      metavar="STRING")
    .option(None, "--random_seed",
      action="store",
      type="int",
      help="Seed for random number generator (affects generation of"
           " R-free flags).",
      metavar="INT")
    .option(None, "--change_of_basis",
      action="store",
      type="string",
      help="Change-of-basis operator: h,k,l or x,y,z"
           " or to_reference_setting, to_primitive_setting, to_niggli_cell,"
           " to_inverse_hand",
      metavar="STRING")
    .option(None, "--eliminate_invalid_indices",
      action="store_true",
      default=False,
      help="Remove indices which are invalid given the change of basis desired")
    .option(None, "--expand_to_p1",
      action="store_true",
      default=False,
      help="Generates all symmetrically equivalent reflections."
           " The space group symmetry is reset to P1."
           " May be used in combination with --change_to_space_group to"
           " lower the symmetry.")
    .option(None, "--change_to_space_group",
      action="store",
      type="string",
      help="Changes the space group and merges equivalent reflections"
           " if necessary",
      metavar="SYMBOL|NUMBER")
    .option(None, "--write_mtz_amplitudes",
      action="store_true",
      default=False,
      help="Converts intensities to amplitudes before writing MTZ format;"
           " requires --mtz_root_label")
    .option(None, "--write_mtz_intensities",
      action="store_true",
      default=False,
      help="Converts amplitudes to intensities before writing MTZ format;"
           " requires --mtz_root_label")
    .option(None,"--remove_negatives",
      action="store_true",
      default=False,
      help="Remove negative intensities or amplitudes from the data set" )
    .option(None,"--massage_intensities",
      action="store_true",
      default=False,
      help="'Treat' negative intensities to get a positive amplitude."
           " |Fnew| = sqrt((Io+sqrt(Io**2 +2sigma**2))/2.0). Requires"
           " intensities as input and the flags --mtz,"
           " --write_mtz_amplitudes and --mtz_root_label.")
    .option(None, "--scale_max",
      action="store",
      type="float",
      help="Scales data such that the maximum is equal to the given value",
      metavar="FLOAT")
    .option(None, "--scale_factor",
      action="store",
      type="float",
      help="Multiplies data with the given factor",
      metavar="FLOAT")
    .option(None, "--write_unmerged",
      action="store_true",
      default=False,
      help="Do not perform any merging of input data")
    .option(None, "--sca",
      action="store",
      type="string",
      help=
        "write data to Scalepack FILE ('--sca .' copies name of input file)",
      metavar="FILE")
    .option(None, "--mtz",
      action="store",
      type="string",
      help="write data to MTZ FILE ('--mtz .' copies name of input file)",
      metavar="FILE")
    .option(None, "--mtz_root_label",
      action="store",
      type="string",
      help="Root label for MTZ file (e.g. Fobs)",
      metavar="STRING")
    .option(None, "--cns",
      action="store",
      type="string",
      help="write data to CNS FILE ('--cns .' copies name of input file)",
      metavar="FILE")
    .option(None, "--shelx",
      action="store",
      type="string",
      help="write intensity or amplitude data to SHELX FILE ('--shelx .' copies name of input file)",
      metavar="FILE")
    .option(None, "--shelx_std_dynamic_range",
      action="store_true",
      default=False,
      help="for ShelX output, do not use full dynamic range")
  ).process(args=args)
  co = command_line.options
  if (co.random_seed is not None):
    random.seed(co.random_seed)
    flex.set_random_seed(value=co.random_seed)
  if (    co.write_mtz_amplitudes
      and co.write_mtz_intensities):
    print()
    print("--write_mtz_amplitudes and --write_mtz_intensities" \
          " are mutually exclusive.")
    print()
    return None
  if (   co.write_mtz_amplitudes
      or co.write_mtz_intensities):
    if (co.mtz_root_label is None):
      print()
      print("--write_mtz_amplitudes and --write_mtz_intensities" \
            " require --mtz_root_label.")
      print()
      return None
  if (    co.scale_max is not None
      and co.scale_factor is not None):
    print()
    print("--scale_max and --scale_factor are mutually exclusive.")
    print()
    return None
  if (len(command_line.args) == 0):
    command_line.parser.show_help()
    return None
  all_miller_arrays = reflection_file_reader.collect_arrays(
    file_names=command_line.args,
    crystal_symmetry=None,
    force_symmetry=False,
    merge_equivalents=False,
    discard_arrays=False,
    verbose=1)
  if (simply_return_all_miller_arrays):
    return all_miller_arrays
  if (len(all_miller_arrays) == 0):
    print()
    print("No reflection data found in input file%s." % (
      plural_s(len(command_line.args))[1]))
    print()
    return None
  label_table = reflection_file_utils.label_table(
    miller_arrays=all_miller_arrays)
  selected_array = label_table.select_array(
    label=co.label, command_line_switch="--label")
  if (selected_array is None): return None
  r_free_flags = None
  r_free_info = None
  if (co.r_free_label is not None):
    r_free_flags = label_table.match_data_label(
      label=co.r_free_label,
      command_line_switch="--r_free_label")
    if (r_free_flags is None):
      return None
    r_free_info = str(r_free_flags.info())
    if (not r_free_flags.is_bool_array()):
      test_flag_value = reflection_file_utils.get_r_free_flags_scores(
        miller_arrays=[r_free_flags],
        test_flag_value=co.r_free_test_flag_value).test_flag_values[0]
      if (test_flag_value is None):
        if (co.r_free_test_flag_value is None):
          raise Sorry(
            "Cannot automatically determine r_free_test_flag_value."
            " Please use --r_free_test_flag_value to specify a value.")
        else:
          raise Sorry("Invalid --r_free_test_flag_value.")
      r_free_flags = r_free_flags.customized_copy(
        data=(r_free_flags.data() == test_flag_value))
  print("Selected data:")
  print(" ", selected_array.info())
  print("  Observation type:", selected_array.observation_type())
  print()
  if (r_free_info is not None):
    print("R-free flags:")
    print(" ", r_free_info)
    print()
  processed_array = selected_array.customized_copy(
    crystal_symmetry=selected_array.join_symmetry(
      other_symmetry=command_line.symmetry,
      force=not co.weak_symmetry)).set_observation_type(
        selected_array.observation_type())
  if (r_free_flags is not None):
    r_free_flags = r_free_flags.customized_copy(
      crystal_symmetry=processed_array)
  print("Input crystal symmetry:")
  crystal.symmetry.show_summary(processed_array, prefix="  ")
  print()
  if (processed_array.unit_cell() is None):
    command_line.parser.show_help()
    print("Unit cell parameters unknown. Please use --symmetry or --unit_cell.")
    print()
    return None
  if (processed_array.space_group_info() is None):
    command_line.parser.show_help()
    print("Space group unknown. Please use --symmetry or --space_group.")
    print()
    return None
  if (r_free_flags is not None):
    r_free_flags = r_free_flags.customized_copy(
      crystal_symmetry=processed_array)
  if (co.change_of_basis is not None):
    processed_array, cb_op = processed_array.apply_change_of_basis(
      change_of_basis=co.change_of_basis,
      eliminate_invalid_indices=co.eliminate_invalid_indices)
    if (r_free_flags is not None):
      r_free_flags = r_free_flags.change_basis(cb_op=cb_op)
  if (not processed_array.is_unique_set_under_symmetry()
      and not co.write_unmerged):
    print("Merging symmetry-equivalent values:")
    merged = processed_array.merge_equivalents()
    merged.show_summary(prefix="  ")
    print()
    processed_array = merged.array()
    del merged
    processed_array.show_comprehensive_summary(prefix="  ")
    print()
  if (r_free_flags is not None
      and not r_free_flags.is_unique_set_under_symmetry()
      and not co.write_unmerged):
    print("Merging symmetry-equivalent R-free flags:")
    merged = r_free_flags.merge_equivalents()
    merged.show_summary(prefix="  ")
    print()
    r_free_flags = merged.array()
    del merged
    r_free_flags.show_comprehensive_summary(prefix="  ")
    print()
  if (co.expand_to_p1):
    print("Expanding symmetry and resetting space group to P1:")
    if (r_free_flags is not None):
      raise Sorry(
        "--expand_to_p1 not supported for arrays of R-free flags.")
    processed_array = processed_array.expand_to_p1()
    processed_array.show_comprehensive_summary(prefix="  ")
    print()
  if (co.change_to_space_group is not None):
    if (r_free_flags is not None):
      raise Sorry(
        "--change_to_space_group not supported for arrays of R-free flags.")
    new_space_group_info = sgtbx.space_group_info(
      symbol=co.change_to_space_group)
    print("Change to space group:", new_space_group_info)
    new_crystal_symmetry = crystal.symmetry(
      unit_cell=processed_array.unit_cell(),
      space_group_info=new_space_group_info,
      assert_is_compatible_unit_cell=False)
    if (not new_crystal_symmetry.unit_cell()
              .is_similar_to(processed_array.unit_cell())):
      print("  *************")
      print("  W A R N I N G")
      print("  *************")
      print("  Unit cell parameters adapted to new space group symmetry are")
      print("  significantly different from input unit cell parameters:")
      print("      Input unit cell parameters:", \
        processed_array.unit_cell())
      print("    Adapted unit cell parameters:", \
        new_crystal_symmetry.unit_cell())
    processed_array = processed_array.customized_copy(
      crystal_symmetry=new_crystal_symmetry)
    print()
    if (not processed_array.is_unique_set_under_symmetry()
        and not co.write_unmerged):
      print("  Merging values symmetry-equivalent under new symmetry:")
      merged = processed_array.merge_equivalents()
      merged.show_summary(prefix="    ")
      print()
      processed_array = merged.array()
      del merged
      processed_array.show_comprehensive_summary(prefix="    ")
      print()
  if (processed_array.anomalous_flag() and co.non_anomalous):
    print("Converting data array from anomalous to non-anomalous.")
    if (not processed_array.is_xray_intensity_array()):
      processed_array = processed_array.average_bijvoet_mates()
    else:
      processed_array = processed_array.average_bijvoet_mates()
      processed_array.set_observation_type_xray_intensity()
  if (r_free_flags is not None
      and r_free_flags.anomalous_flag()
      and co.non_anomalous):
    print("Converting R-free flags from anomalous to non-anomalous.")
    r_free_flags = r_free_flags.average_bijvoet_mates()
  d_max = co.low_resolution
  d_min = co.resolution
  if (d_max is not None or d_min is not None):
    if (d_max is not None):
      print("Applying low resolution cutoff: d_max=%.6g" % d_max)
    if (d_min is not None):
      print("Applying high resolution cutoff: d_min=%.6g" % d_min)
    processed_array = processed_array.resolution_filter(
      d_max=d_max, d_min=d_min)
    print("Number of reflections:", processed_array.indices().size())
    print()
  if (co.scale_max is not None):
    print("Scaling data such that the maximum value is: %.6g" % co.scale_max)
    processed_array = processed_array.apply_scaling(target_max=co.scale_max)
    print()
  if (co.scale_factor is not None):
    print("Multiplying data with the factor: %.6g" % co.scale_factor)
    processed_array = processed_array.apply_scaling(factor=co.scale_factor)
    print()

  if (([co.remove_negatives, co.massage_intensities]).count(True) == 2):
    raise Sorry(
      "It is not possible to use --remove_negatives and"
      " --massage_intensities at the same time.")

  if (co.remove_negatives):
    if processed_array.is_real_array():
      print("Removing negatives items")
      processed_array = processed_array.select(
        processed_array.data() > 0)
      if processed_array.sigmas() is not None:
        processed_array = processed_array.select(
          processed_array.sigmas() > 0)
    else:
      raise Sorry("--remove_negatives not applicable to complex data arrays.")

  if (co.massage_intensities):
    if processed_array.is_real_array():
      if processed_array.is_xray_intensity_array():
        if (co.mtz is not None):
          if (co.write_mtz_amplitudes):
            print("The supplied intensities will be used to estimate")
            print(" amplitudes in the following way:  ")
            print(" Fobs = Sqrt[ (Iobs + Sqrt(Iobs**2 + 2sigmaIobs**2))/2 ]")
            print(" Sigmas are estimated in a similar manner.")
            print()
            processed_array = processed_array.enforce_positive_amplitudes()
          else:
            raise Sorry(
              "--write_mtz_amplitudes has to be specified when using"
              " --massage_intensities")
        else:
          raise Sorry("--mtz has to be used when using --massage_intensities")
      else:
        raise Sorry(
          "Intensities must be supplied when using the option"
          " --massage_intensities")
    else:
      raise Sorry(
        "--massage_intensities not applicable to complex data arrays.")

  if (not co.generate_r_free_flags):
    if (r_free_flags is None):
      r_free_info = []
    else:
      if (r_free_flags.anomalous_flag() != processed_array.anomalous_flag()):
        if (processed_array.anomalous_flag()): is_not = ("", " not")
        else:                                  is_not = (" not", "")
        raise Sorry(
          "The data array is%s anomalous but the R-free array is%s.\n"
            % is_not
          + "  Please try --non_anomalous.")
      r_free_info = ["R-free flags source: " + r_free_info]
      if (not r_free_flags.indices().all_eq(processed_array.indices())):
        processed_array = processed_array.map_to_asu()
        r_free_flags = r_free_flags.map_to_asu().common_set(processed_array)
        n_missing_r_free_flags = processed_array.indices().size() \
                               - r_free_flags.indices().size()
        if (n_missing_r_free_flags != 0):
          raise Sorry("R-free flags not compatible with data array:"
           " missing flag for %d reflections selected for output." %
             n_missing_r_free_flags)
  else:
    if (r_free_flags is not None):
      raise Sorry(
        "--r_free_label and --generate_r_free_flags are mutually exclusive.")
    print("Generating a new array of R-free flags:")
    r_free_flags = processed_array.generate_r_free_flags(
      fraction=co.r_free_flags_fraction,
      max_free=co.r_free_flags_max_free,
      use_lattice_symmetry=co.use_lattice_symmetry_in_r_free_flag_generation,
      format=co.r_free_flags_format)
    test_flag_value = True
    if (co.r_free_flags_format == "ccp4"):
      test_flag_value = 0
    elif (co.r_free_flags_format == "shelx"):
      test_flag_value = -1
    r_free_as_bool = r_free_flags.customized_copy(
      data=r_free_flags.data()==test_flag_value)
    r_free_info = [
      "R-free flags generated by %s:" % command_name]
    r_free_info.append("  "+date_and_time())
    r_free_info.append("  fraction: %.6g" % co.r_free_flags_fraction)
    r_free_info.append("  max_free: %s" % str(co.r_free_flags_max_free))
    r_free_info.append("  size of work set: %d" %
      r_free_as_bool.data().count(False))
    r_free_info.append("  size of free set: %d" %
      r_free_as_bool.data().count(True))
    r_free_info_str = StringIO()
    r_free_as_bool.show_r_free_flags_info(prefix="  ", out=r_free_info_str)
    if (co.r_free_flags_format == "ccp4"):
      r_free_info.append("  convention: CCP4 (test=0, work=1-%d)" %
        flex.max(r_free_flags.data()))
    elif (co.r_free_flags_format == "shelx"):
      r_free_info.append("  convention: SHELXL (test=-1, work=1)")
    else :
      r_free_info.append("  convention: CNS/X-PLOR (test=1, work=0)")
    print("\n".join(r_free_info[2:4]))
    print(r_free_info[-1])
    print(r_free_info_str.getvalue())
    print()

  n_output_files = 0
  if (co.sca is not None):
    if (co.generate_r_free_flags):
      raise Sorry("Cannot write R-free flags to Scalepack file.")
    file_name = reflection_file_utils.construct_output_file_name(
      input_file_names=[selected_array.info().source],
      user_file_name=co.sca,
      file_type_label="Scalepack",
      file_extension="sca")
    print("Writing Scalepack file:", file_name)
    iotbx.scalepack.merge.write(
      file_name=file_name,
      miller_array=processed_array)
    n_output_files += 1
    print()
  if (co.mtz is not None):
    file_name = reflection_file_utils.construct_output_file_name(
      input_file_names=[selected_array.info().source],
      user_file_name=co.mtz,
      file_type_label="MTZ",
      file_extension="mtz")
    print("Writing MTZ file:", file_name)
    mtz_history_buffer = flex.std_string()
    mtz_history_buffer.append(date_and_time())
    mtz_history_buffer.append("> program: %s" % command_name)
    mtz_history_buffer.append("> input file name: %s" %
      os.path.basename(selected_array.info().source))
    mtz_history_buffer.append("> input directory: %s" %
      os.path.dirname(os.path.abspath(selected_array.info().source)))
    mtz_history_buffer.append("> input labels: %s" %
      selected_array.info().label_string())
    mtz_output_array = processed_array
    if (co.write_mtz_amplitudes):
      if (not mtz_output_array.is_xray_amplitude_array()):
        print("  Converting intensities to amplitudes.")
        mtz_output_array = mtz_output_array.f_sq_as_f()
        mtz_history_buffer.append("> Intensities converted to amplitudes.")
    elif (co.write_mtz_intensities):
      if (not mtz_output_array.is_xray_intensity_array()):
        print("  Converting amplitudes to intensities.")
        mtz_output_array = mtz_output_array.f_as_f_sq()
        mtz_history_buffer.append("> Amplitudes converted to intensities.")
    column_root_label = co.mtz_root_label
    if (column_root_label is None):
      # XXX 2013-03-29: preserve original root label by default
      # XXX 2014-12-16: skip trailing "(+)" in root_label if anomalous
      column_root_label = selected_array.info().labels[0]
    column_root_label=remove_anomalous_suffix_if_necessary(
      miller_array=selected_array,
      column_root_label=column_root_label)
    mtz_dataset = mtz_output_array.as_mtz_dataset(
      column_root_label=column_root_label)
    del mtz_output_array
    if (r_free_flags is not None):
      mtz_dataset.add_miller_array(
        miller_array=r_free_flags,
        column_root_label=co.output_r_free_label)
      for line in r_free_info:
        mtz_history_buffer.append("> " + line)
    mtz_history_buffer.append("> output file name: %s" %
      os.path.basename(file_name))
    mtz_history_buffer.append("> output directory: %s" %
      os.path.dirname(os.path.abspath(file_name)))
    mtz_object = mtz_dataset.mtz_object()
    mtz_object.add_history(mtz_history_buffer)
    mtz_object.write(file_name=file_name)
    n_output_files += 1
    print()
  if (co.cns is not None):
    file_name = reflection_file_utils.construct_output_file_name(
      input_file_names=[selected_array.info().source],
      user_file_name=co.cns,
      file_type_label="CNS",
      file_extension="cns")
    print("Writing CNS file:", file_name)
    processed_array.export_as_cns_hkl(
      file_object=open(file_name, "w"),
      file_name=file_name,
      info=["source of data: "+str(selected_array.info())] + r_free_info,
      r_free_flags=r_free_flags)
    n_output_files += 1
    print()
  if (co.shelx is not None):
    if (co.generate_r_free_flags):
      raise Sorry("Cannot write R-free flags to SHELX file.")
    file_extension = "hkl"
    if (co.shelx.endswith("shelx")):
      file_extension = "shelx"
    file_name = reflection_file_utils.construct_output_file_name(
      input_file_names=[selected_array.info().source],
      user_file_name=co.shelx,
      file_type_label="SHELX",
      file_extension=file_extension)

    if processed_array.is_xray_intensity_array():
      data_status = "HKLF 4 (intensity)"
    else:
      data_status = "HKLF 3 (amplitude)"
    print("Writing SHELX", data_status, "file:", file_name)
    processed_array.export_as_shelx_hklf(
      open(file_name, "w"),full_dynamic_range=(not co.shelx_std_dynamic_range))
    n_output_files += 1
    print()
  if (n_output_files == 0):
    command_line.parser.show_help()
    print("Please specify at least one output file format,", end=' ')
    print("e.g. --mtz, --sca, etc.")
    print()
    return None
  return processed_array


 *******************************************************************************


 *******************************************************************************
iotbx/reflection_file_editor.py
"""GUI tool for manipulating reflection file data
"""
# TODO: confirm old_test_flag_value if ambiguous

from __future__ import absolute_import, division, print_function
import iotbx.phil
from libtbx.utils import Sorry, null_out, check_if_output_directory_exists
from libtbx import adopt_init_args, slots_getstate_setstate
import warnings
import random
import string
import re
import os
import sys
import six
from six.moves import range
from six.moves import zip

DEBUG = False

# XXX: note that extend=True in the Phenix GUI
master_phil = iotbx.phil.parse("""
show_arrays = False
  .type = bool
  .help = Command-line option, prints out a list of the arrays in each file
  .style = hidden
dry_run = False
  .type = bool
  .help = Print out final configuration and output summary, but don't write \
          the output file
  .style = hidden
verbose = True
  .type = bool
  .help = Print extra debugging information
  .style = hidden
mtz_file
{
  crystal_symmetry
    .caption = By default, the input crystal symmetry will be used for the \
      output file.
    .style = auto_align menu_item
  {
    unit_cell = None
      .type = unit_cell
      .style = bold noauto
    space_group = None
      .type = space_group
      .style = bold noauto
    output_unit_cell = None
      .type = unit_cell
      .style = bold noauto
    output_space_group = None
      .type = space_group
      .style = bold noauto
    change_of_basis = None
      .type = str
      .expert_level = 2
    eliminate_invalid_indices = False
      .type = bool
      .expert_level = 2
    expand_to_p1 = False
      .type = bool
      .short_caption = Expand to P1
      .style = bold
    disable_unit_cell_check = False
      .type = bool
      .style = noauto
      .short_caption = Disable unit cell isomorphism check
    disable_space_group_check = False
      .type = bool
      .style = noauto
    eliminate_sys_absent = True
      .type = bool
      .short_caption = Eliminate systematic absences
  }
  d_max = None
    .type = float
    .short_caption = Low resolution
    .style = bold renderer:draw_hkltools_resolution_widget
  d_min = None
    .type = float
    .short_caption = High resolution
    .style = bold renderer:draw_hkltools_resolution_widget
  wavelength = None
    .type = float
    .short_caption = Wavelength
    .style = bold
  output_file = None
    .type = path
    .short_caption = Output file
    .style = file_type:mtz new_file bold noauto
  include scope libtbx.phil.interface.tracking_params
  resolve_label_conflicts = False
    .type = bool
    .help = Updates label names as necessary to avoid conflicts
    .short_caption = Automatically resolve output label conflicts
    .style = noauto bold
  exclude_reflection = None
    .multiple = True
    .optional = True
    .type = ints(size=3)
    .input_size = 100
    .style = noauto menu_item
  miller_array
    .multiple = True
    .short_caption = Output data array
    .style = fixed auto_align
  {
    file_name = None
      .type = path
      .style = noedit bold
    labels = None
      .type = str
      .short_caption = Array name
      .style = noedit bold
    output_labels = None
      .type = strings
      .optional = True
      .short_caption = Output column labels
      .input_size = 300
      .help = Most Miller arrays have more than one label, and there must be \
              exactly as many new labels as the number of labels in the \
              old array.  Note however that the output labels do not \
              necessarily correspond to the original array name.  (See caveat \
              in Phenix manual about Scalepack files.)
      .style = fixed
    column_root_label = None
      .type = str
      .optional = True
      .short_caption = Base column label
      .help = If specified, this is applied to all columns in the output \
        array with default prefixes and suffixes.  Overrides the \
        output_labels parameter.
      .input_size = 200
    d_min = None
      .type = float
      .short_caption = High resolution
    d_max = None
      .type = float
      .short_caption = Low resolution
    output_as = *auto intensities amplitudes_fw amplitudes
      .type = choice
      .short_caption = Output diffraction data as
      .caption = Automatic Intensities Amplitudes_(run_French-Wilson) \
        Amplitudes_(simple_conversion)
      .help = If the Miller array is amplitudes or intensities, this flag \
        determines the output data type.  If intensities are being converted \
        to amplitudes, this can optionally be done using the French and \
        Wilson treatment to correct weak and negative values.
    anomalous_data = *Auto merged anomalous
      .type = choice
      .caption = automatic_(keep) merge_if_present force_anomalous_output
      .short_caption = Anomalous data
      .help = Optional averaging or generation of Bijvoet mates.  Note that the \
        number of labels expected may change.
    force_type = *auto amplitudes intensities
      .type = choice
      .short_caption = Force observation type
      .help = If the Miller array is amplitudes or intensites, this flag \
        allows the observation type to be set without modifying the data. \
        This is primarily used for structure factors downloaded from the PDB, \
        which sometimes have the data type specified incorrectly.
    scale_max = None
      .type = float
      .short_caption = Scale to maximum value
      .help = Scales data such that the maximum is equal to the given value
    scale_factor = None
      .type = float
      .help = Multiplies data with the given factor
    remove_negatives = False
      .type = bool
      .short_caption = Remove negative values
    massage_intensities = False
      .type = bool
    filter_by_signal_to_noise = None
      .type = float
      .short_caption = Filter by signal-to-noise ratio
    add_b_iso = None
      .type = float
      .short_caption = Add isotropic B-factor
    add_b_aniso = 0 0 0 0 0 0
      .type = floats(size=6)
      .short_caption = Add anisotropic B-factor
    shuffle_values = False
      .type = bool
    reset_values_to = None
      .type = float
      .short_caption = Reset values to
      .help = If defined, all data values will be set to this number.  Sigmas \
        will be left unmodified.  Only applies to single-value floating-point \
        data (I, F, PHI, etc.).
  }
  r_free_flags
    .short_caption = R-free flags generation
    .style = menu_item auto_align box
  {
    generate = True
      .type = bool
      .short_caption = Generate R-free flags if not already present
      .style = bold noauto
    force_generate = False
      .type = bool
      .short_caption = Generate R-free flags even if they are already present
    new_label = FreeR_flag
      .type = str
      .short_caption = Output label for new R-free flags
      .input_size = 160
      .style = bold noauto
    include scope cctbx.r_free_utils.generate_r_free_params_str
    random_seed = None
      .type = int
      .short_caption = Seed for random number generator
      .expert_level = 2
    extend = None
      .type = bool
      .short_caption = Extend existing R-free array(s) to full resolution range
      .style = bold noauto
    old_test_flag_value = None
      .type = int
      .short_caption = Original test flag value
      .help = Overrides automatic guess of test flag value from existing set. \
        This will usually be 1 for reflection files generated by Phenix, and \
        0 for reflection files from CCP4.  Do not change unless you're sure \
        you know what flag to use!
      .expert_level = 2
    export_for_ccp4 = False
      .type = bool
      .short_caption = Convert R-free flags to CCP4 convention
      .help = If True, R-free flags expressed as boolean values (or 0 and 1) \
        will be converted to random integers from 0 to 19, where 0 denotes \
        the test set.  Phenix will work with either convention, but most CCP4 \
        programs expect the test set to be 0.
      .expert_level = 2
      .style = noauto
    preserve_input_values = True
      .type = bool
      .short_caption = Preserve original flag values
      .help = If True, CCP4-style R-free flags will be left as random \
        integers instead of being converted to a boolean array.  This option \
        is not compatible with the option to export flags to the CCP4 \
        convention.
      .style = noauto
    warn_if_all_same_value = True
      .type = bool
      .short_caption = Warn if R-free flags are all the same value
    adjust_fraction = False
      .type = bool
      .short_caption = Adjust test set size to specified fraction
      .help = If True, the R-free flags will be resized as necessary.  This \
        may be useful when the current set is too large or too small, but \
        needs to be preserved (at least in part).  The target fraction will \
        be relative to all possible reflections, even those not measured in \
        the input.  Note that this option is not compatible with preserving \
        input values.
    d_eps = 0.0001
      .type = float
      .short_caption = Resolution buffer
      .expert_level = 2
    relative_to_complete_set = False
      .type = bool
      .short_caption = Generate R-free flags relative to complete set
    remediate_mismatches = False
      .type = bool
      .short_caption = Remediate mismatching Friedel/Bijvoet pairs
      .help = If True, flags that differ between F+ and F- will be moved into \
        the test set for both reflections.  This option is incompatible with \
        preserving the input values as integers.
  }
}""", process_includes=True)

class array_input(slots_getstate_setstate):
  __slots__ = ["miller_arrays", "file_names", "array_types"]
  def __init__(self, params, input_files=None):
    from iotbx import file_reader
    if (input_files is None):
      input_files = {}
    self.miller_arrays = []
    self.file_names = []
    self.array_types = []
    for i_array, array_params in enumerate(params.mtz_file.miller_array):
      if (array_params.file_name is None):
        raise Sorry("Missing file name for array %d (labels=%s)" %
          (i_array+1, str(array_params.labels)))
      elif (not os.path.isfile(array_params.file_name)):
        raise Sorry("The path '%s' does not exist or is not a file." %
          array_params.file_name)
      input_file = input_files.get(array_params.file_name)
      if input_file is None :
        input_file = file_reader.any_file(array_params.file_name,
          force_type="hkl")
      is_mtz = (input_file.file_object.file_type() == "ccp4_mtz")
      if input_file is None :
        input_file = file_reader.any_file(array_params.file_name)
        input_file.assert_file_type("hkl")
        input_files[input_file.file_name] = input_file
      found = False
      for miller_array in input_file.file_object.as_miller_arrays():
        array_info = miller_array.info()
        label_string = array_info.label_string()
        if label_string == array_params.labels :
          self.miller_arrays.append(miller_array)

          self.file_names.append(input_file.file_name)
          found = True
          if is_mtz :
            array_type = get_original_array_types(input_file,
              original_labels=array_info.labels)
            self.array_types.append(array_type)
          else :
            self.array_types.append(None)
      if not found :
        raise Sorry("Couldn't find the Miller array %s in file %s!" %
                    (array_params.labels, array_params.file_name))

  def resolve_unit_cell(self):
    unit_cells = []
    for any_array_type in [False, True] :
      for array in self.miller_arrays :
        if array.is_experimental_data():
          cs = array.crystal_symmetry()
          if (cs is not None):
            uc = cs.unit_cell()
            unit_cells.append(uc)
      if (len(unit_cells) > 0):
        break
    if (len(unit_cells) == 0):
      raise Sorry("No unit cell found in inputs - please specify explicitly.")
    return unit_cells

class process_arrays(object):
  def __init__(self, params, input_files=None, inputs_object=None,
      log=sys.stderr, accumulation_callback=None, symmetry_callback=None):
    if (input_files is None):
      input_files = {}
    adopt_init_args(self, locals())
    validate_params(params)
    r_free_params = params.mtz_file.r_free_flags
    import cctbx.miller
    from cctbx import r_free_utils
    from cctbx import crystal
    from scitbx.array_family import flex

    #-------------------------------------------------------------------
    # COLLECT ARRAYS
    if (inputs_object is None):
      inputs_object = array_input(params=params, input_files=input_files)
    miller_arrays = inputs_object.miller_arrays
    file_names = inputs_object.file_names
    array_types = inputs_object.array_types
    if params.show_arrays :
      shown_files = []
      for file_name, miller_array in zip(file_names, miller_arrays):
        if not file_name in shown_files :
          print("%s:" % file_name, file=log)
          shown_files.append(file_name)
        print("  %s" % miller_array.info().label_string(), file=log)
      return

    labels = ["H", "K", "L"]
    label_files = [None, None, None]
    self.created_r_free = False
    have_r_free_array = False
    self.final_arrays = []
    self.mtz_dataset = None
    self.wavelength = params.mtz_file.wavelength
    if (self.wavelength is None):
      file_wavelengths = {}
      for miller_array in miller_arrays :
        if (not miller_array.is_xray_data_array()) : continue
        info = miller_array.info()
        if (info is not None):
          if (not info.source in file_wavelengths):
            if (info.wavelength is not None):
              try :
                file_wavelengths[info.source] = float(info.wavelength)
              except ValueError as e :
                print("Warning: bad wavelength '%s'" % info.wavelength, file=log)
      all_wavelengths = set([ w for f, w in six.iteritems(file_wavelengths) ])
      if (len(all_wavelengths) == 1):
        self.wavelength = all_wavelengths.pop()
      elif (len(all_wavelengths) > 1):
        filtered_wavelengths = set([])
        # MTZ files typically have the wavelength set to 1.0 if it was not
        # previously specified, so we ignore these.
        for file_name, wavelength in six.iteritems(file_wavelengths):
          if (file_name.endswith(".mtz") and wavelength in [0.0, 1.0]):
            continue
          filtered_wavelengths.add(wavelength)
        if (len(filtered_wavelengths) == 1):
          self.wavelength = filtered_wavelengths.pop()
        elif (len(filtered_wavelengths) > 1):
          raise Sorry(("Multiple wavelengths present in input experimental "+
            "data arrays: %s.  Please specify the wavelength parameter "+
            "explicitly.") %
            ", ".join([ "%g"%x for x in sorted(list(filtered_wavelengths)) ]))

    #-------------------------------------------------------------------
    # SYMMETRY SETUP
    change_symmetry = change_point_group = False
    input_space_group = params.mtz_file.crystal_symmetry.space_group
    output_space_group = params.mtz_file.crystal_symmetry.output_space_group
    if (output_space_group is not None):
      output_sg = params.mtz_file.crystal_symmetry.output_space_group.group()
      input_point_group = input_space_group.group().build_derived_point_group()
      output_point_group = \
        output_space_group.group().build_derived_point_group()
      pg_number_in = input_point_group.type().number()
      pg_number_out = output_point_group.type().number()
      change_symmetry = True
      if (pg_number_out != pg_number_in):
        change_point_group = True
        print("Will expand to P1 symmetry before merging.", file=log)
    else :
      output_sg = params.mtz_file.crystal_symmetry.space_group.group()
    if params.mtz_file.crystal_symmetry.output_unit_cell is not None :
      output_uc = params.mtz_file.crystal_symmetry.output_unit_cell
      change_symmetry = True
    else :
      output_uc = params.mtz_file.crystal_symmetry.unit_cell
    if params.mtz_file.crystal_symmetry.expand_to_p1 and change_symmetry :
      raise Sorry("Output unit cell and space group must be undefined if "+
          "expand_to_p1 is True.")
    input_symm = crystal.symmetry(
      unit_cell=params.mtz_file.crystal_symmetry.unit_cell,
      space_group_info=params.mtz_file.crystal_symmetry.space_group,
      assert_is_compatible_unit_cell=False,
      force_compatible_unit_cell=False)
    if (not input_symm.is_compatible_unit_cell()):
      raise Sorry(("Input unit cell %s is incompatible with the specified "+
        "space group (%s).") % (str(params.mtz_file.crystal_symmetry.unit_cell),
          str(params.mtz_file.crystal_symmetry.space_group)))
    derived_sg = input_symm.space_group().build_derived_point_group()
    output_symm = crystal.symmetry(
      unit_cell=output_uc,
      space_group=output_sg,
      assert_is_compatible_unit_cell=False,
      force_compatible_unit_cell=False)
    if (not output_symm.is_compatible_unit_cell()):
      raise Sorry(("Output unit cell %s is incompatible with the specified "+
        "space group (%s).") % (str(output_uc), str(output_sg)))

    # Resolution limits
    (d_max, d_min) = get_best_resolution(miller_arrays, input_symm)
    if (d_max is None) and (params.mtz_file.d_max is None):
      raise Sorry("No low-resolution cutoff could be found in the "+
        "parameters or input file(s); you need to explicitly set this value "+
        "for the program to run.")
    if d_min is None :
      if params.mtz_file.d_min is None :
        raise Sorry("No high-resolution cutoff could be found in the "+
          "parameters or input file(s); you need to explicitly set this "+
          "value for the program to run.")
    # XXX resolution limits are used even if outside the range used by the
    # input data, in case we want to generate extra R-free flags for future
    # use
    if (params.mtz_file.d_max is not None):
      d_max = params.mtz_file.d_max
    if (params.mtz_file.d_min is not None):
      d_min = params.mtz_file.d_min
    if r_free_params.random_seed is not None :
      random.seed(r_free_params.random_seed)
      flex.set_random_seed(r_free_params.random_seed)

    #-------------------------------------------------------------------
    # MAIN LOOP
    i = 0
    r_free_arrays = []
    for (array_params, file_name, miller_array) in \
        zip(params.mtz_file.miller_array, file_names, miller_arrays):
      info = miller_array.info()
      array_name = "%s:%s" % (file_name, array_params.labels)

      #-----------------------------------------------------------------
      # APPLY SYMMETRY
      array_sg = miller_array.space_group()
      array_uc = miller_array.unit_cell()
      ignore_sg = params.mtz_file.crystal_symmetry.disable_space_group_check
      ignore_uc = params.mtz_file.crystal_symmetry.disable_unit_cell_check
      if (array_sg is not None) and (not ignore_sg):
        if array_sg.build_derived_point_group() != derived_sg :
          raise Sorry(("The point group for the Miller array %s (%s) does "+
            "not match the point group of the overall space group (%s).") %
            (array_name, str(array_sg), str(input_symm.space_group())))
      if (array_uc is not None) and (not ignore_uc):
        if not array_uc.is_similar_to(input_symm.unit_cell()):
          raise Sorry(("The unit cell for the Miller array %s (%s) is "+
            "significantly different than the output unit cell (%s).  You "+
            "can ignore this by setting disable_unit_cell_check=True (in "+
            "the GUI, enable \"Disable unit cell isomorphism check\").") %
            (array_name, str(array_uc), str(input_symm.unit_cell())))
      miller_array = miller_array.customized_copy(
        crystal_symmetry=input_symm).map_to_asu()
      if params.mtz_file.crystal_symmetry.expand_to_p1 :
        miller_array = miller_array.expand_to_p1()
      elif change_symmetry :
        sg_number = miller_array.space_group_info().type().number()
        if (change_point_group) and (sg_number != 1):
          miller_array = miller_array.expand_to_p1()
        miller_array = miller_array.customized_copy(
          crystal_symmetry=output_symm)
      if not miller_array.is_unique_set_under_symmetry():
        if miller_array.is_integer_array() and not is_rfree_array(miller_array,info):
          raise Sorry(("The data in %s cannot be merged because they are in "+
            "integer format.  If you wish to change symmetry (or the input "+
            "data are unmerged), you must omit this array.  (Note also that "+
            "merging will fail for R-free flags if the flags for symmetry-"+
            "related reflections are not identical.)") % array_name)
        miller_array = miller_array.merge_equivalents().array()

      if DEBUG :
        print("  Adjusted size:  %d" % miller_array.data().size(), file=log)
        if miller_array.sigmas() is not None :
          print("         sigmas:  %d" % miller_array.sigmas().size(), file=log)

      #-----------------------------------------------------------------
      # CHANGE OF BASIS
      # this will actually be done at the last minute - we just check for
      # incompatible options here
      if params.mtz_file.crystal_symmetry.change_of_basis is not None :
        if change_symmetry :
          raise Sorry("You may not change symmetry when change_of_basis is "+
            "defined.")

      output_array = modify_array(
        miller_array=miller_array,
        array_params=array_params,
        array_name=array_name,
        array_info=info,
        log=log,
        verbose=params.verbose)
      if([params.mtz_file.d_min,params.mtz_file.d_max].count(None)==0):
        if(params.mtz_file.d_min > params.mtz_file.d_max):
          msg="High resolution cutoff %s is larger than low resolution %s."
          raise Sorry(msg%(
            str(params.mtz_file.d_min), str(params.mtz_file.d_max)))
      output_array = output_array.resolution_filter(
        d_min=params.mtz_file.d_min,
        d_max=params.mtz_file.d_max)
      if (len(params.mtz_file.exclude_reflection) > 0):
        for hkl in params.mtz_file.exclude_reflection :
          output_array = output_array.delete_index(hkl)

      #-----------------------------------------------------------------
      # OUTPUT
      assert isinstance(output_array, cctbx.miller.array)
      default_label = array_params.column_root_label
      output_labels = array_params.output_labels
      if (default_label is not None):
        output_labels = None
      else :
        if i <= 25:
          default_label = 2 * string.ascii_uppercase[i]
        else:
          i1 = int(i//25)
          i2 = i - i1 * 25
          if i2 > 25:
            raise Sorry("Maximum of 625 columns in reflection file editor")

          default_label = 2 * string.ascii_uppercase[i1]  + \
             2* string.ascii_uppercase[i2]
      column_types = None
      import iotbx.mtz
      default_types = iotbx.mtz.default_column_types(output_array)
      if (array_types[i] is not None):
        if output_array.is_xray_amplitude_array():
          array_types[i] = re.sub("J", "F", array_types[i])
        elif output_array.is_xray_intensity_array():
          array_types[i] = re.sub("F", "J", array_types[i])
        if len(default_types) == len(array_types[i]):
          print("Recovering original column types %s" % array_types[i], file=log)
          column_types = array_types[i]
      if (output_array.data().size() == 0):
        raise Sorry("The array %s:%s ended up empty.  Please check the "+
          "resolution cutoffs to make sure they do not exclude all data "+
          "from the input file.  If you think the parameters were correct, "+
          "this is probably a bug; please contact help@phenix-online.org "+
          "with a description of the problem.")
      if is_rfree_array(output_array, info):
        r_free_arrays.append((output_array, info, output_labels,
          array_params.column_root_label, file_name))
      else :
        if DEBUG :
          print("  Final size:    %d" % output_array.data().size(), file=log)
          if output_array.sigmas() is not None :
            print("      sigmas:    %d" % output_array.sigmas().size(), file=log)
        try:
          self.add_array_to_mtz_dataset(
            output_array=output_array,
            default_label=default_label,
            column_types=column_types,
            out=log)
          ok_array = True
        except Exception as e:
          ok_array = False
          print("Skipping array '%s' which cannot be converted to MTZ" %(
            output_array.info), file = log)
        if ok_array:
          if (output_labels is not None):
            for label in output_labels :
              labels.append(label)
              label_files.append(file_name)
          else :
            n_cols = len(default_types)
            if (output_array.anomalous_flag() and
                not output_array.is_xray_reconstructed_amplitude_array()):
              n_cols *= 2
            for k in range(n_cols):
              labels.append(None)
              label_files.append(file_name)
          self.final_arrays.append(output_array)

      i += 1

    #-------------------------------------------------------------------
    # EXISTING R-FREE ARRAYS
    if len(r_free_arrays) > 0 :
      from iotbx.reflection_file_utils import get_r_free_flags_scores, \
        make_joined_set
      have_r_free_array = True
      combined_set = complete_set = None
      if len(self.final_arrays) > 0 :
        eps = r_free_params.d_eps
        combined_set = make_joined_set(self.final_arrays)
        complete_set = combined_set.complete_set(d_min=d_min-eps,
          d_max=d_max+eps)
        # XXX big hack
        missing = combined_set.lone_set(other=complete_set)
        if (missing.size() > 0):
          complete_set = complete_set.concatenate(other=missing)
      if (len(self.final_arrays) > 1):
        warnings.warn("Multiple Miller arrays are already present in this "+
          "file; the R-free flags will be generated based on the total "+
          "of reflections in all arrays combined.  If you want the fraction "+
          "of test set reflections to be relative to a specific array, you "+
          "should run the editor with that array separately first.",
          UserWarning)
      if (r_free_params.relative_to_complete_set):
        combined_set = complete_set
      elif (combined_set is not None):
        check_and_warn_about_incomplete_r_free_flags(combined_set)
      i = 0
      for (new_array, info, output_labels, root_label, file_name) in \
          r_free_arrays :
        # XXX this is important for guessing the right flag when dealing
        # with CCP4-style files, primarily when the flag values are not
        # very evenly distributed
        new_array.set_info(info)
        test_flag_value = None
        flag_scores = get_r_free_flags_scores(miller_arrays=[new_array],
          test_flag_value=r_free_params.old_test_flag_value)
        test_flag_value = flag_scores.test_flag_values[0]
        if (test_flag_value is None):
          if (r_free_params.old_test_flag_value is not None):
            test_flag_value = r_free_params.old_test_flag_value
          elif ((r_free_params.warn_if_all_same_value) or
                (r_free_params.extend)):
            raise Sorry(("The data in %s:%s appear to be R-free flags, but "+
              "a suitable test flag value (usually 1 or 0) could not be "+
              "automatically determined.  This may indicate that the flags "+
              "are uniform, which is not suitable for refinement; it can "+
              "also happen when there are exactly 3 different values used. "+
              " If this is not the case, you may specify the test flag "+
              "value manually by clicking the button labeled \"R-free flags "+
              "generation...\" and entering the value to use under "+
              "\"Original test flag value\".  Alternately, unchecking the box "+
              "\"Warn if R-free flags are all the same value\" will skip this "+
              "step, but you will not be able to extend the flags to higher "+
              "resolution.") % (file_name, info.label_string()))
        if (r_free_params.preserve_input_values):
          assert (not r_free_params.remediate_mismatches)
          r_free_flags = new_array
        else :
          new_data = (new_array.data()==test_flag_value)
          assert isinstance(new_data, flex.bool)
          r_free_flags = new_array.array(data=new_data)
        r_free_flags = r_free_flags.map_to_asu()
        generate_bijvoet_mates = (array_params.anomalous_data=="anomalous")
        if not r_free_flags.is_unique_set_under_symmetry():
          r_free_flags = r_free_flags.merge_equivalents().array()
        if (r_free_flags.anomalous_flag()):
          if (r_free_params.remediate_mismatches):
            print("Remediating any mismatched flags for Friedel mates...", file=log)
            r_free_flags = r_free_utils.remediate_mismatches(
              array=r_free_flags,
              log=log)
          r_free_flags = r_free_flags.average_bijvoet_mates()
          if (output_labels is not None) and (len(output_labels) != 1):
            assert (not combined_set.anomalous_flag())
            # XXX can't do this operation on a miller set - will expand the
            # r-free flags later
            generate_bijvoet_mates = True
        if (r_free_params.adjust_fraction):
          print("Resizing test set in %s" % array_name, file=log)
          r_free_as_bool = get_r_free_as_bool(r_free_flags,
            test_flag_value)
          r_free_flags = r_free_utils.adjust_fraction(
            miller_array=r_free_as_bool,
            fraction=r_free_params.fraction,
            log=log)
        if (r_free_params.extend):
          r_free_flags = r_free_utils.extend_flags(
            r_free_flags=r_free_flags,
            test_flag_value=test_flag_value,
            array_label=array_name,
            complete_set=combined_set,
            accumulation_callback=accumulation_callback,
            preserve_input_values=r_free_params.preserve_input_values,
            d_max=d_max,
            d_min=d_min,
            log=log)
        output_array = r_free_flags
        if (generate_bijvoet_mates):
          output_array = output_array.generate_bijvoet_mates()
        if (len(params.mtz_file.exclude_reflection) > 0):
          for hkl in params.mtz_file.exclude_reflection :
            output_array = output_array.delete_index(hkl)
        if (r_free_params.export_for_ccp4):
          print("%s: converting to CCP4 convention" % array_name, file=log)
          output_array = export_r_free_flags(
            miller_array=output_array,
            test_flag_value=True)
        default_label = root_label
        if (default_label is None):
          default_label = "A" + string.ascii_uppercase[i+1]
        self.add_array_to_mtz_dataset(
          output_array=output_array,
          default_label=default_label,
          column_types="I",
          out=log)
        if (output_labels is not None):
          validate_output_labels(
            miller_array=output_array,
            array_params=array_params,
            array_name=array_name,
            output_labels=output_labels)
          for label in output_labels :
            labels.append(label)
            label_files.append(file_name)
        else :
          if (output_array.anomalous_flag()):
            labels.extend([None,None])
            label_files.extend([file_name,file_name])
          else :
            labels.append(None)
            label_files.append(file_name)
        self.final_arrays.append(output_array)
        i += 1

    #-------------------------------------------------------------------
    # NEW R-FREE ARRAY
    if ((r_free_params.generate and not have_r_free_array) or
        r_free_params.force_generate):
      if (len(self.final_arrays) > 1):
        warnings.warn("Multiple Miller arrays are already present in this "+
          "file; the R-free flags will be generated based on the total "+
          "of reflections in all arrays combined.  If you want the fraction "+
          "of test set reflections to be relative to a specific array, you "+
          "should run the editor with that array separately first.",
          UserWarning)
      from iotbx.reflection_file_utils import make_joined_set
      combined_set = make_joined_set(self.final_arrays)
      complete_set = combined_set.complete_set(
        d_min=d_min-r_free_params.d_eps,
        d_max=d_max+r_free_params.d_eps)
      if (r_free_params.relative_to_complete_set):
        combined_set = complete_set
      else :
        check_and_warn_about_incomplete_r_free_flags(combined_set)
      # XXX this used to generate the test set from the actually complete set,
      # but the fraction free needs to be relative to the
      new_r_free_array = combined_set.generate_r_free_flags(
        fraction=r_free_params.fraction,
        max_free=r_free_params.max_free,
        lattice_symmetry_max_delta=r_free_params.lattice_symmetry_max_delta,
        use_lattice_symmetry=r_free_params.use_lattice_symmetry,
        use_dataman_shells=r_free_params.use_dataman_shells,
        n_shells=r_free_params.n_shells)
      if r_free_params.new_label is None or r_free_params.new_label == "" :
        r_free_params.new_label = "FreeR_flag"
      if r_free_params.export_for_ccp4 :
        print("%s: converting to CCP4 convention" % array_name, file=log)
        output_array = export_r_free_flags(
          miller_array=new_r_free_array,
          test_flag_value=True)
      else:
        output_array = new_r_free_array
      if (len(params.mtz_file.exclude_reflection) > 0):
        for hkl in params.mtz_file.exclude_reflection :
          output_array = output_array.delete_index(hkl)
      self.add_array_to_mtz_dataset(
        output_array=output_array,
        default_label="ZZZZ",
        column_types="I",
        out=log)
      labels.append(r_free_params.new_label)
      label_files.append("(new array)")
      self.created_r_free = True

    #-------------------------------------------------------------------
    # RE-LABEL COLUMNS
    mtz_object = self.mtz_dataset.mtz_object()
    self.label_changes = []
    self.mtz_object = mtz_object
    if not len(labels) == mtz_object.n_columns():
      print("\n".join([ "LABEL: %s" % label for label in labels ]), file=log)
      self.show(out=log)
      raise Sorry("The number of output labels does not match the final "+
        "number of labels in the MTZ file.  Details have been printed to the "+
        "console.")
    i = 0
    used = dict([ (label, 0) for label in labels ])
    invalid_chars = re.compile(r"[^A-Za-z0-9_\-+\(\)]")
    for column in self.mtz_object.columns():
      if (labels[i] is not None) and (column.label() != labels[i]):
        label = labels[i]
        original_label = label
        if invalid_chars.search(label) is not None :
          raise Sorry(("Invalid label '%s'.  Output labels may only contain "+
            "alphanumeric characters, underscore, plus and minus signs, or "+
            "parentheses.")
            % label)
        if used[label] > 0 :
          if params.mtz_file.resolve_label_conflicts :
            if label.endswith("(+)") or label.endswith("(-)"):
              label = label[0:-3] + ("_%d" % (used[label]+1)) + label[-3:]
            else :
              label += "_%d" % (used[labels[i]] + 1)
            if used.get(label,None) is not None:  # this was already there
              found = False
              for k in range(1,10000):
                if label.endswith("(+)") or label.endswith("(-)"):
                  new_label = \
                     label[0:-3] + ("_%d" % (used[label]+k)) + label[-3:]
                else :
                  new_label = label + "_%d" % (used[labels[i]] + k)
                if used.get(new_label,None) is None: # got it
                  label = new_label
                  used[label] = 0
                  found = True
                  break
              if not found:
                raise Sorry("Unable to resolve multiple similar labels")
            self.label_changes.append((label_files[i], labels[i], label))
          else :
            raise Sorry(("Duplicate column label '%s'.  Specify "+
              "resolve_label_conflicts=True to automatically generate "+
              "non-redundant labels, or edit the parameters to provide your"+
              "own choice of labels.") % labels[i])
        try :
          column.set_label(label)
        except RuntimeError as e :
          if ("new_label is used already" in str(e)):
            col_names = [ col.label() for col in mtz_object.columns() ]
            raise RuntimeError(("Duplicate column label '%s': current labels "+
              "are %s; user-specified output labels are %s.") %
              (label, " ".join(col_names), " ".join(labels)))
        else :
          used[original_label] += 1
      i += 1

  def add_array_to_mtz_dataset(self, output_array, default_label,
      column_types, out=sys.stdout):
    # apply change of basis here
    if (self.params.mtz_file.crystal_symmetry.change_of_basis is not None):
      output_array, cb_op = output_array.apply_change_of_basis(
        change_of_basis=self.params.mtz_file.crystal_symmetry.change_of_basis,
        eliminate_invalid_indices=\
          self.params.mtz_file.crystal_symmetry.eliminate_invalid_indices,
        out=out)
    if (self.params.mtz_file.crystal_symmetry.eliminate_sys_absent):
      output_array = output_array.eliminate_sys_absent()
    if self.mtz_dataset is None :
      self.mtz_dataset = output_array.as_mtz_dataset(
        column_root_label=default_label,
        column_types=column_types,
        wavelength=self.wavelength)
    else :
      self.mtz_dataset.add_miller_array(
        miller_array=output_array,
        column_root_label=default_label,
        column_types=column_types)

  def show(self, out=sys.stdout):
    if self.mtz_object is not None :
      print("", file=out)
      print(("=" * 20) + " Summary of output file " + ("=" * 20), file=out)
      self.mtz_object.show_summary(out=out, prefix="  ")
      print("", file=out)

  def finish(self):
    assert self.mtz_object is not None
    if self.params.verbose :
      self.show(out=self.log)
    self.mtz_object.write(file_name=self.params.mtz_file.output_file)
    n_refl = self.mtz_object.n_reflections()
    del self.mtz_object
    self.mtz_object = None
    return n_refl

#-----------------------------------------------------------------------
# TODO get rid of these two (need to make sure they aren't imported elsewhere)
def get_r_free_stats(*args, **kwds):
  from cctbx import r_free_utils
  return r_free_utils.get_r_free_stats(*args, **kwds)

def get_r_free_as_bool(*args, **kwds):
  from cctbx import r_free_utils
  return r_free_utils.get_r_free_as_bool(*args, **kwds)

def get_best_resolution(miller_arrays, input_symm=None):
  best_d_min = None
  best_d_max = None
  for array in miller_arrays :
    array_symm = array.crystal_symmetry()
    if ((None in [array_symm.space_group(), array_symm.unit_cell()]) and
        (input_symm is not None)):
      array = array.customized_copy(crystal_symmetry=input_symm)
    try :
      (d_max, d_min) = array.d_max_min()
      if best_d_max is None or d_max > best_d_max :
        best_d_max = d_max
      if best_d_min is None or d_min < best_d_min :
        best_d_min = d_min
    except Exception as e :
      pass
  return (best_d_max, best_d_min)

def is_rfree_array(miller_array, array_info):
  from iotbx import reflection_file_utils
  return ((miller_array.is_integer_array() or
           miller_array.is_bool_array()) and
          reflection_file_utils.looks_like_r_free_flags_info(array_info))

def export_r_free_flags(miller_array, test_flag_value):
  from cctbx import r_free_utils
  new_flags = r_free_utils.export_r_free_flags_for_ccp4(
    flags=miller_array.data(),
    test_flag_value=test_flag_value)
  return miller_array.customized_copy(data=new_flags)

def get_original_array_types(input_file, original_labels):
  array_types = ""
  mtz_file = input_file.file_object.file_content()
  mtz_columns = mtz_file.column_labels()
  mtz_types = mtz_file.column_types()
  mtz_crossref = dict(zip(mtz_columns, mtz_types))
  for label in original_labels :
    array_types += mtz_crossref[label]
  return array_types

def guess_array_output_labels(miller_array):
  info = miller_array.info()
  assert info is not None
  labels = info.labels
  output_labels = labels
  if (labels in [["i_obs","sigma"], ["Intensity+-","SigmaI+-"]]):
    if miller_array.anomalous_flag():
      output_labels = ["I(+)", "SIGI(+)", "I(-)", "SIGI(-)"]
    else :
      output_labels = ["I", "SIGI"]
  elif (miller_array.is_xray_reconstructed_amplitude_array()):
    output_labels = ["F", "SIGF", "DANO", "SIGDANO", "ISYM"]
  elif ((miller_array.is_xray_amplitude_array() or
         miller_array.is_xray_intensity_array()) and
        miller_array.anomalous_flag()):
    if (len(labels) == 2) and (miller_array.sigmas() is not None):
      output_labels = [ "%s(+)" % labels[0],  # data(+)
                        "%s(+)" % labels[1],  # sigma(+)
                        "%s(-)" % labels[0],  # data(-)
                        "%s(-)" % labels[1] ] # sigma(-)
    elif (len(labels) == 1) and (miller_array.sigmas() is None):
      output_labels = [ "%s(+)" % labels[0], "%s(-)" % labels[0] ]
    elif (miller_array.anomalous_flag()) and (len(labels) == 5): # mmCIF
      output_labels = ["I(+)","SIGI(+)","I(-)","SIGI(-)"]
    elif (not miller_array.anomalous_flag()) and (len(labels) == 3): # mmCIF
      output_labels = ["I","SIGI"]

  # Catch general mmCIF inputs and remove leading label (it is dataset name)
  #  and remove anything before "." because those are not allowed in output
  n_expected = get_number_of_expected_columns(miller_array,
    raise_sorry_on_errors = False)
  if output_labels and (len(output_labels) == n_expected + 1):
    output_labels = edit_mmcif_output_labels(output_labels,
      n_expected = n_expected)
  return output_labels

def edit_mmcif_output_labels(output_labels, n_expected = None):
  assert n_expected and len(output_labels) == n_expected + 1
  output_labels = output_labels[1:]
  new_output_labels = []
  for o in output_labels:
    if o.find(".") > -1:
      o = o.split(".")[-1]
    new_output_labels.append(o)
  return new_output_labels

def modify_array(
    miller_array,
    array_name,
    array_params,
    array_info=None,
    verbose=True,
    debug=False,
    log=sys.stdout):
  """
  Perform various manipulations on a Miller array.  This can be applied to
  any data type, although certain options are limited to experimental data.
  """
  from scitbx.array_family import flex
  output_labels = array_params.output_labels
  if (output_labels is None) and (array_params.column_root_label is None):
    raise Sorry("Missing output labels for %s!" % array_name)
  if (array_params.column_root_label is not None):
    output_labels = None
  labels_base = re.sub(",merged$", "", array_params.labels)
  input_labels = labels_base.split(",")
  if not None in [array_params.scale_factor, array_params.scale_max] :
    raise Sorry("The parameters scale_factor and scale_max are " +
      "mutually exclusive.")
  if not False in [array_params.remove_negatives,
                   array_params.massage_intensities] :
    raise Sorry("The parameters remove_negatives and massage_intensities "+
      "are mutually exclusive.")
  if debug :
    print("  Starting size:  %d" % miller_array.data().size(), file=log)
    if miller_array.sigmas() is not None :
      print("         sigmas:  %d" % miller_array.sigmas().size(), file=log)
  if verbose :
    print("Processing %s" % array_name, file=log)
  if array_params.d_max is not None and array_params.d_max <= 0 :
    array_params.d_max = None
  if array_params.d_min is not None and array_params.d_min <= 0 :
    array_params.d_min = None
  output_labels = array_params.output_labels
  if (output_labels is None) and (array_params.column_root_label is None):
    raise Sorry("Missing output labels for %s!" % array_name)
  if (array_params.column_root_label is not None):
    output_labels = None
  wavelength = getattr(array_info, "wavelength", None)
  if not None in [array_params.scale_factor, array_params.scale_max] :
    raise Sorry("The parameters scale_factor and scale_max are " +
      "mutually exclusive.")
  if not False in [array_params.remove_negatives,
                   array_params.massage_intensities] :
    raise Sorry("The parameters remove_negatives and massage_intensities "+
      "are mutually exclusive.")
  if debug :
    print("  Starting size:  %d" % miller_array.data().size(), file=log)
    if miller_array.sigmas() is not None :
      print("         sigmas:  %d" % miller_array.sigmas().size(), file=log)
  if debug :
    print("  Resolution before resolution filter: %.2f - %.2f" % (
      miller_array.d_max_min()), file=log)
  # go ahead and perform the array-specific cutoff
  miller_array = miller_array.resolution_filter(
    d_min=array_params.d_min,
    d_max=array_params.d_max)
  if debug :
    print("              after resolution filter: %.2f - %.2f" % (
      miller_array.d_max_min()), file=log)
    print("  Truncated size: %d" % miller_array.data().size(), file=log)
    if miller_array.sigmas() is not None :
      frint >> log, "          sigmas: %d" % miller_array.sigmas().size()
  # anomalous manipulation
  if (miller_array.anomalous_flag() and
      array_params.anomalous_data == "merged"):
    print(("Converting array %s from anomalous to non-anomalous." %
                   array_name), file=log)
    if (not miller_array.is_xray_intensity_array()):
      miller_array = miller_array.average_bijvoet_mates()
      if miller_array.is_xray_reconstructed_amplitude_array():
        miller_array.set_observation_type_xray_amplitude()
    else :
      miller_array = miller_array.average_bijvoet_mates()
      miller_array.set_observation_type_xray_intensity()
  elif ((not miller_array.anomalous_flag()) and
        (array_params.anomalous_data == "anomalous")):
    print("Generating Bijvoet mates for %s" % array_name, file=log)
    miller_array = miller_array.generate_bijvoet_mates()
  # scale factors
  if (array_params.scale_max is not None):
    print(("Scaling %s such that the maximum value is: %.6g" %
                   (array_name, array_params.scale_max)), file=log)
    miller_array = miller_array.apply_scaling(target_max=array_params.scale_max)
  elif (array_params.scale_factor is not None):
    print(("Multiplying data in %s with the factor: %.6g" %
                   (array_name, array_params.scale_factor)), file=log)
    miller_array = miller_array.apply_scaling(factor=array_params.scale_factor)
  # Since this function has many built-in consistency checks, we always run
  # it even for non-experimental arrays
  miller_array = modify_experimental_data_array(
    miller_array=miller_array,
    array_name=array_name,
    array_params=array_params,
    log=log)
  # Information removal for running control experiments - normally these
  # would be used on experimental data, but there is no reason why they can't
  # be applied to other array types.
  if (array_params.shuffle_values):
    print("Shuffling values for %s" % array_name, file=log)
    combined_array = None
    tmp_array = miller_array.deep_copy()
    tmp_array.setup_binner(n_bins=min(100, tmp_array.indices().size()//10))
    for i_bin in tmp_array.binner().range_used():
      bin_sel = tmp_array.binner().selection(i_bin)
      bin_array = tmp_array.select(bin_sel)
      perm = flex.random_permutation(bin_array.data().size())
      sigmas = bin_array.sigmas()
      if (sigmas is not None):
        sigmas = sigmas.select(perm)
      data = bin_array.data().select(perm)
      bin_array = bin_array.customized_copy(data=data, sigmas=sigmas)
      if (combined_array is None):
        combined_array = bin_array.deep_copy()
      else :
        combined_array = combined_array.concatenate(bin_array)
    if (combined_array.indices().size() != miller_array.indices().size()):
      raise RuntimeError("Array size changed: %d versus %d" %
        (combined_array.indices().size(), miller_array.indices().size()))
    miller_array = combined_array
  if (array_params.reset_values_to):
    if (not miller_array.is_real_array() and
        not miller_array.is_integer_array()):
      raise Sorry("Resetting the values for %s is not permitted." %
        array_name)
    print("Resetting values for %s to %g" % (array_name,
      array_params.reset_values_to), file=log)
    data = miller_array.data().deep_copy()
    new_value = array_params.reset_values_to
    if miller_array.is_integer_array():
      new_value = int(new_value)
    data.fill(new_value)
    miller_array = miller_array.customized_copy(data=data)
  if (array_params.force_type != "auto"):
    if (not miller_array.is_xray_data_array()):
      raise Sorry(("You may only override the output observation type for "+
        "amplitudes or intensities - the data in %s are unsupported.") %
        array_name)
    if (array_params.force_type == "amplitudes"):
      miller_array = miller_array.set_observation_type_xray_amplitude()
    elif (array_params.force_type == "intensities"):
      miller_array = miller_array.set_observation_type_xray_intensity()
  if (not is_rfree_array(miller_array, array_info)):
    if (array_params.column_root_label is not None):
      validate_column_root_label(
        miller_array=miller_array,
        array_name=array_name,
        root_label=array_params.column_root_label)
    elif (output_labels is not None):
      validate_output_labels(
        miller_array=miller_array,
        array_params=array_params,
        array_name=array_name,
        output_labels=output_labels)
  return miller_array

def modify_experimental_data_array(
    miller_array,
    array_name,
    array_params,
    log=sys.stdout):
  """
  Manipulations common to amplitude and intensity arrays only.
  """
  # negative intensity/amplitude remediation
  if (array_params.remove_negatives):
    if (miller_array.is_real_array()):
      print("Removing negatives from %s" % array_name, file=log)
      miller_array = miller_array.select(miller_array.data() > 0)
      if (miller_array.sigmas() is not None):
        miller_array = miller_array.select(miller_array.sigmas() > 0)
    else :
      raise Sorry("remove_negatives not applicable to %s." % array_name)
  elif (array_params.massage_intensities):
    if (miller_array.is_xray_intensity_array()):
      if array_params.output_as == "amplitudes" :
        miller_array = miller_array.enforce_positive_amplitudes()
      else :
        raise Sorry(("You must output %s as amplitudes to use the "+
          "massage_intensities option.") % array_name)
    else :
      raise Sorry("The parameter massage_intensities is only valid for "+
        "X-ray intensity arrays.")
  # I/sigma filtering
  if (array_params.filter_by_signal_to_noise is not None):
    if (not miller_array.is_xray_data_array()):
      raise Sorry(("Filtering by signal-to-noise is only supported for "+
        "amplitudes or intensities (failed on %s).") % array_name)
    elif (array_params.filter_by_signal_to_noise <= 0):
      raise Sorry(("A value greater than zero is required for the "+
        "cutoff for filtering by signal to noise ratio (failed array: %s).") %
        array_name)
    sigmas = miller_array.sigmas()
    if (sigmas is None):
      raise Sorry(("Sigma values must be defined to filter by signal "+
        "to noise ratio (failed on %s).") % array_name)
    elif (not sigmas.all_ne(0.0)):
      # XXX should it just remove these too?
      raise Sorry(("The sigma values for the array %s include one or "+
        "more zeros - filtering by signal to noise not supported.") %
        array_name)
    data = miller_array.data()
    miller_array = miller_array.select(
      (data / sigmas) > array_params.filter_by_signal_to_noise)
  # apply B-factors
  # leave the default as [0]*6 to make the format clear, but reset to
  # None if unchanged
  if (array_params.add_b_aniso == [0,0,0,0,0,0]):
    array_params.add_b_aniso = None
  if ((array_params.add_b_iso is not None) or
      (array_params.add_b_aniso is not None)):
    if (not miller_array.is_real_array() and
        not miller_array.is_complex_array()):
      raise Sorry(("Applying a B-factor to the data in %s is not "+
        "permitted.") % array_name)
    if (array_params.add_b_iso is not None):
      miller_array = miller_array.apply_debye_waller_factors(
        b_iso=array_params.add_b_iso,
        apply_to_sigmas=True)
    if (array_params.add_b_aniso is not None):
      miller_array = miller_array.apply_debye_waller_factors(
        b_cart=array_params.add_b_aniso,
        apply_to_sigmas=True)
  # data type manipulation
  if miller_array.is_xray_intensity_array():
    if (array_params.output_as in ["amplitudes", "amplitudes_fw"]):
      if (array_params.output_as == "amplitudes"):
        miller_array = miller_array.f_sq_as_f()
      else :
        from cctbx import french_wilson
        miller_array = french_wilson.french_wilson_scale(
          miller_array=miller_array,
          log=log)
      miller_array.set_observation_type_xray_amplitude()
  elif miller_array.is_xray_amplitude_array():
    if array_params.output_as == "intensities" :
      miller_array = miller_array.f_as_f_sq()
      miller_array.set_observation_type_xray_intensity()
  return miller_array

def validate_output_labels(
    miller_array,
    array_name,
    array_params,
    output_labels):
  """
  Check output labels for consistency with selected options (done before the
  array undergoes most modifications).
  """
  def raise_sorry_if_wrong_number_of_labels(n_expected):
    assert (n_expected is not None)
    msg_extra = "If you are "+ \
        "unsure which labels to provide, use the simpler column root label "+ \
        "instead, which will be expanded as necessary. (Note that the " + \
        "number of input labels from non-MTZ file formats does not " + \
        "necessarily correspond to the expected number of output labels.)"
    n_used = len(output_labels)
    msg_expected = "%d are" % n_expected
    if (n_expected == 1):
      msg_expected = "%d is" % n_expected
    msg_used = "labels \"%s\" have" % " ".join(output_labels)
    if (n_used == 1):
      msg_used = "label \"%s\" has" % " ".join(output_labels)
    if (n_expected > n_used):
      raise Sorry(("You have not specified enough MTZ column labels for the "+
        "array %s; only the %s been specified, but %s "+
        "required for the output array after processing.  " + msg_extra) %
        (array_name, msg_used, msg_expected))
    elif (n_expected < n_used):
      raise Sorry(("You have specified too many column labels for the array "+
        "%s; the %s been specified, but only %s are allowed "+
        "for the output array after processing.  " + msg_extra) %
        (array_name, msg_used, msg_expected))
  n_expected = get_number_of_expected_columns(miller_array,
    array_params = array_params,
    array_name = array_name,
    output_labels = output_labels,
    raise_sorry_on_errors = True)
  raise_sorry_if_wrong_number_of_labels(n_expected)
  if miller_array.is_xray_data_array():
    validate_column_root_label(
      miller_array=miller_array,
      root_label = output_labels[0].upper(),
      array_name=array_name)
  return True

def get_number_of_expected_columns(miller_array,
    output_labels = None,
    array_params = None,
    array_name = None,
    raise_sorry_on_errors = True):
  if raise_sorry_on_errors:
    assert (output_labels is not None) and (array_params is not None) and (
     array_name is not None)
  n_expected = None
  if (miller_array.is_xray_reconstructed_amplitude_array()):
    # FIXME this needs to be handled better - but it should at least
    # catch files from CCP4 data processing
    if (raise_sorry_on_errors) and ((len(output_labels) != 5) and
        (not array_params.anomalous_data == "merged")):
      raise Sorry(("The array %s will be output as "+
        "amplitudes and anomalous differences with sigmas, plus ISYM. "+
        "Five columns will be written, but %d labels were specified.") %
        (array_name, len(output_labels)))
    n_expected = 5
  elif (miller_array.anomalous_flag()):
    if miller_array.is_real_array():
      if (miller_array.sigmas() is not None):
        n_expected = 4
      else :
        n_expected = 2
    elif miller_array.is_complex_array():
      assert miller_array.sigmas() is None
      n_expected = 4
    elif miller_array.is_hendrickson_lattman_array():
      n_expected = 8
    else :
      n_expected = 2
  else :
    if miller_array.is_real_array():
      if (miller_array.sigmas() is not None):
        n_expected = 2
      else :
        n_expected = 1
    elif miller_array.is_complex_array():
      if (raise_sorry_on_errors) and (
           miller_array.sigmas() is not None):
        raise RuntimeError("Combination of sigmas and complex data not allowed for array %s" % array_name)
      n_expected = 2
    elif miller_array.is_hendrickson_lattman_array():
      n_expected = 4
    else :
      n_expected = 1
  return n_expected


def validate_column_root_label(miller_array, root_label, array_name):
  """
  Check the root MTZ label for a Miller array to ensure consistency with
  data type - this is done after the array has been processed.
  """
  root_label = root_label.upper()
  if (miller_array.is_xray_intensity_array()):
    if (not root_label.startswith("I")):
      raise Sorry(("The column label prefix for the array '%s' "+
        "is inconsistent with the output array type (intensities). "+
        "Please use 'I' (either case) as the first character in the "+
        "label.") % (array_name))
  elif (miller_array.is_xray_amplitude_array()):
    if (not root_label.startswith("F")):
      raise Sorry(("The specified column label prefix for the array '%s' "+
        "is inconsistent with the output array type (amplitudes). "+
        "Please use 'F' (either case) as the first character in the "+
        "label.") % (array_name))
  else :
    if (root_label == "I") or (root_label == "F"):
      raise Sorry(("You have specified the column label prefix '%s' "+
        "for the array '%s', which is neither intensities nor "+
        "amplitudes; the base labels 'I' and 'F' are reserved "+
        "for these array data types .") %
        (root_label, array_name))
  return True

# XXX the requirement for defined crystal symmetry in phil input files is
# problematic for automation, where labels and operations are very
# standardized.  so I added this to identify unique symmetry information
# in the collected input files.
def collect_symmetries(file_names):
  from iotbx import crystal_symmetry_from_any
  file_names = set(file_names)
  file_symmetries = []
  for file_name in file_names :
    symm = crystal_symmetry_from_any.extract_from(file_name)
    if (symm is not None):
      file_symmetries.append(symm)
  return file_symmetries

def resolve_symmetry(file_symmetries, current_space_group, current_unit_cell):
  space_groups = []
  unit_cells = []
  for symm in file_symmetries :
    if (symm is not None):
      space_group = symm.space_group_info()
      unit_cell = symm.unit_cell()
      if (space_group is not None) and (unit_cell is not None):
        if (current_space_group is None):
          group = space_group.group()
          for other_sg in space_groups :
            other_group = other_sg.group()
            if (other_sg.type().number() != group.type().number()):
              raise Sorry("Ambiguous space group information in input files - "+
                "please specify symmetry parameters.")
        if (current_unit_cell is None):
          for other_uc in unit_cells :
            if (not other_uc.is_similar_to(unit_cell, 0.001, 0.1)):
              raise Sorry("Ambiguous unit cell information in input files - "+
                "please specify symmetry parameters.")
        space_groups.append(space_group)
        unit_cells.append(unit_cell)
  consensus_space_group = current_space_group
  consensus_unit_cell = current_unit_cell
  if (len(space_groups) > 0) and (current_space_group is None):
    consensus_space_group = space_groups[0]
  if (len(unit_cells) > 0) and (current_unit_cell is None):
    consensus_unit_cell = unit_cells[0]
  return (consensus_space_group, consensus_unit_cell)

def check_and_warn_about_incomplete_r_free_flags(combined_set):
  completeness = combined_set.completeness()
  if (completeness < 0.99):
    warnings.warn(("The arrays in the input file are incomplete "+
      "(%.1f%% of reflections missing), so the newly generated R-free "+
      "flags will be incomplete as well.  This will not cause a problem "+
      "for refinement, but may result in inconsistent flags later on if "+
      "you collect additional data.  You can disable this warning by "+
      "clicking the box labeled \"Generate flags relative to maximum "+
      "complete set\" in the R-free options dialog window.") %
      (100 * (1-completeness)), UserWarning)

def usage(out=sys.stdout, attributes_level=0):
  print("""
# usage: iotbx.reflection_file_editor [file1.mtz ...] [parameters.eff]
#            --help      (print this message)
#            --details   (show parameter help strings)
# Dumping default parameters:
""", file=out)
  master_phil.show(out=out, attributes_level=attributes_level)

def generate_params(file_name, miller_array, include_resolution=False):
  param_str = """mtz_file.miller_array {
  file_name = %s
  labels = %s
""" % (file_name, miller_array.info().label_string())
  output_labels = guess_array_output_labels(miller_array)
  param_str += "  output_labels = " + " ".join(output_labels)
  if include_resolution :
    try :
      (d_max, d_min) = miller_array.d_max_min()
      # FIXME gross gross gross!
      d_max += 0.001
      d_min -= 0.001
      param_str += """  d_max = %.5f\n  d_min = %.5f\n""" % (d_max, d_min)
    except Exception :
      pass
  param_str += "}"
  return param_str

def validate_params(params):
  if (len(params.mtz_file.miller_array) == 0):
    raise Sorry("No Miller arrays have been selected for the output file.")
  elif len(params.mtz_file.miller_array) > 625 :
    raise Sorry("Only 625 or fewer arrays may be used.")
  if None in [params.mtz_file.crystal_symmetry.space_group,
              params.mtz_file.crystal_symmetry.unit_cell] :
    raise Sorry("Missing or incomplete symmetry information.")
  if (params.mtz_file.r_free_flags.preserve_input_values):
    if (not (0 < params.mtz_file.r_free_flags.fraction < 0.5)):
      raise Sorry("The R-free flags fraction must be greater than zero and "+
        "less than 0.5.")
    if (params.mtz_file.r_free_flags.export_for_ccp4):
      raise Sorry("r_free_flags.preserve_input_values and "+
        "r_free_flags.export_for_ccp4 may not be used together.")
    if (params.mtz_file.r_free_flags.adjust_fraction):
      raise Sorry("Preserving input values of R-free flags is not supported "+
        "when resizing a test set to the specified fraction.")
    if (params.mtz_file.r_free_flags.remediate_mismatches):
      raise Sorry("Preserving input values of R-free flags is not supported "+
        "when correcting mismatched Friedel/Bijvoet pairs.")
  check_if_output_directory_exists(file_name=params.mtz_file.output_file)

#-----------------------------------------------------------------------
def run(args, out=sys.stdout):
  from iotbx import file_reader
  crystal_symmetry_from_pdb = None
  crystal_symmetries_from_hkl = []
  reflection_files = []
  reflection_file_names = []
  user_phil = []
  all_arrays = []
  if len(args) == 0 :
    usage()
  interpreter = master_phil.command_line_argument_interpreter(
    home_scope=None)
  for arg in args :
    if arg in ["--help", "--options", "--details"] :
      usage(attributes_level=args.count("--details"))
      return True
    elif arg in ["-q", "--quiet"] :
      out = null_out()
    elif os.path.isfile(arg):
      full_path = os.path.abspath(arg)
      try :
        file_phil = iotbx.phil.parse(file_name=full_path)
      except Exception :
        pass
      else :
        user_phil.append(file_phil)
        continue
      input_file = file_reader.any_file(full_path)
      if input_file.file_type == "hkl" :
        miller_arrays = input_file.file_server.miller_arrays
        for array in miller_arrays :
          symm = array.crystal_symmetry()
          if symm is not None :
            crystal_symmetries_from_hkl.append(symm)
            break
        reflection_files.append(input_file)
        reflection_file_names.append(os.path.abspath(input_file.file_name))
      elif input_file.file_type == "pdb" :
        symm = input_file.file_object.crystal_symmetry()
        if symm is not None :
          crystal_symmetry_from_pdb = symm
    else :
      if arg.startswith("--"):
        arg = arg[2:] + "=True"
      try :
        cmdline_phil = interpreter.process(arg=arg)
      except Exception :
        pass
      else :
        user_phil.append(cmdline_phil)
  input_files = {}
  for input_file in reflection_files :
    input_files[input_file.file_name] = input_file
    file_arrays = input_file.file_object.as_miller_arrays()
    params = []
    for miller_array in file_arrays :
      params.append(generate_params(input_file.file_name, miller_array))
    file_params_str = "\n".join(params)
    user_phil.append(iotbx.phil.parse(file_params_str))

  working_phil = master_phil.fetch(sources=user_phil)
  params = working_phil.extract()
  if crystal_symmetry_from_pdb is not None :
    params.mtz_file.crystal_symmetry.space_group = \
      crystal_symmetry_from_pdb.space_group_info()
    params.mtz_file.crystal_symmetry.unit_cell = \
      crystal_symmetry_from_pdb.unit_cell()
  elif None in [params.mtz_file.crystal_symmetry.space_group,
                params.mtz_file.crystal_symmetry.unit_cell] :
    # extract symmetry information from all reflection files
    all_hkl_file_names = [ p.file_name for p in params.mtz_file.miller_array ]
    need_symmetry_for_files = []
    for file_name in all_hkl_file_names :
      file_name = os.path.abspath(file_name)
      if (not file_name in reflection_file_names):
        need_symmetry_for_files.append(file_name)
    crystal_symmetries_from_hkl.extend(
      collect_symmetries(need_symmetry_for_files))
    (space_group, unit_cell) = resolve_symmetry(
      file_symmetries=crystal_symmetries_from_hkl,
      current_space_group=params.mtz_file.crystal_symmetry.space_group,
      current_unit_cell=params.mtz_file.crystal_symmetry.unit_cell)
    params.mtz_file.crystal_symmetry.space_group = space_group
    params.mtz_file.crystal_symmetry.unit_cell = unit_cell
  if params.mtz_file.output_file is None :
    n = 0
    for file_name in os.listdir(os.getcwd()):
      if file_name.startswith("reflections_") and file_name.endswith(".mtz"):
        n += 1
    params.mtz_file.output_file = "reflections_%d.mtz" % (n+1)
  params.mtz_file.output_file = os.path.abspath(params.mtz_file.output_file)
  process = process_arrays(params, input_files=input_files, log=out)
  if params.dry_run :
    print("# showing final parameters", file=out)
    master_phil.format(python_object=params).show(out=out)
    if params.verbose :
      process.show(out=out)
    return process
  process.finish()
  print("Data written to %s" % params.mtz_file.output_file, file=out)
  return process

if __name__ == "__main__" :
  run(sys.argv[1:])

#---end


 *******************************************************************************


 *******************************************************************************
iotbx/reflection_file_reader.py
"""
This module provides a generic frontend to all of the reflection file formats
supported in ``iotbx``.  Note that this module can also be used indirectly via
the even more generic :py:mod:`iotbx.file_reader` module, which provides
a unified API for reading in any file (but calls
:py:class:`iotbx.reflection_file_reader.any_reflection_file` internally).
Currently, the supported formats include:

- **CIF**: Crystallographic Information Format, the common syntax for
  specifying most structured data encountered in crystallography (but more
  widely used in small-molecule versus macromolecular crystallography), usually
  as ASCII text.  May encapsulate a variety of other data types, but only
  reflection data (of any type) is handled by this particular module.  Uses
  :py:mod:`iotbx.cif` internally.
- **MTZ**: Binary file format established by `CCP4 <http://www.ccp4.ac.uk>`_
  capable of storing any numerical data, and used by most major macromolecular
  crystallography software packages.
  Because of its speed and broad compatibility, this is the primary interchange
  format for reflection data in Phenix.  Uses :py:mod:`iotbx.mtz` internally.
- **Scalepack**: Fixed-format ASCII text produced by the program of the same
  name and the HKL2000 graphical interface.  This is actually two separate
  formats: one for merged intensities (with or without Friedel mates), another
  for unmerged intensities and associated processing metadata.  Uses either
  :py:mod:`iotbx.scalepack.merge` or :py:mod:`iotbx.scalepack.no_merge_original_index` internally.
- **CNS**: ASCII format, not as flexible as MTZ or CIF but able to store
  either amplitudes or intensities, R-free flags, and Hendrickson-Lattman
  coefficients.  Uses :py:mod:`iotbx.cns.reflection_reader` internally.
- **SHELX**: Fixed-format ASCII used by the eponymous software suite.  This
  format has significan disadvantages, discussed below.  Uses
  :py:mod:`iotbx.shelx.hklf` internally.
- **XDS**: ASCII format for processed intensities (both merged and unmerged).
  Uses :py:mod:`iotbx.xds.read_ascii` internally.
- **D*Trek**: ASCII format produced by software sold by Rigaku.

Independently, the :py:class:`cctbx.miller.array` class defines output methods
for CIF, MTZ, CNS, SHELX, and unmerged Scalepack files (although only the first
two are recommended for routine use).

Note that the underlying formats do not always contain complete information
about the crystal or even the data type.  SHELX format is especially
problematic as it not only omits crystal symmetry, but the same format may be
used to store either amplitudes or intensities, without any distinguishing
features.  As a crude workaround for the latter problem, the data type may be
specified as part of the file name::

  hkl_file = any_reflection_file("data.hkl=hklf4")
  hkl_file = any_reflection_file("data.hkl=intensities")

Other formats (CNS, unmerged Scalepack) may have incomplete or missing crystal
symmetry.  MTZ, XDS, and (usually) CIF files will be more complete.
"""

from __future__ import absolute_import, division, print_function
from iotbx import mtz
from iotbx.scalepack import merge as scalepack_merge
from iotbx.scalepack import no_merge_original_index as scalepack_no_merge
from iotbx.cif import reader as cif_reader
from iotbx.cns import reflection_reader as cns_reflection_reader
from iotbx.cns import index_fobs_sigma_reader as cns_index_fobs_sigma_reader
from iotbx.dtrek import reflnlist_reader as dtrek_reflnlist_reader
from iotbx.shelx import hklf as shelx_hklf
from iotbx.shelx import crystal_symmetry_from_ins
from iotbx.xds.read_ascii import reader as xds_ascii_reader
from iotbx.xds.integrate_hkl import reader as xds_integrate_hkl_reader
from iotbx.solve.fpfm_reader import reader as solve_fpfm_reader
from iotbx.option_parser import option_parser
from cctbx import miller
from cctbx import crystal
from libtbx import easy_pickle
from libtbx.utils import Sorry, detect_binary_file
import sys, os, os.path
import re

def unpickle_miller_arrays(file_name):
  result = easy_pickle.load(file_name)
  # Python 3 pickle fix
  # =========================================================================
  if sys.version_info.major == 3:
    result = easy_pickle.fix_py2_pickle(result)
  # =========================================================================
  if (isinstance(result, miller.array)):
    return [result]
  result = list(result)
  for miller_array in result:
    if (not isinstance(miller_array, miller.array)):
      return None
  return result

def _cif_prefilter(file_name):
  f_root, f_ext = os.path.splitext(file_name)
  if f_ext.lower() == '.gz': f_root, f_ext = os.path.splitext(f_root)
  if f_ext.lower() in ['.cif', '.mmcif', '.dic']: return True
  with open(file_name) as f:
    for l in f:
      if l.strip().startswith('#'): continue
      if not l.strip(): continue
      return l.strip().lower().startswith('data_')
  return False

def try_all_readers(file_name):
  try: content = mtz.object(file_name=file_name)
  except RuntimeError: pass
  else: return ("ccp4_mtz", content)
  if (detect_binary_file.from_initial_block(file_name=file_name)):
    try: content = unpickle_miller_arrays(file_name=file_name)
    except KeyboardInterrupt: raise
    except Exception: pass
    else: return ("cctbx.miller.array", content)
  try:
    with open(file_name) as fh:
      content = cns_reflection_reader.cns_reflection_file(fh)
  except cns_reflection_reader.CNS_input_Error: pass
  else: return ("cns_reflection_file", content)
  try: content = cns_index_fobs_sigma_reader.reader(
    file_name=file_name)
  except RuntimeError: pass
  else: return ("cns_index_fobs_sigma", content)
  try:
    with open(file_name) as fh:
      content = scalepack_merge.reader(fh)
  except scalepack_merge.FormatError: pass
  else: return ("scalepack_merge", content)
  try: content = scalepack_no_merge.reader(file_name)
  except KeyboardInterrupt: raise
  except Exception: pass
  else: return ("scalepack_no_merge_original_index", content)
  try:
    with open(file_name) as fh:
      content = dtrek_reflnlist_reader.reflnlist(fh)
  except KeyboardInterrupt: raise
  except Exception: pass
  else: return ("dtrek_reflnlist", content)
  try: content = shelx_hklf.reader(
    file_name=file_name)
  except KeyboardInterrupt: raise
  except Exception: pass
  else: return ("shelx_hklf", content)
  try:
    # The cif parser uses a lot of memory when reading a file with millions
    # of words (like an xds_ascii file). Thus we filter out obvious non-cif
    # files.
    assert _cif_prefilter(file_name)
    content = cif_reader(file_path=file_name)
    looks_like_a_reflection_file = False
    for block in content.model().values():
      if '_refln_index_h' in block or '_refln.index_h' in block:
        looks_like_a_reflection_file = True
        break
    if not looks_like_a_reflection_file:
      raise RuntimeError
  except KeyboardInterrupt: raise
  except Exception: pass
  else: return ("cif", content)
  try:
    with open(file_name) as fh:
      content = xds_ascii_reader(fh)
  except KeyboardInterrupt: raise
  except Exception: pass
  else: return ("xds_ascii", content)
  try:
    content = xds_integrate_hkl_reader()
    content.read_file(file_name)
  except KeyboardInterrupt: raise
  except Exception: pass
  else: return ("xds_integrate_hkl", content)
  try: content = solve_fpfm_reader(file_name=file_name)
  except KeyboardInterrupt: raise
  except Exception: pass
  else: return ("solve_fpfm", content)
  return (None, None)


class any_reflection_file(object):
  """
  Proxy object for reading a reflection file of unspecified format, and
  extracting the Miller arrays contained therein.

  Examples
  --------
  >>> from iotbx.reflection_file_reader import any_reflection_file
  >>> hkl_file = any_reflection_file("data.mtz")
  >>> print hkl_file.file_type()
  'ccp4_mtz'
  >>> print type(hkl_file.file_content())
  <class 'iotbx_mtz_ext.object'>
  >>> miller_arrays = hkl_file.as_miller_arrays()
  """

  def __init__(self, file_name, ensure_read_access=True, strict=True):
    # strict is no longer used
    if (   file_name.startswith("amplitudes=")
        or file_name.startswith("hklf3=")
        or file_name.startswith("intensities=")
        or file_name.startswith("hklf4=")
        or file_name.startswith("hklf+ins/res=") ):
      self._observation_type, self._file_name = file_name.split("=", 1)
    elif (   file_name.endswith("=amplitudes")
          or file_name.endswith("=hklf3")
          or file_name.endswith("=intensities")
          or file_name.endswith("=hklf4")
          or file_name.endswith("=hklf+ins/res")):
      self._file_name, self._observation_type = file_name.split("=", 1)
    else:
      self._file_name = file_name
      self._observation_type = None
    if (self._observation_type == "hklf3"):
      self._observation_type = "amplitudes"
    elif (self._observation_type == "hklf4"):
      self._observation_type = "intensities"
    self._file_type = None
    file_name = self._file_name
    try:
      with open(file_name): # test read access
        pass
    except IOError as e:
      if (ensure_read_access):
        raise Sorry(str(e))
      return
    if (self._observation_type is not None):
      try: self._file_content = shelx_hklf.reader(
        file_name=file_name)
      except KeyboardInterrupt: raise
      except Exception:
        raise Sorry("Not a SHELX reflection file: %s\n"
          "  =%s can only be used for SHELX reflection files."
          % (file_name, self._observation_type))
      else: self._file_type = "shelx_hklf"
    else:
      self._file_type, self._file_content = try_all_readers(
        file_name=file_name)

  def file_name(self):
    """Returns the file name."""
    return self._file_name

  def file_type(self):
    """Return a string specifying the format type (e.g. 'ccp4_mtz')."""
    return self._file_type

  def file_content(self):
    """Return the underlying format-specific object."""
    return self._file_content

  def as_miller_arrays(self,
                       crystal_symmetry=None,
                       force_symmetry=False,
                       merge_equivalents=True,
                       base_array_info=None,
                       assume_shelx_observation_type_is=None,
                       enforce_positive_sigmas=False,
                       anomalous=None,
                       reconstruct_amplitudes=True
     ):
    """
    Convert the contents of the reflection file into a list of
    :py:class:`cctbx.miller.array` objects, each of which may contain multiple
    columns of data from the underlying file.  By default this will
    automatically merge redundant observations to obtain a unique set under
    symmetry.

    :param crystal_symmetry: :py:class:`cctbx.crystal.symmetry` object
      (defaults to using internally specified symmetry, if any)
    :param force_symmetry: TODO
    :param merge_equivalents: merge redundant obervations (default=True)
    :param base_array_info: :py:class:`cctbx.miller.array_info` object
      containing basic information to be propagated to the arrays
    :param assume_shelx_observation_type_is: if specified, instead of raising
      an exception if the SHELX file type is not known from the file name plus
      data type tag, the function will force the specified data type.
    :param reconstruct_amplitudes: ignored by all other readers than mtz reader.
      If set to True mean amplitudes and adjacent anomalous diffference columns will be
      fused into anomalous miller_array object.
      If False, tells the reader not to fuse mean amplitude and adjacent anomalous
      difference columns into anomalous miller_array objects.
    """
    assert (assume_shelx_observation_type_is in
            [None, "amplitudes", "intensities"])
    if (self._file_type is None):
      return []
    info_source = self._file_name
    if (info_source.startswith("./") or info_source.startswith(".\\")):
      info_source = info_source[2:]
    if (base_array_info is None):
      base_array_info = miller.array_info(
        source=info_source,
        source_type=self._file_type)
    if (self._file_type == "cctbx.miller.array"):
      result = []
      for miller_array in self._file_content:
        info = miller_array.info()
        if (info is None or not isinstance(info, miller.array_info)):
          info = base_array_info
        info.source = info_source
        info.crystal_symmetry_from_file = crystal.symmetry(
          unit_cell=miller_array.unit_cell(),
          space_group_info=miller_array.space_group_info(),
          raise_sorry_if_incompatible_unit_cell=True)
        result.append(miller_array.customized_copy(
          crystal_symmetry=miller_array.join_symmetry(
            other_symmetry=crystal_symmetry,
            force=force_symmetry,
            raise_sorry_if_incompatible_unit_cell=True))
              .set_info(info)
              .set_observation_type(miller_array.observation_type()))
      return result
    if ((   crystal_symmetry is None
         or crystal_symmetry.unit_cell() is None)
        and self._observation_type == 'hklf+ins/res'
        ):
        name, ext = os.path.splitext(self._file_name)
        if ext != '.hkl': # it may be compressed: name.hkl.gz
          name, ext = os.path.splitext(name)
        for shelx_file_name in ('%s.ins' % name, '%s.res' % name):
          try:
            shelx_file = open(shelx_file_name)
            break
          except IOError:
            continue
        else:
          raise Sorry("Can't open files %s.ins or %s.res"
                      "required by the option hklf+ins/res" % ((name,)*2))
        crystal_symmetry = crystal_symmetry_from_ins.extract_from(
          file=shelx_file, close_file=False)
        shelx_file.seek(0)
        remaining = shelx_file.read()
        shelx_file.close()
        m = re.search(r"^HKLF\s*(\d)", remaining, re.X|re.M|re.S)
        if m is None:
          raise Sorry("%s does not contain the mandatory HKLF instruction"
                      % shelx_file.name)
        if m.group(1) == "4":
          self._observation_type = "intensities"
        elif m.group(1) == "3":
          self._observation_type = "amplitudes"
        else:
          raise Sorry("HKLF %s not supported" % m.group(1))
    if (self._file_type == "ccp4_mtz"):
      result = self._file_content.as_miller_arrays(
        crystal_symmetry=crystal_symmetry,
        force_symmetry=force_symmetry,
        merge_equivalents=merge_equivalents,
        base_array_info=base_array_info,
        anomalous=anomalous,
        reconstruct_amplitudes=reconstruct_amplitudes
      )
    else:
      result = self._file_content.as_miller_arrays(
        crystal_symmetry=crystal_symmetry,
        force_symmetry=force_symmetry,
        merge_equivalents=merge_equivalents,
        base_array_info=base_array_info,
        anomalous=anomalous,
      )
    if (self.file_type() == "shelx_hklf"):
      if ((self._observation_type == "intensities") or
          (assume_shelx_observation_type_is == "intensities")):
        result[0].set_info(result[0].info().customized_copy(
          labels=["Iobs", "SigIobs"]))
        result[0].set_observation_type_xray_intensity()
      elif ((self._observation_type == "amplitudes") or
            (assume_shelx_observation_type_is == "amplitudes")):
        result[0].set_info(result[0].info().customized_copy(
          labels=["Fobs", "SigFobs"]))
        result[0].set_observation_type_xray_amplitude()
      else:
        raise Sorry("Unresolved amplitude/intensity ambiguity: %s\n"
          "  SHELX reflection files may contain amplitudes or intensities.\n"
          "  Please append   =amplitudes\n"
          "             or   =hklf3\n"
          "             or   =intensities\n"
          "             or   =hklf4\n"
          "  to the file name argument or parameter to resolve the"
          " ambiguity.\n"
          "  If a corresponding .ins file is available, look for the"
          " HKLF codeword.\n"
          "  Alternatively, run the phenix.reflection_statistics"
          " command twice,\n"
          "  once with =amplitudes and once with =intensities. Inspect"
          " the <I^2>/(<I>)^2\n"
          "  statistics. For acentric structures the values should"
          " fluctuate around\n"
          "  2.0, for centric structures around 3.0. If the statistics"
          " are not conclusive\n"
          "  it will be best to recover the original reflection data, such"
          " as SCALEPACK,\n"
          "  SCALA MTZ, XDS, or d*TREK files." % self._file_name)
    # discard reflections where sigma <= 0
    # XXX note that this will happen after data merging, so for unmerged data
    # it is better to specify merge_equivalents=False!
    if (enforce_positive_sigmas):
      result_ = []
      for array in result :
        result_.append(array.enforce_positive_sigmas())
      result = result_
    return result

def collect_arrays(
      file_names,
      crystal_symmetry,
      force_symmetry,
      merge_equivalents=True,
      discard_arrays=False,
      verbose=2,
      report_out=None):
  if (report_out is None):
    report_out = sys.stdout
  if (discard_arrays):
    result = None
  else:
    result = []
  for file_name in file_names:
    if (verbose > 0):
      print(file=report_out)
      print("file_name:", file_name, file=report_out)
      report_out.flush()
    reflection_file = any_reflection_file(file_name)
    if (verbose > 0):
      print("file_type:", reflection_file.file_type(), file=report_out)
      print(file=report_out)
    miller_arrays = reflection_file.as_miller_arrays(
      crystal_symmetry=crystal_symmetry,
      force_symmetry=force_symmetry,
      merge_equivalents=merge_equivalents)
    for miller_array in miller_arrays:
      if (verbose > 0):
        if (verbose > 1):
          miller_array.show_comprehensive_summary(f=report_out)
        else:
          miller_array.show_summary(f=report_out)
        if (verbose > 2):
          miller_array.show_array(f=report_out)
        print(file=report_out)
    if (not discard_arrays):
      result.extend(miller_arrays)
  return result

def run(args):
  command_line = (option_parser(
    usage="iotbx.reflection_file_reader [options] reflection_file ...",
    description="Example: iotbx.reflection_file_reader w1.sca w2.mtz w3.cns")
    .enable_symmetry_comprehensive()
    .option(None, "--weak_symmetry",
      action="store_true",
      default=False,
      help="symmetry on command line is weaker than symmetry found in files")
    .option(None, "--show_data",
      action="store_true",
      default=False,
      help="show Miller indices and data of all arrays")
    .option(None, "--pickle",
      action="store",
      type="string",
      help="write all data to FILE ('--pickle .' copies name of input file)",
      metavar="FILE")
  ).process(args=args)
  if (len(command_line.args) == 0):
    command_line.parser.show_help()
    return
  if (command_line.options.show_data):
    verbose = 3
  else:
    verbose = 2
  all_miller_arrays = collect_arrays(
    file_names=command_line.args,
    crystal_symmetry=command_line.symmetry,
    force_symmetry=not command_line.options.weak_symmetry,
    discard_arrays=command_line.options.pickle is None,
    verbose=verbose,
    report_out=sys.stdout)
  if (all_miller_arrays is not None and len(all_miller_arrays) > 0):
    if (len(all_miller_arrays) == 1):
      all_miller_arrays = all_miller_arrays[0]
    pickle_file_name = command_line.options.pickle
    if (pickle_file_name == "."):
      if (len(command_line.args) > 1):
        raise Sorry(
          "Ambiguous name for pickle file (more than one input file).")
      pickle_file_name = os.path.basename(command_line.args[0])
      if (pickle_file_name.lower().endswith(".pickle")):
        raise Sorry("Input file is already a pickle file.")
    if (not pickle_file_name.lower().endswith(".pickle")):
      pickle_file_name += ".pickle"
    print()
    print("Writing all Miller arrays to file:", pickle_file_name)
    easy_pickle.dump(pickle_file_name, all_miller_arrays)
    print()


 *******************************************************************************


 *******************************************************************************
iotbx/reflection_file_utils.py
"""Tools for extracting information from reflection files
"""
from __future__ import absolute_import, division, print_function
from iotbx import reflection_file_reader
from cctbx import miller
from cctbx.array_family import flex
import libtbx.path
from libtbx.str_utils import show_string
from libtbx.utils import Sorry, null_out
import libtbx.phil
from itertools import count
import math
import sys, os
from functools import cmp_to_key
from six.moves import range
from six.moves import zip

class Sorry_No_array_of_the_required_type(Sorry):
  __orig_module__ = __module__
  __module__ = "exceptions"

class Sorry_Not_a_suitable_array(Sorry):
  __orig_module__ = __module__
  __module__ = "exceptions"

def find_labels(search_labels, info_string):
  for search_label in search_labels:
    if (info_string.find(search_label) < 0):
      return False
  return True

class label_table(object):

  def __init__(self, miller_arrays, err=None):
    self.miller_arrays = miller_arrays
    if (err is None): self.err = sys.stderr
    else: self.err = err
    self.info_strings = []
    self.info_labels = []
    for p_array,miller_array in zip(count(1), miller_arrays):
      info = miller_array.info()
      if (info is not None):
        self.info_strings.append(str(info))
      else:
        self.info_strings.append(str(p_array))
      self.info_labels.append(getattr(info, "labels", None))

  def scores(self, label=None, labels=None):
    assert [label, labels].count(None) == 1
    if (labels is None):
      labels = [label]
    else:
      assert len(labels) > 0
    if(type(labels)==type("")): labels = [labels]
    result = []
    labels_lower = [lbl.lower() for lbl in labels]
    for info_string,info_labels in zip(self.info_strings, self.info_labels):
      if (not find_labels(
               search_labels=labels_lower,
               info_string=info_string.lower())):
        result.append(0)
      elif (not find_labels(
                  search_labels=labels,
                  info_string=info_string)):
        result.append(1)
      else:
        n_exact_matches = 0
        if (info_labels is not None):
          for info_label in info_labels:
            if (info_label in labels):
              n_exact_matches += 1
        result.append(2 + n_exact_matches)
    return result

  def show_possible_choices(self,
        f=None,
        scores=None,
        minimum_score=None,
        parameter_name=None):
    if (f is None): f = self.err
    print("Possible choices:", file=f)
    if (scores is None):
      for info_string in self.info_strings:
        print(" ", info_string, file=f)
    else:
      for info_string,score in zip(self.info_strings, scores):
        if (score >= minimum_score):
          print(" ", info_string, file=f)
    print(file=f)
    if (parameter_name is None): hint = ""
    else: hint = "use %s\nto " % parameter_name
    print("Please %sspecify an unambiguous substring of the target label." % hint, file=f)
    print(file=f)

  def match_data_label(self, label, command_line_switch, f=None):
    if (f is None): f = self.err
    assert label is not None
    scores = self.scores(label=label)
    selected_array = None
    for high_score in range(max(scores),0,-1):
      if (scores.count(high_score) > 0):
        if (scores.count(high_score) > 1):
          print(file=f)
          print("Ambiguous %s=%s" % (command_line_switch, label), file=f)
          print(file=f)
          self.show_possible_choices(
            f=f, scores=scores, minimum_score=high_score)
          return None
        return self.miller_arrays[scores.index(high_score)]
    print(file=f)
    print("Unknown %s=%s" % (command_line_switch, label), file=f)
    print(file=f)
    self.show_possible_choices(f=f)
    return None

  def select_array(self, label, command_line_switch, f=None):
    if (f is None): f = self.err
    if (len(self.miller_arrays) == 0):
      print(file=f)
      print("No reflection arrays available.", file=f)
      print(file=f)
      return None
    if (len(self.miller_arrays) == 1):
      return self.miller_arrays[0]
    if (label is None):
      print(file=f)
      s = command_line_switch
      print("Please use %s to select a reflection array." % s, file=f)
      print("For example: %s=%s" % (s, show_string(str(
        self.miller_arrays[1].info()).split(":")[-1])), file=f)
      print(file=f)
      self.show_possible_choices(f=f)
      return None
    return self.match_data_label(
      label=label,
      command_line_switch=command_line_switch)

def presumably_from_mtz_FQDQY(miller_array):
  lbls = miller_array.info().labels
  return (lbls is not None and len(lbls) == 5)

def get_amplitude_scores(miller_arrays, strict=False):
  result = []
  for miller_array in miller_arrays:
    score = 0
    if (miller_array.is_complex_array()) and (not strict):
      score = 1
    elif (miller_array.is_real_array()):
      if (miller_array.is_xray_reconstructed_amplitude_array()):
        if (presumably_from_mtz_FQDQY(miller_array)):
          score = 3
        else:
          score = 2
      elif (miller_array.is_xray_amplitude_array()):
        score = 5
      elif (miller_array.is_xray_intensity_array()) and (not strict):
        score = 4
    result.append(score)
  return result

def get_phase_scores(miller_arrays):
  result = []
  for miller_array in miller_arrays:
    score = 0
    if (   miller_array.is_complex_array()
        or miller_array.is_hendrickson_lattman_array()):
      score = 4
    elif (miller_array.is_real_array()):
      if (miller_array.is_xray_reconstructed_amplitude_array()):
        pass
      elif (miller_array.is_xray_amplitude_array()):
        pass
      elif (miller_array.is_xray_intensity_array()):
        pass
      elif (miller_array.data().size() == 0):
        pass
      else:
        m = flex.mean(flex.abs(miller_array.data()))
        if (m < 5):
          score = 2
        elif (m < 500):
          score = 3
        else:
          score = 1
    result.append(score)
  return result

def get_xray_data_scores(miller_arrays, ignore_all_zeros,
    prefer_anomalous=None, prefer_amplitudes=None):
  anomalous_bonus = 4
  intensity_bonus = 2
  amplitude_bonus = 0
  if (prefer_amplitudes):
    intensity_bonus = 0
    amplitude_bonus = 2
  result = []
  for miller_array in miller_arrays:
    if (not miller_array.is_real_array()):
      result.append(0)
    else:
      score = None
      if (miller_array.data().all_eq(0)):
        if (ignore_all_zeros):
          score = 0
        else:
          score = 1
      elif (miller_array.is_xray_intensity_array()):
        score = 8 + intensity_bonus
        if (prefer_anomalous is not None):
          if (((prefer_anomalous) and (miller_array.anomalous_flag())) or
              ((not prefer_anomalous) and (not miller_array.anomalous_flag()))):
            score += anomalous_bonus
          else :
            score -= anomalous_bonus
      elif (miller_array.is_xray_amplitude_array()):
        if (miller_array.is_xray_reconstructed_amplitude_array()):
          if (presumably_from_mtz_FQDQY(miller_array)):
            if (prefer_anomalous):
              score = 8 + amplitude_bonus
            else :
              score = 6 + amplitude_bonus
          else:
            score = 4
        else:
          score = 6 + amplitude_bonus
          if (prefer_anomalous is not None):
            if (((prefer_anomalous) and (miller_array.anomalous_flag())) or
                ((not prefer_anomalous) and
                 (not miller_array.anomalous_flag()))):
              score += anomalous_bonus
            else :
              score -= anomalous_bonus
      else:
        score = 2
      assert score is not None
      if (    miller_array.sigmas() is not None
          and isinstance(miller_array.sigmas(), flex.double)
          and miller_array.sigmas_are_sensible()
          and score != 0):
        score += 1
      if (prefer_anomalous is not None) and (miller_array.anomalous_flag()):
        if (prefer_anomalous):
          score += 1
        else :
          score -= 1
      result.append(score)
  return result

def looks_like_r_free_flags_info(array_info):
  if (not isinstance(array_info, miller.array_info)): return False
  if (len(array_info.labels) > 2): return False
  label = array_info.labels[-1].lower()
  for word in ["free", "test", "cross", "status", "flag"]:
    if (label.find(word) >= 0): return True
  return False

class get_r_free_flags_score(object):

  def __init__(self, test_flag_value, n, n_free, miller_array_info):
    if (test_flag_value is not None or n_free < n*0.50):
      self.reversed = False
    else:
      self.reversed = True
      n_free = n - n_free
    self.flag_score = 0
    if (min(1000,n*0.01) < n_free < n*0.35):
      if (   looks_like_r_free_flags_info(miller_array_info)
          or min(2000,n*0.04) < n_free < n*0.20):
        self.flag_score = 3
      else:
        self.flag_score = 2

class get_r_free_flags_scores(object):

  def __init__(self, miller_arrays, test_flag_value):
    self.scores = []
    self.test_flag_values = []
    for miller_array in miller_arrays:
      flag_score = 0
      effective_test_flag_value = None
      data = miller_array.data()
      if (miller_array.is_bool_array()):
        trial_test_flag_value = (
          test_flag_value is None or bool(test_flag_value))
        n_free = data.count(trial_test_flag_value)
        scoring = get_r_free_flags_score(
          test_flag_value=test_flag_value,
          n=data.size(),
          n_free=n_free,
          miller_array_info=miller_array.info())
        if (scoring.flag_score != 0):
          flag_score = scoring.flag_score
          if (scoring.reversed):
            trial_test_flag_value = not trial_test_flag_value
          effective_test_flag_value = trial_test_flag_value
      elif (miller_array.is_integer_array()):
        try: counts = data.counts(max_keys=200)
        except RuntimeError: pass
        else:
          c_keys = list(counts.keys())
          c_values = list(counts.values())
          if (   test_flag_value is None
              or test_flag_value in c_keys):
            if (counts.size() == 2):
              if (test_flag_value is None):
                if (c_values[1] < c_values[0]):
                  i_free = 1
                else:
                  i_free = 0
              elif (test_flag_value == c_keys[0]):
                i_free = 0
              else:
                i_free = 1
              scoring = get_r_free_flags_score(
                test_flag_value=test_flag_value,
                n=data.size(),
                n_free=c_values[i_free],
                miller_array_info=miller_array.info())
              if (scoring.flag_score != 0):
                flag_score = scoring.flag_score
                if (scoring.reversed): i_free = 1-i_free
                effective_test_flag_value = c_keys[i_free]
            elif (counts.size() >= 3):
              c_keys_min = min(c_keys)
              c_keys_max = max(c_keys)
              if (((c_keys_max - c_keys_min) < data.size()) and
                  (c_keys == list(range(c_keys_min, c_keys_max+1)))):
                # XXX 0.55 may be too close a margin - the routine to export
                # R-free flags for CCP4 seems to get this wrong frequently.
                if (min(c_values) > max(c_values)*0.55):
                  if (looks_like_r_free_flags_info(miller_array.info())):
                    flag_score = 3
                  else:
                    flag_score = 2
                elif (looks_like_r_free_flags_info(miller_array.info())):
                  flag_score = 1
                if (flag_score != 0):
                  if (test_flag_value is None):
                    c_keys.sort()
                    # XXX this appears to be a special case (from Axel); I'm
                    # erring on the side of consistency with CNS here
                    if ((c_keys == [-1, 0, 1]) and
                        (counts[0] >= data.size()*0.55)):
                      effective_test_flag_value = 1
                    else :
                      effective_test_flag_value = min(c_keys)
                  else:
                    effective_test_flag_value = test_flag_value
              else : # XXX gross fix to avoid memory leak for corrupted flags
                n_binary = counts.get(0, 0) + counts.get(1, 0)
                if (n_binary >= data.size()*0.95):
                  if (looks_like_r_free_flags_info(miller_array.info())):
                    flag_score = 2
                  else :
                    flag_score = 1
                if (flag_score != 0):
                  if (test_flag_value is None):
                    if (counts[0] >= data.size()*0.55):
                      effective_test_flag_value = 1
                    else :
                      effective_test_flag_value = min(c_keys)
                  else :
                    effective_test_flag_value = test_flag_value
      elif miller_array.is_string_array():
        trial_test_flag_value = "f"
        n_free = data.count(trial_test_flag_value)
        scoring = get_r_free_flags_score(
          test_flag_value=test_flag_value,
          n=data.size(),
          n_free=n_free,
          miller_array_info=miller_array.info())
        if (scoring.flag_score != 0):
          flag_score = scoring.flag_score
          if (scoring.reversed):
            trial_test_flag_value = not trial_test_flag_value
          effective_test_flag_value = trial_test_flag_value
      self.scores.append(flag_score)
      self.test_flag_values.append(effective_test_flag_value)
    assert len(self.scores) == len(miller_arrays)
    assert len(self.test_flag_values) == len(miller_arrays)

def get_experimental_phases_scores(miller_arrays, ignore_all_zeros):
  result = []
  for miller_array in miller_arrays:
    if (miller_array.is_hendrickson_lattman_array()):
      if (not miller_array.data().all_eq((0,0,0,0))):
        result.append(2)
      elif (not ignore_all_zeros):
        result.append(1)
      else:
        result.append(0)
    else:
      result.append(0)
  return result

def sort_arrays_by_score(miller_arrays, array_scores, minimum_score):
  assert len(miller_arrays) == len(array_scores)
  i = 0
  scored_arrays = []
  for array in miller_arrays :
    scored_arrays.append( (array, array_scores[i]) )
    i += 1
  def cmp_fn(x,y):
    return y[1] - x[1]
  scored_arrays.sort(key=cmp_to_key(cmp_fn))
  valid_arrays = []
  for (array, score) in scored_arrays :
    if score >= minimum_score :
      valid_arrays.append(array)
  return valid_arrays

def select_array(
      parameter_name,
      labels,
      miller_arrays,
      data_scores,
      err,
      error_message_no_array,
      error_message_not_a_suitable_array,
      error_message_multiple_equally_suitable,
      raise_no_array=True):
  if (labels is not None): assert parameter_name is not None
  if (len(miller_arrays) == 0):
    raise Sorry_No_array_of_the_required_type(
      "No reflection arrays available.")
  if (data_scores is not None):
    assert max(data_scores) >= 0
  else:
    data_scores = [1]*len(miller_arrays)
  lbl_tab = label_table(miller_arrays=miller_arrays, err=err)
  if (labels is None):
    label_scores = None
  else:
    label_scores = lbl_tab.scores(labels=labels)
  if (label_scores is not None and max(label_scores) == 0):
    error = "No matching array: %s=%s" % (parameter_name, " ".join(labels))
    print("\n" + error + "\n", file=err)
    if (max(data_scores) > 0):
      lbl_tab.show_possible_choices(
        scores=data_scores,
        minimum_score=1,
        parameter_name=parameter_name)
    raise Sorry(error)
  if (max(data_scores) == 0):
    if (label_scores is None):
      if(raise_no_array):
        print("\n" + error_message_no_array + "\n", file=err)
        raise Sorry_No_array_of_the_required_type(error_message_no_array)
      else: return
    error = "%s%s=%s" % (
      error_message_not_a_suitable_array, parameter_name, " ".join(labels))
    print("\n" + error + "\n", file=err)
    raise Sorry_Not_a_suitable_array(error)
  if (label_scores is None):
    combined_scores = data_scores
  else:
    n = max(data_scores) + 1
    combined_scores = []
    for label_score,data_score in zip(label_scores, data_scores):
      combined_scores.append(label_score*n+data_score)
  i = combined_scores.index(max(combined_scores))
  if (combined_scores.count(combined_scores[i]) > 1):
    error = error_message_multiple_equally_suitable
    print("\n" + error + "\n", file=err)
    lbl_tab.show_possible_choices(
      scores=combined_scores,
      minimum_score=max(combined_scores),
      parameter_name=parameter_name)
    raise Sorry(error)
  return i

class reflection_file_server(object):

  def __init__(self,
        crystal_symmetry=None,
        force_symmetry=None,
        reflection_files=None,
        miller_arrays=None,
        err=None):
    self.crystal_symmetry = crystal_symmetry
    self.force_symmetry = force_symmetry
    if (err is None): self.err = sys.stderr
    else: self.err = err
    self.miller_arrays = []
    if (reflection_files is not None):
      for reflection_file in reflection_files:
        self.miller_arrays.extend(reflection_file.as_miller_arrays(
          crystal_symmetry=self.crystal_symmetry,
          force_symmetry=self.force_symmetry))
    if (miller_arrays is not None):
      self.miller_arrays.extend(miller_arrays)
    self.file_name_miller_arrays = {}
    for miller_array in self.miller_arrays:
      self.file_name_miller_arrays.setdefault(
        libtbx.path.canonical_path(
          miller_array.info().source), []).append(miller_array)

  def update_crystal_symmetry(self, crystal_symmetry):
    self.crystal_symmetry = crystal_symmetry
    for i, ma in enumerate(self.miller_arrays):
      info = ma.info()
      self.miller_arrays[i] = ma.customized_copy(
        crystal_symmetry = self.crystal_symmetry)
      self.miller_arrays[i].set_info(info)

  def get_miller_arrays(self, file_name):
    if (file_name is None): return self.miller_arrays
    canonical_file_name = libtbx.path.canonical_path(file_name)
    result = self.file_name_miller_arrays.get(canonical_file_name, None)
    if (result is None and hasattr(os.path, "samefile")):
      for tabulated_file_name in self.file_name_miller_arrays.keys():
        if (os.path.samefile(canonical_file_name, tabulated_file_name)):
          result = self.file_name_miller_arrays[canonical_file_name] \
                 = self.file_name_miller_arrays[tabulated_file_name]
          break
    if (result is None):
      reflection_file = reflection_file_reader.any_reflection_file(
        file_name=file_name)
      if (reflection_file.file_type() is None):
        self.file_name_miller_arrays[canonical_file_name] = None
      else:
        result = self.file_name_miller_arrays[canonical_file_name] \
               = reflection_file.as_miller_arrays(
                   crystal_symmetry=self.crystal_symmetry,
                   force_symmetry=self.force_symmetry)
    if (result is None):
      raise Sorry("No reflection data in file: %s" % file_name)
    return result

  def get_miller_array(self, labels, file_name=None):
    if (file_name is None):
      miller_arrays = self.miller_arrays
    else :
      canonical_file_name = libtbx.path.canonical_path(file_name)
      miller_arrays = self.file_name_miller_arrays[canonical_file_name]
    for array in miller_arrays :
      if (isinstance(labels, str)):
        if (array.info().label_string() == labels):
          return array
      else :
        assert (isinstance(labels, list))
        if (array.info().labels == labels):
          return array
    return None

  def get_amplitudes(self,
        file_name,
        labels,
        convert_to_amplitudes_if_necessary,
        parameter_scope,
        parameter_name,
        return_all_valid_arrays=False,
        minimum_score=1,
        strict=False):
    miller_arrays = self.get_miller_arrays(file_name=file_name)
    data_scores = get_amplitude_scores(miller_arrays=miller_arrays,
      strict=strict)
    if (parameter_scope is not None):
      parameter_name = parameter_scope + "." + parameter_name
    if return_all_valid_arrays :
      return sort_arrays_by_score(miller_arrays, data_scores, minimum_score)
    i = select_array(
      parameter_name=parameter_name,
      labels=labels,
      miller_arrays=miller_arrays,
      data_scores=data_scores,
      err=self.err,
      error_message_no_array
        ="No array of amplitudes found.",
      error_message_not_a_suitable_array
        ="Not a suitable array of amplitudes: ",
      error_message_multiple_equally_suitable
        ="Multiple equally suitable arrays of amplitudes found.")
    result = miller_arrays[i]
    if (convert_to_amplitudes_if_necessary):
      info = result.info()
      if (info is None):
        info_labels = None
      else:
        info_labels = info.labels
      if (result.is_complex_array()):
        result = result.amplitudes()
        if (info_labels is not None):
          result.set_info(info.customized_copy(labels=info_labels[:1]))
        else:
          result.set_info(info=info)
      elif (result.is_xray_intensity_array()):
        result = result.as_amplitude_array()
        if (info_labels is not None):
          result.set_info(info.customized_copy(
            labels=info_labels[:1]+["as_amplitude_array"]))
        else:
          result.set_info(info=info)
    return result

  def get_phases_deg(self,
        file_name,
        labels,
        convert_to_phases_if_necessary,
        original_phase_units,
        parameter_scope,
        parameter_name,
        return_all_valid_arrays=False,
        minimum_score=1):
    assert original_phase_units in [None, "deg", "rad"]
    miller_arrays = self.get_miller_arrays(file_name=file_name)
    data_scores = get_phase_scores(miller_arrays=miller_arrays)
    if (parameter_scope is not None):
      parameter_name = parameter_scope + "." + parameter_name
    if return_all_valid_arrays :
      return sort_arrays_by_score(miller_arrays, data_scores, minimum_score)
    i = select_array(
      parameter_name=parameter_name,
      labels=labels,
      miller_arrays=miller_arrays,
      data_scores=data_scores,
      err=self.err,
      error_message_no_array
        ="No array of phases found.",
      error_message_not_a_suitable_array
        ="Not a suitable array of phases: ",
      error_message_multiple_equally_suitable
        ="Multiple equally suitable arrays of phases found.")
    result = miller_arrays[i]
    info = result.info()
    if (info is None):
      info_labels = None
    else:
      info_labels = info.labels
    if (convert_to_phases_if_necessary):
      if (result.is_complex_array()):
        result = result.phases(deg=True)
        if (info_labels is not None and len(info_labels) == 2):
          result.set_info(info.customized_copy(labels=[info_labels[1]]))
        else:
          result.set_info(info=info)
      elif (result.is_hendrickson_lattman_array()):
        result = result.phase_integrals().phases(deg=True)
        if (info_labels is not None):
          result.set_info(info.customized_copy(
            labels=info_labels+["converted_to_centroid_phases"]))
        else:
          result.set_info(info=info)
    elif (    not result.is_complex_array()
          and original_phase_units == "rad"):
      result = result.customized_copy(data=result.data()*(180/math.pi))
      if (info_labels is not None):
        result.set_info(info.customized_copy(
          labels=info_labels+["converted_to_deg"]))
      else:
        result.set_info(info=info)
    return result

  def get_xray_data(self,
        file_name,
        labels,
        ignore_all_zeros,
        parameter_scope,
        parameter_name="labels",
        return_all_valid_arrays=False,
        minimum_score=1,
        prefer_anomalous=None,
        prefer_amplitudes=None):
    miller_arrays = self.get_miller_arrays(file_name=file_name)
    data_scores = get_xray_data_scores(
      miller_arrays=miller_arrays,
      ignore_all_zeros=ignore_all_zeros,
      prefer_anomalous=prefer_anomalous,
      prefer_amplitudes=prefer_amplitudes)
    if return_all_valid_arrays :
      return sort_arrays_by_score(miller_arrays, data_scores, minimum_score)
    # Recognize phenix.refine file and do the "right thing". May be too ad hoc..
    # XXX can we just check the first label instead?
    new_miller_arrays = miller_arrays
    new_data_scores = data_scores
    if(labels is None):
      new_miller_arrays = []
      new_data_scores = []
      # This allows still use "filtered" if no other option is available
      for j in range(len(miller_arrays)):
        ma = miller_arrays[j]
        ds = data_scores[j]
        if((ma.info().labels in [
           ['F-obs-filtered', 'SIGF-obs-filtered'],
           ['F-obs-filtered(+)', 'SIGF-obs-filtered(+)', 'F-obs-filtered(-)',
            'SIGF-obs-filtered(-)',],
            ]) ):
          data_scores[j] = ds-1
      #
      for ma, ds in zip(miller_arrays, data_scores):
        if(not ((ma.info().labels in [
           ['F-model', 'PHIF-model'],
           ['F-model(+)', 'PHIF-model(+)', 'F-model(-)', 'PHIF-model(-)'] ]) or
           isinstance(ma.data(), flex.complex_double))):
          new_miller_arrays.append(ma)
          new_data_scores.append(ds)
    #
    parameter_name = parameter_name.strip()
    if(len(parameter_name)==0):
      parameter_name_ = parameter_scope
    else:
      parameter_name_ = parameter_scope+"."+parameter_name
    i = select_array(
      parameter_name=parameter_name_,
      labels=labels,
      miller_arrays=new_miller_arrays,
      data_scores=new_data_scores,
      err=self.err,
      error_message_no_array
        ="No array of observed xray data found.",
      error_message_not_a_suitable_array
        ="Not a suitable array of observed xray data: ",
      error_message_multiple_equally_suitable
        ="Multiple equally suitable arrays of observed xray data found.")
    return new_miller_arrays[i]

  def get_r_free_flags(self,
        file_name,
        label,
        test_flag_value,
        disable_suitability_test,
        parameter_scope,
        return_all_valid_arrays=False,
        minimum_score=1):
    miller_arrays = self.get_miller_arrays(file_name=file_name)
    if (disable_suitability_test):
      if (label is None or test_flag_value is None):
        raise Sorry((
          "%s=True: Suitability test for R-free flags can only be disabled"
          " if both %s and %s are defined.") % (
            parameter_scope+".disable_suitability_test",
            parameter_scope+".label",
            parameter_scope+".test_flag_value"))
      elif return_all_valid_arrays :
        raise Sorry("return_all_valid_arrays=True: Suitability test can not "+
          "be disabled in this mode.")
      flag_scores = None
      data_scores = None
    else:
      flag_scores = get_r_free_flags_scores(
        miller_arrays=miller_arrays,
        test_flag_value=test_flag_value)
      data_scores = flag_scores.scores
    if return_all_valid_arrays : # used in PHENIX GUI
      test_flag_values = flag_scores.test_flag_values
      scored_arrays = []
      for i, array in enumerate(miller_arrays):
        scored_arrays.append( (array, test_flag_values[i], data_scores[i]) )
      def cmp_fn(x,y):
        return y[2] - x[2]
      scored_arrays.sort(key=cmp_to_key(cmp_fn))
      valid_arrays_and_flags = []
      for (array, flag_value, score) in scored_arrays :
        if score >= minimum_score :
          if array.is_string_array():
            array, flag_value = cif_status_flags_as_int_r_free_flags(
              array, flag_value)
          valid_arrays_and_flags.append((array, flag_value))
      return valid_arrays_and_flags
    if (label is None): labels = None
    else: labels=[label]
    try:
      i = select_array(
        parameter_name=parameter_scope,
        labels=labels,
        miller_arrays=miller_arrays,
        data_scores=data_scores,
        err=self.err,
        error_message_no_array
          ="No array of R-free flags found.\n\n"
          +"For manual selection define:\n"
          +"  %s.test_flag_value\n"%parameter_scope
          +"  %s.disable_suitability_test=True"%parameter_scope,
        error_message_not_a_suitable_array
          ="Not a suitable array of R-free flags: ",
        error_message_multiple_equally_suitable
          ="Multiple equally suitable arrays of R-free flags found.")
    except Sorry_Not_a_suitable_array as e:
      raise Sorry_Not_a_suitable_array(
        str(e) + "\nTo override the suitability test define:"
               + " %s.disable_suitability_test=True" % parameter_scope)
    miller_array = miller_arrays[i]
    if data_scores is not None:
      test_flag_value = flag_scores.test_flag_values[i]
    if miller_array.is_string_array():
      miller_array, test_flag_value = cif_status_flags_as_int_r_free_flags(
        miller_array, test_flag_value)
    return miller_array, test_flag_value

  def get_experimental_phases(self,
        file_name,
        labels,
        ignore_all_zeros,
        parameter_scope,
        raise_no_array=True,
        parameter_name="labels",
        return_all_valid_arrays=False,
        minimum_score=1):
    miller_arrays = self.get_miller_arrays(file_name=file_name)
    data_scores = get_experimental_phases_scores(
      miller_arrays=miller_arrays,
      ignore_all_zeros=ignore_all_zeros)
    if return_all_valid_arrays : # used in PHENIX GUI
      return sort_arrays_by_score(miller_arrays, data_scores, minimum_score)
    i = select_array(
      parameter_name=parameter_scope+"."+parameter_name,
      labels=labels,
      miller_arrays=miller_arrays,
      data_scores=data_scores,
      err=self.err,
      raise_no_array=raise_no_array,
      error_message_no_array
        ="No array of experimental phases found.",
      error_message_not_a_suitable_array
        ="Not a suitable array of experimental phases: ",
      error_message_multiple_equally_suitable
        ="Multiple equally suitable arrays of experimental phases found.")
    if i is None: return None
    return miller_arrays[i]

def cif_status_flags_as_int_r_free_flags(miller_array, test_flag_value):
  assert test_flag_value == 'f'
  if miller_array.is_string_array():
    selection = (miller_array.data() == 'o') | (miller_array.data() == 'f')
    info = miller_array.info()
    miller_array = miller_array.select(selection)
    data = flex.int(miller_array.size(), 1)
    data.set_selected(miller_array.data() == 'f', 0)
    miller_array = miller_array.array(data=data)
    miller_array.set_info(info)
    test_flag_value = 0
  return miller_array, test_flag_value

def guess_r_free_flag_value(miller_array, test_flag_value=None):
  flag_scores = get_r_free_flags_scores(
    miller_arrays=[miller_array],
    test_flag_value=test_flag_value)
  return flag_scores.test_flag_values[0]

def construct_output_file_name(input_file_names,
                               user_file_name,
                               file_type_label,
                               file_extension,
                               extension_seperator="."):
  if (user_file_name == "."):
    if (len(input_file_names) > 1):
      raise Sorry(
        "Ambiguous name for output %s file (more than one input file)."
          % file_type_label)
    user_file_name = os.path.basename(input_file_names[0])
  if (not user_file_name.lower().endswith(file_extension)):
    user_file_name += extension_seperator + file_extension
  if sys.platform == "win32":
    if (os.path.isfile(user_file_name)
        and user_file_name == input_file_names[0]):
      user_file_name += extension_seperator + file_extension
  else:
    if (os.path.isfile(user_file_name)
        and os.path.samefile(user_file_name, input_file_names[0])):
      user_file_name += extension_seperator + file_extension
  return user_file_name

def make_joined_set(miller_arrays):
  if(len(miller_arrays)==0): return None
  cs0 = miller_arrays[0].crystal_symmetry()
  for ma in miller_arrays:
    if([ma.crystal_symmetry().unit_cell(), cs0.unit_cell()].count(None)>0):
      return None
    if(not ma.crystal_symmetry().is_similar_symmetry(cs0)): return None
  from cctbx import miller
  master_set = miller.set(
    crystal_symmetry=miller_arrays[0].crystal_symmetry(),
    indices=miller_arrays[0].indices(),
    anomalous_flag=False)
  master_indices = miller_arrays[0].indices().deep_copy()
  for array in miller_arrays[1:] :
    current_indices = array.indices()
    missing_isel = miller.match_indices(master_indices,
      current_indices).singles(1)
    missing_indices = current_indices.select(missing_isel)
    master_indices.extend(missing_indices)
  master_set = miller.set(
    crystal_symmetry=miller_arrays[0].crystal_symmetry(),
    indices=master_indices,
    anomalous_flag=False)
  return \
    master_set.map_to_asu().unique_under_symmetry().remove_systematic_absences()

def extract_miller_array_from_file(file_name, label=None, type=None, log=None):
  if(log is None): log = sys.stdout
  assert type in ["complex", "real", None]
  result = None
  miller_arrays = reflection_file_reader.any_reflection_file(file_name =
    file_name).as_miller_arrays()
  def get_flag(ma):
    return (type == "complex" and ma.is_complex_array()) or \
           (type == "real"    and ma.is_real_array()) or \
           type is None
  print("  Available suitable arrays:", file=log)
  suitable_arrays = []
  suitable_labels = []
  for ma in miller_arrays:
    if(get_flag(ma=ma)):
      print("    ", ma.info().label_string(), file=log)
      suitable_arrays.append(ma)
      suitable_labels.append(ma.info().label_string())
  print(file=log)
  if(  len(suitable_arrays) == 0): raise Sorry("No suitable arrays.")
  elif(len(suitable_arrays) == 1): result = suitable_arrays[0]
  elif(len(suitable_arrays) >  1):
    if(label is None):
      msg='''Multiple choices available. No map coefficients array selected.

  See choices listed above and use "label=" to select one.
  Example: label="2FOFCWT,PH2FOFCWT"'''
      raise Sorry(msg)
    else:
      for ma in miller_arrays:
        if(get_flag(ma=ma) and (ma.info().label_string() == label)):
          print("  Selected:", ma.info().label_string(), file=log)
          result = ma
  return result

class process_raw_data(object):
  """
  Automation wrapper - prepares single-wavelength experimental data (and
  optional R-free flags and experimental phases) for any future step in the
  structure determination process.  Used in Phenix for ligand pipeline and
  automated re-refinement of PDB entries.
  """
  __slots__ = [
    "f_obs",
    "r_free_flags",
    "test_flag_value",
    "phases",
    "_generate_new",
  ]
  def __init__(self,
      obs,
      r_free_flags,
      test_flag_value,
      phases=None,
      d_min=None,
      d_max=None,
      r_free_flags_params=None,
      merge_anomalous=False,
      log=sys.stdout,
      verbose=True):
    assert (log is not None) and (obs is not None)
    if (r_free_flags_params is None):
      from cctbx.r_free_utils import generate_r_free_params_str
      r_free_flags_params = libtbx.phil.parse(
        generate_r_free_params_str).extract()
    obs_info = obs.info()
    r_free_flags_info = phases_info = None
    sg = obs.space_group_info()
    obs = obs.map_to_asu().merge_equivalents().array()
    obs = obs.eliminate_sys_absent(log=log)
    obs = obs.resolution_filter(d_min=d_min, d_max=d_max)
    if (obs.is_xray_intensity_array()):
      from cctbx import french_wilson
      if (verbose):
        fw_out = log
      else :
        fw_out = null_out()
      obs = french_wilson.french_wilson_scale(
        miller_array=obs,
        params=None,
        log=fw_out)
    assert (obs is not None)
    merged_obs = obs.average_bijvoet_mates()
    if (merged_obs.completeness() < 0.9):
      print("""
  WARNING: data are incomplete (%.1f%% of possible reflections measured to
  %.2fA).  This may cause problems if you plan to use the maps for building
  and/or ligand fitting!
    """ % (100*merged_obs.completeness(), merged_obs.d_min()), file=log)
    # XXX this is kind of a hack (the reconstructed arrays break some of my
    # assumptions about labels)
    if (merge_anomalous):
      obs = obs.average_bijvoet_mates()
    if (r_free_flags is not None):
      r_free_flags_info = r_free_flags.info()
      format = "cns"
      if (test_flag_value == 0):
        format = "ccp4"
      elif (test_flag_value == -1):
        format = "shelx"
      if (r_free_flags.anomalous_flag()):
        r_free_flags = r_free_flags.average_bijvoet_mates()
      is_compatible_symmetry = False
      obs_pg = obs.space_group().build_derived_point_group()
      flags_pg = r_free_flags.space_group().build_derived_point_group()
      if (obs_pg.type().number() == flags_pg.type().number()):
        is_compatible_symmetry = True
      else :
        pass # TODO unit cell comparison?
      if (is_compatible_symmetry):
        r_free_flags = r_free_flags.map_to_asu().merge_equivalents().array()
        r_free_flags = r_free_flags.eliminate_sys_absent(log=log)
        if (format == "cns"):
          r_free_flags = r_free_flags.customized_copy(
            crystal_symmetry=obs.crystal_symmetry(),
            data=(r_free_flags.data() == test_flag_value))
          test_flag_value = True
        obs_tmp = obs.deep_copy()
        if (obs.anomalous_flag()):
          obs_tmp = obs.average_bijvoet_mates()
        r_free_flags = r_free_flags.common_set(other=obs_tmp)
        n_r_free = r_free_flags.indices().size()
        n_obs = obs_tmp.indices().size()
        if ((test_flag_value is None) or
            (r_free_flags.data().all_eq(r_free_flags.data()[0]))):
          print("""
  WARNING: uniform R-free flags detected; a new test set will be generated,
  but this will bias the refinement statistics.
""", file=log)
          r_free_flags = None
        elif (n_r_free != n_obs):
          missing_set = obs_tmp.lone_set(other=r_free_flags)
          n_missing = missing_set.indices().size()
          if (n_missing > 0):
            print("""
  WARNING: R-free flags are incomplete relative to experimental
  data (%d vs. %d reflections).  The flags will be extended to
  complete the set, but we recommend supplying flags that are already
  generated to the maximum expected resolution.
""" % (n_r_free, n_obs), file=log)
            if (n_missing < 20) : # FIXME
              if (format == "cns"):
                missing_flags = missing_set.array(data=flex.bool(n_missing,
                  False))
              else :
                missing_flags = missing_set.array(data=flex.int(n_missing, 1))
            else :
              missing_flags = missing_set.generate_r_free_flags(
                fraction=(r_free_flags.data().count(test_flag_value)/n_r_free),
                max_free=None,
                use_lattice_symmetry=True,
                format=format)
            r_free_flags = r_free_flags.concatenate(other=missing_flags)
        if (r_free_flags is not None):
          assert (r_free_flags.indices().size() == obs_tmp.indices().size())
      else :
        print("""
    NOTE: incompatible symmetry between the data and the R-free flags:
         Data  : %s  %s
         Flags : %s  %s
       A new test set will be generated.
""" % (str(obs.space_group_info()),
          " ".join([ "%g" % x for x in obs.unit_cell().parameters() ]),
          str(r_free_flags.space_group_info()),
          " ".join(["%g" % x for x in r_free_flags.unit_cell().parameters()])), file=log)
    else :
      print("""
 WARNING: R-free flags not supplied.  This may bias the refinement if the
     structures are very nearly isomorphous!
""", file=log)
    self._generate_new = False
    if (r_free_flags is None):
      r_free_flags = obs.generate_r_free_flags(
        fraction=r_free_flags_params.fraction,
        max_free=r_free_flags_params.max_free,
        use_lattice_symmetry=r_free_flags_params.use_lattice_symmetry,
        use_dataman_shells=r_free_flags_params.use_dataman_shells,
        n_shells=r_free_flags_params.n_shells,
        format="ccp4")
      test_flag_value = 0
      self._generate_new = True
    if (r_free_flags.anomalous_flag()):
      r_free_flags = r_free_flags.average_bijvoet_mates()
    if (phases is not None):
      phases_info = phases.info()
      phases = phases.map_to_asu().resolution_filter(d_min=d_min, d_max=d_max)
    assert (obs.is_xray_amplitude_array())
    self.f_obs = obs.set_info(obs_info)
    self.r_free_flags = r_free_flags.set_info(r_free_flags_info)
    self.test_flag_value = test_flag_value
    self.phases = None
    if (phases is not None):
      self.phases = phases.set_info(phases_info)

  def data_labels(self):
    if (self.f_obs.is_xray_reconstructed_amplitude_array()):
      return "F,SIGF,DANO,SIGDANO,ISYM"
    elif (self.f_obs.anomalous_flag()):
      if (self.f_obs.sigmas() is not None):
        return "F(+),SIGF(+),F(-),SIGF(-)"
      else :
        return "F(+),F(-)"
    elif (self.f_obs.sigmas() is not None):
      return "F,SIGF"
    else :
      return "F"

  def r_free_flags_label(self):
    return "FreeR_flag"

  def r_free_flags_as_boolean_array(self):
    flags = self.r_free_flags.customized_copy(
      data=self.r_free_flags.data()==self.test_flag_value)
    if (self.f_obs.anomalous_flag()) and (not flags.anomalous_flag()):
      flags = flags.generate_bijvoet_mates()
    return flags

  def data_and_flags(self):
    return self.f_obs.common_sets(other=self.r_free_flags_as_boolean_array())

  def phase_labels(self):
    if (self.phases is not None):
      return "HLA,HLB,HLC,HLD"
    return None

  def n_obs(self):
    return self.f_obs.data().size()

  def fraction_free(self):
    return (self.r_free_flags.data().count(self.test_flag_value) /
            self.r_free_flags.data().size())

  def flags_are_new(self):
    return self._generate_new

  def write_mtz_file(self, file_name,
      title=None,
      wavelength=None,
      single_dataset=True):
    mtz_data = self.f_obs.as_mtz_dataset(
      column_root_label="F",
      wavelength=wavelength)
    if (self.f_obs.anomalous_flag()) and (not single_dataset):
      mtz_data.add_miller_array(
        miller_array=self.f_obs.average_bijvoet_mates(),
        column_root_label="F")
    if (self.phases is not None):
      mtz_data.add_miller_array(self.phases,
        column_root_label="HL")
    mtz_data.add_miller_array(self.r_free_flags,
      column_root_label="FreeR_flag")
    mtz_data.mtz_object().write(file_name)

def change_space_group(file_name, space_group_info):
  """
  Update the space group in an MTZ file, writing it in place.
  """
  mtz_in = reflection_file_reader.any_reflection_file(file_name)
  assert (mtz_in.file_type() == "ccp4_mtz")
  mtz_object = mtz_in.file_content()
  mtz_new = mtz_object.set_space_group_info(space_group_info)
  mtz_new.write(file_name)

def load_f_obs_and_r_free(file_name, anomalous_flag=False, phases=False):
  """
  Automation wrapper for reading in MTZ files generated by the process_raw_data
  class.
  """
  mtz_in = reflection_file_reader.any_reflection_file(file_name)
  assert (mtz_in.file_type() == "ccp4_mtz")
  file_server = reflection_file_server(
    crystal_symmetry=None,
    force_symmetry=True,
    reflection_files=[mtz_in],
    err=sys.stderr)
  f_obs = f_obs_anom = r_free = None
  r_free, test_flag_value = file_server.get_r_free_flags(
     file_name=file_name,
     label="FreeR_flag",
     test_flag_value=None,
     disable_suitability_test=False,
     parameter_scope="")
  r_free_info = r_free.info()
  assert (test_flag_value is not None)
  r_free = r_free.customized_copy(data=r_free.data()==test_flag_value)
  f_obs_info = f_obs_anom_info = None
  for array in file_server.miller_arrays :
    label = array.info().label_string()
    if (array.is_xray_amplitude_array()) and (array.anomalous_flag()):
      f_obs_anom = array
      f_obs_anom_info = f_obs_anom.info()
    elif (label == "F,SIGF"):
      f_obs = array
      f_obs_info = array.info()
  if (f_obs is None) and (f_obs_anom is not None):
    f_obs = f_obs_anom.average_bijvoet_mates()
  f_obs = f_obs.eliminate_sys_absent()
  # XXX this may still be necessary
  f_obs = f_obs.common_set(other=r_free)
  r_free = r_free.common_set(other=f_obs)
  assert (not None in [f_obs, r_free])
  if (f_obs_anom is not None) and (anomalous_flag):
    f_obs_anom = f_obs_anom.eliminate_sys_absent()
    r_free = r_free.generate_bijvoet_mates()
    f_obs = f_obs_anom.common_set(other=r_free).set_info(f_obs_anom_info)
    r_free = r_free.common_set(other=f_obs).set_info(r_free_info)
    return f_obs, r_free
  return f_obs.set_info(f_obs_info), r_free.set_info(r_free_info)


 *******************************************************************************


 *******************************************************************************
iotbx/run_tests.py
"""
List of tests to run in regression tests
"""

from __future__ import absolute_import, division, print_function
from libtbx import test_utils
import sys
import libtbx.load_env

tst_list_base = [
  "$D/regression/tst_mrc_io.py",
  "$D/regression/tst_wildcard.py",
  "$D/gui_tools/tst.py",
  "$D/regression/tst_simple_parser.py",
  "$D/regression/tst_phil.py",
  "$D/regression/tst_pdb_cif_inputs.py",
  "$D/regression/tst_pdb_cif_cells.py",
  "$D/regression/tst_data_manager.py",
  "$D/regression/tst_map_manager_wrapping.py",
  "$D/regression/tst_map_manager.py",
  "$D/regression/tst_map_manager_2.py",
  "$D/regression/tst_map_model_manager.py",
  "$D/regression/tst_map_model_manager_2.py",
  "$D/regression/tst_map_model_manager_3.py",
  "$D/regression/tst_map_model_manager_4.py",
  "$D/regression/tst_map_model_manager_model_sharpening_5.py",
  "$D/regression/tst_map_model_manager_model_sharpening_5_cif.py",
  "$D/regression/tst_map_model_manager_call_consistency.py",
  "$D/regression/tst_map_model_manager_external_sharpening_7.py",
  "$D/regression/tst_map_model_manager_half_map_sharpening_6.py",
  "$D/regression/tst_map_model_manager_tls_from_map_8.py",
  "$D/regression/tst_map_model_manager_cif.py",
  "$D/regression/tst_map_model_manager_9_remove_origin_shift_and_unit_cell_crystal_symmetry.py",
  "$D/regression/tst_map_model_manager_9_remove_origin_shift_and_unit_cell_crystal_symmetry_cif.py",
  "$D/regression/tst_map_model_manager_local_resolution_10.py",
  "$D/regression/tst_map_tools.py",
  "$D/regression/tst_patterson.py",
  "$D/regression/tst_restraints_merge.py",
  "$D/regression/tst_atom_selections_10k.py",
  "$D/ranges.py",
  "$D/regression/tst_crystal_symmetry_from_any.py",
  "$D/regression/tst_poscar.py",
  "$D/kriber/tst_strudat.py",
  "$D/cif/tests/tst_geometry.py",
  "$D/cif/tests/tst_crystal_symmetry_builder.py",
  "$D/cif/tests/tst_lex_parse_build.py",
  "$D/cif/tests/tst_model.py",
  "$D/cif/tests/tst_restraints.py",
  "$D/cif/tests/tst_validation.py",
  "$D/cif/tests/tst_ucif_examples_compilation.py",
  "$D/cif/tests/tst_parser.py",
  "$D/cif/tests/tst_citations.py",
  "$D/cif/tests/tst_model_builder.py",
  "$D/shelx/tst_lex_parse_build.py",
  "$D/shelx/tst_hklf.py",
  "$D/shelx/tst_writer.py",
  "$D/shelx/tst_fvar_encoding.py",
  "$D/pdb/tst_pdb.py",
  "$D/pdb/tst_mmcif.py",
  "$D/pdb/tst_mmcif_hierarchy.py",
  "$D/pdb/tst_mmcif_hierarchy_2.py",
  "$D/pdb/tst_tls.py",
  ["$D/pdb/hybrid_36.py", "exercise"],
  "$B/pdb/hybrid_36_fem",
  "$D/pdb/tst_hierarchy.py",
  "$D/pdb/tst_hierarchy_atom_sort.py",
  "$D/pdb/tst_hierarchy_flip_symmetric.py",
  "$D/regression/tst_selected_hierarchy_flip.py",
  "$D/pdb/tst_ext.py",
  "$D/pdb/tst_atom_selection.py",
  "$D/pdb/tst_rna_dna_atom_names.py",
  "$D/pdb/tst_atom_name_interpretation.py",
  "$D/pdb/tst_extract_rfactors_resolutions_sigma.py",
  "$D/pdb/modified_aa_names.py",
  "$D/pdb/modified_rna_dna_names.py",
  "$D/regression/secondary_structure/tst_sheet.py",
  "$D/regression/secondary_structure/tst_annotation.py",
  "$D/regression/secondary_structure/tst_annotation_long.py",
  "$D/pdb/secondary_structure.py",
  "$D/pdb/tst_atom_selection_string.py",
  "$D/pdb/tst_secondary_structure.py",
  "$D/pdb/tst_utils.py",
  "$D/pdb/tst_secondary_structure_2.py",
  "$D/pdb/remediation/tst_remediator.py",
  "$D/examples/pdb_to_map_simple.py",
  "$D/examples/pdb_truncate_to_ala/tst.py",
  "$D/examples/pdb_tardy_conf_sampling_simple.py",
  "$D/regression/tst_examples.py",
  "$D/cns/space_group_symbols.py",
  "$D/cns/tst_cns.py",
  ["$D/scalepack/tst_merge.py", "P31"],
  "$D/scalepack/no_merge_original_index.py",
  "$D/ccp4_map/tst.py",
  "$D/mtz/tst_ext.py",
  "$D/mtz/tst_extract_from_symmetry_lib.py",
  "$D/mtz/tst_dano.py",
  "$D/mtz/tst_miller_dict.py",
  "$D/mtz/tst_unmerged.py",
  ["$D/mtz/tst.py", "P31"],
  "$D/examples/tst_mtz_free_flipper.py",
  "$D/regression/tst_reflection_file_utils.py",
  "$D/detectors/tst_adsc.py",
  "$D/detectors/tst_detectors.py",
  "$D/xplor/tst_xplormap.py",
  ["$D/regression/tst_phases.py", "P31"],
  "$D/regression/tst_pdbx_mmcif_tutorial.py",
  "$D/regression/tst_lattice_symmetry.py",
  ["$D/regression/tst_reflection_statistics.py", "Fdd2 P31m"],
  "$D/regression/tst_data_plots.py",
  "$D/regression/tst_csv_utils.py",
  "$D/regression/tst_file_reader.py",
  "$D/regression/tst_bioinformatics.py",
  "$D/regression/tst_box_around_molecule.py",
  "$D/regression/tst_mmcif_segids.py",
  "$D/regression/tst_mmcif_input.py",
  "$D/regression/tst_hierarchy_forward_compatible_pdb.py",
  "$D/regression/tst_mmcif_multimodel.py",
  "$D/regression/tst_add_conformations.py",
  "$D/regression/tst_symmetry.py",
  "$D/regression/tst_reindex.py",
  "$D/regression/tst_reflection_file_editor.py",
  "$D/regression/tst_split_models.py",
  "$D/regression/tst_pdb_as_fasta.py",
  "$D/regression/tst_pdb_link_records.py",
  "$D/regression/tst_merging_statistics.py",
  "$D/regression/tst_simple_map_coefficients.py",
  "$D/regression/tst_sort_atoms.py",
  "$D/xds/tests/tst_xparm.py",
  "$D/xds/tests/tst_xds_inp.py",
  "$D/xds/tests/tst_integrate_hkl.py",
  "$D/xds/tests/tst_spots_xds.py",
  "$D/regression/tst_pdb_as_cif.py",
  "$D/scalepack/tst_no_merge_original_index.py",
  "$D/regression/tst_export_scalepack_unmerged.py",
  ["$D/dsn6/tst.py", "P31"],
  "$D/regression/ncs/tst_mtrix_biomt_cmdl.py",
  "$D/regression/ncs/tst_mmcif_biomt_reduction_output.py",
  "$D/regression/ncs/tst_ncs_search_ligs.py",
  "$D/regression/ncs/tst_ncs_search_broken_chain.py",
  "$D/regression/ncs/tst_ncs_search_shortcut_1.py",
  "$D/regression/ncs/tst_ncs_groups_preprocessing.py",
  "$D/regression/ncs/tst_ncs_input.py",
  "$D/regression/ncs/tst_ncs_user_selections.py",
  "$D/regression/ncs/tst_ncs.py",
  "$D/regression/ncs/tst_ncs_without_validation.py",
  "$D/pdb/tst_read_mtrix_records_from_cif.py",
  "$D/regression/tst_show_systematic_absences.py",
  "$D/regression/tst_miller_sort_asu.py",
  "$D/regression/tst_reflection_file_reader.py",
  "$D/regression/tst_xray_scale.py",
  "$D/bioinformatics/test/tst_alignment_as_hsearch.py",
  "$D/bioinformatics/test/tst_ebi_wu_blast_xml.py",
  "$D/bioinformatics/test/tst_ncbi_blast_xml.py",
  "$D/bioinformatics/pdb_info.py",
  "$D/regression/tst_cif_as_pdb_1atom.py",
  "$D/regression/tst_cif_1.py",
  "$D/regression/tst_split_data_cif.py",
  "$D/regression/tst_all_chain_ids.py",
  "$D/regression/tst_extract_xtal_data.py",
  "$D/regression/tst_cli_parser.py",
  "$D/regression/tst_mtz_as_cif.py",
  "$D/regression/tst_group_rounding.py",
  "$D/regression/tst_hierarchy_occupancies_rounding.py",
  "$D/regression/tst_hierarchy_merge_atoms_at_end_to_residues.py",
  "$D/regression/tst_hierarchy_long_chain_ids_1.py",
  "$D/regression/tst_hierarchy_long_resname_1.py",
  "$D/regression/tst_hierarchy_long_resname_2.py",
  "$D/regression/tst_hierarchy_long_resname_3.py",
  "$D/regression/tst_hierarchy_long_resname_4.py",
  "$D/regression/tst_hierarchy_copy_select.py",
  "$D/regression/tst_hierarchy_id_str.py",
  "$D/regression/tst_hierarchy_altlocs.py",
  "$D/regression/tst_fetch.py",
  ]

# failing tests on Windows, Python 2.7
tst_list_windows_fail = [
  "$D/detectors/tst_debug_write.py",
]

tst_list_fail = [
  "$D/regression/ncs/tst_ncs_reordered_chains.py",
  "$D/regression/tst_mmcif_to_from_hierarchy.py",
  ]
if sys.platform == 'win32':
  tst_list_fail += tst_list_windows_fail
else:
  tst_list_base += tst_list_windows_fail

tst_list_py3_unstable = []
tst_list_unstable = list()
if sys.version_info > (3, 0):
  tst_list_unstable += tst_list_py3_unstable
else:
  tst_list_base += tst_list_py3_unstable

# final lists
tst_list = tst_list_base
tst_list_expected_failures = tst_list_fail
tst_list_expected_unstable = tst_list_unstable

def run():
  build_dir = libtbx.env.under_build("iotbx")
  dist_dir = libtbx.env.dist_path("iotbx")

  test_utils.run_tests(build_dir, dist_dir, tst_list)

if (__name__ == "__main__"):
  run()


 *******************************************************************************


 *******************************************************************************
iotbx/simple_parser.py
"""Tools for parsing
"""
from __future__ import absolute_import, division, print_function
class operator_priority_evaluator(object):

  def __init__(self, operator_dict):
    self.operator_dict = operator_dict

  def __call__(self, word):
    if (word.quote_token is not None): return 0
    return self.operator_dict.get(word.value.lower(), 0)

def infix_as_postfix(
      word_iterator,
      operator_dict={"not": 3, "and": 2, "or": 1},
      stop_if_parse_stack_is_empty=False,
      stop_word=None,
      expect_nonmatching_closing_parenthesis=False):
  """http://www.programmersheaven.com/2/Art_Expressions_p1"""
  operator_priority = operator_priority_evaluator(operator_dict=operator_dict)
  parse_stack = []
  while True:
    word = word_iterator.try_pop()
    if (word is None): break
    if (stop_word is not None and word.value == stop_word):
      break
    if (word.value == "("):
      parse_stack.append(word)
    elif (word.value == ")"):
      while True:
        if (len(parse_stack) == 0):
          if (expect_nonmatching_closing_parenthesis): return
          raise RuntimeError(
            "Closing parenthesis without a matching opening parenthesis.")
        item = parse_stack.pop()
        if (item.value == "("): break
        yield item, word_iterator
      if (len(parse_stack) == 0 and stop_if_parse_stack_is_empty): return
    else:
      word_priority = operator_priority(word)
      if (word_priority == 0):
        yield word, word_iterator
        if (len(parse_stack) == 0 and stop_if_parse_stack_is_empty): return
      else:
        while True:
          if (len(parse_stack) == 0
              or parse_stack[-1].value == "("
              or word_priority > operator_priority(parse_stack[-1])):
            parse_stack.append(word)
            break
          else:
            yield parse_stack.pop(), None
  assert not expect_nonmatching_closing_parenthesis
  while (len(parse_stack) > 0):
    yield parse_stack.pop(), None


 *******************************************************************************


 *******************************************************************************
iotbx/symmetry.py
"""Tools for keeping track of symmetry information from multiple files"""
from __future__ import absolute_import, division, print_function
from libtbx import group_args
from libtbx.utils import Sorry
import os
import sys

class manager(object):
  """
  Class for keeping track of symmetry information from multiple files.  This
  is particularly problematic in the phenix.refine GUI, where users may supply
  any number of PDB files as input, plus a PDB file representing a reference
  structure, and up to five reflection files.  Automatically taking the latest
  symmetry provided, without taking into account the symmetry information in
  other files and the current GUI state, may result in errors if files contain
  incompatible information.
  """
  def __init__(self, prefer_pdb_space_group=True):
    self.pdb_file_names = []
    self.reflection_file_names = []
    self.symmetry_by_file = {}
    self.current_space_group = None
    self.current_unit_cell = None
    self.prefer_pdb_space_group = prefer_pdb_space_group

  def set_current(self, space_group, unit_cell):
    self.current_space_group = space_group
    self.current_unit_cell = unit_cell

  def get_current(self):
    return (self.current_space_group, self.current_unit_cell)

  def as_symmetry_object(self):
    if (self.current_space_group is None) or (self.current_unit_cell is None):
      raise Sorry("Either the space group or the unit cell (or both) is "+
        "undefined.")
    import cctbx.crystal
    return cctbx.crystal.symmetry(
      space_group_info=self.current_space_group,
      unit_cell=self.current_unit_cell)

  def get_current_as_strings(self):
    sg, uc = self.get_current()
    if (uc is None):
      uc_str = ""
    else :
      uc_str = "%.3g %.3g %.3g %.3g %.3g %.3g" % uc.parameters()
    if (sg is None):
      sg_str = ""
    else :
      sg_str = str(sg)
    return (sg_str, uc_str)

  def set_current_as_strings(self, space_group, unit_cell):
    """Set symmetry from fields in the GUI."""
    if (space_group == "") or (unit_cell is None):
      self.current_space_group = None
    else :
      from cctbx import sgtbx
      try :
        self.current_space_group = sgtbx.space_group_info(space_group)
      except RuntimeError as e :
        if ("symbol not recognized" in str(e)):
          raise Sorry(("The current value for the space group parameter, "+
            "'%s', could not be recognized as a valid space group symbol.") %
            space_group)
        else :
          raise
    if (unit_cell == "") or (unit_cell is None):
      self.current_unit_cell = None
    else :
      from cctbx import uctbx
      self.current_unit_cell = uctbx.unit_cell(unit_cell)

  def process_pdb_file(self, input_file):
    """Extract symmetry info from iotbx.file_reader._any_file object"""
    symm = input_file.file_object.crystal_symmetry()
    if (symm is not None):
      space_group = symm.space_group_info()
      unit_cell = symm.unit_cell()
    else :
      space_group, unit_cell = None, None
    file_name = input_file.file_name
    return self.add_pdb_file(file_name, space_group, unit_cell)

  def add_pdb_file(self, file_name, space_group, unit_cell):
    self.pdb_file_names.append(file_name)
    self.symmetry_by_file[file_name] = (space_group, unit_cell)
    return self.check_consistency_and_set_symmetry(
      file_name=file_name,
      space_group=space_group,
      unit_cell=unit_cell,
      file_type="pdb")

  def process_reflections_file(self, input_file):
    """Extract symmetry info from iotbx.file_reader._any_file object"""
    symm = input_file.file_server.miller_arrays[0].crystal_symmetry()
    if (symm is not None):
      space_group = symm.space_group_info()
      unit_cell = symm.unit_cell()
    else :
      space_group, unit_cell = None, None
    file_name = input_file.file_name
    return self.add_reflections_file(file_name, space_group, unit_cell)

  def add_reflections_file(self, file_name, space_group, unit_cell):
    self.reflection_file_names.append(file_name)
    self.symmetry_by_file[file_name] = (space_group, unit_cell)
    return self.check_consistency_and_set_symmetry(
      file_name=file_name,
      space_group=space_group,
      unit_cell=unit_cell,
      file_type="hkl")

  def check_cell_compatibility(self, program_name,
      raise_error_if_incomplete=False):
    if (self.current_unit_cell is None) or (self.current_space_group is None):
      if (raise_error_if_incomplete):
        raise Sorry("Either the unit cell or the space group (or both) is "+
          "not set; these parameters are required to run %s." % program_name)
      return None
    else :
      from cctbx import crystal
      try :
        symm = crystal.symmetry(space_group=self.current_space_group.group(),
          unit_cell=self.current_unit_cell)
      except AssertionError as e :
        raise Sorry("Unit cell parameters are not consistent with the "+
          "currently set space group.  Please make sure that the symmetry "+
          "information is entered correctly.")
      else :
        return True

  def check_consistency_and_set_symmetry(self, file_name, space_group,
      unit_cell, file_type):
    space_group_mismatch = False
    set_new_space_group = False
    unit_cell_mismatch = False
    incompatible_cell = False
    if (space_group is not None):
      if (self.current_space_group is not None):
        current_sgname = str(self.current_space_group)
        new_sgname = str(space_group)
        if (current_sgname != new_sgname):
          group = self.current_space_group.group()
          derived_sg = group.build_derived_point_group()
          if (space_group.group().build_derived_point_group() != derived_sg):
            space_group_mismatch = True
          elif (file_type == "pdb") and (self.prefer_pdb_space_group):
            self.current_space_group = space_group
      else :
        self.current_space_group = space_group
    if (unit_cell is not None):
      if (self.current_unit_cell is not None):
        if (not self.current_unit_cell.is_similar_to(unit_cell)):
          unit_cell_mismatch = True
      else :
        self.current_unit_cell = unit_cell
    return (space_group_mismatch, unit_cell_mismatch)

  def get_symmetry_choices(self):
    sg_files = []
    uc_files = []
    all_file_names = self.pdb_file_names + self.reflection_file_names
    for file_name in all_file_names :
      space_group, unit_cell = self.symmetry_by_file[file_name]
      if (space_group is not None):
        sg_files.append((file_name, str(space_group)))
      if (unit_cell is not None):
        uc_files.append((file_name, str(unit_cell)))
    return group_args(
      current_space_group=str(self.current_space_group),
      current_unit_cell=str(self.current_unit_cell),
      space_group_files=sg_files,
      unit_cell_files=uc_files)

  def show(self, out=None):
    if (out is None):
      out = sys.stdout
    all_file_names = self.pdb_file_names + self.reflection_file_names
    for file_name in all_file_names :
      space_group, unit_cell = self.symmetry_by_file[file_name]
      print("%s: %s %s" % (os.path.basename(file_name), str(unit_cell),
        str(space_group)), file=out)

# FIXME combine with the above code
# exercised as part of mmtbx/regression/tst_combine_symmetry.py
def combine_model_and_data_symmetry(
    model_symmetry,
    data_symmetry):
  """
  Given data from a model (PDB) file and a reflections file, attempt to
  reconcile them.  Precedence is given to the space group from the PDB file
  and the unit cell from the data file.
  """
  from cctbx import crystal
  use_symmetry = None
  if (model_symmetry is not None) and (data_symmetry is not None):
    if (not model_symmetry.unit_cell().is_similar_to(
        data_symmetry.unit_cell())):
      raise Sorry(("Unit cell mismatch between data and PDB file:\n"+
        "PDB file: %s\nData:%s") % (model_symmetry.unit_cell().parameters(),
        data_symmetry.unit_cell().parameters()))
    pdb_sg = model_symmetry.space_group_info()
    hkl_sg = data_symmetry.space_group_info()
    if (pdb_sg != hkl_sg):
      pdb_group = pdb_sg.group()
      derived_sg = pdb_group.build_derived_point_group()
      if (hkl_sg.group().build_derived_point_group() != derived_sg):
        raise Sorry("Incompatible space groups in data and PDB files:\n" +
          "PDB file: %s\nData:%s" % (pdb_sg, hkl_sg))
    use_symmetry = crystal.symmetry(
      unit_cell=data_symmetry.unit_cell(),
      space_group_info=model_symmetry.space_group_info())
  elif (model_symmetry is not None):
    use_symmetry = model_symmetry
  elif (data_symmetry is not None):
    use_symmetry = data_symmetry
  return use_symmetry


 *******************************************************************************


 *******************************************************************************
iotbx/table_one.py
"""
Tools for formatting the standard "Table 1" in MX papers.  Relies on other
methods (phenix.refine, phenix.model_vs_data, phenix.merging_statistics, etc.)
to extract statistics for display.
"""

from __future__ import absolute_import, division, print_function
from libtbx import slots_getstate_setstate
from libtbx.utils import Sorry
from libtbx import str_utils
import re
from six.moves import range

angstrom = u"\u00C5".encode("utf-8", "strict").strip()

# XXX are these complete, and if not, what is missing?  this appears to cover
# the template suggested by NSMB plus more, but not clear what other journals
# expect.  the VTF paper does not list any explicit Table 1 requirements, but
# the forthcoming PDB validation reports may change expectations.
# TODO fill in missing CIF tags - some of these may not have standard
# tag names yet
keywords = [
  # (attr, label, format,  cif_tag)
  # data-only statistics
  ("wavelength", "Wavelength", "%.4g", None),
  ("d_max_min", "Resolution range", "%.3g - %.3g", None),
  ("space_group", "Space group", "%s", None),
  ("unit_cell", "Unit cell", "%g %g %g %g %g %g", None),
  ("n_refl_all", "Total reflections", "%d", "_reflns.pdbx_number_measured_all"),
  ("n_refl", "Unique reflections", "%d", "_reflns.number_obs"),
  ("multiplicity", "Multiplicity", "%.1f", "_reflns.pdbx_redundancy"),
  ("completeness", "Completeness (%)", "%.2f", "_reflns.percent_possible_obs"),
  ("i_over_sigma", "Mean I/sigma(I)", "%.2f", "_reflns.pdbx_netI_over_sigmaI"),
  ("wilson_b", "Wilson B-factor", "%.2f", "_reflns.B_iso_Wilson_estimate"),
  ("r_sym", "R-merge", "%.4g", "_reflns.pdbx_Rmerge_I_obs"),
  ("r_meas", "R-meas", "%.4g", "_reflns.pdbx_Rrim_I_obs"),
  ("r_pim", "R-pim", "%.4g", "_reflns.pdbx_Rpim_I_obs"),
  ("cc_one_half", "CC1/2", "%.3g", "_reflns.phenix_cc_1/2"),
  ("cc_star", "CC*", "%.3g", "_reflns.phenix_cc_star"),
  # refinement statistics
  # TODO figure out how to extract this...
  ("n_refl_refine", "Reflections used in refinement", "%d", None),
    # XXX this is problematic - I am not sure if our usage is consistent with
    # the intended purpose of this tag
  #  "_refine.ls_number_reflns_obs"),
  ("n_free", "Reflections used for R-free", "%d",
    "_refine.ls_number_reflns_R_free"),
  ("r_work", "R-work", "%.4f", "_refine.ls_R_factor_R_work"),
  ("r_free", "R-free", "%.4f", "_refine.ls_R_factor_R_free"),
  ("cc_work", "CC(work)", "%.3f", None),
  ("cc_free", "CC(free)", "%.3f", None),
  ("n_atoms", "Number of non-hydrogen atoms", "%d", None),
  ("n_macro_atoms", "  macromolecules", "%d", None),
  ("n_ligand_atoms", "  ligands", "%d", None),
  ("n_waters", "  solvent", "%d", None),
  ("n_residues", "Protein residues", "%d", None),
  ("n_nuc", "Nucleic acid bases", "%d", None),
  ("bond_rmsd", "RMS(bonds)", "%.3f", None),
  ("angle_rmsd", "RMS(angles)", "%.2f", None),
  ("rama_favored", "Ramachandran favored (%)", "%.2f", None),
  ("rama_allowed", "Ramachandran allowed (%)", "%.2f", None),
  ("rama_outliers", "Ramachandran outliers (%)", "%.2f", None),
  ("rota_outliers", "Rotamer outliers (%)", "%.2f", None),
  ("clashscore", "Clashscore", "%.2f", None),
  ("adp_mean", "Average B-factor", "%.2f", None),
  ("adp_mean_mm", "  macromolecules", "%.2f", None),
  ("adp_mean_lig", "  ligands", "%.2f", None),
  ("adp_mean_wat", "  solvent", "%.2f", None),
  ("n_tls_groups", "Number of TLS groups", "%d", None),
#  ("solvent_content", "Solvent content", "%.1f%%", None),
]

# statistics that we don't want to show if not applicable (mainly for different
# molecule types)
optional_if_none = set([
  "n_residues", "n_ligand_atoms", "n_waters", "n_nuc",
  "rama_favored", "rama_allowed", "rama_outliers", "rota_outliers",
  "adp_mean_lig", "adp_mean_wat", "cc_work", "cc_free", "cc_star",
  "n_tls_groups",
])

keyword_formats = dict([ (kw, fs) for (kw, label, fs, cif_tag) in keywords ])

class column(slots_getstate_setstate):
  """
  Statistics for a single structure, including optional high-resolution
  shell.  Any combination of standard attributes is permitted as keyword
  arguments to the constructor.  (Note that the high-resolution shell is
  itself another instance of this class, with fewer keywords specified.)
  """

  __slots__ = [ "outer_shell", "label", "anomalous_flag" ] + \
              [ kw[0] for kw in keywords ]

  def __init__(self, **kwds):
    kwds = dict(kwds)
    for name in self.__slots__ :
      if (name in kwds):
        setattr(self, name, kwds[name])
      else :
        setattr(self, name, None)
    self.outer_shell = None

  def add_outer_shell(self, **kwds):
    self.outer_shell = column(**kwds)
    return self

  def format_stat(self, name):
    value = getattr(self, name, None)
    if (value is None) and (name in optional_if_none):
      return None
    shell_value = getattr(self.outer_shell, name, None)
    fs = keyword_formats[name]
    if (name == "d_max_min"):
      cell_value = format_d_max_min(value)
    else :
      cell_value = format_value(fs, value)
    if (shell_value is not None):
      if (name == "d_max_min"):
        subvalue = format_d_max_min(shell_value)
      else :
        subvalue = format_value(fs, shell_value)
      cell_value += " (%s)" % subvalue
    return cell_value

  def __repr__(self):
    lines = []
    for (stat_name, label, format_string, cif_tag) in keywords :
      value = getattr(self, stat_name, None)
      if (value is not None):
        lines.append("%s %s" % (stat_name, value))
    return "\n".join(lines)

class table(slots_getstate_setstate):
  """
  Combined table of statistics for one or more structures published together.
  """
  __slots__ = ["text_field_separation", "columns"]

  def __init__(self, text_field_separation=4):
    self.text_field_separation = text_field_separation
    self.columns = []

  def add_column(self, col):
    self.columns.append(col)

  def format_as_txt(self):
    rows = []
    rows.append([""] + [ column.label for column in self.columns ])
    for (stat_name, label, fstring, cif_tag) in keywords :
      row = []
      for column in self.columns :
        row.append(column.format_stat(stat_name))
      if (row == [ None for x in range(len(row)) ]):
        continue
      row.insert(0, label)
      rows.append(row)
    n_rows = len(rows)
    n_cols = len(self.columns) + 1
    columns = [ [ row[i] for row in rows ] for i in range(n_cols) ]
    columns = [ resize_column(col, "right") for col in columns ]
    table = [ [ col[j] for col in columns ] for j in range(n_rows) ]
    sep = " " * self.text_field_separation
    out = "\n".join([ sep.join(row) for row in table ])
    return out

  def format_as_csv(self):
    rows = []
    rows.append([""] + [ column.label if column.label is not None else "" for column in self.columns ])
    for (stat_name, label, fstring, cif_tag) in keywords :
      row = []
      for column in self.columns :
        stat = column.format_stat(stat_name)
        if stat is None:
          stat = ""
        row.append(stat)
      if ( (row == [ None for x in range(len(row)) ]) or
           (row is None) ):
        continue
      if label is None:
        label = ""
      row.insert(0, label)
      rows.append(row)
    return "\n".join([ ",".join(row) for row in rows ])

  def format_as_rtf(self):
    try :
      import PyRTF
    except ImportError :
      raise Sorry("The PyRTF module is not available.")
    doc = PyRTF.Document()
    ss = doc.StyleSheet
    section = PyRTF.Section()
    doc.Sections.append(section)
    p = PyRTF.Paragraph( ss.ParagraphStyles.Heading2 )
    p.append("Table 1.  Data collection and refinement statistics.")
    section.append(p)
    n_cols = len(self.columns) + 1
    col_widths = [ PyRTF.TabPS.DEFAULT_WIDTH * 3 ] * n_cols
    table = PyRTF.Table(*col_widths)
    header = [ PyRTF.Cell(PyRTF.Paragraph("")) ]
    anomalous_flag = False
    for column in self.columns :
      label = column.label
      if (column.anomalous_flag):
        label += "*"
        anomalous_flag = True
      column_label = PyRTF.Paragraph(ss.ParagraphStyles.Heading2, label)
      header.append(PyRTF.Cell(column_label))
    table.AddRow(*header)
    for (stat_name, label, fstring, cif_tag) in keywords :
      row = [ PyRTF.Cell(PyRTF.Paragraph(ss.ParagraphStyles.Heading2, label)) ]
      n_none = 0
      for column in self.columns :
        txt = column.format_stat(stat_name)
        if (txt is None):
          n_none += 1
        p = PyRTF.Paragraph(ss.ParagraphStyles.Normal,
          PyRTF.ParagraphPS(alignment=2))
        p.append(txt)
        row.append(PyRTF.Cell(p))
      if (n_none == len(row) - 1):
        continue
      table.AddRow(*row)
    section.append(table)
    p = PyRTF.Paragraph(ss.ParagraphStyles.Normal)
    p.append("Statistics for the highest-resolution shell are shown in "+
      "parentheses.")
    section.append(p)
    return doc

  def save_txt(self, file_name):
    f = open(file_name, "w")
    out = self.format_as_txt()
    f.write(out)
    f.close()

  def save_csv(self, file_name):
    f = open(file_name, "w")
    out = self.format_as_csv()
    f.write(out)
    f.close()

  def save_rtf(self, file_name):
    import PyRTF
    DR = PyRTF.Renderer()
    doc = self.format_as_rtf()
    f = open(file_name, "w")
    DR.Write(doc, f)
    f.close()

#-----------------------------------------------------------------------
# UTILITY FUNCTIONS

# XXX is this superfluous?
def format_value(fs, value):
  try :
    val_str = str_utils.format_value(fs, value, replace_none_with="").strip()
  except Exception as e :
    raise RuntimeError("Formatting error: %s, %s" % (fs, value))
  else :
    return val_str

def format_d_max_min(d_max_min):
  """Format a resolution range (e.g. '30 - 2.56')"""
  if (d_max_min is None) or (d_max_min == (None, None)):
    return ""
  else :
    (d_max, d_min) = d_max_min
    d_max_str = "%.4g " % d_max
    d_min_str = re.sub(r"\.$", ".0", re.sub("0*$", "", "%.3f" % d_min))
    return "%s - %s" % (d_max_str, d_min_str)

def resize_column(cell_values, alignment="right"):
  max_width = max([ len(str(cell)) for cell in cell_values ])
  if (alignment == "right"):
    fs = "%%%ds" % max_width
    return [ fs % cell for cell in cell_values ]
  elif (alignment == "left"):
    fs = "%%%-ds" % max_width
    return [ fs % cell for cell in cell_values ]
  else :
    raise RuntimeError("Alignemnt '%s' not supported." % alignment)


 *******************************************************************************
