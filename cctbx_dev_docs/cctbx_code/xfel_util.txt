

 *******************************************************************************
xfel/util/__init__.py


 *******************************************************************************


 *******************************************************************************
xfel/util/cctbx_xfel_installer.py

# XXX This is not a standalone installer!  It must be used as part of the
# framework in libtbx/auto_build.

"""
Installer script for cctbx.xfel packages based on automatically generated
template.  This must be moved to the proper location to work.
"""

from __future__ import absolute_import, division, print_function
import os.path
import sys
libtbx_path = os.path.join(
  os.path.abspath(os.path.dirname(os.path.dirname(__file__))), "lib")
if (not libtbx_path in sys.path) :
  sys.path.append(libtbx_path)
from libtbx.auto_build import install_distribution

class installer (install_distribution.installer) :
  # XXX most settings can be edited here
  product_name = "cctbx.xfel"
  dest_dir_prefix = "xfel"
  make_apps = []
  configure_modules = install_distribution.installer.configure_modules + \
    ['dxtbx', 'wxtbx', "gltbx", "crys3d", "xfel","dials"]
  include_gui_packages = True
  base_package_options = ['--all']
  source_packages = [ "cctbx_bundle" ] + ['cbflib','labelit','dials']
  #

  installer_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

if (__name__ == "__main__") :
  installer(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/util/dials_file_matcher.py
from __future__ import absolute_import, division, print_function
from six.moves import range
import os
from six.moves import zip

def match_dials_files(list1, list2, suffix1, suffix2):
  ids1 = [os.path.join(os.path.dirname(item), os.path.basename(item).split(suffix1)[0])
          for item in list1]
  ids2 = [os.path.join(os.path.dirname(item), os.path.basename(item).split(suffix2)[0])
          for item in list2]
  matches = [(ids1[i] + suffix1,ids2[ids2.index(ids1[i])] + suffix2)
             for i in range(len(ids1)) if ids1[i] in ids2]
  return (list(zip(*matches)))


 *******************************************************************************


 *******************************************************************************
xfel/util/dials_pickle_reader.py
"""
Utility for matching a dials-format non-image pickle file to an experiment list and producing a cctbx-format dictionary
from the pair.
"""
from __future__ import absolute_import, division, print_function

import os
from dials.util.options import Importer, flatten_reflections, flatten_experiments
from serialtbx.util.construct_frame import ConstructFrame

def find_json(pickle, pickle_ext=None, json_ext=None):
  """find the matching json file for a given dials-format non-image pickle file"""
  name = os.path.basename(pickle).split(".refl")[0]
  dirname = os.path.dirname(pickle)
  if pickle_ext is not None:
    if pickle_ext == "":
      base = name
    else:
      base = name.split(pickle_ext)[0]
  elif name.endswith("_indexed"):
    base = name.split("_indexed")[0]
  elif name.endswith("_integrated"):
    base = name.split("_integrated")[0]
  else:
    base = name
  if json_ext is not None:
    json = os.path.join(dirname, base + json_ext + ".expt")
  elif os.path.exists(os.path.join(dirname, base + "_refined.expt")):
    json = os.path.join(dirname, base + "_refined.expt")
  elif os.path.exists(os.path.join(dirname, base + "_indexed.expt")):
    json = os.path.join(dirname, base + "_indexed.expt")
  else:
    json = None
  return json

class read_dials_pickle(object):
  """given a dials-format non-image pickle file with matching json file, return a dictionary pickle"""
  def __init__(self, path, json=None, pickle_ext=None, json_ext=None):
    if json is None:
      json = find_json(path, pickle_ext, json_ext)
    if json is None:
      importer = Importer([path], read_experiments=False, read_reflections=True, check_format=False)
      print("unable to find experiment list")
      self.experiments = None
    else:
      importer = Importer([path, json], read_experiments=True, read_reflections=True, check_format=False)
      try:
        self.experiments = flatten_experiments(importer.experiments)[0]
      except IndexError:
        print("unable to read experiment list")
        self.experiments = None
    try:
      self.reflections = flatten_reflections(importer.reflections)[0]
    except IndexError:
      print("unable to read reflection table")
      self.reflections = None
  def make_pickle(self):
    if (self.experiments is None) or (self.reflections is None):
      self.dictionary = None
    else:
      contents = ConstructFrame(self.reflections, self.experiments)
      self.dictionary = contents.make_frame()


 *******************************************************************************


 *******************************************************************************
xfel/util/drift.py
from __future__ import division
import abc
from collections import OrderedDict, UserList
import functools
import glob
import itertools
from json.decoder import JSONDecodeError
import os
import pickle
import six
import sys
import tempfile
from typing import Any, Callable, Dict, Iterable, List, Sequence, Tuple, Union

from dials.array_family import flex  # noqa
from dxtbx.model.experiment_list import ExperimentList  # noqa
from libtbx.phil import parse
from libtbx.utils import Sorry

import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
from matplotlib.lines import Line2D
from matplotlib.patches import Rectangle
from matplotlib.ticker import FixedLocator, PercentFormatter
import numpy as np
import pandas as pd


pd.set_option('display.max_rows', 100)
pd.set_option('display.max_columns', 25)
pd.set_option('display.max_colwidth', 20)

message = """
This script collects and visualizes the spatial drift of a detector and unit
cell parameters as a function of experimental progress. It requires
the directory structure to follow the one resulting from data processing
(i.e. ensemble refinement) performed by cctbx.xfel.
Data scraping can take some time, especially for large datasets.
For this reason, scraping results can be saved and loaded from a pickle cache.
End result is a plot with detector origin position and unit cell lengths
(vertical position) as a function of run & chunk number (horizontal position).
Numbers of reflections and experiments contributing to each batch are drawn
as a bar height and width on the top of the plot.
By default, point and bars' colors reflect the data's folders of origin.
Error bars can be also derived from the uncertainty of individual reflections'
position in laboratory reference system.

Example usage 1:
Read common detector origin position and average unit cell parameters for all
expts merged in "batch*" datasets / directories, excluding "batch5":
    cctbx.xfel.drift scrap.input.glob=batch* scrap.input.exclude=batch5

Example usage 2:
Not only read, but also cache "batch*" results in a "name.pkl" pickle:
    cctbx.xfel.drift
    scrap.input.glob=batch* scrap.cache.action=write scrap.cache.glob=name.pkl

Example usage 3:
Read cached "batch*" results from all "*.pkl" files and save the plot:
    cctbx.xfel.drift
    scrap.cache.action=read scrap.cache.glob=*.pkl plot.save=True

Example usage 4:
Read distribution of detector origin position, detector origin uncertainty,
and unit cell parameters from selected TDER task209 directories:
    cctbx.xfel.drift
    scrap.input.glob=r0*/039_rg084/task209 scrap.input.kind=tder_task_directory
""".strip()


################################ PHIL HANDLING ################################


phil_scope_str = """
  scrap {
    input {
      glob = None
        .type = str
        .multiple = True
        .help = glob which matches files after TDER to be investigated.
      exclude = None
        .type = str
        .multiple = True
        .help = glob which matches files to exclude from input.glob
      kind = tder_task_directory *merging_directory
        .type = choice
        .help = The type of files located by input.glob
    }
    cache {
      action = *none read write
        .type = choice
        .help = Read drift table instead of generating one, or write one.
      glob = drift_cache.pkl
        .type = str
        .help = Path to cache(s) with pickled scrap results to write/read
    }
    origin = *first average distribution panel_com_first panel_com_average panel_com_distribution
      .type = choice
      .help = Use origin of first experiment only or average/distribution of all?
    uncertainties = True
      .type = bool
      .help = If True, uncertainties will be estimated using differences \
            between predicted and observed refl positions and cell distribution
    unit_cell = *average distribution
      .type = choice
      .help = Use average unit cell or distribution of all?
    }
  plot {
    color {
      by = chunk *merge run rungroup task trial
        .type = choice
        .help = Variable to color individual points on drift plot by;
      correlation = seismic
        .type = str
        .help = Name of matplotlib colormap to be used on correlation plot
      distribution = magma_r
        .type = str
        .help = Name of matplotlib colormap to be used on distribution heatmap
      distribution_bg = white
        .type = str
        .help = Bg color of distribution plot. 'auto' to derive from gradient.
    }
    show = True
      .type = bool
      .help = If False, do not display resulting plot interactively
    save = False
      .type = bool
      .help = If True, save resulting drift plot under plot.path
    path = drift_plot.png
      .type = str
      .help = A path, name, and extension of saved plot (e.g.: drift/fig.png)
    height = 8.0
      .type = float
     .help = Height of saved plot in inches
    width = 10.0
      .type = float
      .help = Width of saved plot in inches
  }
"""
phil_scope = parse(phil_scope_str)


def params_from_phil(phil_scope_, args):
  user_phil = []
  for arg in args:
    if os.path.isfile(arg):
      user_phil.append(parse(file_name=arg))
    else:
      try:
        user_phil.append(parse(arg))
      except Exception:
        raise Sorry("Unrecognized argument: %s" % arg)
  params_ = phil_scope_.fetch(sources=user_phil).extract()
  return params_


DEFAULT_INPUT_SCOPE = parse("""
  input {
    path = None
      .multiple = True
      .type = str
    experiments_suffix = .expt
      .type = str
    reflections_suffix = .refl
      .type = str
    experiments = None
      .multiple = True
      .type = str
    reflections = None
      .multiple = True
      .type = str
}
""")


############################### MATHS FUNCTIONS ###############################


def average(xs: Sequence, weights: Sequence = None) -> float:
  """Calculate weighted arithmetic mean of a sequence"""
  xs = np.array(xs)
  weights = np.ones((len(xs))) if weights is None else np.array(weights)
  return sum(xs * weights) / sum(weights)


def correlation(xs: Sequence, ys: Sequence, weights: Sequence = None) -> float:
  """Calculate weighted Pearson's correlation coefficient between sequences"""
  weights = np.ones((len(xs))) if weights is None else np.array(weights)
  x_variance = variance(xs, weights=weights)
  y_variance = variance(ys, weights=weights)
  xy_covariance = covariance(xs, ys, weights=weights)
  return xy_covariance / (x_variance * y_variance) ** 0.5


def variance(xs: Sequence, weights: Sequence = None) -> float:
  """Calculate weighted variance of a sequence"""
  weights = np.ones((len(xs))) if weights is None else np.array(weights)
  x_average = average(xs, weights=weights)
  x_deviations = np.array(xs) - x_average
  return sum(weights * x_deviations * x_deviations) / sum(weights)


def covariance(xs: Sequence, ys: Sequence, weights: Sequence = None) -> float:
  """Calculate weighted covariance between two sequences"""
  weights = np.ones((len(xs))) if weights is None else np.array(weights)
  x_average = average(xs, weights=weights)
  y_average = average(ys, weights=weights)
  x_deviations = np.array(xs) - x_average
  y_deviations = np.array(ys) - y_average
  return sum(weights * x_deviations * y_deviations) / sum(weights)


def normalize(sequence: Sequence, floor: float = 0., ceiling: float = 1.) \
        -> Sequence:
  """Normalize `sequence`'s values to lie between `floor` and `ceiling`"""
  min_, max_ = min(sequence), max(sequence)
  old_span = max_ - min_
  new_span = ceiling - floor
  normalized = [new_span * (s - min_) / old_span + floor for s in sequence]
  return sequence if min_ == max_ else normalized


class CorrelationMatrix(object):
  """Calculate, store, and print correlation matrix between many sequences"""

  def __init__(self, variables: Dict[str, Sequence], weights: Sequence = None):
    """Calculate corr. matrix between every pair in `variables` dict-like"""
    self.keys = variables.keys()
    self.corr = {k: {} for k in self.keys}
    for i1, k1 in enumerate(self.keys):
      for i2, k2 in enumerate(self.keys):
        if i1 == i2:
          self.corr[k1][k2] = self.corr[k2][k1] = 1.0
        elif len(variables[k1]) < 2:
          self.corr[k1][k2] = self.corr[k2][k1] = 0.0
        elif i2 > i1:
          try:
            corr = correlation(variables[k1], variables[k2], weights=weights)
          except ZeroDivisionError:
            corr = 0
          self.corr[k1][k2] = self.corr[k2][k1] = corr

  def __str__(self) -> str:
    s = 'wPPC    ' + ' '.join('{:>7}'.format(k) for k in self.keys)
    for k1 in self.keys:
      s += '\n{:7}'.format(k1)
      for k2 in self.keys:
        s += ' {:+6.4f}'.format(self.corr[k1][k2])
    return s


############################## UTILITY FUNCTIONS ##############################


def is_iterable(value: object, count_str: bool = False):
  """Return `True` if object is iterable and not string, else `False`"""
  if not count_str and isinstance(value, str):
    return False
  try:
    iter(value)  # noqa
  except TypeError:
    return False
  else:
    return True


def path_join(*path_elements: str) -> str:
  """Join path from elements, resolving all redundant or relative calls"""
  path_elements = [os.pardir if p == '..' else p for p in path_elements]
  return os.path.normpath(os.path.join(*path_elements))


def path_lookup(*path_elements: str) -> List[str]:
  """Join path elements and return a list of all matching files/dirs"""
  return glob.glob(path_join(*path_elements), recursive=True)


def path_split(path: str) -> List[str]:
  """Split path into directories and file basename using os separator"""
  return os.path.normpath(path).split(os.sep)


def read_experiments(*expt_paths: str) -> ExperimentList:
  """Create an instance of ExperimentList from one or more `*expt_paths`"""
  expts = ExperimentList()
  for expt_path in unique_elements(expt_paths):
    expts.extend(ExperimentList.from_file(expt_path, check_format=False))
  return expts


def read_reflections(*refl_paths: str) -> flex.reflection_table:
  """Create an instance of flex.reflection_table from one/more `*refl_paths`"""
  r = [flex.reflection_table.from_file(p) for p in unique_elements(refl_paths)]
  return flex.reflection_table.concat(r)


def represent_range_as_str(sorted_iterable: Sequence, sep: str = None) -> str:
  """Return range of str in iterable, e.g. "r081-94" for ["r081", "r094"]"""
  fs, ls = str(sorted_iterable[0]), str(sorted_iterable[-1])
  d = min([i for i, (fl, ll) in enumerate(zip(fs, ls)) if fl != ll] or [None])
  sep0, sep1 = (sep[0], sep[-1]) if sep is not None else ('', '')
  return fs if not d else fs[:d] + sep0 + fs[d:] + '-' + ls[d:] + sep1


def unique_elements(sequence: Sequence) -> List:
  """Return list of unique elements in sequence while preserving its order"""
  return list(OrderedDict.fromkeys(sequence))


################################ DRIFT STORAGE ################################


class DriftTable(object):
  """Class responsible for storing all info collected by `DriftScraper`"""
  STATIC_KEYS = ['merge', 'run', 'rungroup', 'trial', 'chunk', 'task',
                 'x', 'y', 'z', 'delta_x', 'delta_y', 'delta_z', 'expts',
                 'refls', 'a', 'b', 'c', 'delta_a', 'delta_b', 'delta_c']
  DYNAMIC_KEYS = ['density']

  def __init__(self):
    self.data = pd.DataFrame()

  def __getitem__(self, key: str) -> pd.Series:
    if key in self.DYNAMIC_KEYS:
      self.recalculate_dynamic_column(key)
    return self.data[key]

  def __len__(self) -> int:
    return len(self.data.index)

  def __str__(self) -> str:
    for key in self.DYNAMIC_KEYS:
      self.recalculate_dynamic_column(key)
    return str(self.data)

  def add(self, d: dict) -> None:
    d_is_bumpy = any(is_iterable(d[k]) for k in d.keys() if k != 'refls')
    d_is_all_flat = all(not is_iterable(d[k]) for k in d.keys())
    if d_is_bumpy or d_is_all_flat:
      d2 = d
    else:  # if only 'refls' column is iterable, immediately sum it
      d2 = {k: sum(v) if k == 'refls' else v for k, v in d.items()}
    pd_kwargs = {'index': [0]} #{} if d_is_bumpy else {'index': [0]}
    new_rows = pd.DataFrame(d2, **pd_kwargs)
    self.data = pd.concat([self.data, new_rows], ignore_index=True)

  def get(self, key: str, default: Any = None) -> pd.Series:
    return self[key] if key in self.data.columns else default

  def sort(self, by: Union[str, Sequence[str]]) -> None:
    self.data.sort_values(by=by, ignore_index=True, inplace=True)

  def column_is_flat(self, key: str) -> bool:
    return not is_iterable(self[key][0])

  def assert_not_empty(self):
    if len(self.data) == 0:
      raise ValueError(f"{self.__class__} has no rows. Did you read any data?")

  @property
  def flat(self) -> pd.DataFrame:
    """Pandas' data `DataFrame` with all iterable fields expanded over rows"""
    c = self.column_is_flat
    col_names = self.data.columns
    flatteners = [itertools.repeat if c(k) else lambda x: x for k in col_names]
    set_all_expt_count_to_1 = False
    flat_sub_tables = []
    for i, row in enumerate(self.data.itertuples(index=False)):
      lens = [len(el) for el in row if is_iterable(el)]
      if len(unique_elements(lens)) > 1:
        raise ValueError('All row elements must be scalars or same-length')
      elif len(unique_elements(lens)) == 1:
        flat_rows = zip(*[f(cell) for cell, f in zip(row, flatteners)])
        flat_columns = [flat_col for flat_col in zip(*flat_rows)]
        set_all_expt_count_to_1 = True
      else:
        flat_columns = [[cell] for cell in row]
      fst = pd.DataFrame({k: v for k, v in zip(col_names, flat_columns)})
      fst['original_index'] = i
      flat_sub_tables.append(fst)
    flat_table = pd.concat(flat_sub_tables, ignore_index=True)
    if set_all_expt_count_to_1:
      flat_table['expts'] = 1
    return flat_table

  def recalculate_dynamic_column(self, key: str) -> None:
    self.assert_not_empty()
    if key == 'density':
      refls = self.data['refls'] if self.column_is_flat('refls') \
        else pd.Series([sum(refl) for refl in self.data['refls']])
      self.data['density'] = refls / self.data['expts']
    else:
      raise KeyError(f'Unknown dynamic column key: {key}')


############################### DRIFT SCRAPPING ###############################


class ScrapResults(UserList):
  """Responsible for storing and pickling DriftScraper results."""
  def __init__(self, parameters) -> None:
    super().__init__()
    self.parameters = parameters

  def read(self) -> None:
    scrap_paths, scrap_results = [], []
    for scg in path_lookup(self.parameters.scrap.cache.glob):
      scrap_paths.extend(glob.glob(scg))
    for scrap_path in scrap_paths:
      with open(scrap_path, 'rb') as pickle_file:
        self.extend(pickle.load(pickle_file))

  def write(self) -> None:
    write_path = self.parameters.scrap.cache.glob
    with open(write_path, 'wb') as pickle_file:
      pickle.dump(self, pickle_file)


def autoupdate_scrap_dict_with_return(scrap_method: Callable) -> Callable:
  @functools.wraps(scrap_method)
  def scrap_wrapper(self: Union['BaseDriftScraper', 'DriftScraperMixin'],
                    *args: Any, **kwargs: Any):
    scraped = scrap_method(self, *args, **kwargs)
    self.scrap_dict.update(scraped)
    return scraped
  return scrap_wrapper


def handle_scrap_cache(scrap: Callable) -> Callable:
  @functools.wraps(scrap)
  def scrap_wrapper(self: 'BaseDriftScraper', *args: Any, **kwargs: Any):
    if self.parameters.scrap.cache.action == 'read':
      self.scrap_results.read()
    scrap(self, *args, **kwargs)
    if self.parameters.scrap.cache.action == 'write':
      self.scrap_results.write()
    for scrap_dict in self.scrap_results:
      self.table.add(scrap_dict)
  return scrap_wrapper


class DriftScraperRegistrar(abc.ABCMeta):
  """Metaclass for `DriftScraper`s, auto-registers them by `input_kind`."""
  REGISTRY = {}
  def __new__(mcs, name, bases, attrs):
    new_cls = super().__new__(mcs, name, bases, attrs)
    if hasattr(new_cls, 'input_kind') and new_cls.input_kind:
      mcs.REGISTRY[new_cls.input_kind] = new_cls
    return new_cls


@six.add_metaclass(DriftScraperRegistrar)
class BaseDriftScraper(object):
  """Base class for scraping cctbx.xfel output into instance of `DriftTable`,
  with automatic registration into the `DriftScraperRegistrar`."""

  def __init__(self, table: DriftTable, parameters) -> None:
    self.table = table
    self.parameters = parameters
    self.scrap_dict: Dict[str, Union[str, float, Sequence]] = {}  # scraped now
    self.scrap_results = ScrapResults(parameters)                 # all scraped

  @staticmethod
  def calc_expt_refl_len(expts: ExperimentList, refls: flex.reflection_table) \
          -> Tuple[int, flex.int]:
    expts_len = len(expts)
    refls_lens = flex.int()
    for expt_id, expt in enumerate(expts):
      refls_lens.append((expt_id == refls['id']).count(True))
    return expts_len, refls_lens

  def locate_input_paths(self) -> List:
    """Return all paths (either common files or directories, relative to
    working directory) of specified kind to be processed"""
    input_paths, exclude_paths = [], []
    for ig in self.parameters.scrap.input.glob:
      input_paths.extend(glob.glob(ig))
    for ie in self.parameters.scrap.input.exclude:
      exclude_paths.extend(glob.glob(ie))
    return [it for it in input_paths if it not in exclude_paths]

  @staticmethod
  def locate_combining_phil_paths(scaling_phil_paths: Iterable[str]) -> List:
    """Return paths to all phil files used to combine later-scaled expts"""
    parsed_scaling_phil = [parse(file_name=spp) for spp in scaling_phil_paths]
    phil = DEFAULT_INPUT_SCOPE.fetch(sources=parsed_scaling_phil).extract()
    combine_dirs = [path_join(ip, '..') for ip in phil.input.path]
    combine_phil_paths = []
    for cd in combine_dirs:
      combine_phil_paths.extend(path_lookup(cd, '*chunk*_combine_*.phil'))
    return sorted(set(combine_phil_paths))

  @staticmethod
  def locate_scaling_directories(merging_phil_paths: Iterable[str]) -> List:
    """Return paths to all directories specified as scrap.input.glob in phil"""
    merging_phils = [parse(file_name=mpp) for mpp in merging_phil_paths]
    phil = DEFAULT_INPUT_SCOPE.fetch(sources=merging_phils).extract()
    return sorted(set(phil.input.path))

  @staticmethod
  def locate_refined_expts_refls(combine_phil_path: str) \
          -> Tuple[ExperimentList, flex.reflection_table]:
    """Return all refined expts and refls down-stream from combine_phil_path"""
    path_stem = combine_phil_path.replace('_combine_experiments.phil', '')
    expts_paths = path_lookup(path_stem + '_refined.expt')
    refls_paths = path_lookup(path_stem + '_refined.refl')
    expts = read_experiments(*expts_paths)
    refls = read_reflections(*refls_paths)
    return expts, refls

  @autoupdate_scrap_dict_with_return
  def scrap_db_metadata(self, combine_phil_path: str) -> dict:
    """Get trial, task, rungroup, chunk, run info based on combining phil"""
    parsed_combine_phil = parse(file_name=combine_phil_path)
    phil = DEFAULT_INPUT_SCOPE.fetch(sources=[parsed_combine_phil]).extract()
    index_dirs = [path_join(pie, '..') for pie in phil.input.experiments]
    rungroups = sorted(set(index_dir[-7:-4] for index_dir in index_dirs))
    trials = sorted(set(index_dir[-13:-10] for index_dir in index_dirs))
    runs = sorted(set(index_dir[-19:-14] for index_dir in index_dirs))
    return {'chunk': int(path_split(combine_phil_path)[-1][16:19]),
            'run': represent_range_as_str(runs),
            'rungroup': represent_range_as_str(rungroups),
            'task': path_split(combine_phil_path)[-4],
            'trial': represent_range_as_str(trials)}

  @abc.abstractmethod
  @handle_scrap_cache
  def scrap(self) -> None:
    """Prepare `ScrapResults` list used by `handle_scrap_cache` to create
    `self.table`, instance of `DriftTable`, based on `self.parameters`."""
    pass


class TderTaskDirectoryDriftScraper(BaseDriftScraper):
  """Drift scraper which looks for all TDER downstream from merging"""
  input_kind = 'tder_task_directory'

  @handle_scrap_cache
  def scrap(self) -> None:
    combining_phil_paths = []
    for tder_task_directory in self.locate_input_paths():
      cpp = path_lookup(tder_task_directory, 'combine_experiments_t*',
                        'intermediates', '*chunk*_combine_*.phil')
      combining_phil_paths.extend(cpp)
    for cpp in unique_elements(combining_phil_paths):  # combine.phil paths
      try:
        self.scrap_dict = {'merge': "None"}
        self.scrap_db_metadata(cpp)
        print(f'Processing run {self.scrap_dict["run"]}')
        refined_expts, refined_refls = self.locate_refined_expts_refls(cpp)
        elen, rlen = self.calc_expt_refl_len(refined_expts, refined_refls)
        print(f'Found {elen} expts and {sum(rlen)} refls')
        self.scrap_dict.update({'expts': elen, 'refls': rlen})
        self.scrap_origin(refined_expts)
        self.scrap_unit_cell(refined_expts)
        self.scrap_origin_deltas(refined_expts, refined_refls)
      except (KeyError, IndexError, JSONDecodeError) as e:
        print(e)
      else:
        self.scrap_results.append(self.scrap_dict)


class MergingDirectoryDriftScraper(BaseDriftScraper):
  """Drift scraper which directly looks for TDER task directories"""
  input_kind = 'merging_directory'

  @handle_scrap_cache
  def scrap(self) -> None:
    for merge in self.locate_input_paths():
      merging_phil_paths = path_lookup(merge, '**', '*.phil')
      merging_phil_paths.sort(key=os.path.getmtime)
      for scaling_dir in self.locate_scaling_directories(merging_phil_paths):
        scaled_expt_paths = path_lookup(scaling_dir, 'scaling_*.expt')
        scaled_expts = read_experiments(*scaled_expt_paths)
        scaled_identifiers = list(scaled_expts.identifiers())
        scaling_phil_paths = []
        for sep in scaled_expt_paths:
          scaling_phil_paths.extend(path_lookup(sep, '..', '..', '*.phil'))
        comb_phil_paths = self.locate_combining_phil_paths(scaling_phil_paths)
        for cpp in unique_elements(comb_phil_paths):
          try:
            self.scrap_dict = {'merge': merge}
            self.scrap_db_metadata(cpp)
            print(f'Processing run {self.scrap_dict["run"]} in merge {merge}')
            refined_expts, refined_refls = self.locate_refined_expts_refls(cpp)
            elen, rlen = self.calc_expt_refl_len(refined_expts, refined_refls)
            print(f'Found {elen} expts and {sum(rlen)} refls')
            refined_expts.select_on_experiment_identifiers(scaled_identifiers)
            refined_refls = refined_refls.select(refined_expts)
            elen, rlen = self.calc_expt_refl_len(refined_expts, refined_refls)
            print(f'Accepted {elen} expts and {sum(rlen)} refls')
            self.scrap_dict.update({'expts': elen, 'refls': rlen})
            self.scrap_origin(refined_expts)
            self.scrap_unit_cell(refined_expts)
            self.scrap_origin_deltas(refined_expts, refined_refls)
          except (KeyError, IndexError, JSONDecodeError) as e:
            print(e)
          else:
            self.scrap_results.append(self.scrap_dict)


class DriftScraperMixin(object):
  scrap_dict: Dict[str, Union[str, float, Sequence]]


class FirstOriginMixin(DriftScraperMixin):
  @autoupdate_scrap_dict_with_return
  def scrap_origin(self, expts: ExperimentList) -> Dict[str, float]:
    """Read detector origin (x, y, z) from the first expt file only"""
    x, y, z = expts[0].detector.hierarchy().get_origin()
    return {'x': x, 'y': y, 'z': z}


class AverageOriginMixin(DriftScraperMixin):
  @autoupdate_scrap_dict_with_return
  def scrap_origin(self, expts: ExperimentList) -> Dict[str, float]:
    """Read detector origin (x, y, z) from all files & return their average"""
    xs, ys, zs = flex.double(), flex.double(), flex.double()
    for expt in expts:
      x, y, z = expt.detector.hierarchy().get_origin()
      xs.append(x)
      ys.append(y)
      zs.append(z)
    weights = self.scrap_dict['refls']
    return {k: average(v, weights) for k, v in zip('xyz', (xs, ys, zs))}


class DistributionOriginMixin(DriftScraperMixin):
  @autoupdate_scrap_dict_with_return
  def scrap_origin(self, expts: ExperimentList) -> Dict[str, flex.double]:
    """Read detector origin (x, y, z) from all files & return flex with all"""
    xs, ys, zs = flex.double(), flex.double(), flex.double()
    for expt in expts:
      x, y, z = expt.detector.hierarchy().get_origin()
      xs.append(x)
      ys.append(y)
      zs.append(z)
    return {'x': xs, 'y': ys, 'z': zs}


class FirstPanelCOMOriginMixin(DriftScraperMixin):
  @autoupdate_scrap_dict_with_return
  def scrap_origin(self, expts: ExperimentList) -> Dict[str, float]:
    """Read average (x, y, z) position of all detector panels in first expt"""
    center_of_mass = np.array((0, 0, 0), dtype=float)
    detector = expts[0].detector
    for panel in detector:
      fast, slow = panel.get_image_size()
      for point in (0, 0), (fast - 1, 0), (0, slow - 1), (fast - 1, slow - 1):
        center_of_mass += np.array(panel.get_pixel_lab_coord(point))
    center_of_mass /= 4 * len(detector)
    return {xyz: com_xyz for xyz, com_xyz in zip('xyz', center_of_mass)}


class AveragePanelCOMOriginMixin(DriftScraperMixin):
  @autoupdate_scrap_dict_with_return
  def scrap_origin(self, expts: ExperimentList) -> Dict[str, float]:
    """Read average (x, y, z) position of all detector panels in all expts"""
    centers_of_mass = np.zeros(shape=(len(expts), 3), dtype=float)
    for i, expt in enumerate(expts):
      detector = expt.detector
      for panel in detector:
        fast, slow = panel.get_image_size()
        for point in (0, 0), (fast - 1, 0), (0, slow - 1), (fast - 1, slow - 1):
          centers_of_mass[i] += np.array(panel.get_pixel_lab_coord(point))
      centers_of_mass[i] /= 4 * len(expt.detector)
    weights = self.scrap_dict['refls']
    return {'x': average(centers_of_mass[:, 0], weights),
            'y': average(centers_of_mass[:, 1], weights),
            'z': average(centers_of_mass[:, 2], weights)}


class DistributionPanelCOMOriginMixin(DriftScraperMixin):
  @autoupdate_scrap_dict_with_return
  def scrap_origin(self, expts: ExperimentList) -> Dict[str, flex.double]:
    """Read average (x, y, z) position of all detector panels in every expt"""
    centers_of_mass = np.zeros(shape=(len(expts), 3), dtype=float)
    for i, expt in enumerate(expts):
      detector = expt.detector
      for panel in detector:
        fast, slow = panel.get_image_size()
        for point in (0, 0), (fast - 1, 0), (0, slow - 1), (fast - 1, slow - 1):
          centers_of_mass[i] += np.array(panel.get_pixel_lab_coord(point))
      centers_of_mass[i] /= 4 * len(expt.detector)
    return {'x': flex.double(np.copy(centers_of_mass[:, 0])),
            'y': flex.double(np.copy(centers_of_mass[:, 1])),
            'z': flex.double(np.copy(centers_of_mass[:, 2]))}


class FalseUncertaintiesMixin(DriftScraperMixin):
  @autoupdate_scrap_dict_with_return
  def scrap_origin_deltas(self, *_) -> Dict[str, float]:
    """If uncertainties=False, return dummy zero origin uncertainties"""
    return {'delta_x': 0., 'delta_y': 0., 'delta_z': 0.}


class TrueUncertaintiesMixin(DriftScraperMixin):
  @autoupdate_scrap_dict_with_return
  def scrap_origin_deltas(self, expts: ExperimentList,
                          refls: flex.reflection_table) -> Dict[str, float]:
    """Get uncertainties of origin positions from refl. position deviations"""
    deltas_flex = flex.vec3_double()
    for panel in expts[0].detector:
      pr = refls.select(refls['panel'] == panel.index())      # pr: panel refls
      pr_obs_det = pr['xyzobs.mm.value'].parts()[0:2]  # det: in detector space
      pr_cal_det = pr['xyzcal.mm'].parts()[0:2]        # lab: in labor. space
      pr_obs_lab = panel.get_lab_coord(flex.vec2_double(*pr_obs_det))
      pr_cal_lab = panel.get_lab_coord(flex.vec2_double(*pr_cal_det))
      deltas_flex.extend(pr_obs_lab - pr_cal_lab)
    d = [flex.mean(flex.abs(deltas_flex.parts()[i])) for i in range(3)]
    return {'delta_x': d[0], 'delta_y': d[1], 'delta_z': d[2]}


class BaseUnitCellMixin(DriftScraperMixin):
  @staticmethod
  def _write_tdata(expts: ExperimentList, tdata_path: str) -> None:
    """Read all expt_paths and write a tdata file with unit cells in lines"""
    s = '{:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {}'
    tdata_lines = []
    for expt in expts:
      uc_params = expt.crystal.get_unit_cell().parameters()
      sg = expt.crystal.get_space_group().type().universal_hermann_mauguin_symbol()
      tdata_lines.append(s.format(*uc_params, sg.replace(' ', '')))
    with open(tdata_path, 'w') as tdata_file:
      tdata_file.write('\n'.join(tdata_lines))


class AverageUnitCellMixin(BaseUnitCellMixin):
  @autoupdate_scrap_dict_with_return
  def scrap_unit_cell(self, expts: ExperimentList) -> Dict[str, float]:
    """Retrieve average a, b, c and their deltas using expt paths"""
    af, bf, cf = flex.double(), flex.double(), flex.double()
    with tempfile.NamedTemporaryFile() as tdata_file:
      self._write_tdata(expts, tdata_file.name)
      with open(tdata_file.name, 'r') as tdata:
        for line in tdata.read().splitlines():
          a, b, c = line.strip().split(' ')[:3]
          af.append(float(a))
          bf.append(float(b))
          cf.append(float(c))
    weights = self.scrap_dict['refls']  # weights
    return {'a': average(af, weights),
            'b': average(bf, weights),
            'c': average(cf, weights),
            'delta_a': np.sqrt(variance(af, weights)),
            'delta_b': np.sqrt(variance(bf, weights)),
            'delta_c': np.sqrt(variance(cf, weights))}


class DistributionUnitCellMixin(BaseUnitCellMixin):
  @autoupdate_scrap_dict_with_return
  def scrap_unit_cell(self, expts: ExperimentList) \
          -> Dict[str, Union[flex.double, float]]:
    """Retrieve distribution of a, b, c and their deltas using expt paths"""
    af, bf, cf = flex.double(), flex.double(), flex.double()
    with tempfile.NamedTemporaryFile() as tdata_file:
      self._write_tdata(expts, tdata_file.name)
      with open(tdata_file.name, 'r') as tdata:
        for line in tdata.read().splitlines():
          a, b, c = line.strip().split(' ')[:3]
          af.append(float(a))
          bf.append(float(b))
          cf.append(float(c))
    weights = self.scrap_dict['refls']
    return {'a': af, 'b': bf, 'c': cf,
            'delta_a': np.sqrt(variance(af, weights)),
            'delta_b': np.sqrt(variance(bf, weights)),
            'delta_c': np.sqrt(variance(cf, weights))}


class DriftScraperFactory(object):
  """Produces appropriate DriftScraper class based on phil `parameters`."""
  ORIGIN_MIXINS = {
    'first': FirstOriginMixin,
    'average': AverageOriginMixin,
    'distribution': DistributionOriginMixin,
    'panel_com_first': FirstPanelCOMOriginMixin,
    'panel_com_average': AveragePanelCOMOriginMixin,
    'panel_com_distribution': DistributionPanelCOMOriginMixin,
  }
  UNCERTAINTIES_MIXINS = {
    True: TrueUncertaintiesMixin,
    False: FalseUncertaintiesMixin,
  }
  UNIT_CELL_MIXINS = {
    'average': AverageUnitCellMixin,
    'distribution': DistributionUnitCellMixin,
  }

  @classmethod
  def get_drift_scraper(cls, table: DriftTable, parameters) \
          -> BaseDriftScraper:
    base = DriftScraperRegistrar.REGISTRY[parameters.scrap.input.kind]
    mixins = [
      cls.ORIGIN_MIXINS[parameters.scrap.origin],
      cls.UNCERTAINTIES_MIXINS[parameters.scrap.uncertainties],
      cls.UNIT_CELL_MIXINS[parameters.scrap.unit_cell],
    ]
    class DriftScraper(base, *mixins):
      """The actual data scraping class generated based on phil parameters"""
    return DriftScraper(table=table, parameters=parameters)


############################## DRIFT VISUALIZING ##############################


class DriftArtist(object):
  """Object responsible for plotting an instance of `DriftTable`."""
  def __init__(self, table: DriftTable, parameters):
    self.colormap = plt.get_cmap('tab10')
    self.colormap_period = 10
    self.corr_colormap = plt.get_cmap(parameters.plot.color.correlation)
    self.dist_colormap = plt.get_cmap(parameters.plot.color.distribution)
    dbc = str(parameters.plot.color.distribution_bg)
    self.dist_bg_col = self.dist_colormap(0) if dbc.lower() == 'auto' else dbc
    self.order_by = ['run', 'chunk']
    self.table = table
    self.table_flat: pd.DataFrame
    self.parameters = parameters
    self._init_figure()
    self._setup_figure()

  def _init_figure(self) -> None:
    self.fig = plt.figure(tight_layout=True)
    gs = GridSpec(7, 2, hspace=0, wspace=0, width_ratios=[4, 1],
                  height_ratios=[2, 3, 3, 3, 3, 3, 3])
    self.axh = self.fig.add_subplot(gs[0, 0])
    self.axx = self.fig.add_subplot(gs[1, 0], sharex=self.axh)
    self.axy = self.fig.add_subplot(gs[2, 0], sharex=self.axh)
    self.axz = self.fig.add_subplot(gs[3, 0], sharex=self.axh)
    self.axa = self.fig.add_subplot(gs[4, 0], sharex=self.axh)
    self.axb = self.fig.add_subplot(gs[5, 0], sharex=self.axh)
    self.axc = self.fig.add_subplot(gs[6, 0], sharex=self.axh)
    self.axw = self.fig.add_subplot(gs[0, 1])
    self.axl = self.fig.add_subplot(gs[1:, 1])

  def _setup_figure(self) -> None:
    self.axl.axis('off')
    self.axw.axis('off')
    self.axh.spines['top'].set_visible(False)
    self.axh.spines['right'].set_visible(False)
    self.axh.spines['bottom'].set_visible(False)
    self.axh.set_zorder(self.axx.get_zorder() - 1.0)
    common = {'direction': 'inout', 'top': True, 'bottom': True, 'length': 6}
    main_axes = self.axx, self.axy, self.axz, self.axa, self.axb, self.axc
    for ax, label in zip(main_axes, ['X', 'Y', 'Z', 'a', 'b', 'c']):
      ax.set_ylabel(label)
      ax.tick_params(axis='x', labelbottom=False, **common)
      ax.ticklabel_format(useOffset=False)
    self.axc.tick_params(axis='x', labelbottom=True, rotation=90)

  @property
  def color_array(self) -> List:
    """Registry-length color list with colors corresponding to plot.color.by"""
    color_by = self.parameters.plot.color.by
    color_id_map = {v: i for i, v in enumerate(self.table[color_by].unique())}
    color_ids = [color_id_map[v] for v in self.table[color_by].values]
    return [self.colormap(i % self.colormap_period) for i in color_ids]

  @property
  def x(self) -> pd.Index:
    return self.table.data.index

  @property
  def x_keys(self) -> List[str]:
    is_constant = {k: self.table[k].nunique() == 1 for k in self.order_by[1:]}
    keys_used = [self.order_by[0]]
    keys_used += [k for k in self.order_by[1:] if not is_constant[k]]
    return keys_used

  @property
  def x_label(self) -> str:
    return ':'.join(self.x_keys)

  @property
  def x_tick_labels(self) -> pd.Series:
    return self.table.data[self.x_keys].apply(
      lambda x: ':'.join(x.astype(str)), axis=1)

  def _get_handles_and_labels(self) -> Tuple[List, List]:
    handles, unique_keys = [], []
    for key in self.table[self.parameters.plot.color.by]:
      if key not in unique_keys:
        handles.append(Line2D([], [], c=self.colormap(len(unique_keys) % 10),
                              ls='', ms=12, marker='.'))
        unique_keys.append(key)
    return handles, unique_keys

  def _plot_init(self) -> None:
    self.table.sort(by=self.order_by)
    self.table_flat = self.table.flat
    self.axc.set_xlabel(self.x_label)
    self.axh.set_ylabel('# expts')

  def _plot_bars(self) -> None:
    y = self.table['expts']
    w = normalize([0, *self.table['density']])[1:]
    self.axh.bar(self.x, y, width=w, color=self.color_array, alpha=0.5)
    ax_top = self.axx.secondary_xaxis('top')
    ax_top.tick_params(rotation=90)
    ax_top.xaxis.set_major_locator(FixedLocator(self.x))
    ax_top.set_xticklabels(self.table['expts'])

  def _plot_correlations(self) -> None:
    keys = ['x', 'y', 'z', 'a', 'b', 'c']
    flat_columns = (self.table_flat[key] for key in keys)
    correlated = {col.name: col.values for col in flat_columns}
    cm = CorrelationMatrix(correlated, weights=self.table_flat['refls'])
    print(cm)
    self.axw.set_xlim([0, len(keys)])
    self.axw.set_ylim([0, len(keys)])
    for ix, kx in enumerate(keys):
      for iy, ky in enumerate(keys):
        if ix == iy:
          self.axw.text(x=ix+0.5, y=len(keys)-iy-0.5, s=kx,
                        ha='center', va='center')
        if ix > iy:
          color = self.corr_colormap(normalize([cm.corr[kx][ky], -1, 1])[0])
          r = Rectangle(xy=(ix, len(keys) - iy), width=1, height=-1, fill=True,
                        ec='white', fc=color, linewidth=2)
          self.axw.add_patch(r)

  def _plot_drift(self, axes: plt.Axes, values_key: str,
                  deltas_key: str = None) -> None:
    axes.xaxis.set_major_locator(FixedLocator(self.x))
    y = self.table[values_key]
    if not self.table.column_is_flat(values_key):
      self._plot_drift_distribution(axes, values_key)
    else:
      self._plot_drift_point(axes, y, deltas_key)
    axes.set_xticklabels(self.x_tick_labels)
    flattened_y = self.table_flat[values_key]
    flattened_weights = self.table_flat['refls']
    avg_y = average(flattened_y, weights=flattened_weights)
    if avg_y != 0:
      axes2 = axes.twinx()
      axes2.set_ylim([lim / avg_y - 1 for lim in axes.get_ylim()])
      axes2.yaxis.set_major_formatter(PercentFormatter(xmax=1))

  def _plot_drift_point(self, axes: plt.Axes, y: Sequence,
                        deltas_key: str = None) -> None:
    axes.scatter(self.x, y, c=self.color_array)
    y_err = self.table.get(deltas_key, [])
    axes.errorbar(self.x, y, yerr=y_err, ecolor='black', ls='')

  def _plot_drift_distribution(self, axes: plt.Axes, values_key: str) -> None:
    axes.set_facecolor(self.dist_bg_col)
    x = self.table_flat['original_index']
    y = self.table_flat[values_key]
    weights = self.table_flat['refls']
    bins = (len(self.x), 100)
    ranges = [[-0.5, len(self.x) - 0.5], [min(y), max(y)]]
    axes.hist2d(x, y, weights=weights, bins=bins, range=ranges,
                cmap=self.dist_colormap, cmin=1E-10)
    axes.scatter(self.x, [average(val) for val in self.table[values_key]],
                 c=self.color_array, edgecolors='white')

  def _plot_legend(self) -> None:
    handles, labels = self._get_handles_and_labels()
    self.axl.legend(handles, labels, loc=7)

  def _plot_width_info(self) -> None:
    expt_lens = self.table['expts']
    refl_lens = self.table['refls'] if self.table.column_is_flat('refls') \
      else [sum(refl) for refl in self.table['refls']]
    s = f"#expts/chunk: {min(expt_lens)} - {max(expt_lens)}\n" \
        f"#refls/chunk: {min(refl_lens)} - {max(refl_lens)}"
    self.axl.text(x=0.5, y=0., s=s, clip_on=False, ha='center',
                  ma='center', va='top', transform=self.axl.transAxes)

  def plot(self) -> None:
    if len(self.table):
      self._plot_init()
      self._plot_bars()
      self._plot_correlations()
      self._plot_drift(self.axx, 'x', 'delta_x')
      self._plot_drift(self.axy, 'y', 'delta_y')
      self._plot_drift(self.axz, 'z', 'delta_z')
      self._plot_drift(self.axa, 'a', 'delta_a')
      self._plot_drift(self.axb, 'b', 'delta_b')
      self._plot_drift(self.axc, 'c', 'delta_c')
      self._plot_width_info()
      self._plot_legend()
    self.fig.align_labels()
    if self.parameters.plot.save:
      self.fig.set_size_inches(self.parameters.plot.width,
                               self.parameters.plot.height)
      self.fig.savefig(self.parameters.plot.path)
    if self.parameters.plot.show:
      plt.show()


################################ ENTRY POINTS #################################


def run(params_):
  dt = DriftTable()
  ds = DriftScraperFactory.get_drift_scraper(table=dt, parameters=params_)
  da = DriftArtist(table=dt, parameters=params_)
  ds.scrap()
  print(dt)
  da.plot()


params = []
if __name__ == '__main__':
  if '--help' in sys.argv[1:] or '-h' in sys.argv[1:]:
    print(message)
    exit()
  params = params_from_phil(phil_scope, sys.argv[1:])
  run(params)


 *******************************************************************************


 *******************************************************************************
xfel/util/image_pickle_locator.py
from __future__ import absolute_import, division, print_function

from libtbx.utils import Sorry
import os
import six

def get_image_filename(int_name, prefix="idx-"):
  f = os.path.basename(int_name)
  position = f.index(":")
  middle = f[position-13:position+10]
  suffix = f[position+10:].split("_00000")[-1]
  compressed = middle[0:4] + middle[5:7] + middle[8:10] + middle[11:13] + middle[14:16] + middle[17:19] + middle[20:23]
  return prefix+compressed+suffix

def limited_walk(path, max_depth=4):
  if os.path.isfile(path):
    return [path]
  elif max_depth <= 0:
    return []
  nodes = []
  if os.path.isdir(path):
    for node in os.listdir(path):
      nodes.extend(limited_walk(os.path.join(path, node), max_depth=max_depth-1))
  return nodes

def find_image_in_dirs(dirs, image_name=None, int_name=None, prefix="idx-", max_depth=4, sorry_on_fail=False):
  assert not isinstance(dirs, six.string_types)
  if image_name is None:
    if int_name is None:
      raise Sorry("either image name or integration pickle name must be specified")
    image_name = get_image_filename(int_name, prefix=prefix)
  for d in dirs:
    paths = limited_walk(d, max_depth=max_depth)
    filenames = [os.path.basename(p) for p in paths]
    if image_name in filenames:
      return paths[filenames.index(image_name)]
  if sorry_on_fail:
    raise Sorry("could not locate image")
  return None

def find_many_images_in_dirs(images, dirs, max_depth=4):
  assert not isinstance(dirs, six.string_types)
  image_paths = []
  for d in dirs:
    paths = limited_walk(d, max_depth=max_depth)
    filenames = [os.path.basename(p) for p in paths]
    for image_name in images:
      if image_name in filenames:
        image_paths.append(paths[filenames.index(image_name)])
  return image_paths

def find_all_images_in_dirs(dirs, image_prefix="idx-", max_depth=4):
  assert not isinstance(dirs, six.string_types)
  image_paths = []
  for d in dirs:
    paths = limited_walk(d, max_depth=max_depth)
    filenames = [os.path.basename(p) for p in paths]
    for f in filenames:
      if f.endswith(".pickle") and f.startswith(image_prefix):
        image_paths.append(paths[filenames.index(image_name)])
  return image_paths

def find_all_matching_images_in_dirs(int_pickles, dirs, prefix="idx-", max_depth=4):
  assert not isinstance(dirs, six.string_types)
  image_names = [get_image_filename(int_name, prefix=prefix) for int_name in int_pickles]
  image_paths = []
  for d in dirs:

      for i in image_names:
        if i in filenames:
          image_paths.append(os.path.join(os.path.join(dirpath, *dirnames), i))
  image_names_from_paths = [os.path.basename(p) for p in image_paths]
  image_paths_sorted = []
  for name in image_names:
    idx = image_names_from_paths.index(name)
    path = image_paths[idx]
    image_paths_sorted.append(path)
  return image_paths_sorted


 *******************************************************************************


 *******************************************************************************
xfel/util/kapton_2019_correction_frame_frame_plugin.py
from __future__ import division
# -*- Mode: Python; c-basic-offset: 2; indent-tabs-mode: nil; tab-width: 8 -*-
#
# $Id
from six.moves import range

"""
Plugin to calibrate kapton correction parameters for an XFEL image
"""

import wx
from dials.algorithms.integration.kapton_2019_correction import KaptonTape_2019
from scitbx.matrix import col

class KaptonSettingsFrame(wx.MiniFrame):
  def __init__ (self, *args, **kwds):
    super(KaptonSettingsFrame, self).__init__(*args, **kwds)
    szr = wx.BoxSizer(wx.VERTICAL)
    self.phil_params = args[0].params
    panel = KaptonSettingsPanel(self)
    self.SetSizer(szr)
    szr.Add(panel, 1, wx.EXPAND)
    szr.Fit(panel)
    self.panel = panel
    self.sizer = szr
    self.Fit()
    self.Bind(wx.EVT_CLOSE, lambda evt : self.Destroy(), self)


class KaptonSettingsPanel(wx.Panel):
  def __init__ (self, *args, **kwds) :
    super(KaptonSettingsPanel, self).__init__(*args, **kwds)

    # self.phil_params = args[0].phil_params
    from wx.lib.agw.floatspin import EVT_FLOATSPIN, FloatSpin

    # Needed to draw and delete overlays
    self._pyslip = self.GetParent().GetParent().pyslip

    sizer = wx.BoxSizer(wx.VERTICAL)
    self.SetSizer(sizer)

    # Number of decimal digits for distances.
    self.digits = 4

    # Kapton tape controls and params from phil
    self._xtal_height_mm = 0.035 # LN84 shift 2
    self._tape_thickness_mm = 0.05 # LN84 kapton tape
    self._tape_half_width_mm = 3.175/4. # LN84 kapton tape
    self._tape_angle_deg = 178.8-180.0 # LN84 shift 2 tape drive setup
    self._center = [0, 0]
    self.frame          = self.GetParent().GetParent()
    self.detector       = self._pyslip.tiles.raw_image.get_detector()
    self.panels         = [p for p in self.detector]
    self.beam           = self._pyslip.tiles.raw_image.get_beam()
    self.wavelength_ang = self.beam.get_wavelength()
    self.pixels_size_mm = [p.get_pixel_size()[0] for p in self.panels]
    self.distance       = [p.get_directed_distance() for p in self.panels]




    # determine beam center and detector limits in pixels
    # data reads slow,fast but the dxtbx objects are organized fast,slow -- label explicitly
    self.panel_size = [p.get_image_size() for p in self.panels]
    #self.size_fast, self.size_slow = self.detector[0].get_image_size()
    origin = [p.get_parent_origin()[:2] for p in self.panels]
    #orig_fast, orig_slow = self.detector[0].get_parent_origin()[:2]
    #self.min_px=[map(int,self.panels[ii].millimeter_to_pixel(ori)) for ii,ori in enumerate(origin)]
    self.min_px = [
        [int(x) for x in self.panels[ii].millimeter_to_pixel(ori)]
        for ii, ori in enumerate(origin)
        ]
    #self.fast_min, self.slow_min = map(int, self.detector[0].millimeter_to_pixel((orig_fast, orig_slow)))
    self.max_px=[(self.panel_size[ii][0]+min_px[0]-1,self.panel_size[ii][1]+min_px[1]-1) for ii,min_px in enumerate(self.min_px)]
    #self.fast_max, self.slow_max = map(lambda i: i-1,
    #  (self.size_fast + self.fast_min, self.size_slow + self.slow_min))
    self.s0=[map(int, p.get_beam_centre_px(self.beam.get_s0())) for p in self.panels]
    #self.s0_fast, self.s0_slow = map(int, self.detector[0].get_beam_centre_px(self.beam.get_s0()))
    panel_id, beam_pixel_fast, beam_pixel_slow = self.frame.get_beam_center_px()

    if len(self.detector) > 1:
      beam_pixel_slow, beam_pixel_fast = self.frame.pyslip.tiles.flex_image.tile_readout_to_picture(
        panel_id, beam_pixel_slow - 0.5, beam_pixel_fast - 0.5)

    self.center_lon_lat = self._pyslip.tiles.picture_fast_slow_to_map_relative(
      beam_pixel_fast + self._center[0], beam_pixel_slow + self._center[1])

    #self.center_lon_lat = self._pyslip.tiles.picture_fast_slow_to_map_relative(self.s0_fast, self.s0_slow)

    self._absorption = None

    self._kapton_control_names = ["xtal_height_ctrl", "tape_thickness_ctrl", "tape_width_ctrl",
    "tape_angle_ctrl", "show_shadow_button", "show_contours_button", "show_edge_max_button",
    "save_params"]

    box = wx.BoxSizer(wx.HORIZONTAL)
    self.height = FloatSpin(
      self, digits=self.digits, name=self._kapton_control_names[0], value=self._xtal_height_mm)
    self.height.SetIncrement(0.005)
    self.height.SetRange(0.005, None)
    box.Add(self.height,
            0, wx.RIGHT | wx.TOP | wx.BOTTOM | wx.ALIGN_CENTER_VERTICAL, 5)
    box.Add(wx.StaticText(self, label="Crystal height (mm)"),
            0, wx.ALL | wx.ALIGN_CENTER_VERTICAL, 5)
    self.Bind(EVT_FLOATSPIN, self.OnSpinHeight, self.height)
    sizer.Add(box)


    box = wx.BoxSizer(wx.HORIZONTAL)
    self.thickness = FloatSpin(
      self, digits=self.digits, name=self._kapton_control_names[1], value=self._tape_thickness_mm)
    self.thickness.SetIncrement(0.001)
    self.thickness.SetRange(0.001, None)
    box.Add(self.thickness,
            0, wx.RIGHT | wx.TOP | wx.BOTTOM | wx.ALIGN_CENTER_VERTICAL, 5)
    box.Add(wx.StaticText(self, label="Kapton tape thickness (mm)"),
            0, wx.ALL | wx.ALIGN_CENTER_VERTICAL, 5)
    self.Bind(EVT_FLOATSPIN, self.OnSpinThickness, self.thickness)
    sizer.Add(box)


    box = wx.BoxSizer(wx.HORIZONTAL)
    self.width = FloatSpin(
      self, digits=self.digits, name=self._kapton_control_names[2], value=self._tape_half_width_mm)
    self.width.SetIncrement(0.005)
    self.width.SetRange(0.005, None)
    box.Add(self.width,
            0, wx.RIGHT | wx.TOP | wx.BOTTOM | wx.ALIGN_CENTER_VERTICAL, 5)
    box.Add(wx.StaticText(self, label="Kapton tape half width (mm)"),
            0, wx.ALL | wx.ALIGN_CENTER_VERTICAL, 5)
    self.Bind(EVT_FLOATSPIN, self.OnSpinWidth, self.width)
    sizer.Add(box)


    box = wx.BoxSizer(wx.HORIZONTAL)
    self.angle = FloatSpin(
      self, digits=self.digits, name=self._kapton_control_names[3], value=self._tape_angle_deg)
    self.angle.SetIncrement(0.05)
    self.angle.SetRange(-360, 360)
    box.Add(self.angle,
            0, wx.RIGHT | wx.TOP | wx.BOTTOM | wx.ALIGN_CENTER_VERTICAL, 5)
    box.Add(wx.StaticText(self, label="Kapton tape counterclockwise rotation from vertical (deg)"),
            0, wx.ALL | wx.ALIGN_CENTER_VERTICAL, 5)
    self.Bind(EVT_FLOATSPIN, self.OnSpinAngle, self.angle)
    sizer.Add(box)

    box = wx.BoxSizer(wx.HORIZONTAL)
    self.show_shadow = wx.Button(
      self, name=self._kapton_control_names[4], label="show kapton absorption as shadow")
    box.Add(self.show_shadow,
            0, wx.RIGHT | wx.TOP | wx.BOTTOM | wx.ALIGN_CENTER_VERTICAL, 5)
    self.Bind(wx.EVT_BUTTON, self.OnShowShadow, self.show_shadow)
    sizer.Add(box)

    box = wx.BoxSizer(wx.HORIZONTAL)
    self.show_contours = wx.Button(
      self, name=self._kapton_control_names[5], label="show kapton absorption as contours")
    box.Add(self.show_contours,
            0, wx.RIGHT | wx.TOP | wx.BOTTOM | wx.ALIGN_CENTER_VERTICAL, 5)
    self.Bind(wx.EVT_BUTTON, self.OnShowContours, self.show_contours)
    sizer.Add(box)

    box = wx.BoxSizer(wx.HORIZONTAL)
    self.show_edge_max = wx.Button(
      self, name=self._kapton_control_names[6], label="show kapton absorption edge and max")
    box.Add(self.show_edge_max,
            0, wx.RIGHT | wx.TOP | wx.BOTTOM | wx.ALIGN_CENTER_VERTICAL, 5)
    self.Bind(wx.EVT_BUTTON, self.OnShowEdgeMax, self.show_edge_max)
    sizer.Add(box)

    box = wx.BoxSizer(wx.HORIZONTAL)
    self.save_phil = wx.Button(
      self, name=self._kapton_control_names[7], label="save params and go to next image")
    box.Add(self.save_phil,
            0, wx.RIGHT | wx.TOP | wx.BOTTOM | wx.ALIGN_CENTER_VERTICAL, 5)
    self.Bind(wx.EVT_BUTTON, self.OnSavePhil, self.save_phil)
    sizer.Add(box)

  def update_absorption(self):
    self._absorption = KaptonTape_2019(self._xtal_height_mm,
                                        self._tape_thickness_mm,
                                        self._tape_half_width_mm,
                                        self._tape_angle_deg,
                                        wavelength_ang=self.wavelength_ang)

  def __del__(self):
    if (hasattr(self, "_shadow_layer") and self._shadow_layer is not None):
      self._pyslip.DeleteLayer(self._shadow_layer)
    if (hasattr(self, "_contours_layer") and self._contours_layer is not None):
      self._pyslip.DeleteLayer(self._contours_layer)
    if (hasattr(self, "_edge_max_layer") and self._edge_max_layer is not None):
      self._pyslip.DeleteLayer(self._edge_max_layer)

  def OnSpinHeight(self, event):
    obj = event.EventObject
    name = obj.GetName()

    self._xtal_height_mm = obj.GetValue()
    self.UpdateAbsorptionData()

  def OnSpinThickness(self, event):
    obj = event.EventObject
    name = obj.GetName()

    self._tape_thickness_mm = obj.GetValue()
    self.UpdateAbsorptionData()

  def OnSpinWidth(self, event):
    obj = event.EventObject
    name = obj.GetName()

    self._tape_half_width_mm = obj.GetValue()
    self.UpdateAbsorptionData()

  def OnSpinAngle(self, event):
    obj = event.EventObject
    name = obj.GetName()

    self._tape_angle_deg = obj.GetValue()
    self.UpdateAbsorptionData()

  def OnSpin(self, event):
    self.UpdateAbsorptionData()

  def OnShowShadow(self, event):
    if (hasattr(self, "_shadow_layer") and self._shadow_layer is not None):
      self._pyslip.DeleteLayer(self._shadow_layer)
      self._shadow_layer = None
    else:
      self.DrawShadow()

  def OnShowContours(self, event):
    if (hasattr(self, "_contours_layer") and self._contours_layer is not None):
      self._pyslip.DeleteLayer(self._contours_layer)
      self._contours_layer = None
    else:
      self.DrawContours()

  def OnShowEdgeMax(self, event):
    if (hasattr(self, "_edge_max_layer") and self._edge_max_layer is not None):
      self._pyslip.DeleteLayer(self._edge_max_layer)
      self._edge_max_layer = None
    else:
      self.DrawEdgeMax()

  def OnSavePhil(self, event):
    import os
    if not (hasattr(self, "_phil_destination_dir") and self._phil_destination_dir is not None):
      self._run = os.path.basename(self.frame._img._raw.path).split(".pickle")[0].split("-")[-1]
      name = "Kapton_correction_%s.phil" % self._run
      dialog = wx.FileDialog(
        self,
        defaultDir='',
        defaultFile=name,
        message="Choose Kapton parameter snippet destination",
        style=wx.FD_SAVE,
        wildcard="*")
      if dialog.ShowModal() == wx.ID_OK:
        dest_path = dialog.GetPath()
        if os.path.isdir(dest_path):
          self._phil_destination_dir = dest_path
          self._phil_destination_path = os.path.join(self._phil_destination_dir, name)
        else:
          self._phil_destination_dir = os.path.dirname(dest_path)
          self._phil_destination_path = dest_path
    else:
      prev_run = self._run
      self._run = os.path.basename(self.frame._img._raw.path).split(".pickle")[0].split("-")[-1]
      name_parts = os.path.basename(self._phil_destination_path).split(prev_run)
      name = name_parts[0] + self._run + name_parts[1]
      self._phil_destination_path = os.path.join(self._phil_destination_dir, name)
    wb = open(self._phil_destination_path, "wb")
    wb.write(b"integration {\n")
    wb.write(b"  absorption_correction {\n")
    wb.write(b"    apply = True\n")
    wb.write(b"    algorithm = fuller_kapton\n")
    wb.write(b"    fuller_kapton.xtal_height_above_kapton_mm {\n")
    wb.write(b"      value = %f\n" % self._xtal_height_mm)
    wb.write(b"      }\n")
    wb.write(b"    fuller_kapton.rotation_angle_deg {\n")
    wb.write(b"      value = %f\n" % self._tape_angle_deg)
    wb.write(b"      }\n")
    wb.write(b"    fuller_kapton.kapton_half_width_mm {\n")
    wb.write(b"       value = %f\n" % self._tape_half_width_mm)
    wb.write(b"      }\n")
    wb.write(b"    fuller_kapton.kapton_thickness_mm {\n")
    wb.write(b"       value = %f\n" % self._tape_thickness_mm)
    wb.write(b"      }\n")
    wb.write(b"    fuller_kapton.smart_sigmas = True\n")
    wb.write(b"  }\n")
    wb.write(b"}\n")
    wb.close()
    print("Wrote phil snippet to", self._phil_destination_path)
    if not hasattr(self, "_root_frame") or self._root_frame is None:
      self._root_frame = self._pyslip.GetParent().GetParent()
    self._root_frame.OnNext(event)

  def _draw_shadow_layer(self, dc, data, map_rel):
    """Draw a points layer.

    dc       the device context to draw on
    data     an iterable of point tuples:
             (x, y, place, radius, colour, x_off, y_off, pdata)
    map_rel  points relative to map if True, MUST BE TRUE for lightweight
    Assumes all points are the same colour, saving 100's of ms.
    """

    assert map_rel is True
    if len(data)==0:
      return
    (lon, lat, place, radius, colour, x_off, y_off, pdata) = data[0]

    scale = 2**self._pyslip.tiles.zoom_level

    # Draw points on map/view, using transparency if implemented.
    try:
      dc = wx.GCDC(dc)
    except NotImplementedError:
      pass
    dc.SetBrush(wx.Brush(colour, wx.TRANSPARENT))
    for (lon, lat, place, radius, colour, x_off, y_off, pdata) in data:
      dc.SetPen(wx.Pen(colour))
      (x, y) = self._pyslip.ConvertGeo2View((lon, lat))
      dc.DrawCircle(x, y, 1)
      if colour == "purple":
        dc.DrawCircle(x, y, 10)
      pass

  def _draw_contours_layer(self, dc, data, map_rel):
    """Draw a layer consisting of differently colored contours.

    dc       the device context to draw on
    data     a list of dictionaries:
             {"Paths":[Path, Path, ...], "level":level, "color":color}
             each Path is an iterable of line segments to be overlaid on the pyslip image
             level is the contour level of the Paths
             colour is the colour to be applied to the contour lines at this level
    map_rel  points relative to map if True, MUST BE TRUE for lightweight
    """

    assert map_rel is True
    if len(data) == 0:
      return
    scale = 2**self._pyslip.tiles.zoom_level

    # Draw line segments of the appropriate color on the pyslip map object
    try:
      dc = wx.GCDC(dc)
    except NotImplementedError:
      pass
    for contour_level in data:
      path_list = contour_level["Paths"]
      level = contour_level["level"]
      colour = wx.Colour(*[(1-decimal)*255 for decimal in contour_level["colour"]])
      dc.SetBrush(wx.Brush(colour))
      dc.SetPen(wx.Pen(colour))
      for path in path_list:
        segments = path.iter_segments()
        this_vertex = segments.next()[0]
        vertex_1 = self._pyslip.ConvertGeo2View(this_vertex) # x, y = Geo2View(lon, lat)
        while True:
          try:
            this_vertex = segments.next()[0]
            vertex_2 = self._pyslip.ConvertGeo2View(this_vertex) # x, y = Geo2View(lon, lat)
            dc.DrawLine(vertex_1[0], vertex_1[1], vertex_2[0], vertex_2[1])
            vertex_1 = vertex_2
          except StopIteration:
            break

  def _draw_edge_max_layer(self, dc, data, map_rel):
    """Draw a layer consisting of two line segments, identifying the Kapton absorption edge and max.

    dc       the device context to draw on
    data     a list of line segments (lon1, lat1, lon2, lat2)
    map_rel  points relative to map if True, MUST BE TRUE for lightweight
    """
    assert map_rel is True
    if len(data) == 0:
      return
    scale = 2**self._pyslip.tiles.zoom_level

    # Draw line segments on the pyslip map object
    try:
      dc = wx.GCDC(dc)
    except NotImplementedError:
      pass
    dc.SetBrush(wx.Brush("black"))
    dc.SetPen(wx.Pen("black"))
    for segment in data:
      lon1, lat1, lon2, lat2 = segment
      x1, y1 = self._pyslip.ConvertGeo2View((lon1, lat1))
      x2, y2 = self._pyslip.ConvertGeo2View((lon2, lat2))
      dc.DrawLine(x1, y1, x2, y2)

  def _map_coords(self, x, y, p):
      y, x = self._pyslip.tiles.flex_image.tile_readout_to_picture(
          p, y - 0.5, x - 0.5)
      return self._pyslip.tiles.picture_fast_slow_to_map_relative(
        x, y)

  def UpdateAbsorptionData(self, edge_max_mode=False):
    from scitbx.array_family import flex
    #from IPython import embed; embed(); exit()
    self.update_absorption() # basically resets the self._absorption to a new KaptonTape class
    self.absorption_data = []
    if True: #not hasattr(self, 'cache_mode'):
      self.cache_mode=True
      self.all_s1_flex=flex.vec3_double()
      #self.all_lon_lat=[]
    #FIXME
      detector=self._pyslip.tiles.raw_image.get_detector()
      for panel in detector:
        image_size = panel.get_image_size()
        for f_dir in range(0, image_size[0], 10):
          for s_dir in range(0, image_size[1], 10):
            s1=panel.get_pixel_lab_coord((f_dir, s_dir))
            #self.all_s1_flex.append(s1)
            lon, lat = self._map_coords(float(f_dir), float(s_dir),panel.index())
            #self.all_lon_lat.append((lon,lat))
      #self.cache_mode=True

            absorption_correction = self._absorption.abs_correction(s1)
            self.absorption_data.append((lon, lat, absorption_correction))
    #absorption_corrections = self._absorption.abs_correction_flex(self.all_s1_flex)
    #self.absorption_data=[(x[0],x[1],y) for x,y in zip(self.all_lon_lat, absorption_corrections)]

    # Redraw layers if present
    if (hasattr(self, "_shadow_layer") and self._shadow_layer is not None):
      self.DrawShadow()
    if (hasattr(self, "_contours_layer") and self._contours_layer is not None):
      self.DrawContours()
    if (hasattr(self, "_edge_max_layer") and self._edge_max_layer is not None):
      self.DrawEdgeMax()

  def DrawShadow(self):
    if not hasattr(self, 'absorption_data'):
      self.UpdateAbsorptionData()
    shadow_data = []
    shadow_data.append((self.center_lon_lat[0], self.center_lon_lat[1], {"colour":"purple"}))
    for lon, lat, absorption_correction in self.absorption_data:
      if absorption_correction == 1:
        colour = "white"
      elif absorption_correction < 1.2:
        colour = "blue"
      elif absorption_correction < 1.4:
        colour = "green"
      elif absorption_correction < 1.6:
        colour = "yellow"
      elif absorption_correction < 1.8:
        colour = "orange"
      else:
        colour = "red"
      shadow_data.append((lon, lat, {"colour":colour}))

    # Remove the old shadow layer, and draw a new one.
    if (hasattr(self, "_shadow_layer") and self._shadow_layer is not None):
      self._pyslip.DeleteLayer(self._shadow_layer)
      self._shadow_layer = None
    self._shadow_layer = self._pyslip.AddPointLayer(
      shadow_data,
      map_rel=True,
      visible=True,
      show_levels=[-3, -2, -1, 0, 1, 2, 3, 4, 5],
      renderer=self._draw_shadow_layer,
      name="<shadow_layer>")

  def DrawContours(self):
    if not hasattr(self, 'absorption_data'):
      self.UpdateAbsorptionData()
    import matplotlib.pyplot as plt
    print ('DrawContours does not work currently. Please try the other options')
    return
    def convert_to_tile(slow_fast_ab_tup):
      slow, fast, ab = slow_fast_ab_tup
      local_lon, local_lat = self._pyslip.tiles.picture_fast_slow_to_map_relative(fast, slow)
      return (local_lon, local_lat, ab)
    lon, lat, absorptions = zip(*map(convert_to_tile, self.absorption_data))
    n_lon = len(set(lon))
    n_lat = len(set(lat))
    indices = []
    if lon[0] == lon[1]:
      indices = [[i_lon*n_lat + i_lat
        for i_lat in range(0, n_lat, 1)]
        for i_lon in range(0, n_lon, 1)]
    elif lat[0] == lat[1]:
      indices = [[i_lat*n_lon + i_lon
        for i_lon in range(0, n_lon, 1)]
        for i_lat in range(0, n_lat, 1)]
    else: # appears to be a non-rectangular array
      indices = [range(n_lon*n_lat)]
    lon_array = [[lon[i] for i in l] for l in indices]
    lat_array = [[lat[i] for i in l] for l in indices]
    abs_array = [[absorptions[i] for i in l] for l in indices]
    contours = plt.contour(lon_array, lat_array, abs_array)
    levels = list(contours.levels)
    def convert_level_to_rgb_colour(level):
      r, g, b, a = contours.cmap(level - 1)
      return (r, g, b)
    colours = map(convert_level_to_rgb_colour, levels)
    contour_paths = [{"Paths":contours.collections[i].get_paths(),
                     "level":levels[i],
                     "colour":colours[i]} for i in range(len(contours.collections))]

    # Remove the old contours layer, and draw a new one.
    if (hasattr(self, "_contours_layer") and self._contours_layer is not None):
      self._pyslip.DeleteLayer(self._contours_layer)
      self._contours_layer = None
    self._contours_layer = self._pyslip.AddLayer(
      self._draw_contours_layer,
      contour_paths,
      True, #map_rel
      True, #visible
      [-3, -2, -1, 0, 1, 2, 3, 4, 5], #show_levels
      False, #selectable
      "<contours_layer>", #name
      "contours")

  def DrawEdgeMax(self):
    if not hasattr(self, 'absorption_data'):
      self.UpdateAbsorptionData(edge_max_mode=True)
    detector=self._pyslip.tiles.raw_image.get_detector()
    beam=self._pyslip.tiles.raw_image.get_beam()
    xrayframe=self.GetParent().GetParent()
    int_with_det = self._absorption.abs_bounding_lines_on_image(detector)
    s0=beam.get_s0()
    panel_id, beam_pixel_fast, beam_pixel_slow=xrayframe.get_beam_center_px()
    if len(detector) > 1:
      beam_pixel_slow, beam_pixel_fast = xrayframe.pyslip.tiles.flex_image.tile_readout_to_picture(
        panel_id, beam_pixel_slow - 0.5, beam_pixel_fast - 0.5)
    int_with_det_px = []
    edge_max_data_slow_fast = []

    def panel_of_intersection(elem, detector):
      """Takes in  x1,y1,x2,y2 and returns the panel where these points lie. If outside any panel then returns None"""
      x1,y1,x2,y2=elem
      # FIXME tolerance levels, a bit hacky
      d1=3.09
      d2=-0.09
      int_panels=[None, None]
      for panel in detector:
        # Convert x,y to f,s for that panel
        z_panel=panel.get_distance()
        ori=col(panel.get_origin())
        r1 = col((x1,y1,-z_panel))-ori
        r2 = col((x2,y2,-z_panel))-ori
        f_u = col(panel.get_fast_axis())
        s_u = col(panel.get_slow_axis())
        f1 = r1.dot(f_u)
        s1 = r1.dot(s_u)
        f2 = r2.dot(f_u)
        s2 = r2.dot(s_u)
        detz=panel.get_distance()
        px1=panel.get_ray_intersection_px((f1,s1, -detz))
        px2=panel.get_ray_intersection_px((f2,s2, -detz))
        if px1[0]/panel.get_image_size()[0]<=d1 and px1[0]/panel.get_image_size()[0] >=d2 and \
           px1[1]/panel.get_image_size()[1]<=d1 and px1[1]/panel.get_image_size()[1] >=d2:
           int_panels[0]=panel
        if px2[0]/panel.get_image_size()[0]<=d1 and px2[0]/panel.get_image_size()[0] >=d2 and \
           px2[1]/panel.get_image_size()[1]<=d1 and px2[1]/panel.get_image_size()[1] >=d2:
           int_panels[1]=panel
      #from IPython import embed; embed(); exit()
      # FIXME
      if int_panels[0] is None and int_panels[1] is None:
        from libtbx.utils import Sorry
        raise Sorry("No intersecting panels ?? this is awful .. get out of here")
      if int_panels[0] is None: int_panels[0]=int_panels[1]
      if int_panels[1] is None: int_panels[1]=int_panels[0]
      return int_panels

    for elem in int_with_det:
      # Determine panel number for pts 1 and 2
      p1,p2=panel_of_intersection(elem, detector)
      pixel_size_1=p1.get_pixel_size()[0]
      pixel_size_2=p2.get_pixel_size()[0]
      if True:
        # for panel 1
        z_panel=p1.get_distance()
        ori=col(p1.get_origin())
        r1 = col((elem[0], elem[1], -z_panel))-ori
        f1_u = col(p1.get_fast_axis())
        s1_u = col(p1.get_slow_axis())
        f1=r1.dot(f1_u)
        s1=r1.dot(s1_u)
        # For panel 2
        z_panel=p2.get_distance()
        ori=col(p2.get_origin())
        r2 = col((elem[2], elem[3], -z_panel))-ori
        f2_u = col(p2.get_fast_axis())
        s2_u = col(p2.get_slow_axis())
        f2=r2.dot(f2_u)
        s2=r2.dot(s2_u)

        if True:
          # FIXME is this correct to comment out beam_pixel_fast / beam_pixel_slow
          px_f1=f1/pixel_size_1#+beam_pixel_fast
          px_s1=s1/pixel_size_1#+beam_pixel_slow
          px_f2=f2/pixel_size_2#+beam_pixel_fast
          px_s2=s2/pixel_size_2#+beam_pixel_slow
      # If debugging
      if False:
        px_f1=beam_pixel_fast
        px_s1=beam_pixel_slow
        px_f2=beam_pixel_fast+100
        px_s2=beam_pixel_slow
      edge_max_data_slow_fast.append((px_s1, px_f1, p1.index(), px_s2, px_f2, p2.index()))

    edge_max_data_lonlat = []
    kapton_data=[]

    for segment in edge_max_data_slow_fast:
      slow1, fast1, p1, slow2, fast2, p2= segment
      lon1, lat1 = self._map_coords(fast1,slow1,p1)#self._pyslip.tiles.picture_fast_slow_to_map_relative(fast1, slow1)
      lon2, lat2 = self._map_coords(fast2, slow2, p2)#self._pyslip.tiles.picture_fast_slow_to_map_relative(fast2, slow2)
      edge_max_data_lonlat.append((lon1, lat1, lon2, lat2))
      shoebox_dict = {'width': 2, 'color': '#0000FFA0', 'closed': False}

      if False:
        x0_, y0_=lon1,lat1
        x1_, y1_=lon2,lat2
        my_attrs = dict(shoebox_dict)
        lines = [(((x0_, y0_), (x0_, y1_)), my_attrs),
                 (((x0_, y1_), (x1_, y1_)), my_attrs),
                 (((x1_, y1_), (x1_, y0_)), my_attrs),
                 (((x1_, y0_), (x0_, y0_)), my_attrs)]
        kapton_data.extend(lines)

    if False:
      self.edge_max_layer = self._pyslip.AddPolygonLayer(
          kapton_data, map_rel=True, visible=True,
          show_levels=[-3, -2, -1, 0, 1, 2, 3, 4, 5],
          selectable=False,
          name='<kapton_layer>', update=False)

    # Remove the old edge_max layer, and draw a new one.
    if (hasattr(self, "_edge_max_layer") and self._edge_max_layer is not None):
      self._pyslip.DeleteLayer(self._edge_max_layer)
      self._edge_max_layer = None
    if True:
      self._edge_max_layer = self._pyslip.AddLayer(
        self._draw_edge_max_layer,
        edge_max_data_lonlat,
        map_rel=True, #map_rel
        visible=True, #visible
        show_levels=[-3, -2, -1, 0, 1, 2, 3, 4, 5], #show_levels
        selectable=False, #selectable
        name="<edge_max_layer>",
        type="edge_max")


class PluginHelper(object):
  _plugin_layer = "_kapton_layer"
  _plugin_title = "Kapton 2019 correction tool"
  _plugin_hide_text = "Hide 2019 kapton tool"
  _plugin_show_text = "Show 2019 kapton tool"
  _plugin_settings_frame = KaptonSettingsFrame
  _plugin_settings_panel = KaptonSettingsPanel


 *******************************************************************************


 *******************************************************************************
xfel/util/mp.py
from __future__ import absolute_import, division, print_function
#
# Handle multiprocessing with any of the implemented methods so that this step
# is abstracted away from the use case (e.g. cxi_mpi_submit).
#
from libtbx.utils import Sorry
import os
import math

mp_phil_str = '''
  mp {
    method = local *lsf sge pbs slurm shifter htcondor custom
      .type = choice
      .help = Computing environment
    use_mpi = True
      .type = bool
      .help = Use mpi multiprocessing
    mpi_command = mpirun
      .type = str
      .help = Command to invoke MPI processing. Include extra arguments to \
              this command here.
    mpi_option = "mp.method=mpi"
      .type = str
      .expert_level = 2
      .help = Parameter to turn on MPI in the dispatcher program
    nproc = 1
      .type = int
      .help = Number of processes total (== nnodes x nproc_per_node). \
              If two of the three params (nproc, nnodes, nproc_per_node) are \
              specified, the last will be determined by modular arithmetic. \
              If all three are specified, nnodes is ignored. nproc alone is \
              sufficient for most methods.
    nnodes = 1
      .type = int
      .help = Number of nodes to request
    nnodes_index = None
      .type = int
      .help = If defined, use this many nodes for indexing and integration. \
              Currently only works for mp.method=shifter or slurm.
    nnodes_tder = None
      .type = int
      .help = If defined, use this many nodes for ensemble refinement. \
              Currently only works for mp.method=shifter or slurm.
    nnodes_scale = None
      .type = int
      .help = If defined, use this many nodes for scaling. \
              Currently only works for mp.method=shifter or slurm.
    nnodes_merge = None
      .type = int
      .help = If defined, use this many nodes for merging. \
              Currently only works for mp.method=shifter or slurm.
    nproc_per_node = 1
      .type = int
      .help = Number of processes to allocate per node
    queue = None
      .type = str
      .help = Queue to submit multiprocessing job to (optional for some methods)
    memory = None
      .type = int
      .help = Memory (in MB) to allocate for a job (optional)
    wall_time = None
      .type = int
      .help = Wall time limit (in minutes) to impose (optional)
    max_queued = None
      .type = int
      .help = Maximum number of jobs running or queued
    extra_options = None
      .type = str
      .multiple = True
      .help = Any other options to be included in the job submission command
    extra_args = None
      .type = str
      .multiple = True
      .help = Any other arguments to the main command
    env_script = None
      .type = str
      .multiple = True
      .help = Path to script sourcing a particular environment (optional)
    phenix_script = None
      .type = str
      .multiple = True
      .help = Path to script sourcing a phenix environment (optional)
    local {
      include_mp_in_command = True
        .type = bool
        .help = Whether to decorate command with appropiate multiprocessing \
                arguments. If False, it's assumed the multiprocessing  \
                arguments are provided by the calling program.
    }
    shifter {
      submit_command = "sbatch "
        .type = str
        .help = Command used to run the zero-level script sbatch.sh.
      shifter_image = None
        .type = str
        .help = Name of Shifter image to use for processing, as you would use \
                in an sbatch script. Example: docker:dwpaley/cctbx-xfel:fix18
      sbatch_script_template = None
        .type = path
        .help = Script template to be run with sbatch. The script will be copied \
                to the trial directory as sbatch.sh and modified. Must contain \
                exactly one srun command of the format srun [...] <srun_script> \
                (including the <> brackets). May contain additional srun commands. \
                May also contain substitutables <walltime>, <partition>, <nnodes> \
                and <nproc>.
      srun_script_template = None
        .type = path
        .help = Script template to be run with srun. The script will be copied \
                to the trial directory as srun.sh and modified. Must contain \
                exactly one instance of the string <command> (including the <> \
                brackets) after setting up the necessary environment.
      partition = regular
        .type = str
        .help = Partition to run jobs on, e.g. regular or debug.
      jobname = LCLS_EXP
        .type = str
        .help = Job Name
      project = None
        .type = str
        .help = Name of NERSC project -- formerly "repo"
      reservation = None
        .type = str
        .help = Name of NERSC reservation
      constraint = haswell
        .type = str
        .help = Haswell or KNL
      staging = DataWarp *None
        .type = choice
        .help = Optionally stage logs to the DataWarp burst buffer. Only works \
                when writing to Cori cscratch.
    }
    htcondor {
      executable_path = None
        .type = path
        .help = Path to executable script (should be openmpiscript or mp2script). \
                See examples folder that comes with htcondor.
      filesystemdomain = sdfarm.kr
        .type = str
        .help = Domain of shared filesystem (see htcondor docs)
    }
    custom {
      submit_command_template = None
        .type = str
        .help = Job submission command template. There should be one instance of the \
                string <script> (including the <> brackets) in the command template. \
                Fields <queue>, <nproc>, <memory>, <walltime>, <outfile>, <errfile>, \
                <envscripts>, and <args> will similarly be replaced if present.
      submit_script_template = None
        .type = path
        .help = Submission script template. The script will be copied to the trial \
                directory and modified. There should be one instance of the string \
                <command> (including the <> brackets) in the template script, which \
                will be replaced with the processing command. <queue>, <nproc>, \
                <memory>, <walltime>, <outfile>, <errfile>, <envscripts>, and <args> \
                will similarly be replaced if present.
      wall_time_string = None
        .type = str
        .help = Fully formatted wall time limit (e.g. 00:30:00). For custom computing \
                environments, mp.wall_time is ignored because the format is unknown.
    }
    encapsulate_submit_script = True
      .type = bool
      .help = Encapsulate the submission command itself in another script containing \
              the job submission command (e.g. qsub, bsub, condor_submit, sbatch \
              etc.
  }
'''

class get_submit_command(object):
  def __init__(self, command, submit_path, stdoutdir, params,
               log_name="log.out", err_name="log.err", job_name=None, root_dir=None):
    """ Get a submit command for the various compute environments
    @param command Any command line program and its arguments
    @param submit_path Submit script will be written here
    @param stdoutdir Log file will be created in this directory
    @param params Multiprocessing phil params (see mp_phil_scope)
    @param log_name Filename for stdout (optional).
    @param err_name Filename for stderr (if None, combined with the stdout).
    @param job_name For applicable queueing systems, identifier for the job (optional).
    """
    self.shell_path = "/bin/bash"
    self.source_env_scripts = []
    self.options_inside_submit_script = []
    self.submit_head = "qsub"
    self.submit_path = submit_path
    self.stdoutdir = stdoutdir
    self.log_name = log_name
    self.err_name = err_name
    self.params = params
    self.job_name = job_name
    self.root_dir = root_dir
    self.command = command
    self.options = []
    self.args = []

  def customize_for_method(self):
    pass

  def eval_params(self):
    pass

  def substitute(self, template, marker, value):
    if marker in template:
      if value is None:
        raise Sorry("No value found for %s" % marker)
      return template.replace(marker, value)
    else:
      return template

  def delete(self, template, marker):
    template_lines = template.split('\n')
    return '\n'.join([l for l in template_lines if marker not in l])

  def make_executable(self, file):
    import stat
    st = os.stat(file)
    os.chmod(file, st.st_mode | stat.S_IXUSR)

  def write_script(self):
    command_str = " ".join([self.command] + self.args)
    with open(self.submit_path, 'w') as f:
      f.write("#! %s\n" % self.shell_path)
      for line in self.options_inside_submit_script:
        f.write("%s\n" % line)
      for line in self.source_env_scripts:
        f.write("%s\n" % line)
      f.write("\n")
      f.write("%s\n" % command_str)
    self.make_executable(self.submit_path)

  def generate_submit_command(self):
    return " ".join([self.submit_head] + self.options + [self.submit_path])

  def encapsulate_submit(self):
    path, ext = os.path.splitext(self.submit_path)
    encapsulate_path = path + "_submit" + ext
    with open(encapsulate_path, 'w') as f:
      f.write("#! /bin/%s\n\n" % ext[1:])
      f.write(self.generate_submit_command())
      f.write("\n")

  def __call__(self):
    self.customize_for_method()
    self.eval_params()
    self.write_script()
    if self.params.encapsulate_submit_script:
      self.encapsulate_submit()
    return self.generate_submit_command()

class get_local_submit_command(get_submit_command):

  def customize_for_method(self):
    if self.params.local.include_mp_in_command:
      if self.params.use_mpi:
        self.command = "%s -n %d %s" % (self.params.mpi_command, self.params.nproc, self.command)
        self.command += " %s"%self.params.mpi_option
      elif self.params.nproc > 1:
        self.command += " mp.nproc=%d" % self.params.nproc

  def eval_params(self):
    # <args> (optional, following the command)
    for arg in self.params.extra_args:
      self.args.append(arg)

  def generate_submit_command(self):
    return self.submit_path

class get_lsf_submit_command(get_submit_command):

  def customize_for_method(self):
    self.submit_head = "bsub"
    if self.params.use_mpi:
      self.command = "%s %s" % (self.params.mpi_command, self.command)
      self.command += " %s"%self.params.mpi_option
  def eval_params(self):
    # -n <nproc>
    nproc_str = "-n %d" % self.params.nproc
    self.options.append(nproc_str)

    # -o <outfile>
    out_str = "-o %s" % os.path.join(self.stdoutdir, self.log_name)
    self.options.append(out_str)

    # -e <errfile> (optional)
    if self.err_name is not None:
      err_str = "-e %s" % os.path.join(self.stdoutdir, self.err_name)
      self.options.append(err_str)

    # -q <queue> (optional on LSF)
    if self.params.queue is not None:
      queue_str = "-q %s" % self.params.queue
      self.options.append(queue_str)

    # -W <wall_time_limit> (optional)
    if self.params.wall_time is not None:
      hours = self.params.wall_time // 60
      minutes = self.params.wall_time % 60
      wt_str = "-W %2d:%02d" % (hours, minutes)
      self.options.append(wt_str)

    # -R "rusage[mem=<memory_requested>]" (optional)
    if self.params.memory is not None:
      memory_str = "-R \"rusage[mem=%d]\"" % self.params.memory
      self.options.append(memory_str)

    # <extra_options> (optional, preceding the command)
    for cmd in self.params.extra_options:
      self.options.append(cmd)

    # source </path/to/env.sh> (optional)
    for env in self.params.env_script:
      env_str = "source %s\n" % env
      self.source_env_scripts.append(env_str)

    # <args> (optional, following the command)
    for arg in self.params.extra_args:
      self.args.append(arg)

class get_sge_submit_command(get_submit_command):

  def customize_for_method(self):
    self.shell_path += " -q"
    self.options.append("-cwd")
#    self.options.append("mp.method=sge")
    if self.params.use_mpi:
      self.command = "%s -n ${NSLOTS} %s"%(self.params.mpi_command, self.command) #This command currently (14/10/2020) has problems at Diamond as it will randomly use incorrect number of cores
      self.command += " %s"%self.params.mpi_option
    else:
      self.command = "%s mp.nproc=${NSLOTS}"%(self.command)

  def eval_params(self):
    # -t 1-<nproc>
    if self.params.nproc > 1:
      nproc_str = "-pe smp %d" % self.params.nproc #Change the submission command to smp, as the openmpi currently confilicts with mpi of Dials and cctbx.xfel.merge
      self.options.append(nproc_str)

    # -o <outfile>
    out_str = "-o %s" % os.path.join(self.stdoutdir, self.log_name)
    self.options.append(out_str)

    # -j [y/n] -e <errfile> (optional)
    if self.err_name is not None:
      err_str = "-j n -e %s" % os.path.join(self.stdoutdir, self.err_name)
      self.options.append(err_str)
    else:
      self.options.append("-j y")

    # -q <queue>
    if self.params.queue is None:
      raise Sorry("Queue not specified.")
    queue_str = "-q %s" % self.params.queue
    self.options.append(queue_str)

    # -l h_rt=<wall_time_limit> (optional)
    if self.params.wall_time is not None:
      hours = self.params.wall_time // 60
      minutes = self.params.wall_time % 60
      wt_str = "-l h_rt=%02d:%02d:00" % (hours, minutes)
      self.options.append(wt_str)

    # -l mem_free=<memory_requested> (optional)
    if self.params.memory is not None:
      memory_str = "-l mem_free=%dM" % self.params.memory
      self.options.append(memory_str)

    # -N <job_name>
    if self.job_name is not None:
      name_str = "-N %s" % self.job_name
      self.options.append(name_str)

    # <extra_options> (optional, preceding the command)
    for cmd in self.params.extra_options:
      self.options.append(cmd)

    # source </path/to/env.sh> (optional)
    for env in self.params.env_script:
      env_str = "source %s\n" % env
      self.source_env_scripts.append(env_str)

    # <args> (optional, following the command)
    for arg in self.params.extra_args:
      self.args.append(arg)

class get_pbs_submit_command(get_submit_command):

  def customize_for_method(self):
    if (self.params.nnodes > 1) or (self.params.nproc_per_node > 1):
      self.params.nproc = self.params.nnodes * self.params.nproc_per_node
    if self.params.use_mpi:
      self.command = "mpiexec --hostfile $PBS_NODEFILE %s" % (self.command)
      self.command += " %s"%self.params.mpi_option
  def eval_params(self):

    # # -t 1-<nproc> # deprecated
    # if self.params.nproc > 1:
    #   nproc_str = "#PBS -l mppwidth=%d" % self.params.nproc
    #   self.options_inside_submit_script.append(nproc_str)

    # -l nodes=<nnodes>:ppn=<procs_per_node>
    if max(self.params.nproc, self.params.nproc_per_node, self.params.nnodes) > 1:
      # If specified, nproc overrides procs_per_node and procs_per_node overrides
      # nnodes. One process per node is requested if only nproc is specified.
      if self.params.nproc > 1:
        import math
        if self.params.nproc <= self.params.nproc_per_node:
          procs_per_node = self.params.nproc
          nnodes = 1
        elif self.params.nproc_per_node > 1:
          procs_per_node = self.params.nproc_per_node
          nnodes = int(math.ceil(self.params.nproc/procs_per_node))
        elif self.params.nnodes > 1:
          procs_per_node = int(math.ceil(self.params.nproc/self.params.nnodes))
          nnodes = self.params.nnodes
        else: # insufficient information; allocate 1 proc per node
          procs_per_node = 1
          nnodes = self.params.nproc
      else:
        procs_per_node = self.params.nproc_per_node
        nnodes = self.params.nnodes
      nproc_str = "#PBS -l nodes=%d:ppn=%d" % (nnodes, procs_per_node)
      self.options_inside_submit_script.append(nproc_str)

    # -o <outfile>
    out_str = "#PBS -o %s" % os.path.join(self.stdoutdir, self.log_name)
    self.options_inside_submit_script.append(out_str)

    # [-j oe/-e <errfile>] (optional)
    if self.err_name is not None:
      err_str = "#PBS -e %s" % os.path.join(self.stdoutdir, self.err_name)
      self.options_inside_submit_script.append(err_str)
    else:
      self.options_inside_submit_script.append("#PBS -j oe")

    # -q <queue>
    if self.params.queue is None:
      raise Sorry("Queue not specified.")
    queue_str = "#PBS -q %s" % self.params.queue
    self.options_inside_submit_script.append(queue_str)

    # -l walltime=<wall_time_limit> (optional)
    if self.params.wall_time is not None:
      hours = self.params.wall_time // 60
      minutes = self.params.wall_time % 60
      wt_str = "#PBS -l walltime=%2d:%02d:00" % (hours, minutes)
      self.options_inside_submit_script.append(wt_str)

    # -l mem_free=<memory_requested> (optional)
    if self.params.memory is not None:
      memory_str = "#PBS -l mem=%dmb" % self.params.memory
      self.options_inside_submit_script.append(memory_str)

    # -N <job_name>
    if self.job_name is not None:
      name_str = "#PBS -N %s" % self.job_name
      self.options_inside_submit_script.append(name_str)

    # <extra_options> (optional, preceding the command)
    for cmd in self.params.extra_options:
      cmd_str = "#PBS %s" % cmd
      self.options_inside_submit_script.append(cmd_str)

    if self.root_dir is not None:
      cmd_str = "cd %s"%self.root_dir
      self.options_inside_submit_script.append(cmd_str)

    # source </path/to/env.sh> (optional)
    for env in self.params.env_script:
      env_str = "source %s\n" % env
      self.source_env_scripts.append(env_str)

    if '<output_dir>' in self.command:
      self.command = self.command.replace(
        '<output_dir>',
        os.path.split(self.stdoutdir[0])
      )
    # <args> (optional, following the command)
    image_average_output_dir = os.path.join(os.path.split(self.stdoutdir)[0], 'out')
    for arg in self.params.extra_args:
      if '<output_dir>' in arg:
        arg = arg.replace('<output_dir>', image_average_output_dir)
      self.args.append(arg)

class get_slurm_submit_command(get_submit_command):

  def customize_for_method(self):
    self.submit_head = "sbatch"
    if self.params.use_mpi:
      self.command = "%s %s" % (self.params.mpi_command, self.command)
      self.command += " %s"%self.params.mpi_option

  def eval_params(self):
    nproc_str = "#SBATCH --nodes %d" % self.params.nnodes
    if self.params.nproc_per_node:
      nproc_str += "\n#SBATCH --ntasks-per-node=%d" % self.params.nproc_per_node
    self.options_inside_submit_script.append(nproc_str)

    # -o <outfile>
    out_str = "#SBATCH --output=%s" % os.path.join(self.stdoutdir, self.log_name)
    self.options_inside_submit_script.append(out_str)

    # [-j oe/-e <errfile>] (optional)
    if self.err_name is not None:
      err_str = "#SBATCH --error=%s" % os.path.join(self.stdoutdir, self.err_name)
      self.options_inside_submit_script.append(err_str)

    # -q <queue>
    if self.params.queue and self.params.queue.strip():
      queue_str = "#SBATCH --partition %s" % self.params.queue.strip()
      self.options_inside_submit_script.append(queue_str)

    # -l walltime=<wall_time_limit> (optional)
    if self.params.wall_time is not None:
      hours = self.params.wall_time // 60
      minutes = self.params.wall_time % 60
      wt_str = "#SBATCH --time=%02d:%02d:00" % (hours, minutes)
      self.options_inside_submit_script.append(wt_str)

    # -l mem_free=<memory_requested> (optional)
    if self.params.memory is not None:
      memory_str = "#SBATCH --mem=%dmb" % self.params.memory
      self.options_inside_submit_script.append(memory_str)

    # -N <job_name>
    if self.job_name is not None:
      name_str = "#SBATCH --job-name=%s" % self.job_name
      self.options_inside_submit_script.append(name_str)

    # <extra_options> (optional, preceding the command)
    for cmd in self.params.extra_options:
      cmd_str = "#SBATCH %s" % cmd
      self.options_inside_submit_script.append(cmd_str)

    # source </path/to/env.sh> (optional)
    for env in self.params.env_script:
      env_str = "source %s\n" % env
      self.source_env_scripts.append(env_str)

    if 'phenix' in self.command:
      self.source_env_scripts.append("cd %s\n"%os.path.dirname(self.submit_path))

    if '<output_dir>' in self.command:
      self.command = self.command.replace(
        '<output_dir>',
        os.path.split(self.stdoutdir[0])
      )
    # <args> (optional, following the command)
    image_average_output_dir = os.path.join(os.path.split(self.stdoutdir)[0], 'out')
    for arg in self.params.extra_args:
      if '<output_dir>' in arg:
        arg = arg.replace('<output_dir>', image_average_output_dir)
      self.args.append(arg)


class get_shifter_submit_command(get_submit_command):

  # No need for constructor -- the interited constructor is just fine for shifter

  def customize_for_method(self):
    # template for sbatch.sh
    self.sbatch_template = self.params.shifter.sbatch_script_template
    self.destination = os.path.dirname(self.submit_path)
    self.prefix = os.path.splitext(os.path.basename(self.submit_path))[0]
    if self.prefix: self.prefix += "_"
    if not self.sbatch_template:
      from xfel.ui.db.cfgs import shifter_templates
      self.sbatch_contents = shifter_templates.sbatch_template
    else:
      with open(self.sbatch_template, "r") as sb:
        self.sbatch_contents = sb.read()
    self.sbatch_path = os.path.join(self.destination, self.prefix + "sbatch.sh")

    # template for srun.sh
    self.srun_template = self.params.shifter.srun_script_template
    if not self.srun_template:
      from xfel.ui.db.cfgs import shifter_templates
      self.srun_contents = shifter_templates.srun_template
    else:
      with open(self.srun_template, "r") as sr:
        self.srun_contents = sr.read()
    if self.params.use_mpi:
      self.command = "%s" % (self.command)
      self.command += " %s"%self.params.mpi_option
    self.srun_path = os.path.join(self.destination, self.prefix + "srun.sh")


  def eval_params(self):

    # --image <shifter_image>
    if self.params.shifter.shifter_image:
      self.sbatch_contents = self.substitute(
          self.sbatch_contents,
          "<shifter_image>",
          self.params.shifter.shifter_image
      )
    else:
      raise Sorry("Must supply a shifter image")

    # -N <nnodes>
    self.sbatch_contents = self.substitute(self.sbatch_contents, "<nnodes>",
      str(self.params.nnodes))

    # --tasks-per-node <nproc_per_node> (optional)
    self.sbatch_contents = self.substitute(self.sbatch_contents, "<nproc_per_node>",
      str(self.params.nproc_per_node))

    # For now use nproc = nnodes*nproc_per_node
    # TODO: find a way for the user to specify _either_ nproc_per_node, or nproc
    nproc = self.params.nnodes * self.params.nproc_per_node

    # -n <nproc> (optional)
    self.sbatch_contents = self.substitute(self.sbatch_contents, "<nproc>",
      str(nproc))

    # -W <walltime> (optional)
    if self.params.wall_time is not None:
      hours = self.params.wall_time // 60
      minutes = self.params.wall_time % 60
      wt_str = "%02d:%02d:00" % (hours, minutes)
      self.sbatch_contents = self.substitute(self.sbatch_contents, "<walltime>",
        wt_str)

    # --qos <queue>
    self.sbatch_contents = self.substitute(self.sbatch_contents, "<queue>",
      self.params.queue)

    # --partition <partition>
    self.sbatch_contents = self.substitute(self.sbatch_contents, "<partition>",
      self.params.shifter.partition)

    # --job-name
    self.sbatch_contents = self.substitute(self.sbatch_contents, "<jobname>",
      self.params.shifter.jobname)

    # -A
    self.sbatch_contents = self.substitute(self.sbatch_contents, "<project>",
      self.params.shifter.project)

    # --reservation
    if self.params.shifter.reservation:
      self.sbatch_contents = self.substitute(
          self.sbatch_contents, "<reservation>", self.params.shifter.reservation
      )
    else:
      self.sbatch_contents = self.delete(self.sbatch_contents, "<reservation>")

    # --constraint
    self.sbatch_contents = self.substitute(self.sbatch_contents, "<constraint>",
      self.params.shifter.constraint)

    self.sbatch_contents = self.substitute(self.sbatch_contents, "<out_log>",
      os.path.join(self.destination , self.prefix + "out.log"))

    self.sbatch_contents = self.substitute(self.sbatch_contents, "<err_log>",
      os.path.join(self.destination , self.prefix + "err.log"))

    self.sbatch_contents = self.substitute(self.sbatch_contents, "<output_dir>",
      self.destination)

    # Delete datawarp instructions if we're not staging logs
    if self.params.shifter.staging != "DataWarp":
      self.sbatch_contents = self.delete(self.sbatch_contents, "#DW")

    # <srun_script>
    self.sbatch_contents = self.substitute(self.sbatch_contents, "<srun_script>",
      self.srun_path)

    # <command> and any extra args
    if len(self.params.extra_args) > 0:
      self.srun_contents = self.substitute(self.srun_contents, "<command>",
        "<command> %s" % " ".join(self.params.extra_args))
    self.srun_contents = self.substitute(self.srun_contents, "<command>",
      self.command)


  def generate_submit_command(self):
    return self.params.shifter.submit_command + " " + self.sbatch_path

  def encapsulate_submit(self):
    pass

  def generate_sbatch_script(self):
    with open(self.sbatch_path, "w") as sb:
      sb.write(self.sbatch_contents)
      sb.write("\n")
    self.make_executable(self.sbatch_path)

  def generate_srun_script(self):
    with open(self.srun_path, "w") as sr:
      sr.write(self.srun_contents)
      sr.write("\n")
    self.make_executable(self.srun_path)

  def write_script(self):
    self.generate_sbatch_script()
    self.generate_srun_script()

class get_htcondor_submit_command(get_submit_command):
  def __init__(self, *args, **kwargs):
    super(get_htcondor_submit_command, self).__init__(*args, **kwargs)
    self.destination = os.path.dirname(self.submit_path)
    self.basename = os.path.splitext(os.path.basename(self.submit_path))[0]

  def customize_for_method(self):
    self.submit_head = "condor_submit"
    if self.params.use_mpi:
      self.command = "%s" % (self.command)
      self.command += " %s"%self.params.mpi_option

  def generate_submit_command(self):
    return "condor_submit " + os.path.join(self.destination, self.basename + "_condorparams")

  def eval_params(self):
    if self.params.use_mpi:
      from libtbx import easy_run
      d = dict(executable_path = self.params.htcondor.executable_path,
               arguments       = self.submit_path,
               nproc           = self.params.nproc,
               working_folder  = self.destination,
               log_path        = os.path.join(self.stdoutdir, self.basename + '_condor.log'),
               output_path     = os.path.join(self.stdoutdir, self.log_name),
               error_path      = os.path.join(self.stdoutdir, self.err_name),
               requirements    = 'target.filesystemdomain == "%s"'% self.params.htcondor.filesystemdomain)

      # Find if there is a continguous set of slots available on one node
      r = easy_run.fully_buffered('condor_status | grep Unclaimed | grep %s'%self.params.htcondor.filesystemdomain)
      machines = {}
      for line in r.stdout_lines:
        try:
          machine = line.split()[0].split('@')[1]
        except IndexError: continue
        if machine not in machines:
          machines[machine] = 0
        machines[machine] += 1

      for machine in machines:
        if machines[machine] >= self.params.nproc:
          d['requirements'] += ' && machine == "%s"'%machine
          break

      condor_params = """
universe = parallel
executable = {executable_path}
arguments = {arguments}
machine_count = {nproc}
initialdir = {working_folder}
when_to_transfer_output = on_exit
log                     = {log_path}
output                  = {output_path}
error                   = {error_path}
requirements = {requirements}
+ParallelShutdownPolicy = "WAIT_FOR_ALL"
RunAsOwner = True
queue
"""
    else:
      assert self.params.htcondor.executable_path is None
      d = dict(executable_path = self.submit_path,
               working_folder  = self.destination,
               log_path        = os.path.join(self.stdoutdir, self.basename + '_condor.log'),
               output_path     = os.path.join(self.stdoutdir, self.log_name),
               error_path      = os.path.join(self.stdoutdir, self.err_name),
               filesystemdomain= self.params.htcondor.filesystemdomain)
      condor_params = """
universe = vanilla
executable = {executable_path}
initialdir = {working_folder}
when_to_transfer_output = on_exit
log                     = {log_path}
output                  = {output_path}
error                   = {error_path}
requirements = target.filesystemdomain == "{filesystemdomain}"
RunAsOwner = True
queue
"""

    with open(os.path.join(self.destination, self.basename + "_condorparams"), 'w') as f:
      f.write(condor_params.format(**d))

    # source </path/to/env.sh> (optional)
    for env in self.params.env_script:
      env_str = "source %s\n" % env
      self.source_env_scripts.append(env_str)

class get_custom_submit_command(get_submit_command):

  def customize_for_method(self):
    # template for the script to be submitted, beginning with #!
    self.script_template = self.params.custom.submit_script_template
    if not os.path.exists(self.template):
      raise Sorry("Custom submission template file not found: %s" % self.template)

    # template for the submission command itself, e.g. qsub -n <nproc> -q <queue> script.sh
    self.command_template = self.params.custom.submit_command_template
    if self.command_template is None:
      raise Sorry("Custom submit command must be specified for custom environments.")

  def eval_params(self):
    # any changes to the script to be submitted
    with open(self.script_template, "r") as script:
      self.script_contents = script.read()

    # <command> and any <args>
    if len(self.params.extra_args) > 0:
      self.script_contents = self.script_contents.replace("<command>",
        "<command> %s" % " ".join(self.params.extra_args))
    self.script_contents = self.script_contents.replace("<command>", self.command)

    # other changes to the contents of the script
    for marker, value in [
      ("<queue>", self.params.queue),
      ("<nproc>", self.params.nproc),
      ("<memory>", self.params.memory),
      ("<walltime>", self.params.custom.wall_time_string),
      ("<outfile>", os.path.join(self.stdoutdir, self.log_name)),
      ("<errfile>", os.path.join(self.stdoutdir, self.err_name)),
      ("<envscripts>", self.params.env_script)]:
      self.script_contents = self.substitute(self.script_contents, marker, value)

    # any changes to the submission command
    # <script> and any extra <options>
    if len(self.params.extra_options) > 0:
      self.submit_command_contents = self.params.custom.submit_command_template.replace("<script>",
        "%s <script>" % " ".join(self.params.extra_options))
    self.submit_command_contents = self.submit_command_contents.replace("<script>",
      self.submit_path)

    # other changes to the submission command
    for marker, value in [
      ("<queue>", self.params.queue),
      ("<nproc>", self.params.nproc),
      ("<memory>", self.params.memory),
      ("<walltime>", self.params.custom.wall_time_string),
      ("<outfile>", os.path.join(self.stdoutdir, self.log_name)),
      ("<errfile>", os.path.join(self.stdoutdir, self.err_name))]:
      self.submit_command_contents = self.substitute(self.submit_command_contents, marker, value)

  def write_script(self):
    with open(self.submit_path, "w") as f:
      f.write(self.script_contents)
      f.write("\n")

  def generate_submit_command(self):
    return self.submit_command_contents

def get_submit_command_chooser(command, submit_path, stdoutdir, params,
                               log_name="log.out", err_name="log.err", job_name=None,
                               root_dir=None):
  if params.method == "local":
    choice = get_local_submit_command
  elif params.method == "lsf":
    choice = get_lsf_submit_command
  elif params.method == "sge":
    choice = get_sge_submit_command
  elif params.method == "pbs":
    choice = get_pbs_submit_command
  elif params.method == "slurm":
    choice = get_slurm_submit_command
  elif params.method == "shifter":
    choice = get_shifter_submit_command
  elif params.method == "htcondor":
    choice = get_htcondor_submit_command
  elif params.method == "custom":
    choice = get_custom_submit_command
  else:
    raise Sorry("Multiprocessing method %s not recognized" % params.method)
  command_generator = choice(command, submit_path, stdoutdir, params,
                             log_name=log_name, err_name=err_name, job_name=job_name, root_dir=root_dir)
  return command_generator()


 *******************************************************************************


 *******************************************************************************
xfel/util/preference.py
from __future__ import division

import abc
from collections import Counter, deque, UserDict
from dataclasses import dataclass
import glob
from numbers import Number
from typing import Any, Iterable, List, Sequence, Tuple, TypeVar, Union
import sys

from cctbx import sgtbx
from dxtbx.model import ExperimentList
from iotbx.phil import parse
from libtbx import Auto, table_utils
from libtbx.mpi4py import MPI
from xfel.util.drift import params_from_phil, read_experiments

import numpy as np
import scipy as sp


message = """
This utility tool aims to determinate, characterise, and quantify the degree
of preferential orientation in crystals. To this aim, it investigates the
the distribution of various crystallographic directions on a sphere in 3D.
The code assumes each set of vectors follows Wilson Distribution, and attempts
to model said distribution by fitting its parameter `mu` and `kappa`.

Wilson distribution describes a bimodal arrangement of points / unit vectors
on a sphere around a central axis called `mu`. The distribution is invariant
to any rotation around `mu` and inversion, and its exact type depends on the
concentration parameter `kappa`. For `kappa` > 0, the points are focused in
the polar region around +/- `mu`. In case of  `kappa` < 0, the points
concentrate mostly in a equatorial region far from `mu`. `kappa` close to 0
describes a distribution uniform on sphere: no preferential orientation.

This code has been prepared using the following books & papers as references:
- http://palaeo.spb.ru/pmlibrary/pmbooks/mardia&jupp_2000.pdf, section 10.3.2
- https://www.sciencedirect.com/science/article/pii/S0047259X12002084, sect. 2
- https://www.tandfonline.com/doi/abs/10.1080/03610919308813139
""".strip()


phil_scope_str = """
  input {
    glob = None
      .type = str
      .multiple = True
      .help = glob which matches all expt files to be investigated.
    exclude = None
      .type = str
      .multiple = True
      .help = glob which matches all expt files to be excluded from input.
    space_group = Auto
      .type = space_group
      .help = Reject all expts that are not described by this space group. \
              Investigate only directions that are symmetrically equivalent \
              according to the point symmetry of given group. \
              By default, look at the most common space group only.
    symmetrize = True
      .type = bool
      .help = Apply point group symmetry extracted from `space_group` to \
              extracted unit cell bases to avoid bias introduced by indexing
  }
  plot {
    style = none ascii *hammer hedgehog
      .type = choice
      .help = Which kind of plot should be produced: \
              Ascii writes a crude heatmap of distribution in x/y coords.\
              Hammer plots heatmap of distribution on Hammer projection.\
              Hedgehog draws all individual vectors (use for small data only).
  }
"""
phil_scope = parse(phil_scope_str)


# ~~~~~~~~~~~~~~~~~~~~~~~~~~ CONVENIENCE AND TYPING ~~~~~~~~~~~~~~~~~~~~~~~~~ #


COMM = MPI.COMM_WORLD
Int3 = Tuple[int, int, int]
Number3 = Sequence[Number]
sg_p1 = sgtbx.space_group('P 1')  # noqa
pg_i1 = sg_p1.build_derived_laue_group()
SgtbxPointGroup = Any
SgtbxSpaceGroup = Any
SgtbxSymmOp = Any
T = TypeVar('T')


def flatten(sequence: Sequence[Sequence[T]]) -> List[T]:
  """Flatten a sequence of sequences into a 1-dimensional list"""
  return [element for sub_sequence in sequence for element in sub_sequence]


def locate_paths(include_globs: Iterable[str],
                 exclude_globs: Iterable[str],
                 ) -> List[str]:
  """Return a list of expt paths in `include_globs`, outside `exclude_globs`"""
  include_paths = flatten([glob.glob(ig) for ig in include_globs])
  exclude_paths = flatten([glob.glob(eg) for eg in exclude_globs])
  return [path for path in include_paths if path not in exclude_paths]


def space_group_auto(expts: Iterable[ExperimentList],
                     comm: type(COMM) = None,
                     ) -> Tuple[SgtbxSpaceGroup, str]:
  """Return the most common space groups across comm world, and summary str"""
  counter = Counter([e.crystal.get_space_group().make_tidy() for e in expts])
  if comm is not None:
    counters = comm.allgather(counter)
    counter = sum(counters, Counter())
  message_ = ''
  for sg, sg_count in counter.items():
    message_ += f'Found {sg_count:6d} expts with space group {sg.info()}\n'
  most_common = counter.most_common(1)[0][0]
  message_ += f'Evaluating the most common space group {most_common.info()}'
  return most_common, message_


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SYMMETRY HANDLING ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


def transform(vectors: Union[Number3, Iterable[Number3]],
              symm_op: SgtbxSymmOp,
              ) -> np.ndarray[Number3]:
  """Transform Number3 or Nx3 Iterable of Number3-s using symm_op """
  vectors = np.array(list(vectors))  # read generators, avoid tuples
  symm_op_m3 = np.array(symm_op.as_double_array()[:9]).reshape((3, 3))
  return vectors @ symm_op_m3.T


# ~~~~~~~~~~~~~~~~~~~~~~~~~~ ORIENTATION SCRAPPING ~~~~~~~~~~~~~~~~~~~~~~~~~~ #


class DirectSpaceBases(np.ndarray):
  """
  This class is responsible for scraping and storing vectors a, b, c;
  Preferably, these come from expts, but the class can be initiated with
  a raw numpy array as well. It is 3-dimensional, with individual dimensions
  representing:

  - [n, :, :] - access n-th 3x3 array of a, b, c vectors;
  - [:, n, :] - access an Nx3 array of a (n=0), b (n=1), or c (n=2) vectors;
  - [:, :, n] - access an Nx3 array of x (n=0), y (n=1), or z (n=2) components.

  Consequently, the object is always (and must be init. using) a Nx3n3 array.
  """
  def __new__(cls, abcs: np.ndarray):
    if abcs.ndim != 3 or abcs.shape[1] != 3 or abcs.shape[2] != 3:
      msg = 'DirectSpaceVectors must be init with a Nx3x3 array of abc vectors'
      raise ValueError(msg)
    return super().__new__(cls, abcs.shape, dtype=float, buffer=abcs)

  @classmethod
  def from_expts(cls,
                 expts: ExperimentList,
                 space_group: SgtbxSpaceGroup = sg_p1,
                 ) -> 'DirectSpaceBases':
    """Extract N [a, b, c] arrays from N expts into a Nx3x3 ndarray, return"""
    abcs = []
    for expt in expts:
      expt_sg = expt.crystal.get_space_group()
      if expt_sg != space_group:
        continue
      abcs.append(expt.crystal.get_real_space_vectors().as_numpy_array())
    new = np.stack(abcs, axis=0) if abcs else np.empty(shape=(0, 3, 3))
    return cls(new)

  @property
  def a(self) -> np.ndarray:
    return np.array(self[:, 0, :])

  @property
  def b(self) -> np.ndarray:
    return np.array(self[:, 1, :])

  @property
  def c(self) -> np.ndarray:
    return np.array(self[:, 2, :])

  @property
  def x(self) -> np.ndarray:
    return np.array(self[:, :, 0])

  @property
  def y(self) -> np.ndarray:
    return np.array(self[:, :, 1])

  @property
  def z(self) -> np.ndarray:
    return np.array(self[:, :, 2])

  def transform(self, symm_op: SgtbxSymmOp) -> 'DirectSpaceBases':
    """Transform all vectors in self using sgtbx rt_mx-type object"""
    if self.size:
      x = transform(self.x, symm_op)
      y = transform(self.y, symm_op)
      z = transform(self.z, symm_op)
      new = np.stack([x, y, z], axis=2)
    else:
      new = np.empty(shape=(0, 3, 3))
    return self.__class__(new)

  def symmetrize(self, point_group: SgtbxPointGroup) -> 'DirectSpaceBases':
    """Transform all vectors in self using all symm. ops. in point group"""
    transformed = [self.transform(symm_op) for symm_op in point_group]
    return self.__class__(np.concatenate(transformed, axis=0))


# ~~~~~~~~~~~~~~~~~~~ PREFERENTIAL ORIENTATION CALCULATOR ~~~~~~~~~~~~~~~~~~~ #


def cart2sph(xyz: np.ndarray) -> np.ndarray:
  """Convert an array of xyz vectors into an array of r, polar, azim vectors"""
  r = np.linalg.norm(xyz, axis=1)
  polar = np.arccos(xyz[:, 2] / r)
  azim = np.arctan2(xyz[:, 1], xyz[:, 0])
  return np.vstack([r, polar, azim]).T


class SphericalDistribution:
  """
  General class for handling distribution of unit vectors in 3D. Operates in
  and provides methods to transform between different coordinate systems:
  - `cart` - Cartesian coordinates X, Y, Z in laboratory reference frame;
  - `sph` - Spherical ref. system with e1 = Z, polar e2 from Z to X, azim. XY;
  - `mu_sph` - Spherical reference system with e1 = mu and e2, e3 arbitrary;
  """
  E1 = np.array([1, 0, 0])
  E2 = np.array([0, 1, 0])
  E3 = np.array([0, 0, 1])

  def __init__(self):
    self.vectors: np.ndarray = np.empty((0, 3), dtype=float)
    self.mu: np.ndarray = np.array([1., 0., 0.], dtype=float)

  @staticmethod
  def normalized(vectors: np.ndarray, axis: int = -1) -> np.ndarray:
    """Return `vectors` normalized using standard l2 norm along `axis` """
    l2 = np.atleast_1d(np.linalg.norm(vectors, 2, axis))
    l2[l2 == 0] = 1
    return vectors / np.expand_dims(l2, axis)

  @staticmethod
  def are_parallel(v: np.ndarray, w: np.ndarray, eps: float = 1e-8) -> bool:
    return abs(np.dot(v, w) / (np.linalg.norm(v) * np.linalg.norm(w))) < 1 - eps

  @property
  def mu_basis_vectors(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """Basis vector of cartesian system in which e1 = mu; e2 & e3 arbitrary"""
    e1 = self.mu / np.linalg.norm(self.mu)
    e0 = self.E1 if not self.are_parallel(e1, self.E1) else self.E2
    e2 = np.cross(e1, e0) / np.linalg.norm(np.cross(e1, e0))
    e3 = np.cross(e1, e2) / np.linalg.norm(np.cross(e1, e2))
    return e1, e2, e3

  @property
  def mu_ring(self) -> np.ndarray:
    """A closed 3D ring of points around vector self.mu used for plotting"""
    azim = np.linspace(0.0, 2*np.pi, num=100, endpoint=True)
    r = np.ones_like(azim)
    polar = np.full_like(azim, np.pi / 2)
    return self.mu_sph2cart(np.vstack([r, polar, azim]).T)

  def mu_sph2cart(self, r_polar_azim: np.ndarray) -> np.ndarray:
    """Convert spherical coordinates r, polar, azim to cartesian in mu basis"""
    r, polar, azim = np.hsplit(r_polar_azim, 3)
    e1, e2, e3 = self.mu_basis_vectors
    e1_component = e1 * np.cos(polar)
    e2_component = e2 * np.sin(polar) * np.cos(azim)
    e3_component = e3 * np.sin(polar) * np.sin(azim)
    return r * (e1_component + e2_component + e3_component)


class WatsonDistribution(SphericalDistribution):
  """Class for holding, fitting, and generating Watson distribution.
  Description names reflect those used in respective references:
  - https://arxiv.org/pdf/1104.4422.pdf, page 3
  - http://palaeo.spb.ru/pmlibrary/pmbooks/mardia&jupp_2000.pdf, section 10.3.2
  - https://www.tandfonline.com/doi/abs/10.1080/03610919308813139"""
  def __init__(self, mu: np.ndarray = None, kappa: float = None) -> None:
    super().__init__()
    self.kappa: float = kappa
    self.mu: np.ndarray = mu
    self.nll: float = np.Infinity

  def __str__(self) -> str:
    return f'Watson Distribution around mu={self.mu} with kappa={self.kappa}'

  @classmethod
  def from_vectors(cls, vectors: np.ndarray) -> 'WatsonDistribution':
    """Define the distribution by fitting it to a list of vectors"""
    new = WatsonDistribution()
    new.fit(vectors=cls.normalized(vectors))
    return new

  @property
  def r_avg(self) -> float:
    """Length of the not-normalized mean direction of vectors, bar{R}"""
    return np.linalg.norm(self.x_avg)

  @property
  def x_avg(self) -> np.ndarray:
    """Sum of vectors divided by their count, bar{x}"""
    return np.sum(self.vectors, axis=0) / self.vectors.shape[0]

  @staticmethod
  def kummer_function(a: float, b: float, kappa: float) -> float:
    """Confluent hypergeometric function 1F1, a.k.a. Kummer function"""
    return sp.special.hyp1f1(a, b, kappa)

  @property
  def scatter_matrix(self) -> np.ndarray:
    """Scatter matrix of `vectors` distribution (9.2.10)"""
    return np.matmul(self.vectors.T, self.vectors) / len(self.vectors)

  def log_likelihood(self, kappa: float, mu: np.ndarray) -> float:
    """Log likelihood of given mu, kappa given current vectors. (10.3.30)"""
    t = self.scatter_matrix
    m = self.kummer_function(1/2, 3/2, kappa)
    return len(self.vectors) * (kappa * mu.T @ t @ mu - np.log(m))

  def nll_of_kappa(self, kappa: float, mu: np.ndarray) -> float:
    """Negative log likelihood of this Watson Distribution as a function
    of kappa, with `mu` and `vectors` fixed and given in `params`"""
    return -self.log_likelihood(kappa=kappa, mu=mu)

  def fit(self, vectors: np.ndarray) -> None:
    """Fit distribution to `vectors`, update `self.mu` and `self.kappa`"""
    self.vectors = vectors
    eig_val, eig_vec = np.linalg.eig(self.scatter_matrix)
    fitted = {'mu': np.array([1., 0., 0.]), 'kappa': 0., 'nll': np.inf}
    for eig_val, eig_vec in zip(eig_val, eig_vec.T):
      result = sp.optimize.minimize(self.nll_of_kappa, x0=0., args=eig_vec)
      nll = result['fun']
      if nll < fitted['nll']:
        fitted = {'mu': eig_vec, 'kappa': result['x'][0], 'nll': nll}
    self.kappa = fitted['kappa']
    self.mu = fitted['mu']
    self.nll = fitted['nll']

  def sample(self, n: int, seed: int = 42) -> np.ndarray:
    """Sample `n` vectors from self, based on doi 10.1080/03610919308813139"""
    if n < 0:
      return
    k = self.kappa
    rho = (4 * k) / (2 * k + 3 + ((2 * k + 3) ** 2 - 16 * k) ** 0.5)
    r = ((3 * rho) / (2 * k)) ** 3 * np.exp(-3 + 2 * k / rho)
    rng = np.random.default_rng(seed=seed)

    def cos2_of_polar(_n: int) -> np.ndarray:
      u0 = rng.uniform(size=2*_n)
      u1 = rng.uniform(size=2*_n)
      s = u0 ** 2 / (1 - rho * (1 - u0 ** 2))
      v = (r * u1 ** 2) / (1 - rho * s) ** 3
      good_s = s[v <= np.exp(2 * k * s)]
      return good_s[:_n] if len(good_s) >= _n else \
          np.concatenate([good_s, cos2_of_polar(_n-len(good_s))], axis=None)
    u2 = rng.uniform(size=n)
    theta = np.arccos(cos2_of_polar(n) ** 0.5)
    phi = 4 * np.pi * u2
    theta[u2 < 0.5] = np.pi - theta[u2 < 0.5]
    phi[u2 >= 0.5] = 2 * np.pi * (2 * u2[u2 >= 0.5] - 1)
    self.vectors = self.mu_sph2cart(np.vstack([np.ones_like(theta), theta, phi]).T)


class ZoneAxisFamily(tuple):
  """Class for handling "crystal forms", i.e. sets of symm-equiv. zone axes."""

  def __str__(self) -> str:
    return f'{{{self[0]},{self[1]},{self[2]}}}'


class UniquePseudoNodeGenerator:
  """
  This class generates a list of unique pseudo-nodes; each pseudo-node
  represents a single pseudo-vector expressed using integer coordinates
  in cartesian space. They can be used to express all possible unique
  lattice directions or zone axes with indices up to `radius`.
  For example, pseudo-nodes [1, 1, 0], [-1, -1, 0], and [2, 2, 0] all express
  the same pseudo-vector [1, 1, 0], independent of symmetry
  """
  def __init__(self, laue_group: SgtbxPointGroup = pg_i1) -> None:
    self.point_group = laue_group
    self.nodes_to_yield = deque()
    self.nodes_considered = set()
    self.expand(around=(0, 0, 0), radius=3)

  def __iter__(self) -> 'UniquePseudoNodeGenerator':
    return self

  def __next__(self) -> Int3:
    if self.nodes_to_yield:
      return self.nodes_to_yield.popleft()
    raise StopIteration

  def add(self, nodes: Iterable[Int3]) -> None:
    """Add new pseudo-nodes, but only if they hadn't been yielded yet"""
    for node in nodes:
      node = tuple(node)
      symmetry_equivalents = {tuple(transform(node, symm_op))
                              for symm_op in self.point_group}
      if not symmetry_equivalents.intersection(self.nodes_considered):
        self.nodes_to_yield.append(node)
        self.nodes_considered.add(node)

  def expand(self, around: Int3, radius: int = 2) -> None:
    """Generate new direction pseudo-vectors in a `RADIUS` around `around`."""
    p_range = np.arange(around[0] - radius, around[0] + radius + 1)
    q_range = np.arange(around[1] - radius, around[1] + radius + 1)
    r_range = np.arange(around[2] - radius, around[2] + radius + 1)
    pqr_mesh = np.meshgrid(p_range, q_range, r_range)
    pqr = np.column_stack([mesh_comp.ravel() for mesh_comp in pqr_mesh])
    pqr = pqr[np.linalg.norm(pqr, axis=1) <= radius]
    p, q, r = pqr.T
    pqr = pqr[(p > 0) | ((p == 0) & (q > 0)) | ((p == 0) & (q == 0) & (r == 1))]
    pqr = pqr // np.gcd(np.gcd(pqr[:, 0], pqr[:, 1]), pqr[:, 2])[:, np.newaxis]
    self.add(np.unique(pqr, axis=0))


class PreferentialDistributionResults(UserDict[Int3, WatsonDistribution]):
  """UserDict holding information about fit results with convenient methods"""

  @property
  def best(self) -> Tuple[Int3, WatsonDistribution]:
    """Return a tuple w/ best fit (most offending) direction & distribution"""
    return list(self.sorted.items())[0]

  @property
  def sorted(self) -> 'PreferentialDistributionResults':
    return self.__class__(sorted(self.items(), key=lambda i: i[1].nll))

  def plot(self, kind: str = 'hedgehog'):
    """Plot all results in self as a hedgehog or hammer plot """
    artists = {'hedgehog': HedgehogArtist, 'hammer': HammerArtist}
    artist = artists[kind]()
    for direction, distribution in self.items():
      hh = Hedgehog(distribution=distribution, color='r', name=str(direction))
      artist.register_hedgehog(hh)
    artist.plot()

  @property
  def table(self) -> str:
    """Prepare a pretty string for logging"""
    table_data = [['Direction', 'kappa', 'mu', 'NLL']]
    for dir_, v in self.sorted.items():
      kappa = f'{v.kappa:+.3f}'
      mu = f'[{v.mu[0]:+.3f},{v.mu[1]:+.3f},{v.mu[2]:+.3f}]'
      nll = f'{v.nll:.2E}'
      table_data.append([str(dir_), kappa, mu, nll])
    return table_utils.format(table_data, has_header=1, delim='  ')


def find_preferential_distribution(
        dsv: DirectSpaceBases,
        space_group: SgtbxSpaceGroup
) -> PreferentialDistributionResults:
  """Look for a preferential orientation along any unique zone axis {hkl}"""
  laue_group = space_group.build_derived_laue_group()
  unique_pseudo_node_generator = UniquePseudoNodeGenerator(laue_group)
  results = PreferentialDistributionResults()
  for upn in unique_pseudo_node_generator:
    a_star = np.cross(dsv.b, dsv.c)  # not normalized by volume!
    b_star = np.cross(dsv.c, dsv.a)  # not normalized by volume!
    c_star = np.cross(dsv.a, dsv.b)  # not normalized by volume!
    vectors = a_star * upn[0] + b_star * upn[1] + c_star * upn[2]
    results[ZoneAxisFamily(upn)] = WatsonDistribution.from_vectors(vectors)
  return results


# ~~~~~~~~~~~~~~~~~~~~~~~~~ ORIENTATION VISUALIZING ~~~~~~~~~~~~~~~~~~~~~~~~~ #

@dataclass
class Hedgehog:
  """Class for holding any `SphericalDistribution` with its metadata"""
  distribution: SphericalDistribution
  color: str
  name: str


class BaseDistributionArtist(abc.ABC):
  """Base class for plotting hedgehogs of vector distributions"""
  PROJECTION: str = NotImplemented

  def __init__(self) -> None:
    self.hedgehogs = []
    from mpl_toolkits.mplot3d import Axes3D  # noqa: required to use 3D axes
    import matplotlib.pyplot as plt
    self.plt = plt
    self._init_figure()

  def _init_figure(self) -> None:
    self.fig = self.plt.figure(constrained_layout=True)
    self.axes = []

  def _generate_axes(self) -> None:
    from matplotlib.gridspec import GridSpec
    len_ = len(self.hedgehogs)
    axes_grid_width = np.ceil(np.sqrt(len_)).astype(int)
    axes_grid_height = np.ceil(len_ / axes_grid_width).astype(int)
    gs = GridSpec(axes_grid_height, axes_grid_width, figure=self.fig)
    for h in range(axes_grid_height):
      for w in range(axes_grid_width):
        ax = self.fig.add_subplot(gs[h, w], projection=self.PROJECTION)
        if len(self.axes) >= len_:
          ax.set_axis_off()
        self.axes.append(ax)

  def register_hedgehog(self, hedgehog: Hedgehog) -> None:
    self.hedgehogs.append(hedgehog)

  @abc.abstractmethod
  def plot(self) -> None:
    pass


class HedgehogArtist(BaseDistributionArtist):
  """Class responsible for drawing distribution of vectors as "hedgehogs"."""
  PROJECTION = '3d'

  def _plot_hedgehog(self, axes: 'plt.Axes', hedgehog: Hedgehog) -> None:
    origin = [0., 0., 0.]
    name = hedgehog.name
    v = hedgehog.distribution.vectors
    mu = hedgehog.distribution.mu
    mu_ring = hedgehog.distribution.mu_ring
    alpha = 1 / np.log2(len(v) + 1E-8)
    axes.quiver(*origin, v[:, 0], v[:, 1], v[:, 2], colors=hedgehog.color,
                alpha=alpha, arrow_length_ratio=0.0)
    axes.quiver(*-mu, *2*mu, colors='k', arrow_length_ratio=0.1)
    axes.plot(mu_ring[:, 0], mu_ring[:, 1], mu_ring[:, 2], color='k')
    axes.set_xlim([-1, 1])
    axes.set_ylim([-1, 1])
    axes.set_zlim([-1, 1])
    axes.set_label(axes.get_label() + ' ' + name if axes.get_label() else name)

  def plot(self):
    self._generate_axes()
    for ax, hedgehog in zip(self.axes, self.hedgehogs):
      self._plot_hedgehog(axes=ax, hedgehog=hedgehog)
    self.plt.show()


def calculate_geographic_heat(vectors: np.ndarray, n_bins: int = 10,
                              ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
  """
  For an array of cartesian vectors, calculate and return:
    - edges of bins in azimuth coordinates,
    - edges of bins in polar coordinates,
    - heatmap on sphere in azimuth/polar coordinates,
  Assuming a geographic definition of polar angle (i.e. from pi/2 to -pi/2).
  """
  polar_edges = np.linspace(-np.pi / 2., np.pi / 2., n_bins + 1)
  azim_edges = np.linspace(-np.pi, np.pi, n_bins + 1)
  r_polar_azim = cart2sph(vectors)
  polar = np.pi / 2 - r_polar_azim[:, 1]
  azim = r_polar_azim[:, 2]
  polar_centers = (polar_edges[:-1] + polar_edges[1:]) / 2
  heat, azim_edges, polar_edges = np.histogram2d(
    x=azim, y=polar, bins=[azim_edges, polar_edges])
  heat = np.divide(heat, np.tile(np.cos(polar_centers), (n_bins, 1)))
  return azim_edges, polar_edges, heat.T  # heat transposed for x/y plotting


class HammerArtist(BaseDistributionArtist):
  """Class responsible for drawing distributions as hammer heatmaps"""
  PROJECTION = 'hammer'

  def _plot_hammer(self, ax: 'plt.Axes', hedgehog: Hedgehog) -> None:
    geo_heat = calculate_geographic_heat(vectors=hedgehog.distribution.vectors)
    ax.grid(False)
    ax.pcolor(*geo_heat, cmap=self.plt.get_cmap('viridis'))
    axes_params = {'ls': '', 'marker': 'o', 'mec': 'w'}  # lab x, y, and z-axes
    ax.plot(0., 0., c='r', **axes_params)
    ax.plot([-np.pi / 2, np.pi / 2], [0., 0.], c='g', **axes_params)
    ax.plot([0., 0.], [-np.pi / 2, np.pi / 2], c='b', **axes_params)
    ax.tick_params(labelbottom=False, labelleft=False)
    ax.set_title(hedgehog.name)

  def plot(self) -> None:
    self._generate_axes()
    for ax, hedgehog in zip(self.axes, self.hedgehogs):
      self._plot_hammer(ax=ax, hedgehog=hedgehog)
    self.plt.show()


def ascii_plot(vectors: np.ndarray, n_bins: int = 10) -> str:
  """A string with geographic heat plot on a simple xy cartesian coords"""
  px_width = 4
  _, _, heat = calculate_geographic_heat(vectors=vectors)
  minh, maxh = np.min(heat), np.max(heat)
  int_heat = np.rint(4.0 / (maxh - minh) * (heat.T - minh)).astype(int)
  colormap = ' '
  plot_array = np.empty((px_width * n_bins + 2, n_bins + 2,), dtype=str)  # x/y
  plot_array[0, 0] = ''
  plot_array[-1, 0] = ''
  plot_array[0, -1] = ''
  plot_array[-1, -1] = ''
  plot_array[1:-1, 0] = ''
  plot_array[1:-1, -1] = ''
  plot_array[0, 1:-1] = ''
  plot_array[-1, 1:-1] = ''
  for azim_i in range(n_bins):
    for polar_i in range(n_bins):
      azim_from = px_width * azim_i + 1
      azim_to = px_width * (azim_i + 1) + 1
      color = colormap[int_heat[azim_i, polar_i]]
      plot_array[azim_from:azim_to, polar_i + 1] = color
  mx = 1 + (px_width * n_bins) // 2
  my = 1 + n_bins // 2
  plot_array[0, my] = 'X'
  plot_array[mx, my] = 'X'
  plot_array[-1, my] = 'X'
  plot_array[mx // 2, my] = 'Y'
  plot_array[mx + mx // 2, my] = 'Y'
  plot_array[mx, 0] = 'Z'
  plot_array[mx, -1] = 'Z'
  return '\n'.join(''.join(c for c in line) for line in plot_array.T)


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ENTRY POINTS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ #


def run(params_) -> None:
  expt_paths = locate_paths(params_.input.glob, params_.input.exclude)
  expt_paths = expt_paths[COMM.rank::COMM.size]
  expts = read_experiments(*expt_paths)
  sgi = params_.input.space_group
  space_group = space_group_auto(expts, COMM)[0] if sgi is Auto else sgi.group()
  abc_stack = DirectSpaceBases.from_expts(expts, space_group)
  if params_.input.symmetrize:
    abc_stack = abc_stack.symmetrize(space_group.build_derived_point_group())
  abc_stacks = COMM.gather(abc_stack)
  if COMM.rank != 0:
    return
  abc_stack = DirectSpaceBases(np.concatenate(abc_stacks, axis=0))
  distributions = find_preferential_distribution(abc_stack, space_group)
  print(distributions.table)

  plot_style = params_.plot.style
  if plot_style != 'none':
    if plot_style == 'ascii':
      for direction, distribution in distributions.items():
        print(f'Ascii distribution heat plot for direction {direction}:')
        print(ascii_plot(distribution.vectors))
    else:
      distributions.plot(plot_style)


def exercise_watson_distribution() -> None:
  ha = HammerArtist()
  for kappa in [-10.0, -1.0, -0.1, -0.01, .000001, 0.01, 0.1, 1.0, 10.]:
    wd = WatsonDistribution(mu=np.array([0, 0, 1]), kappa=kappa)
    wd.sample(1_000_000)
    wd.fit(wd.vectors)
    print(wd)
    hh = Hedgehog(distribution=wd, color='r', name='kappa=' + str(kappa))
    ha.register_hedgehog(hh)
    print(ascii_plot(hh.distribution.vectors))
  ha.plot()


params = []
if __name__ == '__main__':
  if '--help' in sys.argv[1:] or '-h' in sys.argv[1:]:
    print(message)
    exit()
  params = params_from_phil(phil_scope, sys.argv[1:])
  run(params)


 *******************************************************************************


 *******************************************************************************
xfel/util/reflection_length.py
from __future__ import absolute_import, division, print_function
from six.moves import range

from libtbx import easy_pickle
from dials.array_family import flex
from scitbx import matrix
import math
from dials.algorithms.shoebox import MaskCode

class ReflectionsRadialLengths(object):
  """Compute the length of each spotfinder spot in the radial direction."""
  valid_code = MaskCode.Valid | MaskCode.Foreground
  def __init__(self, strong_spots, experiment=None):
    self.strong = strong_spots
    assert 'bbox' in self.strong and 'shoebox' in self.strong, \
      "Spotfinder shoeboxes are required for spot length calculations."
    assert experiment, \
      "Supply one experiment object."
    self.det = experiment.detector
    self.beam = experiment.beam
    self.s0 = matrix.col(self.beam.get_unit_s0())
    self.panel_s0_intersections = flex.vec2_double(
      [self.det[i].get_ray_intersection_px(self.s0) for i in range(len(self.det))])
  def get_one_spot_length_width_angle(self, id):
    # the radial direction is along the vector from the beam center to
    # the spot centroid for each spot
    shoebox = self.strong['shoebox'][id]
    pixel_size = self.det[shoebox.panel].get_pixel_size()
    assert pixel_size[0] == pixel_size[1]
    pixel_size = pixel_size[0]
    s0_position = self.panel_s0_intersections[shoebox.panel]
    centroid = shoebox.centroid_strong().px_xy
    s0_position_lab = matrix.col(self.det[shoebox.panel].get_pixel_lab_coord(s0_position))
    centroid_lab = matrix.col(self.det[shoebox.panel].get_pixel_lab_coord(centroid))
    s0_to_spot = centroid_lab - s0_position_lab
    radial = s0_to_spot.normalize()
    transverse = self.s0.normalize().cross(radial)
    mask = flex.bool([(m & self.valid_code) != 0 for m in shoebox.mask])
    bbox = self.strong['bbox'][id]
    x_start, y_start = bbox[0], bbox[2]
    x_range, y_range = bbox[1] - bbox[0], bbox[3] - bbox[2]
    radial_distances = flex.double()
    transverse_distances = flex.double()
    for i, valid_foreground in enumerate(mask):
      if valid_foreground:
        position = x_start + i%x_range, y_start + i//y_range
        position = matrix.col(self.det[shoebox.panel].get_pixel_lab_coord(position))
        projection_radial = position.dot(radial)
        projection_transverse = position.dot(transverse)
        radial_distances.append(projection_radial)
        transverse_distances.append(projection_transverse)
    length = flex.max(radial_distances) - flex.min(radial_distances)
    width = flex.max(transverse_distances) - flex.min(transverse_distances)
    # The angle subtended is centered at the spot centroid, spanning the
    # spot width. Half this angle makes a right triangle with legs of lengths
    # [distance to beam center] and [half the spot width]. Use the tangent.
    angle = 2*math.atan(width/(2*radial.length()))
    length /= pixel_size
    width /= pixel_size
    return (length, width, angle)
  def get_spot_lengths_px(self):
    self.lengths, self.widths, self.angles = \
      flex.vec3_double([self.get_one_spot_length_width_angle(id) for id in range(len(self.strong))]).parts()
    return self.lengths
  def get_spot_width(self):
    if not hasattr(self, "widths"):
      self.get_spot_lengths_px()
    return self.widths
  def get_spot_subtended_angles_deg(self):
    if not hasattr(self, "angles"):
      self.get_spot_lengths_px()
    return self.angles*180/math.pi
  def get_intensities(self):
    return self.strong['intensity.sum.value']

class ReflectionsRadialLengthsFromFiles(ReflectionsRadialLengths):
  def __init__(self, files):
    from dials.util.options import Importer, flatten_reflections, flatten_experiments
    importer = Importer(files, read_experiments=True,
      read_reflections=True, check_format=False)
    if importer.unhandled:
      print("Unable to handle one or more files:", importer.unhandled)
      return
    reflections = flatten_reflections(importer.reflections)
    assert len(reflections) == 1, "Implemented only for one reflection table at a time presently"
    experiment = None
    if importer.experiments:
      experiments = flatten_experiments(importer.experiments)
      assert len(experiments) == 1, "Implemented only for one experiment at a time presently"
      experiment = experiments[0]
    super(ReflectionsRadialLengthsFromFiles, self).__init__(
      reflections[0], experiment=experiment)

if __name__ == "__main__":
  import sys
  assert len(sys.argv) == 3
  strong_spot_lengths = ReflectionsRadialLengthsFromFiles(sys.argv[1:]).get_spot_lengths_px()
  easy_pickle.dump("spot_lengths_px.pickle", strong_spot_lengths)
  print(list(strong_spot_lengths))


 *******************************************************************************


 *******************************************************************************
xfel/util/show_spot_separation.py
from __future__ import division, print_function
from dxtbx.model import DetectorFactory, Crystal
from iotbx.phil import parse
from dials.command_line.dials_import import  phil_scope as detector_phil_scope
from scitbx.matrix import sqr, col
from dials.array_family import flex
from matplotlib import pyplot as plt
import sys, math

"""
This script uses several approaches to simulate a detector and crystal and predict
spot separation and resolution extent.

Invocation:
libtbx.python show_spot_separation.py detector.phil crystal.phil [parameters]

Where detector.phil looks like (for example):
geometry {
  detector {
    panel
    {
      id = 0
      name = Jungfrau Monolithic
      type = PAD
      gain = 1.47
      pixel_size = (0.075, 0.075)
      image_size = (4096, 4096)
      trusted_range = -1, 65535
      thickness = None
      material = Si
      fast_axis = 1,0,0
      slow_axis = 0,-1,0
      origin = -153.6, 153.6, -250
    }
  }
}

And crystal looks like (for example):
unit_cell = 77, 77, 37, 90, 90, 90
space_group = P43212

detector.phil describes a single panel according to the dxtbx specifications while
crystal.phil describes a unit cell and space group.

"""

crystal_scope = parse("""
space_group = None
  .type = space_group
  .help = "Target space group."
unit_cell = None
  .type = unit_cell
  .help = "Target unit cell."
""")

phil_scope = parse("""
energy = 9500.0
  .type = float
d_min = 2.0
  .type = float
reference_reflection = 20
  .type = int
bandpass = None
  .type = float
  .help = Full width
show_plots=True
  .type = bool
""")

def run(args):
  # read in phil files (detector and crystal)
  d_params = detector_phil_scope.fetch(parse(file_name = args[0])).extract()
  detector = DetectorFactory.from_phil(d_params.geometry)
  print(detector)
  assert len(detector) == 1; panel = detector[0]

  c_params = crystal_scope.fetch(parse(file_name = args[1])).extract()
  unit_cell = c_params.unit_cell
  sg_info = c_params.space_group
  a = sqr(unit_cell.orthogonalization_matrix()) * col((1,0,0))
  b = sqr(unit_cell.orthogonalization_matrix()) * col((0,1,0))
  c = sqr(unit_cell.orthogonalization_matrix()) * col((0,0,1))
  crystal = Crystal(a,b,c,sg_info.group())
  print(crystal)

  # load additional parameters
  user_phil = []
  for arg in args[2:]:
    user_phil.append(parse(arg))
  params = phil_scope.fetch(sources=user_phil).extract()

  energy = float(params.energy)
  wavelength = 12398.4/energy
  s0 = col((0,0,-1/wavelength))
  if params.bandpass is not None:
    wavelength1 = 12398.4/(energy-(params.bandpass/2))
    wavelength2 = 12398.4/(energy+(params.bandpass/2))
  vals = []
  print("Reference reflections 1 and 2, resolutions, two theta (deg) 1 and 2:")
  for axis in range(3):
    m1 = [0,0,0]; m1[axis] += params.reference_reflection
    m2 = [0,0,0]; m2[axis] += params.reference_reflection+1

    # n Lambda = 2dsin(theta)
    d = unit_cell.d(flex.miller_index([m1, m2]))
    try:
      if params.bandpass:
        tt_1 = math.asin(wavelength1/(2*d[0])) * 2
        tt_2 = math.asin(wavelength2/(2*d[1])) * 2
      else:
        tt_1 = math.asin(wavelength/(2*d[0])) * 2
        tt_2 = math.asin(wavelength/(2*d[1])) * 2
    except ValueError: # domain error if resolution is too high
      continue

    # Compute two s1 vectors
    s1_1 = s0.rotate(col((0,1,0)), -tt_1)
    s1_2 = s0.rotate(col((0,1,0)), -tt_2)

    print(m1, m2, list(d), tt_1*180/math.pi, tt_2*180/math.pi)

    # Get panel intersections and compute spacing
    v1 = col(panel.get_ray_intersection_px(s1_1))
    v2 = col(panel.get_ray_intersection_px(s1_2))
    vals.append((v1-v2).length())

  print("Spot separations:", vals)
  print("Smallest spot separation: %7.1f px"%(min(vals)))

  # Hack for quick tests
  assert len(detector)==1
  panel = detector[0]
  fast, slow = panel.get_image_size()
  f = fast//2; s = slow//2
  print("Inscribed resolution, assuming single panel centered detector %.3f:"% \
    min([panel.get_resolution_at_pixel(s0, p) for p in [(f,0),(fast,s),(f,slow),(0,s)]]))

  print("Computing pixel resolutions...")
  resolutions = []
  for panel in detector:
    fast, slow = panel.get_image_size()
    resolutions.append(flex.double(flex.grid(slow, fast)))

    for s in range(slow):
      for f in range(fast):
        resolutions[-1][s,f] = panel.get_resolution_at_pixel(s0, (f, s))

  print("Done")

  d_max = params.d_min * 1.1
  in_range = 0; total = 0
  for r in resolutions:
    in_range += len(r.as_1d().select((r.as_1d()>=params.d_min) & (r.as_1d() <= d_max)))
    total += len(r)

  print("%d of %d pixels between %.2f and %.2f angstroms (%.1f%%)"%(in_range, total, params.d_min, d_max, 100*in_range/total))
  two_theta_d_min = math.asin(wavelength/(2*params.d_min))*2
  d_min_radius_mm = math.tan(two_theta_d_min)*panel.get_distance()
  d_min_radius_px = d_min_radius_mm / panel.get_pixel_size()[0]
  possible_coverage_d_min = math.pi*d_min_radius_px**2
  two_theta_d_max = math.asin(wavelength/(2*d_max))*2
  d_max_radius_mm = math.tan(two_theta_d_max)*panel.get_distance()
  d_max_radius_px = d_max_radius_mm / panel.get_pixel_size()[0]
  possible_coverage_d_max = math.pi*d_max_radius_px**2
  possible_coverage = possible_coverage_d_min - possible_coverage_d_max
  print("Ideal detector would include %d pixels between %.2f-%.2f angstroms"%(possible_coverage, params.d_min, d_max))
  print("Coverage: %d/%d = %.1f%%"%(in_range, possible_coverage, 100*in_range/possible_coverage))

  two_theta_values = flex.double()
  step = (two_theta_d_max - two_theta_d_min)/10
  for i in range(11):
    two_theta_values.append(two_theta_d_max + (step*i))
  s0 = flex.vec3_double(len(two_theta_values), (0,0,-1))
  v = s0.rotate_around_origin((0,1,0), two_theta_values)
  all_v = flex.vec3_double()
  for i in range(720):
    i = i/2
    all_v.extend(v.rotate_around_origin((0,0,-1), i*math.pi/180))

  intersecting_rays = flex.bool()

  for i in range(len(all_v)):
    try:
      panel, mm = detector.get_ray_intersection(all_v[i])
    except RuntimeError:
      intersecting_rays.append(False)
    else:
      intersecting_rays.append(panel >=0 and panel < len(detector))

  print("%d rays out of %d projected between %f and %f intersected the detector (%.1f%%)"% \
    (intersecting_rays.count(True), len(intersecting_rays), params.d_min, d_max, intersecting_rays.count(True)*100/len(intersecting_rays)))

  resolutions[0].set_selected(resolutions[0] > 50, 50)
  if params.show_plots:
    plt.imshow(resolutions[0].as_numpy_array(), cmap='gray')
    plt.colorbar()

    plt.figure()
    r = resolutions[0]
    sel = (r.as_1d()>=params.d_min) & (r.as_1d() <= d_max)
    r.as_1d().set_selected(~sel, 0)
    plt.imshow(r.as_numpy_array(), cmap='gray')
    plt.colorbar()

    plt.show()

if __name__ == "__main__":
  run(sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
xfel/util/split_detector.py
from __future__ import division
from dxtbx.model.experiment_list import ExperimentList
from dxtbx.model.detector import Detector
from dials.array_family import flex
import sys

"""
Utility script for debugging. Example of invocation to split the panels
of a detector into 3x3 subpanels:
libtbx.python `libtbx.find_in_repositories xfel`/util/split_detector.py combined.expt combined.refl 3 3
"""

def split_detector(expts, refls, n_fast, n_slow):
  """
  Utitlity function to split a dxtbx detector model into subpanels.
  Each panel in the detector is split into n_fast x n_slow subpanels
  and reflections are reset to match their new panels.

  Columns affected: panel, xyzcal.mm, xyzcal.px, xyzobs.mm.value,
  xyzobs.mm.variance, xyzobs.px.value, and xyzobs.px.variance.
  """

  if "shoebox" in refls:
    print("Splitting shoeboxes not implemented, deleting them")
    del refls["shoebox"]

  assert len(expts.detectors()) == 1
  detector = expts.detectors()[0]
  new_detector = Detector()
  new_panel_ids = flex.size_t(len(refls))
  refl_f, refl_s, _ = refls["xyzobs.px.value"].parts()

  for panel_id, panel in enumerate(detector):
    image_fast, image_slow = panel.get_image_size()
    fast_axis = panel.get_fast_axis()
    slow_axis = panel.get_slow_axis()

    counter_fast = 0
    for fast in range(n_fast):
      counter_slow = 0
      for slow in range(n_slow):
        fast_pix = image_fast // n_fast + (1 if fast < image_fast % n_fast else 0)
        slow_pix = image_slow // n_slow + (1 if slow < image_slow % n_slow else 0)

        new_panel = new_detector.add_panel()
        new_panel.set_name(panel.get_name() + "_%d"%len(new_detector))
        new_panel.set_image_size((fast_pix, slow_pix))
        new_panel.set_frame(fast_axis, slow_axis, panel.get_pixel_lab_coord((counter_fast, counter_slow)))
        new_panel.set_pixel_size(panel.get_pixel_size())
        new_panel.set_trusted_range(panel.get_trusted_range())
        new_panel.set_thickness(panel.get_thickness())
        new_panel.set_material(panel.get_material())
        new_panel.set_mu(panel.get_mu())
        new_panel.set_gain(panel.get_gain())
        new_panel.set_px_mm_strategy(panel.get_px_mm_strategy())

        sel = (refl_f >= counter_fast) & (refl_f < counter_fast + fast_pix) & \
              (refl_s >= counter_slow) & (refl_s < counter_slow + slow_pix) & \
              (refls["panel"] == panel_id)
        subset = refls.select(sel)

        new_panel_ids.set_selected(sel, flex.size_t(len(subset), len(new_detector)-1))

        def reset_column(column):
          f, s, z = column.parts()
          f -= counter_fast; s -= counter_slow
          f_mm, s_mm = new_panel.pixel_to_millimeter(flex.vec2_double(f,s)).parts()
          return flex.vec3_double(f,s,z), flex.vec3_double(f_mm,s_mm,z)

        reset_px, reset_mm = reset_column(subset["xyzobs.px.value"])
        refls["xyzobs.px.value"].set_selected(sel, reset_px)
        refls["xyzobs.mm.value"].set_selected(sel, reset_mm)

        reset_px, reset_mm = reset_column(subset["xyzcal.px"])
        refls["xyzcal.px"].set_selected(sel, reset_px)
        refls["xyzcal.mm"].set_selected(sel, reset_mm)

        counter_slow += slow_pix
      counter_fast += fast_pix

  refls["panel"] = new_panel_ids
  for expt in expts:
    expt.detector = new_detector

  return expts, refls

if __name__ == "__main__":
  expts = ExperimentList.from_file(sys.argv[1], check_format=False)
  refls = flex.reflection_table.from_file(sys.argv[2])
  if len(sys.argv) > 3:
    n_fast, n_slow = map(int, sys.argv[3:5])
  else:
    n_fast = n_slow = 8
  expts, refls = split_detector(expts, refls, n_fast, n_slow)
  expts.as_file("split_%dx%d.expt"%(n_fast, n_slow))
  refls.as_file("split_%dx%d.refl"%(n_fast, n_slow))


 *******************************************************************************


 *******************************************************************************
xfel/util/weather.py
from __future__ import absolute_import, print_function, division
import matplotlib.pyplot as plt
import sys
import os
from iotbx.detectors.cspad_detector_formats import reverse_timestamp
from libtbx.phil import parse
from libtbx.utils import Sorry
from scitbx.array_family import flex
from scitbx.math import five_number_summary

message = """
A script to get a sense of the computational performance of every rank while
processing data. End product is a plot of wall time vs MPI rank number with
every data point being that of a frame processed by dials.stills_process.
The information is read in from the debug files created by dials.stills_process.
Example usage on cxic0415 processed demo data:
    cctbx.xfel.weather input_path=cxic0415/output/debug
""".strip()
phil_scope = parse('''
  input_path = .
    .type = str
    .help = path to where the processing results are. For example path to XXX_rgYYYY
  num_nodes = 1
    .type = int
    .help = Number of nodes used to do data processing. Used in timing information
  num_cores_per_node = 72
    .type = int
    .help = Number of cores per node in the machine (default is for Cori KNL)
  wall_time = None
    .type = int
    .help = total wall time (seconds) taken for job to finish. Used for plotting node-partitioning
  plot_title = Computational weather plot
    .type = str
    .help = title of the computational weather plot
  show_plot = True
    .type = bool
    .help = flag to indicate if plot should be displayed on screen
  pickle_plot = False
    .type = bool
    .help = If True, will pickle matplotlib session so that it can be opened later for analysis/viewing \
            https://stackoverflow.com/questions/29160177/matplotlib-save-file-to-be-reedited-later-in-ipython
  pickle_filename = fig_object.pickle
    .type = str
    .help = Default name of pickled matplotlib plot saved to disk
''')


def params_from_phil(args):
  user_phil = []
  for arg in args:
    if os.path.isfile(arg):
      user_phil.append(parse(file_name=arg))
    else:
      try:
        user_phil.append(parse(arg))
      except Exception as e:
        raise Sorry("Unrecognized argument: %s" % arg)
  params = phil_scope.fetch(sources=user_phil).extract()
  return params


def timestamp_to_seconds(ts):
  sec, ms = reverse_timestamp(ts)
  return sec + ms * 0.001


def run(params):
  root = params.input_path
  fig_object = plt.figure()
  good_total = fail_total = 0
  fail_deltas = []
  good_deltas = []
  rank_walltimes = []
  for filename in os.listdir(root):
    if os.path.splitext(filename)[1] != '.txt': continue
    if 'debug' not in filename: continue
    reference = None
    fail_timepoints = []
    good_timepoints = []
    run_timepoints = []
    rank = int(filename.split('_')[1].split('.')[0])
    ts = None
    for line in open(os.path.join(root, filename)):
      try:
        hostname, psanats, ts, status, result = line.strip().split(',')
      except ValueError:
        continue
      if reference is None:
        reference = timestamp_to_seconds(ts)
        run_timepoints.append(0)
        assert status not in ['stop', 'done', 'fail']

      if status in ['stop', 'done', 'fail']:
        timepoint = timestamp_to_seconds(ts) - reference
        run_timepoints.append(timepoint)
        if status == 'done':
          good_timepoints.append(timepoint)
          good_deltas.append(good_timepoints[-1] - run_timepoints[-2])
        else:
          fail_timepoints.append(timepoint)
          fail_deltas.append(fail_timepoints[-1] - run_timepoints[-2])
        processing_of_most_recent_still_terminated = True
      else:
        processing_of_most_recent_still_terminated = False
    rank_walltimes.append(timestamp_to_seconds(ts) - reference if ts else 0)
    plt.plot(fail_timepoints, [rank]*len(fail_timepoints), 'b.')
    plt.plot(good_timepoints, [rank]*len(good_timepoints), 'g.')
    fail_total += len(fail_timepoints)
    good_total += len(good_timepoints)
    if not processing_of_most_recent_still_terminated:
      plt.plot([rank_walltimes[-1]], [rank], 'rx')

  msg = "Five number summary of {} (s): {:7.2f}, {:7.2f}, {:7.2f}, {:7.2f}, {:7.2f}"
  if fail_deltas:
    process = '{:5d} fail image processing times'.format(fail_total)
    print(msg.format(process, *five_number_summary(flex.double(fail_deltas))))
  if good_deltas:
    process = '{:5d} good image processing times'.format(good_total)
    print(msg.format(process, *five_number_summary(flex.double(good_deltas))))
  if rank_walltimes:
    process = "{:5d} individual ranks' walltimes".format(len(rank_walltimes))
    print(msg.format(process, *five_number_summary(flex.double(rank_walltimes))))

  if params.wall_time and params.num_nodes and params.num_cores_per_node:
    for i in range(params.num_nodes):
      plt.plot([0, params.wall_time], [i*params.num_cores_per_node-0.5, i*params.num_cores_per_node-0.5], 'r-')
  plt.xlabel('Wall time (sec)')
  plt.ylabel('MPI Rank Number')
  plt.title(params.plot_title)
  if params.pickle_plot:
    from libtbx.easy_pickle import dump
    dump('%s' % params.pickle_filename, fig_object)
  if params.show_plot:
    plt.show()


if __name__ == '__main__':
  if '--help' in sys.argv[1:] or '-h' in sys.argv[1:]:
    print(message)
    exit()
  params = params_from_phil(sys.argv[1:])
  run(params)


 *******************************************************************************
