

 *******************************************************************************
libtbx/scheduling/job_scheduler.py
"""
Job scheduler

Dispatch jobs with each submitted calculation

Common methods:
  has_results(): returns whether there are any results available
  results(): return an iterator yielding ( identifier, result ) tuples
  submit(target, args = (), kwargs = {}): submits job, return identifier
  is_empty(): returns whether there are no more jobs or results waiting
  is_full(): returns whether the number of currently processing jobs is at maximum
  shutdown(): no more job is submitted
  resume(): continues to process waiting jobs
  join(): shutdown, and finish processing all currently running jobs
  terminate(): kills all processing

Scheduler methods:
  job_count(): return number of unfinished jobs (waiting + running)
  process_count(): return number of running processes
"""

from __future__ import absolute_import, division, print_function

import time
from collections import deque
from six.moves.queue import Empty

from libtbx.scheduling import result
from libtbx.scheduling import identifier


# Capacity modes
class limited(object):
  """
  Limited number of jobs
  """

  def __init__(self, njobs):

    self.njobs = njobs


  def is_full(self, njobs):

    return self.njobs <= njobs

  def reduce_capacity_if_possible(self, target = None):
    if target is None or target >= self.njobs:
      target = self.njobs - 1

    if target > 0:
      self.njobs = target
      return True # success
    else:
      self.njobs = 1
      return False

class unlimited(object):
  """
  Unlimited number of jobs (to be used with submission queue

  Note: this is a singleton object
  """

  @staticmethod
  def is_full(njobs):

    return False


def job_cycle(outqueue, jobid, target, args, kwargs):

  try:
    value = target( *args, **kwargs )

  except Exception as e:
    res = result.error( exception = e, traceback = result.get_traceback_info() )

  else:
    res = result.success( value = value )

  outqueue.put( ( jobid, res ) )


class manager(object):
  """
  Job scheduler
  """

  def __init__(self, inqueue, job_factory, capacity, waittime = 0.01):

    self.inqueue = inqueue
    self.job_factory = job_factory
    self.capacity = capacity

    self.waittime = waittime

    self.process_data_for = {}
    self.waiting_results = set()
    self.waiting_jobs = deque()
    self.completed_results = deque()

    self.resume()


  def job_count(self):

    return len( self.process_data_for ) + len( self.waiting_jobs )


  def process_count(self):

    return len( self.process_data_for )


  def is_empty(self):

    return not (
      self.waiting_jobs or self.process_data_for or self.waiting_results
      or self.completed_results
      )


  def is_full(self):

    return self.capacity.is_full( njobs = self.process_count() )


  def has_results(self):

    return self.completed_results


  def results(self):

    self.poll()

    while (
      self.process_data_for or self.waiting_results or self.completed_results
      or ( self.waiting_jobs and self.active )
      ):
      while not self.has_results():
        self.wait()
        self.poll()

      yield self.completed_results.popleft()


  def submit(self, target, args = (), kwargs = {}):

    jobid = identifier()
    self.waiting_jobs.append( ( jobid, target, args, kwargs ) )
    self.poll()
    return jobid


  def shutdown(self):

    self.active = False


  def resume(self):

    self.active = True


  def join(self):

    while self.process_data_for:
      self.poll()
      self.wait()

    self.poll()


  def terminate(self):

    self.shutdown()

    for process in self.process_data_for.values():
      if process.is_alive():
        if hasattr( process, "terminate" ): # Thread has no terminate
          try:
            process.terminate()

          except Exception:
            pass

    self.join()


  # Internal methods
  def wait(self):

    time.sleep( self.waittime )


  def poll(self):

    # Check existing jobs
    for jobid in list(self.process_data_for):
      process = self.process_data_for[ jobid ]

      if not process.is_alive():
        self.finish_job( jobid = jobid )

    # Collect results
    while True:
      try:
        ( jobid, res ) = self.inqueue.get( timeout = self.waittime )

      except Empty:
        break

      if jobid in self.process_data_for:
        self.finish_job( jobid = jobid )

      self.waiting_results.remove( jobid )
      self.completed_results.append( ( jobid, res ) )

    # Submit new jobs
    while ( not self.capacity.is_full( njobs = self.process_count() )
      and self.waiting_jobs and self.active ):
      ( jobid, target, args, kwargs ) = self.waiting_jobs.popleft()

      process = self.job_factory(
        target = job_cycle,
        args = ( self.inqueue, jobid, target, args, kwargs ),
        )
      try:
        process.start()
      except Exception as e:
        # It will crash if process cannot start. See if we can just reduce
        #   capacity
        if hasattr(self.capacity, 'reduce_capacity_if_possible'):
          ok = self.capacity.reduce_capacity_if_possible(
            target = self.process_count())
          if ok:
            continue # back to top
        raise Exception(e) # Process could not start

      self.process_data_for[ jobid ] = process


  def finish_job(self, jobid):

    process = self.process_data_for[ jobid ]
    process.join()
    exit_code = getattr( process, "exitcode", 0 ) # Thread has no "exitcode" attribute

    if exit_code != 0:
      res = result.error(
        exception = result.get_exception( process = process, exit_code = exit_code ),
        traceback = result.get_crash_info( process = process ),
        )
      self.completed_results.append( ( jobid, res ) )

    else:
      self.waiting_results.add( jobid )

    del self.process_data_for[ jobid ]


class creator(object):
  """
  Information to create and destroy manager
  """

  def __init__(self, job_factory, queue_factory, capacity, waittime = 0.01):

    self.job_factory = job_factory
    self.queue_factory = queue_factory
    self.capacity = capacity
    self.waittime = waittime


  def create(self):

    return manager(
      inqueue = self.queue_factory.create(),
      job_factory = self.job_factory,
      capacity = self.capacity,
      waittime = self.waittime,
      )


  def destroy(self, manager):

    manager.terminate()
    manager.join()
    self.queue_factory.destroy( manager.inqueue )


 *******************************************************************************


 *******************************************************************************
libtbx/scheduling/mainthread.py
"""
Mainthread

Offers low-overhead execution on a single thread

Methods:
  has_results(): returns whether there are any results available
  results(): return an iterator yielding ( identifier, result ) tuples
  submit(target, args = (), kwargs = {}): submits job, return identifier
  is_empty(): returns whether there are no more jobs or results waiting
  is_full(): returns whether there are any jobs waiting
  shutdown(): no effect
  join(): finish processing all jobs
  terminate(): no effect
"""

from __future__ import absolute_import, division, print_function

from collections import deque

from libtbx.scheduling import identifier
from libtbx.scheduling import result


class manager(object):
  """
  Mainthread manager
  """

  def __init__(self):

    self.outqueue = deque()
    self.inqueue = deque()


  def job_count(self):

    return len( self.outqueue )


  def has_results(self):

    return self.inqueue


  def results(self):

    while not self.is_empty():
      while not self.has_results():
        self.poll()

      yield self.inqueue.popleft()


  def submit(self, target, args = (), kwargs = {}):

    jobid = identifier()
    self.outqueue.append( ( jobid, target, args, kwargs ) )
    return jobid


  def is_empty(self):

    return not self.outqueue and not self.inqueue


  def is_full(self):

    return self.outqueue


  def join(self):

    while self.outqueue:
      self.poll()


  def shutdown(self):

    pass


  def resume(self):

    pass


  def terminate(self):

    pass


  # Internal methods
  def poll(self):

    if self.outqueue:
      ( jobid, target, args, kwargs ) = self.outqueue.popleft()

      try:
        value = target( *args, **kwargs )

      except Exception as e:
        res = result.error( exception = e, traceback = result.get_traceback_info() )

      else:
        res = result.success( value = value )

      self.inqueue.append( ( jobid, res ) )


class creator(object):
  """
  Creator for manager

  Note this is a singleton object
  """

  @staticmethod
  def create():

    return manager()


  @staticmethod
  def destroy(manager):

    pass


 *******************************************************************************


 *******************************************************************************
libtbx/scheduling/mp_handler.py
from __future__ import absolute_import, division, print_function

import multiprocessing

class stderr_capturing_process(multiprocessing.Process):
  """
  A Process-specialization that adds the .err attribute
  """

  def __init__(
    self,
    group = None,
    target = None,
    name = None,
    propagate_error_message = False,
    args = (),
    kwargs = {},
    ):

    super( stderr_capturing_process, self ).__init__(group, target, name, args, kwargs)
    self.propagate_error_message = propagate_error_message

    import tempfile
    import os
    ( fd, self.errfile ) = tempfile.mkstemp()
    os.close( fd )


  def run(self):

    stderr = open( self.errfile, "w" )
    import sys
    #  XXX CATCH CASE WHERE SYS.STDERR HAS BEEN MODIFIED
    stderr_fileno = sys.stderr.fileno() if hasattr(sys.stderr, 'fileno') else 2
    sys.stderr = stderr # adjust Python level
    import os
    os.dup2( stderr.fileno(), stderr_fileno ) # adjust OS level

    super( stderr_capturing_process, self ).run()


  def join(self, timeout = None):

    super( stderr_capturing_process, self ).join( timeout )

    if not self.errfile:
      return

    try:
      stderr = open( self.errfile )
      self.stacktrace = stderr.read()

    except IOError:
      self.stacktrace = ""

    else:
      stderr.close()

      import os
      os.remove( self.errfile )

    finally:
      self.errfile = None

    if self.propagate_error_message and self.stacktrace:
      import sys
      sys.stderr.write( self.stacktrace )
      sys.stderr.write( "\n" )


class fifo_qfactory(object):
  """
  Creator pattern for multiprocessing.Queue, also include destruction

  Note this is a singleton object
  """

  @staticmethod
  def create():

    return multiprocessing.Queue()


  @staticmethod
  def destroy(queue):

    pass


class managed_qfactory(object):
  """
  Creator pattern for multiprocessing.Manager.Queue, also include destruction

  Note this is not a singleton object
  """

  def __init__(self):

    self.manager = multiprocessing.Manager()


  def create(self):

    return self.manager.Queue()


  def destroy(self, queue):

    pass


 *******************************************************************************


 *******************************************************************************
libtbx/scheduling/parallel_for.py
from __future__ import absolute_import, division, print_function

from builtins import object
from collections import deque
from six.moves import range, zip

class single_pooler(object):
  """
  Does not group jobs
  """

  @staticmethod
  def submit_one_job(calcsiter, manager):

    ( target, args, kwargs ) = next(calcsiter)
    return (
      manager.submit( target = target, args = args, kwargs = kwargs ),
      ( target, args, kwargs ),
      )


  @staticmethod
  def process_one_job(calculation, result):

    return ( calculation, result )


  @staticmethod
  def insert_result_into(result, resiter):

    resiter.append( result )


class pooled_run(object):
  """
  Runs multiple jobs
  """

  def __init__(self, calculations):

    self.calculations = calculations


  def __call__(self):

    from libtbx.scheduling import result
    results = []

    for ( target, args, kwargs ) in self.calculations:
      try:
        value = target( *args, **kwargs )

      except Exception as e:
        results.append( result.error( exception = e ) )

      else:
        results.append( result.success( value = value ) )

    return results


class multi_pooler(object):
  """
  Pools up a number of jobs and runs this way

  size - number of jobs to pool
  """

  def __init__(self, size):

    assert 0 < size
    self.size = size


  def submit_one_job(self, calcsiter, manager):

    calculations = [ c for ( i, c ) in zip( range(self.size), calcsiter ) ]

    if not calculations:
      raise StopIteration

    return (
      manager.submit( target = pooled_run( calculations = calculations ) ),
      calculations,
      )


  @staticmethod
  def process_one_job(calculation, result):

    try:
      results = result()

    except Exception:
      return [ ( c, result ) for c in calculation ]

    else:
      assert len( results ) == len( calculation )
      return [ ( c, r ) for ( c, r ) in zip( calculation, results ) ]


  @staticmethod
  def insert_result_into(result, resiter):

    resiter.extend( result )


class finishing_order(object):
  """
  Orders output finishing order
  """

  @staticmethod
  def next_submitted_job(identifier):

    pass


  @staticmethod
  def next_returned_result(identifier, result, pooler, resiter):

    pooler.insert_result_into( result = result, resiter = resiter )


class submission_order(object):
  """
  Orders output in submission order
  """

  def __init__(self):

    self.identifiers = deque()
    self.result_for = {}


  def next_submitted_job(self, identifier):

    self.identifiers.append( identifier )


  def next_returned_result(self, identifier, result, pooler, resiter):

    self.result_for[ identifier ] = result

    while self.identifiers and self.identifiers[0] in self.result_for:
      top = self.identifiers.popleft()
      pooler.insert_result_into( result = self.result_for[ top ], resiter = resiter )
      del self.result_for[ top ]


class ongoing_state(object):
  """
  The iteration is ongoing, iterable has not been exhausted

  Note this is a singleton
  """

  @staticmethod
  def iterate(pfi):

    ( identifier, calcdata ) = pfi.pooler.submit_one_job(
      calcsiter = pfi.calcsiter,
      manager = pfi.manager,
      )

    pfi.calculation_data_for[ identifier ] = calcdata
    pfi.orderer.next_submitted_job( identifier = identifier )


  @classmethod
  def fillup(cls, pfi):

    if pfi.manager.is_empty():
      cls.iterate( pfi = pfi )

    while not pfi.manager.is_full():
      cls.iterate( pfi = pfi )


  @staticmethod
  def emptyhandler():

    pass


class exhausted_state(object):
  """
  The iteration is finishing, iterable has been exhausted

  Note this is a singleton
  """

  @staticmethod
  def fillup(pfi):

    pass


  @staticmethod
  def emptyhandler():

    raise StopIteration


class iterator(object):
  """
  Creates an iterator that executes calls on a Manager-like object

  calculations - an iterable of calculations yielding ( target, args, kwargs ) tuples
  manager - execution manager
  """

  def __init__(self, calculations, manager, poolsize = 1, keep_input_order = False):

    self.manager = manager
    self.calcsiter = iter( calculations )
    self.resiter = deque()

    assert 0 < poolsize

    if poolsize == 1:
      self.pooler = single_pooler

    else:
      self.pooler = multi_pooler( size = poolsize )

    self.calculation_data_for = {}

    if keep_input_order:
      self.orderer = submission_order()

    else:
      self.orderer = finishing_order

    self.resume()


  def __iter__(self):

    return self


  def __next__(self):

    while not self.resiter:
      self.process_next_one()

      try:
        self.state.fillup( pfi = self )

      except StopIteration:
        self.state = exhausted_state

    return self.resiter.popleft()


  def suspend(self):

    self.state = exhausted_state


  def resume(self):

    self.state = ongoing_state


  # Internal
  def process_next_one(self):

    try:
      ( identifier, result ) = next(self.manager.results()) # raise StopIteration
      processed = self.pooler.process_one_job(
        calculation = self.calculation_data_for[ identifier ],
        result = result,
        )
      self.orderer.next_returned_result(
        identifier = identifier,
        result = processed,
        pooler = self.pooler,
        resiter = self.resiter,
        )

    except StopIteration:
      self.state.emptyhandler()


 *******************************************************************************


 *******************************************************************************
libtbx/scheduling/philgen.py
from __future__ import absolute_import, division, print_function

from libtbx.scheduling import SetupError

# Module exception
class ConfigurationError(SetupError):
  """
  Error in setting up the PHIL generator
  """

# Multiprocessing queue options
def mp_fifo_queue():

  from libtbx.scheduling import mp_handler
  return mp_handler.fifo_qfactory


def mp_managed_queue():

  from libtbx.scheduling import mp_handler
  return mp_handler.managed_qfactory()


# Cluster queue options
class cluster_file_queue(object):

  def __init__(self, prefix = "tmp", folder = ".", waittime = 0.1, multifile = None):

    self.prefix = prefix
    self.folder = folder
    self.waittime = waittime
    self.multifile = multifile


  def __call__(self, params):

    from libtbx.scheduling import file_queue
    qfac = file_queue.qfactory(
      prefix = self.prefix,
      folder = self.folder,
      waittime = self.waittime,
      )

    if self.multifile is not None:
      mqfac = file_queue.mqfactory(
        count = self.multifile,
        prefix = self.prefix,
        folder = self.folder,
        waittime = self.waittime,
        )

    else:
      mqfac = qfac

    return ( mqfac , qfac, mqfac )


  def __str__(self):

    return ""


class cluster_socket_queue(object):

  def __init__(self, port = 0, keylength = 16):

    self.port = port
    self.keylength = keylength


  def __call__(self, params):

    from libtbx.scheduling import socket_queue
    qfac = socket_queue.QFactory( port = self.port, keylength = self.keylength )

    return ( qfac , qfac, qfac )


  def __str__(self):

    return ""


# Pool lifecycle controller options
def pool_unlimited_lifecycle():

  from libtbx.scheduling import process_pool
  return process_pool.unlimited


class pool_runtime_limit_lifecycle(object):

  def __init__(self, value):

    self.seconds = value


  def __call__(self):

    from libtbx.scheduling import process_pool
    import functools
    return functools.partial( process_pool.runtime_limit, seconds = self.seconds )


class pool_wallclock_limit_lifecycle(object):

  def __init__(self, value):

    self.seconds = value


  def __call__(self):

    from libtbx.scheduling import process_pool
    import functools
    return functools.partial( process_pool.wallclock_limit, seconds = self.seconds )


class pool_jobcount_limit_lifecycle(object):

  def __init__(self, value):

    self.maxjobs = value


  def __call__(self):

    from libtbx.scheduling import process_pool
    import functools
    return functools.partial( process_pool.jobcount_limit, maxjobs = self.maxjobs )


# Pool autoscaling options
def pool_constant_capacity(ncpus):

  from libtbx.scheduling import process_pool
  return process_pool.constant_capacity( capacity = ncpus )


class pool_upscaling_capacity(object):

  def __init__(self, minimum = 0, buffering = 1):

    self.minimum = minimum
    self.buffering = buffering


  def __call__(self, ncpus):

    from libtbx.scheduling import process_pool
    return process_pool.upscaling_capacity(
      minimum = self.minimum,
      maximum = ncpus,
      buffering = self.buffering,
      )


class pool_breathing_capacity(object):

  def __init__(self, minimum = 0, buffering = 1, downsize_step = 2, downsize_delay = 5):

    self.minimum = minimum
    self.buffering = buffering
    self.downsize_step = downsize_step
    self.downsize_delay = downsize_delay


  def __call__(self, ncpus):

    from libtbx.scheduling import process_pool
    return process_pool.breathing_capacity(
      minimum = self.minimum,
      maximum = ncpus,
      buffering = self.buffering,
      downsize_step = self.downsize_step,
      downsize_delay = self.downsize_delay,
      )


# Classes describing aspects of the system
class constraint(object):
  """
  Properties of the call to be parallelized
  """

  def __init__(
    self,
    pickleable_target = False,
    pickleable_return_value = False,
    offload_main_thread = False,
    concurrent_python_execution = True
    ):

    self.pickleable_target = pickleable_target
    self.pickleable_return_value = pickleable_return_value
    self.offload_main_thread = offload_main_thread
    self.concurrent_python_execution = concurrent_python_execution


class ability(object):
  """
  Properties of the parallelization technology
  """

  def __init__(
    self,
    allows_startup_for_nonpickleable_target,
    allows_transfer_for_nonpickleable_object,
    allows_unlimited_capacity,
    allows_concurrent_python_execution,
    ):

    self.allows_startup_for_nonpickleable_target = allows_startup_for_nonpickleable_target
    self.allows_transfer_for_nonpickleable_object = allows_transfer_for_nonpickleable_object
    self.allows_unlimited_capacity = allows_unlimited_capacity
    self.allows_concurrent_python_execution = allows_concurrent_python_execution


class setting(object):
  """
  Data for setting up a parallel run
  """

  def __init__(
    self,
    jfactory,
    qfactory,
    inqfactory,
    outqfactory,
    lifecycle,
    ):

    self.jfactory = jfactory
    self.qfactory = qfactory
    self.inqfactory = inqfactory
    self.outqfactory = outqfactory
    self.lifecycle = lifecycle


# Technologies
class threading(object):
  """
  Phil generator for threading option
  """

  def __init__(
    self,
    capture_exception = False,
    pool_lifecycle = pool_unlimited_lifecycle,
    ):

    self.capture_exception = capture_exception
    self.pool_lifecycle = pool_lifecycle


  def abilities(self):

    return ability(
      allows_startup_for_nonpickleable_target = True,
      allows_transfer_for_nonpickleable_object = True,
      allows_unlimited_capacity = False,
      allows_concurrent_python_execution = False,
      )


  def jfactory(self):

    if self.capture_exception:
      from libtbx.scheduling import thread_handler
      jfactory = thread_handler.exception_capturing_thread

    else:
      import threading
      jfactory = threading.Thread

    return jfactory


  def qfactory(self):

    from libtbx.scheduling import thread_handler

    return ( thread_handler.qfactory, ) * 3


  def settings(self, params):

    # No user-controlled parameters expected
    ( qfac, inqfac, outqfac ) = self.qfactory()

    return setting(
      jfactory = self.jfactory(),
      qfactory = qfac,
      inqfactory = inqfac,
      outqfactory = outqfac,
      lifecycle = self.pool_lifecycle,
      )


  def phil(self):

    return ""


class multiprocessing(object):
  """
  Phil generator for multiprocessing option
  """

  def __init__(
    self,
    capture_stderr = True,
    qtype = mp_fifo_queue,
    pool_lifecycle = pool_unlimited_lifecycle,
    ):

    self.capture_stderr = capture_stderr
    self.qtype = qtype
    self.pool_lifecycle = pool_lifecycle


  def abilities(self):

    import sys
    return ability(
      allows_startup_for_nonpickleable_target = not sys.platform.startswith( "win32" ),
      allows_transfer_for_nonpickleable_object = False,
      allows_unlimited_capacity = False,
      allows_concurrent_python_execution = True,
      )


  def jfactory(self):

    if self.capture_stderr:
      from libtbx.scheduling import mp_handler
      jfactory = mp_handler.stderr_capturing_process

    else:
      import multiprocessing
      jfactory = multiprocessing.Process

    return jfactory


  def qfactory(self):

    return ( self.qtype(), ) * 3


  def settings(self, params):

    # No user-controlled parameters expected
    ( qfac, inqfac, outqfac ) = self.qfactory()

    return setting(
      jfactory = self.jfactory(),
      qfactory = qfac,
      inqfactory = inqfac,
      outqfactory = outqfac,
      lifecycle = self.pool_lifecycle,
      )


  def phil(self):

    return ""


class cluster(object):
  """
  Phil generator for managed cluster option (processing package)
  """

  def __init__(
    self,
    platforms = [ "sge", "lsf", "pbs", "pbspro", "condor", "slurm" ],
    queue_philgen_for = {
      "file": cluster_file_queue(),
      "network": cluster_socket_queue(),
      },
    default_queue_option = "file",
    lifecycle_factory_for = {
      "runtime": pool_runtime_limit_lifecycle,
      "wallclock": pool_wallclock_limit_lifecycle,
      },
    default_lifecycle_factory = pool_unlimited_lifecycle,
    name = "libtbx_python",
    asynchronous = True,
    use_target_file = True,
    include = None,
    poller = None,
    handler = None,
    capture_stderr = True,
    ):

    assert platforms
    assert queue_philgen_for
    assert default_queue_option in queue_philgen_for

    self.platforms = platforms
    self.queue_philgen_for = queue_philgen_for
    self.default_queue_option = default_queue_option
    self.lifecycle_factory_for = lifecycle_factory_for
    self.default_lifecycle_factory = default_lifecycle_factory

    # These settings are not user controlled
    self.name = name
    self.asynchronous = asynchronous
    self.use_target_file = use_target_file
    self.include = include
    self.poller = poller
    self.handler = handler
    self.capture_stderr = capture_stderr


  def abilities(self):

    return ability(
      allows_startup_for_nonpickleable_target = False,
      allows_transfer_for_nonpickleable_object = False,
      allows_unlimited_capacity = True,
      allows_concurrent_python_execution = True,
      )


  def jfactory(self, platform, command):

    from libtbx.scheduling import cluster_handler

    return cluster_handler.JobFactory(
      platform = platform,
      name = self.name,
      command = command,
      asynchronous = self.asynchronous,
      use_target_file = self.use_target_file,
      include = self.include,
      poller = self.poller,
      handler = self.handler,
      save_error = self.capture_stderr,
      )


  def settings(self, params):

    ( qfac, inqfac, outqfac ) = self.queue_philgen_for[ params.channel.use ](
      params = getattr( params.channel, params.channel.use, None ),
      )

    if params.limit.type is None:
      lifecycle = self.default_lifecycle_factory

    else:
      lifecycle = self.lifecycle_factory_for[ params.limit.type ]( value = params.limit.value )

    return setting(
      jfactory = self.jfactory( platform = params.platform, command = params.command ),
      qfactory = qfac,
      inqfactory = inqfac,
      outqfactory = outqfac,
      lifecycle = lifecycle,
      )


  def phil(self):

    platform = """
platform = %(platform)s
  .help = "Management software platform"
  .type = choice
  .optional = False
""" % {
  "platform": phil_choice( choices = self.platforms, default = self.platforms[0] ),
  }

    command = """
command = None
  .help = "Custom submission command"
  .type = strings
  .optional = False
"""

    limit = """
limit
  .help = "Job limits"
{
  type = %(choices)s
    .help = "Limit type"
    .type = choice
    .optional = True

  value = None
    .help = "Limit value (s)"
    .type = int( value_min = 1 )
    .optional = False
}
""" % {
  "choices": phil_choice( choices = self.lifecycle_factory_for, default = None ),
  }

    queues = """
channel
  .help = "Data channel setup"
{
  use = %(choices)s
    .help = "Channel type"
    .type = choice
    .optional = False

  %(options)s
}
""" % {
  "choices": phil_choice(
    choices = self.queue_philgen_for,
    default = self.default_queue_option,
    ),
  "options": "\n".join( "%s%s" % ( k ,v ) for ( k, v ) in self.queue_philgen_for.items() if str( v ) )
  }


    return """
.help = "Managed cluster parameters"
{
%(platform)s
%(command)s
%(limit)s
%(queues)s
}
""" % {
  "platform": platform,
  "command": command,
  "limit": limit,
  "queues": queues,
  }


# Scheduler backends
class main_thread_backend(object):
  """
  Creates a mainthread.manager

  Note: this is a singleton object
  """

  @staticmethod
  def is_valid_option(constraint):

    if constraint.offload_main_thread:
      return False

    return True


  @staticmethod
  def creator():

    from libtbx.scheduling import mainthread

    return mainthread.creator


class job_scheduler_backend(object):
  """
  Creates a job_scheduler.manager
  """

  def __init__(self, waittime = 0.01):

    self.waittime = waittime


  def is_valid_option(self, ability, constraint):

    if not constraint.pickleable_target and not ability.allows_startup_for_nonpickleable_target:
      return False

    if not constraint.pickleable_return_value and not ability.allows_transfer_for_nonpickleable_object:
      return False

    return True


  def creator(self, technology, ncpus, params):

    from libtbx.scheduling import job_scheduler

    if ncpus is None: # special for unlimited option
      ability = technology.abilities()

      if not ability.allows_unlimited_capacity:
        raise ConfigurationError("option does not allow unlimited capacity (ncpus=None)")

      else:
        capacity = job_scheduler.unlimited

    else:
      capacity = job_scheduler.limited( njobs = ncpus )

    setting = technology.settings( params = params )

    return job_scheduler.creator(
      job_factory = setting.jfactory,
      queue_factory = setting.qfactory,
      capacity = capacity,
      waittime = self.waittime,
      )


class process_pool_backend(object):
  """
  Creates a process_pool.manager
  """

  def __init__(
    self,
    autoscaling = pool_breathing_capacity(),
    waittime = 0.01,
    stalltime = 2,
    idle_timeout = 120,
    ):

    self.autoscaling = autoscaling
    self.waittime = waittime
    self.stalltime = stalltime
    self.idle_timeout = idle_timeout


  def is_valid_option(self, ability, constraint):

    if not constraint.pickleable_target and not ability.allows_transfer_for_nonpickleable_object:
      return False

    if not constraint.pickleable_return_value and not ability.allows_transfer_for_nonpickleable_object:
      return False

    return True


  def creator(self, technology, ncpus, params):

    from libtbx.scheduling import process_pool

    if ncpus is None: # special for unlimited option
      raise ConfigurationError("option does not allow unlimited capacity (ncpus=None)")

    setting = technology.settings( params = params )

    return process_pool.creator(
      job_factory = setting.jfactory,
      inq_factory = setting.inqfactory,
      outq_factory = setting.outqfactory,
      autoscaling = self.autoscaling( ncpus = ncpus ),
      lifecycle = setting.lifecycle(),
      waittime = self.waittime,
      stalltime = self.stalltime,
      idle_timeout = self.idle_timeout,
      )


# Parallelisation options
class parallel_option(object):
  """
  Valid technology/backend combination
  """

  def __init__(self, caption, technology, backend):

    self.caption = caption
    self.technology = technology
    self.backend = backend


  def __call__(self, ncpus, params):

    return self.backend.creator(
      technology = self.technology,
      ncpus = ncpus,
      params = params,
      )


class single_cpu_option(object):
  """
  Valid technology/backend combination forcing single CPU
  """

  def __init__(self, caption, technology, backend):

    self.caption = caption
    self.technology = technology
    self.backend = backend


  def __call__(self, ncpus, params):

    return self.backend.creator(
      technology = self.technology,
      ncpus = 1,
      params = params,
      )


class main_thread_option(object):
  """
  Special option for mainthread execution
  """

  def __init__(self, caption):

    self.caption = caption


  def __call__(self, ncpus, params):

    return main_thread_backend.creator()


class single(object):
  """
  No parallel execution
  """

  def __init__(
    self,
    caption = "single",
    backend = job_scheduler_backend(),
    ):

    self.caption = caption
    self.backend = backend


  def __call__(self, constraint, technologies):

    if main_thread_backend.is_valid_option( constraint = constraint ):
      return main_thread_option( caption = self.caption )

    else:
      for technology in technologies:
        if self.backend.is_valid_option(
          ability = technology.abilities(),
          contraint = constraint,
          ):
          return single_cpu_option(
            caption = self.caption,
            technology = technology,
            backend = self.backend,
            )

      raise ConfigurationError("No valid '%s' option with these constraints" % self.caption)


class multicore(object):
  """
  Parallel execution within a physical machine
  """

  def __init__(
    self,
    caption = "multicore",
    backends = [
      process_pool_backend(),
      job_scheduler_backend(),
      ],
    ):

    self.caption = caption
    self.backends = backends


  def __call__(self, constraint, technologies):

    if constraint.concurrent_python_execution:
      usefuls = [ t for t in technologies if t.abilities().allows_concurrent_python_execution ]

    else:
      usefuls = technologies

    import itertools

    for ( backend, tech ) in itertools.product( self.backends, usefuls ):
      if backend.is_valid_option( ability = tech.abilities(), constraint = constraint ):
        return parallel_option(
          caption = self.caption,
          technology = tech,
          backend = backend,
          )

    raise ConfigurationError("No valid '%s' option with these constraints" % self.caption)


class batch(object):
  """
  Parallel execution via a batch queue
  """

  def __init__(
    self,
    caption = "cluster",
    backend = process_pool_backend(),
    conf_cluster = cluster(),
    ):

    self.caption = caption
    self.backend = backend
    self.conf_cluster = conf_cluster


  def __call__(self, constraint):

    if self.backend.is_valid_option(
      ability = self.conf_cluster.abilities(),
      constraint = constraint,
      ):
      return parallel_option(
        caption = self.caption,
        technology = self.conf_cluster,
        backend = self.backend,
        )

    raise ConfigurationError("No valid '%s' option with these constraints" % self.caption)


  def __str__(self):

    return "%s%s" % ( self.caption, self.conf_cluster.phil() )


class manager(object):
  """
  Creates PHIL scope for setting up a manager
  """

  def __init__(
    self,
    constraint,
    caption = "concurrency",
    single = single(),
    multicore = multicore(),
    extras = [ batch() ],
    conf_threading = threading(),
    conf_multiprocessing = multiprocessing(),
    prefer_mp = False,
    ):

    self.caption = caption
    self.single = single
    self.multicore = multicore

    if prefer_mp:
      technologies = [ conf_multiprocessing, conf_threading ]

    else:
      technologies = [ conf_threading, conf_multiprocessing ]

    self.option_for = {}

    try:
      self.option_for[ single.caption ] = single(
        constraint = constraint,
        technologies = technologies,
        )

    except ConfigurationError:
      raise ConfigurationError("'%s' option not valid with this setup" % single.caption)

    try:
      self.option_for[ multicore.caption ] = multicore(
        constraint = constraint,
        technologies = technologies,
        )

    except ConfigurationError:
      pass

    self.extras = []

    for platform in extras:
      try:
        self.option_for[ platform.caption ] = platform( constraint = constraint )

      except ConfigurationError:
        continue

      self.extras.append( platform )


  def __call__(self, params):

    scope = getattr( params, self.caption )

    if scope.ncpus == 1 and scope.technology == self.multicore.caption:
      technology = self.single.caption

    else:
      technology = scope.technology

    try:
      return self.option_for[ technology ](
        ncpus = scope.ncpus,
        params = getattr( scope, technology, None ),
        )

    except ConfigurationError as e:
      from libtbx.utils import Sorry
      raise Sorry("Error setting up '%s': %s" % ( technology, e ))


  def __str__(self):

    return """
%(caption)s
  .help = "Concurrency parameters"
{
  technology = %(technology)s
    .help = "Parallelisation option to use"
    .type = choice
    .optional = False

  ncpus = 1
    .help = "Maximum number of CPUs"
    .type = int( value_min = 1 )
    .optional = False

  %(others)s
}
""" % {
  "caption": self.caption,
  "technology": phil_choice(
    choices = self.option_for,
    default = self.single.caption,
    ),
  "others": "\n".join( str( p ) for p in self.extras ),
  }


def phil_choice(choices, default):

  if not choices:
    return None

  if default is not None and default not in choices:
    raise ValueError("Default not among choices")

  return " ".join( "*%s" % c if c == default else c for c in choices )


 *******************************************************************************


 *******************************************************************************
libtbx/scheduling/process_pool.py
"""
Process Pool

Pools offer lower control over execution, as it is not possible to know the
state of individual workers

Common methods:
  has_results(): returns whether there are any results available
  results(): return an iterator yielding ( identifier, result ) tuples
  submit(target, args = (), kwargs = {}): submits job, return identifier
  is_empty(): returns whether there are no more jobs or results waiting
  is_full(): returns whether the number of currently processing jobs is at maximum
  shutdown(): sets the number of workers to zero
  resume(): starts workers as necessary
  join(): finish processing all jobs, and then shutdown
  terminate(): kills all processing

Pool methods:
  job_count(): return number of jobs submitted (waiting + running)
  worker_count(): return an approximate number of active workers
"""

from __future__ import absolute_import, division, print_function

import time
from collections import deque
from six.moves.queue import Empty, Full

from libtbx.scheduling import SchedulingError, identifier
from libtbx.scheduling import result

# Accounting
class process_register(object):
  """
  A simple class that encapsulates job accounting
  """

  def __init__(self):

    self.running_on = {}
    self.terminateds = deque()
    self.requested_shutdowns = deque()
    self.results = deque()


  def process_count(self):

    return len( self.running_on )


  def record_process_startup(self, pid):

    if pid in self.running_on:
      raise SchedulingError("Existing worker with identical processID")

    self.running_on[ pid ] = None


  def record_job_start(self, jobid, pid):

    if pid not in self.running_on:
      raise SchedulingError("Unknown processID")

    if self.running_on[ pid ] is not None:
      raise SchedulingError("Attempt to start process on busy worker")

    self.running_on[ pid ] = jobid


  def record_job_finish(self, jobid, pid, value):

    if pid not in self.running_on:
      raise SchedulingError("Unknown processID")

    if self.running_on[ pid ] != jobid:
      raise SchedulingError("Inconsistent register information: jobid/pid mismatch")

    self.running_on[ pid ] = None
    self.results.append( ( jobid, value ) )


  def record_process_shutdown(self, pid):

    self.record_process_exit( pid = pid, container = self.requested_shutdowns )


  def record_process_termination(self, pid):

    self.record_process_exit( pid = pid, container = self.terminateds )


  def record_process_exit(self, pid, container):

    if pid not in self.running_on:
      raise SchedulingError("Unknown processID")

    if self.running_on[ pid ] is not None:
      raise SchedulingError("Shutdown of busy worker")

    container.append( pid )
    del self.running_on[ pid ]


  def record_process_crash(self, pid, exception, traceback):

    if pid not in self.running_on:
      raise SchedulingError("Unknown processID")

    jobid = self.running_on[ pid ]

    if jobid is not None:
      self.results.append(
        ( jobid, result.error( exception = exception, traceback = traceback ) )
        )

    del self.running_on[ pid ]
    self.terminateds.append( pid )


# Stand-alone functions for pickling
def job_started_event(register, data):

  ( jobid, pid ) = data
  register.record_job_start( jobid = jobid, pid = pid )


def job_finished_event(register, data):

  ( jobid, pid, value ) = data
  register.record_job_finish( jobid = jobid, pid = pid, value = value )


def worker_startup_event(register, data):

  register.record_process_startup( pid = data )


def worker_shutdown_event(register, data):

  register.record_process_shutdown( pid = data )


def worker_termination_event(register, data):

  register.record_process_termination( pid = data )


def worker_crash_event(register, data):

  ( pid, exception, traceback ) = data
  register.record_process_crash( pid = pid, exception = exception, traceback = traceback )


# Lifecycle controllers
class unlimited(object):
  """
  A worker that is not supposed to die
  """

  def active(self):

    return True


  def record_job_start(self):

    pass


  def record_job_end(self):

    pass


class jobcount_limit(object):
  """
  A worker that completes N jobs, and then exits
  """

  def __init__(self, maxjobs):

    self.jobs_to_go = maxjobs


  def active(self):

    return 0 < self.jobs_to_go


  def record_job_start(self):

    pass


  def record_job_end(self):

    self.jobs_to_go -= 1


class runtime_limit(object):
  """
  A worker that exits when a time limit has been reached
  """

  def __init__(self, seconds):

    self.endtime = time.time() + seconds


  def active(self):

    return time.time() < self.endtime


  def record_job_start(self):

    pass


  def record_job_done(self):

    pass


class wallclock_limit(object):
  """
  A worker that exits when a wall clock time limit has been reached
  """

  def __init__(self, seconds):
    from libtbx.development.timers import work_clock
    self.maxtime = work_clock() + seconds


  def active(self):
    from libtbx.development.timers import work_clock
    return work_clock() < self.maxtime


  def record_job_start(self):

    pass


  def record_job_done(self):

    pass


def pool_process_cycle(
  pid,
  inqueue,
  outqueue,
  waittime,
  lifecycle,
  termination_signal,
  idle_timeout,
  ):

  controller = lifecycle()
  outqueue.put( ( worker_startup_event, pid ) )
  last_activity = time.time()

  while controller.active():
    if last_activity + idle_timeout < time.time():
      outqueue.put( ( worker_termination_event, pid ) )
      break

    try:
      data = inqueue.get( timeout = waittime )

    except Empty:
      continue

    if data == termination_signal:
      outqueue.put( ( worker_shutdown_event, pid ) )
      break

    assert len( data ) == 4
    ( jobid, target, args, kwargs ) = data
    outqueue.put( ( job_started_event, ( jobid, pid ) ) )
    controller.record_job_start()

    try:
      value = target( *args, **kwargs )

    except Exception as e:
      res = result.error( exception = e, traceback = result.get_traceback_info() )

    else:
      res = result.success( value = value )

    outqueue.put( ( job_finished_event, ( jobid, pid, res ) ) )
    controller.record_job_end()
    last_activity = time.time()

  else:
    outqueue.put( ( worker_termination_event, pid ) )


# Autoscaling
class constant_capacity(object):
  """
  Keep pool size constant
  """

  def __init__(self, capacity):

    self.capacity = capacity


  def __call__(self, manager):

    if manager.worker_count() != self.capacity:
      manager.set_worker_count( target = self.capacity )


class upscaling_capacity(object):
  """
  Increase pool size when needed, rely on timeout for downscaling
  """

  def __init__(self, minimum, maximum, buffering):

    self.minimum = minimum
    self.maximum = maximum
    self.buffering = buffering


  @property
  def capacity(self):

    return self.maximum


  def __call__(self, manager):

    ideal = max(
      min( self.maximum, manager.job_count() + self.buffering ),
      self.minimum,
      )

    worker_count = manager.worker_count()

    if worker_count < ideal:
      manager.set_worker_count( target = ideal )


class breathing_capacity(object):
  """
  Varies pool size according to load
  """

  def __init__(self, minimum, maximum, buffering, downsize_step = 2, downsize_delay = 5):

    self.minimum = minimum
    self.maximum = maximum
    self.buffering = buffering
    self.downsize_step = downsize_step
    self.downsize_delay = downsize_delay

    self.downsize_wait_start = None


  @property
  def capacity(self):

    return self.maximum


  def __call__(self, manager):

    ideal = max(
      min( self.maximum, manager.job_count() + self.buffering ),
      self.minimum,
      )

    worker_count = manager.worker_count()

    if worker_count < ideal:
      manager.set_worker_count( target = ideal )
      self.downsize_wait_start = None

    elif ideal < worker_count:
      if self.downsize_wait_start is None:
        self.downsize_wait_start = time.time()

      elif self.downsize_delay < time.time() - self.downsize_wait_start:
        target = max( worker_count - self.downsize_step, ideal )
        manager.set_worker_count( target = target )
        self.downsize_wait_start = time.time()

    else:
      self.downsize_wait_start = None


class manager(object):
  """
  Process pool
  """

  TERMINATION_SIGNAL = None

  def __init__(self,
    inqueue,
    outqueue,
    job_factory,
    autoscaling,
    lifecycle,
    waittime = 0.01,
    stalltime = 2,
    idle_timeout = 120,
    ):

    self.job_factory = job_factory
    self.autoscaling = autoscaling
    self.lifecycle = lifecycle

    self.inqueue = inqueue
    self.outqueue = outqueue

    self.waittime = waittime
    self.stalltime = stalltime
    self.idle_timeout = idle_timeout

    self.register = process_register()

    from itertools import count

    self.pid_assigner = count()
    self.process_numbered_as = {}
    self.recycleds = deque()
    self.terminatings = set()
    self.unreporteds = deque()
    self.outstanding_shutdown_requests = 0

    self.running_jobs = set()
    self.completed_results = deque()

    self.manage()


  def job_count(self):

    return len( self.running_jobs )


  def worker_count(self):

    return max(
      ( len( self.process_numbered_as ) + len( self.terminatings )
        - self.outstanding_shutdown_requests ),
      0,
      )


  def is_empty(self):

    return not self.running_jobs and not self.completed_results


  def is_full(self):

    return self.autoscaling.capacity <= self.job_count()


  def has_results(self):

    return self.completed_results


  def results(self):

    while not self.is_empty():
      while not self.has_results():
        self.wait()
        self.poll()
        self.manage()

      yield self.completed_results.popleft()


  def submit(self, target, args = (), kwargs = {}):

    jobid = identifier()
    self.outqueue.put( ( jobid, target, args, kwargs ) )
    self.running_jobs.add( jobid )
    return jobid


  def shutdown(self):

    self.set_worker_count( target = 0 )


  def resume(self):

    self.manage()


  def join(self):

    while self.running_jobs:
      self.poll()
      self.wait()

    self.poll()


  def terminate(self):

    self.shutdown()
    self.wait()
    self.poll()

    for process in self.process_numbered_as.values():
      if process.is_alive():
        if hasattr( process, "terminate" ): # Thread has no terminate
          try:
            process.terminate()

          except Exception:
            pass

    while self.process_numbered_as:
      self.poll()
      self.wait()


  # Internal methods
  def start_process(self):

    try:
      pid = self.recycleds.popleft()

    except IndexError:
      pid = next(self.pid_assigner)

    process = self.job_factory(
      target = pool_process_cycle,
      kwargs = {
        "pid": pid,
        "inqueue": self.outqueue,
        "outqueue": self.inqueue,
        "waittime": self.waittime,
        "lifecycle": self.lifecycle,
        "termination_signal": self.TERMINATION_SIGNAL,
        "idle_timeout": self.idle_timeout,
        },
      )

    process.start()
    self.process_numbered_as[ pid ] = process


  def stop_process(self):

    self.outqueue.put( self.TERMINATION_SIGNAL )
    self.outstanding_shutdown_requests += 1


  def wait(self):

    time.sleep( self.waittime )


  def poll(self):

    for (pid, process) in list(self.process_numbered_as.items()):
      if not process.is_alive():
        process.join()

        exit_code = getattr( process, "exitcode", 0 ) # Thread has no "exitcode" attribute

        if exit_code != 0:
          data = (
            pid,
            result.get_exception( process = process, exit_code = exit_code ),
            result.get_crash_info( process = process ),
            )
          self.unreporteds.append( ( worker_crash_event, data ) )

        self.terminatings.add( pid )
        del self.process_numbered_as[ pid ]

    while self.unreporteds:
      try:
        self.inqueue.put( self.unreporteds[0], timeout = self.stalltime )

      except Full:
        break

      self.unreporteds.popleft()

    while True:
      try:
        ( event, data ) = self.inqueue.get( timeout = self.waittime )

      except Empty:
        break

      event( register = self.register, data = data )

    while self.register.terminateds:
      pid = self.register.terminateds[0]

      try:
        self.terminatings.remove( pid )

      except KeyError:
        break

      self.register.terminateds.popleft()
      self.recycleds.append( pid )

    while self.register.requested_shutdowns:
      pid = self.register.requested_shutdowns[0]

      try:
        self.terminatings.remove( pid )

      except KeyError:
        break

      self.register.requested_shutdowns.popleft()
      self.recycleds.append( pid )
      assert 0 < self.outstanding_shutdown_requests
      self.outstanding_shutdown_requests -= 1

    while self.register.results:
      ( jobid, res ) = self.register.results.popleft()
      self.completed_results.append( ( jobid, res ) )
      self.running_jobs.remove( jobid )


  def set_worker_count(self, target):

    while self.worker_count() < target:
      self.start_process()

    while target < self.worker_count():
      self.stop_process()


  def manage(self):

    self.autoscaling( manager = self )


class creator(object):
  """
  Information to create and destroy a manager
  """

  def __init__(
    self,
    job_factory,
    inq_factory,
    outq_factory,
    autoscaling,
    lifecycle,
    waittime = 0.01,
    stalltime = 2,
    idle_timeout = 120,
    ):

    self.job_factory = job_factory
    self.inq_factory = inq_factory
    self.outq_factory = outq_factory
    self.autoscaling = autoscaling
    self.lifecycle = lifecycle
    self.waittime = waittime
    self.stalltime = stalltime
    self.idle_timeout = idle_timeout


  def create(self):

    return manager(
      inqueue = self.inq_factory.create(),
      outqueue = self.outq_factory.create(),
      job_factory = self.job_factory,
      autoscaling = self.autoscaling,
      lifecycle = self.lifecycle,
      waittime = self.waittime,
      stalltime = self.stalltime,
      idle_timeout = self.idle_timeout,
      )


  def destroy(self, manager):

    manager.terminate()
    manager.join()
    self.inq_factory.destroy( manager.inqueue )
    self.outq_factory.destroy( manager.outqueue )


 *******************************************************************************


 *******************************************************************************
libtbx/scheduling/result.py
from __future__ import absolute_import, division, print_function

class success(object):
  """
  Valid result
  """

  def __init__(self, value):

    self.value = value


  def __call__(self):

    return self.value


  def __str__(self):

    return "success( value = %s )" % self.value

# Exception handling classes
class regular_exception(object):
  """
  Unexpected result
  """

  def __init__(self, exception):

    self.exception = exception


  def __call__(self):

    return self.exception


  def __str__(self):

    return "%s( message = '%s' )" % ( self.exception.__class__.__name__, self.exception )


class sorry_exception(object):
  """
  Special handling for Sorry
  """

  def __init__(self, exception):

    self.args = exception.args


  def __call__(self):

    return self.exception_class()( *self.args )


  def __str__(self):

    return "Sorry( message = '%s' )" % self.exception_class()( *self.args )


  @classmethod
  def exception_class(self):

    from libtbx.utils import Sorry
    return Sorry

# Extendable exception handling registry
class exception_to_result_registry(object):
  """
  Translates unpickleable exceptions
  """

  def __init__(self):

    self.handlers = []


  def add_handler(self, handler_class):

    self.handlers.append( handler_class )


  def convert(self, exception):

    for handler_class in self.handlers:
      if isinstance( exception, handler_class.exception_class() ):
        return handler_class( exception = exception )

    else:
      return regular_exception( exception = exception )


TRANSLATOR = exception_to_result_registry()


def register_unpickleable_exception(handler):

  TRANSLATOR.add_handler( handler_class = handler )

# Register Sorry
register_unpickleable_exception( handler = sorry_exception )


class failure(object):
  """
  Exception and stacktrace propagation
  """

  def __init__(self, exception, traceback):

    self.exception = exception
    self.traceback = traceback


  def __call__(self):

    self.traceback( exception = self.exception() )


  def __str__(self):

    return str( self.exception )


def get_exception(process, exit_code):

  return getattr( process, "err", RuntimeError( "exit code = %s" % exit_code ) )


def get_crash_info(process):

  from libtbx.scheduling import stacktrace
  printout = getattr( process, "stacktrace", None )

  if printout is None:
    return stacktrace.no_crash_info

  else:
    return stacktrace.stacktrace_info.from_stderr( message = printout )


def get_traceback_info():

  from libtbx.scheduling import stacktrace
  import sys
  return stacktrace.traceback_info( traceback = sys.exc_info()[2] )


def error(exception, traceback = None):

  if traceback is None:
    from libtbx.scheduling import stacktrace
    traceback = stacktrace.no_crash_info

  return failure(
    exception = TRANSLATOR.convert( exception = exception ),
    traceback = traceback,
    )


 *******************************************************************************


 *******************************************************************************
libtbx/scheduling/shared_resource.py
"""
Virtual Manager

Behaves as a normal manager instance, but performs processing through an internally
held manager, and only handles jobs submitted through it.

As this is a virtual resource, there is no creator associated with it.

Common methods:
  has_results(): returns whether there are any results available
  results(): return an iterator yielding ( identifier, result ) tuples
  submit(target, args = (), kwargs = {}): submits job, return identifier
  is_empty(): returns whether there are no more jobs or results waiting
  is_full(): returns whether the number of currently processing jobs is at maximum
  shutdown(): sets the number of workers to zero
  resume(): starts workers as necessary
  join(): finish processing all jobs, and then shutdown
  terminate(): kills all processing
"""

from __future__ import absolute_import, division, print_function

from collections import deque


class server(object):
  """
  Holds the underlying manager and sorts finished jobs according to submitter
  """

  def __init__(self, resource):

    self.resource = resource
    self.results_submitted_through = {}
    self.unfinisheds_submitted_through = {}
    self.submitter_for = {}


  def manager(self):

    identifier = object()
    assert identifier not in self.results_submitted_through
    assert identifier not in self.unfinisheds_submitted_through
    self.results_submitted_through[ identifier ] = deque()
    self.unfinisheds_submitted_through[ identifier ] = 0
    return manager( server = self, identifier = identifier )


  # Internal methods
  def submit(self, identifier, target, args = (), kwargs = {}):

    jobid = self.resource.submit( target = target, args = args, kwargs = kwargs )
    assert jobid not in self.submitter_for
    self.submitter_for[ jobid ] = identifier
    self.unfinisheds_submitted_through[ identifier ] += 1

    return jobid


  def results_for(self, identifier):

    return self.results_submitted_through[ identifier ]


  def unfinisheds_for(self, identifier):

    return self.unfinisheds_submitted_through[ identifier ]


  def update(self):

    while self.resource.has_results():
      self.fetch()


  def fetch(self):

    ( jobid, result ) = next(self.resource.results())
    identifier = self.submitter_for[ jobid ]
    del self.submitter_for[ jobid ]
    self.results_submitted_through[ identifier ].append( ( jobid, result ) )
    self.unfinisheds_submitted_through[ identifier ] -= 1


  # Calls forwarded to underlying manager
  def is_full(self):

    return self.resource.is_full()


  def shutdown(self):

    self.resource.shutdown()


  def resume(self):

    self.resource.resume()


  def join(self):

    self.resource.join()


  def terminate(self):

    self.resource.terminate()


class manager(object):
  """
  Behaves like a manager, but uses an external resource to run the jobs

  Unintuitive feature:
    manager.is_empty() == True and manager.is_full() == True is possible
    simultaneously, even if the number of cpus is not zero. This is because
    adapter.is_empty() report the status of the adapter queue, while
    adapter.is_full() reports that of the manager. This gives the behaviour
    one normally expects, i.e. no submission if there are no free cpus, and
    empty status when jobs submitted through the adaptor have all been
    processed.
  """

  def __init__(self, server, identifier):

    self.server = server
    self.identifier = identifier


  @property
  def completed_results(self):

    return self.server.results_for( identifier = self.identifier )


  def has_results(self):

    self.server.update()
    return self.completed_results


  def results(self):

    self.server.update()
    results = self.completed_results

    while not self.is_empty():
      while not results:
        self.server.fetch()

      yield results.popleft()


  def submit(self, target, args = (), kwargs = {}):

    return self.server.submit(
      identifier = self.identifier,
      target = target,
      args = args,
      kwargs = kwargs,
      )


  def is_empty(self):

    return ( self.server.unfinisheds_for( identifier = self.identifier ) == 0
      and not self.completed_results )


  def is_full(self):

    return self.server.is_full()


  def shutdown(self):

    self.server.shutdown()


  def resume(self):

    self.server.resume()


  def join(self):

    self.server.join()


  def terminate(self):

    self.server.terminate()


 *******************************************************************************


 *******************************************************************************
libtbx/scheduling/socket_queue.py
from __future__ import absolute_import, division, print_function

from six.moves import range
class MultiQueue(object):

  def __init__(self):

    self.queue_for = {}


  def create(self, name):

    from six.moves import queue
    self.queue_for[ name ] = queue.Queue()


  def remove(self, name):

    del self.queue_for[ name ]


  def put(self, name, item, block = True, timeout = None):

    self.queue_for[ name ].put( item, block = block, timeout = timeout )


  def get(self, name, block = True, timeout = None):

    return self.queue_for[ name ].get( block = block, timeout = timeout )


  def put_nowait(self, name, item):

    return self.queue_for[ name ].put_nowait( item )


  def get_nowait(self, name):

    return self.queue_for[ name ].get_nowait()


class Queue(object):

  def __init__(self, server):

    import socket
    import os
    self.identifier = "%s-%s-%s" % ( socket.getfqdn(), os.getpid(), id( self ) )

    self.server = server
    self.server.multiqueue.create( self.identifier )


  def shutdown(self):

    self.server.multiqueue.remove( self.identifier )
    self.server = None


  def put(self, value, block = True, timeout = None):

    self.server.multiqueue.put(
      self.identifier,
      value,
      block = block,
      timeout = timeout,
      )


  def get(self, block = True, timeout = None):

    return self.server.multiqueue.get(
      self.identifier,
      block = block,
      timeout = timeout,
      )


  def put_nowait(self, value):

    return self.server.multiqueue.put_nowait( self.identifier, value )


  def get_nowait(self):

    return self.server.multiqueue.get_nowait( self.identifier )


class Manager(object):

  def __init__(self, manager):

    self.manager = manager
    self.multiqueue = manager.get() # caching


  def Queue(self):

    return Queue( server = self )


  def __getstate__(self):

    result = self.__dict__.copy()
    result[ "address" ] = self.manager.address
    result[ "authkey" ] = str( self.manager._authkey )
    del result[ "manager" ]
    del result[ "multiqueue" ]
    return result


  def __setstate__(self, result):

    manager = self.get_client_manager(
      address = result[ "address" ],
      authkey = result[ "authkey" ],
      )
    result[ "manager" ] = manager
    result[ "multiqueue" ] = manager.get()
    del result[ "address" ]
    del result[ "authkey" ]
    self.__dict__ = result


  @classmethod
  def Server(cls, port = 0, keylength = 16):

    from multiprocessing.managers import BaseManager

    class QManager(BaseManager):

      pass

    multiqueue = MultiQueue()
    QManager.register( "get", lambda: multiqueue )

    import socket, string
    import random

    manager = QManager(
      address = ( socket.getfqdn(), port ),
      authkey = "".join(
        random.choice( string.ascii_letters ) for i in list(range( keylength))
        ),
      )
    manager.start()

    return cls( manager = manager )


  @classmethod
  def Client(cls, server, port, authkey):

    return cls(
      manager = cls.get_client_manager(
        address = (server, port ),
        autkey = authkey,
        )
      )


  @staticmethod
  def get_client_manager(address, authkey):

    from multiprocessing.managers import BaseManager

    class QManager(BaseManager):

      pass

    QManager.register( "get" )

    manager = QManager( address = address, authkey = authkey )
    manager.connect()

    return manager


class QFactory(object):
  """
  Creator pattern for socket queue, also include destruction
  """

  def __init__(self, port = 0, keylength = 16):

    self.manager = Manager.Server( port = port, keylength = keylength )


  def create(self):

    return self.manager.Queue()


  @staticmethod
  def destroy(queue):

    queue.shutdown()


 *******************************************************************************


 *******************************************************************************
libtbx/scheduling/stacktrace.py
from __future__ import absolute_import, division, print_function

from six import reraise as raise_

__prev_excepthook = None
__last_exception = ( None, None )


def set_last_exception(exception, printout):

  global __last_exception
  __last_exception = ( exception, printout )


def cleanup():

  set_last_exception( None, None )


def exc_info():

  return __last_exception


def is_enabled():

  return __prev_excepthook is not None


def enable():

  global __prev_excepthook

  if not is_enabled():
    import sys
    __prev_excepthook = sys.excepthook
    sys.excepthook = stacktrace_excepthook


def disable():

  global __prev_excepthook

  if is_enabled():
    import sys
    sys.excepthook = __prev_excepthook
    __prev_excepthook = None


def stacktrace_excepthook(ex_cls, ex, tb):
  """
  Prints traceback printout from module global if exists
  """

  data = exc_info()

  if ex is data[0]:
    import traceback
    import sys
    sys.stderr.write(
      "".join( data[1] + traceback.format_exception_only( ex_cls, ex ) )
      )

  else:
    __prev_excepthook( ex_cls, ex, tb )


# Information handling
def no_crash_info(exception):

  raise exception


class stacktrace_info(object):
  """
  Crash printout
  """

  def __init__(self, printout, header):

    self.header = header
    self.printout = printout


  def __call__(self, exception):

    set_last_exception( exception, [ self.header ] + self.printout )
    raise exception


  @classmethod
  def from_stderr(cls, message):

    return cls( printout = [ message ], header = "Stacktrace:\n" )


  @classmethod
  def from_traceback(cls, lines):

    return cls( printout = lines, header = "Traceback (most recent call last):\n" )


class traceback_info(object):
  """
  Transparent traceback transformation on pickling
  """

  def __init__(self, traceback):

    self.traceback = traceback
    self.raise_handler = self.raise_with_traceback
    self.getstate_handler = self.getstate_with_traceback


  def __call__(self, exception):

    self.raise_handler( exception = exception )


  def __getstate__(self):

    return self.getstate_handler()


  def __setstate__(self, state):

    self.stacktrace = state
    self.raise_handler = self.raise_with_stacktrace
    self.getstate_handler = self.getstate_with_stacktrace


  def raise_with_traceback(self, exception):

    raise_(type(exception), exception, self.traceback)


  def getstate_with_traceback(self):

    import traceback
    return stacktrace_info.from_traceback(
      lines = traceback.format_tb( self.traceback )
      )


  def raise_with_stacktrace(self, exception):

    self.stacktrace( exception = exception )


  def getstate_with_stacktrace(self):

    return self.stacktrace


 *******************************************************************************


 *******************************************************************************
libtbx/scheduling/thread_handler.py
from __future__ import absolute_import, division, print_function

import threading
from six.moves import queue


class exception_capturing_thread(threading.Thread):
  """
  A Thread-specialization that adds the exitcode attribute

  http://stackoverflow.com/questions/986616/python-thread-exit-code
  """

  def __init__(
    self,
    group=None,
    target=None,
    name=None,
    propagate_error_message = False,
    args=(),
    kwargs={},
    ):

    super( exception_capturing_thread, self ).__init__(group, target, name, args, kwargs)
    self.propagate_error_message = propagate_error_message


  def run(self):

    try:
      super( exception_capturing_thread, self ).run()

    except Exception as e:
      self.exitcode = 1
      self.err = e

      import traceback
      import sys
      self.stacktrace = "".join( traceback.format_tb( tb = sys.exc_info()[2] ) )

      if self.propagate_error_message:
        traceback.print_exc()

    else:
      self.exitcode = 0
      self.err = None
      self.stacktrace = None


class qfactory(object):
  """
  Creator pattern for queue.Queue, also include destruction

  Note this is a singleton object
  """

  @staticmethod
  def create():

    return queue.Queue()


  @staticmethod
  def destroy(q):

    pass


 *******************************************************************************


 *******************************************************************************
libtbx/server/FileClient.py
from __future__ import absolute_import, division, print_function
from six.moves import cPickle as pickle
import socket
import sys
import os

class FileClient:

  def __init__(self,
               host='',
               port=8000):
    if host=='127.0.0.1' or host=='localhost':
      host = ''
    if not host: # or host[0].isalpha():
      host = socket.getfqdn()
    self.host=host
    if self.host.find('\n')>-1:
      self.host = self.host.split('\n')[0]
    self.IP = socket.gethostbyname(self.host)
    self.port=int(port)
    self.sock=None

  def GetServerDetails(self,*arg,**kw):
    return self._remote_call('GetServerDetails',arg,kw)

  def GetHost(self,*arg,**kw):
    return self._remote_call('GetHost',arg,kw)

  def GetPort(self,*arg,**kw):
    return self._remote_call('GetPort',arg,kw)

  #def GetUser(self,*arg,**kw):
  #  return self._remote_call('GetUser',arg,kw)

##############################################################################
# File functions
##############################################################################

  def LockFile(self,*arg,**kw):
    return self._remote_call('LockFile',arg,kw)

  def ReadFile(self,*arg,**kw):
    return self._remote_call('ReadFile',arg,kw)

  def WriteFile(self,*arg,**kw):
    return self._remote_call('WriteFile',arg,kw)

  def ReadPickleFile(self,*arg,**kw):
    return self._remote_call('ReadPickleFile',arg,kw)

  def WritePickleFile(self,*arg,**kw):
    return self._remote_call('WritePickleFile',arg,kw)

  def UnlockFile(self,*arg,**kw):
    return self._remote_call('UnlockFile',arg,kw)

  def FileExists(self,*arg,**kw):
    return self._remote_call('FileExists',arg,kw)

##############################################################################
# Process commands
##############################################################################
  def getpid(self,*arg,**kw):
    return self._remote_call('getpid',arg,kw)

  def tester(self,*arg,**kw):
    return self._remote_call('tester',arg,kw)

##############################################################################
# Client functions
##############################################################################

  def _remote_call(self,
                   meth,
                   args=(),
                   kwds={}):
    result = None
    if self.sock is None:
      self.sock=socket.socket(socket.AF_INET,socket.SOCK_STREAM)
      self.sock.connect((self.host, self.port))
      self.send = self.sock.makefile('wb')
      self.recv = self.sock.makefile('rb')
    pickle.dump(meth,self.send,1) # binary by default
    pickle.dump(args,self.send,1)
    pickle.dump(kwds,self.send,1)

    # this flush somethings causes a crash WHY? maybe try/except
    self.send.flush()
    try:
      result = pickle.load(self.recv)
    except Exception:
      result = None
    #if result.__class__.__name__ == "PropagateExceptionClass":
    #  cmd = "raise %s, \"%s\"" % (result.type, result.message)
    #  exec cmd

    # Closing the socket connection increases the speed of transfer but ruins
    # the possibility of contacting the clients.
    self.send.close()
    self.recv.close()
    self.sock.close()
    self.sock=None
    return result

  def __repr__(self):
    outl = '\nClass '+self.__class__.__name__
    outl += '\n\thost '+str(self.host)
    outl += '\n\tIP   '+str(self.IP)
    outl += '\n\tport '+str(self.port)
    outl += '\n\tsock '+str(self.sock)
    try:
      outl += '\n\tsend '+str(self.send)
      outl += '\n\trecv '+str(self.recv)
    except Exception:
      pass
    return outl

  def shutdown(self):
    try:
      # multiple connections are sometimes required...
      self._remote_call('shutdown',(),{})
      self.sock=None
      self._remote_call('shutdown',(),{})
      self.sock=None
      self._remote_call('shutdown',(),{})
      self.sock=None
      self._remote_call('shutdown',(),{})
    except Exception:
      pass
    return None

#-----------------------------------------------------------------------------

def LockReadProcessWriteUnlock(client, file, id, func):
  """
  This method will lock a file, read the contents and pass it to
  the user defined function.  This function should return the
  return item and the new contents of the file.  The contents is
  writing and the file unlocked.
  """
  client.LockFile(file, id)
  try:
    lines = client.ReadFile(file)

    return_item = None
    if lines:
      return_item, lines = func(lines)
      client.WriteFile(lines, file)

  finally:
    client.UnlockFile(file, id)

  return return_item

def LockReadPickleProcessWritePickleUnlock(client, file, id, func):
  """
  Same as above except a pickled obj is used.
  """
  client.LockFile(file, id)
  try:
    obj = client.ReadPickleFile(file)

    return_item = None
    if obj:
      return_item, obj = func(obj)
      client.WritePickleFile(obj, file)

  finally:
    client.UnlockFile(file, id)

  return return_item

if __name__=="__main__":

  from . import FileServer
  import time
  import libtbx.load_env

  cmd = libtbx.env.under_dist("libtbx", "libtbx/server/StartServer.py")
  python_path = sys.executable

  if sys.platform == 'win32':
    os.spawnv(os.P_NOWAIT, python_path,
              (python_path, cmd)
              )
  else:
    os.spawnvp(os.P_NOWAIT, python_path,
              (python_path, cmd)
               )

  time.sleep(5)

  client = FileServer.GetServerClient()

  print(client)
  if client:
    print(client.tester())
    #client.shutdown()


 *******************************************************************************


 *******************************************************************************
libtbx/server/FileServer.py
from __future__ import absolute_import, division, print_function
import threading
from six.moves import cPickle as pickle
from . import SocketServer
import sys
import traceback
import os
import socket
import time

from . import FileClient
from . import FileSocket

# Socket Server

class FileServer:
  def __init__(self, port='', handler=''):
    sys.setcheckinterval(0)

    local = socket.getfqdn()
    server_address = (local, port)

    if handler:
      ddbs = _FileServer(server_address, handler)
    else:
      ddbs = _FileServer(server_address, FileRequestHandler)

    # now serve requests forever
    ddbs.keep_alive = 1
    while ddbs.keep_alive:
      ddbs.handle_request()


class _FileServer(SocketServer.ThreadingTCPServer):

  def server_bind(self):
    """Override server_bind to store the server name."""
    SocketServer.ThreadingTCPServer.server_bind(self)
    host, port = self.socket.getsockname()
    if not host or host == '0.0.0.0':
      host = socket.getfqdn()

##############################################################################
# Attributes accessable using self.server.server_name for example
##############################################################################
    self.server_name = host
    self.server_port = port
    self.main_thread = threading.enumerate()[0]
    self.ServerLock = threading.Lock()
    self.pid = os.getpid()

    self.FileLockDictionary = {}

class FileRequestHandler(SocketServer.StreamRequestHandler):

#------------------------------------------------------------------------------
# File functions
#------------------------------------------------------------------------------

  def _LockFile(self, file, id):
    DICT = self.server.FileLockDictionary
    rc = 1
    self.server.ServerLock.acquire()
    try:
      if id in DICT.keys():
        rc = 0
      if file in DICT.values():
        rc = 0
      if rc:
        DICT[id] = file
    finally:
      self.server.ServerLock.release()
    return rc

  def LockFile(self, file, id):
    rc = self._LockFile(file, id)
    while not rc:
      time.sleep(.01)
      rc = self._LockFile(file, id)

  def ReadPickleFile(self, file):
    try:
      f = open(file, 'rb')
      obj = pickle.load(f)
      f.close()
    except Exception:
      print('failed to load',file)
      obj = None
    return obj

  def WritePickleFile(self, obj, file):
    try:
      f = open(file, 'wb')
      pickle.dump(obj, f)
      f.close()
    except Exception:
      print('failed to dump',file)

  def ReadFile(self, file):
    try:
      f = open(file, 'rb')
      lines = f.read()
      f.close()
    except Exception:
      print('failed to read',file)
      lines = ''
    return lines

  def WriteFile(self, lines, file):
    try:
      f = open(file, 'wb')
      f.write(lines)
      f.close()
    except Exception:
      print('failed to write',file)

  def _UnlockFile(self, file, id):
    DICT = self.server.FileLockDictionary
    rc = 1
    self.server.ServerLock.acquire()
    try:
      if not id in DICT.keys():
        rc = 0
      if not file in DICT.values():
        rc = 0
      if rc:
        del DICT[id]
    finally:
      self.server.ServerLock.release()
    return rc

  def UnlockFile(self, file, id):
    rc = self._UnlockFile(file, id)
    while not rc:
      time.sleep(.01)
      rc = self._UnlockFile(file, id)

  def FileExists(self, file):
    return os.path.exists(file)

#------------------------------------------------------------------------------
# Server information
#------------------------------------------------------------------------------
  def GetServerDetails(self):
    #user = self.GetUser()
    server_sc = self.GetSocketConnection()
    #return user, server_sc
    return server_sc

  #def GetUser(self):
  #  return GetUser()

  def GetSocketConnection(self):
    """
    Returns the socket connection data
    """
    s = FileSocket.SocketConnection(self.server.server_name,
                                      os.getcwd(),
                                      self.server.server_port,
                                      os.getpid()
                                      )
    return s

  def GetHost(self):
    return self.server.server_name

  def GetPort(self):
    return self.server.server_port

#------------------------------------------------------------------------------
# Client tester
#------------------------------------------------------------------------------
  def _tester(self, host, port):
    client = FileClient.FileClient(host = host,
                                     port = port)
    try:
      client.tester()
      self.event.set()
    except Exception:
      pass

  def ClientTest(self, host, port, timeout=2):
    self.event = threading.Event()
    self.event.clear()

    t = threading.Thread(target=self._tester,
                         args=(host,port))
    t.setDaemon(1)
    t.start()

    self.event.wait(timeout) # timed wait may be too long

    if self.event.isSet():
      return 1
    return None

#------------------------------------------------------------------------------
# Misc
#------------------------------------------------------------------------------
  def getpid(self):
    return os.getpid()

  def shutdown(self):
    self.server.ServerLock.acquire()
    try:
      self.server.keep_alive = 0
    finally:
      self.server.ServerLock.release()

  def tester(self):
    """
    Called by client to test if server is alive
    """
    return " Hello from\n\t%s\n on %s %s" \
           % (str(self), str(self.server.server_name), sys.platform)

#------------------------------------------------------------------------------
# handler
#------------------------------------------------------------------------------
  def handle(self):
    """
    This method handles the arguments of a remote call
    """
    try:

      while self.server.keep_alive: # still alive?
        # get method name from client
        try:
          method = pickle.load(self.rfile) # use pickle to avoid blocking
        except Exception:
          #print 'failed to load method'
          #self.wfile.flush() #doesn't fixed socket problem!!!!!!!!!!
          break
        #except EOFError, socket.error: # socket closed?
        #  break

        if method == 'shutdown':
          self.server.keep_alive = 0

        # get arguments from client
        try:
          args = pickle.load(self.rfile)
        except Exception:
          args = ()
          print('Method',method)
          print('error in args ',args)
        try:
          kw = pickle.load(self.rfile)
        except Exception:
          kw = {}
          print('error in kw ',kw)

        #print method
        #print args
        #print kw

        # get method pointer
        meth_obj = getattr(self,method)
        #print meth_obj

        # call method
        try:
          result = meth_obj(*args, **kw)
          pickle.dump(result,self.wfile,1) # binary by default
        except Exception:
          try:
            pickle.dump('0\n',self.wfile,1)
          except Exception:
            pass
          print('Method',meth_obj)
          print('Args  ',args)
          print('Kw    ',kw)
          raise
        try:
          self.wfile.flush()
        except Exception:
          pass

    except Exception:
      print(self.__class__.__name__+'.handle: ')
      traceback.print_exc()
      pass

    sys.stdout.flush()

#-----------------------------------------------------------------------------

def StartServer():
  s = FileSocket.GetSocket()
  port = (s.getsockname()[1])
  t = threading.Thread(target=FileServer,
                       args=(port,))
  #t.setDaemon(1)
  return (t,port)

def BindClient(host, port):
  nobind = 1
  while nobind:
    try:
      client = FileClient.FileClient(host = host,
                                     port = port
                                     )
      client.tester()
      nobind = 0
    except Exception:
      time.sleep(.2)
      nobind += 1
      if nobind > 10:
        return None

  return client

def ReadServerFile():
  host = None
  port = None
  try:
    f = open('server.port', 'rb')
    host, port = pickle.load(f)
    f.close()
  except Exception:
    print('failed to read server port file')
  return host, port

def WriteServerFile(port):
  try:
    f = open('server.port', 'wb')
    pickle.dump((socket.getfqdn(),
                 port),
                f
                )
    f.close()
  except Exception:
    print('failed to write server port file')

def GetServerClient():
  host, port = ReadServerFile()
  client = None
  if host:
    client = BindClient(host, port)

  return client


 *******************************************************************************


 *******************************************************************************
libtbx/server/FileSocket.py
from __future__ import absolute_import, division, print_function
import socket
import random
import os, sys
from string import *

def GetSocket(HOST=""):
  unbind=1
  lower_limit=20000
  upper_limit=65535
  if not HOST:
    HOST = socket.getfqdn(socket.gethostname())
  i = 0
  while unbind:
    portnumber = int(random.random()*(upper_limit-lower_limit)+lower_limit)
    try:
      s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
      s.bind((HOST, portnumber))
      s.listen(5)
      portnumber = (s.getsockname()[1])
      unbind=0
      return s
    except Exception:
      s.close()
      del s

    if i > 5:
      HOST = 'localhost'
    if i > 10:
      print('Port assignment failed.', HOST, portnumber)
      sys.exit()

    i += 1


class SocketConnection:
  def __init__(self, host, dir='', port=0, pid=0, user=''):
    """
    Contains all the information about a socket connection and more.

    Arguments.

      host - hostname of machine #IP address of the host

      dir - Working directory on host

      port - Port on host

      pid - PID of process on host

      name - hostname determined from host

      user - process owner

    Attributes

      secret - secret key for passing sensitive data
    """
    self.SetHost(host)
    self.IP   = socket.gethostbyname(self.host)
    self.port = int(port)

    try:
      dir = self._deconPath(dir,[])
    except Exception:
      dir = ' '
    self.dir=dir

    self.pid=int(pid)
    self.user = user

  def __repr__(self):
    try:
      return '\n '+'-'*10+' '+self.host+' ('+self.IP+') '+ \
             '\n '+'-'*10+' '+os.path.join(*tuple(self.dir))+ \
             '\n '+'-'*10+' Port '+str(self.port)+' PID '+str(self.pid)
    except Exception:
      return 'socket connection '+str(self.__dict__)

  def _deconPath(self, path, plist):
    path = os.path.split(path)
    tmp_list = plist[:]
    if path[1]:
      tmp_list.append(path[1])
      tmp_list = self._deconPath(path[0], tmp_list)
    else:
      tmp_list.append(path[0])
      tmp_list.reverse()
    return tmp_list

  def GetStr(self):
    return self.__repr__()

  def GetPort(self):
    return int(self.port)

  def GetHost(self):
    return self.host

  def GetHostname(self):
    return self.host

  def GetIP(self):
    return self.IP

  def GetDir(self):
    dir = os.path.join(*self.dir)
    dir = replace(dir,'\\','/')
    return dir

  def GetPID(self):
    return self.pid

  def GetUser(self):
    return self.user

  def SetPort(self, port):
    self.port=int(port)

  def SetHost(self, host):
    for i in host:
      if not i in letters:
        self.host = socket.gethostbyaddr(host)[0]
        break
    else:
      self.host=socket.getfqdn(host)

  def SetHostOLD(self, host):
    for i in host:
      if i in letters:
        self.host = socket.gethostbyname(host)
        break
    else:
      self.host=host

  def SetDir(self, dir):
    self.dir=self._deconPath(dir, [])

  def SetPID(self, pid):
    self.pid=pid

  def SetUser(self, user):
    self.user=user


 *******************************************************************************


 *******************************************************************************
libtbx/server/SocketServer.py
"""Generic socket server classes.

This module tries to capture the various aspects of defining a server:

- address family:
        - AF_INET: IP (Internet Protocol) sockets (default)
        - AF_UNIX: Unix domain sockets
        - others, e.g. AF_DECNET are conceivable (see <socket.h>
- socket type:
        - SOCK_STREAM (reliable stream, e.g. TCP)
        - SOCK_DGRAM (datagrams, e.g. UDP)
- client address verification before further looking at the request
        (This is actually a hook for any processing that needs to look
         at the request before anything else, e.g. logging)
- how to handle multiple requests:
        - synchronous (one request is handled at a time)
        - forking (each request is handled by a new process)
        - threading (each request is handled by a new thread)

The classes in this module favor the server type that is simplest to
write: a synchronous TCP/IP server.  This is bad class design, but
save some typing.  (There's also the issue that a deep class hierarchy
slows down method lookups.)

There are four classes in an inheritance diagram that represent
synchronous servers of four types:

        +-----------+        +------------------+
        | TCPServer |------->| UnixStreamServer |
        +-----------+        +------------------+
              |
              v
        +-----------+        +--------------------+
        | UDPServer |------->| UnixDatagramServer |
        +-----------+        +--------------------+

Note that UnixDatagramServer derives from UDPServer, not from
UnixStreamServer -- the only difference between an IP and a Unix
stream server is the address family, which is simply repeated in both
unix server classes.

Forking and threading versions of each type of server can be created
using the ForkingServer and ThreadingServer mix-in classes.  For
instance, a threading UDP server class is created as follows:

        class ThreadingUDPServer(ThreadingMixIn, UDPServer): pass

The Mix-in class must come first, since it overrides a method defined
in UDPServer!

To implement a service, you must derive a class from
BaseRequestHandler and redefine its handle() method.  You can then run
various versions of the service by combining one of the server classes
with your request handler class.

The request handler class must be different for datagram or stream
services.  This can be hidden by using the mix-in request handler
classes StreamRequestHandler or DatagramRequestHandler.

Of course, you still have to use your head!

For instance, it makes no sense to use a forking server if the service
contains state in memory that can be modified by requests (since the
modifications in the child process would never reach the initial state
kept in the parent process and passed to each child).  In this case,
you can use a threading server, but you will probably have to use
locks to avoid two requests that come in nearly simultaneous to apply
conflicting changes to the server state.

On the other hand, if you are building e.g. an HTTP server, where all
data is stored externally (e.g. in the file system), a synchronous
class will essentially render the service "deaf" while one request is
being handled -- which may be for a very long time if a client is slow
to reqd all the data it has requested.  Here a threading or forking
server is appropriate.

In some cases, it may be appropriate to process part of a request
synchronously, but to finish processing in a forked child depending on
the request data.  This can be implemented by using a synchronous
server and doing an explicit fork in the request handler class's
handle() method.

Another approach to handling multiple simultaneous requests in an
environment that supports neither threads nor fork (or where these are
too expensive or inappropriate for the service) is to maintain an
explicit table of partially finished requests and to use select() to
decide which request to work on next (or whether to handle a new
incoming request).  This is particularly important for stream services
where each client can potentially be connected for a long time (if
threads or subprocesses can't be used).

Future work:
- Standard classes for Sun RPC (which uses either UDP or TCP)
- Standard mix-in classes to implement various authentication
  and encryption schemes
- Standard framework for select-based multiplexing

XXX Open problems:
- What to do with out-of-band data?

"""
from __future__ import absolute_import, division, print_function


__version__ = "0.2"


import socket
import sys
import os


class TCPServer:

    """Base class for various socket-based server classes.

    Defaults to synchronous IP stream (i.e., TCP).

    Methods for the caller:

    - __init__(server_address, RequestHandlerClass)
    - serve_forever()
    - handle_request()  # if you don't use serve_forever()
    - fileno() -> int   # for select()

    Methods that may be overridden:

    - server_bind()
    - server_activate()
    - get_request() -> request, client_address
    - verify_request(request, client_address)
    - process_request(request, client_address)
    - handle_error()

    Methods for derived classes:

    - finish_request(request, client_address)

    Class variables that may be overridden by derived classes or
    instances:

    - address_family
    - socket_type
    - request_queue_size (only for stream sockets)

    Instance variables:

    - server_address
    - RequestHandlerClass
    - socket

    """

    address_family = socket.AF_INET

    socket_type = socket.SOCK_STREAM

    request_queue_size = 5

    def __init__(self, server_address, RequestHandlerClass):
        """Constructor.  May be extended, do not override."""
        self.server_address = server_address
        self.RequestHandlerClass = RequestHandlerClass
        self.socket = socket.socket(self.address_family,
                                    self.socket_type)
        self.server_bind()
        self.server_activate()

    def server_bind(self):
        """Called by constructor to bind the socket.

        May be overridden.

        """
        #print self.server_address
        self.socket.bind(self.server_address)

    def server_activate(self):
        """Called by constructor to activate the server.

        May be overridden.

        """
        self.socket.listen(self.request_queue_size)

    def fileno(self):
        """Return socket file number.

        Interface required by select().

        """
        return self.socket.fileno()

    def serve_forever(self):
        """Handle one request at a time until doomsday."""
        while True:
            self.handle_request()

    # The distinction between handling, getting, processing and
    # finishing a request is fairly arbitrary.  Remember:
    #
    # - handle_request() is the top-level call.  It calls
    #   get_request(), verify_request() and process_request()
    # - get_request() is different for stream or datagram sockets
    # - process_request() is the place that may fork a new process
    #   or create a new thread to finish the request
    # - finish_request() instantiates the request handler class;
    #   this constructor will handle the request all by itself

    def handle_request(self):
        """Handle one request, possibly blocking."""
        request, client_address = self.get_request()
        if self.verify_request(request, client_address):
            try:
                self.process_request(request, client_address)
            except Exception:
                self.handle_error(request, client_address)

    def get_request(self):
        """Get the request and client address from the socket.

        May be overridden.

        """
        return self.socket.accept()

    def verify_request(self, request, client_address):
        """Verify the request.  May be overridden.

        Return true if we should proceed with this request.

        """
        return 1

    def process_request(self, request, client_address):
        """Call finish_request.

        Overridden by ForkingMixIn and ThreadingMixIn.

        """
        self.finish_request(request, client_address)

    def finish_request(self, request, client_address):
        """Finish one request by instantiating RequestHandlerClass."""
        self.RequestHandlerClass(request, client_address, self)

    def handle_error(self, request, client_address):
        """Handle an error gracefully.  May be overridden.

        The default is to print a traceback and continue.

        """
        print('-'*40)
        print('Exception happened during processing of request from', end=' ')
        print(client_address)
        import traceback
        traceback.print_exc()
        print('-'*40)


class UDPServer(TCPServer):

    """UDP server class."""

    socket_type = socket.SOCK_DGRAM

    max_packet_size = 8192

    def get_request(self):
        data, client_addr = self.socket.recvfrom(self.max_packet_size)
        return (data, self.socket), client_addr

    def server_activate(self):
        # No need to call listen() for UDP.
        pass


class ForkingMixIn:

    """Mix-in class to handle each request in a new process."""

    active_children = None

    def collect_children(self):
        """Internal routine to wait for died children."""
        while self.active_children:
            pid, status = os.waitpid(0, os.WNOHANG)
            if not pid: break
            self.active_children.remove(pid)

    def process_request(self, request, client_address):
        """Fork a new subprocess to process the request."""
        self.collect_children()
        pid = os.fork()
        if pid:
            # Parent process
            if self.active_children is None:
                self.active_children = []
            self.active_children.append(pid)
            return
        else:
            # Child process.
            # This must never return, hence os._exit()!
            try:
                self.finish_request(request, client_address)
                os._exit(0)
            except Exception:
                try:
                    self.handle_error(request,
                                      client_address)
                finally:
                    os._exit(1)


class ThreadingMixIn:

    """Mix-in class to handle each request in a new thread."""

    def process_request(self, request, client_address):
        """Start a new thread to process the request."""
        import thread, threading

        t=threading.Thread(target=self.finish_request,
                                args=(request, client_address))
        t.setDaemon(1)
        t.start()

class ForkingUDPServer(ForkingMixIn, UDPServer): pass
class ForkingTCPServer(ForkingMixIn, TCPServer): pass

class ThreadingUDPServer(ThreadingMixIn, UDPServer): pass
class ThreadingTCPServer(ThreadingMixIn, TCPServer): pass

if hasattr(socket, 'AF_UNIX'):

    class UnixStreamServer(TCPServer):
        address_family = socket.AF_UNIX

    class UnixDatagramServer(UDPServer):
        address_family = socket.AF_UNIX

    class ThreadingUnixStreamServer(ThreadingMixIn, UnixStreamServer): pass

    class ThreadingUnixDatagramServer(ThreadingMixIn, UnixDatagramServer): pass

class BaseRequestHandler:

    """Base class for request handler classes.

    This class is instantiated for each request to be handled.  The
    constructor sets the instance variables request, client_address
    and server, and then calls the handle() method.  To implement a
    specific service, all you need to do is to derive a class which
    defines a handle() method.

    The handle() method can find the request as self.request, the
    client address as self.client_address, and the server (in case it
    needs access to per-server information) as self.server.  Since a
    separate instance is created for each request, the handle() method
    can define arbitrary other instance variariables.

    """

    def __init__(self, request, client_address, server):
        self.request = request
        self.client_address = client_address
        self.server = server
        try:
            self.setup()
            self.handle()
            self.finish()
        finally:
            sys.exc_info()[2] = None    # Help garbage collection

    def setup(self):
        pass

    def __del__(self):
        pass

    def handle(self):
        pass

    def finish(self):
        pass


# The following two classes make it possible to use the same service
# class for stream or datagram servers.
# Each class sets up these instance variables:
# - rfile: a file object from which receives the request is read
# - wfile: a file object to which the reply is written
# When the handle() method returns, wfile is flushed properly


class StreamRequestHandler(BaseRequestHandler):

    """Define self.rfile and self.wfile for stream sockets."""

    def setup(self):
        self.connection = self.request
        self.rfile = self.connection.makefile('rb', 0)
        self.wfile = self.connection.makefile('wb', 0)

    def finish(self):
        self.wfile.flush()
        self.wfile.close()
        self.rfile.close()


class DatagramRequestHandler(BaseRequestHandler):

    """Define self.rfile and self.wfile for datagram sockets."""

    def setup(self):
        from six.moves import cStringIO as StringIO
        self.packet, self.socket = self.request
        self.rfile = StringIO(self.packet)
        self.wfile = StringIO(self.packet)

    def finish(self):
        self.socket.sendto(self.wfile.getvalue(), self.client_address)


 *******************************************************************************


 *******************************************************************************
libtbx/server/StartServer.py
from __future__ import absolute_import, division, print_function
from .FileServer import StartServer, GetServerClient, WriteServerFile

def run():
  client = GetServerClient()
  if not client:
    print('Failed to find running server')
    thread, port = StartServer()
    WriteServerFile(port)
    thread.start()

if (__name__ == "__main__"):
  run()


 *******************************************************************************


 *******************************************************************************
libtbx/server/__init__.py


 *******************************************************************************


 *******************************************************************************
libtbx/simplexml.py
from __future__ import absolute_import, division, print_function
class SimpleNode:
  def __init__(self,tag,contents='',indent=True):
    self.tag=tag
    self.m_attributes=[]
    self.children=[]
    self.content=contents
    self.indent_f=indent
  def attribute(self,key,value):
    self.m_attributes.append((key,value))
    return self
  def child(self,node):
    self.children.append(node)
    return node
  def contents(self,c):
    self.content=c
    return self
  def emit(self,channel,indent=0):
    if self.indent_f==False: indent=0
    attrs = [' %s="%s"'%(item[0],item[1]) for item in self.m_attributes]
    all_attrs = "".join(attrs)

    if self.content!='' and len(self.content)<80:
      print("%s<%s%s>%s</%s>"%(' '*indent,self.tag,all_attrs,self.content,self.tag), file=channel)
      return
    print("%s<%s%s>"%(' '*indent,self.tag,all_attrs), end=' ', file=channel)
    if self.content!='' and len(self.content)>=80:
      print(file=channel)
      print(self.content, file=channel)
    else: print(file=channel)
    for item in self.children:
      item.emit(channel,indent=indent+2)
    if len(self.children)>0:
      print("%s</%s>"%(' '*indent,self.tag), file=channel)
    else:
      channel.seek(channel.tell()-3)
      print("/>", file=channel)


 *******************************************************************************


 *******************************************************************************
libtbx/smart_open.py
from __future__ import absolute_import, division, print_function

import os

from libtbx.utils import escape_sh_double_quoted, gzip_open, bz2_open
from libtbx import easy_run
from libtbx.str_utils import show_string
from six.moves import cStringIO as StringIO

def for_reading(file_name, mode="r", gzip_mode="rb"):
  assert mode in ["r", "rb"]
  assert gzip_mode in ["r", "rb", "rt"]
  file_name = os.path.expanduser(file_name)
  if file_name.endswith(".gz"):
    return gzip_open(file_name=file_name, mode=gzip_mode)
  if file_name.endswith(".Z"):
    return StringIO(easy_run.fully_buffered(
      command='gunzip -c "%s"' % escape_sh_double_quoted(file_name),
      stdout_splitlines=False).raise_if_errors().stdout_buffer)
  if file_name.endswith('.bz2'):
    return bz2_open(file_name=file_name, mode=mode)
  try:
    return open(file_name, mode)
  except IOError as e:
    raise IOError(
      "Cannot open file for reading: %s\n" % show_string(file_name)
      + "  "+str(e))

def for_writing(file_name, mode="w", gzip_mode="wb"):
  assert mode in ["w", "wb", "a", "ab"]
  assert gzip_mode in ["w", "wb", "wt", "a", "ab"]
  file_name = os.path.expanduser(file_name)
  if file_name.endswith(".gz"):
    return gzip_open(file_name=file_name, mode=gzip_mode)
  elif file_name.endswith(".bz2"):
    return bz2_open(file_name=file_name, mode=mode)
  try:
    return open(file_name, mode)
  except IOError as e:
    raise IOError(
      "Cannot open file for writing: %s\n" % show_string(file_name)
      + "  "+str(e))

def file(file_name, mode):
  assert mode in ["r", "rb", "w", "wb", "a", "ab"]
  file_name = os.path.expanduser(file_name)
  if mode[0] in ("r", "rb"):
    return for_reading(file_name=file_name, mode=mode)
  return for_writing(file_name=file_name, mode=mode)

def exercise():
  import sys
  for file_name in sys.argv[1:]:
    gzip_mode = "rb"
    if file_name.endswith('.gz'):
      gzip_mode = "rt"
    assert for_reading(file_name=file_name, gzip_mode=gzip_mode).read().splitlines() \
        == ["line 1", "line 2", "the end"]
  print("line 1", file=for_writing(file_name="tmp_plain"))
  print("line 1", file=for_writing(file_name="tmp.gz", gzip_mode="wt"))
  print("OK")

if __name__ == "__main__":
  exercise()


 *******************************************************************************


 *******************************************************************************
libtbx/sphinx/__init__.py


 *******************************************************************************


 *******************************************************************************
libtbx/sphinx/phil.py
from __future__ import absolute_import, division, print_function

import docutils.nodes
import iotbx.phil
import libtbx.phil
import libtbx.utils
from docutils.parsers.rst import Directive
from docutils.parsers.rst import directives
from six import string_types
from six.moves import cStringIO as StringIO
from types import ModuleType


def setup(app):
  app.add_directive('phil', PhilDirective)
  return {"parallel_read_safe": True}


class PhilDirective(Directive):
  # this disables content in the directive
  has_content = False
  required_arguments = 1
  option_spec = {'expert-level': directives.nonnegative_int,
                 'attributes-level': directives.nonnegative_int}

  def run(self):
    phil_include = self.arguments[0]
    expert_level = self.options.get('expert-level', None)
    attributes_level = self.options.get('attributes-level', 0)

    self.master_params = self._find_phil_scope(phil_include)
    s = StringIO()
    self.master_params.show(
      s, expert_level=expert_level, attributes_level=attributes_level)

    text = s.getvalue()
    node = docutils.nodes.literal_block(text, text)
    self.add_name(node)
    return [node]

  def _find_phil_scope(self, phil_include):
    # common phil variable names
    search = [
      "", "master_phil_scope", "master_params", "master_phil",
      "master_params_str", "master_phil_str", "get_master_phil"]

    master_params = None
    for i in search:
      try:
        if i == "":
          import_path = phil_include
        else:
          import_path = "%s.%s" %(phil_include, i)
        master_params = libtbx.utils.import_python_object(
          import_path=import_path,
          error_prefix="",
          target_must_be="",
          where_str="").object
        # check that we haven't just imported a module
        if isinstance(master_params, ModuleType):
          continue
        break
      except Exception as e:
        print(import_path)
        print(e)

    # Check if the module attribute is a string, a scope, ...
    if isinstance(master_params, libtbx.phil.scope):
      pass
    elif isinstance(master_params, string_types):
      master_params = iotbx.phil.parse(master_params, process_includes=True)
    elif hasattr(master_params, '__call__'):
      master_params = master_params()

    if not master_params:
      raise Exception("No PHIL command found for %s." % phil_include)
    return master_params


 *******************************************************************************


 *******************************************************************************
libtbx/sphinx/pubmed.py
from __future__ import absolute_import, division, print_function

import docutils.parsers.rst
import io
import multiprocessing
import os
import six
from Bio import Entrez

_biolock = multiprocessing.Lock()

def setup(app):
  app.add_directive('pubmed', PubMedDirective)
  return {"parallel_read_safe": True}

class PubMedDirective(docutils.parsers.rst.Directive):
  # this disables content in the directive
  has_content = False
  required_arguments = 1
  optional_arguments = 1
  final_argument_whitespace = True
  option_spec = {'reprint-url': docutils.parsers.rst.directives.unicode_code}

  def run(self):
    PMID = self.arguments[0]
    reprint_url = self.options.get('reprint-url', None)
    Entrez.email = 'cctbxbb@phenix-online.org'
    with _biolock:
      raw_cache = None
      if os.getenv("BIOCACHE"):
        cache_file = os.path.join(os.getenv("BIOCACHE"), str(PMID))
        if os.path.exists(cache_file):
          with open(cache_file, "rb") as fh:
            raw_cache = fh.read()
      if not raw_cache:
        handle = Entrez.efetch(db="pubmed", id=PMID, retmode="xml")
        raw_cache = handle.read()
        if six.PY3 and not isinstance(raw_cache, bytes):
          raw_cache = raw_cache.encode('utf-8')
      handle = io.BytesIO(raw_cache)
      XML = Entrez.read(handle)['PubmedArticle']
      if os.getenv("BIOCACHE") and not os.path.exists(cache_file):
        with open(cache_file, "wb") as fh:
          fh.write(raw_cache)

    def raw_html_link_new_tab(identifier, link_text, link):
      return '.. |%s| raw:: html\n\n' %identifier + \
        '   <a class="reference external" href="%s" target="_blank">%s</a>' %(
          link, link_text)

    raw_directives = []
    text = []

    for tag in XML:
      # Title/doi link:
      possible_doi = [ idx for idx in tag["PubmedData"]["ArticleIdList"]
                       if idx.attributes["IdType"]=="doi" ]
      article = tag["MedlineCitation"]["Article"]
      # Remove trailing dot and all control characters, including newline chars, from title.
      get_title = ''.join(c for c in article['ArticleTitle'].rstrip('.') if ord(c) >= 32)

      doi_link_text = None
      if len(possible_doi) > 0:
        text.append('| |%s|' %possible_doi[0])
        raw_directives.append(raw_html_link_new_tab(
          possible_doi[0], get_title, "https://doi.org/%s" %possible_doi[0]))

      # Author list
      authors = [ " ".join([elem["LastName"],elem["Initials"]])
                  for elem in article["AuthorList"] ]
      text.append("| %s." %(", ".join(authors)))

      # Journal reference
      journal = article["Journal"]
      journal_text = "| *%s*" %(journal["ISOAbbreviation"])
      issue = journal["JournalIssue"]
      if 'Volume' in issue:
        journal_text += " **%s**" % issue['Volume']
        if 'Pagination' in article:
          journal_text += ", %s" % article["Pagination"]["MedlinePgn"]
      date = issue["PubDate"]
      if 'Month' in date:
        journal_text += " (%s %s %s)."%(date.get("Day",1), date["Month"], date["Year"])
      else:
        journal_text += " (%s)"%(date["Year"])
      journal_text += " [PMID:%s]"%PMID
      possible_pmc = [ idx for idx in tag["PubmedData"]["ArticleIdList"]
                       if idx.attributes["IdType"]=="pmc" ]
      if len(possible_pmc) > 0:
        journal_text += " [PMC reprint: |%s|]" %possible_pmc[0]
        raw_directives.append(raw_html_link_new_tab(
          possible_pmc[0], "%s" %possible_pmc[0],
          "http://ncbi.nlm.nih.gov/pmc/articles/%s/" %possible_pmc[0]))

      if reprint_url is not None:
        journal_text += " |%s_reprint|" %PMID
        raw_directives.append(raw_html_link_new_tab(
          "%s_reprint" %PMID, "(Reprint)", reprint_url))
      text.append(journal_text)
      for directive in raw_directives:
        text.append("\n%s\n" %directive)

#   try:
#     print("vvv")
#     print("\n".join(text))
#     print("^^^")
#   except Exception:
#     pass

    # insert rst
    source = self.state_machine.input_lines.source(
      self.lineno - self.state_machine.input_offset - 1)
    lines = docutils.statemachine.string2lines(
      "\n".join(text), self.state.document.settings.tab_width, convert_whitespace=True)
    self.state_machine.insert_input(lines, source)
    return []


 *******************************************************************************


 *******************************************************************************
libtbx/sphinx/python_string.py
from __future__ import absolute_import, division, print_function

import docutils.statemachine
import libtbx.utils
from docutils.parsers.rst import Directive
from six import string_types

def setup(app):
  app.add_directive('python_string', PythonStringDirective)
  return {"parallel_read_safe": True}

class PythonStringDirective(Directive):
  # this disables content in the directive
  has_content = False
  required_arguments = 1

  def run(self):
    python_path = self.arguments[0]
    python_string = libtbx.utils.import_python_object(
      import_path=python_path,
      error_prefix="",
      target_must_be="",
      where_str="").object

    assert isinstance(python_string, string_types)

    include_lines = docutils.statemachine.string2lines(
        python_string, tab_width=2, convert_whitespace=True)
    self.state_machine.insert_input(include_lines, python_path)
    return []


 *******************************************************************************


 *******************************************************************************
libtbx/start_print_trace.py
from __future__ import absolute_import, division, print_function
from libtbx import introspection
introspection.start_print_trace()


 *******************************************************************************


 *******************************************************************************
libtbx/str_utils.py
from __future__ import absolute_import, division, print_function

from functools import cmp_to_key
from six import string_types
from six.moves import cStringIO

import sys
from six.moves import zip

def format_none(format, null_value=0, replace_with="None"):
  assert isinstance(replace_with, str)
  value_width = len(replace_with)
  return " " * max(0, len(format % null_value) - value_width) + replace_with

def format_value(format, value, null_value=0, replace_none_with="None"):
  if (format is None): return str(value)
  if (value is None or str(value).upper()=="NONE"):
    return format_none(format=format, null_value=null_value,
      replace_with=replace_none_with)
  return format % value

def round_2_for_cif(value):
  """
  shortcut for cif output
  """
  if value is None or str(value).upper()=="NONE":
    return '?'
  return format_value(format="%.2f", value=value, null_value=0, replace_none_with="?")

def round_4_for_cif(value):
  """
  shortcut for cif output
  """
  if value is None or str(value).upper()=="NONE":
    return '?'
  return format_value(format="%.4f", value=value, null_value=0, replace_none_with="?")

def pad_string(line, width=72, border="|"):
  n_spaces = width - (len(border) * 2) - len(line)
  return border + line + (" " * n_spaces) + border

def py_string_representation(string, preferred_quote, alternative_quote):
  "based on stringobject.c, Python 2.7.2"
  quote = preferred_quote
  if (    alternative_quote != preferred_quote
      and string.find(preferred_quote) >= 0
      and string.find(alternative_quote) < 0):
    quote = alternative_quote
  result = [quote]
  rapp = result.append
  for c in string:
    if (c == quote or c == '\\'):
      rapp('\\')
      rapp(c)
    elif (c == '\t'):
      rapp('\\t')
    elif (c == '\n'):
      rapp('\\n')
    elif (c == '\r'):
      rapp('\\r')
    elif (c < ' ' or c >= chr(0x7f)):
      rapp("\\x%02x" % (ord(c) & 0xff))
    else:
      rapp(c)
  rapp(quote)
  return "".join(result)

_have_string_representation = False
def string_representation(string, preferred_quote, alternative_quote):
  global string_representation
  global _have_string_representation
  if (not _have_string_representation):
    _have_string_representation = True
    try:
      from boost_adaptbx.boost.python import ext as _
      string_representation = _.string_representation
    except Exception:
      string_representation = py_string_representation
  return string_representation(string, preferred_quote, alternative_quote)

def split_keeping_spaces(s):
  result = []
  field = []
  prev = " "
  for c in s:
    if ((prev == " ") != (c == " ")):
      if (len(field) != 0):
        result.append("".join(field))
        field = []
    field.append(c)
    prev = c
  if (len(field) != 0):
    result.append("".join(field))
  return result

def size_as_string_with_commas(sz):
  if (sz is None): return "unknown"
  if (sz < 0):
    sz = -sz
    sign = "-"
  else:
    sign = ""
  result = []
  while True:
    if (sz >= 1000):
      result.insert(0, "%03d" % (sz % 1000))
      sz //= 1000
      if (sz == 0): break
    else:
      result.insert(0, "%d" % sz)
      break
  return sign + ",".join(result)

def show_string(s):
  if (s is None): return None
  if (s.find('"') < 0): return '"'+s+'"'
  if (s.find("'") < 0): return "'"+s+"'"
  return '"'+s.replace('"','\\"')+'"'

def prefix_each_line_suffix(prefix, lines_as_one_string, suffix, rstrip=True):
  def do_rstip(s): return s.rstrip()
  def do_nothing(s): return s
  if (rstrip): do = do_rstip
  else:        do = do_nothing
  return "\n".join([do(prefix+line+suffix)
    for line in lines_as_one_string.splitlines()])

def prefix_each_line(prefix, lines_as_one_string, rstrip=True):
  return prefix_each_line_suffix(
    prefix=prefix,
    lines_as_one_string=lines_as_one_string,
    suffix="",
    rstrip=rstrip)

def show_text(line, prefix="", out=None):
  if(out is None): out = sys.stdout
  print(file=out)
  print("%s%s"%(prefix, line), file=out)
  print(file=out)

def make_header(line, out=None, header_len=80, repeat=1, new_line_start=False,
                new_line_end=False):
  if (out is None): out = sys.stdout
  #if(new_line_start):
  #  print("\n",file=out)
  def one():
    line_len = len(line)
    #assert line_len <= header_len
    fill_len = header_len - line_len
    fill_rl = fill_len//2
    fill_r = fill_rl
    fill_l = fill_rl
    if (fill_rl*2 != fill_len):
      fill_r +=1
    out_string = "\n"+"="*(fill_l-1)+" "+line+" "+"="*(fill_r-1)+"\n"
    if(len(out_string) > 80):
      out_string = "\n"+"="*(fill_l-1)+" "+line+" "+"="*(fill_r-2)+"\n"
    print(out_string, file=out)
  for it in range(0,repeat):
    one()
  #if(new_line_end):
  #  print("\n",file=out)
  out.flush()

def make_sub_header(text, out=None, header_len=80, sep='-'):
  """
  Prints a subheader to an output location.

  Parameters
  ----------
  text : str
  out : file, optional
  header_len : int, optional
  sep : str, optional

  Examples
  --------
  >>> from libtbx.str_utils import make_sub_header
  >>> make_sub_header("Predicted ions", header_len=20)

     ----------Predicted ions----------

  >>>
  """
  if (out is None): out = sys.stdout
  assert isinstance(sep, string_types)
  border = sep*10
  line = border+text+border
  line_len = len(line)
  #assert line_len <= header_len
  fill_len = header_len - line_len
  fill_rl = fill_len//2
  fill_r = fill_rl
  fill_l = fill_rl
  if (fill_rl*2 != fill_len):
    fill_r +=1
  out_string = "\n"+" "*(fill_l-1)+" "+line+" "+" "*(fill_r-1)+"\n"
  if(len(out_string) > 80):
    out_string = "\n"+" "*(fill_l-1)+" "+line+" "+" "*(fill_r-2)+"\n"
  print(out_string, file=out)
  out.flush()

def wordwrap(text, max_chars=80):
  words = text.split()
  lines = []
  current_line = ""
  for word in words :
    if ((len(current_line) + len(word)) >= max_chars):
      lines.append(current_line)
      current_line = word
    else :
      current_line += " " + word
  lines.append(current_line)
  return "\n".join(lines)

def reformat_terminal_text(text):
  text.strip()
  lines = text.splitlines()
  newtext = ""
  lastline = ""
  for i, line in enumerate(lines):
    if line == "" and i != 0 :
      newtext += "\n"
    else :
      if lastline != "" :
        newtext += " "
      newtext += line.strip()
    lastline = line
  return newtext

def rstrip_lines(text):
  new = []
  for line in text.splitlines():
    new.append(line.rstrip())
  return "\n".join(new)

def strip_lines(text):
  new = []
  for line in text.splitlines():
    new.append(line.strip())
  return "\n".join(new)

def show_sorted_by_counts(
      label_count_pairs,
      reverse=True,
      out=None,
      prefix="",
      annotations=None):
  assert annotations is None or len(annotations) == len(label_count_pairs)
  if (out is None): out = sys.stdout
  if (len(label_count_pairs) == 0): return False
  def sort_function(a, b):
    if (reverse):
      if (a[1] > b[1]): return -1
      if (a[1] < b[1]): return  1
    else:
      if (a[1] < b[1]): return -1
      if (a[1] > b[1]): return  1
    return (a[0] > b[0]) - (a[0] < b[0]) # cmp(a[0], b[0])
  if (annotations is None):
    annotations = [None]*len(label_count_pairs)
  lca = [(show_string(l), c, a)
    for (l,c),a in zip(label_count_pairs, annotations)]
  lca.sort(key=cmp_to_key(sort_function))
  fmt = "%%-%ds %%%dd" % (
    max([len(l) for l,c,a in lca]),
    max([len(str(lca[i][1])) for i in [0,-1]]))
  for l,c,a in lca:
    if (a is not None and len(a) > 0):
      print(prefix + fmt % (l,c), end=' ', file=out)
      print(a, end='', file=out)
    else:
      print(prefix + fmt % (l,c), end='', file=out)
    print(file=out)
  return True

def overwrite_at(s, offset, replacement):
  return s[:offset] + replacement + s[offset+len(replacement):]

def contains_one_of(label, patterns):
  for pattern in patterns:
    if (label.find(pattern) >= 0):
      return True
  return False

def line_breaker(string, width):
  if (width <= 0 or len(string) == 0):
    yield string
  else:
    i_block_start = 0
    i_last_space = None
    for i,c in enumerate(string):
      if (c == " "):
        i_last_space = i
      if (i-i_block_start >= width and i_last_space is not None):
        yield string[i_block_start:i_last_space]
        i_block_start = i_last_space + 1
        i_last_space = None
    if (i_block_start < len(string)):
      yield string[i_block_start:]

class line_feeder(object):

  def __init__(self, f):
    self.f = iter(f)
    self.eof = False

  def __iter__(self):
    return self

  def next(self):
    if (not self.eof):
      try:
        return self.f.next()[:-1]
      except StopIteration:
        self.eof = True
    return ""

  def next_non_empty(self):
    while True:
      result = next(self)
      if (self.eof or len(result.strip()) != 0):
        return result

# StringIO with pickling support and extra read-only semantics
class StringIO(object):
  def __init__(self, *args, **kwds):
    self._buffer = cStringIO(*args, **kwds)
    self.read_only = bool(args)

  def __getattr__(self, *args, **kwds):
    if self.read_only and args[0] in ('softspace', 'truncate', 'write', 'writelines'):
      raise AttributeError("'libtbx.str_utils.StringIO' has no attribute '%s'" % args[0])
    return getattr(self._buffer, *args, **kwds)

  def __getstate__(self):
    is_output_object = (type(self._buffer).__name__ == 'StringO')
    return (self._buffer.getvalue(), is_output_object)

  def __setstate__(self, state):
    (buffer_value, is_output_object) = state
    if is_output_object :
      self.__init__()
      self._buffer.write(buffer_value)
      self.read_only = True
    else :
      self.__init__(buffer_value)

def expandtabs_track_columns(s, tabsize=8):
  result_e = []
  result_j = []
  j = 0
  for i,c in enumerate(s):
    if (c == "\t"):
      if (tabsize > 0):
        n = tabsize - (j % tabsize)
        result_e.extend([" "]*n)
        result_j.append(j)
        j += n
    else:
      result_e.append(c)
      result_j.append(j)
      if (c == "\n" or c == "\r"):
        j = 0
      else:
        j += 1
  return "".join(result_e), result_j

class framed_output(object):
  """
  Pseudo-output file wrapper for drawing a box around arbitrary content, for
  example:

    |----------Refinement stats--------|
    | r_free = 0.1234  r_work = 0.1567 |
    |----------------------------------|

  The actual content will be buffered until the close() method is called or
  the object is deleted, at which point the formatted text will be printed to
  the actual filehandle.

  :param out: actual output filehandle
  """
  def __init__(self,
      out,
      title=None,
      width=None,
      frame='-',
      center=False,
      center_title=None,
      #center_frame=None,
      max_width=80,
      prefix=None):
    self.out = out
    if (title is not None):
      title = title.strip()
    self.title = title
    if (center_title is None):
      center_title = center
    self.center_title = center_title
    self.width = width
    self.frame = frame
    self.center = center
    #self.center_frame = center_frame
    self.max_width = max_width
    self.prefix = prefix
    self.content = []
    self._current_line = None
    self._closed = False

  def get_best_text_width(self):
    return self.width - 4

  def write(self, text):
    current_text = ""
    if (self._current_line is not None):
      current_text += self._current_line
      self._current_line = None
    for char in text :
      if (char == "\n"):
        self.content.append(current_text)
        current_text = ""
      else :
        current_text += char
    if (current_text != ""):
      self._current_line = current_text

  def flush(self):
    pass

  def add_separator(self):
    if (self._current_line is not None):
      self.content.append(self._current_line)
      self._current_line = None
    self.content.append(None)

  def close(self):
    assert (not self._closed)
    self._closed = True
    if (not self._current_line in [None, ""]):
      self.content.append(self._current_line)
    out = self.out
    lines = self.content
    # misc setup
    text_lines = [ s for s in lines if (s is not None) ]
    side_frame = self.frame
    if (side_frame in ['-', '_']):
      side_frame = "|"
    width = self.width
    if (width is None):
      width = min(self.max_width, 4 + max([ len(s) for s in text_lines ]))
    def get_padding(text, margin=2, center=self.center):
      from libtbx.math_utils import ifloor, iceil
      fill = max(0, width - len(text) - (margin * 2))
      if (center):
        rfill = ifloor(fill / 2)
        lfill = iceil(fill / 2)
      else :
        rfill = 0
        lfill = fill
      return (rfill, lfill)
    def write_corner():
      if (self.frame != '_'):
        out.write(side_frame)
      else :
        out.write("_")
    def write_sep():
      if (self.prefix is not None):
        out.write(self.prefix)
      out.write(side_frame)
      out.write(self.frame * (width - 2))
      out.write(side_frame + "\n")
    # header
    out.write("\n")
    if (self.prefix is not None):
      out.write(self.prefix)
    write_corner()
    if (self.title is not None):
      rf, lf = get_padding(self.title, margin=1, center=self.center_title)
      rf += 1
      lf -= 1
      out.write(self.frame * rf)
      out.write(self.title)
      if (lf > 0):
        out.write(self.frame * lf)
    else :
      out.write(self.frame * (width - 2))
    write_corner()
    out.write("\n")
    # main output
    for line in lines :
      if (line is None):
        write_sep()
      else :
        if (self.prefix is not None):
          out.write(self.prefix)
        out.write(side_frame + " ")
        rf, lf = get_padding(line)
        if (rf > 0):
          out.write(" " * rf)
        out.write(line)
        if (lf > 0):
          out.write(" " * lf)
        out.write(" " + side_frame)
        out.write("\n")
    # footer
    if (self.prefix is not None):
      out.write(self.prefix)
    write_corner()
    self.out.write(self.frame * (width - 2))
    write_corner()
    out.write("\n")

  def __del__(self):
    if (not self._closed):
      self.close()

def print_message_in_box(message, **kwds):
  box = framed_output(**kwds)
  for line in line_breaker(message, box.get_best_text_width()):
    print(line, file=box)
  del box

def make_big_header(line, out=None, header_len=80, border_char="#"):
  """
  Alternate API for print_message_in_box, for compatibility with make_header
  """
  if (out is None) : out = sys.stdout
  print_message_in_box(
    out=out,
    message=line,
    width=header_len,
    frame=border_char,
    center=True)

class find_matching_closing_symbol(object):
  """ This functor deals with a pair of symbol, an opening symbol and a closing one,
      the archetypical example being parentheses. Given the position of an opening
      symbol, it returns the position of the matching closing symbol, or -1 if it does not
      exist.
  """

  def __init__(self, opening, closing):
    """ Initialise the functor to work with the given pair. Each of `opening` and `closing`
        may be several character long.
    """
    import re
    self.opening, self.closing = opening, closing
    self._regex = re.compile("(%s)|(%s)" % (re.escape(opening), re.escape(closing)))

  def __call__(self, string, pos):
    """ An opening symbol shall start at position `pos` of the given `string`.
    """
    level = 1
    delta = (None, +1, -1)
    for m in self._regex.finditer(string, pos + len(self.opening)):
      if not m:
        return -1
      level += delta[m.lastindex]
      if level == 0:
        return m.start()
    return -1


 *******************************************************************************


 *******************************************************************************
libtbx/subprocess_with_fixes.py
# This module is obsolete and only here for compatibility reasons.
# Use the Python subprocess module directly.
from __future__ import absolute_import, division, print_function
from subprocess import *
import warnings
warnings.warn("libtbx.subprocess_with_fixes is deprecated", DeprecationWarning)


 *******************************************************************************


 *******************************************************************************
libtbx/table_utils.py
from __future__ import absolute_import, division, print_function
## Some functionality for formatted printing,
## indentation and simple tables
##
## Take from
##   http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/267662
## slightly modifed functionality.
##
from six.moves import range, zip, map
try: # Python 3
    from itertools import zip_longest
except ImportError: # Python 2
    from itertools import izip_longest as zip_longest
from functools import reduce
from six.moves import cStringIO as StringIO
import libtbx.forward_compatibility # for sum
import operator

def format(rows,
           comments=None,
           has_header=0,
           header_char='-',
           delim=' | ',
           justify='left',
           separate_rows=False,
           leading_and_terminal_separator=True ,
           prefix='',
           postfix='',
           wrapfunc=lambda x:x):
    """Indents a table by column.
       - rows: A sequence of sequences of items, one sequence per row.
       - hasHeader: True if the first row consists of the columns' names.
       - headerChar: Character to be used for the row separator line
         (if hasHeader==True or separateRows==True).
       - delim: The column delimiter.
       - justify: Determines how are data justified in their column.
         Valid values are 'left','right' and 'center'.
       - separateRows: True if rows are to be separated by a line
         of 'headerChar's.
       - prefix: A string prepended to each printed row.
       - postfix: A string appended to each printed row.
       - wrapfunc: A function f(text) for wrapping text; each element in
         the table is first wrapped by this function."""
    if has_header is True: has_header=1
    if has_header is False: has_header=0
    # closure for breaking logical rows to physical, using wrapfunc
    def row_wrapper(row):
        new_rows = [wrapfunc(item).split('\n') for item in row]
        return [[substr or '' for substr in item] for item in
                map(lambda *a: a, *list(zip(*zip_longest(*new_rows))))]
    # break each logical row into one or more physical ones
    logical_rows = [row_wrapper(row) for row in rows]
    # columns of physical rows
    columns = list(map(lambda *a: a, *list(zip(*zip_longest(*reduce(operator.add, logical_rows))))))
    # get the maximum of each column by the string length of its items
    max_widths = [max([len(str(item))
      for item in column]) for column in columns]
    prefix_minus_leading_spaces = prefix.lstrip()
    indent = len(prefix) - len(prefix_minus_leading_spaces)
    row_separator = " "*indent + header_char * (
                      len(prefix_minus_leading_spaces)
                    + len(postfix)
                    + sum(max_widths)
                    + len(delim)*(len(max_widths)-1))
    # select the appropriate justify method
    justify = {'center':str.center, 'right':str.rjust, 'left':str.ljust}[justify.lower()]
    output=StringIO()

    total_row_width=0
    line=None
    # Printing the table with values
    if leading_and_terminal_separator: print(row_separator, file=output)
    for physical_rows in logical_rows:
        for row in physical_rows:
            line =  prefix \
                + delim.join([justify(str(item),width) for (item,width) in zip(row,max_widths)]) \
                + postfix
            print(line, file=output)
        if separate_rows or has_header == 1: print(row_separator, file=output)
        if has_header : has_header-=1
    if not separate_rows:
        if leading_and_terminal_separator:
            print(row_separator, file=output)
    if comments is not None:
        print(wrap_onspace(comments,len(line)), file=output)
    return "\n".join(
      [line.rstrip() for line in output.getvalue().splitlines()])

def manage_columns(table_data, include_columns):
  for row in table_data:
    assert len(row) == len(include_columns)
  new_table = []
  for row in table_data:
    new_row = [row[idx] for idx in range(len(row)) if include_columns[idx]]
    new_table.append(new_row)
  return new_table

class simple_table(object):
  """
  Container for generic table contents, used in Xtriage and elsewhere.  The
  table cells are assumed to be pre-formatted as strings (but not fixed-width).
  This is designed to be easily fed into the format() function, but it can also
  be displayed in GUIs or HTML tables.
  """
  def __init__(self,
      table_rows,
      column_headers=(),
      comments=None):
    self.column_headers = list(column_headers)
    self._rows = table_rows

  def format(self, indent=0, equal_widths=None):
    # FIXME equal_widths is a placeholder, does not currently work
    prefix = " " * indent
    return format(
      rows=[ self.column_headers ]+self._rows,
      has_header=(len(self.column_headers) != 0),
      prefix=prefix+'| ',
      postfix=' |')

  def export(self):
    if (len(self.column_headers) != 0):
      return [ self.column_headers ] + self._rows
    else :
      return self._rows

  def export_rows(self) : return self.export()

  @property
  def n_rows(self):
    return len(self._rows)

# written by Mike Brown
# http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/148061
def wrap_onspace(text, width):
    """
    A word-wrap function that preserves existing line breaks
    and most spaces in the text. Expects that existing line
    breaks are posix newlines (\n).
    """
    return reduce(lambda line, word, width=width: '%s%s%s' %
                  (line,
                   ' \n'[(len(line[line.rfind('\n')+1:])
                         + len(word.split('\n',1)[0]
                              ) >= width)],
                   word),
                  text.split(' ')
                 )

import re
def wrap_onspace_strict(text, width):
    """Similar to wrap_onspace, but enforces the width constraint:
       words longer than width are split."""
    wordRegex = re.compile(r'\S{'+str(width)+r',}')
    return wrap_onspace(wordRegex.sub(lambda m: wrap_always(m.group(),width),text),width)

import math
def wrap_always(text, width):
    """A simple word-wrap function that wraps text on exactly width characters.
       It doesn't split the text in words."""
    return '\n'.join([ text[width*i:width*(i+1)] \
                       for i in range(int(math.ceil(1.*len(text)/width))) ])

## Additional functionality; go beyond formatted printing; emulate spreadsheet
## Typical use case to include:
##   column headers, column formatting specifiers, default column values,
##   choice of which rows to print, optional summation or averaging of column values,
##   spreadsheet-style cell formulae
##
## This is original source code developed for the LABELIT project

class Spreadsheet:

  def __init__(self,rows=0):
    self.S_table_rows = rows
    self.summaries = []

  def addColumn(self,label,default_value = None):
    setattr(self,label,SpreadsheetColumn(self,self.S_table_rows,default_value))

  def addRow(self):
    self.S_table_rows += 1
    for item in self.__dict__:
      if isinstance(self.__dict__[item],SpreadsheetColumn):
        self.__dict__[item].append()

  def printTable(self,columns,printed_rows=None,out=None):
    if out==None: import sys; out=sys.stdout
    if printed_rows is None:  printed_rows=[True]*self.S_table_rows
    master = {}
    padding = {}
    for column in columns:
      padding[column] = max(
          len(column),
          len(self.__dict__[column].format%self.__dict__[column][0])
        )
      master[column] = "%%%ds "%(padding[column])
      print(master[column]%column, end=' ', file=out)
    print(file=out)
    for i in range(self.S_table_rows):
      if not printed_rows[i]: continue
      for column in columns:
        print(master[column]%(
          self.__dict__[column].format%self.__dict__[column][i] ,), end=' ', file=out)
      print(file=out)
    #print summations at the bottom
    for column in columns:
      if column in self.summaries:
        print(" "+"-"*(padding[column]), end=' ', file=out)
      else:
        print(" "*(1+padding[column]), end=' ', file=out)
    print(file=out)
    for column in columns:
      if column in self.summaries:
        print(master[column]%(
          self.__dict__[column].format%self.__dict__[column].sum(),), end=' ', file=out)
      elif hasattr(self,"wtmean") and column in self.wtmean:
        normalizer = self.weights.sum()
        summation = 0
        for x in range(self.S_table_rows):
            summation += self.weights[x] * self.__dict__[column][x]
        mean = summation/normalizer
        print((master[column]%(
          self.__dict__[column].format%mean,) ), end=' ', file=out)
      else:
        print(" "*(1+padding[column]), end=' ', file=out)
    print(file=out)

  def rows(self):
    return self.S_table_rows

  def eval(self,expression):
    return eval(expression)

class SpreadsheetColumn:

  def __init__(self,parent,rows,default_value):
    self.parent = parent
    self.default = default_value
    self.C_data = [self.default]*rows

  def __setitem__(self,index,value):
    #general case, normal integer index
    if isinstance(index, type(0)):
      self.C_data[index]=value

    #special case, consult parent to get pointer to index[0], then +=index[1]
    #this will be used to refer to the correct image frame for sublattice calc
    if isinstance(index, type((0,1))):
      pointer = self.parent.pointer(index[0])
      self.C_data[pointer + index[1]]=value

  def __getitem__(self,index):
    #general case, normal integer index with specializations for Formulae
   if isinstance(index, type(0)):
    V = self.C_data[index]
    if isinstance(V,Formula):
      if V.expression=='self.Population[%row] / self.Fract[%row]':
        if self.parent.Fract[index]==0:return 0.0 # save from Zero Division
        return self.parent.Population[index] / self.parent.Fract[index]
      else:
        E = V.expression.replace('%row','%d'%index)
        return self.parent.eval(E)
    else:
      return V
   if isinstance(index, type((0,1))):
      pointer = self.parent.pointer(index[0])
      return self.C_data[pointer + index[1]]

  def __len__(self):
    return len(self.C_data)

  def append(self):
    self.C_data.append(self.default)

  def setFormat(self,format):
    self.format = format

  def sum(self):
    def typesum(x,y): return(x+y)
    return reduce(typesum,[self[i] for i in range(len(self))])

class Formula:
  def __init__(self,expression):
    self.expression = expression

#---
#--- tests
#---
def excercise_spreadsheet():
  class typical_use_case_1(Spreadsheet): #Case 1 is required for spotfinder spot statistics
    def __init__(self):

      Spreadsheet.__init__(self,rows=6) # make 6 rows in the Table
      self.addColumn('Limit')
      self.addColumn('MeanI')
      self.addColumn('Population',0) # 0 is the default value

      for xrow in range(0,self.S_table_rows):
        self.Limit[xrow] = [12.543,9.959,8.700,7.90,7.34,6.90][xrow]
        self.MeanI[xrow] = [4348,461,313,378,376,0][xrow]

      population_data = [926,121,8,2,6]
      for c in range(len(population_data)): #reconcile inconsistent bin counts
          self.Population[c]=population_data[c]

      self.Limit.format = "%.2f"
      self.MeanI.format = "%.0f"
      self.Population.format = "%7d"

    def show(self,message,out,columns_to_print=['Limit','Population','MeanI']):
      self.summaries=['Population'] #display the sum of the Population column
      self.wtmean=['MeanI']         #display the weighted mean of MeanI column,
      self.weights=self.Population  #...weighted by the Population
      legend = """Detailed explanation of each column here"""
      print(message+":\n", file=out)
      to_print = [True,True,True,True,True,True]
      self.printTable(columns_to_print,printed_rows=to_print,out=out)
      print(legend, file=out)

  OC = StringIO()
  derived_1 = typical_use_case_1()
  derived_1.show(message="Analysis of signals after noise suppression",out=OC)
  assert "\n".join([i.rstrip() for i in OC.getvalue().split("\n")]) == \
"""Analysis of signals after noise suppression:

Limit  Population  MeanI
12.54         926   4348
 9.96         121    461
 8.70           8    313
 7.90           2    378
 7.34           6    376
 6.90           0      0
        ----------
             1063   3845
Detailed explanation of each column here
"""
  class typical_use_case_2(Spreadsheet): #Case 2 is required for spotfinder ice detection
    def __init__(self):
      #Total rows by formula
      Total_rows = 4
      Spreadsheet.__init__(self,rows=Total_rows)
      self.addColumn('Limit')
      self.addColumn('Fract')
      self.addColumn('Population',0)
      self.addColumn('adjustPop',
                     Formula('self.Population[%row] / self.Fract[%row]'))

      for xrow in range(Total_rows):
        self.Limit[xrow] = [39.34,31.22,27.28,24.78][xrow]
        self.Fract[xrow] = [1.0,2.0,3.0,4.0][xrow]
        self.Population[xrow]=[53,102,107,86][xrow]

      self.Limit.format = "%.2f"
      self.Fract.format = "%.2f"
      self.Population.format = "%7d"
      self.adjustPop.format = "%7.1f"

    def show(self,default=['Limit','Population','Fract','adjustPop'],out=None):
      self.printTable(default,out=out)
      #print >>out,"0"

  OC = StringIO()
  derived_2 = typical_use_case_2()
  derived_2.show(out=OC)

  assert "\n".join([i.rstrip() for i in OC.getvalue().split("\n")]) == \
"""Limit  Population  Fract  adjustPop
39.34          53   1.00       53.0
31.22         102   2.00       51.0
27.28         107   3.00       35.7
24.78          86   4.00       21.5


"""

def exercise_flat_table():
  formatted_table = format(
    rows=[
      ["alpha","beta","gamma"],
      ["10","20","30"],
      ["40","50","60"]],
    comments="comments here",
    has_header=True)
  assert formatted_table == """\
--------------------
alpha | beta | gamma
--------------------
10    | 20   | 30
40    | 50   | 60
--------------------
comments here"""
  print("OK")

def exercise_general():
  labels = ('First Name', 'Last Name', 'Age', 'Position')
  data = \
      '''John,Smith,24,Software Engineer
      Mary,Brohowski,23,Sales Manager
      Aristidis,Papageorgopoulos,28,Senior Reseacher'''
  rows = [row.strip().split(',')  for row in data.splitlines()]

  table = format([labels]+rows, has_header=True)
  assert table == '''\
-------------------------------------------------------
First Name | Last Name        | Age | Position
-------------------------------------------------------
John       | Smith            | 24  | Software Engineer
Mary       | Brohowski        | 23  | Sales Manager
Aristidis  | Papageorgopoulos | 28  | Senior Reseacher
-------------------------------------------------------'''

  # test indent with different wrapping functions
  width = 10
  wrapped_output = [
      '''\
----------------------------------------------
| First Name | Last Name  | Age | Position   |
----------------------------------------------
| John       | Smith      | 24  | Software E |
|            |            |     | ngineer    |
----------------------------------------------
| Mary       | Brohowski  | 23  | Sales Mana |
|            |            |     | ger        |
----------------------------------------------
| Aristidis  | Papageorgo | 28  | Senior Res |
|            | poulos     |     | eacher     |
----------------------------------------------''',
      '''\
---------------------------------------------------
| First Name | Last Name        | Age | Position  |
---------------------------------------------------
| John       | Smith            | 24  | Software  |
|            |                  |     | Engineer  |
---------------------------------------------------
| Mary       | Brohowski        | 23  | Sales     |
|            |                  |     | Manager   |
---------------------------------------------------
| Aristidis  | Papageorgopoulos | 28  | Senior    |
|            |                  |     | Reseacher |
---------------------------------------------------''',
      '''\
---------------------------------------------
| First Name | Last Name  | Age | Position  |
---------------------------------------------
| John       | Smith      | 24  | Software  |
|            |            |     | Engineer  |
---------------------------------------------
| Mary       | Brohowski  | 23  | Sales     |
|            |            |     | Manager   |
---------------------------------------------
| Aristidis  | Papageorgo | 28  | Senior    |
|            | poulos     |     | Reseacher |
---------------------------------------------'''
  ]
  for wrapper, output in zip((wrap_always,wrap_onspace,wrap_onspace_strict),
                             wrapped_output):
      table = format([labels]+rows, has_header=True, separate_rows=True,
                     prefix='| ', postfix=' |',
                     wrapfunc=lambda x: wrapper(x,width))
      assert table == output

if (__name__ == "__main__"):
  excercise_spreadsheet()
  exercise_flat_table()
  exercise_general()


 *******************************************************************************


 *******************************************************************************
libtbx/test_utils/__init__.py
from __future__ import absolute_import, division, print_function
from libtbx.option_parser import option_parser
from libtbx.utils import Sorry
from libtbx.str_utils import show_string
from libtbx import easy_run
from libtbx import introspection
try:
  from contextlib import AbstractContextManager
except ImportError:
  AbstractContextManager = object
import difflib
import libtbx.load_env
import math
import os
import sys
import time
import types

import six
from six.moves import cStringIO as StringIO
from six.moves import range
try:
  import threading
except ImportError:
  threading = None
else:
  from six.moves import queue

class _string_with_attributes(type("")):
  """Subclass string so that it can accept additional attributes."""

diff_function = getattr(difflib, "unified_diff", difflib.ndiff)

def compose_tmp_suffix(suffix, frames_back=0):
  from libtbx.introspection import caller_location
  caller = caller_location(frames_back=frames_back+1)
  s = os.path.basename(caller.file_name)
  if (s.lower().endswith(".py")): s = s[:-3]
  s += "_" + str(caller.line_number)
  return "_" + s + suffix

def open_tmp_file(suffix="", mode="w"):
  import tempfile
  (fd, name) = tempfile.mkstemp(
    suffix=compose_tmp_suffix(suffix, frames_back=1),
    dir=".")
  os.close(fd)
  return open(name, mode)

def open_tmp_directory(suffix=""):
  import tempfile
  return tempfile.mkdtemp(
    suffix=compose_tmp_suffix(suffix, frames_back=1),
    dir=".")

class pickle_detector(object):
  def __init__(O):
    O.unpickled_counter = None
    O.counter = 0
  def __getstate__(O):
    O.counter += 1
    return {"counter": O.counter}
  def __setstate__(O, state):
    assert len(state) == 1
    O.unpickled_counter = state["counter"]
    O.counter = 0

Exception_expected = RuntimeError("Exception expected.")
Exception_not_expected = RuntimeError("Exception not expected.")

class raises(AbstractContextManager):
  def __init__(self, expected_exception):
    self.expected_exception = expected_exception
    self.type = None
    self.value = None
    self.traceback = None

  def __enter__(self):  # this is only needed for Python 2
    return self

  def __exit__(self, exc_type, exc_value, traceback):
    self.type = exc_type
    self.value = exc_value
    self.traceback = traceback
    if isinstance(self.type(), self.expected_exception):
      return self

class Default: pass

def test_usage(cmd):
  result = easy_run.fully_buffered(cmd)
  if (result.return_code == 0):
    return True
  else :
    if (len(result.stderr_lines) > 0):
      if ("Usage" in result.stdout_lines[0]):
        return True
      else :
        raise Sorry("Bad stderr output from %s:\n%s" % (cmd,
          "\n".join(result.stderr_lines)))
    else :
      raise Sorry("Bad return code from %s - dumping stdout:\n%s" %
        (cmd, "\n".join(result.stdout_lines)))
  assert 0

def run_tests(build_dir, dist_dir, tst_list, display_times=False):
  if display_times:
    t0=time.time()
    start = time.asctime()
  libtbx.env.full_testing = True
  args = [arg.lower() for arg in sys.argv[1:]]
  command_line = (option_parser(
    usage="run_tests [-j n]",
    description="Run several threads in parallel, each picking and then"
                " running tests one at a time.")
    .option("-j", "--threads",
      action="store",
      type="int",
      default=1,
      help="number of threads",
      metavar="INT")
    .option("-v", "--verbose",
      action="store_true",
      default=False)
    .option("-q", "--quick",
      action="store_true",
      default=False)
    .option("-g", "--valgrind",
      action="store_true",
      default=False)
  ).process(args=args, max_nargs=0)
  co = command_line.options
  if (threading is None or co.threads == 1):
    for cmd in iter_tests_cmd(co, build_dir, dist_dir, tst_list):
      print(cmd)
      sys.stdout.flush()
      easy_run.call(command=cmd)
      print()
      sys.stderr.flush()
      sys.stdout.flush()
  else:
    cmd_queue = queue.Queue()
    for cmd in iter_tests_cmd(co, build_dir, dist_dir, tst_list):
      cmd_queue.put(cmd)
    threads_pool = []
    log_queue = queue.Queue()
    interrupted = threading.Event()
    for i in range(co.threads):
      working_dir = os.tempnam()
      os.mkdir(working_dir)
      t = threading.Thread(
        target=make_pick_and_run_tests(working_dir, interrupted,
                                       cmd_queue, log_queue))
      threads_pool.append(t)
    for t in threads_pool:
      t.start()
    finished_threads = 0
    while(1):
      try:
        log = log_queue.get()
        if isinstance(log, tuple):
          msg = log[0]
          print("\n +++ thread %s +++\n" % msg, file=sys.stderr)
          finished_threads += 1
          if finished_threads == co.threads: break
        else:
          print(log)
      except KeyboardInterrupt:
        print()
        print("********************************************")
        print("** Received Keyboard Interrupt            **")
        print("** Waiting for running tests to terminate **")
        print("********************************************")
        interrupted.set()
        break
  if display_times:
    print("TIME (%s) (%s) %7.2f %s" % (start,
                                       time.asctime(),
                                       time.time()-t0,
                                       tst_list,
                                       ))

def make_pick_and_run_tests(working_dir, interrupted,
                            cmd_queue, log_queue):
  def func():
    while(not interrupted.isSet()):
      try:
        cmd = cmd_queue.get(block=True, timeout=0.2)
        log = "\n%s\n" % cmd
        proc = easy_run.subprocess.Popen(cmd,
                                  shell=True,
                                  stdout=easy_run.subprocess.PIPE,
                                  stderr=easy_run.subprocess.STDOUT,
                                  cwd=working_dir)
        proc.wait()
        log += "\n%s" % proc.stdout.read()
        log_queue.put(log)
      except queue.Empty:
        log_queue.put( ("done",) )
        break
      except KeyboardInterrupt:
        break
  return func

def iter_tests_cmd(co, build_dir, dist_dir, tst_list):
  for tst in tst_list:
    test_class = getattr(tst, 'test_class', None)
    test_name = getattr(tst, 'test_name', None)
    cmd_args = ""
    if isinstance(tst, list):
      if (co is not None) and (co.verbose):
        cmd_args = " " + " ".join(["--Verbose"] + tst[1:])
      elif (co is not None) and (co.quick):
        cmd_args = " " + " ".join(tst[1:])
      elif (co is not None):
        cmd_args = " " + " ".join(tst[1:])
      tst = tst[0]
    elif isinstance(tst, types.FunctionType): #adds ability to execute a function defined within run_tests.py
      cmd = "libtbx.python -c "
      base_module = os.path.basename(dist_dir)
      cmd +='"from %s.run_tests import %s; %s()"'%(base_module,tst.__name__,tst.__name__)
      yield cmd
      continue
    elif (co is not None) and (co.verbose):
      continue
    if (tst.startswith("$B")):
      tst_path = tst.replace("$B", build_dir)
    else:
      tst_path = tst.replace("$D", dist_dir)
    assert tst_path.find("$") < 0
    assert tst_path.find('"') < 0
    tst_path = os.path.normpath(tst_path)
    cmd = ""
    if (tst_path.endswith(".py")):
      if (co is not None) and (co.valgrind):
        cmd = "libtbx.valgrind "
      cmd += "libtbx.python "
    else:
      if (co is not None) and (co.valgrind):
        cmd = os.environ.get(
          "LIBTBX_VALGRIND", "valgrind --tool=memcheck") + " "
    if tst_path.strip() != "libtbx.python":
      tst_path = '"{}"'.format(tst_path)
    cmd += tst_path
    cmd += cmd_args

    if hasattr(co,'python_keyword_text') and co.python_keyword_text:
      cmd=cmd.replace("libtbx.python","libtbx.python %s" %(
         co.python_keyword_text))

    if test_class and test_name:
      # If the passed object contains test class and name information
      # then attach that information to the string object.
      cmd = _string_with_attributes(cmd)
      cmd.test_class = test_class
      cmd.test_name = test_name
    yield cmd

def approx_equal_core(a1, a2, eps, multiplier, out, prefix):
  if isinstance(a1, (six.text_type, six.binary_type)):
    return a1 == a2

  # Dictionaries
  if isinstance(a1, dict) and isinstance(a2, dict):
    # Check if dictionaries have the same keys
    if set(a1.keys()) != set(a2.keys()):
      raise AssertionError(
        "approx_equal ERROR: a1.keys() != a2.keys(): %s != %s" % (a1.keys(), a2.keys()))
    # Compare each key-value pair
    for key in a1:
      if not approx_equal_core(
        a1[key], a2[key], eps, multiplier, out, prefix + str(key) + ": "):
        return False
    return True

  # List-like objects
  if hasattr(a1, "__len__"):
    if (len(a1) != len(a2)):
      raise AssertionError(
        "approx_equal ERROR: len(a1) != len(a2): %d != %d" % (
          len(a1), len(a2)))
    for i in range(len(a1)):
      if not approx_equal_core(
                a1[i], a2[i], eps, multiplier, out, prefix+"  "):
        return False
    return True

  # Complex numbers
  is_complex_1 = isinstance(a1, complex)
  is_complex_2 = isinstance(a2, complex)
  if (is_complex_1 and is_complex_2): # complex & complex
    if not approx_equal_core(
              a1.real, a2.real, eps, multiplier, out, prefix+"real "):
      return False
    if not approx_equal_core(
              a1.imag, a2.imag, eps, multiplier, out, prefix+"imag "):
      return False
    return True
  elif (is_complex_1): # complex & number
    if not approx_equal_core(
              a1.real, a2, eps, multiplier, out, prefix+"real "):
      return False
    if not approx_equal_core(
              a1.imag, 0, eps, multiplier, out, prefix+"imag "):
      return False
    return True
  elif (is_complex_2): # number & complex
    if not approx_equal_core(
              a1, a2.real, eps, multiplier, out, prefix+"real "):
      return False
    if not approx_equal_core(
              0, a2.imag, eps, multiplier, out, prefix+"imag "):
      return False
    return True

  # Regular numbers
  ok = True
  d = a1 - a2
  if (abs(d) > eps):
    if (multiplier is None):
      ok = False
    else:
      am = max(a1,a2) * multiplier
      d = (am - d) - am
      if (d != 0):
        ok = False
  if (out is not None):
    annotation = ""
    if (not ok):
      annotation = " approx_equal ERROR"
    print(prefix + str(a1) + annotation, file=out)
    print(prefix + str(a2) + annotation, file=out)
    print(prefix.rstrip(), file=out)
    return True
  return ok

def approx_equal(a1, a2, eps=1.e-6, multiplier=1.e10, out=Default, prefix=""):
  ok = approx_equal_core(a1, a2, eps, multiplier, None, prefix)
  if (not ok and out is not None):
    if (out is Default): out = sys.stdout
    print(prefix + "approx_equal eps:", eps, file=out)
    print(prefix + "approx_equal multiplier:", multiplier, file=out)
    assert approx_equal_core(a1, a2, eps, multiplier, out, prefix)
  return ok

def not_approx_equal(a1, a2, eps=1.e-6, multiplier=1.e10):
  return not approx_equal(a1, a2, eps, multiplier, out=None)

def eps_eq_core(a1, a2, eps, out, prefix):
  if (hasattr(a1, "__len__")): # traverse list
    assert len(a1) == len(a2)
    for i in range(len(a1)):
      if (not eps_eq_core(a1[i], a2[i], eps, out, prefix+"  ")):
        return False
    return True
  if (isinstance(a1, complex)): # complex numbers
    if (not eps_eq_core(a1.real, a2.real, eps, out, prefix+"real ")):
      return False
    if (not eps_eq_core(a1.imag, a2.imag, eps, out, prefix+"imag ")):
      return False
    return True
  ok = True
  if (a1 == 0 or a2 == 0):
    if (abs(a1-a2) > eps):
      ok = False
  else:
    l1 = round(math.log(abs(a1)))
    l2 = round(math.log(abs(a2)))
    m = math.exp(-max(l1, l2))
    if (abs(a1*m-a2*m) > eps):
      ok = False
  if (out is not None):
    annotation = ""
    if (not ok):
      annotation = " eps_eq ERROR"
    print(prefix + str(a1) + annotation, file=out)
    print(prefix + str(a2) + annotation, file=out)
    print(prefix.rstrip(), file=out)
    return True
  return ok

def eps_eq(a1, a2, eps=1.e-6, out=Default, prefix=""):
  ok = eps_eq_core(a1, a2, eps, None, prefix)
  if (not ok and out is not None):
    if (out is Default): out = sys.stdout
    print(prefix + "eps_eq eps:", eps, file=out)
    assert eps_eq_core(a1, a2, eps, out, prefix)
  return ok

def not_eps_eq(a1, a2, eps=1.e-6):
  return not eps_eq(a1, a2, eps, None)

def is_below_limit(
      value,
      limit,
      eps=1.e-6,
      info_low_eps=None,
      out=Default,
      info_prefix="INFO LOW VALUE: "):
  if (isinstance(value, (int, float)) and value < limit + eps):
    if (info_low_eps is not None and value < limit - info_low_eps):
      if (out is not None):
        if (out is Default): out = sys.stdout
        introspection.show_stack(
          frames_back=1, reverse=True, out=out, prefix=info_prefix)
        print("%sis_below_limit(value=%s, limit=%s, info_low_eps=%s)" % (
            info_prefix, str(value), str(limit), str(info_low_eps)), file=out)
    return True
  if (out is not None):
    if (out is Default): out = sys.stdout
    print("ERROR:", \
      "is_below_limit(value=%s, limit=%s, eps=%s)" % (
        str(value), str(limit), str(eps)), file=out)
  return False

def is_above_limit(
      value,
      limit,
      eps=1.e-6,
      info_high_eps=None,
      out=Default,
      info_prefix="INFO HIGH VALUE: "):
  if (isinstance(value, (int, float)) and value > limit - eps):
    if (info_high_eps is not None and value > limit + info_high_eps):
      if (out is not None):
        if (out is Default): out = sys.stdout
        introspection.show_stack(
          frames_back=1, reverse=True, out=out, prefix=info_prefix)
        print("%sis_above_limit(value=%s, limit=%s, info_high_eps=%s)" % (
            info_prefix, str(value), str(limit), str(info_high_eps)), file=out)
    return True
  if (out is not None):
    if (out is Default): out = sys.stdout
    print("ERROR:", \
      "is_above_limit(value=%s, limit=%s, eps=%s)" % (
        str(value), str(limit), str(eps)), file=out)
  return False

def precision_approx_equal(self,other,precision=24):
  # Use concepts from IEEE-754 to determine if the difference between
  # two numbers is within floating point error. Precision is expressed in
  # bits (single precision=24; double precision=53). Not within scope to
  # do this for double precision literals; only interested in the case
  # where the data are from a ~single precision digital-analog converter.
  if precision > 52: raise ValueError()
  if self==other:
    return True
  if (self > 0.) != (other > 0.):
    return False
  #compute the exponent
  import math
  T = abs(self)
  Np = math.floor(precision-math.log(T,2))
  significand = int(T * 2**Np)
  val1 = significand/(2**Np) # nearest floating point representation of self
  val2 = (significand+1)/(2**Np) # next-nearest
  return abs(T-abs(other)) <= abs(val1-val2)

def show_diff(a, b, out=sys.stdout,
    selections=None, expected_number_of_lines=None,
    strip_trailing_whitespace=False):
  if (not isinstance(a, (str,bytes))):
    a = "\n".join(a)+"\n"
  if (selections is None):
    assert expected_number_of_lines is None
  else:
    a_lines = a.splitlines(1)
    a = []
    for selection in selections:
      for i in selection:
        if (i < 0): i += len(a_lines)
        a.append(a_lines[i])
      a.append("...\n")
    a = "".join(a[:-1])
  if strip_trailing_whitespace:
    import re
    a = re.sub(r'[ \t]*\n', '\n', a, 0, re.MULTILINE)
    b = re.sub(r'[ \t]*\n', '\n', b, 0, re.MULTILINE)
  if (a != b):
    if (not a.endswith("\n") or not b.endswith("\n")):
      a += "\n"
      b += "\n"
    print("".join(diff_function(b.splitlines(1), a.splitlines(1))), file=out)
    return True
  if (    expected_number_of_lines is not None
      and len(a_lines) != expected_number_of_lines):
    print("show_diff: expected_number_of_lines != len(a.splitlines()): %d != %d" \
        % (expected_number_of_lines, len(a_lines)), file=out)
    return True
  return False

def block_show_diff(lines, expected, last_startswith=False):
  if (isinstance(lines, str)):
    lines = lines.splitlines()
  if (isinstance(expected, str)):
    expected = expected.splitlines()
  assert len(expected) > 1
  def raise_not_found():
    print("block_show_diff() lines:")
    print("-"*80)
    print("\n".join(lines))
    print("-"*80)
    raise AssertionError('Expected line not found: "%s"' % eline)
  eline = expected[0]
  for i,line in enumerate(lines):
    if (line == eline):
      lines = lines[i:]
      break
  else:
    raise_not_found()
  eline = expected[-1]
  for i,line in enumerate(lines):
    if (last_startswith):
      if (line.startswith(eline)):
        lines = lines[:i]
        expected = expected[:-1]
        break
    else:
      if (line == eline):
        lines = lines[:i+1]
        break
  else:
    raise_not_found()
  lines = "\n".join(lines)+"\n"
  expected = "\n".join(expected)+"\n"
  return show_diff(lines, expected)

def blocks_show_diff(lines, expected, block_sperator="\n...\n"):
  assert isinstance(expected, str)
  result = False
  expected_blocks = expected.split(block_sperator)
  for expected in expected_blocks:
    if (block_show_diff(lines=lines, expected=expected)):
      result = True
  return result

def anchored_block_show_diff(lines, anchor_index, expected):
  if (isinstance(lines, str)):
    lines = lines.splitlines()
  if (isinstance(expected, str)):
    expected = expected.splitlines()
  assert len(expected) > 0
  if (anchor_index < 0):
    anchor_index = len(lines) + anchor_index - len(expected) + 1
  lines = "\n".join(lines[anchor_index:anchor_index+len(expected)])+"\n"
  expected = "\n".join(expected)+"\n"
  return show_diff(lines, expected)

def contains_substring(
      actual,
      expected,
      failure_prefix="contains_substring() "):
  assert isinstance(actual, str)
  assert isinstance(expected, str)
  if (actual.find(expected) < 0):
    print("%sFAILURE:" % failure_prefix)
    def show(s):
      print("v"*79)
      if (s.endswith("\n") or s.endswith(os.linesep)):
        sys.stdout.write(s)
      else:
        print(s)
      print("^"*79)
    show(actual)
    print("  ACTUAL ----^")
    print("EXPECTED ----v")
    show(expected)
    return False
  return True

def contains_lines(lines, expected):
  return contains_substring(
    actual=lines, expected=expected, failure_prefix="contains_lines() ")

def assert_lines_text(text, lines, present=True,
    remove_white_spaces=True, remove_newline=True):
  """Tests if lines present or absent in the text.

  Args:
      text (str): source text
      lines (str): lines to search for
      remove_white_spaces (bool, optional): Remove whitespaces for more robust search.
          Defaults to True.
      remove_newline (bool, optional): Remove newlines for more robust search.
          Defaults to True.
  """
  filtered_lines = lines
  if remove_white_spaces:
    text = text.replace(" ", "")
    filtered_lines = filtered_lines.replace(" ", "")
  if remove_newline:
    text = text.replace(os.linesep,"")
    filtered_lines = filtered_lines.replace(os.linesep,"")
  if present:
    assert text.find(filtered_lines) >= 0, \
        "Lines:\n %s\n are not found" % (lines)
  else:
    assert text.find(filtered_lines) < 0, \
        "Lines:\n %s\n are found, but they should not be there." % (lines)


def assert_lines_in_text(text, lines,
    remove_white_spaces=True, remove_newline=True):
  """Tests if lines present in the text.

  Args:
      text (str): source text
      lines (str): lines to search for
      remove_white_spaces (bool, optional): Remove whitespaces for more robust search.
          Defaults to True.
      remove_newline (bool, optional): Remove newlines for more robust search.
          Defaults to True.
  """
  assert_lines_text(text, lines, True, remove_white_spaces, remove_newline)

def assert_lines_not_in_text(text, lines,
    remove_white_spaces=True, remove_newline=True):
  """Ensures that lines are NOT present in the text.

  Args:
      text (str): source text
      lines (str): lines to search for
      remove_white_spaces (bool, optional): Remove whitespaces for more robust search.
          Defaults to True.
      remove_newline (bool, optional): Remove newlines for more robust search.
          Defaults to True.
  """
  assert_lines_text(text, lines, False, remove_white_spaces, remove_newline)

def assert_lines_in_file(file_name, lines,
    remove_white_spaces=True, remove_newline=True):
  """
  lines here is a text, not a list of lines.
  """
  f = open(file_name, "r")
  f_lines = f.read()
  f.close()
  assert_lines_in_text(f_lines, lines=lines,
      remove_white_spaces=remove_white_spaces, remove_newline=remove_newline)

def assert_lines_not_in_file(file_name, lines,
    remove_white_spaces=True, remove_newline=True):
  """
  lines here is a text, not a list of lines.
  """
  with open(file_name,'r') as f:
    f_lines = f.read()
    assert_lines_not_in_text(f_lines, lines=lines,
        remove_white_spaces=remove_white_spaces, remove_newline=remove_newline)


class RunCommandError(RuntimeError): pass

def _check_command_output(
    lines=None,
    file_name=None,
    show_command_if_error=None,
    sorry_expected=False):
  assert [lines, file_name].count(None) == 1
  if (lines is None):
    lines = open(file_name).read().splitlines()
  def show_and_raise(detected):
    if (show_command_if_error):
      print(show_command_if_error)
      print()
    print("\n".join(lines))
    msg = detected + " detected in output"
    if (file_name is None):
      msg += "."
    else:
      msg += ": " + show_string(file_name)
    raise RunCommandError(msg)
  have_sorry = False
  for line in lines:
    if (line == "Traceback (most recent call last):"):
      show_and_raise(detected="Traceback")
    if (line.startswith("Sorry:")):
      have_sorry = True
  if (have_sorry and not sorry_expected):
    show_and_raise(detected='"Sorry:"')

def run_command(
      command,
      verbose=0,
      buffered=True,
      log_file_name=None,
      stdout_file_name=None,
      result_file_names=[],
      show_diff_log_stdout=False,
      sorry_expected=False,
      join_stdout_stderr=False):
  """\
This function starts another process to run command, with some
pre-call and post-call processing.
Before running command, the expected output files are removed:

  log_file_name
  stdout_file_name
  result_file_names

After command is finished, log_file_name and stdout_file_name are scanned
for Traceback and Sorry. An exception is raised if there are any
matches. sorry_expected=True suppresses the scanning for Sorry.

With buffered=True easy_run.fully_buffered() is used. If there
is any output to stderr of the child process, an exception is
raised. The run_command() return value is the result of the
easy_run.fully_buffered() call.

With buffered=False easy_run.call() is used. I.e. stdout and stderr
of the command are connected to stdout and stderr of the parent
process. stderr is not checked. The run_command() return value is None.

It is generally best to use buffered=True, and even better not to
use this function at all if command is another Python script. It
is better to organize the command script such that it can be called
directly from within the same Python process running the unit tests.
"""
  assert verbose >= 0
  if (verbose > 0):
    print(command)
    print()
    show_command_if_error = None
  else:
    show_command_if_error = command
  all_file_names = [log_file_name, stdout_file_name] + result_file_names
  for file_name in all_file_names:
    if (file_name is None): continue
    if (os.path.isfile(file_name)): os.remove(file_name)
    if (os.path.exists(file_name)):
      raise RunCommandError(
        "Unable to remove file: %s" % show_string(file_name))
  if (buffered):
    sys.stdout.flush()
    sys.stderr.flush()
    cmd_result = easy_run.fully_buffered(
      command=command,
      join_stdout_stderr=join_stdout_stderr)
    if (len(cmd_result.stderr_lines) != 0):
      if (verbose == 0):
        print(command)
        print()
      print("\n".join(cmd_result.stdout_lines))
      cmd_result.raise_if_errors()
    _check_command_output(
      lines=cmd_result.stdout_lines,
      show_command_if_error=show_command_if_error,
      sorry_expected=sorry_expected)
  else:
    easy_run.call(command=command)
    cmd_result = None
  for file_name in [log_file_name, stdout_file_name]:
    if (file_name is None or not os.path.isfile(file_name)): continue
    _check_command_output(
      file_name=file_name,
      show_command_if_error=show_command_if_error,
      sorry_expected=sorry_expected)
  for file_name in all_file_names:
    if (file_name is None): continue
    if (not os.path.isfile(file_name)):
      raise RunCommandError(
        "Missing output file: %s" % show_string(file_name))
  if (verbose > 1 and cmd_result is not None):
    print("\n".join(cmd_result.stdout_lines))
    print()
  if (    show_diff_log_stdout
      and log_file_name is not None
      and stdout_file_name is not None):
    if (verbose > 0):
      print("diff %s %s" % (show_string(log_file_name),
                            show_string(stdout_file_name)))
      print()
    if (show_diff(open(log_file_name).read(), open(stdout_file_name).read())):
      introspection.show_stack(
        frames_back=1, reverse=True, prefix="INFO_LOG_STDOUT_DIFFERENCE: ")
      print("ERROR_LOG_STDOUT_DIFFERENCE")
  sys.stdout.flush()
  return cmd_result

def exercise():
  assert approx_equal(1, 1)
  out = StringIO()
  assert not approx_equal(1, 0, out=out)
  assert not show_diff(out.getvalue().replace("1e-006", "1e-06"), """\
approx_equal eps: 1e-06
approx_equal multiplier: 10000000000.0
1 approx_equal ERROR
0 approx_equal ERROR

""")
  out = StringIO()
  assert not approx_equal(1, 2, out=out)
  assert not show_diff(out.getvalue().replace("1e-006", "1e-06"), """\
approx_equal eps: 1e-06
approx_equal multiplier: 10000000000.0
1 approx_equal ERROR
2 approx_equal ERROR

""")
  out = StringIO()
  assert not approx_equal(1, 1+1.e-5, out=out)
  assert approx_equal(1, 1+1.e-6)
  out = StringIO()
  assert not approx_equal(0, 1.e-5, out=out)
  assert approx_equal(0, 1.e-6)
  out = StringIO()
  assert not approx_equal([[0,1],[2j,3]],[[0,1],[complex(0,-2),3]], out=out,
                          prefix="$%")
  assert not show_diff(out.getvalue().replace("1e-006", "1e-06"), """\
$%approx_equal eps: 1e-06
$%approx_equal multiplier: 10000000000.0
$%    0
$%    0
$%
$%    1
$%    1
$%
$%    real 0.0
$%    real 0.0
$%    real
$%    imag 2.0 approx_equal ERROR
$%    imag -2.0 approx_equal ERROR
$%    imag
$%    3
$%    3
$%
""")
  assert eps_eq(1, 1)
  out = StringIO()
  assert not eps_eq(1, 0, out=out)
  assert not show_diff(out.getvalue().replace("1e-006", "1e-06"), """\
eps_eq eps: 1e-06
1 eps_eq ERROR
0 eps_eq ERROR

""")
  out = StringIO()
  assert not eps_eq(1, 2, out=out)
  assert not show_diff(out.getvalue().replace("1e-006", "1e-06"), """\
eps_eq eps: 1e-06
1 eps_eq ERROR
2 eps_eq ERROR

""")
  out = StringIO()
  assert not eps_eq(1, 1+1.e-5, out=out)
  assert eps_eq(1, 1+1.e-6)
  out = StringIO()
  assert not eps_eq(0, 1.e-5, out=out)
  assert eps_eq(0, 1.e-6)
  out = StringIO()
  assert not eps_eq([[0,1],[2j,3]],[[0,1],[complex(0,-2),3]], out=out,
                    prefix="$%")
  assert not show_diff(out.getvalue().replace("1e-006", "1e-06"), """\
$%eps_eq eps: 1e-06
$%    0
$%    0
$%
$%    1
$%    1
$%
$%    real 0.0
$%    real 0.0
$%    real
$%    imag 2.0 eps_eq ERROR
$%    imag -2.0 eps_eq ERROR
$%    imag
$%    3
$%    3
$%
""")
  assert is_below_limit(value=5, limit=10, eps=2)
  out = StringIO()
  assert is_below_limit(value=5, limit=10, eps=2, info_low_eps=1, out=out)
  assert not show_diff(out.getvalue(), """\
INFO LOW VALUE: is_below_limit(value=5, limit=10, info_low_eps=1)
""", selections=[[-1]], expected_number_of_lines=3)
  out = StringIO()
  assert not is_below_limit(value=15, limit=10, eps=2, out=out)
  assert not show_diff(out.getvalue(), """\
ERROR: is_below_limit(value=15, limit=10, eps=2)
""")
  out = StringIO()
  assert not is_below_limit(value=None, limit=3, eps=1, out=out)
  assert not is_below_limit(value=None, limit=-3, eps=1, out=out)
  assert not show_diff(out.getvalue(), """\
ERROR: is_below_limit(value=None, limit=3, eps=1)
ERROR: is_below_limit(value=None, limit=-3, eps=1)
""")
  assert is_above_limit(value=10, limit=5, eps=2)
  out = StringIO()
  assert is_above_limit(value=10, limit=5, eps=2, info_high_eps=1, out=out)
  assert not show_diff(out.getvalue(), """\
INFO HIGH VALUE: is_above_limit(value=10, limit=5, info_high_eps=1)
""", selections=[[-1]], expected_number_of_lines=3)
  out = StringIO()
  assert not is_above_limit(value=10, limit=15, eps=2, out=out)
  assert not show_diff(out.getvalue(), """\
ERROR: is_above_limit(value=10, limit=15, eps=2)
""")
  out = StringIO()
  assert not is_above_limit(value=None, limit=-3, eps=1, out=out)
  assert not is_above_limit(value=None, limit=3, eps=1, out=out)
  assert not show_diff(out.getvalue(), """\
ERROR: is_above_limit(value=None, limit=-3, eps=1)
ERROR: is_above_limit(value=None, limit=3, eps=1)
""")
  #
  from six.moves import cPickle as pickle
  for p in [pickle]:
    d = pickle_detector()
    assert d.unpickled_counter is None
    assert d.counter == 0
    s = p.dumps(d, 1)
    assert d.unpickled_counter is None
    assert d.counter == 1
    l = p.loads(s)
    assert l.unpickled_counter == 1
    assert l.counter == 0
    p.dumps(d, 1)
    assert d.counter == 2
    assert l.counter == 0
    p.dumps(l, 1)
    assert l.counter == 1
    s = p.dumps(l, 1)
    assert l.counter == 2
    k = p.loads(s)
    assert k.unpickled_counter == 2
    assert k.counter == 0
  #
  assert precision_approx_equal(0.799999,0.800004,precision=17)==True
  assert precision_approx_equal(0.799999,0.800004,precision=18)==False
  print("OK")

def iterate_tests_without_and_with_mmCIF_conversion():
  """ Return a simple iterator that first does nothing and returns False,
      then converts all pdb_str_xxxx locals to mmCIF-only format
      and prints a notice and returns True.

      Behavior can be modified by specifying "skip_mmcif" or "mmcif_only"
       in sys.argv or by setting the value of the environmental
      variable REGRESSION_SKIP_CIF ( mmcif_only, skip_mmcif,
       blank or missing is run both)
  """
  skip_string = os.environ.get("REGRESSION_SKIP_CIF",None)
  if skip_string is None:
    if 'mmcif_only' in sys.argv:
      skip_string = 'mmcif_only'
    elif 'skip_mmcif' in sys.argv:
      skip_string = 'skip_mmcif'

  if skip_string in [None, '']:
    run_list = [False, True]
  elif skip_string == 'mmcif_only':
    run_list = [True]
  elif skip_string == 'skip_mmcif':
    run_list = [False]
  else:
    raise Sorry(
      "Unrecognized value of REGRESSION_SKIP_CIF: '%s'" %(skip_string))

  return run_list

def convert_pdb_to_cif_for_pdb_str(locals, chain_addition = "ZXLONG",
   key_str="pdb_str", hetatm_name_addition = "ZY", print_new_string = True):
  #  Converts all the strings that start with "pdb_str" from PDB to mmcif
  #  format, adding chain_addition to chain names
  #  If hetatm_name_addition is set, add to hetatm names
  #  If print_new_string is set, print the new strings
  keys = list(locals.keys())
  for key in keys:
    if (not key.startswith(key_str)) or (type(locals[key]) != type("abc")):
      continue

    original_string = locals[key]

    new_string = convert_string_to_cif_long(original_string,
      chain_addition = chain_addition,
      hetatm_name_addition = hetatm_name_addition)
    locals[key] = new_string
    if print_new_string:
       print("\n",79*"=","\n",
          "ORIGINAL STRING '%s':\n%s" %(key, original_string))
       print("\n",79*"=","\n",
          "MODIFIED STRING '%s':\n%s" %(key, new_string),
          "\n",79*"=","\n")

def convert_string_to_cif_long(original_string,  chain_addition = "ZXLONG",
   hetatm_name_addition = "ZY"):
    from iotbx.pdb.utils import get_pdb_input
    pdb_inp = get_pdb_input(original_string)
    ph = pdb_inp.construct_hierarchy()
    if ph.overall_counts().n_residues < 1:
      return ""
    for model in ph.models():
     for chain in model.chains():
       chain.id = "%s%s" %(chain.id.strip(),chain_addition)
       if hetatm_name_addition:
         for rg in chain.residue_groups():
           for ag in rg.atom_groups():
             for at in ag.atoms():
               if at.hetero and len(ag.resname)<=3:
                 ag.resname = "%s%s" %(ag.resname.strip(), hetatm_name_addition)
                 break
    new_string = ph.as_mmcif_string(
      crystal_symmetry = pdb_inp.crystal_symmetry())
    return new_string
def tst_convert():
  text = """
ATOM      1  N   VAL A   1      -5.111   0.049  13.245  1.00  9.36           N
"""
  assert convert_string_to_cif_long(text).strip() == """
data_phenix
loop_
  _atom_site.group_PDB
  _atom_site.id
  _atom_site.label_atom_id
  _atom_site.label_alt_id
  _atom_site.label_comp_id
  _atom_site.auth_asym_id
  _atom_site.auth_seq_id
  _atom_site.pdbx_PDB_ins_code
  _atom_site.Cartn_x
  _atom_site.Cartn_y
  _atom_site.Cartn_z
  _atom_site.occupancy
  _atom_site.B_iso_or_equiv
  _atom_site.type_symbol
  _atom_site.pdbx_formal_charge
  _atom_site.label_asym_id
  _atom_site.label_entity_id
  _atom_site.label_seq_id
  _atom_site.pdbx_PDB_model_num
  ATOM  1  N  .  VAL  AZXLONG  1  ?  -5.11100  0.04900  13.24500  1.000  9.36000  N  ?  A  ?  1  1

loop_
  _chem_comp.id
  VAL

loop_
  _struct_asym.id
  A
""".strip()

def tst_raises():
  # check passing behavior
  with raises(AssertionError) as e:
    raise AssertionError('abc')
  assert str(e.value) == 'abc'

  # check failing behavior
  try:
    with raises(RuntimeError) as e:
      raise AssertionError('def')
  except AssertionError as e:
    assert str(e) == 'def'

  # catch subclass
  with raises(Exception) as e:
    raise AssertionError('ghi')
  assert str(e.value) == 'ghi'

  # reject parent class
  try:
    with raises(ValueError) as e:
      raise Exception('jkl')
  except Exception as e:
    assert str(e) == 'jkl'

def exercise_dict():
  # test 1
  assert approx_equal({'a':1, 'b':2}, {'a':1, 'b':2})

  # test 2
  out = StringIO()
  assert not approx_equal({'a':1, 'b':2}, {'a':2, 'b':1}, out=out)
  assert not show_diff(out.getvalue().replace("1e-006", "1e-06"), """\
approx_equal eps: 1e-06
approx_equal multiplier: 10000000000.0
a: 1 approx_equal ERROR
a: 2 approx_equal ERROR
a:
b: 2 approx_equal ERROR
b: 1 approx_equal ERROR
b:
""")

  # test 3
  with raises(AssertionError) as e:
    approx_equal({'a':1, 'b':2}, {'a':1, 'b':2, 'c':3}, out=out)
  expected_error = """approx_equal ERROR: a1.keys() != a2.keys(): dict_keys(['a', 'b']) != dict_keys(['a', 'b', 'c'])"""
  if sys.version_info.major == 2:
    expected_error = """approx_equal ERROR: a1.keys() != a2.keys(): ['a', 'b'] != ['a', 'c', 'b']"""
  assert str(e.value) == expected_error

if (__name__ == "__main__"):
  tst_convert()
  tst_raises()
  exercise()
  exercise_dict()


 *******************************************************************************


 *******************************************************************************
libtbx/test_utils/parallel.py
from __future__ import absolute_import, division, print_function
from libtbx import easy_run
from libtbx import test_utils
import libtbx.load_env
from libtbx.utils import import_python_object, multi_out
from libtbx import group_args, Auto
from multiprocessing import Pool, cpu_count
from six.moves import cStringIO as StringIO
import traceback
import time
import os
import re
import sys
import re
import codecs

try:
  from enum import IntEnum
except ImportError:
  IntEnum = object

class Status(IntEnum):
  OK = 0
  WARNING = 1
  FAIL = 2
  EXPECTED_FAIL = 3
  EXPECTED_UNSTABLE = 4

QUIET = 0
DEFAULT_VERBOSITY = 1
EXTRA_VERBOSE = 2 # for nightly builds

def get_test_list(module_name, test_type='tst_list', valgrind=False,
         python_keyword_text=None, skip_missing = False):
  dist_path = libtbx.env.dist_path(module_name)
  if dist_path is None:
    if skip_missing:
      return []
    else:
      raise AssertionError("'%s' is not a valid CCTBX module." % module_name)
  # XXX don't check for file name, because import conventions differ among
  # derived modules - dist_path can be either the sys.path entry or the actual
  # module contents.  If the file is missing the import below will fail, which
  # is okay for testing purposes.
  try:
    tst_list = list(import_python_object(
      import_path="%s.run_tests.%s" % (module_name, test_type),
      error_prefix="",
      target_must_be="",
      where_str="").object)
  except AttributeError:
    tst_list = []
  build_path = libtbx.env.under_build(module_name)
  assert (build_path is not None) and (dist_path is not None)
  commands = []
  co = group_args(
    verbose=False,
    quick=True,
    valgrind=valgrind,
    python_keyword_text=python_keyword_text)
  for cmd in test_utils.iter_tests_cmd(
      co=co,
      build_dir=build_path,
      dist_dir=dist_path,
      tst_list=tst_list):
    commands.append(cmd)
  return commands

def get_module_tests(module_name, valgrind=False, slow_tests=False,
      python_keyword_text=None, skip_missing = False):
  tst_list = get_test_list(module_name, valgrind=valgrind,
     python_keyword_text=python_keyword_text, skip_missing = skip_missing)
  if slow_tests:
    tst_list.extend(get_test_list(module_name, test_type='tst_list_slow',
                    valgrind=valgrind,
                    python_keyword_text=python_keyword_text,
                    skip_missing = skip_missing))
  return tst_list

def get_module_expected_test_failures(module_name, skip_missing = False):
  return get_test_list(module_name, test_type='tst_list_expected_failures',
                    skip_missing = skip_missing)

def get_module_expected_unstable_tests(module_name, skip_missing = False):
  return get_test_list(module_name, test_type='tst_list_expected_unstable',
                    skip_missing = skip_missing)

def get_module_parallel_tests(module_name, skip_missing = False):
  return get_test_list(module_name, test_type='tst_list_parallel',
                     skip_missing = skip_missing)

def find_tests(dir_name):
  if not os.path.isdir(dir_name):
    raise RuntimeError("'%s' is not a directory." % dir_name)
  all_tests = []
  for root, dirnames, filenames in os.walk(dir_name):
    for file_name in filenames :
      base, ext = os.path.splitext(file_name)
      if (file_name.startswith("tst")) and (ext in [".py", ".csh", ".sh"]):
        all_tests.append(os.path.join(root, file_name))
  return all_tests

def run_command(command,
                verbosity=DEFAULT_VERBOSITY):
  try:
    t0=time.time()
    cmd_result = easy_run.fully_buffered(
      command=command,
      )
    cmd_result.wall_time = time.time()-t0
    cmd_result.error_lines = phenix_separate_output(cmd_result)
    return cmd_result
  except KeyboardInterrupt:
    traceback_str = "\n".join(traceback.format_tb(sys.exc_info()[2]))
    sys.stdout.write(traceback_str)
    return None

def phenix_separate_output(cmd_result):
  """Separates warning and error lines from a commands output streams.

  This will only give results if the phenix_regression package exists,
  and returns an empty list otherwise.
  """
  try:
    from phenix_regression.command_line import find_errors_and_warnings
  except ImportError:
    return []
  out = cmd_result.stdout_lines
  err = cmd_result.stderr_lines
  bad_lines = []
  for i, line in enumerate(out+err):
    if find_errors_and_warnings.is_error_or_warning(line, line.lower()):
      bad_lines.append(line)
  return bad_lines

def reconstruct_test_name(command):
  if hasattr(command, 'test_class') and hasattr(command, 'test_name'):
    return command.test_class, command.test_name

  pattern = '^[^"]*"([^"]*)"([^"]*)$'
  m = re.search(pattern, command)
  # command =  libtbx.python "/file.py" 90000
  #            \-- ignore --/ \--m1--/ \-m2-/
  if m:
    file = m.group(1)
    parameter = m.group(2).lstrip()
    filtered_parameter = re.sub(r"[^A-Za-z0-9\-_]", '', parameter)
    if filtered_parameter == '':
      test_name = file
    else:
      test_name = "%s.%s" % (file, filtered_parameter)

    pattern2 = r'^/?(.*?/((modules|build)/(cctbx\_project/|xia2/Test/)?))?(.*)/([^/&]*?)(\.py)?$'
    m2 = re.search(pattern2, file)
    if m2:
#     print "M (%s) (%s) (%s) (%s) (%s) (%s) (%s)" % (m2.group(1,2,3,4,5,6,7))
      test_name = m2.group(6).replace('/', '.')
      test_class = m2.group(5).replace('/', '.')
      is_python_dot_identifier = r"^([^\d\W]\w*)\.([^\d\W]\w*)\Z"
      if re.search(is_python_dot_identifier, parameter):
        # if parameter consists of two joined python identifiers, use it as test name
        test_class = "%s.%s" % (test_class, test_name)
        test_name = parameter
      elif filtered_parameter != '':
        # otherwise append sanitized (filtered) parameter to test name
        # so that each test has again a unique name
        test_name = "%s.%s" % (test_name, filtered_parameter)
      return (test_class, test_name)
    return (file, test_name)
  return (command, command)

def write_JUnit_XML(results, output_filename="output.xml"):
  """Write a JUnit XML test report to a file if junit_xml is available."""
  try:
    import junit_xml
  except ImportError:
    return

  test_cases = []
  for result in results:
    test_name = reconstruct_test_name(result.command)
    tc = junit_xml.TestCase(classname=test_name[0],
                  name=test_name[1],
                  elapsed_sec=result.wall_time,
                  stdout='\n'.join(result.stdout_lines),
                  stderr='\n'.join(result.stderr_lines))
    if result.return_code == 0:
      # Identify skipped tests
      output = '\n'.join(result.stdout_lines + result.stderr_lines)
      if re.search('skip', output, re.IGNORECASE):
        # find first line including word 'skip' and use it as message
        skipline = re.search('^((.*)skip(.*))$', output, re.IGNORECASE | re.MULTILINE).group(1)
        tc.add_skipped_info(skipline)
    elif result.alert_status == Status.EXPECTED_FAIL:
      tc.add_skipped_info("Expected test failure")
    elif result.alert_status == Status.EXPECTED_UNSTABLE:
      tc.add_skipped_info("Expected test instability")
    else:
      # Test failed. Extract error message and stack trace if possible
      error_message = 'exit code %d' % result.return_code
      error_output = '\n'.join(result.stderr_lines)
      if result.stderr_lines:
        error_message = result.stderr_lines[-1]
        if len(result.stderr_lines) > 20:
          error_output = '\n'.join(result.stderr_lines[-20:])
      tc.add_failure_info(message=error_message, output=error_output)
    test_cases.append(tc)
  ts = junit_xml.TestSuite("libtbx.run_tests_parallel", test_cases=test_cases)
  with codecs.open(output_filename, "w", encoding="utf-8") as f:
    ts.to_file(f, [ts], prettyprint=True, encoding="utf-8")

class run_command_list(object):
  def __init__(self,
                cmd_list,
                expected_failure_list=None,
                expected_unstable_list=None,
                parallel_list=None,
                nprocs=1,
                out=sys.stdout,
                log=None,
                verbosity=DEFAULT_VERBOSITY,
                max_time=180):
    if (log is None) : log = null_out()
    self.out = multi_out()
    self.log = log
    self.out.register("stdout", out)
    self.out.register("log", log)
    self.verbosity = verbosity
    self.quiet = (verbosity == 0)
    self.results = []
    self.pool = None

    self.expected_failure_list = expected_failure_list
    if self.expected_failure_list is None:
      self.expected_failure_list = []
    self.expected_unstable_list = expected_unstable_list
    if self.expected_unstable_list is None:
      self.expected_unstable_list = []
    self.parallel_list = parallel_list
    if self.parallel_list is None:
      self.parallel_list = []

    # Filter cmd list for duplicates.
    self.cmd_list = []
    for cmd in cmd_list :
      if (not cmd in self.cmd_list):
        self.cmd_list.append(cmd)
      else :
        print("Test %s repeated, skipping"%cmd, file=self.out)

    # Set number of processors.
    if (nprocs is Auto):
      nprocs = cpu_count()
    if len(self.parallel_list) == 0:
      nprocs = min(nprocs, len(self.cmd_list))

    # Starting summary.
    if (self.verbosity > 0):
      print("Running %d tests on %s processors:"%
        (len(self.cmd_list) + len(self.parallel_list), nprocs), file=self.out)
      for cmd in self.cmd_list:
        print("  %s"%cmd, file=self.out)
      for cmd in self.parallel_list:
        print("  %s [Parallel]"%cmd, file=self.out)
      print("", file=self.out)
      self.out.flush()

    # Either run tests in parallel or run parallel tests, but
    # can't run parallel tests in parallel (cctbx#95)
    os.environ['OPENBLAS_NUM_THREADS'] = "1"

    t_start = time.time()

    if nprocs > 1:
      # Run the tests with multiprocessing pool.
      self.pool = Pool(processes=nprocs)
      for command in self.cmd_list:
        self.pool.apply_async(
          run_command,
          [command, verbosity],
          callback=self.save_result)
      try:
        self.pool.close()
      except KeyboardInterrupt:
        print("Caught KeyboardInterrupt, terminating", file=self.out)
        self.pool.terminate()
      finally:
        try:
          self.pool.join()
        except KeyboardInterrupt:
          pass
    else:
      # Run tests serially.
      self.run_serial(self.cmd_list)

    # run parallel tests with multiple processors per test
    os.environ['OMP_NUM_THREADS'] = str(nprocs)
    self.run_serial(self.parallel_list)
    os.environ['OMP_NUM_THREADS'] = "1"

    # Print ending summary.
    t_end = time.time()
    print("="*80, file=self.out)
    print("", file=self.out)
    print("Tests finished. Elapsed time: %.2fs" %(t_end-t_start), file=self.out)
    print("", file=self.out)

    # Process results for errors and warnings.
    extra_stderr = len([result for result in self.results if result.stderr_lines])
    longjobs = [result for result in self.results if result.wall_time > max_time]
    warnings = [result for result in self.results if result.alert_status == Status.WARNING]
    failures = [result for result in self.results if result.alert_status == Status.FAIL]
    expected_failures = [result for result in self.results if result.alert_status == Status.EXPECTED_FAIL]
    expected_unstable = [result for result in self.results if result.alert_status == Status.EXPECTED_UNSTABLE]
    self.finished = len(self.results)
    self.failure = len(failures)
    self.warning = len(warnings)
    self.expected_failures = len(expected_failures)
    self.expected_unstable = len(expected_unstable)

    # Try writing the XML result file
    write_JUnit_XML(self.results, "output.xml")

    # Run time distribution.
    if (libtbx.env.has_module("scitbx")):
      try:
        from scitbx.array_family import flex
        print("Distribution of test runtimes:", file=self.out)
        hist = flex.histogram(flex.double([result.wall_time for result in self.results]), n_slots=10)
        hist.show(f=self.out, prefix="  ", format_cutoffs="%.1fs")
        print("", file=self.out)
      except Exception:
        # Failing on histogram rendering is a poor way to end a test run
        print("(Failed to render histogram of test results)", file=self.out)

    # Long job warning.
    if longjobs:
      print("", file=self.out)
      print("Warning: the following jobs took at least %d seconds:"%max_time, file=self.out)
      for result in sorted(longjobs, key=lambda result:result.wall_time):
        print("  %s: %.1fs"%(result.command, result.wall_time), file=self.out)
    else:
      # Just print 5 worst offenders to encourage developers to check them out
      print("", file=self.out)
      print("Warning: the following are 5 longest jobs:", file=self.out)
      for result in sorted(self.results, key=lambda result:-result.wall_time)[:5]:
        print("  %s: %.1fs"%(result.command, result.wall_time), file=self.out)
    print("Please try to reduce overall runtime - consider splitting up these tests.", file=self.out)
    print("", file=self.out)


    # Failures.
    if failures:
      print("", file=self.out)
      print("Error: the following jobs returned non-zero exit codes or suspicious stderr output:", file=self.out)
      print("", file=self.out)
      for result in warnings:
        self.display_result(result, alert=Status.WARNING, out=self.out, log_return=self.out, log_stderr=self.out)
      for result in failures:
        self.display_result(result, alert=Status.FAIL, out=self.out, log_return=self.out, log_stderr=self.out)
      print("", file=self.out)
      print("Please verify these tests manually.", file=self.out)
      print("", file=self.out)

    # Summary
    print("Summary:", file=self.out)
    print("  Tests run                    :",self.finished, file=self.out)
    print("  Failures                     :",self.failure, file=self.out)
    print("  Warnings (possible failures) :",self.warning, file=self.out)
    print("  Known Failures (% 3d)         :" % len(self.expected_failure_list),
      self.expected_failures, file=self.out)
    print("  Known Unstable (% 3d)         :" % len(self.expected_unstable_list),
      self.expected_unstable, file=self.out)
    print("  Stderr output (discouraged)  :",extra_stderr, file=self.out)
    if (self.finished != len(self.parallel_list) + len(self.cmd_list)):
      print("*" * 80, file=self.out)
      print("  WARNING: NOT ALL TESTS FINISHED!", file=self.out)
      print("*" * 80, file=self.out)

  def determine_result_status(self, result):
    alert = Status.OK
    # Note: error_lines is populated when package phenix_regression is configured
    if result.error_lines:
      if self.verbosity == EXTRA_VERBOSE:
        print("ERROR BEGIN "*10, file=self.out)
        print(result.error_lines, file=self.out)
        print('-'*80, file=self.out)
        print(result.stderr_lines, file=self.out)
        print("ERROR -END- "*10, file=self.out)
      alert = Status.WARNING
    if result.return_code != 0:
      if self.verbosity == EXTRA_VERBOSE:
        print("RETURN CODE BEGIN "*5, file=self.out)
        print(result.return_code, file=self.out)
        print("RETURN CODE -END- "*5, file=self.out)
      alert = Status.FAIL
      if result.command in self.expected_failure_list:
        alert = Status.EXPECTED_FAIL
      elif result.command in self.expected_unstable_list:
        alert = Status.EXPECTED_UNSTABLE
    return alert

  def run_serial(self, command_list):
    for command in command_list:
      rc = run_command(command, verbosity=self.verbosity)
      if self.save_result(rc) == False:
        break

  def save_result(self, result):
    if result is None:
      print("ERROR: job returned None, assuming CTRL+C pressed", file=self.out)
      if self.pool: self.pool.terminate()
      return False
    result.alert_status = self.determine_result_status(result)
    # If we got an 'OK' status but have stderr output, we can ignore this if
    # "OK" was the last line (e.g. python's unittest does this)
    if result.alert_status == Status.OK and result.stderr_lines:
      test_ok = (result.stderr_lines[-1].strip().startswith("OK"))
      if test_ok:
        result.stderr_lines = []
    self.results.append(result)
    kw = {}
    kw['out'] = self.out
    kw['log_return'] = self.log
    kw['log_stderr'] = True
    kw['log_stdout'] = self.log
    if self.quiet:
      kw['out'] = self.log
      kw['log_stderr'] = False
    elif self.verbosity == EXTRA_VERBOSE:
      kw['log_return'] = self.out
      kw['log_stderr'] = True
      kw['log_stdout'] = self.out
    # For any "not good" result, print out some more details
    elif not result.alert_status == Status.OK:
      kw['log_return'] = self.out
      kw['log_stderr'] = True
    self.display_result(
      result,
      alert=result.alert_status,
      **kw
    )

  def display_result(self, result, alert, out=None, log_return=True, log_stderr=True, log_stdout=False):
    status = {Status.OK: 'OK', Status.WARNING: 'WARNING', Status.FAIL: 'FAIL',
              Status.EXPECTED_FAIL: 'EXPECTED FAIL',
              Status.EXPECTED_UNSTABLE: 'EXPECTED UNSTABLE'}
    if out:
      print("%s [%s] %.1fs"%(result.command, status[alert], result.wall_time), file=out)
      out.flush()
    if log_return:
      print("  Time: %5.2f"%result.wall_time, file=log_return)
      print("  Return code: %s"%result.return_code, file=log_return)
      print("  OKs:", len([x for x in result.stdout_lines if 'OK' in x]), file=log_return)
      log_return.flush()
    if log_stdout and (len(result.stdout_lines) > 0):
      print("  Standard out:", file=log_stdout)
      print("    "+"\n    ".join(result.stdout_lines), file=log_stdout)
      log_stdout.flush()
    if log_stderr and (len(result.stderr_lines) > 0):
      print("  Standard error:", file=sys.stderr)
      print("    "+"\n    ".join(result.stderr_lines), file=sys.stderr)
      sys.stderr.flush()

def make_commands(files,python_keyword_text=""):
  commands = []
  non_executable = []
  unrecognized = []
  for file_name in files :
    if file_name.endswith('.py'):
      if python_keyword_text:
        cmd = 'libtbx.python %s "%s"'%(python_keyword_text,file_name)
      else:
        cmd = 'libtbx.python "%s"'%(file_name) # usual
    elif file_name.endswith('.sh'):
      # interpreter = 'libtbx.bash'
      cmd = file_name
    elif file_name.endswith('.csh'):
      # interpreter = 'libtbx.csh'
      cmd = file_name
    else:
      unrecognized.append(file_name)
      continue
    # cmd = '%s "%s"'%(interpreter, file_name)
    if cmd not in commands:
      commands.append(cmd)
  if (len(unrecognized) > 0):
    raise RuntimeError("""\
The following files could not be recognized as programs:

  %s

Please use the extensions .py, .sh, or .csh for all tests.  Shell scripts will
also need to be made executable.""" % "\n  ".join(unrecognized))
  if (len(non_executable) > 0):
    raise RuntimeError("""\
The following shell scripts are not executable:

  %s

Please enable execution of these to continue.""" % "\n  ".join(non_executable))
  return commands

def exercise():
  log=open("test_utils_parallel_zlog", "wb")
  f0 = open("test_parallel.csh", "wb")
  f0.write("""\
#!/bin/csh

echo "hello world"

exit""")
  f0.close()
  easy_run.call("chmod 755 test_parallel.csh")
  f1 = open("fails.py", "wb")
  f1.write("""\
print "this will crash"
assert 0""")
  f1.close()
  f2 = open("works.py", "wb")
  f2.write("""\
print "hello world"
print "OK"
""")
  f2.close()
  out = StringIO()
  run_command_list([
    "libtbx.python fails.py",
    "libtbx.python works.py",
    "csh test_parallel.csh",
    "./test_parallel.csh",
    ],
    nprocs=1,
    log=log,
    out=out)
  log.close()
  assert ("ERROR - return code 1" in out.getvalue())
  assert ("  Tests run                    : 4" in out.getvalue())
  assert ("  Failures                     : 1" in out.getvalue())
  assert ("  Warnings (possible failures) : 0" in out.getvalue())

if (__name__ == "__main__"):
  exercise()
  print("OK")


 *******************************************************************************


 *******************************************************************************
libtbx/thread_utils.py

from __future__ import absolute_import, division, print_function
from libtbx import easy_pickle
from libtbx.utils import Abort
from libtbx import object_oriented_patterns as oop
from libtbx import adopt_init_args
import libtbx.callbacks # import dependency
from six.moves import queue
import threading
import warnings
import traceback
import signal
import time
import os
import sys

class thread_with_callback_and_wait(threading.Thread):

  def __init__(self,
          run,
          callback,
          first_callback=None,
          run_args=(),
          run_kwds={}):
    self._run = run
    self._run_args = run_args
    self._run_kwds = dict(run_kwds) # copy, to avoid side effects
    self._run_kwds['callback'] = self._callback_proxy
    self._callback = callback
    self._first_callback = first_callback
    self._caller_wait = queue.Queue(0)
    self._queue = queue.Queue(0)
    threading.Thread.__init__(self)

  def run(self):
    self._run(*self._run_args, **self._run_kwds)

  def start_and_wait_for_first_callback(self):
    self.start()
    self._caller_wait.get()

  def _callback_proxy(self, *args, **kwds):
    if (self._first_callback is not None):
      result = self._first_callback(*args, **kwds)
      self._first_callback = None
      self._caller_wait.put(None)
    else:
      result = self._callback(*args, **kwds)
    if (result == False):
      return False
    last_iteration = self._queue.get()
    if (last_iteration):
      return False
    return result

  def resume(self, last_iteration=False):
    if (self.is_alive()):
      self._queue.put(last_iteration)
    return self

null_callback = oop.null()

class queue_monitor_thread(threading.Thread):
  def __init__(self, q, callback, sleep_interval=1.0):
    self.q = q
    self.cb = callback
    self._exit = False
    self._sleep_interval = sleep_interval
    threading.Thread.__init__(self)

  def run(self):
    t = self._sleep_interval
    while not self._exit :
      if (self.q.qsize() > 0):
        result = self.q.get(timeout=1)
        self.cb(result)
      else :
        time.sleep(t)

  def exit(self):
    self._exit = True

class file_monitor_thread(threading.Thread):
  def __init__(self, dir_name, file_names, callback, sleep_interval=1.0):
    assert os.path.isdir(dir_name)
    self.cb = callback
    self.dir_name = dir_name
    self.file_names = file_names
    self._exit = False
    self._sleep_interval = sleep_interval
    threading.Thread.__init__(self)

  def run(self):
    t = self._sleep_interval
    existing_files = []
    pending_files = []
    while not self._exit :
      files = os.listdir(self.dir_name)
      for file_name in files :
        if (file_name in existing_files):
          continue
        elif (file_name in pending_files):
          data = easy_pickle.load(os.path.join(self.dir_name, file_name))
          existing_files.append(file_name)
          #pending_files.remove(file_name)
          self.cb(data)
        elif (file_name in self.file_names):
          pending_files.append(file_name)
      time.sleep(t)

  def exit(self):
    self._exit = True

class simple_task_thread(threading.Thread):
  def __init__(self, thread_function, parent_window=None):
    threading.Thread.__init__(self)
    self.f = thread_function
    self.return_value = None
    self._exception = None

  def is_complete(self):
    return (self.return_value is not None)

  def exception_raised(self):
    return (self._exception is not None)

  def run(self):
    try :
      self.return_value = self.f()
    except Exception as e :
      sys.stderr.write(str(e))
      self._exception = e

  def get_error(self):
    return str(self._exception)

class child_process_message(object):
  def __init__(self, message_type, data):
    adopt_init_args(self, locals())

class child_process_pipe(object):
  def __init__(self, connection_object):
    adopt_init_args(self, locals())

  def send_confirm_abort(self, info=None):
    message = child_process_message(message_type="aborted", data=info)
    self.connection_object.send(message)

  def send(self, *args, **kwds):
    return self.connection_object.send(*args, **kwds)

  def recv(self, *args, **kwds):
    return self.connection_object.recv(*args, **kwds)

# fake filehandle, sends data up pipe to parent process
class stdout_pipe(object):
  def __init__(self, connection):
    self._c = connection
    self._data = ""

  def write(self, data):
    self._data += data
    self.flush() # this needs to be done immediately for some reason

  def flush(self):
    self._flush()

  def _flush(self):
    try :
      if self._data != "" :
        message = child_process_message(message_type="stdout", data=self._data)
        self._c.send(message)
        self._data = ""
    except Exception as e :
      sys.__stderr__.write("Exception in stdout_pipe: %s\n" % str(e))

  def close(self):
    pass

wait_before_flush = 0.2 # minimum time between send() calls
max_between_flush = 5

# this slows down the output so it won't stall a GUI
class stdout_pipe_buffered(stdout_pipe):
  def __init__(self, *args, **kwds):
    stdout_pipe.__init__(self, *args, **kwds)
    self._last_t = time.time()

  def write(self, data):
    self._data += data
    t = time.time()
    if (t >= (self._last_t + max_between_flush)):
      self.flush()

  def flush(self):
    t = time.time()
    if (t >= (self._last_t + wait_before_flush)):
      self._flush()
      self._last_t = t

  def close(self):
    pass

  def __del__(self):
    if self._data != "" :
      self._flush()

try:
  import multiprocessing
except ImportError:
  class process_with_callbacks(object):
    def __init__(self, *args, **kwds):
      raise ImportError("The multiprocessing module is not available.")
else:
  class _process_with_stdout_redirect(multiprocessing.Process):
    def __init__(self, group=None, target=None, name=None, args=(), kwargs={},
        connection=None, buffer_stdout=True):
      multiprocessing.Process.__init__(self, group, target, name, args, kwargs)
      self._c = connection
      if buffer_stdout :
        self._stdout = stdout_pipe_buffered(connection)
      else :
        self._stdout = stdout_pipe(connection)

    def run(self):
      import libtbx.callbacks # import dependency
      libtbx.call_back.add_piped_callback(self._c)
      old_stdout = sys.stdout
      sys.stdout = self._stdout
      message = None
      old_showwarning = warnings.showwarning
      warnings.showwarning = libtbx.call_back.showwarning
      try :
        try :
          if self._target :
            return_value = self._target(self._args, self._kwargs, self._c)
            message = child_process_message(message_type="return",
                                            data=return_value)
        except Abort :
          message = child_process_message(message_type="abort", data=None)
        except Exception as e :
          if e.__class__.__module__ == "Boost.Python" :
            e = RuntimeError("Boost.Python.%s: %s" % (e.__class__.__name__,
              str(e)))
          elif hasattr(e, "reset_module"):
            e.reset_module()
          traceback_str = "\n".join(traceback.format_tb(sys.exc_info()[2]))
          message = child_process_message(message_type="exception",
                                          data=(e, traceback_str))
      finally :
        if message is not None :
          self._stdout._flush()
          self._c.send(message)
        warnings.showwarning = old_showwarning
      sys.stdout = old_stdout

  #XXX: target functions must use this call signature!!!
  # I normally just write a very short wrapper function that invokes the
  # command I actually care about with args/kwds.
  def _process_target_function(args, kwds, connection):
    connection.send(True)

  # not really a process, but a wrapper for one.
  class process_with_callbacks(threading.Thread):
    def __init__(self,
        target,
        args=(),
        kwargs={},
        callback_stdout=null_callback,
        callback_final=null_callback,
        callback_err=null_callback,
        callback_abort=null_callback,
        callback_other=null_callback,
        callback_pause=null_callback,  # XXX experimental
        callback_resume=null_callback, # XXX experimental
        buffer_stdout=True,
        sleep_after_start=None):
      threading.Thread.__init__(self)
      self._target = target
      self._args = args
      self._kwargs = dict(kwargs)
      assert (hasattr(callback_stdout, "__call__") and
              hasattr(callback_final, "__call__") and
              hasattr(callback_err, "__call__") and
              hasattr(callback_abort, "__call__") and
              hasattr(callback_other, "__call__") and
              hasattr(callback_pause, "__call__") and
              hasattr(callback_resume, "__call__"))
      self._cb_stdout = callback_stdout
      self._cb_final  = callback_final
      self._cb_err    = callback_err
      self._cb_abort  = callback_abort
      self._cb_other  = callback_other
      self._cb_pause  = callback_pause
      self._cb_resume = callback_resume
      self._buffer_stdout = buffer_stdout
      self._abort = False
      self._killed = False
      self._completed = False
      self._error = False
      self._pid = None
      self._child_process = None
      assert ((sleep_after_start is None) or
              isinstance(sleep_after_start, int) or
              isinstance(sleep_after_start, float))
      self._sleep_after_start = sleep_after_start

    def abort(self, force=False):
      self._abort = True
      if (force):
        self.kill()

    # XXX experimental
    def kill(self):
      if (self._child_process is not None):
        try :
          self._child_process.terminate()
          self.join(0.1)
          self._killed = True
        except OSError as e :
          print(e)
        else :
          self._cb_abort()

    def send_signal(self, signal_number) : # XXX experimental
      """
      Signals the process using os.kill, which despite the name, can also
      pause or resume processes on Unix.
      """
      assert (self._child_process is not None)
      try :
        os.kill(self._child_process.pid, signal_number)
      except OSError as e :
        print(e)
        if (not self._child_process.is_alive()):
          self._cb_abort() # XXX not sure if this is ideal
        return False
      else :
        return True

    def pause(self):
      if sys.platform != "win32":
        if (self.send_signal(signal.SIGSTOP)):
          self._cb_pause()
      else:
        import psutil # really should use psutil globally as it is platform independent
        p = psutil.Process(self._child_process.pid)
        p.suspend()
        self._cb_pause()

    def resume(self):
      if sys.platform != "win32":
        if (self.send_signal(signal.SIGCONT)):
          self._cb_resume()
      else:
        import psutil
        p = psutil.Process(self._child_process.pid)
        p.resume()
        self._cb_resume()

    def run(self):
      if (self._sleep_after_start is not None):
        time.sleep(self._sleep_after_start)
      child_process = None
      parent_conn, child_conn = multiprocessing.Pipe()
      try :
        child_process = _process_with_stdout_redirect(
          target        = self._target,
          args          = self._args,
          kwargs        = self._kwargs,
          connection    = child_process_pipe(connection_object=child_conn),
          buffer_stdout = self._buffer_stdout)
        os.environ["OMP_NUM_THREADS"] = "1"
        self._child_process = child_process
        child_process.start()
      except Exception as e :
        sys.__stderr__.write("Error starting child process: %s\n" % str(e))
        child_process = None
      while child_process is not None :
        if (self._killed) : break
        if self._abort :
          child_process.terminate()
          self._cb_abort()
          break
        if not parent_conn.poll(1):
          continue
        pipe_output = parent_conn.recv()
        if isinstance(pipe_output, child_process_message):
          message = pipe_output
          (error, traceback_info) = (None, None)
          if message.message_type == "aborted" :
            child_process.terminate()
            self._cb_abort()
            break
          elif message.message_type == "stdout" :
            self._cb_stdout(message.data)
          elif message.message_type == "return" :
            self._cb_final(message.data)
            self._completed = True
            break
          elif message.message_type == "exception" :
            (error, traceback_info) = message.data
            self._cb_err(error, traceback_info)
            self._error = True
            child_process.terminate()
            break
        else :
          self._cb_other(pipe_output)
      if child_process is not None and child_process.is_alive():
        child_process.join()


 *******************************************************************************


 *******************************************************************************
libtbx/topological_sort.py
from __future__ import absolute_import, division, print_function
def stable(connections):
  ranks = {}
  for node, deps in connections:
    assert node not in ranks
    ranks[node] = len(ranks)
  deps_by_node = {}
  for node, deps in connections:
    deps_by_node[node] = deps
    for d in deps:
      if (d not in ranks):
        ranks[d] = len(ranks)
  lower_bounds = {}
  node_list = []
  def process(dependent_node, node):
    if (node in lower_bounds):
      return
    if (dependent_node is None):
      lower_bounds[node] = len(node_list)
      node_list.append(node)
    else:
      n = len(node_list)
      i = lower_bounds[dependent_node]
      while (i < n):
        if (node_list[i] == dependent_node):
          break
        i += 1
      else:
        raise AssertionError
      lower_bounds[node] = i
      node_list.insert(i, node)
    deps = deps_by_node.get(node)
    if (deps is not None):
      del deps_by_node[node]
      for rank,dependency in sorted([(ranks[d],d) for d in deps]):
        process(dependent_node=node, node=dependency)
  for node, deps in connections:
    process(dependent_node=None, node=node)
  return node_list

def strongly_connected_components(
      successors_by_node,
      omit_single_node_components=True,
      low_infinite=2**30):
  """
successors_by_node = {
  "node1": ["successor1", "successor2"],
  "node2": ["successor1", "successor3"]
}

http://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm
http://www.logarithmic.net/pfh-files/blog/01208083168/sort.py

Original implementation (by Paul Harrison), modified to accommodate
successors that do not appear as a key in successors_by_node.
  """
  result = []
  stack = []
  low = {}
  def visit(node):
    if (node in low):
      return
    num = len(low)
    low[node] = num
    stack_pos = len(stack)
    stack.append(node)
    for successor in successors_by_node.get(node, []):
      visit(successor)
      low[node] = min(low[node], low[successor])
    if (num == low[node]):
      component = tuple(stack[stack_pos:])
      del stack[stack_pos:]
      if (not omit_single_node_components or len(component) != 1):
        result.append(component)
      for item in component:
        low[item] = low_infinite
  for node in successors_by_node:
    visit(node)
  return result

def find_path(successors_by_node, from_node, to_node):
  visited = set()
  path = []
  def depth_first_search(node):
    visited.add(node)
    for successor in successors_by_node.get(node, []):
      if (successor == to_node):
        return True
      if (successor not in visited):
        path.append(successor)
        if (depth_first_search(successor)):
          return True
        path.pop()
    return False
  if (depth_first_search(from_node)):
    return path
  return None


 *******************************************************************************


 *******************************************************************************
libtbx/try_hello_world_c.py
r"""
Intended use:
  /usr/bin/python try_hello_world_c.py
  if ($status != 0) then
    echo "Problems with file access permissions, or broken compiler/linker" \
      "(status=$status)."
  endif
"""
from __future__ import absolute_import, division, print_function

import sys, os

def run(args):
  assert len(args) == 0
  try:
    print("""\
#include <stdio.h>
int
main(
  int argc,
  const char* argv[])
{
  printf("Hello, world.\\n");
  return 0;
}
""", file=open("libtbx_hello_world.c", "w"))
  except Exception: return 1
  if (not os.path.exists("libtbx_hello_world.c")):
    return 2
  if (os.path.exists("a.out")):
    try: os.remove("a.out")
    except Exception: return 3
  if (os.path.exists("a.out")):
    return 4
  if (sys.platform in ["linux", "linux2", "linux3", "darwin"]):
    try: os.system("gcc libtbx_hello_world.c")
    except Exception: return 5
  else:
    return 6
  if (not os.path.exists("a.out")):
    return 7
  try: os.remove("a.out")
  except Exception: return 8
  if (os.path.exists("a.out")):
    return 9
  try: os.remove("libtbx_hello_world.c")
  except Exception: return 10
  if (os.path.exists("libtbx_hello_world.c")):
    return 11
  return 0

if (__name__ == "__main__"):
  sys.exit(run(args=sys.argv[1:]))


 *******************************************************************************


 *******************************************************************************
libtbx/tst_binary_search.py
from __future__ import absolute_import, division, print_function
from libtbx import binary_search
import random
import sys
from six.moves import range

class random_callback(object):

  def __init__(O, rng, low, high):
    O.low = low
    O.high = high
    O.critical_point = min(
      low + max(1, int((high-low+1) * rng.random())),
      high)
    n_bad = int((high-low-1) * 0.2 + 0.5)
    O.bad_points = set()
    while (len(O.bad_points) != n_bad):
      n_block = min(rng.randrange(1,21), n_bad - len(O.bad_points))
      begin = low + 1 + rng.randrange(high-low-1)
      assert begin < high
      for i in range(begin, min(begin+n_block,high)):
        if (i != O.critical_point):
          O.bad_points.add(i)
          if (len(O.bad_points) == n_bad):
            break

  def __call__(O, point):
    assert point > O.low
    assert point < O.high
    if (point in O.bad_points):
      return None
    return (point < O.critical_point)

def run(args):
  assert len(args) == 0
  #
  info = binary_search.true_false_bad_biased_up(
    low=17, high=18, callback=None)
  assert info.number_of_iterations == 0
  #
  points_tested = []
  def callback(point):
    points_tested.append(point)
    return None
  info = binary_search.true_false_bad_biased_up(
    low=0, high=9, callback=callback)
  assert points_tested == [4,6,7,8,5,2,3,1] # see example in docstring
  del points_tested[:]
  info = binary_search.true_false_bad_biased_up(
    low=30, high=39, callback=callback)
  assert points_tested == [34,36,37,38,35,32,33,31]
  #
  for i_trial in range(32):
    callback = random_callback(
      rng=random.Random(i_trial),
      low=i_trial*13,
      high=i_trial*14+567)
    info = binary_search.true_false_bad_biased_up(
      low=callback.low,
      high=callback.high,
      callback=callback)
    assert info.low_point_false == callback.critical_point
    assert info.high_point_true < info.low_point_false
    assert info.high_point_true not in callback.bad_points
    for i in range(info.high_point_true+1, info.low_point_false):
      assert i in callback.bad_points
    assert info.bad_points.issubset(callback.bad_points)
    assert info.number_of_iterations >= info.number_of_callbacks
    assert info.bad_gap_width >= 0
  #
  print("OK")

if (__name__ == "__main__"):
  run(args=sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
libtbx/tst_citations.py
from __future__ import absolute_import, division, print_function

from io import StringIO

from libtbx import citations
from libtbx.test_utils import show_diff

def test_phil():
  citation = citations.citations_db['cctbx']
  assert(citation.authors ==
         'Grosse-Kunstleve RW, Sauter NK, Moriarty NW, Adams PD')

  journal = citations.journals_db['Nature']
  assert(journal.id_ASTM == 'NATUAS')

def test_sort():
  article_ids = ['phenix.polder', 'phenix2010', 'phenix.refine', 'phaser',
                 'molprobity']
  citation_list = [ citations.citations_db[article_id]
                    for article_id in article_ids ]

  # check unsorted first because sorting is in place
  with StringIO() as output:
    citations.show_citations(citation_list, out=output, sort_by_name=False)
    assert(output.getvalue().startswith('Liebschner'))

  with StringIO() as output:
    citations.show_citations(citation_list, out=output)
    assert(output.getvalue().startswith('Adams'))

def test_format():
  authors = citations.author_list_with_periods('Poon BK')
  assert(authors[0].count('B.K.') == 1)

  expected = """Adams PD, Afonine PV, Bunkoczi G, Chen VB, Davis IW, Echols N, Headd JJ, Hung LW, Kapral GJ, Grosse-Kunstleve RW, McCoy AJ, Moriarty NW, Oeffner R, Read RJ, Richardson DC, Richardson JS, Terwilliger TC, Zwart PH. (2010) PHENIX: a comprehensive Python-based system for macromolecular structure solution. Acta Cryst. D66:213-221."""
  citation = citations.citations_db['phenix2010']
  actual = citations.format_citation(citation)
  assert not show_diff(actual, expected)

  expected = """Grosse-Kunstleve R.W., Sauter N.K., Moriarty N.W., and Adams P.D. (2002). The Computational Crystallography Toolbox: crystallographic algorithms in a reusable software framework. J. Appl. Cryst. 35, 126-136."""
  citation = citations.citations_db['cctbx']
  actual = citations.format_citation_cell(citation)
  assert not show_diff(actual, expected)

  expected = """McCoy A.J., Grosse-Kunstleve R.W., Adams P.D., Winn M.D., Storoni L.C., & Read R.J. (2007). J Appl Crystallogr 40, 658-674."""
  citation = citations.citations_db['phaser']
  actual = citations.format_citation_iucr(citation)
  assert not show_diff(actual, expected)

  expected = """<b>Iterative-build OMIT maps: map improvement by iterative model building and refinement without model bias.</b> T.C. Terwilliger, R.W. Grosse-Kunstleve, P.V. Afonine, N.W. Moriarty, P.D. Adams, R.J. Read, P.H. Zwart, and L.-W. Hung. <a href="https://doi.org/doi:10.1107/S0907444908004319"><i>Acta Cryst.</i> D<b>64</b>, 515-524 (2008)</a>."""
  citation = citations.citations_db['autobuild_omit']
  actual = citations.format_citation_html(citation)
  assert not show_diff(actual, expected)

if (__name__ == '__main__'):
  test_phil()
  test_sort()
  test_format()
  print('OK')


 *******************************************************************************


 *******************************************************************************
libtbx/tst_containers.py
from __future__ import absolute_import, division, print_function
def exercise_oset():
  from libtbx.containers import OrderedSet as oset
  o = oset()
  assert repr(o) == "OrderedSet()"
  assert len(o) == 0
  o = oset([3,5,2,5,4,2,1])
  assert list(o) == [3, 5, 2, 4, 1]
  assert 3 in o
  assert 6 not in o
  o.add(3)
  assert len(o) == 5
  o.add(6)
  assert 6 in o
  assert list(reversed(o)) == [6,1,4,2,5,3]
  assert o.pop() == 6
  assert len(o) == 5
  assert o.pop(last=False) == 3
  assert len(o) == 4
  assert repr(o) == "OrderedSet([5, 2, 4, 1])"
  assert o == oset([5, 2, 4, 1])
  assert o != oset([5, 4, 2, 1])
  assert o == set([5, 2, 4, 1])
  assert o == set([5, 4, 2, 1])
  o1 = oset([6, 5, 4, 3, 2, 1])
  o2 = o1 - o
  assert o2 == oset([6, 3])

def exercise_odict():
  from libtbx.containers import OrderedDict as odict
  d = odict([('banana',3), ('apple',4), ('pear',1)])
  d.setdefault('orange', 2)
  assert 'orange' in d
  assert d['orange'] == 2
  assert list(d.keys()) == ['banana', 'apple', 'pear', 'orange']
  assert list(d.values()) == [3, 4, 1, 2]
  d = odict.fromkeys(('b','c','a'))
  assert list(d.keys()) == ['b', 'c', 'a']

def run(args):
  assert len(args) == 0
  exercise_oset()
  exercise_odict()
  print("OK")

if (__name__ == "__main__"):
  import sys
  run(args=sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
libtbx/tst_dlite.py
from __future__ import absolute_import, division, print_function
from libtbx import dlite
from libtbx.utils import null_out
import time
import sys, os

def update_target(out):
  target_db = dlite.target_db(file_name="tmp_dlite")
  pair_info = target_db.pair_info(
    source_path="tmp_source",
    target_path="tmp_target")
  update_done = False
  if (pair_info.needs_update):
    pair_info.start_building_target()
    with open(pair_info.target.path, "w") as fw, open(pair_info.source.path) as fr:
      fw.write(fr.read().upper())
    pair_info.done_building_target()
    update_done = True
  target_db.write()
  target_db.show(out=out)
  return update_done

def exercise(args, mtime_resolution=2):
  if ("--verbose" in args):
    out = None
  else:
    out = null_out()
  if (os.path.exists("tmp_dlite")):
    os.remove("tmp_dlite")
  with open("tmp_source", "w") as f:
    print("a", file=f)
  assert update_target(out=out)
  assert not update_target(out=out)
  with open("tmp_source", "w") as f:
    print("b", file=f)
  assert update_target(out=out)
  assert not update_target(out=out)
  time.sleep(mtime_resolution)
  assert not update_target(out=out)
  with open("tmp_source", "w") as f:
    print("b", file=f)
  assert not update_target(out=out)
  with open("tmp_target", "w") as f:
    print("B", file=f)
  assert not update_target(out=out)
  with open("tmp_target", "w") as f:
    print("C", file=f)
  assert update_target(out=out)
  assert not update_target(out=out)
  os.remove("tmp_target")
  assert update_target(out=out)
  assert not update_target(out=out)
  print("OK")

if (__name__ == "__main__"):
  exercise(args=sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
libtbx/tst_easy_mp.py
from __future__ import absolute_import, division, print_function
from libtbx import unpicklable
from libtbx.test_utils import Exception_expected
from libtbx import Auto
from six.moves import cStringIO as StringIO
import sys
from six.moves import range, zip

def exercise_func_wrapper_sub_directories():
  from libtbx.easy_mp import func_wrapper_sub_directories as f
  w = f("")
  assert w.sub_name_format == "%03d"
  w = f("x")
  assert w.sub_name_format == "x%03d"
  w = f("%05d")
  assert w.sub_name_format == "%05d"
  w = f("#")
  assert w.sub_name_format == "%01d"
  w = f("##")
  assert w.sub_name_format == "%02d"
  w = f("y####")
  assert w.sub_name_format == "y%04d"
  w = f("#z###")
  assert w.sub_name_format == "#z###%03d"

class potentially_large(unpicklable):

  def __init__(self, size):
    self.array = list(range(3, size+3))

  def __call__(self, i):
    return self.array[i]

def eval_parallel(
      data,
      func_wrapper="simple",
      index_args=True,
      log=None,
      exercise_out_of_range=False,
      exercise_fail=False):
  size = len(data.array)
  args = list(range(size))
  if (exercise_out_of_range):
    args.append(size)
  from libtbx import easy_mp
  if (exercise_fail):
    mp_results = easy_mp.pool_map(func=data, args=args)
  else:
    if (func_wrapper == "simple" and exercise_out_of_range):
      func_wrapper = "buffer_stdout_stderr"
    mp_results = easy_mp.pool_map(
      fixed_func=data,
      args=args,
      func_wrapper=func_wrapper,
      index_args=index_args,
      log=log)
  if (not exercise_out_of_range):
    assert mp_results == list(range(3, size+3))
  else:
    assert mp_results[:size] == list(zip([""]*size, range(3, size+3)))
    assert mp_results[size][0].startswith("CAUGHT EXCEPTION:")
    assert mp_results[size][0].find("IndexError: ") > 0
    assert mp_results[size][1] is None

def exercise(exercise_fail):
  exercise_func_wrapper_sub_directories()
  from libtbx import easy_mp
  mp_problem = easy_mp.detect_problem()
  if (mp_problem is not None):
    print("Skipping tst_easy_mp.py: %s" % mp_problem)
    return
  check_if_stacktrace_is_propagated_properly(method='threading', nproc=2)
  check_if_stacktrace_is_propagated_properly(method='multiprocessing', nproc=2)
  check_if_stacktrace_is_propagated_properly(method='threading', nproc=1)
  check_if_stacktrace_is_propagated_properly(method='multiprocessing', nproc=1)
  data = potentially_large(size=1000)
  eval_parallel(data)
  assert len(easy_mp.fixed_func_registry) == 0
  eval_parallel(data, func_wrapper=None, index_args=False)
  assert len(easy_mp.fixed_func_registry) == 1
  eval_parallel(data, func_wrapper=None, index_args=False, log=sys.stdout)
  assert len(easy_mp.fixed_func_registry) == 2
  sio = StringIO()
  eval_parallel(data, func_wrapper=None, index_args=False, log=sio)
  assert len(easy_mp.fixed_func_registry) == 3
  lines = sio.getvalue().splitlines()
  assert len(lines) == 2
  assert lines[0].startswith("multiprocessing pool size: ")
  assert lines[1].startswith("wall clock time: ")
  eval_parallel(data, exercise_out_of_range=True)
  if (exercise_fail):
    eval_parallel(data, exercise_fail=True)
    raise Exception_expected
  results = easy_mp.pool_map(fixed_func=data, args=list(range(1000)), processes=Auto)
  del data
  assert len(easy_mp.fixed_func_registry) == 0
  #
  from libtbx.clear_paths import \
    remove_or_rename_files_and_directories_if_possible as clear
  import os
  op = os.path
  def fixed_func(arg):
    print("hello world", arg)
    return 10*arg
  def go():
    return easy_mp.pool_map(
      fixed_func=fixed_func,
      func_wrapper="sub_directories",
      args=[1,2])
  clear(paths=["mp000", "mp001"])
  results = go()
  assert results == [(None, 10), (None, 20)]
  for i in [1,2]:
    with open("mp%03d/log" % (i-1), 'r') as fh:
      assert fh.read().splitlines() == ["hello world %d" % i]
  results = go()
  assert results == [
    ('sub-directory exists already: "mp000"', None),
    ('sub-directory exists already: "mp001"', None)]
  clear(paths=["mp001"])
  results = go()
  assert results == [
    ('sub-directory exists already: "mp000"', None),
    (None, 20)]
  clear(paths=["mp000", "mp001"])
  results = easy_mp.pool_map(
    fixed_func=fixed_func,
    func_wrapper=easy_mp.func_wrapper_sub_directories(makedirs_mode=0),
    args=[1,2])
  assert results == [
    ('cannot chdir to sub-directory: "mp000"', None),
    ('cannot chdir to sub-directory: "mp001"', None)]
  clear(paths=["mp000", "mp001"])
  clear(paths=["bf000", "bf001"])
  def bad_func(arg):
    raise RuntimeError(str(arg))
  results = easy_mp.pool_map(
    fixed_func=bad_func,
    func_wrapper="sub_directories:bf",
    args=[1,2])
  assert results == [
    ('CAUGHT EXCEPTION: "bf000/log"', None),
    ('CAUGHT EXCEPTION: "bf001/log"', None)]
  for i in [1,2]:
    with open("bf%03d/log" % (i-1)) as f:
      assert f.read().splitlines()[-1] == "RuntimeError: %d" % i
  out = StringIO()
  def simple_func(arg):
    from math import log
    x = float(arg)
    y = 0
    for i in range(2, 1000):
      y += log(x) / log(float(i))
    return y
  def cb(result):
    out.write("%.3f\n" % result)
  result = easy_mp.pool_map(
    func=simple_func,
    args=[1,2,3,4,5,6,7,8],
    call_back_for_serial_run=cb,
    processes=1)
  assert (out.getvalue() == """\
0.000
122.891
194.777
245.782
285.344
317.668
344.998
368.673
""")

def _may_divide_by_zero(divideby):
  '''
  A helper function for the check_if_stacktrace_is_propagated_properly test.
  Must be on module level for parallelization on Windows machines.
  '''
  return 7 / divideby

def check_if_stacktrace_is_propagated_properly(method, nproc):
  exception_seen = False
  from libtbx.easy_mp import parallel_map
  import traceback

  try:
    results = parallel_map(
      func=_may_divide_by_zero,
      iterable=[2,1,0],
      method=method,
      processes=nproc,
      preserve_exception_message=True)
  except ZeroDivisionError as e:
    exception_seen = True
    exc_type, exc_value, exc_traceback = sys.exc_info()
    assert "division by zero" in str(exc_value), "Exception value mismatch: '%s'" % exc_value

    stack_contains_fail_function = False
    # Two options: Either the original stack is available directly
    for (filename, line, function, text) in traceback.extract_tb(exc_traceback):
      if function == _may_divide_by_zero.__name__:
        stack_contains_fail_function = True
    # or it should be preserved in the string representation of the exception
    from libtbx.scheduling import stacktrace
    ex, st = stacktrace.exc_info()
    if ex is not None and _may_divide_by_zero.__name__ in "".join( st ):
      stack_contains_fail_function = True
    if not stack_contains_fail_function:
      print("Thrown exception: %s:" % str(e))
      traceback.print_tb(exc_traceback)
      print("")
      assert stack_contains_fail_function, "Stacktrace lost"
  except Exception as e:
    print("Exception type mismatch, expected ZeroDivisionError")
    raise
  assert exception_seen, "Expected exception not thrown"

def run(args):
  assert args in [[], ["--fail"]]
  exercise_fail = (len(args) != 0)
  exercise(exercise_fail)
  print("OK")

if (__name__ == "__main__"):
  run(args=sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
libtbx/tst_easy_mp_multicore.py
from __future__ import absolute_import, division, print_function

def factorial(n):
  f = 1
  while n>1:
    f*=n
    n-=1
  return f

def main(nproc=2):
  from libtbx import easy_mp
  argss = []
  for i in range(7):
    argss.append([i+1])
  for args, res, err_str in easy_mp.multi_core_run(factorial,
                                                   tuple(argss),
                                                   nproc,
                                                   ):
    if res:
      print (args, res)
    assert not err_str, 'Error: %s' % err_str

if __name__ == '__main__':
  main()


 *******************************************************************************


 *******************************************************************************
libtbx/tst_easy_mp_state.py
from __future__ import absolute_import, division, print_function
from libtbx import easy_mp
import os
import random
import string
import threading
import time
from six.moves import range

class state_object():
  ''' A simple thread-safe object keeping an internal state. '''
  def __init__(self):
    self._lock = threading.Lock()
    self._state = "-init-"

  def generate_state(self):
    with self._lock:
      oldstate = self._state
      self._state = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in list(range(6)))
      print("(%s-%5s) changing state from %s to %s" % (format(id(self), "#x"), os.getpid(), oldstate, self._state))
      return self._state

  def get_state(self):
    with self._lock:
      print("(%s-%5s) object in state %s" % (format(id(self), "#x"), os.getpid(), self._state))
      return self._state

def exercise_multiprocessing(mp_nproc=1, mp_threads=1, mp_method="multiprocessing", tasks=3):
  print("Running %s test with %d processes, %d threads, %d tasks" % \
    (mp_method, mp_nproc, mp_threads, tasks))

  # Create one shared instance of the state object and extract its initial state
  master_state_object = state_object()
  initial_state = master_state_object.get_state()

  # This is a function that changes the state on the object
  def change_stored_state(task):
    time.sleep(random.random() / 2 / tasks)
    master_state_object.generate_state()

  # Call the state-changing function in parallel
  easy_mp.parallel_map(
    iterable=range(tasks),
    func=change_stored_state,
    processes=mp_nproc,
    method=mp_method)

  # Get the final state of the object
  final_state = master_state_object.get_state()

  # Did it change?
  assert initial_state != final_state

def run():
  exercise_multiprocessing(mp_nproc=1, mp_method="threading")
  print("OK")

  exercise_multiprocessing(mp_nproc=2, mp_method="threading")
  print("OK")

  exercise_multiprocessing(mp_nproc=1, mp_method="multiprocessing")
  print("OK")

  #exercise_multiprocessing(mp_nproc=2, mp_method="multiprocessing")
  #
  # This will fail. Multiprocessing creates copy-on-write copies of the
  # initial object. Later changes in the state are lost.
  # Problems can occur when an instance of state_object does not check
  # that it has been copied, and when it carries an internal reference to
  # something that cannot be copied properly, possibly a C object, eg.
  # a BZ2File()-instance or, more probably, a file handle.
  # State_objects can not directly detect that it has been copied, so
  # as a fallback it could check whether os.getpid() has changed.

if __name__ == "__main__":
  run()


 *******************************************************************************


 *******************************************************************************
libtbx/tst_easy_pickle.py
from __future__ import absolute_import, division, print_function
from six.moves import range
def exercise(n, use_dumps=False):
  from libtbx import easy_pickle
  import time
  obj = []
  for i in range(n):
    obj.append([i,i])
  for dgz in ["", ".gz"]:
    t0 = time.time()
    if (use_dumps):
      print("dumps/loads")
      pickle_string = easy_pickle.dumps(obj=obj)
    else:
      file_name = "test.dat"+dgz
      print(file_name)
      easy_pickle.dump(file_name=file_name, obj=obj)
    print("  dump: %.2f s" % (time.time()-t0))
    del obj
    t0 = time.time()
    if (use_dumps):
      obj = easy_pickle.loads(pickle_string)
    else:
      obj = easy_pickle.load(file_name=file_name)
    print("  load buffered: %.2f s" % (time.time()-t0))
    if (use_dumps):
      break
    else:
      del obj
      t0 = time.time()
      obj = easy_pickle.load(
        file_name=file_name, faster_but_using_more_memory=False)
      print("  load direct: %.2f s" % (time.time()-t0))
  # XXX pickling and unpickling of libtbx.Auto
  # couldn't think of a better place to test this...
  from libtbx import Auto
  a = easy_pickle.dumps(Auto)
  b = easy_pickle.loads(a)
  assert (b is Auto) and (b == Auto)

def run(args):
  assert len(args) in [0,1]
  if (len(args) == 0):
    n = 100
  else:
    n = int(args[0])
    assert n >= 0
  for use_dumps in [False, True]:
    exercise(n, use_dumps)
  print("OK")

if (__name__ == "__main__"):
  import sys
  run(args=sys.argv[1:])


 *******************************************************************************


 *******************************************************************************
libtbx/tst_find_unused_imports.py
#!/usr/bin/env python
# coding: utf-8

"""
Test new features of the find_unused_imports_crude tool.

At the moment this only tests new features to verify that it's
not breaking *the existing behaviour*. Any unwanted existing
behaviour is not reinforced with this test.
"""

from __future__ import absolute_import, division, print_function
from libtbx.command_line.find_unused_imports_crude import inspect

test_cases_to_catch = {
    "from x import y": {"y"},
    "import x": {"x"},
    "import a,b,c\na, b": {"c"},
    "import x # noqa: W334": {"x"},
    "import x, y, z # noqa: W334": {"x", "y", "z"},
}

test_cases_to_ignore = [
    "from x import y # noqa",
    "import x\nx",
    "import x # noqa",
    "import x # noqa: F401",
    "import x, y, z # noqa: F401",
    "import x, y, z # noqa - all of these are essential",
]


def test_find_unused_imports_crude():
    # Check the cases where we expect to catch
    for case, results in test_cases_to_catch.items():
        assert set(inspect(case.splitlines())) == results, case

    # Test cases where we expect to pass
    for case in test_cases_to_ignore:
        assert not inspect(case.splitlines())


if __name__ == "__main__":
    test_find_unused_imports_crude()
    print("OK")


 *******************************************************************************


 *******************************************************************************
libtbx/tst_fully_buffered_timeout.py
from __future__ import absolute_import, division, print_function
from libtbx.easy_run import fully_buffered
from time import time
from libtbx.test_utils import approx_equal
import sys
import libtbx.load_env

def test_command(cmd, to, expected_time):
  t0 = time()
  fb = fully_buffered(command=cmd, timeout=to)
  t1 = time()
  if to > expected_time:
    assert fb.return_code == 0
  else:
    assert fb.return_code == -15
  assert approx_equal(t1-t0, expected_time, 0.5)

def exercise():
  test_command("sleep 1", 2, 1)
  test_command("sleep 1000", 2, 2)
  test_command("sleep 5", 3, 3)
  test_command("sleep 5", 10, 5)
  print("OK")

if __name__ == "__main__":
  if not libtbx.env.has_module(name="probe"):
    # This test does not need probe per se. This check is done to skip this
    # test in DIALS testing environment because of their mac mini
    # which fails this test occasionly.
    print("Skipping test: probe not configured")
  else:
    if sys.platform != 'win32':
      exercise()


 *******************************************************************************


 *******************************************************************************
libtbx/tst_mahalanobis.py
from __future__ import division
from io import StringIO

diamonds_short = '''"carat","cut","color","clarity","depth","table","price","x","y","z"
0.23,"Ideal","E","SI2",61.5,55,326,3.95,3.98,2.43
0.21,"Premium","E","SI1",59.8,61,326,3.89,3.84,2.31
0.23,"Good","E","VS1",56.9,65,327,4.05,4.07,2.31
0.29,"Premium","I","VS2",62.4,58,334,4.2,4.23,2.63
0.31,"Good","J","SI2",63.3,58,335,4.34,4.35,2.75
0.24,"Very Good","J","VVS2",62.8,57,336,3.94,3.96,2.48
0.24,"Very Good","I","VVS1",62.3,57,336,3.95,3.98,2.47
0.26,"Very Good","H","SI1",61.9,55,337,4.07,4.11,2.53
0.22,"Fair","E","VS2",65.1,61,337,3.87,3.78,2.49
0.23,"Very Good","H","VS1",59.4,61,338,4,4.05,2.39
0.3,"Good","J","SI1",64,55,339,4.25,4.28,2.73
0.23,"Ideal","J","VS1",62.8,56,340,3.93,3.9,2.46
0.22,"Premium","F","SI1",60.4,61,342,3.88,3.84,2.33
0.31,"Ideal","J","SI2",62.2,54,344,4.35,4.37,2.71
0.2,"Premium","E","SI2",60.2,62,345,3.79,3.75,2.27
0.32,"Premium","E","I1",60.9,58,345,4.38,4.42,2.68
0.3,"Ideal","I","SI2",62,54,348,4.31,4.34,2.68
0.3,"Good","J","SI1",63.4,54,351,4.23,4.29,2.7
0.3,"Good","J","SI1",63.8,56,351,4.23,4.26,2.71
'''

zz=[(0.23, 61.5, 326),
    (0.23, 56.9, 329),
    (0.23, 56.9, 349),
   ]

def mahalanobis_using_pandas():
  import numpy as np
  import pandas as pd
  from scipy.stats import chi2

  # calculateMahalanobis Function to calculate
  # the Mahalanobis distance
  def calculateMahalanobis(y=None, data=None, cov=None):

      y_mu = y - np.mean(data)
      print(data)
      print(y)
      print(y_mu)
      if not cov:
        cov = np.cov(data.values.T)
        print(cov)
      inv_covmat = np.linalg.inv(cov)
      left = np.dot(y_mu, inv_covmat)
      mahal = np.dot(left, y_mu.T)
      return mahal.diagonal()

  # data
  data = { 'Price': [100000, 800000, 650000, 700000,
                     860000, 730000, 400000, 870000,
                     780000, 400000],
           'Distance': [16000, 60000, 300000, 10000,
                        252000, 350000, 260000, 510000,
                        2000, 5000],
           'Emission': [300, 400, 1230, 300, 400, 104,
                        632, 221, 142, 267],
           'Performance': [60, 88, 90, 87, 83, 81, 72,
                           91, 90, 93],
           'Mileage': [76, 89, 89, 57, 79, 84, 78, 99,
                       97, 99]
             }

  # Creating dataset
  df = pd.DataFrame(data,columns=['Price', 'Distance',
                                  'Emission','Performance',
                                  'Mileage'])

  # Creating a new column in the dataframe that holds
  # the Mahalanobis distance for each row
  data = { 'Price': [100000],
           'Distance': [60000],
           'Emission': [104,
                        ],
           'Performance': [ 87],
           'Mileage': [99]
             }
  y=pd.DataFrame(data,columns=['Price', 'Distance',
                                  'Emission','Performance',
                                  'Mileage'])
  rc = calculateMahalanobis(y=y, data=df[[
    'Price', 'Distance', 'Emission','Performance', 'Mileage']])
  print(rc)

  # calculate p-value for each mahalanobis distance
  rc = 1 - chi2.cdf(rc, 4)
  print(rc)

  # display first five rows of dataframe
  print(df)
  assert 0

def another_pandas():
  import numpy as np

  def mahalanobis(x, y, cov=None):
      x_mean = np.mean(x,0)
      Covariance = np.cov(np.transpose(y))
      inv_covmat = np.linalg.inv(Covariance)
      x_minus_mn = x - x_mean
      D_square = np.dot(np.dot(x_minus_mn, inv_covmat), np.transpose(x_minus_mn))
      return D_square.diagonal()

  import pandas as pd

  # filepath = 'https://raw.githubusercontent.com/selva86/datasets/master/diamonds.csv'
  filepath='diamonds_short.csv'
  f=open(filepath, 'w')
  f.write(diamonds_short)
  del f
  df = pd.read_csv(filepath).iloc[:, [0,4,6]]
  df.head()

  #"carat","cut","color","clarity","depth","table","price","x","y","z"
  data = { 'carat': [],
           'depth': [],
           'price': []
        }
  for i in range(len(zz)):
    print(i,zz[i])
    data['carat'].append(zz[i][0])
    data['depth'].append(zz[i][1])
    data['price'].append(zz[i][2])
  print(data)

  X=pd.DataFrame(data,columns=['carat', 'depth', 'price'])
  print(X)

  X = np.asarray(X[['carat', 'depth', 'price']].values)
  Y = np.asarray(df[['carat', 'depth', 'price']].values)
  print(X)
  print(Y)

  rc=mahalanobis(X, Y)
  print('another_pandas',rc)

def main():
  from libtbx import math_utils
  import csv
  data=[]
  f=StringIO(diamonds_short)
  spamreader = csv.reader(f, delimiter=',', quotechar='|')
  for i, row in enumerate(spamreader):
    if not i: continue
    print(i,', '.join(row))
    data.append([float(row[0]), float(row[4]), float(row[6])])

  use_pandas=0
  if use_pandas:
    another_pandas()

  rc=math_utils.mahalanobis(zz, data)
  print(rc)
  rc=math_utils.mahalanobis_p_values(zz, data, verbose=False)
  print(rc)
  rc=math_utils.mahalanobis_p_values_outlier_indices(zz, data)
  assert rc==[2]
  for i, p in enumerate(math_utils.mahalanobis_p_values(zz, data)):
    outlier=''
    if i in rc:
      outlier='OUTLIER'
    print('  %s %0.4f %s' % (i,p,outlier))
  cov = math_utils.covariance_using_sklearn(data, verbose=False)
  print('Empirical',cov)
  mu=[2.60000000e-01, 6.18473684e+01, 3.38789474e+02]
  cov=[
      [1.55789474e-03, 3.08947368e-02, 1.42105263e-01],
      [3.08947368e-02, 3.50038781e+00, 5.25207756e+00],
      [1.42105263e-01, 5.25207756e+00, 5.39556787e+01]]
  inv_cov=[
      [ 9.20610847e+02, -5.25486917e+00, -1.91313813e+00],
      [-5.25486917e+00,  3.64538468e-01, -2.16444268e-02],
      [-1.91313813e+00, -2.16444268e-02,  2.56793212e-02]]

  rc1=math_utils.mahalanobis(zz, mu=mu, cov=cov)
  print(rc1)
  rc2=math_utils.mahalanobis_p_values(zz, mu=mu, inv_cov=inv_cov)
  print(rc2)
  rc=math_utils.mahalanobis_p_values_outlier_indices(zz, mu=mu, cov=cov)
  assert rc==[2]
  rc=math_utils.mahalanobis_p_values_outlier_indices(zz[:1], mu=mu, cov=cov)
  print(rc)

  cov = math_utils.covariance_using_sklearn(data, choice='Robust', verbose=False)
  print('Robust',cov)
  rc=math_utils.mahalanobis(zz, mu=mu, cov=cov)
  print(rc)
  rc=math_utils.mahalanobis_p_values_outlier_indices(zz, mu=mu, cov=cov)
  print(rc)
  assert rc==[1,2]

  cov = math_utils.covariance_using_numpy(data)
  print('???',cov)
  rc=math_utils.mahalanobis(zz, mu=mu, cov=cov)
  print(rc)

if __name__ == '__main__':
  main()


 *******************************************************************************


 *******************************************************************************
libtbx/tst_math_utils.py
from __future__ import absolute_import, division, print_function
from libtbx.test_utils import approx_equal
from six.moves import range

def exercise_integer():
  from libtbx.math_utils import iround, iceil, ifloor, nearest_integer
  assert iround(0) == 0
  assert iround(1.4) == 1
  assert iround(-1.4) == -1
  assert iround(1.6) == 2
  assert iround(-1.6) == -2
  assert iceil(0) == 0
  assert iceil(1.1) == 2
  assert iceil(-1.1) == -1
  assert iceil(1.9) == 2
  assert iceil(-1.9) == -1
  assert ifloor(0) == 0
  assert ifloor(1.1) == 1
  assert ifloor(-1.1) == -2
  assert ifloor(1.9) == 1
  assert ifloor(-1.9) == -2
  for i in range(-3,3+1):
    assert nearest_integer(i+0.3) == i
    assert nearest_integer(i+0.7) == i+1

def exercise_logical():
  from libtbx.math_utils import does_imply, are_equivalent
  #
  assert does_imply(True, True)
  assert not does_imply(True, False)
  assert does_imply(False, True)
  assert does_imply(False, False)
  #
  assert are_equivalent(True, True)
  assert not are_equivalent(True, False)
  assert not are_equivalent(False, True)
  assert are_equivalent(False, False)

def exercise_nested_loop():
  from libtbx.math_utils import nested_loop as nl
  assert [list(i) for i in nl([])] == []
  assert [list(i) for i in nl([1])] == [[0]]
  assert [list(i) for i in nl([1], open_range=False)] == [[0], [1]]
  assert [list(i) for i in nl([3])] == [[0], [1], [2]]
  assert [list(i) for i in nl(begin=[-2], end=[3])] == [
    [-2], [-1], [0], [1], [2]]
  assert [list(i) for i in nl(begin=[-1], end=[1], open_range=False)] == [
    [-1], [0], [1]]
  assert [list(i) for i in nl(begin=[-2,4], end=[3,6])] == [
    [-2, 4], [-2, 5], [-1, 4], [-1, 5], [0, 4], [0, 5], [1, 4], [1, 5],
    [2, 4], [2, 5]]
  assert [list(i) for i in nl(begin=[-2,4], end=[3,6], open_range=False)] == [
    [-2, 4], [-2, 5], [-2, 6], [-1, 4], [-1, 5], [-1, 6], [0, 4], [0, 5],
    [0, 6], [1, 4], [1, 5], [1, 6], [2, 4], [2, 5], [2, 6], [3, 4], [3, 5],
    [3, 6]]
  assert [list(i) for i in nl(begin=[-1,0,-1], end=[1,2,1])] == [
    [-1, 0, -1], [-1, 0, 0], [-1, 1, -1], [-1, 1, 0], [0, 0, -1], [0, 0, 0],
    [0, 1, -1], [0, 1, 0]]

def exercise_next_permutation():
  from libtbx.math_utils import next_permutation
  seq = []
  assert next_permutation(seq) is False
  seq = [0]
  assert next_permutation(seq) is False
  seq = [0,1]
  assert next_permutation(seq)
  assert seq == [1, 0]
  assert not next_permutation(seq)
  assert seq == [0, 1]
  seq = [0,1,2]
  result = []
  while True:
    result.append(tuple(seq))
    if (not next_permutation(seq)):
      break
  assert result == [
    (0, 1, 2),
    (0, 2, 1),
    (1, 0, 2),
    (1, 2, 0),
    (2, 0, 1),
    (2, 1, 0)]
  assert seq == [0,1,2]
  expected_n = 1
  for m in range(1,7):
    expected_n *= m
    seq = list(range(m))
    n = 0
    while True:
      n += 1
      if (not next_permutation(seq)):
        break
    assert seq == list(range(m))
    assert n == expected_n

def exercise_random_permutation_in_place():
  from libtbx.math_utils import random_permutation_in_place
  import random
  random.seed(0)
  l = list(range(8))
  for i_trial in range(10):
    random_permutation_in_place(list=l)
    if (l != list(range(8))):
      break
  else:
    raise AssertionError
  assert sorted(l) == list(range(8))

def exercise_prime_factors_of():
  from libtbx.math_utils import prime_factors_of
  assert prime_factors_of(n=1) == []
  prime_set = set()
  for n in range(2, 100):
    primes = prime_factors_of(n)
    pp = 1
    for p in primes:
      pp *= p
    assert pp == n
    prime_set.update(primes)
    if (n == 30):
      assert prime_set == set([2,3,5,7,11,13,17,19,23,29])
  for n in prime_set:
    assert prime_factors_of(n) == [n]
  assert len(prime_set) == 25

def exercise_normalize_angle():
  from libtbx.math_utils import normalize_angle as n
  import math
  for deg,period in [(False, 2*math.pi), (True, 360.)]:
    assert approx_equal(n(0, deg=deg), 0, eps=1.e-12)
    assert approx_equal(n(1.e-8, deg=deg), 1.e-8, eps=1.e-12)
    assert approx_equal(n(-1.e-8, deg=deg), period-1.e-8, eps=1.e-12)
    assert approx_equal(n(1, deg=deg), 1, eps=1.e-12)
    assert approx_equal(n(-1, deg=deg), period-1, eps=1.e-12)
  assert approx_equal(n(1.e+8), 1.9426951384)
  assert approx_equal(n(-1.e+8), 4.34049016878)
  assert approx_equal(n(1.e+8, deg=True), 280)
  assert approx_equal(n(-1.e+8, deg=True), 80)

def exercise_percentile_based_spread():
  from libtbx.math_utils import percentile_based_spread
  import random
  import math
  n_points = 123456
  deltas = []
  for i in range(n_points):
    x = random.gauss(100, 10)
    deltas.append(x)
  for i in range(1000):
    x = random.gauss(300, 30)
    deltas.append(x)
  pbs = percentile_based_spread(deltas)
  pbs_1 = percentile_based_spread(deltas, sort = False)
  assert abs(pbs - pbs_1) < 0.01
  rmsd = math.sqrt(sum([ x**2 for x in deltas]) / n_points)
  assert (pbs > 100) and (pbs < rmsd)
  # Test small list processing
  assert percentile_based_spread([1,1]) > 0


def exercise():
  exercise_integer()
  exercise_logical()
  exercise_nested_loop()
  exercise_next_permutation()
  exercise_random_permutation_in_place()
  exercise_prime_factors_of()
  exercise_normalize_angle()
  exercise_percentile_based_spread()
  print("OK")

if (__name__ == "__main__"):
  exercise()


 *******************************************************************************


 *******************************************************************************
libtbx/tst_object_oriented_patterns.py
from __future__ import absolute_import, division, print_function
from libtbx import object_oriented_patterns as oop
from libtbx.test_utils import approx_equal, Exception_expected
from six.moves import range

def exercise_injector():
  class a(object):
    """ doc for a """
    def __init__(self, i): self.i = i
    def get(self): return self.i
    def set(self, i): self.i = i

  class a_extension(oop.injector, a):
    """ doc for extension of a """
    var = 1
    def get_square(self): return self.get() * self.get()

  assert a.var == 1
  o = a(2)
  assert o.get() == 2
  assert o.get_square() == 4

  class b(object):
    def __init__(self, i): self.i = i
    def get(self): return self.i-1

  class c(b):
    def get(self): return self.i + 1

  class c_extension(oop.injector, c):
    def get_square(self): return self.get() * self.get()

  o = c(-3)
  assert o.get() == -2
  assert o.get_square() == 4

  try:
    class d(a): pass
    class d_extension(oop.injector, d):
      def get_square(self): return 0
  except AssertionError as err:
    assert str(err) == "class d already has attribute 'get_square'"

def exercise_memoize():
  diagnostic = []
  def f(x):
    """ Documentation for function f """
    diagnostic.append('+')
    return x+1

  mf = oop.memoize(f)
  assert mf(0) == 1
  assert diagnostic == ['+']
  assert mf(0) == 1
  assert diagnostic == ['+']
  assert mf(1) == 2
  assert diagnostic == ['+']*2
  assert mf(0) == 1
  assert diagnostic == ['+']*2
  assert mf(1) == 2
  assert diagnostic == ['+']*2
  try:
    mf(x=1)
  except TypeError as e:
    pass
  else:
    raise Exception_expected
  # Python 3.13 automatically strips leading spaces
  assert mf.__doc__ == """ Documentation for function f """ \
    or mf.__doc__ == """Documentation for function f """

def exercise_memoize_with_injector():
  class foo(object):
    def __init__(self, a):
      self.a = a
    def f(self, x):
      """ Documentation for method f """
      diagnostic.append('+')
      return self.a + x
    f = oop.memoize_method(f)

  diagnostic = []
  o1 = foo(1)
  o2 = foo(2)
  assert o1.f(4) == 5
  assert diagnostic == ['+']
  assert o2.f(5) == 7
  assert diagnostic == ['+']*2
  assert o1.f(2) == 3
  assert diagnostic == ['+']*3
  assert o1.f(4) == 5
  assert diagnostic == ['+']*3
  assert o2.f(5) == 7
  assert diagnostic == ['+']*3
  o3 = o1
  assert o3.f(4) == 5
  assert diagnostic == ['+']*3
  assert o3.f(1) == 2
  assert diagnostic == ['+']*4
  assert o1.f(1) == 2
  assert diagnostic == ['+']*4
  assert o1.f.__doc__ == " Documentation for method f " \
    or o1.f.__doc__ == "Documentation for method f "

  class _foo_extension(oop.injector, foo):
    def g(self, n):
      return 'x'*n
    g = oop.memoize_method(g)

  efoo = foo(0)
  assert efoo.g(2) == "xx"
  assert efoo.g(3) == "xxx"
  assert efoo.g(2) == "xx"
  assert efoo._memoized_g.cached == {(2,):'xx', (3,): 'xxx'}

def exercise_null():
  n = oop.null("a", 1, [])
  assert not n
  assert isinstance(n.f, oop.null)
  assert isinstance(n.g(1, b=2), oop.null)
  h = n.h = 2
  assert h == 2
  assert isinstance(n.h, oop.null)
  assert isinstance(n[23085], oop.null)
  x = n[234] = 2
  assert x == 2
  assert isinstance(n[234], oop.null)


def exercise_journal():

  class test(object):

    def __init__(self):
      for i in range(10):
        self.x = i
      for k in range(5):
        self.z = k

  # create a subclass of test that journals x and y attributes
  class test1(oop.journal_mixin, test):
    __journal__ = ["x", "y"]

  class test2(oop.journal_mixin, test):
    __journal__ = ["x", "z"]
    __journal_suffix__ = "_journal"

  a = test1()
  assert a.x == 9
  assert approx_equal(a.x_history, list(range(10)))
  try: a.y
  except AttributeError: pass
  else: raise Exception_expected
  try: a.y_history
  except AttributeError: pass
  else: raise Exception_expected
  assert a.z == 4
  try: a.z_history
  except AttributeError: pass
  else: raise Exception_expected
  b = test2()
  assert approx_equal(b.x_journal, list(range(10)))
  assert approx_equal(b.z_journal, list(range(5)))
  del b.x
  try: b.x
  except AttributeError: pass
  else: raise Exception_expected
  try: b.x_journal
  except AttributeError: pass
  else: raise Exception_expected


def run():
  exercise_journal()
  exercise_null()
  exercise_memoize()
  # tests for injection with metaclass
  exercise_injector()
  exercise_memoize_with_injector()

if __name__ == '__main__':
  run()


 *******************************************************************************
